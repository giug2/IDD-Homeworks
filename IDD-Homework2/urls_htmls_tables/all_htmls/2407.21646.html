<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent</title>
<!--Generated on Fri Aug 30 06:51:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.21646v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S1" title="In Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S2" title="In Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S2.SS1" title="In 2 Methods ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S2.SS2" title="In 2 Methods ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S2.SS3" title="In 2 Methods ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Data Driven Read-Write Policy: <span class="ltx_text ltx_font_typewriter">&lt;INPUT&gt;</span> and <span class="ltx_text ltx_font_typewriter">&lt;OUTPUT&gt;</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S2.SS4" title="In 2 Methods ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Context Information: <span class="ltx_text ltx_font_typewriter">&lt;LOAD_MEM&gt;</span> and <span class="ltx_text ltx_font_typewriter">&lt;UPDATE_MEM&gt;</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S2.SS5" title="In 2 Methods ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Multi-Modal Retrieval Augmented Generation: <span class="ltx_text ltx_font_typewriter">&lt;RETRIEVE&gt;</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S3" title="In Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Multi-Stage Training</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S3.SS1" title="In 3 Multi-Stage Training ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Pretraining</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S3.SS2" title="In 3 Multi-Stage Training ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Multi-task Continual Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S3.SS3" title="In 3 Multi-Stage Training ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Multi-task Supervised Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S3.SS4" title="In 3 Multi-Stage Training ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Multi-Modal Retriever Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4" title="In Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.SS1" title="In 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Evaluation Benchmark</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.SS2" title="In 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.SS3" title="In 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Translation Quality</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.SS4" title="In 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Latency</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.SS5" title="In 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Supplementary Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.SS6" title="In 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>MM-RAG Performance</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.SS6.SSS1" title="In 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6.1 </span>Retriever</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.SS6.SSS2" title="In 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6.2 </span>ICL Performance</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.SS7" title="In 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>Case Study</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S5" title="In 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S5.SS0.SSS0.Px1" title="In 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title">Large language model.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S5.SS0.SSS0.Px2" title="In 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title">Simultaneous Speech Translation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph">
<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S5.SS0.SSS0.Px3" title="In 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title">Human Evaluation.</span></a>
<ol class="ltx_toclist ltx_toclist_paragraph">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S6" title="In Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A1" title="In Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Human Evaluation Guidelines</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A1.SS1" title="In Appendix A Human Evaluation Guidelines ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Key Indicator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A1.SS2" title="In Appendix A Human Evaluation Guidelines ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Evaluation Process</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A1.SS3" title="In Appendix A Human Evaluation Guidelines ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Correlation with Automatic Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A2" title="In Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Supplementary Materials</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A2.SS1" title="In Appendix B Supplementary Materials ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Supplementary Case Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A2.SS2" title="In Appendix B Supplementary Materials ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Example of Detailed Evaluation Result on RealSI</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Cross Language Agent Team 
<br class="ltx_break"/>ByteDance Research
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">In this paper, we present <span class="ltx_text ltx_font_bold" id="id1.id1.1">C</span>ross <span class="ltx_text ltx_font_bold" id="id1.id1.2">L</span>anguage <span class="ltx_text ltx_font_bold" id="id1.id1.3">A</span>gent - <span class="ltx_text ltx_font_bold" id="id1.id1.4">S</span>imultaneous <span class="ltx_text ltx_font_bold" id="id1.id1.5">I</span>nterpretation, CLASI, a high-quality and human-like Simultaneous Speech Translation (SiST)<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>In this paper, we use Simultaneous Interpretation and Simultaneous Speech Translation interchangeably.</span></span></span> System.
Inspired by professional human interpreters, we utilize a novel data-driven read-write strategy to balance the translation quality and latency.
To address the challenge of translating in-domain terminologies, CLASI employs a multi-modal retrieving module to obtain relevant information to augment the translation.
Supported by LLMs, our approach can generate error-tolerated translation by considering the input audio, historical context, and retrieved information.
Experimental results show that our system outperforms other systems by significant margins. Aligned with professional human interpreters, we evaluate CLASI with a better human evaluation metric, valid information proportion (VIP), which measures the amount of information that can be successfully conveyed to the listeners.
In the real-world scenarios, where the speeches are often disfluent, informal, and unclear, CLASI achieves VIP of 81.3% and 78.0% for Chinese-to-English and English-to-Chinese translation directions, respectively. In contrast, state-of-the-art commercial or open-source systems only achieve 35.4% and 41.6%. On the extremely hard dataset, where other systems achieve under 13% VIP, CLASI can still achieve 70% VIP. Demonstrations and human-annotated test sets are available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://byteresearchcla.github.io/clasi" title="">https://byteresearchcla.github.io/clasi</a>.</p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">{CJK}</span>
<p class="ltx_p" id="p1.2">UTF8gbsn</p>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="415" id="S0.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S0.F1.3.2" style="font-size:90%;">Performance evaluation. CLASI significantly outperforms the leading commercial and open-source systems using a more reliable VIP metric, achieving human interpreter parity. </span></figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Simultaneous speech translation (SiST) is recognized as one of the most challenging tasks in the translation domain <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib33" title="">jones2014conference </a></cite>.
Machine-assisted automatic interpretation has been receiving much attention in the natural language processing (NLP) community <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib18" title="">iwslt-2021-international </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib19" title="">iwslt-2020-international </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib66" title="">iwslt-2022-international </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib65" title="">iwslt-2023-international </a></cite>.
Traditional simultaneous translation approaches <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib11" title="">cho2016can </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib24" title="">gu2017learning </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib89" title="">zhao2021volctrans </a></cite>
usually employs a cascaded system, involving a streaming Automatic Speech Recognition (ASR) model, a punctuation model and a Machine Translation (MT) model.
However, such cascaded systems often suffer error propagation and latency from the ASR module.
Despite these advancements in both academic SiST models <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib7" title="">barrault2023seamless </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib22" title="">fukuda-etal-2023-naist </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib41" title="">liu2024recentadvancesendtoendsimultaneous </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib51" title="">papi-etal-2023-direct </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib63" title="">ren2020simulspeech </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib84" title="">zeng-etal-2021-realtrans </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib87" title="">zhang2023end </a></cite> and commercial SiST engines, the translation quality is still far from satisfactory.
As shown in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S0.F1" title="In Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>, we conduct a human assessment of the current accessible SiST systems. From the user-centered perspective, these systems only deliver less than 42% of the valid information to listeners, which heavily affects communication effectiveness.
In contrast, professional human interpreters usually deliver more than 70% of the necessary information <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib10" title="">chmiel2021effects </a></cite> and 95% ideally. Thus in this paper, we use 80% to indicate high-level human interpreters.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Motivated by the huge success of LLMs in machine translation <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib2" title="">achiam2023gpt </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib9" title="">brown2020gpt3 </a></cite> and speech translation <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib12" title="">chu2023qwen </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib30" title="">huang2023speechtranslationlargelanguage </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib62" title="">reid2024gemini </a></cite>, we propose to employ LLMs to accomplish the SiST task.
Specifically, we identify three primary challenges.
First, a key challenge for incorporating LLM into the SiST is the read-write policy, where LLM needs to provide partial translation for input speech.
Second, achieving human equivalent performance requires understanding and translation of terminologies and uncommon phrases that LLMs cannot learn from training data.
Lastly, the scarcity of training data continues to hinder the performance on the SiST task.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address these challenges, we introduce our end-to-end approach, CLASI, a <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">C</span>ross-<span class="ltx_text ltx_font_bold" id="S1.p3.1.2">L</span>ingual <span class="ltx_text ltx_font_bold" id="S1.p3.1.3">A</span>gent that accomplishes <span class="ltx_text ltx_font_bold" id="S1.p3.1.4">S</span>imultaneous <span class="ltx_text ltx_font_bold" id="S1.p3.1.5">I</span>nterpretation by iteratively performing multiple actions, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">2</span></a>.
Regarding the first challenge, we imitate professional human interpreters to learn their policy of segmenting a complete sentence into several semantic “chunks” through syntactic boundaries (pauses, commas, conjunctions, etc.) and contextual meaning.
To enable CLASI to learn such a policy, we follow a data-driven policy learning process and invite human interpreters to annotate real-world speech, which includes the read-write timing for segmentation. From the data, CLASI learns the robust read-write policy for SiST from humans.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">For the second challenge, we include two external modules to augment our CLASI agent: an external knowledge database that stores terminologies and paired translations, and a memory that stores the context of speech.
However, the external knowledge database may contain tremendous terms that not only increase the inference time but may also lower the performance of our approach because of noisy intervention. Therefore, we propose a novel Multi-Modal Retrieval Augmented Generation (MM-RAG) process.
A multi-modal retriever extracts knowledge from the external database based on the speech input.
The retrieved information and the context from memory are then appended to the prompt of our LLM agent to augment the translation through in-context learning.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Addressing the data scarcity of the SiST task, we adopt a three-stage training methodology: pretraining, continual training, and fine-tuning. First, our LLM and audio encoder are independently pretrained on our large-size in-house datasets.
Then, our model is continually trained with billions of tokens of mediocre-quality synthesized speech translation data, aiming to align the speech and text modalities.
We also include multiple tasks to enhance the in-context learning ability of LLM to better utilize the contextual information from the retriever and prior translation.
In the last stage, we fine-tune the model with a small amount of human-annotated data, further imitating professional human interpreters to improve the robustness and translation quality.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="316" id="S1.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.4.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S1.F2.5.2" style="font-size:90%;">Overall framework of CLASI. The process begins in Step 1, where CLASI processes the incoming audio data. Optionally, the retriever is activated to obtain the relevant information from the external knowledge database. For instance, translating “<span class="ltx_text ltx_framed ltx_framed_underline" id="S1.F2.5.2.1">伊辛模型</span>” to “<span class="ltx_text ltx_framed ltx_framed_underline" id="S1.F2.5.2.2">Ising model</span>” for accurate speech translation. Step 3 involves accessing transcription (optional) and translation in the last round memory. Steps 4 and 5 entail using the Chain-of-Thought (CoT) method to generate both the transcription (optional) and translation, followed by a memory update. The cycle then repeats from Step 1 for the subsequent speech segment.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In addition, we would like to highlight that the conventional automatic evaluation metrics <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib52" title="">papi2022over </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib54" title="">papineni2002bleu </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib61" title="">rei2020comet </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib68" title="">sellam2020bleurt </a></cite> of simultaneous interpretation might not be good indicators for reflecting the performance of SiST, which often contains compaction, abstraction, and paraphrasing.
Aligned with human interpreters <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib49" title="">moores2024nerle </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib81" title="">wu2010assessing </a></cite>, we propose a new evaluation metric named Valid Information Proportion (VIP)<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Detailed guidelines of our proposed VIP metric can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A1" title="Appendix A Human Evaluation Guidelines ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">A</span></a>.</span></span></span>. VIP represents the percentage of information that can be precisely delivered, reflecting the central objective of SiST: communication in real-time.
Through thorough human evaluation on diverse and challenging real-world long speech datasets, our approach outperforms other currently accessible systems by a large margin.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S0.F1" title="Figure 1 ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">1</span></a>, taking the Chinese-to-English direction as an example, CLASI achieves a VIP score of 81.3%, significantly narrowing the gap between machine-assisted systems and human interpreters.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Our contributions can be summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce our end-to-end approach, CLASI, an LLM agent that is designed to perform high-quality and human-like simultaneous translation. Through human evaluation, our approach demonstrates significantly better performance compared to existing accessible SiST systems.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose a new data-driven read-write strategy by imitating professional human interpreters. Without the requirement of complicated human pre-design, the strategy could balance translation quality and latency effortlessly. Unlike most commercial systems where the outputs are frequently rewritten during the translation process for better quality, our strategy guarantees all the outputs are deterministic while maintaining high quality.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Motivated by the preparatory trajectory of human interpreters, we introduce a novel Multi-Modal Retrieval Augmented Generation (MM-RAG) process that empowers the LLM with domain-specific knowledge in real time. The proposed module
further improves the translation quality with minimal computational overhead during inference.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We work closely with professional human interpreters to develop our evaluation strategy, Valid Information Proportion (VIP), and detailed guidelines are open-sourced. Meanwhile, we release a human-annotated test set focusing on diverse real-world scenarios and long speech translations.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="416" id="S2.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.7.3.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.4.2" style="font-size:90%;">Architecture of CLASI agent. At round <math alttext="r" class="ltx_Math" display="inline" id="S2.F3.3.1.m1.1"><semantics id="S2.F3.3.1.m1.1b"><mi id="S2.F3.3.1.m1.1.1" xref="S2.F3.3.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.F3.3.1.m1.1c"><ci id="S2.F3.3.1.m1.1.1.cmml" xref="S2.F3.3.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.3.1.m1.1d">r</annotation><annotation encoding="application/x-llamapun" id="S2.F3.3.1.m1.1e">italic_r</annotation></semantics></math>, our model processes the current input audio stream alongside the memory from the previous round (<math alttext="r-1" class="ltx_Math" display="inline" id="S2.F3.4.2.m2.1"><semantics id="S2.F3.4.2.m2.1b"><mrow id="S2.F3.4.2.m2.1.1" xref="S2.F3.4.2.m2.1.1.cmml"><mi id="S2.F3.4.2.m2.1.1.2" xref="S2.F3.4.2.m2.1.1.2.cmml">r</mi><mo id="S2.F3.4.2.m2.1.1.1" xref="S2.F3.4.2.m2.1.1.1.cmml">−</mo><mn id="S2.F3.4.2.m2.1.1.3" xref="S2.F3.4.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.F3.4.2.m2.1c"><apply id="S2.F3.4.2.m2.1.1.cmml" xref="S2.F3.4.2.m2.1.1"><minus id="S2.F3.4.2.m2.1.1.1.cmml" xref="S2.F3.4.2.m2.1.1.1"></minus><ci id="S2.F3.4.2.m2.1.1.2.cmml" xref="S2.F3.4.2.m2.1.1.2">𝑟</ci><cn id="S2.F3.4.2.m2.1.1.3.cmml" type="integer" xref="S2.F3.4.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.4.2.m2.1d">r-1</annotation><annotation encoding="application/x-llamapun" id="S2.F3.4.2.m2.1e">italic_r - 1</annotation></semantics></math>), and any retrieved knowledge. CLASI generates a response based on specified instructions and concurrently updates its memory. Additionally, the model determines the cut-off timestamp of the last semantic chunk. For instance, in the provided example, the phrase preceding “<span class="ltx_text ltx_framed ltx_framed_underline" id="S2.F3.4.2.1">就在</span>” is identified as a complete semantic chunk, with the cut-off timestamp positioned right after this phrase. </span></figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Framework</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.9"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S1.F2" title="In 1 Introduction ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> presents a flow of operation of CLASI. To perform the SiST task, we design 5 operations: <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p1.9.1">&lt;INPUT&gt;</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p1.9.2">&lt;OUTPUT&gt;</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p1.9.3">&lt;RETRIEVE&gt;</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p1.9.4">&lt;LOAD_MEM&gt;</span>, and <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p1.9.5">&lt;UPDATE_MEM&gt;</span>. The following sections describe the details of each operation.
As further illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S2.F3" title="In 2 Methods ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>, CLASI is an LLM agent that can take input speech, instruction, relevant information retrieved from external knowledge, and last round memory as context. The memory stores previous transcriptions (optional) and translations. At round <math alttext="r" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">italic_r</annotation></semantics></math>, it first reads speech <math alttext="{\mathbf{x}}_{t^{r-1}:T^{r}}" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><msub id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">𝐱</mi><mrow id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml"><msup id="S2.SS1.p1.2.m2.1.1.3.2" xref="S2.SS1.p1.2.m2.1.1.3.2.cmml"><mi id="S2.SS1.p1.2.m2.1.1.3.2.2" xref="S2.SS1.p1.2.m2.1.1.3.2.2.cmml">t</mi><mrow id="S2.SS1.p1.2.m2.1.1.3.2.3" xref="S2.SS1.p1.2.m2.1.1.3.2.3.cmml"><mi id="S2.SS1.p1.2.m2.1.1.3.2.3.2" xref="S2.SS1.p1.2.m2.1.1.3.2.3.2.cmml">r</mi><mo id="S2.SS1.p1.2.m2.1.1.3.2.3.1" xref="S2.SS1.p1.2.m2.1.1.3.2.3.1.cmml">−</mo><mn id="S2.SS1.p1.2.m2.1.1.3.2.3.3" xref="S2.SS1.p1.2.m2.1.1.3.2.3.3.cmml">1</mn></mrow></msup><mo id="S2.SS1.p1.2.m2.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S2.SS1.p1.2.m2.1.1.3.1.cmml">:</mo><msup id="S2.SS1.p1.2.m2.1.1.3.3" xref="S2.SS1.p1.2.m2.1.1.3.3.cmml"><mi id="S2.SS1.p1.2.m2.1.1.3.3.2" xref="S2.SS1.p1.2.m2.1.1.3.3.2.cmml">T</mi><mi id="S2.SS1.p1.2.m2.1.1.3.3.3" xref="S2.SS1.p1.2.m2.1.1.3.3.3.cmml">r</mi></msup></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">𝐱</ci><apply id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3"><ci id="S2.SS1.p1.2.m2.1.1.3.1.cmml" xref="S2.SS1.p1.2.m2.1.1.3.1">:</ci><apply id="S2.SS1.p1.2.m2.1.1.3.2.cmml" xref="S2.SS1.p1.2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.3.2.1.cmml" xref="S2.SS1.p1.2.m2.1.1.3.2">superscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.3.2.2.cmml" xref="S2.SS1.p1.2.m2.1.1.3.2.2">𝑡</ci><apply id="S2.SS1.p1.2.m2.1.1.3.2.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3.2.3"><minus id="S2.SS1.p1.2.m2.1.1.3.2.3.1.cmml" xref="S2.SS1.p1.2.m2.1.1.3.2.3.1"></minus><ci id="S2.SS1.p1.2.m2.1.1.3.2.3.2.cmml" xref="S2.SS1.p1.2.m2.1.1.3.2.3.2">𝑟</ci><cn id="S2.SS1.p1.2.m2.1.1.3.2.3.3.cmml" type="integer" xref="S2.SS1.p1.2.m2.1.1.3.2.3.3">1</cn></apply></apply><apply id="S2.SS1.p1.2.m2.1.1.3.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.3.3.1.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3">superscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.3.3.2.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3.2">𝑇</ci><ci id="S2.SS1.p1.2.m2.1.1.3.3.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3.3">𝑟</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">{\mathbf{x}}_{t^{r-1}:T^{r}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">bold_x start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT : italic_T start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="t^{r-1}" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.1"><semantics id="S2.SS1.p1.3.m3.1a"><msup id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml">t</mi><mrow id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml"><mi id="S2.SS1.p1.3.m3.1.1.3.2" xref="S2.SS1.p1.3.m3.1.1.3.2.cmml">r</mi><mo id="S2.SS1.p1.3.m3.1.1.3.1" xref="S2.SS1.p1.3.m3.1.1.3.1.cmml">−</mo><mn id="S2.SS1.p1.3.m3.1.1.3.3" xref="S2.SS1.p1.3.m3.1.1.3.3.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">superscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2">𝑡</ci><apply id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3"><minus id="S2.SS1.p1.3.m3.1.1.3.1.cmml" xref="S2.SS1.p1.3.m3.1.1.3.1"></minus><ci id="S2.SS1.p1.3.m3.1.1.3.2.cmml" xref="S2.SS1.p1.3.m3.1.1.3.2">𝑟</ci><cn id="S2.SS1.p1.3.m3.1.1.3.3.cmml" type="integer" xref="S2.SS1.p1.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">t^{r-1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">italic_t start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT</annotation></semantics></math> is the predicted cut-off time of round <math alttext="r-1" class="ltx_Math" display="inline" id="S2.SS1.p1.4.m4.1"><semantics id="S2.SS1.p1.4.m4.1a"><mrow id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml">r</mi><mo id="S2.SS1.p1.4.m4.1.1.1" xref="S2.SS1.p1.4.m4.1.1.1.cmml">−</mo><mn id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><minus id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1.1"></minus><ci id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2">𝑟</ci><cn id="S2.SS1.p1.4.m4.1.1.3.cmml" type="integer" xref="S2.SS1.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">r-1</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.1d">italic_r - 1</annotation></semantics></math> and <math alttext="T^{r}" class="ltx_Math" display="inline" id="S2.SS1.p1.5.m5.1"><semantics id="S2.SS1.p1.5.m5.1a"><msup id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.2" xref="S2.SS1.p1.5.m5.1.1.2.cmml">T</mi><mi id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml">r</mi></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">superscript</csymbol><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">𝑇</ci><ci id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">T^{r}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.5.m5.1d">italic_T start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT</annotation></semantics></math> is the end time for audio stream at round <math alttext="r" class="ltx_Math" display="inline" id="S2.SS1.p1.6.m6.1"><semantics id="S2.SS1.p1.6.m6.1a"><mi id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><ci id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">r</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.6.m6.1d">italic_r</annotation></semantics></math>.
Then the agent retrieves relevant information <math alttext="{\mathbf{k}}_{r}" class="ltx_Math" display="inline" id="S2.SS1.p1.7.m7.1"><semantics id="S2.SS1.p1.7.m7.1a"><msub id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml"><mi id="S2.SS1.p1.7.m7.1.1.2" xref="S2.SS1.p1.7.m7.1.1.2.cmml">𝐤</mi><mi id="S2.SS1.p1.7.m7.1.1.3" xref="S2.SS1.p1.7.m7.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><apply id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.1.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S2.SS1.p1.7.m7.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2">𝐤</ci><ci id="S2.SS1.p1.7.m7.1.1.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">{\mathbf{k}}_{r}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.7.m7.1d">bold_k start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math> from the external knowledge and loads context <math alttext="{\mathbf{y}}_{1:r-1}" class="ltx_Math" display="inline" id="S2.SS1.p1.8.m8.1"><semantics id="S2.SS1.p1.8.m8.1a"><msub id="S2.SS1.p1.8.m8.1.1" xref="S2.SS1.p1.8.m8.1.1.cmml"><mi id="S2.SS1.p1.8.m8.1.1.2" xref="S2.SS1.p1.8.m8.1.1.2.cmml">𝐲</mi><mrow id="S2.SS1.p1.8.m8.1.1.3" xref="S2.SS1.p1.8.m8.1.1.3.cmml"><mn id="S2.SS1.p1.8.m8.1.1.3.2" xref="S2.SS1.p1.8.m8.1.1.3.2.cmml">1</mn><mo id="S2.SS1.p1.8.m8.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S2.SS1.p1.8.m8.1.1.3.1.cmml">:</mo><mrow id="S2.SS1.p1.8.m8.1.1.3.3" xref="S2.SS1.p1.8.m8.1.1.3.3.cmml"><mi id="S2.SS1.p1.8.m8.1.1.3.3.2" xref="S2.SS1.p1.8.m8.1.1.3.3.2.cmml">r</mi><mo id="S2.SS1.p1.8.m8.1.1.3.3.1" xref="S2.SS1.p1.8.m8.1.1.3.3.1.cmml">−</mo><mn id="S2.SS1.p1.8.m8.1.1.3.3.3" xref="S2.SS1.p1.8.m8.1.1.3.3.3.cmml">1</mn></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S2.SS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2">𝐲</ci><apply id="S2.SS1.p1.8.m8.1.1.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3"><ci id="S2.SS1.p1.8.m8.1.1.3.1.cmml" xref="S2.SS1.p1.8.m8.1.1.3.1">:</ci><cn id="S2.SS1.p1.8.m8.1.1.3.2.cmml" type="integer" xref="S2.SS1.p1.8.m8.1.1.3.2">1</cn><apply id="S2.SS1.p1.8.m8.1.1.3.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3"><minus id="S2.SS1.p1.8.m8.1.1.3.3.1.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3.1"></minus><ci id="S2.SS1.p1.8.m8.1.1.3.3.2.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3.2">𝑟</ci><cn id="S2.SS1.p1.8.m8.1.1.3.3.3.cmml" type="integer" xref="S2.SS1.p1.8.m8.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">{\mathbf{y}}_{1:r-1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.8.m8.1d">bold_y start_POSTSUBSCRIPT 1 : italic_r - 1 end_POSTSUBSCRIPT</annotation></semantics></math> from the last round memory.
Once CLASI “think” sufficient context is loaded, it generates the transcription (optional), translation, and cut-off timestamp <math alttext="t^{r}" class="ltx_Math" display="inline" id="S2.SS1.p1.9.m9.1"><semantics id="S2.SS1.p1.9.m9.1a"><msup id="S2.SS1.p1.9.m9.1.1" xref="S2.SS1.p1.9.m9.1.1.cmml"><mi id="S2.SS1.p1.9.m9.1.1.2" xref="S2.SS1.p1.9.m9.1.1.2.cmml">t</mi><mi id="S2.SS1.p1.9.m9.1.1.3" xref="S2.SS1.p1.9.m9.1.1.3.cmml">r</mi></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.9.m9.1b"><apply id="S2.SS1.p1.9.m9.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.9.m9.1.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1">superscript</csymbol><ci id="S2.SS1.p1.9.m9.1.1.2.cmml" xref="S2.SS1.p1.9.m9.1.1.2">𝑡</ci><ci id="S2.SS1.p1.9.m9.1.1.3.cmml" xref="S2.SS1.p1.9.m9.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.9.m9.1c">t^{r}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.9.m9.1d">italic_t start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="{\mathbf{y}}_{r};t^{r}=\text{TextDecoder}({\mathbf{x}}_{t^{r-1}:T^{r}},{%
\mathbf{k}}_{r},{\mathbf{y}}_{1:r-1})\;" class="ltx_Math" display="block" id="S2.E1.m1.5"><semantics id="S2.E1.m1.5a"><mrow id="S2.E1.m1.5.5" xref="S2.E1.m1.5.5.cmml"><mrow id="S2.E1.m1.2.2.2.2" xref="S2.E1.m1.2.2.2.3.cmml"><msub id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.2.cmml">𝐲</mi><mi id="S2.E1.m1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.3.cmml">r</mi></msub><mo id="S2.E1.m1.2.2.2.2.3" xref="S2.E1.m1.2.2.2.3.cmml">;</mo><msup id="S2.E1.m1.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.2.cmml"><mi id="S2.E1.m1.2.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.2.2.cmml">t</mi><mi id="S2.E1.m1.2.2.2.2.2.3" xref="S2.E1.m1.2.2.2.2.2.3.cmml">r</mi></msup></mrow><mo id="S2.E1.m1.5.5.6" xref="S2.E1.m1.5.5.6.cmml">=</mo><mrow id="S2.E1.m1.5.5.5" xref="S2.E1.m1.5.5.5.cmml"><mtext id="S2.E1.m1.5.5.5.5" xref="S2.E1.m1.5.5.5.5a.cmml">TextDecoder</mtext><mo id="S2.E1.m1.5.5.5.4" xref="S2.E1.m1.5.5.5.4.cmml">⁢</mo><mrow id="S2.E1.m1.5.5.5.3.3" xref="S2.E1.m1.5.5.5.3.4.cmml"><mo id="S2.E1.m1.5.5.5.3.3.4" stretchy="false" xref="S2.E1.m1.5.5.5.3.4.cmml">(</mo><msub id="S2.E1.m1.3.3.3.1.1.1" xref="S2.E1.m1.3.3.3.1.1.1.cmml"><mi id="S2.E1.m1.3.3.3.1.1.1.2" xref="S2.E1.m1.3.3.3.1.1.1.2.cmml">𝐱</mi><mrow id="S2.E1.m1.3.3.3.1.1.1.3" xref="S2.E1.m1.3.3.3.1.1.1.3.cmml"><msup id="S2.E1.m1.3.3.3.1.1.1.3.2" xref="S2.E1.m1.3.3.3.1.1.1.3.2.cmml"><mi id="S2.E1.m1.3.3.3.1.1.1.3.2.2" xref="S2.E1.m1.3.3.3.1.1.1.3.2.2.cmml">t</mi><mrow id="S2.E1.m1.3.3.3.1.1.1.3.2.3" xref="S2.E1.m1.3.3.3.1.1.1.3.2.3.cmml"><mi id="S2.E1.m1.3.3.3.1.1.1.3.2.3.2" xref="S2.E1.m1.3.3.3.1.1.1.3.2.3.2.cmml">r</mi><mo id="S2.E1.m1.3.3.3.1.1.1.3.2.3.1" xref="S2.E1.m1.3.3.3.1.1.1.3.2.3.1.cmml">−</mo><mn id="S2.E1.m1.3.3.3.1.1.1.3.2.3.3" xref="S2.E1.m1.3.3.3.1.1.1.3.2.3.3.cmml">1</mn></mrow></msup><mo id="S2.E1.m1.3.3.3.1.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S2.E1.m1.3.3.3.1.1.1.3.1.cmml">:</mo><msup id="S2.E1.m1.3.3.3.1.1.1.3.3" xref="S2.E1.m1.3.3.3.1.1.1.3.3.cmml"><mi id="S2.E1.m1.3.3.3.1.1.1.3.3.2" xref="S2.E1.m1.3.3.3.1.1.1.3.3.2.cmml">T</mi><mi id="S2.E1.m1.3.3.3.1.1.1.3.3.3" xref="S2.E1.m1.3.3.3.1.1.1.3.3.3.cmml">r</mi></msup></mrow></msub><mo id="S2.E1.m1.5.5.5.3.3.5" xref="S2.E1.m1.5.5.5.3.4.cmml">,</mo><msub id="S2.E1.m1.4.4.4.2.2.2" xref="S2.E1.m1.4.4.4.2.2.2.cmml"><mi id="S2.E1.m1.4.4.4.2.2.2.2" xref="S2.E1.m1.4.4.4.2.2.2.2.cmml">𝐤</mi><mi id="S2.E1.m1.4.4.4.2.2.2.3" xref="S2.E1.m1.4.4.4.2.2.2.3.cmml">r</mi></msub><mo id="S2.E1.m1.5.5.5.3.3.6" xref="S2.E1.m1.5.5.5.3.4.cmml">,</mo><msub id="S2.E1.m1.5.5.5.3.3.3" xref="S2.E1.m1.5.5.5.3.3.3.cmml"><mi id="S2.E1.m1.5.5.5.3.3.3.2" xref="S2.E1.m1.5.5.5.3.3.3.2.cmml">𝐲</mi><mrow id="S2.E1.m1.5.5.5.3.3.3.3" xref="S2.E1.m1.5.5.5.3.3.3.3.cmml"><mn id="S2.E1.m1.5.5.5.3.3.3.3.2" xref="S2.E1.m1.5.5.5.3.3.3.3.2.cmml">1</mn><mo id="S2.E1.m1.5.5.5.3.3.3.3.1" lspace="0.278em" rspace="0.278em" xref="S2.E1.m1.5.5.5.3.3.3.3.1.cmml">:</mo><mrow id="S2.E1.m1.5.5.5.3.3.3.3.3" xref="S2.E1.m1.5.5.5.3.3.3.3.3.cmml"><mi id="S2.E1.m1.5.5.5.3.3.3.3.3.2" xref="S2.E1.m1.5.5.5.3.3.3.3.3.2.cmml">r</mi><mo id="S2.E1.m1.5.5.5.3.3.3.3.3.1" xref="S2.E1.m1.5.5.5.3.3.3.3.3.1.cmml">−</mo><mn id="S2.E1.m1.5.5.5.3.3.3.3.3.3" xref="S2.E1.m1.5.5.5.3.3.3.3.3.3.cmml">1</mn></mrow></mrow></msub><mo id="S2.E1.m1.5.5.5.3.3.7" stretchy="false" xref="S2.E1.m1.5.5.5.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.5b"><apply id="S2.E1.m1.5.5.cmml" xref="S2.E1.m1.5.5"><eq id="S2.E1.m1.5.5.6.cmml" xref="S2.E1.m1.5.5.6"></eq><list id="S2.E1.m1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2"><apply id="S2.E1.m1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.2">𝐲</ci><ci id="S2.E1.m1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.3">𝑟</ci></apply><apply id="S2.E1.m1.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.2">superscript</csymbol><ci id="S2.E1.m1.2.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2">𝑡</ci><ci id="S2.E1.m1.2.2.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2.2.3">𝑟</ci></apply></list><apply id="S2.E1.m1.5.5.5.cmml" xref="S2.E1.m1.5.5.5"><times id="S2.E1.m1.5.5.5.4.cmml" xref="S2.E1.m1.5.5.5.4"></times><ci id="S2.E1.m1.5.5.5.5a.cmml" xref="S2.E1.m1.5.5.5.5"><mtext id="S2.E1.m1.5.5.5.5.cmml" xref="S2.E1.m1.5.5.5.5">TextDecoder</mtext></ci><vector id="S2.E1.m1.5.5.5.3.4.cmml" xref="S2.E1.m1.5.5.5.3.3"><apply id="S2.E1.m1.3.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.1.1.1.1.cmml" xref="S2.E1.m1.3.3.3.1.1.1">subscript</csymbol><ci id="S2.E1.m1.3.3.3.1.1.1.2.cmml" xref="S2.E1.m1.3.3.3.1.1.1.2">𝐱</ci><apply id="S2.E1.m1.3.3.3.1.1.1.3.cmml" xref="S2.E1.m1.3.3.3.1.1.1.3"><ci id="S2.E1.m1.3.3.3.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.3.1.1.1.3.1">:</ci><apply id="S2.E1.m1.3.3.3.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.3.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.1.1.1.3.2.1.cmml" xref="S2.E1.m1.3.3.3.1.1.1.3.2">superscript</csymbol><ci id="S2.E1.m1.3.3.3.1.1.1.3.2.2.cmml" xref="S2.E1.m1.3.3.3.1.1.1.3.2.2">𝑡</ci><apply id="S2.E1.m1.3.3.3.1.1.1.3.2.3.cmml" xref="S2.E1.m1.3.3.3.1.1.1.3.2.3"><minus id="S2.E1.m1.3.3.3.1.1.1.3.2.3.1.cmml" xref="S2.E1.m1.3.3.3.1.1.1.3.2.3.1"></minus><ci id="S2.E1.m1.3.3.3.1.1.1.3.2.3.2.cmml" xref="S2.E1.m1.3.3.3.1.1.1.3.2.3.2">𝑟</ci><cn id="S2.E1.m1.3.3.3.1.1.1.3.2.3.3.cmml" type="integer" xref="S2.E1.m1.3.3.3.1.1.1.3.2.3.3">1</cn></apply></apply><apply id="S2.E1.m1.3.3.3.1.1.1.3.3.cmml" xref="S2.E1.m1.3.3.3.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.1.1.1.3.3.1.cmml" xref="S2.E1.m1.3.3.3.1.1.1.3.3">superscript</csymbol><ci id="S2.E1.m1.3.3.3.1.1.1.3.3.2.cmml" xref="S2.E1.m1.3.3.3.1.1.1.3.3.2">𝑇</ci><ci id="S2.E1.m1.3.3.3.1.1.1.3.3.3.cmml" xref="S2.E1.m1.3.3.3.1.1.1.3.3.3">𝑟</ci></apply></apply></apply><apply id="S2.E1.m1.4.4.4.2.2.2.cmml" xref="S2.E1.m1.4.4.4.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.4.2.2.2.1.cmml" xref="S2.E1.m1.4.4.4.2.2.2">subscript</csymbol><ci id="S2.E1.m1.4.4.4.2.2.2.2.cmml" xref="S2.E1.m1.4.4.4.2.2.2.2">𝐤</ci><ci id="S2.E1.m1.4.4.4.2.2.2.3.cmml" xref="S2.E1.m1.4.4.4.2.2.2.3">𝑟</ci></apply><apply id="S2.E1.m1.5.5.5.3.3.3.cmml" xref="S2.E1.m1.5.5.5.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.5.3.3.3.1.cmml" xref="S2.E1.m1.5.5.5.3.3.3">subscript</csymbol><ci id="S2.E1.m1.5.5.5.3.3.3.2.cmml" xref="S2.E1.m1.5.5.5.3.3.3.2">𝐲</ci><apply id="S2.E1.m1.5.5.5.3.3.3.3.cmml" xref="S2.E1.m1.5.5.5.3.3.3.3"><ci id="S2.E1.m1.5.5.5.3.3.3.3.1.cmml" xref="S2.E1.m1.5.5.5.3.3.3.3.1">:</ci><cn id="S2.E1.m1.5.5.5.3.3.3.3.2.cmml" type="integer" xref="S2.E1.m1.5.5.5.3.3.3.3.2">1</cn><apply id="S2.E1.m1.5.5.5.3.3.3.3.3.cmml" xref="S2.E1.m1.5.5.5.3.3.3.3.3"><minus id="S2.E1.m1.5.5.5.3.3.3.3.3.1.cmml" xref="S2.E1.m1.5.5.5.3.3.3.3.3.1"></minus><ci id="S2.E1.m1.5.5.5.3.3.3.3.3.2.cmml" xref="S2.E1.m1.5.5.5.3.3.3.3.3.2">𝑟</ci><cn id="S2.E1.m1.5.5.5.3.3.3.3.3.3.cmml" type="integer" xref="S2.E1.m1.5.5.5.3.3.3.3.3.3">1</cn></apply></apply></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.5c">{\mathbf{y}}_{r};t^{r}=\text{TextDecoder}({\mathbf{x}}_{t^{r-1}:T^{r}},{%
\mathbf{k}}_{r},{\mathbf{y}}_{1:r-1})\;</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.5d">bold_y start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ; italic_t start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT = TextDecoder ( bold_x start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT : italic_T start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT end_POSTSUBSCRIPT , bold_k start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , bold_y start_POSTSUBSCRIPT 1 : italic_r - 1 end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS1.p1.14">where <math alttext="t^{r}" class="ltx_Math" display="inline" id="S2.SS1.p1.10.m1.1"><semantics id="S2.SS1.p1.10.m1.1a"><msup id="S2.SS1.p1.10.m1.1.1" xref="S2.SS1.p1.10.m1.1.1.cmml"><mi id="S2.SS1.p1.10.m1.1.1.2" xref="S2.SS1.p1.10.m1.1.1.2.cmml">t</mi><mi id="S2.SS1.p1.10.m1.1.1.3" xref="S2.SS1.p1.10.m1.1.1.3.cmml">r</mi></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.10.m1.1b"><apply id="S2.SS1.p1.10.m1.1.1.cmml" xref="S2.SS1.p1.10.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m1.1.1.1.cmml" xref="S2.SS1.p1.10.m1.1.1">superscript</csymbol><ci id="S2.SS1.p1.10.m1.1.1.2.cmml" xref="S2.SS1.p1.10.m1.1.1.2">𝑡</ci><ci id="S2.SS1.p1.10.m1.1.1.3.cmml" xref="S2.SS1.p1.10.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.10.m1.1c">t^{r}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.10.m1.1d">italic_t start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT</annotation></semantics></math> is the predicted cut-off timestamp indicating the end time for the current translation round <math alttext="r" class="ltx_Math" display="inline" id="S2.SS1.p1.11.m2.1"><semantics id="S2.SS1.p1.11.m2.1a"><mi id="S2.SS1.p1.11.m2.1.1" xref="S2.SS1.p1.11.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.11.m2.1b"><ci id="S2.SS1.p1.11.m2.1.1.cmml" xref="S2.SS1.p1.11.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.11.m2.1c">r</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.11.m2.1d">italic_r</annotation></semantics></math>.
<math alttext="{\mathbf{y}}_{r}" class="ltx_Math" display="inline" id="S2.SS1.p1.12.m3.1"><semantics id="S2.SS1.p1.12.m3.1a"><msub id="S2.SS1.p1.12.m3.1.1" xref="S2.SS1.p1.12.m3.1.1.cmml"><mi id="S2.SS1.p1.12.m3.1.1.2" xref="S2.SS1.p1.12.m3.1.1.2.cmml">𝐲</mi><mi id="S2.SS1.p1.12.m3.1.1.3" xref="S2.SS1.p1.12.m3.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.12.m3.1b"><apply id="S2.SS1.p1.12.m3.1.1.cmml" xref="S2.SS1.p1.12.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.12.m3.1.1.1.cmml" xref="S2.SS1.p1.12.m3.1.1">subscript</csymbol><ci id="S2.SS1.p1.12.m3.1.1.2.cmml" xref="S2.SS1.p1.12.m3.1.1.2">𝐲</ci><ci id="S2.SS1.p1.12.m3.1.1.3.cmml" xref="S2.SS1.p1.12.m3.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.12.m3.1c">{\mathbf{y}}_{r}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.12.m3.1d">bold_y start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math> is then forwarded to update the memory.
When instructed to output the transcription, the LLM optionally engages CoT to generate transcription first and then the speech translation.
For the following round <math alttext="r+1" class="ltx_Math" display="inline" id="S2.SS1.p1.13.m4.1"><semantics id="S2.SS1.p1.13.m4.1a"><mrow id="S2.SS1.p1.13.m4.1.1" xref="S2.SS1.p1.13.m4.1.1.cmml"><mi id="S2.SS1.p1.13.m4.1.1.2" xref="S2.SS1.p1.13.m4.1.1.2.cmml">r</mi><mo id="S2.SS1.p1.13.m4.1.1.1" xref="S2.SS1.p1.13.m4.1.1.1.cmml">+</mo><mn id="S2.SS1.p1.13.m4.1.1.3" xref="S2.SS1.p1.13.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.13.m4.1b"><apply id="S2.SS1.p1.13.m4.1.1.cmml" xref="S2.SS1.p1.13.m4.1.1"><plus id="S2.SS1.p1.13.m4.1.1.1.cmml" xref="S2.SS1.p1.13.m4.1.1.1"></plus><ci id="S2.SS1.p1.13.m4.1.1.2.cmml" xref="S2.SS1.p1.13.m4.1.1.2">𝑟</ci><cn id="S2.SS1.p1.13.m4.1.1.3.cmml" type="integer" xref="S2.SS1.p1.13.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.13.m4.1c">r+1</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.13.m4.1d">italic_r + 1</annotation></semantics></math>, the audio stream begins with the predicted cut-off timestamp <math alttext="t^{r}" class="ltx_Math" display="inline" id="S2.SS1.p1.14.m5.1"><semantics id="S2.SS1.p1.14.m5.1a"><msup id="S2.SS1.p1.14.m5.1.1" xref="S2.SS1.p1.14.m5.1.1.cmml"><mi id="S2.SS1.p1.14.m5.1.1.2" xref="S2.SS1.p1.14.m5.1.1.2.cmml">t</mi><mi id="S2.SS1.p1.14.m5.1.1.3" xref="S2.SS1.p1.14.m5.1.1.3.cmml">r</mi></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.14.m5.1b"><apply id="S2.SS1.p1.14.m5.1.1.cmml" xref="S2.SS1.p1.14.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.14.m5.1.1.1.cmml" xref="S2.SS1.p1.14.m5.1.1">superscript</csymbol><ci id="S2.SS1.p1.14.m5.1.1.2.cmml" xref="S2.SS1.p1.14.m5.1.1.2">𝑡</ci><ci id="S2.SS1.p1.14.m5.1.1.3.cmml" xref="S2.SS1.p1.14.m5.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.14.m5.1c">t^{r}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.14.m5.1d">italic_t start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Architecture</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.8">CLASI employs an Encoder-Conditioned LLM architecture. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S2.F3" title="In 2 Methods ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>, the audio encoder transforms input speech stream <math alttext="{\mathbf{x}}" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">𝐱</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝐱</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">{\mathbf{x}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">bold_x</annotation></semantics></math> to a series of continuous representations <math alttext="{\mathbf{s}}" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1"><semantics id="S2.SS2.p1.2.m2.1a"><mi id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">𝐬</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">𝐬</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">{\mathbf{s}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.2.m2.1d">bold_s</annotation></semantics></math>.
Then, the LLM takes the speech representation <math alttext="{\mathbf{s}}" class="ltx_Math" display="inline" id="S2.SS2.p1.3.m3.1"><semantics id="S2.SS2.p1.3.m3.1a"><mi id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">𝐬</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><ci id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">𝐬</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">{\mathbf{s}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.3.m3.1d">bold_s</annotation></semantics></math>, retrieved knowledge <math alttext="{\mathbf{k}}" class="ltx_Math" display="inline" id="S2.SS2.p1.4.m4.1"><semantics id="S2.SS2.p1.4.m4.1a"><mi id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml">𝐤</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><ci id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">𝐤</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">{\mathbf{k}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.4.m4.1d">bold_k</annotation></semantics></math>, historical translation <math alttext="{\mathbf{y}}" class="ltx_Math" display="inline" id="S2.SS2.p1.5.m5.1"><semantics id="S2.SS2.p1.5.m5.1a"><mi id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml">𝐲</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><ci id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1">𝐲</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">{\mathbf{y}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.5.m5.1d">bold_y</annotation></semantics></math> and instruction <math alttext="{\mathbf{I}}" class="ltx_Math" display="inline" id="S2.SS2.p1.6.m6.1"><semantics id="S2.SS2.p1.6.m6.1a"><mi id="S2.SS2.p1.6.m6.1.1" xref="S2.SS2.p1.6.m6.1.1.cmml">𝐈</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m6.1b"><ci id="S2.SS2.p1.6.m6.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1">𝐈</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m6.1c">{\mathbf{I}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.6.m6.1d">bold_I</annotation></semantics></math> as a sequence of prompt <math alttext="({\mathbf{y}},{\mathbf{s}},{\mathbf{k}},{\mathbf{I}})" class="ltx_Math" display="inline" id="S2.SS2.p1.7.m7.4"><semantics id="S2.SS2.p1.7.m7.4a"><mrow id="S2.SS2.p1.7.m7.4.5.2" xref="S2.SS2.p1.7.m7.4.5.1.cmml"><mo id="S2.SS2.p1.7.m7.4.5.2.1" stretchy="false" xref="S2.SS2.p1.7.m7.4.5.1.cmml">(</mo><mi id="S2.SS2.p1.7.m7.1.1" xref="S2.SS2.p1.7.m7.1.1.cmml">𝐲</mi><mo id="S2.SS2.p1.7.m7.4.5.2.2" xref="S2.SS2.p1.7.m7.4.5.1.cmml">,</mo><mi id="S2.SS2.p1.7.m7.2.2" xref="S2.SS2.p1.7.m7.2.2.cmml">𝐬</mi><mo id="S2.SS2.p1.7.m7.4.5.2.3" xref="S2.SS2.p1.7.m7.4.5.1.cmml">,</mo><mi id="S2.SS2.p1.7.m7.3.3" xref="S2.SS2.p1.7.m7.3.3.cmml">𝐤</mi><mo id="S2.SS2.p1.7.m7.4.5.2.4" xref="S2.SS2.p1.7.m7.4.5.1.cmml">,</mo><mi id="S2.SS2.p1.7.m7.4.4" xref="S2.SS2.p1.7.m7.4.4.cmml">𝐈</mi><mo id="S2.SS2.p1.7.m7.4.5.2.5" stretchy="false" xref="S2.SS2.p1.7.m7.4.5.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m7.4b"><vector id="S2.SS2.p1.7.m7.4.5.1.cmml" xref="S2.SS2.p1.7.m7.4.5.2"><ci id="S2.SS2.p1.7.m7.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1">𝐲</ci><ci id="S2.SS2.p1.7.m7.2.2.cmml" xref="S2.SS2.p1.7.m7.2.2">𝐬</ci><ci id="S2.SS2.p1.7.m7.3.3.cmml" xref="S2.SS2.p1.7.m7.3.3">𝐤</ci><ci id="S2.SS2.p1.7.m7.4.4.cmml" xref="S2.SS2.p1.7.m7.4.4">𝐈</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m7.4c">({\mathbf{y}},{\mathbf{s}},{\mathbf{k}},{\mathbf{I}})</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.7.m7.4d">( bold_y , bold_s , bold_k , bold_I )</annotation></semantics></math> to generate the translation result <math alttext="{\mathbf{y}}" class="ltx_Math" display="inline" id="S2.SS2.p1.8.m8.1"><semantics id="S2.SS2.p1.8.m8.1a"><mi id="S2.SS2.p1.8.m8.1.1" xref="S2.SS2.p1.8.m8.1.1.cmml">𝐲</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.8.m8.1b"><ci id="S2.SS2.p1.8.m8.1.1.cmml" xref="S2.SS2.p1.8.m8.1.1">𝐲</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.8.m8.1c">{\mathbf{y}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.8.m8.1d">bold_y</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.1">Audio Encoder.</span> The audio encoder module contains a large-scale speech conformer <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib25" title="">gulati2020conformer, </a>)</cite> pretrained on millions of hours of speech data to achieve human parity performance on ASR, and an audio adapter to connect the audio encoder and LLM.
The adapter downsamples the speech representations and the resulting representations are linearly projected to match the dimension of the LLM embedding layer.
The projected speech representations lower the computational latency for SiST.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p3.1.1">Large Language Model.</span> The language model<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We use Doubao LLM as our foundation model.</span></span></span> is a medium size decoder-only transformer <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib73" title="">vaswani2017attention </a></cite> to balance performance and computation efficiency. It is pretrained on a large amount of text data and fine-tuned with instructions. The LLM directly takes the continuous embedding from both the audio encoder and text embedder as input. It autoregressively generates the transcription and translation response of the provided speech stream.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p4.1.1">Multi-Modal Retriever.</span>
The multi-modal retriever framework employs audio and text encoders to independently encode the audio stream and text key of the terminologies in the external knowledge database.
To enhance the alignment between audio embeddings and text embeddings, we incorporate an embedding fusion layer, which includes a multi-head attention module followed by a pooling layer.
The resulting pooled representation is subsequently fed into a linear projection layer to produce the final scores, indicating the probability of the text key’s presence in the audio stream.
Terminologies with top scores are forwarded to the CLASI agent to enhance the translation quality.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Data Driven Read-Write Policy: <span class="ltx_text ltx_font_typewriter" id="S2.SS3.1.1">&lt;INPUT&gt;</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SS3.2.2">&lt;OUTPUT&gt;</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Unlike predetermined read-write probabilities and heuristic waiting policies detailed in prior research <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib7" title="">barrault2023seamless </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib45" title="">ma-etal-2019-stacl </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib39" title="">li-etal-2023-hw-tsc </a></cite>, interpreters engage in a dynamic process of listening (read) and translating (write).
They attentively listen to the speaker’s speech and segment lengthy sentences into semantic chunks, representing the smallest linguistic units capable of conveying a complete thought independently <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib33" title="">jones2014conference </a></cite>.
Upon identifying a chunk that encapsulates sufficient information, they proceed to translate this segment into the target language, thereby providing an accurate and contextually appropriate translation.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.8">Emulating the strategies of human interpreters, CLASI does not require to explicitly define the read-write policy. CLASI imitates their policies by waiting for complete semantic chunks. Specifically, given partial speech, CLASI only generates the translation for the complete chunks of the input speech. The model is trained with segmented speech data to learn such ability.
Mathematically, given source audio <math alttext="{\mathbf{x}}_{1:M}" class="ltx_Math" display="inline" id="S2.SS3.p2.1.m1.1"><semantics id="S2.SS3.p2.1.m1.1a"><msub id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml"><mi id="S2.SS3.p2.1.m1.1.1.2" xref="S2.SS3.p2.1.m1.1.1.2.cmml">𝐱</mi><mrow id="S2.SS3.p2.1.m1.1.1.3" xref="S2.SS3.p2.1.m1.1.1.3.cmml"><mn id="S2.SS3.p2.1.m1.1.1.3.2" xref="S2.SS3.p2.1.m1.1.1.3.2.cmml">1</mn><mo id="S2.SS3.p2.1.m1.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S2.SS3.p2.1.m1.1.1.3.1.cmml">:</mo><mi id="S2.SS3.p2.1.m1.1.1.3.3" xref="S2.SS3.p2.1.m1.1.1.3.3.cmml">M</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.1b"><apply id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p2.1.m1.1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.p2.1.m1.1.1.2.cmml" xref="S2.SS3.p2.1.m1.1.1.2">𝐱</ci><apply id="S2.SS3.p2.1.m1.1.1.3.cmml" xref="S2.SS3.p2.1.m1.1.1.3"><ci id="S2.SS3.p2.1.m1.1.1.3.1.cmml" xref="S2.SS3.p2.1.m1.1.1.3.1">:</ci><cn id="S2.SS3.p2.1.m1.1.1.3.2.cmml" type="integer" xref="S2.SS3.p2.1.m1.1.1.3.2">1</cn><ci id="S2.SS3.p2.1.m1.1.1.3.3.cmml" xref="S2.SS3.p2.1.m1.1.1.3.3">𝑀</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.1c">{\mathbf{x}}_{1:M}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.1.m1.1d">bold_x start_POSTSUBSCRIPT 1 : italic_M end_POSTSUBSCRIPT</annotation></semantics></math>, we segment its translation into a series of <math alttext="n" class="ltx_Math" display="inline" id="S2.SS3.p2.2.m2.1"><semantics id="S2.SS3.p2.2.m2.1a"><mi id="S2.SS3.p2.2.m2.1.1" xref="S2.SS3.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.2.m2.1b"><ci id="S2.SS3.p2.2.m2.1.1.cmml" xref="S2.SS3.p2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.2.m2.1d">italic_n</annotation></semantics></math> “chunks” <math alttext="{\mathbf{y}}_{1:n}" class="ltx_Math" display="inline" id="S2.SS3.p2.3.m3.1"><semantics id="S2.SS3.p2.3.m3.1a"><msub id="S2.SS3.p2.3.m3.1.1" xref="S2.SS3.p2.3.m3.1.1.cmml"><mi id="S2.SS3.p2.3.m3.1.1.2" xref="S2.SS3.p2.3.m3.1.1.2.cmml">𝐲</mi><mrow id="S2.SS3.p2.3.m3.1.1.3" xref="S2.SS3.p2.3.m3.1.1.3.cmml"><mn id="S2.SS3.p2.3.m3.1.1.3.2" xref="S2.SS3.p2.3.m3.1.1.3.2.cmml">1</mn><mo id="S2.SS3.p2.3.m3.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S2.SS3.p2.3.m3.1.1.3.1.cmml">:</mo><mi id="S2.SS3.p2.3.m3.1.1.3.3" xref="S2.SS3.p2.3.m3.1.1.3.3.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.3.m3.1b"><apply id="S2.SS3.p2.3.m3.1.1.cmml" xref="S2.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS3.p2.3.m3.1.1.1.cmml" xref="S2.SS3.p2.3.m3.1.1">subscript</csymbol><ci id="S2.SS3.p2.3.m3.1.1.2.cmml" xref="S2.SS3.p2.3.m3.1.1.2">𝐲</ci><apply id="S2.SS3.p2.3.m3.1.1.3.cmml" xref="S2.SS3.p2.3.m3.1.1.3"><ci id="S2.SS3.p2.3.m3.1.1.3.1.cmml" xref="S2.SS3.p2.3.m3.1.1.3.1">:</ci><cn id="S2.SS3.p2.3.m3.1.1.3.2.cmml" type="integer" xref="S2.SS3.p2.3.m3.1.1.3.2">1</cn><ci id="S2.SS3.p2.3.m3.1.1.3.3.cmml" xref="S2.SS3.p2.3.m3.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.3.m3.1c">{\mathbf{y}}_{1:n}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.3.m3.1d">bold_y start_POSTSUBSCRIPT 1 : italic_n end_POSTSUBSCRIPT</annotation></semantics></math> and obtain the corresponding pair <math alttext="\{({\mathbf{x}}_{t^{j}:t^{j+1}},{\mathbf{y}}_{j}\}_{j=1}^{n}" class="ltx_math_unparsed" display="inline" id="S2.SS3.p2.4.m4.1"><semantics id="S2.SS3.p2.4.m4.1a"><mrow id="S2.SS3.p2.4.m4.1b"><mo id="S2.SS3.p2.4.m4.1.1" stretchy="false">{</mo><msubsup id="S2.SS3.p2.4.m4.1.2"><mrow id="S2.SS3.p2.4.m4.1.2.2.2"><mo id="S2.SS3.p2.4.m4.1.2.2.2.1" stretchy="false">(</mo><msub id="S2.SS3.p2.4.m4.1.2.2.2.2"><mi id="S2.SS3.p2.4.m4.1.2.2.2.2.2">𝐱</mi><mrow id="S2.SS3.p2.4.m4.1.2.2.2.2.3"><msup id="S2.SS3.p2.4.m4.1.2.2.2.2.3.2"><mi id="S2.SS3.p2.4.m4.1.2.2.2.2.3.2.2">t</mi><mi id="S2.SS3.p2.4.m4.1.2.2.2.2.3.2.3">j</mi></msup><mo id="S2.SS3.p2.4.m4.1.2.2.2.2.3.1" lspace="0.278em" rspace="0.278em">:</mo><msup id="S2.SS3.p2.4.m4.1.2.2.2.2.3.3"><mi id="S2.SS3.p2.4.m4.1.2.2.2.2.3.3.2">t</mi><mrow id="S2.SS3.p2.4.m4.1.2.2.2.2.3.3.3"><mi id="S2.SS3.p2.4.m4.1.2.2.2.2.3.3.3.2">j</mi><mo id="S2.SS3.p2.4.m4.1.2.2.2.2.3.3.3.1">+</mo><mn id="S2.SS3.p2.4.m4.1.2.2.2.2.3.3.3.3">1</mn></mrow></msup></mrow></msub><mo id="S2.SS3.p2.4.m4.1.2.2.2.3">,</mo><msub id="S2.SS3.p2.4.m4.1.2.2.2.4"><mi id="S2.SS3.p2.4.m4.1.2.2.2.4.2">𝐲</mi><mi id="S2.SS3.p2.4.m4.1.2.2.2.4.3">j</mi></msub><mo id="S2.SS3.p2.4.m4.1.2.2.2.5" stretchy="false">}</mo></mrow><mrow id="S2.SS3.p2.4.m4.1.2.2.3"><mi id="S2.SS3.p2.4.m4.1.2.2.3.2">j</mi><mo id="S2.SS3.p2.4.m4.1.2.2.3.1">=</mo><mn id="S2.SS3.p2.4.m4.1.2.2.3.3">1</mn></mrow><mi id="S2.SS3.p2.4.m4.1.2.3">n</mi></msubsup></mrow><annotation encoding="application/x-tex" id="S2.SS3.p2.4.m4.1c">\{({\mathbf{x}}_{t^{j}:t^{j+1}},{\mathbf{y}}_{j}\}_{j=1}^{n}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.4.m4.1d">{ ( bold_x start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT : italic_t start_POSTSUPERSCRIPT italic_j + 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT , bold_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="{\mathbf{x}}_{t^{j}:t^{j+1}}" class="ltx_Math" display="inline" id="S2.SS3.p2.5.m5.1"><semantics id="S2.SS3.p2.5.m5.1a"><msub id="S2.SS3.p2.5.m5.1.1" xref="S2.SS3.p2.5.m5.1.1.cmml"><mi id="S2.SS3.p2.5.m5.1.1.2" xref="S2.SS3.p2.5.m5.1.1.2.cmml">𝐱</mi><mrow id="S2.SS3.p2.5.m5.1.1.3" xref="S2.SS3.p2.5.m5.1.1.3.cmml"><msup id="S2.SS3.p2.5.m5.1.1.3.2" xref="S2.SS3.p2.5.m5.1.1.3.2.cmml"><mi id="S2.SS3.p2.5.m5.1.1.3.2.2" xref="S2.SS3.p2.5.m5.1.1.3.2.2.cmml">t</mi><mi id="S2.SS3.p2.5.m5.1.1.3.2.3" xref="S2.SS3.p2.5.m5.1.1.3.2.3.cmml">j</mi></msup><mo id="S2.SS3.p2.5.m5.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S2.SS3.p2.5.m5.1.1.3.1.cmml">:</mo><msup id="S2.SS3.p2.5.m5.1.1.3.3" xref="S2.SS3.p2.5.m5.1.1.3.3.cmml"><mi id="S2.SS3.p2.5.m5.1.1.3.3.2" xref="S2.SS3.p2.5.m5.1.1.3.3.2.cmml">t</mi><mrow id="S2.SS3.p2.5.m5.1.1.3.3.3" xref="S2.SS3.p2.5.m5.1.1.3.3.3.cmml"><mi id="S2.SS3.p2.5.m5.1.1.3.3.3.2" xref="S2.SS3.p2.5.m5.1.1.3.3.3.2.cmml">j</mi><mo id="S2.SS3.p2.5.m5.1.1.3.3.3.1" xref="S2.SS3.p2.5.m5.1.1.3.3.3.1.cmml">+</mo><mn id="S2.SS3.p2.5.m5.1.1.3.3.3.3" xref="S2.SS3.p2.5.m5.1.1.3.3.3.3.cmml">1</mn></mrow></msup></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.5.m5.1b"><apply id="S2.SS3.p2.5.m5.1.1.cmml" xref="S2.SS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS3.p2.5.m5.1.1.1.cmml" xref="S2.SS3.p2.5.m5.1.1">subscript</csymbol><ci id="S2.SS3.p2.5.m5.1.1.2.cmml" xref="S2.SS3.p2.5.m5.1.1.2">𝐱</ci><apply id="S2.SS3.p2.5.m5.1.1.3.cmml" xref="S2.SS3.p2.5.m5.1.1.3"><ci id="S2.SS3.p2.5.m5.1.1.3.1.cmml" xref="S2.SS3.p2.5.m5.1.1.3.1">:</ci><apply id="S2.SS3.p2.5.m5.1.1.3.2.cmml" xref="S2.SS3.p2.5.m5.1.1.3.2"><csymbol cd="ambiguous" id="S2.SS3.p2.5.m5.1.1.3.2.1.cmml" xref="S2.SS3.p2.5.m5.1.1.3.2">superscript</csymbol><ci id="S2.SS3.p2.5.m5.1.1.3.2.2.cmml" xref="S2.SS3.p2.5.m5.1.1.3.2.2">𝑡</ci><ci id="S2.SS3.p2.5.m5.1.1.3.2.3.cmml" xref="S2.SS3.p2.5.m5.1.1.3.2.3">𝑗</ci></apply><apply id="S2.SS3.p2.5.m5.1.1.3.3.cmml" xref="S2.SS3.p2.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS3.p2.5.m5.1.1.3.3.1.cmml" xref="S2.SS3.p2.5.m5.1.1.3.3">superscript</csymbol><ci id="S2.SS3.p2.5.m5.1.1.3.3.2.cmml" xref="S2.SS3.p2.5.m5.1.1.3.3.2">𝑡</ci><apply id="S2.SS3.p2.5.m5.1.1.3.3.3.cmml" xref="S2.SS3.p2.5.m5.1.1.3.3.3"><plus id="S2.SS3.p2.5.m5.1.1.3.3.3.1.cmml" xref="S2.SS3.p2.5.m5.1.1.3.3.3.1"></plus><ci id="S2.SS3.p2.5.m5.1.1.3.3.3.2.cmml" xref="S2.SS3.p2.5.m5.1.1.3.3.3.2">𝑗</ci><cn id="S2.SS3.p2.5.m5.1.1.3.3.3.3.cmml" type="integer" xref="S2.SS3.p2.5.m5.1.1.3.3.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.5.m5.1c">{\mathbf{x}}_{t^{j}:t^{j+1}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.5.m5.1d">bold_x start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT : italic_t start_POSTSUPERSCRIPT italic_j + 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="{\mathbf{y}}_{j}" class="ltx_Math" display="inline" id="S2.SS3.p2.6.m6.1"><semantics id="S2.SS3.p2.6.m6.1a"><msub id="S2.SS3.p2.6.m6.1.1" xref="S2.SS3.p2.6.m6.1.1.cmml"><mi id="S2.SS3.p2.6.m6.1.1.2" xref="S2.SS3.p2.6.m6.1.1.2.cmml">𝐲</mi><mi id="S2.SS3.p2.6.m6.1.1.3" xref="S2.SS3.p2.6.m6.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.6.m6.1b"><apply id="S2.SS3.p2.6.m6.1.1.cmml" xref="S2.SS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS3.p2.6.m6.1.1.1.cmml" xref="S2.SS3.p2.6.m6.1.1">subscript</csymbol><ci id="S2.SS3.p2.6.m6.1.1.2.cmml" xref="S2.SS3.p2.6.m6.1.1.2">𝐲</ci><ci id="S2.SS3.p2.6.m6.1.1.3.cmml" xref="S2.SS3.p2.6.m6.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.6.m6.1c">{\mathbf{y}}_{j}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.6.m6.1d">bold_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> denote the <math alttext="j" class="ltx_Math" display="inline" id="S2.SS3.p2.7.m7.1"><semantics id="S2.SS3.p2.7.m7.1a"><mi id="S2.SS3.p2.7.m7.1.1" xref="S2.SS3.p2.7.m7.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.7.m7.1b"><ci id="S2.SS3.p2.7.m7.1.1.cmml" xref="S2.SS3.p2.7.m7.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.7.m7.1c">j</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.7.m7.1d">italic_j</annotation></semantics></math>-th segment of the audio and the corresponding translation. For training, our objective is to output all complete segmented translations and the cut-off time given random partial input audio <math alttext="{\mathbf{x}}_{1:t}" class="ltx_Math" display="inline" id="S2.SS3.p2.8.m8.1"><semantics id="S2.SS3.p2.8.m8.1a"><msub id="S2.SS3.p2.8.m8.1.1" xref="S2.SS3.p2.8.m8.1.1.cmml"><mi id="S2.SS3.p2.8.m8.1.1.2" xref="S2.SS3.p2.8.m8.1.1.2.cmml">𝐱</mi><mrow id="S2.SS3.p2.8.m8.1.1.3" xref="S2.SS3.p2.8.m8.1.1.3.cmml"><mn id="S2.SS3.p2.8.m8.1.1.3.2" xref="S2.SS3.p2.8.m8.1.1.3.2.cmml">1</mn><mo id="S2.SS3.p2.8.m8.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S2.SS3.p2.8.m8.1.1.3.1.cmml">:</mo><mi id="S2.SS3.p2.8.m8.1.1.3.3" xref="S2.SS3.p2.8.m8.1.1.3.3.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.8.m8.1b"><apply id="S2.SS3.p2.8.m8.1.1.cmml" xref="S2.SS3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS3.p2.8.m8.1.1.1.cmml" xref="S2.SS3.p2.8.m8.1.1">subscript</csymbol><ci id="S2.SS3.p2.8.m8.1.1.2.cmml" xref="S2.SS3.p2.8.m8.1.1.2">𝐱</ci><apply id="S2.SS3.p2.8.m8.1.1.3.cmml" xref="S2.SS3.p2.8.m8.1.1.3"><ci id="S2.SS3.p2.8.m8.1.1.3.1.cmml" xref="S2.SS3.p2.8.m8.1.1.3.1">:</ci><cn id="S2.SS3.p2.8.m8.1.1.3.2.cmml" type="integer" xref="S2.SS3.p2.8.m8.1.1.3.2">1</cn><ci id="S2.SS3.p2.8.m8.1.1.3.3.cmml" xref="S2.SS3.p2.8.m8.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.8.m8.1c">{\mathbf{x}}_{1:t}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.8.m8.1d">bold_x start_POSTSUBSCRIPT 1 : italic_t end_POSTSUBSCRIPT</annotation></semantics></math></p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\min\mathbb{E}_{t\sim\mathcal{U}[1,M]}-\log p_{\theta}({\mathbf{y}}_{1:j};t^{j%
}|{\mathbf{x}}_{1:t})\quad j=\max_{j}\{j|t^{j}&lt;t\}\;" class="ltx_Math" display="block" id="S2.E2.m1.6"><semantics id="S2.E2.m1.6a"><mrow id="S2.E2.m1.6.6" xref="S2.E2.m1.6.6.cmml"><mrow id="S2.E2.m1.4.4.1.1" xref="S2.E2.m1.4.4.1.2.cmml"><mrow id="S2.E2.m1.4.4.1.1.1" xref="S2.E2.m1.4.4.1.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1.1.4" xref="S2.E2.m1.4.4.1.1.1.4.cmml"><mi id="S2.E2.m1.4.4.1.1.1.4.1" xref="S2.E2.m1.4.4.1.1.1.4.1.cmml">min</mi><mo id="S2.E2.m1.4.4.1.1.1.4a" lspace="0.167em" xref="S2.E2.m1.4.4.1.1.1.4.cmml">⁡</mo><msub id="S2.E2.m1.4.4.1.1.1.4.2" xref="S2.E2.m1.4.4.1.1.1.4.2.cmml"><mi id="S2.E2.m1.4.4.1.1.1.4.2.2" xref="S2.E2.m1.4.4.1.1.1.4.2.2.cmml">𝔼</mi><mrow id="S2.E2.m1.2.2.2" xref="S2.E2.m1.2.2.2.cmml"><mi id="S2.E2.m1.2.2.2.4" xref="S2.E2.m1.2.2.2.4.cmml">t</mi><mo id="S2.E2.m1.2.2.2.3" xref="S2.E2.m1.2.2.2.3.cmml">∼</mo><mrow id="S2.E2.m1.2.2.2.5" xref="S2.E2.m1.2.2.2.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.2.2.2.5.2" xref="S2.E2.m1.2.2.2.5.2.cmml">𝒰</mi><mo id="S2.E2.m1.2.2.2.5.1" xref="S2.E2.m1.2.2.2.5.1.cmml">⁢</mo><mrow id="S2.E2.m1.2.2.2.5.3.2" xref="S2.E2.m1.2.2.2.5.3.1.cmml"><mo id="S2.E2.m1.2.2.2.5.3.2.1" stretchy="false" xref="S2.E2.m1.2.2.2.5.3.1.cmml">[</mo><mn id="S2.E2.m1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml">1</mn><mo id="S2.E2.m1.2.2.2.5.3.2.2" xref="S2.E2.m1.2.2.2.5.3.1.cmml">,</mo><mi id="S2.E2.m1.2.2.2.2" xref="S2.E2.m1.2.2.2.2.cmml">M</mi><mo id="S2.E2.m1.2.2.2.5.3.2.3" stretchy="false" xref="S2.E2.m1.2.2.2.5.3.1.cmml">]</mo></mrow></mrow></mrow></msub></mrow><mo id="S2.E2.m1.4.4.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.3.cmml">−</mo><mrow id="S2.E2.m1.4.4.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.2.cmml"><mrow id="S2.E2.m1.4.4.1.1.1.2.4" xref="S2.E2.m1.4.4.1.1.1.2.4.cmml"><mi id="S2.E2.m1.4.4.1.1.1.2.4.1" xref="S2.E2.m1.4.4.1.1.1.2.4.1.cmml">log</mi><mo id="S2.E2.m1.4.4.1.1.1.2.4a" lspace="0.167em" xref="S2.E2.m1.4.4.1.1.1.2.4.cmml">⁡</mo><msub id="S2.E2.m1.4.4.1.1.1.2.4.2" xref="S2.E2.m1.4.4.1.1.1.2.4.2.cmml"><mi id="S2.E2.m1.4.4.1.1.1.2.4.2.2" xref="S2.E2.m1.4.4.1.1.1.2.4.2.2.cmml">p</mi><mi id="S2.E2.m1.4.4.1.1.1.2.4.2.3" xref="S2.E2.m1.4.4.1.1.1.2.4.2.3.cmml">θ</mi></msub></mrow><mo id="S2.E2.m1.4.4.1.1.1.2.3" xref="S2.E2.m1.4.4.1.1.1.2.3.cmml">⁢</mo><mrow id="S2.E2.m1.4.4.1.1.1.2.2.2" xref="S2.E2.m1.4.4.1.1.1.2.2.3.cmml"><mo id="S2.E2.m1.4.4.1.1.1.2.2.2.3" stretchy="false" xref="S2.E2.m1.4.4.1.1.1.2.2.3.cmml">(</mo><msub id="S2.E2.m1.4.4.1.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.cmml">𝐲</mi><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.3.cmml"><mn id="S2.E2.m1.4.4.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.3.2.cmml">1</mn><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.3.1.cmml">:</mo><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo id="S2.E2.m1.4.4.1.1.1.2.2.2.4" xref="S2.E2.m1.4.4.1.1.1.2.2.3.cmml">;</mo><mrow id="S2.E2.m1.4.4.1.1.1.2.2.2.2" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.cmml"><msup id="S2.E2.m1.4.4.1.1.1.2.2.2.2.2" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.2.cmml"><mi id="S2.E2.m1.4.4.1.1.1.2.2.2.2.2.2" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.2.2.cmml">t</mi><mi id="S2.E2.m1.4.4.1.1.1.2.2.2.2.2.3" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.2.3.cmml">j</mi></msup><mo fence="false" id="S2.E2.m1.4.4.1.1.1.2.2.2.2.1" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.1.cmml">|</mo><msub id="S2.E2.m1.4.4.1.1.1.2.2.2.2.3" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.cmml"><mi id="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.2" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.2.cmml">𝐱</mi><mrow id="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.3" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.3.cmml"><mn id="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.3.2" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.3.2.cmml">1</mn><mo id="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.3.1" lspace="0.278em" rspace="0.278em" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.3.1.cmml">:</mo><mi id="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.3.3" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.3.3.cmml">t</mi></mrow></msub></mrow><mo id="S2.E2.m1.4.4.1.1.1.2.2.2.5" stretchy="false" xref="S2.E2.m1.4.4.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mspace id="S2.E2.m1.4.4.1.1.2" width="1em" xref="S2.E2.m1.4.4.1.2.cmml"></mspace><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">j</mi></mrow><mo id="S2.E2.m1.6.6.4" xref="S2.E2.m1.6.6.4.cmml">=</mo><mrow id="S2.E2.m1.6.6.3.2" xref="S2.E2.m1.6.6.3.3.cmml"><munder id="S2.E2.m1.5.5.2.1.1" xref="S2.E2.m1.5.5.2.1.1.cmml"><mi id="S2.E2.m1.5.5.2.1.1.2" xref="S2.E2.m1.5.5.2.1.1.2.cmml">max</mi><mi id="S2.E2.m1.5.5.2.1.1.3" xref="S2.E2.m1.5.5.2.1.1.3.cmml">j</mi></munder><mo id="S2.E2.m1.6.6.3.2a" xref="S2.E2.m1.6.6.3.3.cmml">⁡</mo><mrow id="S2.E2.m1.6.6.3.2.2" xref="S2.E2.m1.6.6.3.3.cmml"><mo id="S2.E2.m1.6.6.3.2.2.2" stretchy="false" xref="S2.E2.m1.6.6.3.3.cmml">{</mo><mrow id="S2.E2.m1.6.6.3.2.2.1" xref="S2.E2.m1.6.6.3.2.2.1.cmml"><mrow id="S2.E2.m1.6.6.3.2.2.1.2" xref="S2.E2.m1.6.6.3.2.2.1.2.cmml"><mi id="S2.E2.m1.6.6.3.2.2.1.2.2" xref="S2.E2.m1.6.6.3.2.2.1.2.2.cmml">j</mi><mo fence="false" id="S2.E2.m1.6.6.3.2.2.1.2.1" xref="S2.E2.m1.6.6.3.2.2.1.2.1.cmml">|</mo><msup id="S2.E2.m1.6.6.3.2.2.1.2.3" xref="S2.E2.m1.6.6.3.2.2.1.2.3.cmml"><mi id="S2.E2.m1.6.6.3.2.2.1.2.3.2" xref="S2.E2.m1.6.6.3.2.2.1.2.3.2.cmml">t</mi><mi id="S2.E2.m1.6.6.3.2.2.1.2.3.3" xref="S2.E2.m1.6.6.3.2.2.1.2.3.3.cmml">j</mi></msup></mrow><mo id="S2.E2.m1.6.6.3.2.2.1.1" xref="S2.E2.m1.6.6.3.2.2.1.1.cmml">&lt;</mo><mi id="S2.E2.m1.6.6.3.2.2.1.3" xref="S2.E2.m1.6.6.3.2.2.1.3.cmml">t</mi></mrow><mo id="S2.E2.m1.6.6.3.2.2.3" stretchy="false" xref="S2.E2.m1.6.6.3.3.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.6b"><apply id="S2.E2.m1.6.6.cmml" xref="S2.E2.m1.6.6"><eq id="S2.E2.m1.6.6.4.cmml" xref="S2.E2.m1.6.6.4"></eq><list id="S2.E2.m1.4.4.1.2.cmml" xref="S2.E2.m1.4.4.1.1"><apply id="S2.E2.m1.4.4.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1"><minus id="S2.E2.m1.4.4.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.3"></minus><apply id="S2.E2.m1.4.4.1.1.1.4.cmml" xref="S2.E2.m1.4.4.1.1.1.4"><min id="S2.E2.m1.4.4.1.1.1.4.1.cmml" xref="S2.E2.m1.4.4.1.1.1.4.1"></min><apply id="S2.E2.m1.4.4.1.1.1.4.2.cmml" xref="S2.E2.m1.4.4.1.1.1.4.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.4.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.4.2">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.4.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.4.2.2">𝔼</ci><apply id="S2.E2.m1.2.2.2.cmml" xref="S2.E2.m1.2.2.2"><csymbol cd="latexml" id="S2.E2.m1.2.2.2.3.cmml" xref="S2.E2.m1.2.2.2.3">similar-to</csymbol><ci id="S2.E2.m1.2.2.2.4.cmml" xref="S2.E2.m1.2.2.2.4">𝑡</ci><apply id="S2.E2.m1.2.2.2.5.cmml" xref="S2.E2.m1.2.2.2.5"><times id="S2.E2.m1.2.2.2.5.1.cmml" xref="S2.E2.m1.2.2.2.5.1"></times><ci id="S2.E2.m1.2.2.2.5.2.cmml" xref="S2.E2.m1.2.2.2.5.2">𝒰</ci><interval closure="closed" id="S2.E2.m1.2.2.2.5.3.1.cmml" xref="S2.E2.m1.2.2.2.5.3.2"><cn id="S2.E2.m1.1.1.1.1.cmml" type="integer" xref="S2.E2.m1.1.1.1.1">1</cn><ci id="S2.E2.m1.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2">𝑀</ci></interval></apply></apply></apply></apply><apply id="S2.E2.m1.4.4.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.2"><times id="S2.E2.m1.4.4.1.1.1.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.2.3"></times><apply id="S2.E2.m1.4.4.1.1.1.2.4.cmml" xref="S2.E2.m1.4.4.1.1.1.2.4"><log id="S2.E2.m1.4.4.1.1.1.2.4.1.cmml" xref="S2.E2.m1.4.4.1.1.1.2.4.1"></log><apply id="S2.E2.m1.4.4.1.1.1.2.4.2.cmml" xref="S2.E2.m1.4.4.1.1.1.2.4.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.2.4.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.2.4.2">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.2.4.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.2.4.2.2">𝑝</ci><ci id="S2.E2.m1.4.4.1.1.1.2.4.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.2.4.2.3">𝜃</ci></apply></apply><list id="S2.E2.m1.4.4.1.1.1.2.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.2"><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.2">𝐲</ci><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.3"><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.3.1">:</ci><cn id="S2.E2.m1.4.4.1.1.1.1.1.1.1.3.2.cmml" type="integer" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.3.2">1</cn><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply><apply id="S2.E2.m1.4.4.1.1.1.2.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2"><csymbol cd="latexml" id="S2.E2.m1.4.4.1.1.1.2.2.2.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.1">conditional</csymbol><apply id="S2.E2.m1.4.4.1.1.1.2.2.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.2.2.2.2.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.2">superscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.2.2.2.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.2.2">𝑡</ci><ci id="S2.E2.m1.4.4.1.1.1.2.2.2.2.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.2.3">𝑗</ci></apply><apply id="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.3">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.2">𝐱</ci><apply id="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.3.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.3"><ci id="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.3.1">:</ci><cn id="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.3.2.cmml" type="integer" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.3.2">1</cn><ci id="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.3.3.cmml" xref="S2.E2.m1.4.4.1.1.1.2.2.2.2.3.3.3">𝑡</ci></apply></apply></apply></list></apply></apply><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">𝑗</ci></list><apply id="S2.E2.m1.6.6.3.3.cmml" xref="S2.E2.m1.6.6.3.2"><apply id="S2.E2.m1.5.5.2.1.1.cmml" xref="S2.E2.m1.5.5.2.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.5.5.2.1.1.1.cmml" xref="S2.E2.m1.5.5.2.1.1">subscript</csymbol><max id="S2.E2.m1.5.5.2.1.1.2.cmml" xref="S2.E2.m1.5.5.2.1.1.2"></max><ci id="S2.E2.m1.5.5.2.1.1.3.cmml" xref="S2.E2.m1.5.5.2.1.1.3">𝑗</ci></apply><apply id="S2.E2.m1.6.6.3.2.2.1.cmml" xref="S2.E2.m1.6.6.3.2.2.1"><lt id="S2.E2.m1.6.6.3.2.2.1.1.cmml" xref="S2.E2.m1.6.6.3.2.2.1.1"></lt><apply id="S2.E2.m1.6.6.3.2.2.1.2.cmml" xref="S2.E2.m1.6.6.3.2.2.1.2"><csymbol cd="latexml" id="S2.E2.m1.6.6.3.2.2.1.2.1.cmml" xref="S2.E2.m1.6.6.3.2.2.1.2.1">conditional</csymbol><ci id="S2.E2.m1.6.6.3.2.2.1.2.2.cmml" xref="S2.E2.m1.6.6.3.2.2.1.2.2">𝑗</ci><apply id="S2.E2.m1.6.6.3.2.2.1.2.3.cmml" xref="S2.E2.m1.6.6.3.2.2.1.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.6.6.3.2.2.1.2.3.1.cmml" xref="S2.E2.m1.6.6.3.2.2.1.2.3">superscript</csymbol><ci id="S2.E2.m1.6.6.3.2.2.1.2.3.2.cmml" xref="S2.E2.m1.6.6.3.2.2.1.2.3.2">𝑡</ci><ci id="S2.E2.m1.6.6.3.2.2.1.2.3.3.cmml" xref="S2.E2.m1.6.6.3.2.2.1.2.3.3">𝑗</ci></apply></apply><ci id="S2.E2.m1.6.6.3.2.2.1.3.cmml" xref="S2.E2.m1.6.6.3.2.2.1.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.6c">\min\mathbb{E}_{t\sim\mathcal{U}[1,M]}-\log p_{\theta}({\mathbf{y}}_{1:j};t^{j%
}|{\mathbf{x}}_{1:t})\quad j=\max_{j}\{j|t^{j}&lt;t\}\;</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.6d">roman_min blackboard_E start_POSTSUBSCRIPT italic_t ∼ caligraphic_U [ 1 , italic_M ] end_POSTSUBSCRIPT - roman_log italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_y start_POSTSUBSCRIPT 1 : italic_j end_POSTSUBSCRIPT ; italic_t start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT | bold_x start_POSTSUBSCRIPT 1 : italic_t end_POSTSUBSCRIPT ) italic_j = roman_max start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT { italic_j | italic_t start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT &lt; italic_t }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS3.p2.9">where <math alttext="\mathcal{U}" class="ltx_Math" display="inline" id="S2.SS3.p2.9.m1.1"><semantics id="S2.SS3.p2.9.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p2.9.m1.1.1" xref="S2.SS3.p2.9.m1.1.1.cmml">𝒰</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.9.m1.1b"><ci id="S2.SS3.p2.9.m1.1.1.cmml" xref="S2.SS3.p2.9.m1.1.1">𝒰</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.9.m1.1c">\mathcal{U}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.9.m1.1d">caligraphic_U</annotation></semantics></math> indicates uniform distribution over time of speech.
Trained with <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S2.E2" title="In 2.3 Data Driven Read-Write Policy: &lt;INPUT&gt; and &lt;OUTPUT&gt; ‣ 2 Methods ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">2</span></a>, CLASI learns to generate the cut-off time for the input speech. Additionally, the objective function makes the CLASI wait for appropriate time before starting translation as the LLM will output nothing when it “think” current speech stream does not contain a complete speech chunk.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Context Information: <span class="ltx_text ltx_font_typewriter" id="S2.SS4.1.1">&lt;LOAD_MEM&gt;</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SS4.2.2">&lt;UPDATE_MEM&gt;</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">The memory stores translations and transcriptions in previous rounds <math alttext="{\mathbf{y}}_{1:r-1}" class="ltx_Math" display="inline" id="S2.SS4.p1.1.m1.1"><semantics id="S2.SS4.p1.1.m1.1a"><msub id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml"><mi id="S2.SS4.p1.1.m1.1.1.2" xref="S2.SS4.p1.1.m1.1.1.2.cmml">𝐲</mi><mrow id="S2.SS4.p1.1.m1.1.1.3" xref="S2.SS4.p1.1.m1.1.1.3.cmml"><mn id="S2.SS4.p1.1.m1.1.1.3.2" xref="S2.SS4.p1.1.m1.1.1.3.2.cmml">1</mn><mo id="S2.SS4.p1.1.m1.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S2.SS4.p1.1.m1.1.1.3.1.cmml">:</mo><mrow id="S2.SS4.p1.1.m1.1.1.3.3" xref="S2.SS4.p1.1.m1.1.1.3.3.cmml"><mi id="S2.SS4.p1.1.m1.1.1.3.3.2" xref="S2.SS4.p1.1.m1.1.1.3.3.2.cmml">r</mi><mo id="S2.SS4.p1.1.m1.1.1.3.3.1" xref="S2.SS4.p1.1.m1.1.1.3.3.1.cmml">−</mo><mn id="S2.SS4.p1.1.m1.1.1.3.3.3" xref="S2.SS4.p1.1.m1.1.1.3.3.3.cmml">1</mn></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.1b"><apply id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.1.m1.1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS4.p1.1.m1.1.1.2.cmml" xref="S2.SS4.p1.1.m1.1.1.2">𝐲</ci><apply id="S2.SS4.p1.1.m1.1.1.3.cmml" xref="S2.SS4.p1.1.m1.1.1.3"><ci id="S2.SS4.p1.1.m1.1.1.3.1.cmml" xref="S2.SS4.p1.1.m1.1.1.3.1">:</ci><cn id="S2.SS4.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S2.SS4.p1.1.m1.1.1.3.2">1</cn><apply id="S2.SS4.p1.1.m1.1.1.3.3.cmml" xref="S2.SS4.p1.1.m1.1.1.3.3"><minus id="S2.SS4.p1.1.m1.1.1.3.3.1.cmml" xref="S2.SS4.p1.1.m1.1.1.3.3.1"></minus><ci id="S2.SS4.p1.1.m1.1.1.3.3.2.cmml" xref="S2.SS4.p1.1.m1.1.1.3.3.2">𝑟</ci><cn id="S2.SS4.p1.1.m1.1.1.3.3.3.cmml" type="integer" xref="S2.SS4.p1.1.m1.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.1c">{\mathbf{y}}_{1:r-1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.1.m1.1d">bold_y start_POSTSUBSCRIPT 1 : italic_r - 1 end_POSTSUBSCRIPT</annotation></semantics></math>. It has two functions. Firstly, it works with the input speech to determine which part of the speech has been translated and which part has not, helping CLASI make the read-write decisions and outputs the translation of the unfinished parts.
Secondly, understanding human speech often requires context. For example, when a speaker talks about “barrel bridge”, it often refers to the bridges built upon rivers that are supported by barrels. However, in the context of “watch”, it refers to a mechanical structure in the watch. The phenomenon of polysemy in different contexts can lead to vastly different translation outcomes. Therefore, CLASI should be able to retrieve the context of the long speech for translating some keywords, and make appropriate translations under different contexts.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.4">As shown in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S2.F3" title="In 2 Methods ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>, at round <math alttext="r" class="ltx_Math" display="inline" id="S2.SS4.p2.1.m1.1"><semantics id="S2.SS4.p2.1.m1.1a"><mi id="S2.SS4.p2.1.m1.1.1" xref="S2.SS4.p2.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.1.m1.1b"><ci id="S2.SS4.p2.1.m1.1.1.cmml" xref="S2.SS4.p2.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.1.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.1.m1.1d">italic_r</annotation></semantics></math>, <span class="ltx_text ltx_font_typewriter" id="S2.SS4.p2.4.1">&lt;LOAD_MEM&gt;</span> forwards relevant translations <math alttext="{\mathbf{y}}_{1:r-1}" class="ltx_Math" display="inline" id="S2.SS4.p2.2.m2.1"><semantics id="S2.SS4.p2.2.m2.1a"><msub id="S2.SS4.p2.2.m2.1.1" xref="S2.SS4.p2.2.m2.1.1.cmml"><mi id="S2.SS4.p2.2.m2.1.1.2" xref="S2.SS4.p2.2.m2.1.1.2.cmml">𝐲</mi><mrow id="S2.SS4.p2.2.m2.1.1.3" xref="S2.SS4.p2.2.m2.1.1.3.cmml"><mn id="S2.SS4.p2.2.m2.1.1.3.2" xref="S2.SS4.p2.2.m2.1.1.3.2.cmml">1</mn><mo id="S2.SS4.p2.2.m2.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S2.SS4.p2.2.m2.1.1.3.1.cmml">:</mo><mrow id="S2.SS4.p2.2.m2.1.1.3.3" xref="S2.SS4.p2.2.m2.1.1.3.3.cmml"><mi id="S2.SS4.p2.2.m2.1.1.3.3.2" xref="S2.SS4.p2.2.m2.1.1.3.3.2.cmml">r</mi><mo id="S2.SS4.p2.2.m2.1.1.3.3.1" xref="S2.SS4.p2.2.m2.1.1.3.3.1.cmml">−</mo><mn id="S2.SS4.p2.2.m2.1.1.3.3.3" xref="S2.SS4.p2.2.m2.1.1.3.3.3.cmml">1</mn></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.2.m2.1b"><apply id="S2.SS4.p2.2.m2.1.1.cmml" xref="S2.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.p2.2.m2.1.1.1.cmml" xref="S2.SS4.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS4.p2.2.m2.1.1.2.cmml" xref="S2.SS4.p2.2.m2.1.1.2">𝐲</ci><apply id="S2.SS4.p2.2.m2.1.1.3.cmml" xref="S2.SS4.p2.2.m2.1.1.3"><ci id="S2.SS4.p2.2.m2.1.1.3.1.cmml" xref="S2.SS4.p2.2.m2.1.1.3.1">:</ci><cn id="S2.SS4.p2.2.m2.1.1.3.2.cmml" type="integer" xref="S2.SS4.p2.2.m2.1.1.3.2">1</cn><apply id="S2.SS4.p2.2.m2.1.1.3.3.cmml" xref="S2.SS4.p2.2.m2.1.1.3.3"><minus id="S2.SS4.p2.2.m2.1.1.3.3.1.cmml" xref="S2.SS4.p2.2.m2.1.1.3.3.1"></minus><ci id="S2.SS4.p2.2.m2.1.1.3.3.2.cmml" xref="S2.SS4.p2.2.m2.1.1.3.3.2">𝑟</ci><cn id="S2.SS4.p2.2.m2.1.1.3.3.3.cmml" type="integer" xref="S2.SS4.p2.2.m2.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.2.m2.1c">{\mathbf{y}}_{1:r-1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.2.m2.1d">bold_y start_POSTSUBSCRIPT 1 : italic_r - 1 end_POSTSUBSCRIPT</annotation></semantics></math> to the LLM as a prompt. After CLASI agent generates the translation <math alttext="{\mathbf{y}}_{r}" class="ltx_Math" display="inline" id="S2.SS4.p2.3.m3.1"><semantics id="S2.SS4.p2.3.m3.1a"><msub id="S2.SS4.p2.3.m3.1.1" xref="S2.SS4.p2.3.m3.1.1.cmml"><mi id="S2.SS4.p2.3.m3.1.1.2" xref="S2.SS4.p2.3.m3.1.1.2.cmml">𝐲</mi><mi id="S2.SS4.p2.3.m3.1.1.3" xref="S2.SS4.p2.3.m3.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.3.m3.1b"><apply id="S2.SS4.p2.3.m3.1.1.cmml" xref="S2.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS4.p2.3.m3.1.1.1.cmml" xref="S2.SS4.p2.3.m3.1.1">subscript</csymbol><ci id="S2.SS4.p2.3.m3.1.1.2.cmml" xref="S2.SS4.p2.3.m3.1.1.2">𝐲</ci><ci id="S2.SS4.p2.3.m3.1.1.3.cmml" xref="S2.SS4.p2.3.m3.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.3.m3.1c">{\mathbf{y}}_{r}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.3.m3.1d">bold_y start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>, <span class="ltx_text ltx_font_typewriter" id="S2.SS4.p2.4.2">&lt;UPDATE_MEM&gt;</span> stores it to the memory and obtains <math alttext="{\mathbf{y}}_{1:r}" class="ltx_Math" display="inline" id="S2.SS4.p2.4.m4.1"><semantics id="S2.SS4.p2.4.m4.1a"><msub id="S2.SS4.p2.4.m4.1.1" xref="S2.SS4.p2.4.m4.1.1.cmml"><mi id="S2.SS4.p2.4.m4.1.1.2" xref="S2.SS4.p2.4.m4.1.1.2.cmml">𝐲</mi><mrow id="S2.SS4.p2.4.m4.1.1.3" xref="S2.SS4.p2.4.m4.1.1.3.cmml"><mn id="S2.SS4.p2.4.m4.1.1.3.2" xref="S2.SS4.p2.4.m4.1.1.3.2.cmml">1</mn><mo id="S2.SS4.p2.4.m4.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S2.SS4.p2.4.m4.1.1.3.1.cmml">:</mo><mi id="S2.SS4.p2.4.m4.1.1.3.3" xref="S2.SS4.p2.4.m4.1.1.3.3.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.4.m4.1b"><apply id="S2.SS4.p2.4.m4.1.1.cmml" xref="S2.SS4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS4.p2.4.m4.1.1.1.cmml" xref="S2.SS4.p2.4.m4.1.1">subscript</csymbol><ci id="S2.SS4.p2.4.m4.1.1.2.cmml" xref="S2.SS4.p2.4.m4.1.1.2">𝐲</ci><apply id="S2.SS4.p2.4.m4.1.1.3.cmml" xref="S2.SS4.p2.4.m4.1.1.3"><ci id="S2.SS4.p2.4.m4.1.1.3.1.cmml" xref="S2.SS4.p2.4.m4.1.1.3.1">:</ci><cn id="S2.SS4.p2.4.m4.1.1.3.2.cmml" type="integer" xref="S2.SS4.p2.4.m4.1.1.3.2">1</cn><ci id="S2.SS4.p2.4.m4.1.1.3.3.cmml" xref="S2.SS4.p2.4.m4.1.1.3.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.4.m4.1c">{\mathbf{y}}_{1:r}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.4.m4.1d">bold_y start_POSTSUBSCRIPT 1 : italic_r end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Multi-Modal Retrieval Augmented Generation: <span class="ltx_text ltx_font_typewriter" id="S2.SS5.1.1">&lt;RETRIEVE&gt;</span>
</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">In real-world scenarios, the accurate speech transcription or translation of professional and domain-specific terminologies is challenging. Even human interpreters require prior domain knowledge to understand those terminologies, including names of people, locations, jargon, or special in-domain terms.
For example, an interpreter unfamiliar with the machine learning theory may not recognize the word “Rademacher complexity” when hearing it.
Therefore, in various scenarios, human interpreters often prepare in advance to get familiar with the corresponding domain knowledge.</p>
</div>
<div class="ltx_para" id="S2.SS5.p2">
<p class="ltx_p" id="S2.SS5.p2.1">Motivated by the preparatory trajectory of human interpreters, we propose to integrate an external database to empower LLM with necessary domain-specific knowledge. Each item in the database contains a key and the corresponding value in text modality. The key, which may appear in the speech, is used as the input for the retriever. The value of the item may be itself, a paired translation of the target language, or even an explanation of the key.</p>
</div>
<div class="ltx_para" id="S2.SS5.p3">
<p class="ltx_p" id="S2.SS5.p3.1">Theoretically, all items in the external database might be added into the prompt to provide information for the translation. However, the external knowledge database often contains tremendous items.
Simply prompting LLM with all the terms not only increases the
inference time but may also hurt the performance of CLASI because of noisy intervention.
Therefore, we design a novel Multi-Modal Retrieval Augmented Generation (MM-RAG) process.
Our multi-modal retriever first retrieves the relevant terminologies from the database based on the input speech.
A small number of filtered items are incorporated into the prompt of CLASI agent for in-context learning as shown in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S2.F3" title="In 2 Methods ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS5.p4">
<p class="ltx_p" id="S2.SS5.p4.1">With the retrieved knowledge and previous context from the memory, our LLM has the in-context learning ability to better utilize the provided contextual information.
To achieve this, we collect a series of in-context learning data to train the model. Compared with the previous approaches for intervention, such as shallow-fusion <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib8" title="">Borgeaud2021ImprovingLM </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib34" title="">Kolehmainen2024MultiModalRF </a></cite> and traditional substitution-based methods <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib38" title="">li2019neural </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib44" title="">luong2015addressing </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib78" title="">wang2017sogou </a></cite>, which generates fixed translation for given translation pair. Our method achieves better results and generates more coherent text. For example, in some internet companies, “大盘 == overall performance”, while in most cases, it should be “stock market”. Our method can choose the correct translation given different context. Besides, our method can use monolingual text from both source and target language to help the translation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Multi-Stage Training</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our CLASI follows a multi-stage training process: pretraining, multi-task continual training, and multi-task supervised fine-tuning.
In the first stage, the LLM and audio encoder are separately pretrained with massive amounts of in-house speech and text data.
Next, a large amount of speech-text paired data is used to align audio and text modalities, building the fundamental capability for cross-modal multitasking.
In the final stage, CLASI agent is fine-tuned with a small amount of human-annotated data to imitate the translation behavior of professional human interpreters.
Our multi-stage training process enables high efficiency of learning with a small amount of human-labeled data.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Pretraining</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The in-house LLM and audio encoder are independently pretrained on different modalities of data.
The LLM follows a decoder-only transformer architecture and is first pretrained on a massive amount of monolingual and bilingual text data with cross-entropy loss and then fine-tuned on instruction-following data. The LLM performs excellently on various downstream tasks, especially translation tasks.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The audio encoder also follows a classic pretrain-finetune paradigm <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">bai2024</span>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib5" title="">baevski2022data2vec, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib28" title="">hsu2021hubert, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib88" title="">zhang2023usm, </a>)</cite> with a massive amount of speech-related data.
The pretraining stage provides a proficiently trained LLM and audio encoder, setting a solid foundation for the following stages.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Multi-task Continual Training</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.1">Training Method.</span>
We follow the work of <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib30" title="">huang2023speechtranslationlargelanguage, </a>)</cite> for multi-task training. Specifically, for streaming and higher-quality translation, we mainly focus on three tasks for training CLASI: Automatic Speech Recognition (ASR), Speech Translation (ST), and Text Translation (MT).
To align the modalities of the pretrained LLM and audio encoder,
CLASI is continually trained on various tasks with a substantial volume of paired data.
We further strengthen the in-context learning ability of our approach by incorporating translation in the memory and knowledge from external databases.
As a result, we expand the ST tasks to different configurations as shown in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S3.T1" title="In 3.2 Multi-task Continual Training ‣ 3 Multi-Stage Training ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>. An ST translation can either be streaming or offline, direct or COT, with or without context, which leads to 8 different tasks.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.2" style="width:433.6pt;height:111.1pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-31.1pt,7.9pt) scale(0.874477164098435,0.874477164098435) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T1.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.1.1.1">Configuration</span></th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.1.2.1">Explanation</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.2.1.2.2.1">Direct</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.1.2.2.2">Speech is directly translated into the target language.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.2.1.3.3.1">COT</th>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.3.3.2">Speech is first transcribed to the source language and then translated to the target language.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.2.1.4.4.1">Streaming</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.1.4.4.2">Given <span class="ltx_text ltx_font_bold" id="S3.T1.2.1.4.4.2.1">partial</span> speech, translate segments with complete semantics to the target language.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.2.1.5.5.1">Offline</th>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.5.5.2">Given <span class="ltx_text ltx_font_bold" id="S3.T1.2.1.5.5.2.1">complete</span> speech, translate the whole content to the target language.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.2.1.6.6.1">w/o Context</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.1.6.6.2">No historical translations and external knowledge are provided.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.2.1.7.7.1">w/ Context</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.1.7.7.2">Historical translations or external knowledge are provided as context.</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.4.2" style="font-size:90%;">Illustration of different configurations of ST task in Multi-task Continual Training.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Training Data Construction.</span>
Both ASR and MT tasks have been last for a while, and there is a relatively large amount of ASR and MT data.
The major challenge of developing an end-to-end SiST model is the data scarcity of simultaneous ST.
To this end, we propose a synthetic data construction pipeline. With a strong LLM, we synthesize two types of speech translation data for continual training: offline ST data and context-aware segmented streaming ST data.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">1. Offline ST data: We mainly rely on ASR data to construct the offline ST data. Given the ground-truth transcription of speech, we use in-house LLM to translate the source language to target languages.
To ensure the readability and conciseness of the target language, the LLM is prompted to conduct Inverse Text Normalization (ITN), filler word smoothing, etc.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">2. Context-aware segmented streaming ST data: The streaming ST data consists of fine-grained audio-text alignments and translation pairs for segmented semantic chunks.
Compared to offline ST data, streaming ST data
is even more challenging to collect.
We find that human interpreter often segments long speech into a few semantic chunks, each of which can be translated independently to ensure an effective and smooth translation.
Motivated by such findings, we leverage LLM to construct streaming ST data by imitating the chunking process.
Long speech data are used to construct the streaming ST data, as the additional history can provide better contextual information.
First, we prompt the LLM to break down the ASR transcription into multiple independent semantic chunks, which are then translated into the target language.
Subsequently, we align the semantic chunks with the corresponding audio chunks, obtaining the streaming ST data.
Such data enable our model to handle incomplete speech inputs and generate partial translation in coherent semantics.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">To measure the quality of the synthetic data, we conduct human evaluations based on our proposed VIP metric.
The synthetic data achieves a VIP score of 81%, satisfying the minimal requirement for further training.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Multi-task Supervised Fine-tuning</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p1.1.1">Training Method.</span>
Even though CLASI possesses a good translation quality on the SiST tasks after the previous multi-task continual training stage, we further boost the performance by fine-tuning on human-annotated streaming ST data with diverse tasks listed in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S3.T1" title="In 3.2 Multi-task Continual Training ‣ 3 Multi-Stage Training ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>.
Such high-quality data enables our model to better align with the segmentation methodologies of professional human interpreters.
Furthermore, this process enhances our model’s robustness to speech disfluencies such as stuttering, ensuring smoother communication in real-world scenarios.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">Training Data Construction.</span>
The source of human-annotated streaming ST data originates from real-world scenarios that contain various speech characteristics, such as disfluencies, stuttering, code-mixing, and specialized terminologies.
Such features ensure the robustness of our model in diverse conditions.
We engage professional human interpreters to provide high-quality annotations for simultaneous segmentation and interpretation of the speech data.
Additionally, terminologies are identified and translated within the context, further strengthening the context-aware capabilities of CLASI.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Multi-Modal Retriever Training</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.3">The multi-modal retriever is independently trained with a substantial dataset of speech recognition data.
During training, words are randomly selected from speech transcription to serve as the positive sample, indicating their appearance in the speech.
Negative words are selected from different sentences, indicating the speech does not mention these words.
We assign a label of <math alttext="1" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><mn id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><cn id="S3.SS4.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS4.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">1</annotation></semantics></math> to positive samples and <math alttext="0" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m2.1"><semantics id="S3.SS4.p1.2.m2.1a"><mn id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><cn id="S3.SS4.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS4.p1.2.m2.1.1">0</cn></annotation-xml></semantics></math> to negative samples, aiming to minimize the Binary Cross Entropy (BCE) loss. This approach helps refine the model’s ability to distinguish relevant from irrelevant information, enhancing its overall performance and accuracy.
We label the positive sample as <math alttext="1" class="ltx_Math" display="inline" id="S3.SS4.p1.3.m3.1"><semantics id="S3.SS4.p1.3.m3.1a"><mn id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><cn id="S3.SS4.p1.3.m3.1.1.cmml" type="integer" xref="S3.SS4.p1.3.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">1</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.3.m3.1d">1</annotation></semantics></math> and the negative sample as 0, minimizing the Binary Cross Entropy (BCE) loss.
To evaluate the effectiveness of our retriever, we build an in-house retrieve development set. Each sample in the development set includes a short audio chunk and the mentioned terms in the audio.
Note that the term here is defined as special keywords, such as name, location, abbreviation, and domain-specific word.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation Benchmark</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Quite a few number of evaluation benchmarks have been proposed for SiST over the past years, including MuST-C <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib15" title="">di2019must </a></cite>, FLUERS <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib14" title="">conneau2023fleurs </a></cite>, CoVoST <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib74" title="">wang-etal-2020-covost </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib75" title="">wang2020covost </a></cite>, BSTC <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib85" title="">zhang-etal-2021-bstc </a></cite>, and GigaST <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib83" title="">ye23b_interspeech </a></cite>, etc.
However, although much effort has been spent to build these benchmarks, they still suffer some shortcomings when facing real-world SiST applications.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">First, these benchmarks often contain speeches that are either recorded by volunteers (<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1">e.g.</span> CoVoST and FLUERS) or collected from formally, clearly, and fluently talk and podcasts by well-prepared speakers (<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.2">e.g.</span> MuST-C and GigaST). In real-world scenarios such as online meetings or social media videos, the characteristics of the speech might inevitably be informal, unclear, or disfluent.
Second, these benchmarks provide a shortcut for evaluating the translation quality by giving the manually segmented sentences. Such a shortcut offers a gap between the current benchmark and real-world applications, where the models might need to take long speech and conduct segmentation <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib4" title="">anastasopoulos2021findings </a></cite> by themselves. Consequently, evaluations on manually segmented datasets are likely to overestimate the performances of a real-world SiST system.
These discrepancies result in the evaluation on these benchmarks are not reliable for practical SiST systems.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">As a preliminary attempt to address the shortcomings as mentioned earlier, we propose a new benchmark RealSI for Chinese-to-English (zh-en) and English-to-Chinese (en-zh). RealSI is collected from diverse sources, and most speakers talk naturally and casually without careful preparation.
We choose 10 popular domains: technology, healthcare, education, finance, law, environment, entertainment, science, sports, and art.
One video clip is selected for each domain from a well-known online video platform for both zh-en and en-zh settings.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>RealSI is available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/byteresearchcla/RealSI" title="">https://github.com/byteresearchcla/RealSI</a>. We do not own the copyright of the videos and only release our annotations together with the publicly available website links of the corresponding videos. If anyone believes that the content constitutes infringement, please contact us. We will remove the relevant content as soon as it is confirmed.</span></span></span>
Each sample in RealSI is a nearly 5-minute speech to mock SiST without manual segmentation.
For systems that cannot take long-form audio as input, we also provide sentence-level timestamps for segmentation.
<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.T2" title="In 4.1 Evaluation Benchmark ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a> presents the detailed statistics of our RealSI.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.2" style="width:273.2pt;height:208pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.0pt,13.0pt) scale(0.889040014369004,0.889040014369004) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.2.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.1.1">Domain</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T2.2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.1">zh-en</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T2.2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.3.1">en-zh</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.2.2.1">Duration</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.2.2.2">#Segments</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.2.2.3">Duration</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.2.2.4">#Segments</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.2.1.3.1.1">Technology</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.3.1.2">5:23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.3.1.3">51</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.3.1.4">3:25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.3.1.5">31</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.4.2">
<td class="ltx_td ltx_align_left" id="S4.T2.2.1.4.2.1">Healthcare</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.2.2">3:16</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.2.3">30</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.2.4">3:34</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.2.5">22</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.5.3">
<td class="ltx_td ltx_align_left" id="S4.T2.2.1.5.3.1">Education</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.3.2">4:56</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.3.3">48</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.3.4">5:00</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.3.5">41</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.6.4">
<td class="ltx_td ltx_align_left" id="S4.T2.2.1.6.4.1">Finance</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.6.4.2">5:22</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.6.4.3">29</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.6.4.4">5:01</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.6.4.5">40</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.7.5">
<td class="ltx_td ltx_align_left" id="S4.T2.2.1.7.5.1">Law</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.5.2">4:38</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.5.3">49</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.5.4">4:48</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.5.5">29</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.8.6">
<td class="ltx_td ltx_align_left" id="S4.T2.2.1.8.6.1">Environment</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.8.6.2">4:18</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.8.6.3">34</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.8.6.4">4:24</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.8.6.5">31</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.9.7">
<td class="ltx_td ltx_align_left" id="S4.T2.2.1.9.7.1">Entertainment</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.9.7.2">5:16</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.9.7.3">53</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.9.7.4">5:12</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.9.7.5">39</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.10.8">
<td class="ltx_td ltx_align_left" id="S4.T2.2.1.10.8.1">Science</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.10.8.2">4:47</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.10.8.3">37</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.10.8.4">5:11</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.10.8.5">35</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.11.9">
<td class="ltx_td ltx_align_left" id="S4.T2.2.1.11.9.1">Sports</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.11.9.2">5:22</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.11.9.3">33</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.11.9.4">3:25</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.11.9.5">58</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.12.10">
<td class="ltx_td ltx_align_left" id="S4.T2.2.1.12.10.1">Art</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.12.10.2">7:54</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.12.10.3">67</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.12.10.4">4:17</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.12.10.5">21</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.13.11">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T2.2.1.13.11.1">Total</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.2.1.13.11.2">51:12</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.2.1.13.11.3">431</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.2.1.13.11.4">44:17</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.2.1.13.11.5">347</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.4.2" style="font-size:90%;">Statistics of our proposed RealSI benchmark.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Baselines</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We compare CLASI with the open-sourced SiST model, <em class="ltx_emph ltx_font_italic" id="S4.SS2.p1.1.1">SeamlessStreaming</em> <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib7" title="">barrault2023seamless </a></cite>. In addition, because of the limited number of available SiST models, we choose to compare CLASI with several commercial systems. We denote the commercial systems as <em class="ltx_emph ltx_font_italic" id="S4.SS2.p1.1.2">Commercial</em> 1-4. It is worth noting that unlike CLASI, most of the commercial SiST systems will first generate a temporary translation as soon as possible, then rewrite the temporary translation with a potentially better translation after getting more context.
Notwithstanding, we evaluate the finalized translation of these systems in all our experiments.
Although this re-writing strategy could improve translation quality, continually revising existing translations might affect the user experience, potentially leading to additional confusion.
We would also like to highlight that human interpreters usually do
not employ such a rewriting strategy during translation.
During the entire evaluation, we employ a general external knowledge database that maintains the same for all the evaluation in this paper. It does not contain domain-specific external knowledge to form unfair comparisons. The improvement of external knowledge is independently reported in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.SS6" title="4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.6</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Translation Quality</h3>
<figure class="ltx_table" id="S4.SS3.8">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" id="S4.SS3.2.2" style="width:398.9pt;height:117.1pt;vertical-align:-106.9pt;"><span class="ltx_transformed_inner" style="transform:translate(35.9pt,-0.9pt) scale(1.21922698465255,1.21922698465255) ;"><span class="ltx_ERROR undefined" id="S4.SS3.2.2.3">{NiceTabular}</span>
<p class="ltx_p" id="S4.SS3.2.2.2">lccccccc[colortbl-like]
<span class="ltx_ERROR undefined" id="S4.SS3.2.2.2.3">\CodeBefore</span><span class="ltx_text" id="S4.SS3.2.2.2.2" style="background-color:#F2F2FF;">8 <span class="ltx_ERROR undefined" id="S4.SS3.2.2.2.2.3">\Body</span><span class="ltx_text ltx_font_bold" id="S4.SS3.2.2.2.2.4" style="font-size:120%;background-color:#F2F2FF;">zh-en</span> <span class="ltx_text" id="S4.SS3.2.2.2.2.2" style="font-size:90%;">BLEU    BLEURT    COMET   <span class="ltx_text" id="S4.SS3.1.1.1.1.1.1"> VIP<sup class="ltx_sup" id="S4.SS3.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S4.SS3.1.1.1.1.1.1.1.1">†</span></sup> (%)</span>
<br class="ltx_break"/>  
 doc  sent  doc  sent  doc  sent  
<br class="ltx_break"/><em class="ltx_emph ltx_font_italic" id="S4.SS3.2.2.2.2.2.3">SeamlessStreaming</em>  11.3  8.9  33.9  42.7  75.9  65.9  13.2 
<br class="ltx_break"/><em class="ltx_emph ltx_font_italic" id="S4.SS3.2.2.2.2.2.4">Commercial 1</em>  15.0  10.8  30.6  43.9  72.4  67.1  10.4 
<br class="ltx_break"/><em class="ltx_emph ltx_font_italic" id="S4.SS3.2.2.2.2.2.5">Commercial 2</em>  19.6  15.0  37.7  53.4  79.8  75.8  14.6 
<br class="ltx_break"/><em class="ltx_emph ltx_font_italic" id="S4.SS3.2.2.2.2.2.6">Commercial 3</em>  24.5  19.9  40.2  56.4  81.8  78.9  25.0 
<br class="ltx_break"/><em class="ltx_emph ltx_font_italic" id="S4.SS3.2.2.2.2.2.7">Commercial 4</em>  25.2  21.4  40.8  59.3  82.9  80.8  35.4 
<br class="ltx_break"/><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.SS3.2.2.2.2.2.8" style="background-color:#F2F2FF;">CLASI</em> <span class="ltx_text ltx_font_bold" id="S4.SS3.2.2.2.2.2.2">32.6  28.1  44.4  65.9  84.6  84.7  81.3<sup class="ltx_sup" id="S4.SS3.2.2.2.2.2.2.1"><span class="ltx_text ltx_font_medium" id="S4.SS3.2.2.2.2.2.2.1.1">∗</span></sup>
<br class="ltx_break"/></span></span></span></p>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" id="S4.SS3.4.4" style="width:398.9pt;height:121.8pt;vertical-align:-111.2pt;background-color:#F2F2FF;"><span class="ltx_transformed_inner" style="transform:translate(42.3pt,-1.1pt) scale(1.26889867549775,1.26889867549775) ;"><span class="ltx_ERROR undefined" id="S4.SS3.4.4.3">{NiceTabular}</span>
<p class="ltx_p" id="S4.SS3.4.4.2"><span class="ltx_text ltx_font_bold" id="S4.SS3.4.4.2.3" style="font-size:90%;">lccccccc[colortbl-like]
</span><span class="ltx_ERROR undefined" id="S4.SS3.4.4.2.4">\CodeBefore</span><span class="ltx_text ltx_font_bold" id="S4.SS3.4.4.2.2" style="font-size:90%;">8 <span class="ltx_ERROR undefined" id="S4.SS3.4.4.2.2.2">\Body</span><span class="ltx_text" id="S4.SS3.4.4.2.2.3" style="font-size:133%;background-color:#F2F2FF;">en-zh</span>  BLEU    BLEURT    COMET   <span class="ltx_text" id="S4.SS3.3.3.1.1.1"> VIP<sup class="ltx_sup" id="S4.SS3.3.3.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.SS3.3.3.1.1.1.1.1">†</span></sup> (%)</span>
<br class="ltx_break"/>  
 doc  sent  doc  sent  doc  sent  
<br class="ltx_break"/><em class="ltx_emph ltx_font_italic" id="S4.SS3.4.4.2.2.4" style="background-color:#F2F2FF;">SeamlessStreaming</em>  14.8  10.4  22.1  27.8  64.6  60.6  2.0 
<br class="ltx_break"/><em class="ltx_emph ltx_font_italic" id="S4.SS3.4.4.2.2.5" style="background-color:#F2F2FF;">Commercial 1</em>  25.6  20.4  40.2  38.0  70.3  70.9  12.8 
<br class="ltx_break"/><em class="ltx_emph ltx_font_italic" id="S4.SS3.4.4.2.2.6" style="background-color:#F2F2FF;">Commercial 2</em>  29.6  26.0  50.5  46.8  78.2  75.8  16.8 
<br class="ltx_break"/><em class="ltx_emph ltx_font_italic" id="S4.SS3.4.4.2.2.7" style="background-color:#F2F2FF;">Commercial 3</em>  31.6  28.7  51.2  52.8  81.0  79.9  29.5 
<br class="ltx_break"/><em class="ltx_emph ltx_font_italic" id="S4.SS3.4.4.2.2.8" style="background-color:#F2F2FF;">Commercial 4</em>  29.8  26.4  47.0  48.0  77.5  76.7  41.6 
<br class="ltx_break"/><em class="ltx_emph ltx_font_italic" id="S4.SS3.4.4.2.2.9" style="background-color:#F2F2FF;">CLASI</em>  37.4  32.8  54.2  61.3  87.4  85.6  78.0<sup class="ltx_sup" id="S4.SS3.4.4.2.2.10"><span class="ltx_text ltx_font_medium" id="S4.SS3.4.4.2.2.10.1">∗</span></sup>
<br class="ltx_break"/><span class="ltx_text" id="S4.SS3.4.4.2.2.11"></span></span></p>
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_font_bold" style="font-size:90%;background-color:#F2F2FF;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_medium" id="S4.SS3.8.15.1.1">Table 3</span>: </span>Experiment results of translation quality. Automatic evaluations were calculated on both document level and sentence level. In document level evaluations, each translation of a nearly 5-minute audio was considered one instance. For sentence level, automatic scores are calculated on human-segmented translations. VIP refers to the human-evaluated Valid Information Proportion that reflects the translation quality of these systems.
<sup class="ltx_sup" id="S4.SS3.8.16.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.SS3.8.16.2.1">†</span></sup> Due to the limitations in human evaluation capacity, the VIP scores are calculated on 4 randomly selected samples out of 10 in RealSI across all systems for fair comparison, while automatic metrics are evaluated on 10 samples.
<sup class="ltx_sup" id="S4.SS3.8.17.3"><span class="ltx_text ltx_font_medium" id="S4.SS3.8.17.3.1">∗</span></sup> Additionally, we evaluate the performance of CLASI on all 10 samples, achieving VIP scores of 78.0% for zh-en and 74.9% for en-zh.
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<p class="ltx_p ltx_figure_panel" id="S4.SS3.8.18"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.18.1" style="background-color:#F2F2FF;">Evaluation Metrics.<span class="ltx_text ltx_font_medium" id="S4.SS3.8.18.1.1">
Automatic evaluation metrics such as BLEU <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib54" title="">papineni2002bleu </a></cite>, BLEURT <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib68" title="">sellam2020bleurt </a></cite>, and COMET <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib61" title="">rei2020comet </a></cite> are widely used for evaluating the translation quality <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib7" title="">barrault2023seamless </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib65" title="">iwslt-2023-international </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib66" title="">iwslt-2022-international </a></cite>.
However, they may not be able to fully reflect the quality of the translation, especially for paragraph-level translation of long speech. It is argued that the current evaluation metrics are not sufficient for ST and SiST tasks <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib23" title="">gaido2024speech </a></cite>. The work of <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib47" title="">machavcek2023mt </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib79" title="">wein2024barriers </a></cite> also highlighted that there might be a discrepancy between automatic evaluation metrics with human evaluation.</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.SS3.8.19"><span class="ltx_text" id="S4.SS3.8.19.1" style="background-color:#F2F2FF;">Therefore, besides the automatic evaluation, we collaborated with senior professional human simultaneous interpreters to standardize the guidelines for a more realistic human evaluation.
Our proposed human evaluation metric focuses on whether the output of the translation model can accurately convey the speaker’s original intention for each semantic fragment.
This is also the key objective of human interpreters in real-time translation. Note that a single semantic fragment indicates a complete piece of source speech. Typically, a single semantic fragment is a complete sentence.
Detailed definition can be found in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A1.SS2" title="A.2 Evaluation Process ‣ Appendix A Human Evaluation Guidelines ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.2</span></a>.
The percentage of valid information fragments within a complete speech session is defined as VIP, which is consistent with real-world criteria for human simultaneous interpretation <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib81" title="">wu2010assessing </a></cite>.</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.SS3.8.20"><span class="ltx_text ltx_font_bold" id="S4.SS3.8.20.1" style="background-color:#F2F2FF;">Quantitative Analysis.<span class="ltx_text ltx_font_medium" id="S4.SS3.8.20.1.1">
As shown in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.SS3" title="4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>, we compare CLASI with the baseline methods on RealSI dataset.
In terms of the reliable human evaluation metrics, VIP, CLASI achieves scores of 81.3% and 78.0% for zh-en and en-zh, respectively. While all other models’ VIP scores are lower than 42%. For more references, we use 3 widely-used automatic evaluation metrics: BLEU<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We use SacreBLEU <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib58" title="">post-2018-call </a></cite> for all the BLEU calculations in this paper.</span></span></span>, BLEURT, COMET. Under the automatic evaluation metrics, CLASI also surpasses baselines by a large margin. The detailed human evaluation results of CLASI can be found in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A2.SS2" title="B.2 Example of Detailed Evaluation Result on RealSI ‣ Appendix B Supplementary Materials ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">B.2</span></a>.</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S4.SS3.8.21"><span class="ltx_text" id="S4.SS3.8.21.1" style="background-color:#F2F2FF;">High VIP marks CLASI a practical system that can help listeners understand real-time speech without professional human interpreters. Note that we only consider a system is better than others when the VIP is higher. For example, even though <em class="ltx_emph ltx_font_italic" id="S4.SS3.8.21.1.1">Commercial 1</em> achieves higher scores than <em class="ltx_emph ltx_font_italic" id="S4.SS3.8.21.1.2">SeamlessStreaming</em> on BLEU and COMET, we still consider <em class="ltx_emph ltx_font_italic" id="S4.SS3.8.21.1.3">SeamlessStreaming</em> is a better system for zh-en translation based on VIP.</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section class="ltx_subsection ltx_figure_panel" id="S4.SS4" style="background-color:#F2F2FF;">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Latency</h3>
<div class="ltx_para ltx_noindent" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p1.1.1">Evaluation Metrics.</span> Due to the differences of grammatical structures between languages, a delay in simultaneous interpretation is inevitable. In this paper, we adopt the widely-used Average Lagging (AL) <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib45" title="">ma-etal-2019-stacl </a></cite>, Length Adaptive Average Lagging (LAAL) <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib52" title="">papi2022over </a></cite> for comparing the latency of different methods.
To achieve a fair comparison with systems that rewrite the translation, we calculate the time of the definite translation of these systems.
We also propose an additional metric, First Letter Appearance Lagging (FLAL), to reflect user experience on each system.
FLAL represents the time that each system outputs the first determined translation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">Quantitative Results.</span> <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.T4" title="In 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a> compares the latency of our model with various systems in terms of AL, LAAL, and our proposed FLAL on the RealSI and CoVoST.
We find that the existing metrics AL and LAAL are not suitable latency measurements of paragraph-level SiST on RealSI.
When the results are significantly shorter or longer than the reference translation, AL and LAAL may be largely exaggerated, leading to unreliable high latency.
In these scenarios, FLAL is a more reliable and stable metric for all the systems.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">Besides the paragraph-level latency evaluation, we compare our approach with other systems on the sentence-level dataset CoVoST2 zh-en, where both AL and LAAL produce reasonable values and the results are shown on the right side of <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.T4" title="In 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>.
Since the commercial systems usually rewrite the translation, their latency is higher than the CLASI.
Compared with the fastest approach <em class="ltx_emph ltx_font_italic" id="S4.SS4.p3.1.1">SeamlessStreaming</em>, CLASI achieves comparable latency but much better translation quality.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.2" style="width:433.6pt;height:144.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(1.0pt,-0.3pt) scale(1.00452333346389,1.00452333346389) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.2.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.T4.2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.1.1.2.1">RealSI (zh-en)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.T4.2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.1.1.3.1">RealSI (en-zh)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.T4.2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.1.1.4.1">CoVoST2 (zh-en)</span></th>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.1.2.2.1">AL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.1.2.2.2">LAAL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.1.2.2.3">FLAL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.1.2.2.4">AL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.1.2.2.5">LAAL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.1.2.2.6">FLAL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.1.2.2.7">AL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.1.2.2.8">LAAL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.1.2.2.9">FLAL</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.2.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.2.1.3.1.1"><em class="ltx_emph ltx_font_italic" id="S4.T4.2.1.3.1.1.1">SeamlessStreaming</em></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.1.2">3.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.1.3">42.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.1.4">2.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.1.5">3.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.1.6">16.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.1.7">2.24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.1.8">2.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.1.9">2.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.3.1.10">4.03</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.1.4.2.1"><em class="ltx_emph ltx_font_italic" id="S4.T4.2.1.4.2.1.1">Commercial 1</em></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.4.2.2">2.10</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.4.2.3">13.22</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.4.2.4">3.27</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.4.2.5">4.53</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.4.2.6">20.71</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.4.2.7">1.88</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.4.2.8">3.05</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.4.2.9">3.26</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.4.2.10">4.01</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.1.5.3.1"><em class="ltx_emph ltx_font_italic" id="S4.T4.2.1.5.3.1.1">Commercial 2</em></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.5.3.2">2.92</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.5.3.3">4.30</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.5.3.4">5.90</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.5.3.5">1.05</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.5.3.6">8.02</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.5.3.7">12.42</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.5.3.8">2.65</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.5.3.9">2.88</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.5.3.10">3.82</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.1.6.4.1"><em class="ltx_emph ltx_font_italic" id="S4.T4.2.1.6.4.1.1">Commercial 3</em></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.6.4.2">12.31</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.6.4.3">12.65</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.6.4.4">15.70</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.6.4.5">8.45</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.6.4.6">15.81</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.6.4.7">9.68</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.6.4.8">3.67</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.6.4.9">3.86</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.6.4.10">6.14</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.1.7.5.1"><em class="ltx_emph ltx_font_italic" id="S4.T4.2.1.7.5.1.1">Commercial 4</em></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.7.5.2">26.59</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.7.5.3">27.17</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.7.5.4">6.62</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.7.5.5">16.94</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.7.5.6">24.47</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.7.5.7">5.73</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.7.5.8">3.53</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.7.5.9">3.71</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.7.5.10">6.20</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T4.2.1.8.6.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.T4.2.1.8.6.1.1">CLASI</em></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.1.8.6.2">2.17</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.1.8.6.3">6.34</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.1.8.6.4">4.20</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.1.8.6.5">0.34</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.1.8.6.6">3.17</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.1.8.6.7">6.00</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.1.8.6.8">2.63</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.1.8.6.9">2.83</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.2.1.8.6.10">5.02</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.3.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S4.T4.4.2" style="font-size:90%;">Comparison of latency between CLASI and baselines. AL and LAAL are standard metrics for measuring latency in sentence-level datasets. Even though AL and LAAL yield reliable results on the sentence-level CoVoST2 dataset, we argue that they are less effective for long speeches due to the complexity of long-speech translation.
Therefore, we propose First Letter Appearance Lagging (FLAL), representing the time that each system outputs the first determined translation.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p4.1.1">Discussion.</span> While existing works put a lot of emphasis on the latency-quality trade-off <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib35" title="">koshkin2024transllamallmbasedsimultaneoustranslation </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib53" title="">papi2022attention </a></cite>, human interpretation usually uses Ear-Voice-Span (EVS) to evaluate the lagging. EVS measures the average time from when the speaker finishes conveying a piece of information to when the audience hears the corresponding translation, which is similar to AL. The typical EVS of professional human interpreters usually ranges from 3 to 6 seconds <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib26" title="">gumul2007time </a></cite> to achieve high-quality translation.</p>
</div>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.1">Consequently, we perform user studies and argue that the latency is less important than the translation quality for a practical SiST system. In the recent IWSLT 2023 simultaneous track <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib65" title="">iwslt-2023-international </a></cite>, the ranking of models is also evaluated by the translation quality within certain latency constraints. We verify whether the latency of CLASI is acceptable to users through real-world user surveys. To the publication date of this paper, we collected 14 user surveys on zh-en direction, each user using CLASI for at least 30 minutes. Under the current latency performance shown in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.T4" title="In 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>, only 1/14 == 7% of them suggest that the latency significantly affects their user experiences while the rest think the improvement of translation quality outweighs the latency and overall output of CLASI largely helps them to understand the speech. Considering that the latency of CLASI is even lower than most of the commercial systems, We believe the latency of CLASI can be acceptable on most cases.</p>
</div>
<div class="ltx_para" id="S4.SS4.p6">
<p class="ltx_p" id="S4.SS4.p6.1">Current latency metrics are proposed on sentence-level SiST. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.T4" title="In 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>, such metrics may not be suitable latency measurements for paragraph-level.
As the importance of end-to-end evaluation for long speech keeps increasing, more refined metrics are required to measure the latency and provide a deeper insight into the systems.</p>
</div>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Supplementary Experiments</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">To ensure a comprehensive evaluation of CLASI, our model is further evaluated on four additional datasets, including BSTC (zh-en) <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib85" title="">zhang-etal-2021-bstc </a></cite>, CoVoST2 (zh-en) <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib75" title="">wang2020covost </a></cite>, MuST-C (en-zh) <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib15" title="">di2019must </a></cite>, and GigaST (en-zh) <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib83" title="">ye23b_interspeech </a></cite><span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>We use the subset from in <a class="ltx_ref ltx_href" href="https://github.com/SpeechTranslation/GigaS2S" title="">GigaS2S</a> for evaluation</span></span></span>.
<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.T5" title="In 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a> presents the results of the automatic evaluation metrics for both zh-en and en-zh.
Due to the high cost of human evaluation, we are not able to provide VIP for these four datasets.
We observe that our model achieves consistently better performance than the baseline models.
Even though our system achieves the best automatic evaluation results among all the compared systems,
we still would like to emphasize that such a sentence-level evaluation scheme might overestimate the performance of SiST systems.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T5.2" style="width:433.6pt;height:206.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-85.8pt,40.8pt) scale(0.716364288114482,0.716364288114482) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T5.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T5.2.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S4.T5.2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.1.1.2.1">BSTC zh-en</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S4.T5.2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.1.1.3.1">CoVoST2 zh-en</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.2.2.1">BLEU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.2.2.2">BLEURT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.2.2.3">COMET</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.2.2.4">AL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.2.2.5">LAAL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.2.2.6">FLAL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.2.2.7">BLEU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.2.2.8">BLEURT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.2.2.9">COMET</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.2.2.10">AL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.2.2.11">LAAL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.2.2.12">FLAL</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.2.1.3.3.1"><em class="ltx_emph ltx_font_italic" id="S4.T5.2.1.3.3.1.1">SeamlessStreaming</em></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.3.3.2">9.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.3.3.3">34.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.3.3.4">78.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.3.3.5">11.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.3.3.6">68.92</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.3.3.7">3.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.3.3.8">19.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.3.3.9">54.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.3.3.10">77.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.3.3.11">2.27</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.3.3.12">2.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.3.3.13">4.03</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.1.4.4.1"><em class="ltx_emph ltx_font_italic" id="S4.T5.2.1.4.4.1.1">Commercial 1</em></th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.4.4.2">14.1</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.4.4.3">32.0</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.4.4.4">73.0</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.4.4.5">9.01</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.4.4.6">16.73</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.4.4.7">13.95</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.4.4.8">17.6</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.4.4.9">47.6</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.4.4.10">69.3</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.4.4.11">3.05</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.4.4.12">3.26</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.4.4.13">4.01</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.1.5.5.1"><em class="ltx_emph ltx_font_italic" id="S4.T5.2.1.5.5.1.1">Commercial 2</em></th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.5.5.2">17.6</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.5.5.3">39.2</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.5.5.4">81.2</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.5.5.5">6.35</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.5.5.6">7.92</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.5.5.7">13.04</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.5.5.8"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.5.5.8.1">24.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.5.5.9">56.7</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.5.5.10">78.5</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.5.5.11">2.65</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.5.5.12">2.88</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.5.5.13">3.82</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.1.6.6.1"><em class="ltx_emph ltx_font_italic" id="S4.T5.2.1.6.6.1.1">Commercial 3</em></th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.6.6.2">21.5</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.6.6.3">41.6</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.6.6.4">83.7</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.6.6.5">12.88</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.6.6.6">13.63</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.6.6.7">22.55</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.6.6.8">24.2</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.6.6.9">54.1</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.6.6.10">75.9</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.6.6.11">3.67</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.6.6.12">3.86</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.6.6.13">6.14</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.1.7.7.1"><em class="ltx_emph ltx_font_italic" id="S4.T5.2.1.7.7.1.1">Commercial 4</em></th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.7.7.2">21.2</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.7.7.3">41.9</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.7.7.4">82.3</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.7.7.5">30.50</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.7.7.6">31.84</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.7.7.7">9.61</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.7.7.8">22.1</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.7.7.9">56.1</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.7.7.10">76.8</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.7.7.11">3.53</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.7.7.12">3.71</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.7.7.13">6.20</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.2.1.8.8.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.T5.2.1.8.8.1.1">CLASI</em></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.8.8.2"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.8.8.2.1">25.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.8.8.3"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.8.8.3.1">44.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.8.8.4"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.8.8.4.1">85.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.8.8.5">4.68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.8.8.6">9.03</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.8.8.7">13.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.8.8.8">24.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.8.8.9"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.8.8.9.1">56.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.8.8.10"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.8.8.10.1">81.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.8.8.11">2.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.8.8.12">2.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.8.8.13">5.02</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T5.2.1.9.9.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.9.9.1.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S4.T5.2.1.9.9.2"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.9.9.2.1">MuST-C en-zh</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S4.T5.2.1.9.9.3"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.9.9.3.1">GigaST en-zh</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.10.10.1">BLEU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.10.10.2">BLEURT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.10.10.3">COMET</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.10.10.4">AL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.10.10.5">LAAL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.10.10.6">FLAL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.10.10.7">BLEU</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.10.10.8">BLEURT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.10.10.9">COMET</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.10.10.10">AL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.10.10.11">LAAL</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.10.10.12">FLAL</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.2.1.11.11.1"><em class="ltx_emph ltx_font_italic" id="S4.T5.2.1.11.11.1.1">SeamlessStreaming</em></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.11.11.2">17.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.11.11.3">48.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.11.11.4">75.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.11.11.5">1.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.11.11.6">1.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.11.11.7">2.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.11.11.8">26.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.11.11.9">48.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.11.11.10">75.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.11.11.11">1.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.11.11.12">1.57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.11.11.13">2.16</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.1.12.12.1"><em class="ltx_emph ltx_font_italic" id="S4.T5.2.1.12.12.1.1">Commercial 1</em></th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.12.12.2">24.0</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.12.12.3">55.2</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.12.12.4">81.2</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.12.12.5">2.62</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.12.12.6">2.91</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.12.12.7">2.07</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.12.12.8">43.1</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.12.12.9">59.6</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.12.12.10">83.4</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.12.12.11">2.55</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.12.12.12">2.73</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.12.12.13">2.33</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.1.13.13.1"><em class="ltx_emph ltx_font_italic" id="S4.T5.2.1.13.13.1.1">Commercial 2</em></th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.13.13.2"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.13.13.2.1">28.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.13.13.3">59.5</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.13.13.4">83.1</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.13.13.5">3.25</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.13.13.6">3.51</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.13.13.7">4.84</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.13.13.8">45.7</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.13.13.9">63.2</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.13.13.10">85.0</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.13.13.11">3.13</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.13.13.12">3.28</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.13.13.13">5.12</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.1.14.14.1"><em class="ltx_emph ltx_font_italic" id="S4.T5.2.1.14.14.1.1">Commercial 3</em></th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.14.14.2">26.9</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.14.14.3">59.9</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.14.14.4">83.7</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.14.14.5">3.59</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.14.14.6">3.90</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.14.14.7">4.86</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.14.14.8">48.3</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.14.14.9">66.2</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.14.14.10">86.7</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.14.14.11">3.18</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.14.14.12">3.36</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.14.14.13">4.97</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.1.15.15.1"><em class="ltx_emph ltx_font_italic" id="S4.T5.2.1.15.15.1.1">Commercial 4</em></th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.15.15.2">27.3</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.15.15.3">60.0</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.15.15.4">83.4</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.15.15.5">3.25</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.15.15.6">3.54</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.15.15.7">4.86</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.15.15.8">43.3</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.15.15.9">59.9</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.15.15.10">83.5</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.15.15.11">3.06</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.15.15.12">3.23</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.15.15.13">5.00</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T5.2.1.16.16.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.T5.2.1.16.16.1.1">CLASI</em></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.2.1.16.16.2">26.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.2.1.16.16.3"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.16.16.3.1">61.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.2.1.16.16.4"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.16.16.4.1">85.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.2.1.16.16.5">3.76</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.2.1.16.16.6">3.90</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.2.1.16.16.7">4.97</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.2.1.16.16.8"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.16.16.8.1">50.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.2.1.16.16.9"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.16.16.9.1">69.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.2.1.16.16.10"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.16.16.10.1">88.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.2.1.16.16.11">3.30</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.2.1.16.16.12">3.40</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.2.1.16.16.13">5.01</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T5.3.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S4.T5.4.2" style="font-size:90%;">Comparisons of CLASI and baselines on paragraph-level (BSTC) and sentence-level (CoVoST2, MuST-C, and GigaST) zh-en and en-zh datasets in terms of automatic evaluation metrics. We would like to emphasize that sentence-level evaluation schemes by automatic metrics cannot truly reflect the models’ performance. VIP in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.SS3" title="4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3</span></a> is a better metrics for comparing different systems.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>MM-RAG Performance</h3>
<section class="ltx_subsubsection" id="S4.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.6.1 </span>Retriever</h4>
<div class="ltx_para" id="S4.SS6.SSS1.p1">
<p class="ltx_p" id="S4.SS6.SSS1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.T6" title="Table 6 ‣ 4.6.1 Retriever ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">6</span></a> presents the performance of various retrieve models on the development set of our proprietary dataset. Each sample in the test set includes a short audio chunk and the mentioned terms in the audio.
Our MM-RAG retriever outperforms other open-source models by a large margin, achieving 91.3 % vs. 26.0% for Top-10 retrieve accuracy.
We compare two types of methodologies: audio-to-audio and audio-to-text.
In the audio-to-audio approach, a Text-to-Speech (TTS) model is utilized to convert the text keys from the external knowledge database into audio format,
forming a database with audio-based keys.
The audio keys and the user-input audio are then encoded with the ASR model to produce the corresponding representations.
The Top-<math alttext="k" class="ltx_Math" display="inline" id="S4.SS6.SSS1.p1.1.m1.1"><semantics id="S4.SS6.SSS1.p1.1.m1.1a"><mi id="S4.SS6.SSS1.p1.1.m1.1.1" mathbackground="#F2F2FF" xref="S4.SS6.SSS1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS1.p1.1.m1.1b"><ci id="S4.SS6.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS6.SSS1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.SSS1.p1.1.m1.1d">italic_k</annotation></semantics></math> retrieved items are subsequently determined using the Maximum Inner Product Search (MIPS) algorithm.
For audio-to-text approach, we compare MM-RAG with CLAP <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib17" title="">Elizalde2023CLAPLA </a></cite>. As indicated in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.T6" title="Table 6 ‣ 4.6.1 Retriever ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">6</span></a>, the effectiveness of these models remains significantly below acceptable standards, and MM-RAG significantly outperforms them.</p>
</div>
<div class="ltx_para" id="S4.SS6.SSS1.p2">
<p class="ltx_p" id="S4.SS6.SSS1.p2.1">It is worth noting that the same audio encoder employed in our CLASI is utilized for generating audio embedding in the MM-RAG retriever.
Such a design ensures that the integration
brings minimal computational latency to the overall framework.</p>
</div>
<figure class="ltx_table" id="S4.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T6.4" style="width:346.9pt;height:78.2pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-108.2pt,24.2pt) scale(0.615884061256433,0.615884061256433) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T6.4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T6.4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.1.2.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.1.3.1">Finetuned</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.1.4.1">Top-1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.4.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.1.5.1">Top-5</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.4.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.1.1.6.1">Top-10</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.4.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T6.4.1.2.2.1">
<em class="ltx_emph ltx_font_italic" id="S4.T6.4.1.2.2.1.1">CLAP</em> <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib17" title="">Elizalde2023CLAPLA </a></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.4.1.2.2.2">Audio-to-Audio</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.4.1.2.2.3">No</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.4.1.2.2.4">2.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.4.1.2.2.5">7.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.4.1.2.2.6">13.8</td>
</tr>
<tr class="ltx_tr" id="S4.T6.4.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.4.1.3.3.1">
<em class="ltx_emph ltx_font_italic" id="S4.T6.4.1.3.3.1.1">Wav2Clip</em> <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib80" title="">Wu2021Wav2CLIPLR </a></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T6.4.1.3.3.2">Audio-to-Audio</td>
<td class="ltx_td ltx_align_center" id="S4.T6.4.1.3.3.3">No</td>
<td class="ltx_td ltx_align_center" id="S4.T6.4.1.3.3.4">3.3</td>
<td class="ltx_td ltx_align_center" id="S4.T6.4.1.3.3.5">9.6</td>
<td class="ltx_td ltx_align_center" id="S4.T6.4.1.3.3.6">16.3</td>
</tr>
<tr class="ltx_tr" id="S4.T6.4.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.4.1.4.4.1">
<em class="ltx_emph ltx_font_italic" id="S4.T6.4.1.4.4.1.1">Whisper</em> <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib60" title="">Radford2022RobustSR </a></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T6.4.1.4.4.2">Audio-to-Audio</td>
<td class="ltx_td ltx_align_center" id="S4.T6.4.1.4.4.3">No</td>
<td class="ltx_td ltx_align_center" id="S4.T6.4.1.4.4.4">2.6</td>
<td class="ltx_td ltx_align_center" id="S4.T6.4.1.4.4.5">9.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.4.1.4.4.6">15.1</td>
</tr>
<tr class="ltx_tr" id="S4.T6.4.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.4.1.5.5.1"><em class="ltx_emph ltx_font_italic" id="S4.T6.4.1.5.5.1.1">In-house ASR</em></th>
<td class="ltx_td ltx_align_center" id="S4.T6.4.1.5.5.2">Audio-to-Audio</td>
<td class="ltx_td ltx_align_center" id="S4.T6.4.1.5.5.3">No</td>
<td class="ltx_td ltx_align_center" id="S4.T6.4.1.5.5.4">7.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.4.1.5.5.5">19.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.4.1.5.5.6">26.0</td>
</tr>
<tr class="ltx_tr" id="S4.T6.4.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T6.4.1.6.6.1">
<em class="ltx_emph ltx_font_italic" id="S4.T6.4.1.6.6.1.1">CLAP</em> <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib17" title="">Elizalde2023CLAPLA </a></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.4.1.6.6.2">Audio-to-Text</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.4.1.6.6.3">No</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.4.1.6.6.4">2.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.4.1.6.6.5">6.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.4.1.6.6.6">10.8</td>
</tr>
<tr class="ltx_tr" id="S4.T6.4.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T6.4.1.7.7.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.T6.4.1.7.7.1.1">MM-RAG (Ours)</em></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.4.1.7.7.2">Audio-to-Text</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.4.1.7.7.3">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.4.1.7.7.4"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.7.7.4.1">63.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.4.1.7.7.5"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.7.7.5.1">88.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.4.1.7.7.6"><span class="ltx_text ltx_font_bold" id="S4.T6.4.1.7.7.6.1">91.3</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T6.5.2.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S4.T6.2.1" style="font-size:90%;">Top-<math alttext="k" class="ltx_Math" display="inline" id="S4.T6.2.1.m1.1"><semantics id="S4.T6.2.1.m1.1b"><mi id="S4.T6.2.1.m1.1.1" mathbackground="#F2F2FF" xref="S4.T6.2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.T6.2.1.m1.1c"><ci id="S4.T6.2.1.m1.1.1.cmml" xref="S4.T6.2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.1.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S4.T6.2.1.m1.1e">italic_k</annotation></semantics></math> retrieve accuracy (%).</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.6.2 </span>ICL Performance</h4>
<div class="ltx_para" id="S4.SS6.SSS2.p1">
<p class="ltx_p" id="S4.SS6.SSS2.p1.1">By incorporating the retrieved terms from the external knowledge database as contextual information, our model’s in-context learning ability significantly improves the performance of speech translation for in-domain terminologies. <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.T7" title="In 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7</span></a> compares our method with the widely-used shallow fusion for intervention in the generated conclusion. When calculating the Recall, we input 1 ground-truth keyword with 9 similar negative words as context. When calculating the false positive rate for precision, we input 10 similar negative words as context. ICL is able to achieve a high recall rate with good precision, obtaining the highest F1 while shallow fusion only gets a recall rate that is only half of ICL.</p>
</div>
<div class="ltx_para" id="S4.SS6.SSS2.p2">
<p class="ltx_p" id="S4.SS6.SSS2.p2.1">Additionally, we conduct an ablation study on our MM-RAG module within terminology-intensive scenarios incorporating the whole <span class="ltx_text ltx_font_typewriter" id="S4.SS6.SSS2.p2.1.1">&lt;RETRIEVE&gt;</span> pipeline.
The incorporation of the external knowledge database results in a significant increment in the VIP score by about 10%, highlighting the effectiveness of our proposed MM-RAG.</p>
</div>
<figure class="ltx_table" id="S4.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T7.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T7.2.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T7.2.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T7.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T7.2.1.1.2.1">Recall (%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T7.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T7.2.1.1.3.1">Precision (%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T7.2.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T7.2.1.1.4.1">F1 (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T7.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T7.2.2.1.1">Shallow-Fusion</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.2.2.1.2">40.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.2.2.1.3">94.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.2.2.1.4">56.9</td>
</tr>
<tr class="ltx_tr" id="S4.T7.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T7.2.3.2.1">ICL + Shallow Fusion</th>
<td class="ltx_td ltx_align_center" id="S4.T7.2.3.2.2">79.2</td>
<td class="ltx_td ltx_align_center" id="S4.T7.2.3.2.3">73.4</td>
<td class="ltx_td ltx_align_center" id="S4.T7.2.3.2.4">76.2</td>
</tr>
<tr class="ltx_tr" id="S4.T7.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T7.2.4.3.1">ICL</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.2.4.3.2">79.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.2.4.3.3">86.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.2.4.3.4"><span class="ltx_text ltx_font_bold" id="S4.T7.2.4.3.4.1">82.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T7.3.1.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="S4.T7.4.2" style="font-size:90%;">Recall and Precision of ICL and shallow-fusion for the intervention of the keywords. When calculating the Recall, we input 1 ground-truth keyword with 9 similar negative words as context. When calculating the false positive rate for precision, we input 10 similar negative words as context. </span></figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Case Study</h3>
<div class="ltx_para" id="S4.SS7.p1">
<p class="ltx_p" id="S4.SS7.p1.1">We present case studies to show the ability of CLASI in translating complicated speech for zh-en and en-zh in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.T8" title="In 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">8</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.T9" title="In 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">9</span></a>. We choose one of the most-performed cascaded systems Commerical 4 for comparison. The Commerical 4 adopted a cascaded approach for SiST and it is shown in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#S4.SS3" title="4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3</span></a> to be one of the best previous SiST systems. Detailed explanations are described in the tables. For zh-en direction, we present cases regarding robustness to recognition errors, reasoning ability, and trending words translation. For the en-zh direction, we present cases regarding native, expressive, and accurate terminology translations. More cases are shown in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A2.T11" title="In B.1 Supplementary Case Study ‣ Appendix B Supplementary Materials ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T8.2" style="width:433.6pt;height:489.1pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-10.2pt,11.5pt) scale(0.955034240825323,0.955034240825323) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T8.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T8.2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="2" id="S4.T8.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T8.2.1.1.1.1.1">CASE 1: Robustness to recognition errors</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T8.2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.2.1.1">Golden Transcription</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.2.1.2.1">
<span class="ltx_p" id="S4.T8.2.1.2.1.2.1.1" style="width:313.0pt;">欧文两罚命中，四分<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.2.1.2.1.1.1">分差<sup class="ltx_sup" id="S4.T8.2.1.2.1.2.1.1.1.1">1</sup></span>，<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.2.1.2.1.1.2">不到最后<sup class="ltx_sup" id="S4.T8.2.1.2.1.2.1.1.2.1">2</sup></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.3.2.1">Commerical 4 ASR</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.3.2.2.1">
<span class="ltx_p" id="S4.T8.2.1.3.2.2.1.1" style="width:313.0pt;">欧文两罚命中，四分<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.3.2.2.1.1.1">分叉<sup class="ltx_sup" id="S4.T8.2.1.3.2.2.1.1.1.1">1</sup></span>，<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.3.2.2.1.1.2">不到最后<sup class="ltx_sup" id="S4.T8.2.1.3.2.2.1.1.2.1">2</sup></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.4.3.1">Commerical 4 Translation</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.4.3.2.1">
<span class="ltx_p" id="S4.T8.2.1.4.3.2.1.1" style="width:313.0pt;">Irving hit two free throws and <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.4.3.2.1.1.1">split<sup class="ltx_sup" id="S4.T8.2.1.4.3.2.1.1.1.1">1</sup></span> the four-point spread <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.4.3.2.1.1.2">to the end,<sup class="ltx_sup" id="S4.T8.2.1.4.3.2.1.1.2.1">2</sup></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.5.4.1">CLASI ASR</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.5.4.2.1">
<span class="ltx_p" id="S4.T8.2.1.5.4.2.1.1" style="width:313.0pt;">欧文两罚命中，四分<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.5.4.2.1.1.1">分叉<sup class="ltx_sup" id="S4.T8.2.1.5.4.2.1.1.1.1">1</sup></span>，<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.5.4.2.1.1.2">不到最后<sup class="ltx_sup" id="S4.T8.2.1.5.4.2.1.1.2.1">2</sup></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.6.5.1">CLASI Translation</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.6.5.2.1">
<span class="ltx_p" id="S4.T8.2.1.6.5.2.1.1" style="width:313.0pt;">Kyrie makes both free throws, <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.6.5.2.1.1.1">a four-point gap<sup class="ltx_sup" id="S4.T8.2.1.6.5.2.1.1.1.1">1</sup></span>,</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.7.6.1">Explanation</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.7.6.2.1">
<span class="ltx_p" id="S4.T8.2.1.7.6.2.1.1" style="width:313.0pt;">The word <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.7.6.2.1.1.1">分差<sup class="ltx_sup" id="S4.T8.2.1.7.6.2.1.1.1.1">1</sup></span> is mis-transcripted to <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.7.6.2.1.1.2">分叉<sup class="ltx_sup" id="S4.T8.2.1.7.6.2.1.1.2.1">1</sup></span>, which actually means “branch” or “split” in English. CLASI still generates the correct translation. After only hearing <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.7.6.2.1.1.3">不到最后<sup class="ltx_sup" id="S4.T8.2.1.7.6.2.1.1.3.1">2</sup></span>, CLASI decides not to translate immediately and leaves <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.7.6.2.1.1.4">不到最后<sup class="ltx_sup" id="S4.T8.2.1.7.6.2.1.1.4.1">2</sup></span> to the next translation because of lacking context. While Commerical 4 translates it incorrectly.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="2" id="S4.T8.2.1.8.7.1"><span class="ltx_text ltx_font_bold" id="S4.T8.2.1.8.7.1.1">CASE 2: Reasoning Ability for Translation</span></th>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.9.8.1">Golden Transcription</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.9.8.2.1">
<span class="ltx_p" id="S4.T8.2.1.9.8.2.1.1" style="width:313.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.9.8.2.1.1.1">绍兴二十年<sup class="ltx_sup" id="S4.T8.2.1.9.8.2.1.1.1.1">1</sup></span>担任右正言，弹劾胡寅</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.10.9.1">Commerical 4 ASR</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.10.9.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.10.9.2.1">
<span class="ltx_p" id="S4.T8.2.1.10.9.2.1.1" style="width:313.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.10.9.2.1.1.1">绍兴二十年<sup class="ltx_sup" id="S4.T8.2.1.10.9.2.1.1.1.1">1</sup></span>担任佑正言，弹劾胡莹。</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.11.10.1">Commerical 4 Translation</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.11.10.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.11.10.2.1">
<span class="ltx_p" id="S4.T8.2.1.11.10.2.1.1" style="width:313.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.11.10.2.1.1.1">Shaoxing twenty years<sup class="ltx_sup" id="S4.T8.2.1.11.10.2.1.1.1.1">1</sup></span> as YouZhengYan, impeach Hu Ying.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.12.11.1">CLASI ASR</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.12.11.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.12.11.2.1">
<span class="ltx_p" id="S4.T8.2.1.12.11.2.1.1" style="width:313.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.12.11.2.1.1.1">绍兴二十年<sup class="ltx_sup" id="S4.T8.2.1.12.11.2.1.1.1.1">1</sup></span>担任右正言，弹劾胡寅</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.13.12.1">CLASI Translation</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.13.12.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.13.12.2.1">
<span class="ltx_p" id="S4.T8.2.1.13.12.2.1.1" style="width:313.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.13.12.2.1.1.1">In 1150, during the 20th year of Emperor Gaozong’s Shaoxing era<sup class="ltx_sup" id="S4.T8.2.1.13.12.2.1.1.1.1">1</sup></span>, he served as the Right Censor and impeached Hu Ying</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.14.13.1">Explanation</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.14.13.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.14.13.2.1">
<span class="ltx_p" id="S4.T8.2.1.14.13.2.1.1" style="width:313.0pt;">Literally, <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.14.13.2.1.1.1">绍兴二十年<sup class="ltx_sup" id="S4.T8.2.1.14.13.2.1.1.1.1">1</sup></span> could be translated as “Shaoxing 20th Year”, while CLASI could understand the actual year of <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.14.13.2.1.1.2">绍兴二十年<sup class="ltx_sup" id="S4.T8.2.1.14.13.2.1.1.2.1">1</sup></span> is AD 1150, the 20th year under the reign of Emperor Gaozong.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.15.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="2" id="S4.T8.2.1.15.14.1"><span class="ltx_text ltx_font_bold" id="S4.T8.2.1.15.14.1.1">CASE 3：Trending words or slangs</span></th>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.16.15.1">Golden Transcription</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.16.15.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.16.15.2.1">
<span class="ltx_p" id="S4.T8.2.1.16.15.2.1.1" style="width:313.0pt;">我们常说，你们也太<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.16.15.2.1.1.1">卷<sup class="ltx_sup" id="S4.T8.2.1.16.15.2.1.1.1.1">1</sup></span>了吧，别卷了，还是<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.16.15.2.1.1.2">躺平<sup class="ltx_sup" id="S4.T8.2.1.16.15.2.1.1.2.1">2</sup></span>舒服。</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.17.16.1">Commerical 4 ASR</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.17.16.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.17.16.2.1">
<span class="ltx_p" id="S4.T8.2.1.17.16.2.1.1" style="width:313.0pt;">我们常说，你们也太<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.17.16.2.1.1.1">卷<sup class="ltx_sup" id="S4.T8.2.1.17.16.2.1.1.1.1">1</sup></span>了吧，别卷了，还是<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.17.16.2.1.1.2">躺平<sup class="ltx_sup" id="S4.T8.2.1.17.16.2.1.1.2.1">2</sup></span>舒服。</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.18.17.1">Commerical 4 Translation</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.18.17.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.18.17.2.1">
<span class="ltx_p" id="S4.T8.2.1.18.17.2.1.1" style="width:313.0pt;">We often say that you are <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.18.17.2.1.1.1">too curly<sup class="ltx_sup" id="S4.T8.2.1.18.17.2.1.1.1.1">1</sup></span>, don’t curl up, or <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.18.17.2.1.1.2">lie down<sup class="ltx_sup" id="S4.T8.2.1.18.17.2.1.1.2.1">2</sup></span> comfortably.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.19.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.19.18.1">CLASI ASR</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.19.18.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.19.18.2.1">
<span class="ltx_p" id="S4.T8.2.1.19.18.2.1.1" style="width:313.0pt;">我们常说，你们也太<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.19.18.2.1.1.1">卷<sup class="ltx_sup" id="S4.T8.2.1.19.18.2.1.1.1.1">1</sup></span>了吧，别卷了，还是<span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.19.18.2.1.1.2">躺平<sup class="ltx_sup" id="S4.T8.2.1.19.18.2.1.1.2.1">2</sup></span>舒服。</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.20.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.2.1.20.19.1">CLASI translation</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T8.2.1.20.19.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.20.19.2.1">
<span class="ltx_p" id="S4.T8.2.1.20.19.2.1.1" style="width:313.0pt;">We often say, “You are <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.20.19.2.1.1.1">too competitive<sup class="ltx_sup" id="S4.T8.2.1.20.19.2.1.1.1.1">1</sup></span>. Stop it. It’s more comfortable to <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.20.19.2.1.1.2">lie flat<sup class="ltx_sup" id="S4.T8.2.1.20.19.2.1.1.2.1">2</sup></span>.”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.21.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.T8.2.1.21.20.1">Explanation</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S4.T8.2.1.21.20.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T8.2.1.21.20.2.1">
<span class="ltx_p" id="S4.T8.2.1.21.20.2.1.1" style="width:313.0pt;">Although in Chinese <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.21.20.2.1.1.1">卷<sup class="ltx_sup" id="S4.T8.2.1.21.20.2.1.1.1.1">1</sup></span> could be translated to “curly” in some cases, it actually means “involution” in this context. CLASI translates it to “competitive”, which is acceptable. <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.21.20.2.1.1.2">Lie flat<sup class="ltx_sup" id="S4.T8.2.1.21.20.2.1.1.2.1">2</sup></span> is comparable to <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.21.20.2.1.1.3">lie down<sup class="ltx_sup" id="S4.T8.2.1.21.20.2.1.1.3.1">2</sup></span> for translating <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T8.2.1.21.20.2.1.1.4">躺平<sup class="ltx_sup" id="S4.T8.2.1.21.20.2.1.1.4.1">2</sup></span>, but the whole sentence is translated more naturally by CLASI.</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T8.3.1.1" style="font-size:90%;">Table 8</span>: </span><span class="ltx_text" id="S4.T8.4.2" style="font-size:90%;">Comparison between CLASI and Commerical 4 for zh-en direction.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T9">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T9.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T9.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="2" id="S4.T9.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T9.2.1.1.1.1" style="font-size:90%;">CASE 1: Native and Accurate Translation</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T9.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.2.1.1"><span class="ltx_text" id="S4.T9.2.2.1.1.1" style="font-size:90%;">Golden Transcription</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.2.1.2.1">
<span class="ltx_p" id="S4.T9.2.2.1.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.2.1.2.1.1.1" style="font-size:90%;">You can’t think of it on a </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.2.1.2.1.1.2" style="font-size:90%;">case-by-case<sup class="ltx_sup" id="S4.T9.2.2.1.2.1.1.2.1">1</sup></span><span class="ltx_text" id="S4.T9.2.2.1.2.1.1.3" style="font-size:90%;"> basis. Either we all have rights or </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.2.1.2.1.1.4" style="font-size:90%;">not have rights. Right<sup class="ltx_sup" id="S4.T9.2.2.1.2.1.1.4.1">2</sup></span><span class="ltx_text" id="S4.T9.2.2.1.2.1.1.5" style="font-size:90%;">.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.3.2.1"><span class="ltx_text" id="S4.T9.2.3.2.1.1" style="font-size:90%;">Commerical 4 ASR</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.3.2.2.1">
<span class="ltx_p" id="S4.T9.2.3.2.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.3.2.2.1.1.1" style="font-size:90%;">You can’t think of it on a </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.3.2.2.1.1.2" style="font-size:90%;">case-by-case<sup class="ltx_sup" id="S4.T9.2.3.2.2.1.1.2.1">1</sup></span><span class="ltx_text" id="S4.T9.2.3.2.2.1.1.3" style="font-size:90%;"> basis. Either we all have rights or </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.3.2.2.1.1.4" style="font-size:90%;">nut have rights. Right<sup class="ltx_sup" id="S4.T9.2.3.2.2.1.1.4.1">2</sup></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.4.3.1"><span class="ltx_text" id="S4.T9.2.4.3.1.1" style="font-size:90%;">Commerical 4 Translation</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.4.3.2.1">
<span class="ltx_p" id="S4.T9.2.4.3.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.4.3.2.1.1.1" style="font-size:90%;">你不能</span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.4.3.2.1.1.2" style="font-size:90%;">根据具体情况<sup class="ltx_sup" id="S4.T9.2.4.3.2.1.1.2.1">1</sup></span><span class="ltx_text" id="S4.T9.2.4.3.2.1.1.3" style="font-size:90%;">来考虑。要么我们都有权利，要么</span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.4.3.2.1.1.4" style="font-size:90%;">疯子都有权利<sup class="ltx_sup" id="S4.T9.2.4.3.2.1.1.4.1">2</sup></span><span class="ltx_text" id="S4.T9.2.4.3.2.1.1.5" style="font-size:90%;">，对吧？</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.5.4.1"><span class="ltx_text" id="S4.T9.2.5.4.1.1" style="font-size:90%;">CLASI ASR</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.5.4.2.1">
<span class="ltx_p" id="S4.T9.2.5.4.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.5.4.2.1.1.1" style="font-size:90%;">You can’t think of it on a </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.5.4.2.1.1.2" style="font-size:90%;">case-by-case<sup class="ltx_sup" id="S4.T9.2.5.4.2.1.1.2.1">1</sup></span><span class="ltx_text" id="S4.T9.2.5.4.2.1.1.3" style="font-size:90%;"> basis. Either we all have rights or </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.5.4.2.1.1.4" style="font-size:90%;">not have rights. Right<sup class="ltx_sup" id="S4.T9.2.5.4.2.1.1.4.1">2</sup></span><span class="ltx_text" id="S4.T9.2.5.4.2.1.1.5" style="font-size:90%;">.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.6.5.1"><span class="ltx_text" id="S4.T9.2.6.5.1.1" style="font-size:90%;">CLASI Translation</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.6.5.2.1">
<span class="ltx_p" id="S4.T9.2.6.5.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.6.5.2.1.1.1" style="font-size:90%;">你不能</span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.6.5.2.1.1.2" style="font-size:90%;">就事论事<sup class="ltx_sup" id="S4.T9.2.6.5.2.1.1.2.1">1</sup></span><span class="ltx_text" id="S4.T9.2.6.5.2.1.1.3" style="font-size:90%;">地考虑这个问题。 要么我们都有权利，要么我们</span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.6.5.2.1.1.4" style="font-size:90%;">都没有权利<sup class="ltx_sup" id="S4.T9.2.6.5.2.1.1.4.1">2</sup></span><span class="ltx_text" id="S4.T9.2.6.5.2.1.1.5" style="font-size:90%;">。</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.7.6.1"><span class="ltx_text" id="S4.T9.2.7.6.1.1" style="font-size:90%;">Explanation</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.7.6.2.1">
<span class="ltx_p" id="S4.T9.2.7.6.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.7.6.2.1.1.1" style="font-size:90%;">Although the Commerical 4 translation of </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.7.6.2.1.1.2" style="font-size:90%;">case-by-case<sup class="ltx_sup" id="S4.T9.2.7.6.2.1.1.2.1">1</sup></span><span class="ltx_text" id="S4.T9.2.7.6.2.1.1.3" style="font-size:90%;"> is correct, CLASI uses </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.7.6.2.1.1.4" style="font-size:90%;">就事论事<sup class="ltx_sup" id="S4.T9.2.7.6.2.1.1.4.1">1</sup></span><span class="ltx_text" id="S4.T9.2.7.6.2.1.1.5" style="font-size:90%;">, a well-known Chinese idiom, which is more native. Besides, Commerical 4 ASR mis-transcripted </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.7.6.2.1.1.6" style="font-size:90%;">not have rights<sup class="ltx_sup" id="S4.T9.2.7.6.2.1.1.6.1">2</sup></span><span class="ltx_text" id="S4.T9.2.7.6.2.1.1.7" style="font-size:90%;"> as </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.7.6.2.1.1.8" style="font-size:90%;">nut have rights<sup class="ltx_sup" id="S4.T9.2.7.6.2.1.1.8.1">2</sup></span><span class="ltx_text" id="S4.T9.2.7.6.2.1.1.9" style="font-size:90%;">, leading to a completely non-sense translation.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="2" id="S4.T9.2.8.7.1"><span class="ltx_text ltx_font_bold" id="S4.T9.2.8.7.1.1" style="font-size:90%;">CASE 2: Expressive Translation</span></th>
</tr>
<tr class="ltx_tr" id="S4.T9.2.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.9.8.1"><span class="ltx_text" id="S4.T9.2.9.8.1.1" style="font-size:90%;">Golden Transcription</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.9.8.2.1">
<span class="ltx_p" id="S4.T9.2.9.8.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.9.8.2.1.1.1" style="font-size:90%;">She was sobbing in fear that this test in a foreign language has been put in front of her.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.10.9.1"><span class="ltx_text" id="S4.T9.2.10.9.1.1" style="font-size:90%;">Commerical 4 ASR</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.10.9.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.10.9.2.1">
<span class="ltx_p" id="S4.T9.2.10.9.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.10.9.2.1.1.1" style="font-size:90%;">She was sobbing in fear that this test in a foreign language has been put in front of her.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.11.10.1"><span class="ltx_text" id="S4.T9.2.11.10.1.1" style="font-size:90%;">Commerical 4 Translation</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.11.10.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.11.10.2.1">
<span class="ltx_p" id="S4.T9.2.11.10.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.11.10.2.1.1.1" style="font-size:90%;">她哭了，害怕这个外语的测试摆在她面前，</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.12.11.1"><span class="ltx_text" id="S4.T9.2.12.11.1.1" style="font-size:90%;">CLASI ASR</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.12.11.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.12.11.2.1">
<span class="ltx_p" id="S4.T9.2.12.11.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.12.11.2.1.1.1" style="font-size:90%;">She was sobbing in fear that this test in a foreign language has been put in front of her.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.13.12.1"><span class="ltx_text" id="S4.T9.2.13.12.1.1" style="font-size:90%;">CLASI Translation</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.13.12.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.13.12.2.1">
<span class="ltx_p" id="S4.T9.2.13.12.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.13.12.2.1.1.1" style="font-size:90%;">她因为害怕这门外语考试而哭泣，</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.14.13.1"><span class="ltx_text" id="S4.T9.2.14.13.1.1" style="font-size:90%;">Explanation</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.14.13.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.14.13.2.1">
<span class="ltx_p" id="S4.T9.2.14.13.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.14.13.2.1.1.1" style="font-size:90%;">Theoretically, the Commerical 4 translation is correct literally. However, it’s not expressive for native Chinese speakers. CLASI translation is expressive, conveying the same meaning of the source English sentence, which means “She was sobbing because of fearing the foreign language test.”</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.15.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="2" id="S4.T9.2.15.14.1"><span class="ltx_text ltx_font_bold" id="S4.T9.2.15.14.1.1" style="font-size:90%;">CASE 3: Named Entity, Terminology Recognition and Translation</span></th>
</tr>
<tr class="ltx_tr" id="S4.T9.2.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.16.15.1"><span class="ltx_text" id="S4.T9.2.16.15.1.1" style="font-size:90%;">Golden Transcription</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.16.15.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.16.15.2.1">
<span class="ltx_p" id="S4.T9.2.16.15.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.16.15.2.1.1.1" style="font-size:90%;">So let’s let me put the </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.16.15.2.1.1.2" style="font-size:90%;">COVID-19<sup class="ltx_sup" id="S4.T9.2.16.15.2.1.1.2.1">1</sup></span><span class="ltx_text" id="S4.T9.2.16.15.2.1.1.3" style="font-size:90%;"> for example, so now we we know that there are a lot of people are infected and they have um positive antibody tests</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.17.16.1"><span class="ltx_text" id="S4.T9.2.17.16.1.1" style="font-size:90%;">Commerical 4 ASR</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.17.16.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.17.16.2.1">
<span class="ltx_p" id="S4.T9.2.17.16.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.17.16.2.1.1.1" style="font-size:90%;">So let’s let me put the </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.17.16.2.1.1.2" style="font-size:90%;">CUBA 19<sup class="ltx_sup" id="S4.T9.2.17.16.2.1.1.2.1">1</sup></span><span class="ltx_text" id="S4.T9.2.17.16.2.1.1.3" style="font-size:90%;"> for example, so now we we know that there are a lot of people are infected and they have um positive, </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.17.16.2.1.1.4" style="font-size:90%;">anybody? tests,<sup class="ltx_sup" id="S4.T9.2.17.16.2.1.1.4.1">2</sup></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.18.17.1"><span class="ltx_text" id="S4.T9.2.18.17.1.1" style="font-size:90%;">Commerical 4 Translation</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.18.17.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.18.17.2.1">
<span class="ltx_p" id="S4.T9.2.18.17.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.18.17.2.1.1.1" style="font-size:90%;">所以让我以</span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.18.17.2.1.1.2" style="font-size:90%;">古巴19人<sup class="ltx_sup" id="S4.T9.2.18.17.2.1.1.2.1">1</sup></span><span class="ltx_text" id="S4.T9.2.18.17.2.1.1.3" style="font-size:90%;">为例。所以现在我们知道有很多人被感染，他们是阳性的。</span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.18.17.2.1.1.4" style="font-size:90%;">有人吗？测试，<sup class="ltx_sup" id="S4.T9.2.18.17.2.1.1.4.1">2</sup></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.19.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.19.18.1"><span class="ltx_text" id="S4.T9.2.19.18.1.1" style="font-size:90%;">CLASI ASR</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.19.18.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.19.18.2.1">
<span class="ltx_p" id="S4.T9.2.19.18.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.19.18.2.1.1.1" style="font-size:90%;">So let’s let me put the </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.19.18.2.1.1.2" style="font-size:90%;">COVID nineteen<sup class="ltx_sup" id="S4.T9.2.19.18.2.1.1.2.1">1</sup></span><span class="ltx_text" id="S4.T9.2.19.18.2.1.1.3" style="font-size:90%;"> for example, so now we we know that there are a lot of people are infected and they have um </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.19.18.2.1.1.4" style="font-size:90%;">positive antibody tests.<sup class="ltx_sup" id="S4.T9.2.19.18.2.1.1.4.1">2</sup></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.20.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.2.20.19.1"><span class="ltx_text" id="S4.T9.2.20.19.1.1" style="font-size:90%;">CLASI Translation</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T9.2.20.19.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.20.19.2.1">
<span class="ltx_p" id="S4.T9.2.20.19.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.20.19.2.1.1.1" style="font-size:90%;">以</span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.20.19.2.1.1.2" style="font-size:90%;">Covid-19<sup class="ltx_sup" id="S4.T9.2.20.19.2.1.1.2.1">1</sup></span><span class="ltx_text" id="S4.T9.2.20.19.2.1.1.3" style="font-size:90%;">为例。 我们现在知道 有很多人被感染了， 并且他们的</span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.20.19.2.1.1.4" style="font-size:90%;">抗体检测呈阳性。<sup class="ltx_sup" id="S4.T9.2.20.19.2.1.1.4.1">2</sup></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.21.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.T9.2.21.20.1"><span class="ltx_text" id="S4.T9.2.21.20.1.1" style="font-size:90%;">Explanation</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S4.T9.2.21.20.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T9.2.21.20.2.1">
<span class="ltx_p" id="S4.T9.2.21.20.2.1.1" style="width:327.2pt;"><span class="ltx_text" id="S4.T9.2.21.20.2.1.1.1" style="font-size:90%;">Commerical 4 cannot correctly recognize </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.21.20.2.1.1.2" style="font-size:90%;">COVID-19<sup class="ltx_sup" id="S4.T9.2.21.20.2.1.1.2.1">1</sup></span><span class="ltx_text" id="S4.T9.2.21.20.2.1.1.3" style="font-size:90%;"> and </span><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.2.21.20.2.1.1.4" style="font-size:90%;">antibody tests<sup class="ltx_sup" id="S4.T9.2.21.20.2.1.1.4.1">2</sup></span><span class="ltx_text" id="S4.T9.2.21.20.2.1.1.5" style="font-size:90%;">, while CLASI successfully recognize and translate.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 9: </span>Comparison between CLASI and Commerical 4 for en-zh direction.</figcaption>
</figure>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Large language model.</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">The encoder-decoder architecture <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib56" title="">polak-etal-2023-towards </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib83" title="">ye23b_interspeech </a></cite> has been widely explored in early speech translation research, but with the advent of large language models <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib2" title="">achiam2023gpt </a></cite>, there has been a growing interest in employing decoder-only architectures <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib21" title="">fu2023decoderonlyencoderdecoderinterpretinglanguage </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib67" title="">seide2024speechreallmrealtime </a></cite> for sequence-to-sequence problems. While recent efforts have emerged in utilizing large language models for machine translation <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib36" title="">li2024mtpatcherselectiveextendableknowledge </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib37" title="">li2024eliciting </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib90" title="">zheng2024finetuninglargelanguagemodels </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib91" title="">zhu2024multilingual </a></cite> and speech translation <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib13" title="">chu2023qwenaudioadvancinguniversalaudio </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib30" title="">huang2023speechtranslationlargelanguage </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib70" title="">tang2024salmonngenerichearingabilities </a></cite>, the application of such models in simultaneous translation tasks remains limited. Although there has been early attempts to utilize LLM for simultaneous machine translation <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib3" title="">agostinelli2023simul </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib29" title="">hu2024gentranslate </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib35" title="">koshkin2024transllamallmbasedsimultaneoustranslation </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib86" title="">zhang2024streamspeech </a></cite>, to the best of our knowledge, no existing work has been found that explores the utilization of large language models for end-to-end simultaneous speech translation with such remarkable improvement.</p>
</div>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p2.1">Furthermore, LLMs have demonstrated impressive capabilities in tasks such as instruction following <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib2" title="">achiam2023gpt </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib6" title="">bai2023qwen </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib20" title="">feng2024agilenovelframeworkllm </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib72" title="">touvron2023llama </a></cite>, reasoning <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib55" title="">paul2024refinerreasoningfeedbackintermediate </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib69" title="">shinn2023reflexionlanguageagentsverbal </a></cite>, and planning <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib59" title="">qiao2024autoactautomaticagentlearning </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib64" title="">ruan2023tptulargelanguagemodelbased </a></cite>. Recent research studies have leveraged prompt engineering to develop remarkable LLM agents that autonomously tackle complex tasks in diverse environments <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib76" title="">wang2024survey </a></cite>.
In our work, we empower the LLM to perform sequential instructions to accomplish the simulation speech translation task.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Simultaneous Speech Translation.</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">One of the important components of simultaneous speech translation is the segmentation strategy, which determines how the speech frames are fed to the models.
Different strategies could affect the latency and performance of the translation.
According to <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib41" title="">liu2024recentadvancesendtoendsimultaneous </a></cite>, segmentation strategies can be classified into fixed-length, word-based, and adaptive segmentation. Fixed-length strategies <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib50" title="">nguyen2021empiricalstudyendtoendsimultaneous </a></cite> divide the speech into equally-length segments, while word-based strategies <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib46" title="">ma-etal-2020-simulmt </a></cite> identify word boundaries within the speech. Adaptive segmentation <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib16" title="">dong-etal-2022-learning </a></cite> detects boundaries for speech units. Among these categories, our method utilizes a fixed-length strategy.</p>
</div>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p2.1">Regarding the read/wait policy, the Wait-k method <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib45" title="">ma-etal-2019-stacl </a></cite> and its variants <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib50" title="">nguyen2021empiricalstudyendtoendsimultaneous </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib84" title="">zeng-etal-2021-realtrans </a></cite> have been extensively studied in the context of text translation and speech translation. In comparison to these approaches, which explicitly learn the generation of read/write signals, another line of research focus on how to leverage offline translation models <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib82" title="">yan-etal-2023-cmus </a></cite>.
When utilizing an offline translation model for simultaneous translation, it is important to address the stabilization of generated hypotheses to prevent excessive content refreshing experienced by the user.
<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib40" title="">liu2020lowlatencysequencetosequencespeechrecognition </a></cite> first proposed a local agreement policy to stabilize the partial hypothesis, while <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib57" title="">Polk2023 </a></cite> introduced an incremental blockwise beam-search algorithm. In contrast to these methods, we enforce our model to generate consistent hypotheses by constraining the prompt to the language model.</p>
</div>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p3">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p3.1">For the model architecture, there are two primary methods for implementing speech translation systems: cascaded solutions <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib27" title="">guo-etal-2023-hw </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib31" title="">iranzo2021streaming </a></cite> and end-to-end solutions <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib22" title="">fukuda-etal-2023-naist </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib51" title="">papi-etal-2023-direct </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib56" title="">polak-etal-2023-towards </a></cite>. Cascaded solutions involve separated ASR and MT components, while end-to-end solutions directly map speech to translations. Cascaded systems benefit from established techniques but suffer from latency and error propagation. End-to-end models offer real-time translation and improved quality through deep learning but require large-size training data.
In our work, we implement an end-to-end model which combines the capabilities of ASR, MT, and ST.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Human Evaluation.</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">In the realm of speech translation, the choice of evaluation metrics plays a crucial role in assessing the quality and effectiveness of translation systems. While automatic metrics, such as BLEU <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib54" title="">papineni2002bleu </a></cite>, BLEURT <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib68" title="">sellam2020bleurt </a></cite>, and COMET <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib61" title="">rei2020comet </a></cite>, have traditionally been relied upon for evaluation, there is a growing recognition that they may not be the most suitable or comprehensive measure of performance <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib48" title="">marie-etal-2021-scientific </a></cite>.
We observe that in more recent works <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib7" title="">barrault2023seamless </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib42" title="">liu2024chatqasurpassinggpt4conversational </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib71" title="">nllbteam2022languageleftbehindscaling </a>; <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib77" title="">wang-etal-2023-document-level </a></cite>, there is an increasing trend of evaluating systems using human assessments, particularly when LLM is employed in the work.
While human evaluation requires more resources and time compared to automatic metrics, its benefits outweigh the drawbacks. By incorporating human judgment, speech translation systems can be refined and optimized to align with user expectations, ensuring translations that are not only technically accurate but also linguistically and contextually appropriate.
In contrast to the existing human evaluation metrics, e.g.“continuous rating” <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib32" title="">javorsky-etal-2022-continuous </a></cite> and MQM (Multidimensional Quality Metrics) <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib43" title="">lommel2014multidimensional </a></cite>, we have taken inspiration from professional human interpreters <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib49" title="">moores2024nerle </a></cite> and propose to use VIP (Valid Information Proportion) as a human evaluation metric which precisely reflects the goal of the simultaneous translation task.</p>
</div>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we introduce <span class="ltx_text ltx_font_bold" id="S6.p1.1.1">C</span>ross <span class="ltx_text ltx_font_bold" id="S6.p1.1.2">L</span>anguage <span class="ltx_text ltx_font_bold" id="S6.p1.1.3">A</span>gent - <span class="ltx_text ltx_font_bold" id="S6.p1.1.4">S</span>imultaneous <span class="ltx_text ltx_font_bold" id="S6.p1.1.5">I</span>nterpretation, CLASI, an LLM agent to produce end-to-end simultaneous speech translation. Benefits from massive pretraining and imitation learning, CLASI achieves significantly better performances than state-of-the-art systems. Take the Chinese-to-English direction as an example, under strict and challenging human-evaluated metrics proposed by professional human interpreters, Valid Information Proportion (VIP), CLASI significantly outperforms baselines by a large margin. While all other systems obtain VIP by less than 40%, CLASI achieves a VIP of 81.3%, demonstrating human parity performance.
More specifically, we propose the following crucial components for the supreme performance of CLASI: (1) An encoder-conditioned LLM agent architecture that performs high-quality or even human-parity SiST process through simple actions. (2) Imitation learning from human interpreters for a natural read-write policy balances translation quality and latency in a data-driven manner, without complex human pre-designing. Under such policy, CLASI achieves a stable output scheme, where each output is deterministic, thus potentially better user experience than most commercial systems. (3) Motivated by the preparatory trajectory of human interpreters, CLASI could perform in-context learning from historical translations and external knowledge to provide sufficient information for translation.
With the powerful translation ability of CLASI, we believe it can further make cross-lingual communication seamless across different places all over the world.</p>
</div>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitation and Future Work</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Although we achieved significant improvements over commercial systems in Chinese-to-English and English-to-Chinese tasks, more languages should be considered in the future.
In our current implementation, CLASI performs a full action sequence for each translation round. Some of the actions, e.g., <span class="ltx_text ltx_font_typewriter" id="Sx1.p1.1.1">&lt;RETRIEVE&gt;</span> is optional for easy translation scenarios since the model is capable of translating correctly without the help of external knowledge. Training the model to better determine whether to skip unnecessary actions is a future direction.
For a product-level system, even though the latency of CLASI is acceptable in most cases, how to reduce the translation latency without lowering the translation quality is still interesting and potentially helpful for user experience.
Furthermore, we argue that the current automatic metrics are not comprehensive for SiST evaluation. Most of the quality measurements do not consider key information, which is crucial in SiST scenarios. As such, we proposed VIP for better human evaluation. Consequently, more reliable automatic quality and latency metrics should be proposed in the future as well.
Reinforcement learning from human feedback (RLHF) has been proven to be effective in enhancing LLM performance. Although CLASI achieves significantly superior results than previous state-of-the-art systems, further studies on how to build better multi-modal reward models and better RL methods for SiST is also an important direction. Incorporating more modalities, for example, end-to-end speech-to-speech generation, or even end-to-end video-to-video generation are also promising research topics.</p>
</div>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Social Impact</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">The powerful SiST system CLASI can be applied to various scenarios to facilitate cross-lingual communications. For example, it can be deployed to various conferences or daily meetings to help listeners understand speech in different languages. It can also be deployed as a system-level translation module to help users watch videos that are conveyed in different languages. For online gaming, it can also help to bridge the gap of cross-lingual communication and connect people speaking different languages. A powerful SiST system with human parity performance may significantly improve the efficiency of professional human interpreters.</p>
</div>
<div class="ltx_para" id="Sx2.p2">
<p class="ltx_p" id="Sx2.p2.1">Despite the huge positive social impact that CLASI may bring, every coin has two sides. Neglecting some low-resource languages may also bring unfairness to some minorities. Resolving these problems needs further cooperation from the society. We leave more languages supporting as our future work.</p>
</div>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Authorship and Acknowledgements</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">All contributors are listed in alphabetical order by last name. Corresponding to this work can be sent to any core authors’ email.</p>
</div>
<div class="ltx_para" id="Sx3.p2">
<p class="ltx_p" id="Sx3.p2.1"><span class="ltx_text ltx_font_bold" id="Sx3.p2.1.1">Core Authors.</span>
All core authors contributed equally to this work.</p>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<ul class="ltx_itemize" id="Sx3.I1">
<li class="ltx_item" id="Sx3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I1.i1.p1">
<p class="ltx_p" id="Sx3.I1.i1.p1.1">Shanbo Cheng</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I1.i2.p1">
<p class="ltx_p" id="Sx3.I1.i2.p1.1">Zhichao Huang</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I1.i3.p1">
<p class="ltx_p" id="Sx3.I1.i3.p1.1">Tom Ko</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I1.i4.p1">
<p class="ltx_p" id="Sx3.I1.i4.p1.1">Hang Li</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I1.i5.p1">
<p class="ltx_p" id="Sx3.I1.i5.p1.1">Ningxin Peng</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I1.i6.p1">
<p class="ltx_p" id="Sx3.I1.i6.p1.1">Lu Xu</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I1.i7.p1">
<p class="ltx_p" id="Sx3.I1.i7.p1.1">Qini Zhang</p>
<ul class="ltx_itemize" id="Sx3.I1.i7.I1">
<li class="ltx_item" id="Sx3.I1.i7.I1.i1" style="list-style-type:none;">
<div class="ltx_para" id="Sx3.I1.i7.I1.i1.p1">
<p class="ltx_p" id="Sx3.I1.i7.I1.i1.p1.1">chengshanbo@bytedance.com</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I1.i7.I1.i2" style="list-style-type:none;padding-top:2.5pt;">
<div class="ltx_para" id="Sx3.I1.i7.I1.i2.p1">
<p class="ltx_p" id="Sx3.I1.i7.I1.i2.p1.1">zhichao.huang@bytedance.com</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I1.i7.I1.i3" style="list-style-type:none;padding-top:2.5pt;">
<div class="ltx_para" id="Sx3.I1.i7.I1.i3.p1">
<p class="ltx_p" id="Sx3.I1.i7.I1.i3.p1.1">tom.ko@bytedance.com</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I1.i7.I1.i4" style="list-style-type:none;padding-top:2.5pt;">
<div class="ltx_para" id="Sx3.I1.i7.I1.i4.p1">
<p class="ltx_p" id="Sx3.I1.i7.I1.i4.p1.1">lihang.lh@bytedance.com</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I1.i7.I1.i5" style="list-style-type:none;padding-top:2.5pt;">
<div class="ltx_para" id="Sx3.I1.i7.I1.i5.p1">
<p class="ltx_p" id="Sx3.I1.i7.I1.i5.p1.1">pengningxin@bytedance.com</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I1.i7.I1.i6" style="list-style-type:none;padding-top:2.5pt;">
<div class="ltx_para" id="Sx3.I1.i7.I1.i6.p1">
<p class="ltx_p" id="Sx3.I1.i7.I1.i6.p1.1">xu.lu1@bytedance.com</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I1.i7.I1.i7" style="list-style-type:none;padding-top:2.5pt;">
<div class="ltx_para" id="Sx3.I1.i7.I1.i7.p1">
<p class="ltx_p" id="Sx3.I1.i7.I1.i7.p1.1">qini.z@bytedance.com</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</div>
<div class="ltx_para" id="Sx3.p3">
<p class="ltx_p" id="Sx3.p3.1"><span class="ltx_text ltx_font_bold" id="Sx3.p3.1.1">Labeling, Evaluation and Interpretation Team.</span>
Our data labeling and human evaluation team led by Yifu Li, made diligent efforts in all kinds of help needed, which is irreplaceable in the success of this project. Special thanks to the human interpreter team led by Anna Liu for providing insightful and comprehensive analysis on data labeling, human evaluation, and other recommendations.</p>
<ul class="ltx_itemize" id="Sx3.I2">
<li class="ltx_item" id="Sx3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I2.i1.p1">
<p class="ltx_p" id="Sx3.I2.i1.p1.1">Jingwen Chen</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I2.i2.p1">
<p class="ltx_p" id="Sx3.I2.i2.p1.1">Xiaoya Chen</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I2.i3.p1">
<p class="ltx_p" id="Sx3.I2.i3.p1.1">Yifu Li</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I2.i4.p1">
<p class="ltx_p" id="Sx3.I2.i4.p1.1">Huiying Lin</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I2.i5.p1">
<p class="ltx_p" id="Sx3.I2.i5.p1.1">Anna Liu</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Sx3.p4">
<p class="ltx_p" id="Sx3.p4.1"><span class="ltx_text ltx_font_bold" id="Sx3.p4.1.1">Engineering Team.</span>
We collaborated with our engineering team led by Tingshuai Yan, their infrastructure support is crucial for this project.</p>
<ul class="ltx_itemize" id="Sx3.I3">
<li class="ltx_item" id="Sx3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I3.i1.p1">
<p class="ltx_p" id="Sx3.I3.i1.p1.1">Weicheng Fu</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I3.i2.p1">
<p class="ltx_p" id="Sx3.I3.i2.p1.1">Tingshuai Yan</p>
</div>
</li>
<li class="ltx_item" id="Sx3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx3.I3.i3.p1">
<p class="ltx_p" id="Sx3.I3.i3.p1.1">Liehao Zou</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Sx3.p5">
<p class="ltx_p" id="Sx3.p5.1"><span class="ltx_text ltx_font_bold" id="Sx3.p5.1.1">Acknowledgements.</span> We appreciate the Speech Understanding team for all kinds of help, especially data sharing, thanks to their tremendous work for all the in-house data. We would like to express our deepest thanks to all the contributors to this project, their brilliant work guarantees the success of this project. We also want to thank Wenda Xu and Xi Xu for their suggestions on automatic evaluations.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Kendall Rank Correlation Coefficient</span>, pages 278–281.

</span>
<span class="ltx_bibblock">Springer New York, New York, NY, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2303.08774</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Victor Agostinelli, Max Wild, Matthew Raffel, Kazi Asif Fuad, and Lizhong Chen.

</span>
<span class="ltx_bibblock">Simul-llm: A framework for exploring high-quality simultaneous
translation with large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2312.04691</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Antonios Anastasopoulos, Ondřej Bojar, Jacob Bremerman, Roldano Cattoni,
Maha Elbayad, Marcello Federico, Xutai Ma, Satoshi Nakamura, Matteo Negri,
Jan Niehues, et al.

</span>
<span class="ltx_bibblock">Findings of the iwslt 2021 evaluation campaign.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 18th International Conference on Spoken
Language Translation (IWSLT 2021)</span>, pages 1–29, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael
Auli.

</span>
<span class="ltx_bibblock">Data2vec: A general framework for self-supervised learning in speech,
vision and language.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">International Conference on Machine Learning</span>, pages
1298–1312. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,
Wenbin Ge, Yu Han, Fei Huang, et al.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2309.16609</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Loïc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong,
Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin
Haaheim, et al.

</span>
<span class="ltx_bibblock">Seamless: Multilingual expressive and streaming speech translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2312.05187</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza
Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau,
Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick,
Roman Ring, T. W. Hennigan, Saffron Huang, Lorenzo Maggiore, Chris Jones,
Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals,
Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and L. Sifre.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">International Conference on Machine Learning</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin,
editors, <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Advances in Neural Information Processing Systems</span>, volume 33,
pages 1877–1901. Curran Associates, Inc., 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Agnieszka Chmiel.

</span>
<span class="ltx_bibblock">Effects of simultaneous interpreting experience and training on
anticipation, as measured by word-translation latencies.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Interpreting</span>, 23(1):18–44, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Kyunghyun Cho and Masha Esipova.

</span>
<span class="ltx_bibblock">Can neural machine translation do simultaneous translation?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:1606.02012</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang
Zhou, and Jingren Zhou.

</span>
<span class="ltx_bibblock">Qwen-audio: Advancing universal audio understanding via unified
large-scale audio-language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2311.07919</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang
Zhou, and Jingren Zhou.

</span>
<span class="ltx_bibblock">Qwen-audio: Advancing universal audio understanding via unified
large-scale audio-language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth
Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna.

</span>
<span class="ltx_bibblock">Fleurs: Few-shot learning evaluation of universal representations of
speech.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">2022 IEEE Spoken Language Technology Workshop (SLT)</span>, pages
798–805. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Mattia A Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco
Turchi.

</span>
<span class="ltx_bibblock">Must-c: a multilingual speech translation corpus.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</span>, pages 2012–2017.
Association for Computational Linguistics, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Qian Dong, Yaoming Zhu, Mingxuan Wang, and Lei Li.

</span>
<span class="ltx_bibblock">Learning when to translate for streaming speech.

</span>
<span class="ltx_bibblock">In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,
<span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</span>, pages 680–694, Dublin,
Ireland, May 2022. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang.

</span>
<span class="ltx_bibblock">Clap learning audio concepts from natural language supervision.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">ICASSP 2023 - 2023 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Marcello Federico, Alex Waibel, Marta R. Costa-jussà, Jan Niehues,
Sebastian Stuker, and Elizabeth Salesky, editors.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 18th International Conference on Spoken
Language Translation (IWSLT 2021)</span>, Bangkok, Thailand (online), August 2021.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Marcello Federico, Alex Waibel, Kevin Knight, Satoshi Nakamura, Hermann Ney,
Jan Niehues, Sebastian Stüker, Dekai Wu, Joseph Mariani, and Francois
Yvon, editors.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 17th International Conference on Spoken
Language Translation</span>, Online, July 2020. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Peiyuan Feng, Yichen He, Guanhua Huang, Yuan Lin, Hanchong Zhang, Yuchen Zhang,
and Hang Li.

</span>
<span class="ltx_bibblock">Agile: A novel framework of llm agents, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Zihao Fu, Wai Lam, Qian Yu, Anthony Man-Cho So, Shengding Hu, Zhiyuan Liu, and
Nigel Collier.

</span>
<span class="ltx_bibblock">Decoder-only or encoder-decoder? interpreting language model as a
regularized encoder-decoder, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Ryo Fukuda, Yuta Nishikawa, Yasumasa Kano, Yuka Ko, Tomoya Yanagita, Kosuke
Doi, Mana Makinae, Sakriani Sakti, Katsuhito Sudoh, and Satoshi Nakamura.

</span>
<span class="ltx_bibblock">NAIST simultaneous speech-to-speech translation system for IWSLT
2023.

</span>
<span class="ltx_bibblock">In Elizabeth Salesky, Marcello Federico, and Marine Carpuat, editors,
<span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 20th International Conference on Spoken Language
Translation (IWSLT 2023)</span>, pages 330–340, Toronto, Canada (in-person and
online), July 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Marco Gaido, Sara Papi, Matteo Negri, and Luisa Bentivogli.

</span>
<span class="ltx_bibblock">Speech translation with speech foundation models and large language
models: What is there and what is missing?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2402.12025</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor OK Li.

</span>
<span class="ltx_bibblock">Learning to translate in real-time with neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 15th Conference of the European Chapter of
the Association for Computational Linguistics: Volume 1, Long Papers</span>, pages
1053–1062, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu,
Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al.

</span>
<span class="ltx_bibblock">Conformer: Convolution-augmented transformer for speech recognition.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2005.08100</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Ewa Gumul and Andrzej Łyda.

</span>
<span class="ltx_bibblock">The time constraint in conference interpreting: Simultaneous vs.
consecutive.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Research in language</span>, 5:165–183, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Jiaxin Guo, Daimeng Wei, Zhanglin Wu, Zongyao Li, Zhiqiang Rao, Minghan Wang,
Hengchao Shang, Xiaoyu Chen, Zhengzhe Yu, Shaojun Li, Yuhao Xie, Lizhi Lei,
and Hao Yang.

</span>
<span class="ltx_bibblock">The HW-TSC’s simultaneous speech-to-text translation system for
IWSLT 2023 evaluation.

</span>
<span class="ltx_bibblock">In Elizabeth Salesky, Marcello Federico, and Marine Carpuat, editors,
<span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 20th International Conference on Spoken Language
Translation (IWSLT 2023)</span>, pages 376–382, Toronto, Canada (in-person and
online), July 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan
Salakhutdinov, and Abdelrahman Mohamed.

</span>
<span class="ltx_bibblock">Hubert: Self-supervised speech representation learning by masked
prediction of hidden units.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">IEEE/ACM transactions on audio, speech, and language
processing</span>, 29:3451–3460, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Dong Zhang, Zhehuai Chen,
and Eng Siong Chng.

</span>
<span class="ltx_bibblock">Gentranslate: Large language models are generative multilingual
speech and machine translators.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2402.06894</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Zhichao Huang, Rong Ye, Tom Ko, Qianqian Dong, Shanbo Cheng, Mingxuan Wang, and
Hang Li.

</span>
<span class="ltx_bibblock">Speech translation with large language models: An industrial
practice, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Javier Iranzo-Sánchez, Javier Jorge, Pau Baquero-Arnal, Joan Albert
Silvestre-Cerdà, Adrià Giménez, Jorge Civera, Albert Sanchis, and
Alfons Juan.

</span>
<span class="ltx_bibblock">Streaming cascade-based speech translation leveraged by a direct
segmentation model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">Neural Networks</span>, 142:303–315, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Dávid Javorský, Dominik Macháček, and Ondřej Bojar.

</span>
<span class="ltx_bibblock">Continuous rating as reliable human evaluation of simultaneous speech
translation.

</span>
<span class="ltx_bibblock">In Philipp Koehn, Loïc Barrault, Ondřej Bojar, Fethi
Bougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann,
Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman
Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes,
Tom Kocmi, André Martins, Makoto Morishita, Christof Monz, Masaaki
Nagata, Toshiaki Nakazawa, Matteo Negri, Aurélie Névéol, Mariana
Neves, Martin Popel, Marco Turchi, and Marcos Zampieri, editors, <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</span>, pages
154–164, Abu Dhabi, United Arab Emirates (Hybrid), December 2022.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Roderick Jones.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">Conference interpreting explained</span>.

</span>
<span class="ltx_bibblock">Routledge, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Jari Kolehmainen, Aditya Gourav, Prashanth Gurunath Shivakumar, Yile Gu, Ankur
Gandhe, Ariya Rastrow, Grant P. Strimel, and Ivan Bulyko.

</span>
<span class="ltx_bibblock">Multi-modal retrieval for large language model based speech
recognition.

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Roman Koshkin, Katsuhito Sudoh, and Satoshi Nakamura.

</span>
<span class="ltx_bibblock">Transllama: Llm-based simultaneous translation system, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Jiahuan Li, Shanbo Cheng, Shujian Huang, and Jiajun Chen.

</span>
<span class="ltx_bibblock">Mt-patcher: Selective and extendable knowledge distillation from
large language models for machine translation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Cheng, and Jiajun Chen.

</span>
<span class="ltx_bibblock">Eliciting the translation ability of large language models via
multilingual finetuning with translation instructions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Transactions of the Association for Computational Linguistics</span>,
12:576–592, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Xiaoqing Li, Jinghui Yan, Jiajun Zhang, and Chengqing Zong.

</span>
<span class="ltx_bibblock">Neural name translation improves neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Machine Translation: 14th China Workshop, CWMT 2018,
Wuyishan, China, October 25-26, 2018, Proceedings 14</span>, pages 93–100.
Springer, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Yuang Li, Chang Su, Ming Zhu, Mengyao Piao, Xinglin Lyu, Min Zhang, and Hao
Yang.

</span>
<span class="ltx_bibblock">HW-TSC 2023 submission for the quality estimation shared task.

</span>
<span class="ltx_bibblock">In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz,
editors, <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Proceedings of the Eighth Conference on Machine Translation</span>,
pages 835–840, Singapore, December 2023. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Danni Liu, Gerasimos Spanakis, and Jan Niehues.

</span>
<span class="ltx_bibblock">Low-latency sequence-to-sequence speech recognition and translation
by partial hypothesis selection, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Xiaoqian Liu, Guoqiang Hu, Yangfan Du, Erfeng He, YingFeng Luo, Chen Xu, Tong
Xiao, and Jingbo Zhu.

</span>
<span class="ltx_bibblock">Recent advances in end-to-end simultaneous speech translation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and
Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Chatqa: Surpassing gpt-4 on conversational qa and rag, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Arle Lommel, Hans Uszkoreit, and Aljoscha Burchardt.

</span>
<span class="ltx_bibblock">Multidimensional quality metrics (mqm): A framework for declaring and
describing translation quality metrics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">Tradumàtica</span>, (12):0455–463, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Minh-Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals, and Wojciech Zaremba.

</span>
<span class="ltx_bibblock">Addressing the rare word problem in neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</span>, pages 11–19, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu, Baigong Zheng,
Chuanqiang Zhang, Zhongjun He, Hairong Liu, Xing Li, Hua Wu, and Haifeng
Wang.

</span>
<span class="ltx_bibblock">STACL: Simultaneous translation with implicit anticipation and
controllable latency using prefix-to-prefix framework.

</span>
<span class="ltx_bibblock">In Anna Korhonen, David Traum, and Lluís Màrquez, editors,
<span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</span>, pages 3025–3036, Florence, Italy, July 2019.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Xutai Ma, Juan Pino, and Philipp Koehn.

</span>
<span class="ltx_bibblock">SimulMT to SimulST: Adapting simultaneous text translation to
end-to-end simultaneous speech translation.

</span>
<span class="ltx_bibblock">In Kam-Fai Wong, Kevin Knight, and Hua Wu, editors, <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Proceedings
of the 1st Conference of the Asia-Pacific Chapter of the Association for
Computational Linguistics and the 10th International Joint Conference on
Natural Language Processing</span>, pages 582–587, Suzhou, China, December 2020.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Dominik Macháček, Ondřej Bojar, and Raj Dabre.

</span>
<span class="ltx_bibblock">Mt metrics correlate with human ratings of simultaneous speech
translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">Proceedings of the 20th International Conference on Spoken
Language Translation (IWSLT 2023)</span>, pages 169–179, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Benjamin Marie, Atsushi Fujita, and Raphael Rubino.

</span>
<span class="ltx_bibblock">Scientific credibility of machine translation research: A
meta-evaluation of 769 papers.

</span>
<span class="ltx_bibblock">In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,
<span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">Proceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</span>, pages 7297–7306,
Online, August 2021. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Zoe Moores.

</span>
<span class="ltx_bibblock">The nerle model–a tool for assessing the quality of intralingual
subtitles at live events.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">Universal Access in the Information Society</span>, 23(2):589–607,
2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Ha Nguyen, Yannick Estève, and Laurent Besacier.

</span>
<span class="ltx_bibblock">An empirical study of end-to-end simultaneous speech translation
decoding strategies, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Sara Papi, Marco Gaido, and Matteo Negri.

</span>
<span class="ltx_bibblock">Direct models for simultaneous translation and automatic subtitling:
FBK@IWSLT2023.

</span>
<span class="ltx_bibblock">In Elizabeth Salesky, Marcello Federico, and Marine Carpuat, editors,
<span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">Proceedings of the 20th International Conference on Spoken Language
Translation (IWSLT 2023)</span>, pages 159–168, Toronto, Canada (in-person and
online), July 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Sara Papi, Marco Gaido, Matteo Negri, and Marco Turchi.

</span>
<span class="ltx_bibblock">Over-generation cannot be rewarded: Length-adaptive average lagging
for simultaneous speech translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2206.05807</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Sara Papi, Matteo Negri, and Marco Turchi.

</span>
<span class="ltx_bibblock">Attention as a guide for simultaneous speech translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2212.07850</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">Proceedings of the 40th annual meeting of the Association for
Computational Linguistics</span>, pages 311–318, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine
Bosselut, Robert West, and Boi Faltings.

</span>
<span class="ltx_bibblock">Refiner: Reasoning feedback on intermediate representations, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Peter Polák, Danni Liu, Ngoc-Quan Pham, Jan Niehues, Alexander Waibel, and
Ondřej Bojar.

</span>
<span class="ltx_bibblock">Towards efficient simultaneous speech translation: CUNI-KIT
system for simultaneous track at IWSLT 2023.

</span>
<span class="ltx_bibblock">In Elizabeth Salesky, Marcello Federico, and Marine Carpuat, editors,
<span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">Proceedings of the 20th International Conference on Spoken Language
Translation (IWSLT 2023)</span>, pages 389–396, Toronto, Canada (in-person and
online), July 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Peter Polák, Brian Yan, Shinji Watanabe, Alex Waibel, and Ondřej Bojar.

</span>
<span class="ltx_bibblock">Incremental blockwise beam search for simultaneous speech translation
with controllable quality-latency tradeoff.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">INTERSPEECH 2023</span>, interspeech_2023. ISCA, August 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Matt Post.

</span>
<span class="ltx_bibblock">A call for clarity in reporting BLEU scores.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">Proceedings of the Third Conference on Machine Translation:
Research Papers</span>, pages 186–191, Belgium, Brussels, October 2018.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou,
Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen.

</span>
<span class="ltx_bibblock">Autoact: Automatic agent learning from scratch for qa via
self-planning, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and
Ilya Sutskever.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">ArXiv</span>, abs/2212.04356, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie.

</span>
<span class="ltx_bibblock">Comet: A neural framework for mt evaluation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</span>, pages 2685–2702, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy
Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
Firat, Julian Schrittwieser, et al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of
tokens of context.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:2403.05530</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Yi Ren, Jinglin Liu, Xu Tan, Chen Zhang, Tao Qin, Zhou Zhao, and Tie-Yan Liu.

</span>
<span class="ltx_bibblock">Simulspeech: End-to-end simultaneous speech to text translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics</span>, pages 3787–3796, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du,
Shiwei Shi, Hangyu Mao, Ziyue Li, Xingyu Zeng, and Rui Zhao.

</span>
<span class="ltx_bibblock">Tptu: Large language model-based ai agents for task planning and tool
usage, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Elizabeth Salesky, Marcello Federico, and Marine Carpuat, editors.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib65.1.1">Proceedings of the 20th International Conference on Spoken
Language Translation (IWSLT 2023)</span>, Toronto, Canada (in-person and online),
July 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Elizabeth Salesky, Marcello Federico, and Marta Costa-jussà, editors.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">Proceedings of the 19th International Conference on Spoken
Language Translation (IWSLT 2022)</span>, Dublin, Ireland (in-person and online),
May 2022. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Frank Seide, Morrie Doulaty, Yangyang Shi, Yashesh Gaur, Junteng Jia, and
Chunyang Wu.

</span>
<span class="ltx_bibblock">Speech reallm – real-time streaming speech recognition with
multimodal llms by teaching the flow of time, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Thibault Sellam, Dipanjan Das, and Ankur Parikh.

</span>
<span class="ltx_bibblock">Bleurt: Learning robust metrics for text generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib68.1.1">Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics</span>, pages 7881–7892, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik
Narasimhan, and Shunyu Yao.

</span>
<span class="ltx_bibblock">Reflexion: Language agents with verbal reinforcement learning, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu,
Zejun Ma, and Chao Zhang.

</span>
<span class="ltx_bibblock">Salmonn: Towards generic hearing abilities for large language models,
2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad,
Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi
Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John
Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit,
Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov,
Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn,
Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and
Jeff Wang.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
et al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">arXiv preprint arXiv:2307.09288</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Changhan Wang, Juan Pino, Anne Wu, and Jiatao Gu.

</span>
<span class="ltx_bibblock">CoVoST: A diverse multilingual speech-to-text translation
corpus.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">Proceedings of The 12th Language Resources and Evaluation
Conference</span>, pages 4197–4203, Marseille, France, May 2020. European Language
Resources Association.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Changhan Wang, Anne Wu, and Juan Pino.

</span>
<span class="ltx_bibblock">Covost 2: A massively multilingual speech-to-text translation corpus,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan
Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al.

</span>
<span class="ltx_bibblock">A survey on large language model based autonomous agents.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">Frontiers of Computer Science</span>, 18(6), 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and
Zhaopeng Tu.

</span>
<span class="ltx_bibblock">Document-level machine translation with large language models.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali, editors, <span class="ltx_text ltx_font_italic" id="bib.bib77.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language
Processing</span>, pages 16646–16661, Singapore, December 2023. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Yuguang Wang, Shanbo Cheng, Liyang Jiang, Jiajun Yang, Wei Chen, Muze Li, Lin
Shi, Yanfeng Wang, and Hongtao Yang.

</span>
<span class="ltx_bibblock">Sogou neural machine translation systems for wmt17.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">Proceedings of the Second Conference on Machine Translation</span>,
pages 410–415, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Shira Wein, I Te, Colin Cherry, Juraj Juraska, Dirk Padfield, and Wolfgang
Macherey.

</span>
<span class="ltx_bibblock">Barriers to effective evaluation of simultaneous interpretation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">Findings of the Association for Computational Linguistics:
EACL 2024</span>, pages 209–219, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and Juan Pablo Bello.

</span>
<span class="ltx_bibblock">Wav2clip: Learning robust audio representations from clip.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib80.1.1">ICASSP 2022 - 2022 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Shao-Chuan Wu.

</span>
<span class="ltx_bibblock">Assessing simultaneous interpreting.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib81.1.1">A study on test reliability and Examiners’ assessment
behaviour (Doctoral dissertation). Newcastle University, Newcastle</span>, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Brian Yan, Jiatong Shi, Soumi Maiti, William Chen, Xinjian Li, Yifan Peng,
Siddhant Arora, and Shinji Watanabe.

</span>
<span class="ltx_bibblock">CMU’s IWSLT 2023 simultaneous speech translation system.

</span>
<span class="ltx_bibblock">In Elizabeth Salesky, Marcello Federico, and Marine Carpuat, editors,
<span class="ltx_text ltx_font_italic" id="bib.bib82.1.1">Proceedings of the 20th International Conference on Spoken Language
Translation (IWSLT 2023)</span>, pages 235–240, Toronto, Canada (in-person and
online), July 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Rong Ye, Chengqi Zhao, Tom Ko, Chutong Meng, Tao Wang, Mingxuan Wang, and Jun
Cao.

</span>
<span class="ltx_bibblock">GigaST: A 10,000-hour Pseudo Speech Translation Corpus.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">Proc. INTERSPEECH 2023</span>, pages 2168–2172, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Xingshan Zeng, Liangyou Li, and Qun Liu.

</span>
<span class="ltx_bibblock">RealTranS: End-to-end simultaneous speech translation with
convolutional weighted-shrinking transformer.

</span>
<span class="ltx_bibblock">In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,
<span class="ltx_text ltx_font_italic" id="bib.bib84.1.1">Findings of the Association for Computational Linguistics: ACL-IJCNLP
2021</span>, pages 2461–2474, Online, August 2021. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Ruiqing Zhang, Xiyang Wang, Chuanqiang Zhang, Zhongjun He, Hua Wu, Zhi Li,
Haifeng Wang, Ying Chen, and Qinfei Li.

</span>
<span class="ltx_bibblock">BSTC: A large-scale Chinese-English speech translation dataset.

</span>
<span class="ltx_bibblock">In Hua Wu, Colin Cherry, Liang Huang, Zhongjun He, Qun Liu, Maha
Elbayad, Mark Liberman, Haifeng Wang, Mingbo Ma, and Ruiqing Zhang, editors,
<span class="ltx_text ltx_font_italic" id="bib.bib85.1.1">Proceedings of the Second Workshop on Automatic Simultaneous
Translation</span>, pages 28–35, Online, June 2021. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Shaolei Zhang, Qingkai Fang, Shoutao Guo, Zhengrui Ma, Min Zhang, and Yang
Feng.

</span>
<span class="ltx_bibblock">Streamspeech: Simultaneous speech-to-speech translation with
multi-task learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib86.1.1">arXiv preprint arXiv:2406.03049</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Shaolei Zhang and Yang Feng.

</span>
<span class="ltx_bibblock">End-to-end simultaneous speech translation with differentiable
segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib87.1.1">Findings of the Association for Computational Linguistics:
ACL 2023</span>, pages 7659–7680, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin
Chen, Bo Li, Vera Axelrod, Gary Wang, et al.

</span>
<span class="ltx_bibblock">Google usm: Scaling automatic speech recognition beyond 100
languages.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib88.1.1">arXiv preprint arXiv:2303.01037</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Chengqi Zhao, Zhicheng Liu, Jian Tong, Tao Wang, Mingxuan Wang, Rong Ye,
Qianqian Dong, Jun Cao, and Lei Li.

</span>
<span class="ltx_bibblock">The volctrans neural speech translation system for iwslt 2021.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib89.1.1">Proceedings of the 18th International Conference on Spoken
Language Translation (IWSLT 2021)</span>, pages 64–74, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Jiawei Zheng, Hanghai Hong, Xiaoli Wang, Jingsong Su, Yonggui Liang, and Shikai
Wu.

</span>
<span class="ltx_bibblock">Fine-tuning large language models for domain-specific machine
translation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng
Kong, Jiajun Chen, and Lei Li.

</span>
<span class="ltx_bibblock">Multilingual machine translation with large language models:
Empirical results and analysis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib91.1.1">Findings of the Association for Computational Linguistics:
NAACL 2024</span>, pages 2765–2781, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Human Evaluation Guidelines</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">In this Appendix, we provide detailed human evaluation guidelines which are formulated by professional human interpreters.</p>
</div>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Key Indicator</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">We define the key indicator as the “Valid Information Proportion,” denoted as VIP. This metric measures the proportion of valid semantic fragments within a complete speech session. A semantic fragment is deemed valid if it effectively conveys the core information, accurately representing the speaker’s original intent. Typically, one complete sentence is considered a single semantic fragment. VIP assesses the model’s ability to capture and communicate the essence of the spoken content.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Evaluation Process</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">First, the human evaluators segment the long translation result into semantic fragments according to the formal rules as follows:</p>
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i1.p1.1.1">Semantic Completeness:</span> Each fragment should contain one complete concept or information point. For example, in a conference translation, an ideal semantic fragment often corresponds to a full sentence.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i2.p1.1.1">Natural Language Pauses:</span> Pauses that naturally occur in speech often indicate the boundaries of semantic fragments. During segmentation, natural pauses should be extensively considered and avoid irrational interruptions. Also, punctuation and conjunction in the text should be considered to maintain integrity and clarity of information.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i3.p1.1.1">Logical Coherence:</span> Each segment should contain information that are logically coherent and continuous. Conditional sentences, causative sentences, or both parts of antithesis sentences should be kept within the same segment.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i4.p1">
<p class="ltx_p" id="A1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i4.p1.1.1">Grammatical Completeness:</span> Each segment should include all necessary grammatical components (e.g., subject, verb, object) and have a complete grammatical structure.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i5.p1">
<p class="ltx_p" id="A1.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i5.p1.1.1">Proper Information Density:</span> Each segment should have a moderate amount of information, avoiding information overload. It is recommended that each segment not exceed 50 words.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A1.SS2.p2">
<p class="ltx_p" id="A1.SS2.p2.1">After segmentation, the human evaluators follow the instructions below to evaluate the validity of the semantic fragments:</p>
<ul class="ltx_itemize" id="A1.I2">
<li class="ltx_item" id="A1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i1.p1">
<p class="ltx_p" id="A1.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i1.p1.1.1">Key Information Recognition</span>. Key information refers to the content that can constitute core information, including but not limited to proper nouns, keywords, terminologies, sentence structures, etc.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i2.p1">
<p class="ltx_p" id="A1.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i2.p1.1.1">Correctness Assessment</span>. Evaluators assess whether the translation of key information is accurate and successful in conveying the correct spoken intentions. Misinterpretations of the speaker’s words, inaccuracies in analyzing the context, or erroneous translations of specific terms can all contribute to the failure of the assessment.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i3.p1">
<p class="ltx_p" id="A1.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i3.p1.1.1">Expressiveness Assessment</span>. Evaluators assess whether the whole segment is translated accurately, comprehensibly, and expressively to humans. Assessing for any vague, ambiguous, or misleading statements. This indicator primarily evaluates the clarity, fluency, and intuitiveness of the translation, rather than its accuracy. Typically, verbosity, complex sentence structures, or challenging grammatical constructions that are unnecessary would reduce the expressiveness of the translation, thus leading to failure of the assessment.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="A1.SS2.p2.2">If the translation fails any of the above assessments, the translation will be marked as invalid. After the evaluators assessed all semantic fragments, the VIP could be simply calculated as dividing the number of valid semantic fragments by the total number of fragments.</p>
</div>
<div class="ltx_para" id="A1.SS2.p3">
<p class="ltx_p" id="A1.SS2.p3.1">We illustrate the evaluation criteria with two examples in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A1.T10" title="Table 10 ‣ A.2 Evaluation Process ‣ Appendix A Human Evaluation Guidelines ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">10</span></a>. Although these translations achieve “high accuracy” in automatic evaluations, we still categorize them as invalid. It’s important to note that our standard aims to emulate human interpreters, presenting a significant challenge to both human evaluators and translation systems.</p>
</div>
<div class="ltx_para" id="A1.SS2.p4">
<span class="ltx_ERROR undefined" id="A1.SS2.p4.1">{CJK*}</span>
<p class="ltx_p" id="A1.SS2.p4.2">UTF8gbsn</p>
</div>
<figure class="ltx_table" id="A1.T10">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T10.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T10.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="2" id="A1.T10.2.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T10.2.1.1.1.1" style="font-size:90%;">Example 1: Correctness Assessment</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T10.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T10.2.2.1.1"><span class="ltx_text" id="A1.T10.2.2.1.1.1" style="font-size:90%;">Golden Chinese</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T10.2.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.2.2.1.2.1">
<span class="ltx_p" id="A1.T10.2.2.1.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="A1.T10.2.2.1.2.1.1.1" style="font-size:90%;">请确保服务器后端的API接口完全遵循</span><span class="ltx_text ltx_framed ltx_framed_underline" id="A1.T10.2.2.1.2.1.1.2" style="font-size:90%;">RESTful<sup class="ltx_sup" id="A1.T10.2.2.1.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A1.T10.2.2.1.2.1.1.3" style="font-size:90%;">架构原则。</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T10.2.3.2.1"><span class="ltx_text" id="A1.T10.2.3.2.1.1" style="font-size:90%;">Reference</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T10.2.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.2.3.2.2.1">
<span class="ltx_p" id="A1.T10.2.3.2.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="A1.T10.2.3.2.2.1.1.1" style="font-size:90%;">Please ensure the server backend’s API interface fully complies with </span><span class="ltx_text ltx_framed ltx_framed_underline" id="A1.T10.2.3.2.2.1.1.2" style="font-size:90%;">RESTful<sup class="ltx_sup" id="A1.T10.2.3.2.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A1.T10.2.3.2.2.1.1.3" style="font-size:90%;"> architectural principles.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T10.2.4.3.1"><span class="ltx_text" id="A1.T10.2.4.3.1.1" style="font-size:90%;">Translation</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T10.2.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.2.4.3.2.1">
<span class="ltx_p" id="A1.T10.2.4.3.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="A1.T10.2.4.3.2.1.1.1" style="font-size:90%;">Please ensure the server backend’s API interface fully complies with </span><span class="ltx_text ltx_framed ltx_framed_underline" id="A1.T10.2.4.3.2.1.1.2" style="font-size:90%;">restless<sup class="ltx_sup" id="A1.T10.2.4.3.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A1.T10.2.4.3.2.1.1.3" style="font-size:90%;"> architectural principles.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T10.2.5.4.1"><span class="ltx_text" id="A1.T10.2.5.4.1.1" style="font-size:90%;">Explanation</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T10.2.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.2.5.4.2.1">
<span class="ltx_p" id="A1.T10.2.5.4.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="A1.T10.2.5.4.2.1.1.1" style="font-size:90%;">Although the sentence-level BLEU score is near 80, the translation is still considered invalid, because the keyword "RESTful" is mistranslated.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.2.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="2" id="A1.T10.2.6.5.1"><span class="ltx_text ltx_font_bold" id="A1.T10.2.6.5.1.1" style="font-size:90%;">Example 2: Expressiveness Assessment</span></th>
</tr>
<tr class="ltx_tr" id="A1.T10.2.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T10.2.7.6.1"><span class="ltx_text" id="A1.T10.2.7.6.1.1" style="font-size:90%;">Golden Chinese</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T10.2.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.2.7.6.2.1">
<span class="ltx_p" id="A1.T10.2.7.6.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="A1.T10.2.7.6.2.1.1.1" style="font-size:90%;">这部分跟资源那边，前端资源这一块搞定了吗？</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.2.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T10.2.8.7.1"><span class="ltx_text" id="A1.T10.2.8.7.1.1" style="font-size:90%;">Reference</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T10.2.8.7.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.2.8.7.2.1">
<span class="ltx_p" id="A1.T10.2.8.7.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="A1.T10.2.8.7.2.1.1.1" style="font-size:90%;">Did you arrange the front-end resources well?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.2.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T10.2.9.8.1"><span class="ltx_text" id="A1.T10.2.9.8.1.1" style="font-size:90%;">Translation 1</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T10.2.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.2.9.8.2.1">
<span class="ltx_p" id="A1.T10.2.9.8.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="A1.T10.2.9.8.2.1.1.1" style="font-size:90%;">Is this part related to the front-end resources? Did you finish the front-end resources?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.2.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T10.2.10.9.1"><span class="ltx_text" id="A1.T10.2.10.9.1.1" style="font-size:90%;">Translation 2</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T10.2.10.9.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.2.10.9.2.1">
<span class="ltx_p" id="A1.T10.2.10.9.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="A1.T10.2.10.9.2.1.1.1" style="font-size:90%;">Has the front-end resource been settled?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.2.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="A1.T10.2.11.10.1"><span class="ltx_text" id="A1.T10.2.11.10.1.1" style="font-size:90%;">Explanation</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="A1.T10.2.11.10.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T10.2.11.10.2.1">
<span class="ltx_p" id="A1.T10.2.11.10.2.1.1" style="width:313.0pt;"><span class="ltx_text" id="A1.T10.2.11.10.2.1.1.1" style="font-size:90%;">Translation 1 is redundant, disfluent and contains minor errors, thus not easy to understand by human evaluators, while Translation 2 generated by CLASI is concise and fluent, and conveys the speaker’s intention appropriately.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 10: </span>Human evaluation examples of Chinese-to-English Translation task.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Correlation with Automatic Metrics</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A1.F4" title="In A.3 Correlation with Automatic Metrics ‣ Appendix A Human Evaluation Guidelines ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a> shows the distribution and regression curve for VIP with regard to BLEU, BLEURT, and COMET, respectively. From the scatter points in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A1.F4" title="In A.3 Correlation with Automatic Metrics ‣ Appendix A Human Evaluation Guidelines ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a> we may observe that as VIP score increases, the growth of the automatic metric curves slows down and becomes less significant, making it hard to reflect the real changes in translation quality. The correlation curves in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A1.F4" title="In A.3 Correlation with Automatic Metrics ‣ Appendix A Human Evaluation Guidelines ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a> also demonstrate the finding. Here, we calculate Kendall’s Tau correlation coefficient <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#bib.bib1" title="">1</a>]</cite>, which measures the monotonic correlation between two ordered variables. In low VIP ranges, the correlation between VIP and automatic metrics is observable; as the score increases, the correlation harshly drops, which indicates a significant distortion of the automatic metrics. A possible reason is that the translations may differ from the groundtruths by only a few words in the mediocre ranges. However, in a real simultaneous interpretation scenario, these words are likely to be keywords that play important roles in conveying precise information, which may significantly impact VIP scores.</p>
</div>
<figure class="ltx_figure" id="A1.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F4.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="578" id="A1.F4.1.g1" src="x4.png" width="830"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A1.F4.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="579" id="A1.F4.2.g1" src="x5.png" width="830"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F4.4.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="A1.F4.5.2" style="font-size:90%;">Analysis of VIP vs different automatic metrics on the zh-en direction. The distribution and regression curve of the data points for each metric are shown in the above-left figure. Line charts for the calculated correlation between VIP and Automatic metric within multiple intervals are shown in the right figure. Due to the limitation of human labeling capacity, we collect 35 rounds of human evaluation results for zh-en direction on our in-house testset. </span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Supplementary Materials</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Supplementary Case Study</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">We provide more case studies in <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A2.T11" title="In B.1 Supplementary Case Study ‣ Appendix B Supplementary Materials ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">11</span></a>. In terms of informal, disfluent, code-mixing, and named-entity translation, CLASI could achieve much better results than the commercial products. Benefits from the end-to-end approach, CLASI could also understand the original speech tone and generate better translations.</p>
</div>
<figure class="ltx_table" id="A2.T11">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T11.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T11.2.1.1">
<th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A2.T11.2.1.1.1"><span class="ltx_text ltx_font_bold" id="A2.T11.2.1.1.1.1" style="font-size:90%;">CASE 1： Informal, disfluent speech translation</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T11.2.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.2.1.1.1">
<span class="ltx_p" id="A2.T11.2.2.1.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.2.1.1.1.1.1" style="font-size:90%;">Golden Transcription</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.2.1.2.1">
<span class="ltx_p" id="A2.T11.2.2.1.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.2.1.2.1.1.1" style="font-size:90%;">那基于这些观察，</span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.2.1.2.1.1.2" style="font-size:90%;">那我们是不是啊，我，我，我<sup class="ltx_sup" id="A2.T11.2.2.1.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A2.T11.2.2.1.2.1.1.3" style="font-size:90%;">，我们是不是可以去找一种，就是像GPT3.5一样的，统一的建模方法？</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.3.2.1.1">
<span class="ltx_p" id="A2.T11.2.3.2.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.3.2.1.1.1.1" style="font-size:90%;">Commerical 4 ASR</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.3.2.2.1">
<span class="ltx_p" id="A2.T11.2.3.2.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.3.2.2.1.1.1" style="font-size:90%;">那基于这些观察，</span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.3.2.2.1.1.2" style="font-size:90%;">那我们是不是啊？我们是不是我？<sup class="ltx_sup" id="A2.T11.2.3.2.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A2.T11.2.3.2.2.1.1.3" style="font-size:90%;">我们是不是可以去找一种，就是像</span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.3.2.2.1.1.4" style="font-size:90%;">gvt3.5<sup class="ltx_sup" id="A2.T11.2.3.2.2.1.1.4.1">2</sup></span><span class="ltx_text" id="A2.T11.2.3.2.2.1.1.5" style="font-size:90%;">一样的，统一的建模方法？</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.4.3.1.1">
<span class="ltx_p" id="A2.T11.2.4.3.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.4.3.1.1.1.1" style="font-size:90%;">Commerical 4 Translation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.4.3.2.1">
<span class="ltx_p" id="A2.T11.2.4.3.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.4.3.2.1.1.1" style="font-size:90%;">So based on these observations, </span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.4.3.2.1.1.2" style="font-size:90%;">are we? Are we me? Can we go<sup class="ltx_sup" id="A2.T11.2.4.3.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A2.T11.2.4.3.2.1.1.3" style="font-size:90%;">, like </span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.4.3.2.1.1.4" style="font-size:90%;">gvt3.5<sup class="ltx_sup" id="A2.T11.2.4.3.2.1.1.4.1">2</sup></span><span class="ltx_text" id="A2.T11.2.4.3.2.1.1.5" style="font-size:90%;">, and do this unified modeling</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.5.4.1.1">
<span class="ltx_p" id="A2.T11.2.5.4.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.5.4.1.1.1.1" style="font-size:90%;">CLASI ASR</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.5.4.2.1">
<span class="ltx_p" id="A2.T11.2.5.4.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.5.4.2.1.1.1" style="font-size:90%;">那基于这些观察，</span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.5.4.2.1.1.2" style="font-size:90%;">那我们是不是啊，我，我，我，<sup class="ltx_sup" id="A2.T11.2.5.4.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A2.T11.2.5.4.2.1.1.3" style="font-size:90%;">我们是不是可以去找一种，就是像GPT3.5一样的，统一的建模方法？</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.6.5.1.1">
<span class="ltx_p" id="A2.T11.2.6.5.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.6.5.1.1.1.1" style="font-size:90%;">CLASI Translation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.6.5.2.1">
<span class="ltx_p" id="A2.T11.2.6.5.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.6.5.2.1.1.1" style="font-size:90%;">Based on these observations, </span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.6.5.2.1.1.2" style="font-size:90%;">can we find<sup class="ltx_sup" id="A2.T11.2.6.5.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A2.T11.2.6.5.2.1.1.3" style="font-size:90%;"> a unified modeling method </span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.6.5.2.1.1.4" style="font-size:90%;">like GPT3.5<sup class="ltx_sup" id="A2.T11.2.6.5.2.1.1.4.1">2</sup></span><span class="ltx_text" id="A2.T11.2.6.5.2.1.1.5" style="font-size:90%;">?</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.7.6.1.1">
<span class="ltx_p" id="A2.T11.2.7.6.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.7.6.1.1.1.1" style="font-size:90%;">Explanation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.7.6.2.1">
<span class="ltx_p" id="A2.T11.2.7.6.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.7.6.2.1.1.1" style="font-size:90%;">The first labeled Chinese phrase (superscript 1) actually means </span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.7.6.2.1.1.2" style="font-size:90%;">"can we find"<sup class="ltx_sup" id="A2.T11.2.7.6.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A2.T11.2.7.6.2.1.1.3" style="font-size:90%;">. CLASI can generate a much more fluent, concise translation than the Commerical 4. Besides, for keyword </span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.7.6.2.1.1.4" style="font-size:90%;">GPT3.5<sup class="ltx_sup" id="A2.T11.2.7.6.2.1.1.4.1">2</sup></span><span class="ltx_text" id="A2.T11.2.7.6.2.1.1.5" style="font-size:90%;">, CLASI can generate correct ASR and translation.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.8.7">
<th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A2.T11.2.8.7.1"><span class="ltx_text ltx_font_bold" id="A2.T11.2.8.7.1.1" style="font-size:90%;">CASE 2：Disfluent and code-mixing speech</span></th>
</tr>
<tr class="ltx_tr" id="A2.T11.2.9.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.9.8.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.9.8.1.1">
<span class="ltx_p" id="A2.T11.2.9.8.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.9.8.1.1.1.1" style="font-size:90%;">Golden Transcription</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.9.8.2.1">
<span class="ltx_p" id="A2.T11.2.9.8.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.9.8.2.1.1.1" style="font-size:90%;">我听过一句话叫，</span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.9.8.2.1.1.2" style="font-size:90%;">pri, pri, prioritization, prioritization<sup class="ltx_sup" id="A2.T11.2.9.8.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A2.T11.2.9.8.2.1.1.3" style="font-size:90%;"> is only real when it hurts。</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.10.9.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.10.9.1.1">
<span class="ltx_p" id="A2.T11.2.10.9.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.10.9.1.1.1.1" style="font-size:90%;">Commerical 4 ASR</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.10.9.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.10.9.2.1">
<span class="ltx_p" id="A2.T11.2.10.9.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.10.9.2.1.1.1" style="font-size:90%;">我听过一句话叫，</span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.10.9.2.1.1.2" style="font-size:90%;">Pro, 不管, prioritization, prioritization<sup class="ltx_sup" id="A2.T11.2.10.9.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A2.T11.2.10.9.2.1.1.3" style="font-size:90%;"> is only real when it hurts.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.11.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.11.10.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.11.10.1.1">
<span class="ltx_p" id="A2.T11.2.11.10.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.11.10.1.1.1.1" style="font-size:90%;">Commerical 4 Translation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.11.10.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.11.10.2.1">
<span class="ltx_p" id="A2.T11.2.11.10.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.11.10.2.1.1.1" style="font-size:90%;">I heard a saying called </span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.11.10.2.1.1.2" style="font-size:90%;">Pro, Anyway, it is
Prioritization<sup class="ltx_sup" id="A2.T11.2.11.10.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A2.T11.2.11.10.2.1.1.3" style="font-size:90%;"> is only real when it hurts.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.12.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.12.11.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.12.11.1.1">
<span class="ltx_p" id="A2.T11.2.12.11.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.12.11.1.1.1.1" style="font-size:90%;">CLASI ASR</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.12.11.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.12.11.2.1">
<span class="ltx_p" id="A2.T11.2.12.11.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.12.11.2.1.1.1" style="font-size:90%;">我听过一句话叫，</span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.12.11.2.1.1.2" style="font-size:90%;">priortizaiton, prioritization<sup class="ltx_sup" id="A2.T11.2.12.11.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A2.T11.2.12.11.2.1.1.3" style="font-size:90%;"> is only real when it hurts。</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.13.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.13.12.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.13.12.1.1">
<span class="ltx_p" id="A2.T11.2.13.12.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.13.12.1.1.1.1" style="font-size:90%;">CLASI Translation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.13.12.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.13.12.2.1">
<span class="ltx_p" id="A2.T11.2.13.12.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.13.12.2.1.1.1" style="font-size:90%;">I heard a saying that </span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.13.12.2.1.1.2" style="font-size:90%;">prioritization<sup class="ltx_sup" id="A2.T11.2.13.12.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A2.T11.2.13.12.2.1.1.3" style="font-size:90%;"> is only real when it hurts.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.14.13">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.14.13.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.14.13.1.1">
<span class="ltx_p" id="A2.T11.2.14.13.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.14.13.1.1.1.1" style="font-size:90%;">Explanation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.14.13.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.14.13.2.1">
<span class="ltx_p" id="A2.T11.2.14.13.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.14.13.2.1.1.1" style="font-size:90%;">The speaker stutters when saying the English sentence, which is very common in real-world scenarios. CLASI can fully understand and generate the correct English text without any repetition.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.15.14">
<th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A2.T11.2.15.14.1"><span class="ltx_text ltx_font_bold" id="A2.T11.2.15.14.1.1" style="font-size:90%;">CASE 3: Named-entity recognition and translation</span></th>
</tr>
<tr class="ltx_tr" id="A2.T11.2.16.15">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.16.15.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.16.15.1.1">
<span class="ltx_p" id="A2.T11.2.16.15.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.16.15.1.1.1.1" style="font-size:90%;">Golden Transcription</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.16.15.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.16.15.2.1">
<span class="ltx_p" id="A2.T11.2.16.15.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.16.15.2.1.1.1" style="font-size:90%;">好球！</span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.16.15.2.1.1.2" style="font-size:90%;">迪亚斯<sup class="ltx_sup" id="A2.T11.2.16.15.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A2.T11.2.16.15.2.1.1.3" style="font-size:90%;">的传中，C罗来争抢，这个就是</span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.16.15.2.1.1.4" style="font-size:90%;">C罗<sup class="ltx_sup" id="A2.T11.2.16.15.2.1.1.4.1">2</sup></span><span class="ltx_text" id="A2.T11.2.16.15.2.1.1.5" style="font-size:90%;">最喜欢的</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.17.16">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.17.16.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.17.16.1.1">
<span class="ltx_p" id="A2.T11.2.17.16.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.17.16.1.1.1.1" style="font-size:90%;">Commerical 4 ASR</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.17.16.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.17.16.2.1">
<span class="ltx_p" id="A2.T11.2.17.16.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.17.16.2.1.1.1" style="font-size:90%;">好球,</span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.17.16.2.1.1.2" style="font-size:90%;">比亚斯<sup class="ltx_sup" id="A2.T11.2.17.16.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A2.T11.2.17.16.2.1.1.3" style="font-size:90%;">的传统，C罗来争抢这个就这是</span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.17.16.2.1.1.4" style="font-size:90%;">C罗<sup class="ltx_sup" id="A2.T11.2.17.16.2.1.1.4.1">2</sup></span><span class="ltx_text" id="A2.T11.2.17.16.2.1.1.5" style="font-size:90%;">最喜欢的</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.18.17">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.18.17.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.18.17.1.1">
<span class="ltx_p" id="A2.T11.2.18.17.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.18.17.1.1.1.1" style="font-size:90%;">Commerical 4 Translation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.18.17.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.18.17.2.1">
<span class="ltx_p" id="A2.T11.2.18.17.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.18.17.2.1.1.1" style="font-size:90%;">Nice shot, </span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.18.17.2.1.1.2" style="font-size:90%;">Bias’<sup class="ltx_sup" id="A2.T11.2.18.17.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A2.T11.2.18.17.2.1.1.3" style="font-size:90%;"> traditional C Ronaldo to compete for this is </span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.18.17.2.1.1.4" style="font-size:90%;">C Luo’s<sup class="ltx_sup" id="A2.T11.2.18.17.2.1.1.4.1">2</sup></span><span class="ltx_text" id="A2.T11.2.18.17.2.1.1.5" style="font-size:90%;"> favorite</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.19.18">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.19.18.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.19.18.1.1">
<span class="ltx_p" id="A2.T11.2.19.18.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.19.18.1.1.1.1" style="font-size:90%;">CLASI ASR</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.19.18.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.19.18.2.1">
<span class="ltx_p" id="A2.T11.2.19.18.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.19.18.2.1.1.1" style="font-size:90%;">好球！</span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.19.18.2.1.1.2" style="font-size:90%;">迪亚斯<sup class="ltx_sup" id="A2.T11.2.19.18.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A2.T11.2.19.18.2.1.1.3" style="font-size:90%;">的传中，C罗来争抢。 这个就这是C罗最喜欢的</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.20.19">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.20.19.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.20.19.1.1">
<span class="ltx_p" id="A2.T11.2.20.19.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.20.19.1.1.1.1" style="font-size:90%;">CLASI Translation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.20.19.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.20.19.2.1">
<span class="ltx_p" id="A2.T11.2.20.19.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.20.19.2.1.1.1" style="font-size:90%;">Nice cross by </span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.20.19.2.1.1.2" style="font-size:90%;">Dias<sup class="ltx_sup" id="A2.T11.2.20.19.2.1.1.2.1">1</sup></span><span class="ltx_text" id="A2.T11.2.20.19.2.1.1.3" style="font-size:90%;">, Ronaldo goes for it. This is </span><span class="ltx_text ltx_framed ltx_framed_underline" id="A2.T11.2.20.19.2.1.1.4" style="font-size:90%;">Ronaldo’s<sup class="ltx_sup" id="A2.T11.2.20.19.2.1.1.4.1">2</sup></span><span class="ltx_text" id="A2.T11.2.20.19.2.1.1.5" style="font-size:90%;"> favorite.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.21.20">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.21.20.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.21.20.1.1">
<span class="ltx_p" id="A2.T11.2.21.20.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.21.20.1.1.1.1" style="font-size:90%;">Explanation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.21.20.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.21.20.2.1">
<span class="ltx_p" id="A2.T11.2.21.20.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.21.20.2.1.1.1" style="font-size:90%;">Commerical 4 cannot correctly recognize the name of the famous football player, Ruben Dias. As for the name of Cristiano Ronaldo, although it translates correctly the first time, but fails the second time. CLASI can perform perfect recognition and translation.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.22.21">
<th class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A2.T11.2.22.21.1"><span class="ltx_text ltx_font_bold" id="A2.T11.2.22.21.1.1" style="font-size:90%;">CASE 4: Speech tone understanding</span></th>
</tr>
<tr class="ltx_tr" id="A2.T11.2.23.22">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.23.22.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.23.22.1.1">
<span class="ltx_p" id="A2.T11.2.23.22.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.23.22.1.1.1.1" style="font-size:90%;">Golden Transcription</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.23.22.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.23.22.2.1">
<span class="ltx_p" id="A2.T11.2.23.22.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.23.22.2.1.1.1" style="font-size:90%;">门前斜传，漂亮！这下漂亮！球进了！摆乌龙！哎，自摆乌龙。</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.24.23">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.24.23.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.24.23.1.1">
<span class="ltx_p" id="A2.T11.2.24.23.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.24.23.1.1.1.1" style="font-size:90%;">Commerical 4 ASR</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.24.23.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.24.23.2.1">
<span class="ltx_p" id="A2.T11.2.24.23.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.24.23.2.1.1.1" style="font-size:90%;">球没有斜转漂亮，这下球进了白骨龙，这白骨龙</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.25.24">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.25.24.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.25.24.1.1">
<span class="ltx_p" id="A2.T11.2.25.24.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.25.24.1.1.1.1" style="font-size:90%;">Commerical 4 Translation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.25.24.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.25.24.2.1">
<span class="ltx_p" id="A2.T11.2.25.24.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.25.24.2.1.1.1" style="font-size:90%;">The ball didn’t spin beautifully, and now the ball went into the bone dragon, the bone dragon</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.26.25">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.26.25.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.26.25.1.1">
<span class="ltx_p" id="A2.T11.2.26.25.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.26.25.1.1.1.1" style="font-size:90%;">CLASI ASR</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.26.25.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.26.25.2.1">
<span class="ltx_p" id="A2.T11.2.26.25.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.26.25.2.1.1.1" style="font-size:90%;">门前斜转，漂亮！这下漂亮！球进了！摆乌龙！哎，自摆乌龙。</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.27.26">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A2.T11.2.27.26.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.27.26.1.1">
<span class="ltx_p" id="A2.T11.2.27.26.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.27.26.1.1.1.1" style="font-size:90%;">CLASI Translation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T11.2.27.26.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.27.26.2.1">
<span class="ltx_p" id="A2.T11.2.27.26.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.27.26.2.1.1.1" style="font-size:90%;">Diagonal shot in front of the goal. Beautiful! What a beauty! Goal! Own goal! Yes, own goal.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T11.2.28.27">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t" id="A2.T11.2.28.27.1">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.28.27.1.1">
<span class="ltx_p" id="A2.T11.2.28.27.1.1.1" style="width:85.4pt;"><span class="ltx_text" id="A2.T11.2.28.27.1.1.1.1" style="font-size:90%;">Explanation</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="A2.T11.2.28.27.2">
<span class="ltx_inline-block ltx_align_top" id="A2.T11.2.28.27.2.1">
<span class="ltx_p" id="A2.T11.2.28.27.2.1.1" style="width:341.4pt;"><span class="ltx_text" id="A2.T11.2.28.27.2.1.1.1" style="font-size:90%;">CLASI could recognize the speaker with an exciting tone, thus generating the translation with exclamation marks. Besides, in this case, which is a complicate scenario of a football game, the ASR outputs of the Commerical 4 are mostly incorrect, leads to nonsense translation.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 11: </span>Comparision between CLASI and Commerical 4 for zh-en direction.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Example of Detailed Evaluation Result on RealSI</h3>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">We provide a detailed human evaluation result for CLASI in Figure <a class="ltx_ref" href="https://arxiv.org/html/2407.21646v2#A2.F5" title="Figure 5 ‣ B.2 Example of Detailed Evaluation Result on RealSI ‣ Appendix B Supplementary Materials ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"><span class="ltx_text ltx_ref_tag">5</span></a>, where we provide golden source transcription, CLASI output, human evaluation results, and reference translation. We randomly choose one of the test samples in RealSI. We share the full detailed evaluation results at online sheets<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>We provide the full evaluation results of CLASI at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://bit.ly/clasi-eval" title="">https://bit.ly/clasi-eval</a></span></span></span>
for academic reference. Note that to ensure fair comparison, when evaluating multiple systems, we randomly shuffle the ordering between systems for each semantic fragment so that human evaluators cannot identify the specific system.</p>
</div>
<figure class="ltx_figure" id="A2.F5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.F5.1" style="width:433.6pt;height:535.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(67.5pt,-83.3pt) scale(1.45199469596205,1.45199469596205) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="708" id="A2.F5.1.g1" src="x6.png" width="561"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F5.3.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="A2.F5.4.2" style="font-size:90%;">The first column indicates the golden transcription of the source text. Each row indicates one semantic fragment split by human evaluators. The second column is the translation results of CLASI. The third and fourth columns indicate the validity of translation and reference translation, respectively. In this case, the VIP is 24/29 == 82.8%.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Aug 30 06:51:58 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
