<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2201.01409] Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness</title><meta property="og:description" content="Federated learning (FL) is a distributed learning paradigm that preserves users’ data privacy while leveraging the entire dataset of all participants. In FL, multiple models are trained independently on the clients and…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2201.01409">

<!--Generated on Wed Mar  6 19:58:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on Received: date / Accepted: date.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated Learning Robustness Byzantine Attacks Mutation Testing Defense Methods">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">∎

</p>
</div>
<span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>A. Eslami Abyane </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>University of Calgary 
<br class="ltx_break"><span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>amin.eslamiabyane@ucalgary.ca</span></span></span>
</span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>D. Zhu </span></span></span><span id="id4" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>Technical University of Munich 
<br class="ltx_break"><span id="id4.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">email: </span>derui.zhu@tum.de</span></span></span>
</span></span></span><span id="id5" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">institutetext: </span>R. Souza </span></span></span><span id="id6" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_note_type">institutetext: </span>University of Calgary 
<br class="ltx_break"><span id="id6.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_note_type">email: </span>roberto.medeirosdeso@ucalgary.ca
<span id="id6.1.1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_note_type">institutetext: </span>L. Ma </span></span></span><span id="id6.1.2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_note_type">institutetext: </span>University of Alberta 
<br class="ltx_break"><span id="id6.1.2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_note_type">email: </span>ma.lei@acm.org</span></span></span>
</span></span></span><span id="id6.1.3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_note_type">institutetext: </span>H. Hemmati </span></span></span><span id="id6.1.4" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_note_type">institutetext: </span>York University and University of Calgary 
<br class="ltx_break"><span id="id6.1.4.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_note_type">email: </span>hemmati@yorku.ca</span></span></span> </span></span></span></span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amin Eslami Abyane
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Derui Zhu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Roberto Souza
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lei Ma
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hadi Hemmati
</span></span>
</div>
<div class="ltx_dates">(Received: date / Accepted: date)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated learning (FL) is a distributed learning paradigm that preserves users’ data privacy while leveraging the entire dataset of all participants. In FL, multiple models are trained independently on the clients and aggregated centrally to update a global model in an iterative process. Although this approach is excellent at preserving privacy, FL still suffers from quality issues such as attacks or byzantine faults. Recent attempts have been made to address such quality challenges on the robust aggregation techniques for FL.
However, the effectiveness of state-of-the-art (SOTA) robust FL techniques is still unclear and lacks a comprehensive study. Therefore, to better understand the current quality status and challenges of these SOTA FL techniques in the presence of attacks and faults, we perform a large-scale empirical study to investigate the SOTA FL’s quality from multiple angles of attacks, simulated faults (via mutation operators), and aggregation (defense) methods.
In particular, we study FL’s performance on the image classification tasks and use Deep Neural Networks as our model type.
Furthermore, we perform our study on two generic image datasets and one real-world federated medical image dataset. We also systematically investigate the effect of the proportion of affected clients and the dataset distribution factors on the robustness of FL.
After a large-scale analysis with 496 configurations, we find that most mutators on each user have a negligible effect on the final model in the generic datasets, and only one of them is effective in the medical dataset. Furthermore, we show that model poisoning attacks are more effective than data poisoning attacks. Moreover, choosing the most robust FL aggregator depends on the attacks and datasets. Finally, we illustrate that a simple ensemble of aggregators achieves a more robust solution than any single aggregator and is the best choice in 75% of the cases. The data that support the findings of this study are available in our repository at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/aminesi/federated</span>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Federated Learning Robustness Byzantine Attacks Mutation Testing Defense Methods
</div>
<span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>Empirical Software Engineering</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Mobile devices have become quite powerful and essential part in our lives in recent years. The increased computational power and the need and capability to support Deep Learning (DL) in many mainstream intelligent tasks such as image classification has helped introduce a new learning approach called Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite>. FL is a distributed learning paradigm where data resides only in each device of a user/client. Since data is only on clients’ devices and is not transferred between clients and the server, FL can fully preserve clients’ data privacy by design. As a result, FL is now being quickly adopted in privacy-sensitive areas like DL for medical imaging, where images are from different hospitals and patients’ privacy is of utmost importance. With the recent trend of more strict data privacy regulation, FL continuously grows and is expected to expand its industrial adoption in the next few years.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In FL, clients receive a model from the server, perform the training on their data, and then send the new model to the server; then, the server aggregates the clients’ models into a new model. This process continues multiple times till the training converges.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Like any other software, FL-based software systems may be prone to quality issues like robustness against adversarial attacks. Recent studies have identified several challenges with the FL process, including FL quality in the presence of attacks, heterogeneous data, and communication issues <cite class="ltx_cite ltx_citemacro_citep">(Kairouz et al., <a href="#bib.bib18" title="" class="ltx_ref">2021</a>; Xia et al., <a href="#bib.bib39" title="" class="ltx_ref">2021</a>; Li et al., <a href="#bib.bib23" title="" class="ltx_ref">2020a</a>)</cite>. One of the most studied topics within these quality challenges is the robustness of FL for byzantine attacks <cite class="ltx_cite ltx_citemacro_citep">(Blanchard et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>; Bagdasaryan et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>; Fang et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>; Yin et al., <a href="#bib.bib41" title="" class="ltx_ref">2018</a>; Li et al., <a href="#bib.bib22" title="" class="ltx_ref">2019</a>; Bhagoji et al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>; Lyu et al., <a href="#bib.bib26" title="" class="ltx_ref">2020</a>; Sun et al., <a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>. Since clients can access the data and the training process, adversarial clients can cause various data and model poisoning attacks <cite class="ltx_cite ltx_citemacro_citep">(Bhagoji et al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>. So, many FL aggregation techniques have been proposed recently to make FL robust against these quality problems <cite class="ltx_cite ltx_citemacro_citep">(Blanchard et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>; Yin et al., <a href="#bib.bib41" title="" class="ltx_ref">2018</a>; Li et al., <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Aside from being vulnerable to adversarial attacks, FL, like any other system, can also be prone to common faults. Mutation testing is a well-known technique in software testing that introduces minor faults (“mutation”) in the source code. It enables to assess the test cases’ adequacy and sufficiency in detecting (killing) those faulty versions of the code (“mutants”). In recent years, mutation testing has also been applied to DL applications by defining mutation operators working on DL models and datasets <cite class="ltx_cite ltx_citemacro_citep">(Shen et al., <a href="#bib.bib33" title="" class="ltx_ref">2018</a>; Hu et al., <a href="#bib.bib16" title="" class="ltx_ref">2019</a>; Wang et al., <a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite>. Some studies show the effectiveness of these mutants in terms of robustness analysis of DL programs for real faults <cite class="ltx_cite ltx_citemacro_citep">(Humbatova et al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To better understand the current state-of-the-art (SOTA) FL techniques against common attacks and quality issues, we perform experiments on four FL attacks and four DL mutation operators.
The attacks in our study are Label Flip <cite class="ltx_cite ltx_citemacro_citep">(Fang et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>, Sign Flip <cite class="ltx_cite ltx_citemacro_citep">(Blanchard et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>, Random Update <cite class="ltx_cite ltx_citemacro_citep">(Blanchard et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>, and Backdoor attack <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>, and we use Noise, Overlap, Delete and Unbalance mutator to simulate faults <cite class="ltx_cite ltx_citemacro_citep">(Humbatova et al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>.
Since each attack/fault can appear independently in each client in FL, we further study the effect of the proportion of affected clients on the attack/fault’s success.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our study is focused on image classification tasks using Deep Neural Networks (DNNs) in FL. These choices are made because of the popularity of image classification tasks and DNNs in FL-related studies <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib28" title="" class="ltx_ref">2017</a>; Fang et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>; Yin et al., <a href="#bib.bib41" title="" class="ltx_ref">2018</a>)</cite>. DNNs are also used because they are shown to outperform linear models in these tasks in most cases.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">We also run the experiments on generic and real-world federated medical image datasets. Since generic datasets are not distributed, we distribute them between multiple clients to see the effect of distribution, and we make the distribution with three levels of non-iid.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">We then evaluate four well-known aggregation techniques (Federated Averaging (baseline) <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite>, Krum <cite class="ltx_cite ltx_citemacro_citep">(Blanchard et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>, Median and Trimmed Mean <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a href="#bib.bib41" title="" class="ltx_ref">2018</a>)</cite>). Finally, we study the feasibility of creating a more robust aggregator using the existing aggregation methods.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">Our large-scale study consists of 496 configurations, each executed 10 times (i.e., 4,960 models trained and tested) to account for the randomness of algorithms. The results show that even the baseline FL aggregator (Federated Averaging) is quite robust against mutation operators on the generic image datasets. However, the Overlap mutator causes noticeable damage when applied to the medical dataset. Furthermore, our findings show that all examined attacks are effective against the baseline aggregator, but attacks that poison the model are more effective, as expected.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">Our comparison of FL aggregators shows that Krum faces issues on more non-iid datasets and does not work as well as others, where all clients are benign. We also observe that no single aggregation method can resist all the attacks (i.e., each has pros and cons). In other words, the FL aggregator’s robustness depends on the dataset, the attack, and other factors like the data distribution. Finally, considering these strengths and weaknesses of aggregators, we propose a simple ensemble of existing aggregators. The results show that the ensemble aggregator can perform as well or even better than any of the aggregators alone in 75% cases and achieves the highest accuracy on average.</p>
</div>
<div id="S1.p11" class="ltx_para">
<p id="S1.p11.1" class="ltx_p">To summarize, the contributions of this paper are:</p>
</div>
<div id="S1.p12" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Analysis of four FL aggregators under three untargeted attacks, a targeted attack, and four simulated data faults (using mutators) with different proportions of affected clients.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Analysis of different configurations of FL (cross-device and cross-silo) using both generic and medical datasets with different distributions.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">A simple ensemble aggregator that is a better choice than any of its constituent aggregators in 75% of the cases.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p13" class="ltx_para">
<p id="S1.p13.1" class="ltx_p">Given that FL is a promising ML technique for many privacy-aware use cases, this very early study in this direction has the potential to impact a broad set of DL software in practice. It also provides guidance on FL robustness areas that require further research.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we briefly introduce basic concepts needed for understanding this paper, including FL and its attacks and defense mechanisms and mutation testing for deep learning.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Federated Learning</h3>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2201.01409/assets/FL-workflow.drawio.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="413" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A typical Federated Learning workflow. The black arrows represent updates sent to the server, and the orange ones represent the updated global model sent to the clients. Each device has a dataset and a trained model which are represented by different colors.</figcaption>
</figure>
<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In recent years, a new learning technique has been proposed called Federated Learning <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite>. The goal of FL is to make learning possible on distributed data while maintaining clients’ privacy. To that end, FL does not collect clients’ data; instead, it performs learning on each client with their data. Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Federated Learning ‣ 2 Background ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows how FL works along with the components we focus on in this study (green boxes).
At first, the server broadcasts the global model to the clients; then, each client trains their version of the model using their dataset for several epochs. When clients are done with their local training, they calculate the updates (the difference between the new model and the model received from the server) and send back the updates to the server. Once the server has all the updates, it aggregates them and updates the global model in the server. This process is a single round of communication, and FL requires multiple rounds of communication till the model converges.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Although FL initially focused on training on mobile devices, it now has a broader definition. FL that works on many devices is called cross-device FL, and if it works on a small number of clients (that are more reliable and accessible like centers), it is called cross-silo FL <cite class="ltx_cite ltx_citemacro_citep">(Kairouz et al., <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">In a cross-device setting, since not all the clients are available and training on all clients is extremely resource-consuming and has a significant communication overhead, only a fraction of clients is selected at each round. Selecting a small fraction can hurt convergence speed, but <cite class="ltx_cite ltx_citemacro_citet">McMahan et al. (<a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite> have shown that a small fraction, such as 10% of clients, can produce good results. In contrast, in cross-silo FL, all the clients are available, and training is done on all of them. Moreover, since the number of clients is small, communication overhead is often not an issue.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">In this paper, we will study how the attacks/faults and defenses can impact the quality of the FL process. To that end, we need to alter three components of the basic FL procedure: aggregator, local model (update), and local dataset, shown as green boxes in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Federated Learning ‣ 2 Background ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The first component is where we implement the defense techniques, and the other two are where attacks and faults are implemented.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Aggregation methods in Federated Learning</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">As discussed, FL aggregates all updates to update the model on the server. There have been many aggregation techniques proposed for FL with different characteristics. We discuss the most important and well-known ones here.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Federated Averaging</span> <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite>:
This technique is the first, most straightforward, and perhaps the most well-known among all aggregation methods. The process of Federated Averaging is relatively straightforward. When the aggregator receives all the client updates, it calculates the mean of all the values for each axis and generates the aggregated model. Federated Averaging has shown to be effective in FL under different data distributions. However, it has a big flaw: it cannot perform well when adversaries operate some clients. That is why new approaches have been proposed under the robust aggregation category, which we will discuss next.
This aggregator is the baseline in all of our experiments as it is the most used non-robust aggregator in related studies
<cite class="ltx_cite ltx_citemacro_citep">(Fung et al., <a href="#bib.bib12" title="" class="ltx_ref">2020</a>; Blanchard et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>; Zhao et al., <a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite>. Other variations of Federated Averaging <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib37" title="" class="ltx_ref">2020</a>; Li et al., <a href="#bib.bib24" title="" class="ltx_ref">2020b</a>)</cite> aim to improve Federated Averaging in non-iid scenarios but are still not designed as robust aggregators. Thus we still use Federated Averaging as the baseline following previous studies.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Krum</span> <cite class="ltx_cite ltx_citemacro_citep">(Blanchard et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>: This is one of the first robust aggregation techniques. The main idea here is that clients that produce similar updates are most likely benign, and choosing any of them as the update is reasonable for the global model update. More specifically, it first calculates the distances between clients’ updates, then sorts the clients based on the sum of their distance to their closest k clients. Where <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="k=n-f-2" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mrow id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml"><mi id="S2.SS2.p3.1.m1.1.1.2" xref="S2.SS2.p3.1.m1.1.1.2.cmml">k</mi><mo id="S2.SS2.p3.1.m1.1.1.1" xref="S2.SS2.p3.1.m1.1.1.1.cmml">=</mo><mrow id="S2.SS2.p3.1.m1.1.1.3" xref="S2.SS2.p3.1.m1.1.1.3.cmml"><mi id="S2.SS2.p3.1.m1.1.1.3.2" xref="S2.SS2.p3.1.m1.1.1.3.2.cmml">n</mi><mo id="S2.SS2.p3.1.m1.1.1.3.1" xref="S2.SS2.p3.1.m1.1.1.3.1.cmml">−</mo><mi id="S2.SS2.p3.1.m1.1.1.3.3" xref="S2.SS2.p3.1.m1.1.1.3.3.cmml">f</mi><mo id="S2.SS2.p3.1.m1.1.1.3.1a" xref="S2.SS2.p3.1.m1.1.1.3.1.cmml">−</mo><mn id="S2.SS2.p3.1.m1.1.1.3.4" xref="S2.SS2.p3.1.m1.1.1.3.4.cmml">2</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><apply id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1"><eq id="S2.SS2.p3.1.m1.1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1.1"></eq><ci id="S2.SS2.p3.1.m1.1.1.2.cmml" xref="S2.SS2.p3.1.m1.1.1.2">𝑘</ci><apply id="S2.SS2.p3.1.m1.1.1.3.cmml" xref="S2.SS2.p3.1.m1.1.1.3"><minus id="S2.SS2.p3.1.m1.1.1.3.1.cmml" xref="S2.SS2.p3.1.m1.1.1.3.1"></minus><ci id="S2.SS2.p3.1.m1.1.1.3.2.cmml" xref="S2.SS2.p3.1.m1.1.1.3.2">𝑛</ci><ci id="S2.SS2.p3.1.m1.1.1.3.3.cmml" xref="S2.SS2.p3.1.m1.1.1.3.3">𝑓</ci><cn type="integer" id="S2.SS2.p3.1.m1.1.1.3.4.cmml" xref="S2.SS2.p3.1.m1.1.1.3.4">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">k=n-f-2</annotation></semantics></math>, n is the number of clients, and f is the number of byzantine clients. Finally, it picks the client with the lowest sum as the update for the model. Since it chooses the update of only one client, it causes some concerns for privacy and its performance under non-iid distributions.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Median</span> <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a href="#bib.bib41" title="" class="ltx_ref">2018</a>)</cite>: Coordinate wise Median is another robust aggregation method. As the name suggests, Median calculates the median of clients’ updates per coordinate (if updates are like arrays, it gets the median for each axis and index) and creates an aggregated update. Intuitively it tries to filter outlier values.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p"><span id="S2.SS2.p5.1.1" class="ltx_text ltx_font_bold">Trimmed Mean</span> <cite class="ltx_cite ltx_citemacro_citep">(Yin et al., <a href="#bib.bib41" title="" class="ltx_ref">2018</a>)</cite>:
This approach is very much like the Federated Averaging. However, instead of getting the mean over all client updates, it first excludes a fraction of the lowest and highest values per coordinate and then calculates the mean.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Attacks in Federated Learning</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Byzantine faults are known issues in distributed settings
<cite class="ltx_cite ltx_citemacro_citep">(Lamport et al., <a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>. Since FL is a distributed technique, it faces byzantine faults and attacks that can cause quality problems. In byzantine attacks, clients collude to achieve goals like breaking the learning process. Byzantine attacks are not to be mistaken with adversarial attacks applied in the testing phase, which try to fool the model by adding perturbations to the data <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow et al., <a href="#bib.bib14" title="" class="ltx_ref">2015</a>; Madry et al., <a href="#bib.bib27" title="" class="ltx_ref">2018</a>)</cite>.
Moreover, clients have access to both data and the training process (local training), so they can cause problems for the entire training process if adversaries operate them. A recent study has categorized FL attacks into two groups: data poisoning and model poisoning <cite class="ltx_cite ltx_citemacro_citep">(Bhagoji et al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>.
The first group is not exclusive to FL, and it has been a problem in centralized training as well <cite class="ltx_cite ltx_citemacro_citep">(Muñoz González et al., <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite>. However, since clients have access to the model and training process, FL must withstand a new category of model poisoning attacks. In model poisoning, adversaries poison the model updates before sending them to the server and attack the learning process.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">More generally, attacks can be categorized into two groups, namely untargeted and targeted <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al., <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>. In untargeted attacks, adversaries want to stop the model from achieving its goal. For instance, the attacker’s goal in the classification task is to make the model misclassify. However, the attacker has a specific goal in a targeted attack, like misclassifying images into a particular label in image classification.
In the following, we introduce some of the most important attacks in FL from both categories since our study aims to investigate how current SOTA FL methods perform against different attacks and whether quality issues would occur.
These attacks are significant due to their success rate and utilization in related works <cite class="ltx_cite ltx_citemacro_citep">(Fang et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>; Bhagoji et al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>; Blanchard et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>; Li et al., <a href="#bib.bib22" title="" class="ltx_ref">2019</a>; Bagdasaryan et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p"><span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_bold">Label Flip</span>: This is an example of a data poisoning attack <cite class="ltx_cite ltx_citemacro_citep">(Fang et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite> in which byzantine clients change labels of their dataset randomly to make the final model inaccurate. Since the changed label is selected based on a uniform distribution, it is considered an untargeted attack. According to <cite class="ltx_cite ltx_citemacro_citet">Bhagoji et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>, this is the most effective attack in the FL setting among data poisoning attacks.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p"><span id="S2.SS3.p4.1.1" class="ltx_text ltx_font_bold">Random Update</span>: This is a model poisoning attack where the adversary does not send an update based on its training. Instead, it sends random Gaussian updates to the server <cite class="ltx_cite ltx_citemacro_citep">(Blanchard et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>; Fang et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite> which also makes it untargeted.
Since the client is sending the update, it can increase the distribution variance to make more powerful attacks. This was impossible in data poisoning attacks (the number of labels that can be altered is limited).</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p"><span id="S2.SS3.p5.1.1" class="ltx_text ltx_font_bold">Sign Flip attack</span>: Another example of a model poisoning attack is Sign Flip, where the attacker trains the model on its dataset and calculates the updates, but it changes the sign of updates and sends an update in the opposite direction <cite class="ltx_cite ltx_citemacro_citep">(Blanchard et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>; Li et al., <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>. The client can also multiply the update to make the attack more practical, like the Random Update. This attack is also untargeted as the attacker does not pursue a particular goal, but it is more directed than the Random Update attack.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para">
<p id="S2.SS3.p6.1" class="ltx_p"><span id="S2.SS3.p6.1.1" class="ltx_text ltx_font_bold">Backdoor attack</span>: In contrast to all previous attacks, this attack is targeted and tries to achieve a specific goal. The Backdoor attack aims to make the model misclassify images containing certain features as a specific class <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib9" title="" class="ltx_ref">2017</a>)</cite>. The Backdoor attack can be categorized into data or model groups based on its implementation. Papers that are not focused on FL, only use data to add a backdoor to the model <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib9" title="" class="ltx_ref">2017</a>; Gu et al., <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>. However, FL papers have used both data and model updates to make the attacks more effective <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>; Sun et al., <a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Bagdasaryan et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite> have shown that Backdoor can be done using a semantic feature like classifying green cars as birds. The semantic feature has a significant advantage over other techniques: it only requires a training time attack, and data does not have to be changed in testing time. Although this makes the semantic feature a great option, this approach is not easily generalizable for different datasets.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-plane-original.png" id="S2.F2.sf1.g1" class="ltx_graphics ltx_img_square" width="120" height="120" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Original image</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-plane-backdoored.png" id="S2.F2.sf2.g1" class="ltx_graphics ltx_img_square" width="120" height="120" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Image with backdoor pixel pattern</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An example of the Backdoor attack (with a pixel pattern) on a sample from the CIFAR-10 dataset that will force the classifier to misclassify a plane as a car.</figcaption>
</figure>
<div id="S2.SS3.p7" class="ltx_para">
<p id="S2.SS3.p7.1" class="ltx_p">Another Backdoor technique is called pixel pattern, which was introduced in <cite class="ltx_cite ltx_citemacro_citep">(Gu et al., <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>. This technique works by adding a small pattern of pixels to the images, which acts as the backdoor. This approach requires a test time attack and a training time attack, but it is more scalable than semantic features.
Figure <a href="#S2.F2" title="Figure 2 ‣ 2.3 Attacks in Federated Learning ‣ 2 Background ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows an example of this type of Backdoor attack. In this example, the ”Z” pattern added to the top left corner of the plane image will make the model misclassify the image (as a car, for instance).</p>
</div>
<div id="S2.SS3.p8" class="ltx_para">
<p id="S2.SS3.p8.1" class="ltx_p">Finally, byzantine clients multiply their updates and make their updates dominant in the aggregation phase to make a more powerful attack. This multiplication is more effective if the model has converged since other updates are small, and byzantine updates can replace the global model. In the Backdoor attack, there should be enough benign samples in the training batch. Otherwise model classifies all samples as the target class, and the main task fails, which is not desirable for the attacker.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Mutation operators in Deep Learning</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Mutation testing is a well-known topic in the Software Engineering community, and recently it has been applied in DL <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a href="#bib.bib16" title="" class="ltx_ref">2019</a>; Shen et al., <a href="#bib.bib33" title="" class="ltx_ref">2018</a>)</cite>. Even though these studies have introduced many mutation operators for DL, many of these operators do not reflect real-world faults. More recently, a new mutation operator set has been proposed that is based on real faults <cite class="ltx_cite ltx_citemacro_citep">(Humbatova et al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>. These operators consist of the following categories: training data, hyper-parameters, activation function, regularisation, weights, loss function, optimization function, and validation.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">In this paper, we leverage mutation testing to simulate the potential quality issues and how they impact the quality of SOTA FL techniques.
In particular, we focus on mutation operators from the data category to simulate actual (potential) faults in the federated context. Other categories are mostly related to the training process, and although clients have access to that, if faults were to happen, they would be happening on all clients as the code for all clients is the same. Thus, it is unlikely that clients can change the model parameters by mistake. As a result, other categories are not applicable in the FL setting (they make more sense in centralized learning).
An important note is that the selected mutators are not specific to FL; rather, they apply to FL. Since there are no FL-specific mutators at the time of conducting this study, we use these FL-applicable mutators.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">Now we discuss mutation operators related to training data.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p"><span id="S2.SS4.p4.1.1" class="ltx_text ltx_font_bold">Change labels of training data</span>: This is like a Label Flip attack, so we will not discuss it more.</p>
</div>
<div id="S2.SS4.p5" class="ltx_para">
<p id="S2.SS4.p5.1" class="ltx_p"><span id="S2.SS4.p5.1.1" class="ltx_text ltx_font_bold">Delete mutator</span>: This operator simulates the lack of training data by removing a portion (we call this delete percentage in our experiments) of training data.</p>
</div>
<div id="S2.SS4.p6" class="ltx_para">
<p id="S2.SS4.p6.1" class="ltx_p"><span id="S2.SS4.p6.1.1" class="ltx_text ltx_font_bold">Unbalance mutator</span>: This operator simulates the situation where data is unbalanced by removing a portion (we call this unbalance percentage in our experiments) of samples in classes that occur less than average.</p>
</div>
<div id="S2.SS4.p7" class="ltx_para">
<p id="S2.SS4.p7.1" class="ltx_p"><span id="S2.SS4.p7.1.1" class="ltx_text ltx_font_bold">Overlap mutator</span>: This operator simulates a situation where very similar samples have different classes. It works by finding the two most dominant classes, then copies a portion (we call this overlap percentage in our experiments) of data from one class and labels it as the other.</p>
</div>
<div id="S2.SS4.p8" class="ltx_para">
<p id="S2.SS4.p8.1" class="ltx_p"><span id="S2.SS4.p8.1.1" class="ltx_text ltx_font_bold">Noise mutator</span>: This imitates a scenario where training data is noisy by adding Gaussian noise to the samples. This mutator takes the variance of the pixels in the image. Then, it adds noise to the image with a mean of zero and a multiply (we call this sigma multiplier in our experiments) of calculated variance.
This mutator can represent a category of faults where the clients’ images are occluded because of the problems with the camera, for instance.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Empirical study</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Objectives and research questions</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The main objective of this paper is to investigate how current SOTA FL performs under different potential quality contexts, such as attacks and faults, using regular and robust aggregators. To achieve this, we mainly consider the following research questions:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">RQ1: How robust is Federated Averaging against attacks and faults in a well-known image domain dataset?</span></p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">In this RQ, we focus on the robustness of the Federated Averaging as the baseline aggregator in FL. We study four attacks and four mutators and consider other factors such as data distribution and the proportion of affected clients to comprehensively study the attack/fault effects on the final results. In this RQ, we use well-known image datasets where we can have control over these factors.
The FL setting for this RQ and RQ2 is cross-device.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">RQ2: How effective are the existing robust aggregation techniques against these attacks and faults?</span></p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">Following the same settings as RQ1, in this RQ, we study FL aggregators that are built as a robust technique along with Federated Averaging and evaluate their robustness against different attacks.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p"><span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_bold">RQ3: In a real-world federated dataset, what is the effect of attacks and faults on different aggregators?</span></p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">This RQ aims to evaluate the aggregation techniques in a cross-silo federated setting where clients are different centers. Note that we do experiments similar to RQ1 and RQ2 just on a real Federated dataset and a cross-silo setting. So in between RQ1 and RQ3, all the experiments are done on both cross-device and cross-silo settings. We will go through more details in the design section.</p>
</div>
<div id="S3.SS1.p8" class="ltx_para">
<p id="S3.SS1.p8.1" class="ltx_p"><span id="S3.SS1.p8.1.1" class="ltx_text ltx_font_bold">RQ4: How robust is an ensemble method that combines existing aggregators in detecting all attacks and mutations in all datasets and configurations?</span></p>
</div>
<div id="S3.SS1.p9" class="ltx_para">
<p id="S3.SS1.p9.1" class="ltx_p">This RQ aims to investigate the possibility of having a general solution that works well for all configurations of untargeted attacks without knowing what attack or fault the system will be facing.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Experiment design</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2201.01409/assets/design.drawio.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="356" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Overview of our experiment design. (Only one of the attacker components, highlighted with grey color, is used in our experiments at a time)</figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">This section describes the design of our empirical study. Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2 Experiment design ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows an overview of our experiments procedure.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Datasets and models</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Image classification is one of the leading DL tasks, and it is being used as a primary subject in many studies in FL <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib28" title="" class="ltx_ref">2017</a>; Blanchard et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>. Thus, we choose this task as our main task.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p"><span id="S3.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Generic datasets</span>:
For the first two questions, which are focused on the generic image dataset, we choose two popular datasets from the image classification task, Fashion MNIST <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a href="#bib.bib40" title="" class="ltx_ref">2017</a>)</cite>, and CIFAR-10 <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky et al., <a href="#bib.bib19" title="" class="ltx_ref">2009</a>)</cite>.
The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6000 images per class. The images are split into 50000 training images and 10000 test images.
Fashion MNIST consists of a training set of 60,000 samples and a test set of 10,000 samples each being a 28x28 gray-scale image. There are a total of 10 classes in this dataset.</p>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p">We use these datasets since they are well-known and well-studied and represent different classification difficulty levels.
Lastly, in RQ1 and RQ2, we study the effects of different datasets and models on the quality of the FL process when it is under byzantine attacks and mutation faults. We also chose these centralized datasets to control how we distribute them and study the effect of data distribution.</p>
</div>
<div id="S3.SS2.SSS1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.p4.8" class="ltx_p"><span id="S3.SS2.SSS1.p4.8.1" class="ltx_text ltx_font_bold">Federated dataset</span>: In RQ3, we aim to study a more realistic scenario and see its effect on FL quality. Since tasks on medical images are one of the main FL applications <cite class="ltx_cite ltx_citemacro_citep">(Kairouz et al., <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>, given patients’ privacy concerns, a distributed medical image dataset is a perfect match for RQ3’s goal. Our medical imaging dataset is obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database <cite class="ltx_cite ltx_citemacro_citep">(Mueller et al., <a href="#bib.bib29" title="" class="ltx_ref">2005</a>)</cite>. The ADNI was launched in 2003 as a public-private partnership led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to assess and model the progression of mild cognitive impairment (MCI) and early Alzheimer’s disease (AD). For up-to-date information, see <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">www.adni-info.org</span>. In this work, we are leveraging 1,723 magnetic resonance images from 594 (<math id="S3.SS2.SSS1.p4.1.m1.1" class="ltx_Math" alttext="34.5\%" display="inline"><semantics id="S3.SS2.SSS1.p4.1.m1.1a"><mrow id="S3.SS2.SSS1.p4.1.m1.1.1" xref="S3.SS2.SSS1.p4.1.m1.1.1.cmml"><mn id="S3.SS2.SSS1.p4.1.m1.1.1.2" xref="S3.SS2.SSS1.p4.1.m1.1.1.2.cmml">34.5</mn><mo id="S3.SS2.SSS1.p4.1.m1.1.1.1" xref="S3.SS2.SSS1.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.1.m1.1b"><apply id="S3.SS2.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS1.p4.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS1.p4.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1.2">34.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.1.m1.1c">34.5\%</annotation></semantics></math>) Alzheimer’s patients and
1,129 (<math id="S3.SS2.SSS1.p4.2.m2.1" class="ltx_Math" alttext="65.5\%" display="inline"><semantics id="S3.SS2.SSS1.p4.2.m2.1a"><mrow id="S3.SS2.SSS1.p4.2.m2.1.1" xref="S3.SS2.SSS1.p4.2.m2.1.1.cmml"><mn id="S3.SS2.SSS1.p4.2.m2.1.1.2" xref="S3.SS2.SSS1.p4.2.m2.1.1.2.cmml">65.5</mn><mo id="S3.SS2.SSS1.p4.2.m2.1.1.1" xref="S3.SS2.SSS1.p4.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.2.m2.1b"><apply id="S3.SS2.SSS1.p4.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p4.2.m2.1.1"><csymbol cd="latexml" id="S3.SS2.SSS1.p4.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p4.2.m2.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS1.p4.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p4.2.m2.1.1.2">65.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.2.m2.1c">65.5\%</annotation></semantics></math>) presumed normal controls. These data were collected from six different centers that are used to simulate our federation. The number of data samples is unbalanced across centers (A: <math id="S3.SS2.SSS1.p4.3.m3.1" class="ltx_Math" alttext="15.7\%" display="inline"><semantics id="S3.SS2.SSS1.p4.3.m3.1a"><mrow id="S3.SS2.SSS1.p4.3.m3.1.1" xref="S3.SS2.SSS1.p4.3.m3.1.1.cmml"><mn id="S3.SS2.SSS1.p4.3.m3.1.1.2" xref="S3.SS2.SSS1.p4.3.m3.1.1.2.cmml">15.7</mn><mo id="S3.SS2.SSS1.p4.3.m3.1.1.1" xref="S3.SS2.SSS1.p4.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.3.m3.1b"><apply id="S3.SS2.SSS1.p4.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p4.3.m3.1.1"><csymbol cd="latexml" id="S3.SS2.SSS1.p4.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p4.3.m3.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS1.p4.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p4.3.m3.1.1.2">15.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.3.m3.1c">15.7\%</annotation></semantics></math>, B:<math id="S3.SS2.SSS1.p4.4.m4.1" class="ltx_Math" alttext="16.7\%" display="inline"><semantics id="S3.SS2.SSS1.p4.4.m4.1a"><mrow id="S3.SS2.SSS1.p4.4.m4.1.1" xref="S3.SS2.SSS1.p4.4.m4.1.1.cmml"><mn id="S3.SS2.SSS1.p4.4.m4.1.1.2" xref="S3.SS2.SSS1.p4.4.m4.1.1.2.cmml">16.7</mn><mo id="S3.SS2.SSS1.p4.4.m4.1.1.1" xref="S3.SS2.SSS1.p4.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.4.m4.1b"><apply id="S3.SS2.SSS1.p4.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p4.4.m4.1.1"><csymbol cd="latexml" id="S3.SS2.SSS1.p4.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.p4.4.m4.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS1.p4.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.p4.4.m4.1.1.2">16.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.4.m4.1c">16.7\%</annotation></semantics></math>, C:<math id="S3.SS2.SSS1.p4.5.m5.1" class="ltx_Math" alttext="4.3\%" display="inline"><semantics id="S3.SS2.SSS1.p4.5.m5.1a"><mrow id="S3.SS2.SSS1.p4.5.m5.1.1" xref="S3.SS2.SSS1.p4.5.m5.1.1.cmml"><mn id="S3.SS2.SSS1.p4.5.m5.1.1.2" xref="S3.SS2.SSS1.p4.5.m5.1.1.2.cmml">4.3</mn><mo id="S3.SS2.SSS1.p4.5.m5.1.1.1" xref="S3.SS2.SSS1.p4.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.5.m5.1b"><apply id="S3.SS2.SSS1.p4.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p4.5.m5.1.1"><csymbol cd="latexml" id="S3.SS2.SSS1.p4.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.p4.5.m5.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS1.p4.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.p4.5.m5.1.1.2">4.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.5.m5.1c">4.3\%</annotation></semantics></math>, D:<math id="S3.SS2.SSS1.p4.6.m6.1" class="ltx_Math" alttext="12.5\%" display="inline"><semantics id="S3.SS2.SSS1.p4.6.m6.1a"><mrow id="S3.SS2.SSS1.p4.6.m6.1.1" xref="S3.SS2.SSS1.p4.6.m6.1.1.cmml"><mn id="S3.SS2.SSS1.p4.6.m6.1.1.2" xref="S3.SS2.SSS1.p4.6.m6.1.1.2.cmml">12.5</mn><mo id="S3.SS2.SSS1.p4.6.m6.1.1.1" xref="S3.SS2.SSS1.p4.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.6.m6.1b"><apply id="S3.SS2.SSS1.p4.6.m6.1.1.cmml" xref="S3.SS2.SSS1.p4.6.m6.1.1"><csymbol cd="latexml" id="S3.SS2.SSS1.p4.6.m6.1.1.1.cmml" xref="S3.SS2.SSS1.p4.6.m6.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS1.p4.6.m6.1.1.2.cmml" xref="S3.SS2.SSS1.p4.6.m6.1.1.2">12.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.6.m6.1c">12.5\%</annotation></semantics></math>, E:<math id="S3.SS2.SSS1.p4.7.m7.1" class="ltx_Math" alttext="14.9\%" display="inline"><semantics id="S3.SS2.SSS1.p4.7.m7.1a"><mrow id="S3.SS2.SSS1.p4.7.m7.1.1" xref="S3.SS2.SSS1.p4.7.m7.1.1.cmml"><mn id="S3.SS2.SSS1.p4.7.m7.1.1.2" xref="S3.SS2.SSS1.p4.7.m7.1.1.2.cmml">14.9</mn><mo id="S3.SS2.SSS1.p4.7.m7.1.1.1" xref="S3.SS2.SSS1.p4.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.7.m7.1b"><apply id="S3.SS2.SSS1.p4.7.m7.1.1.cmml" xref="S3.SS2.SSS1.p4.7.m7.1.1"><csymbol cd="latexml" id="S3.SS2.SSS1.p4.7.m7.1.1.1.cmml" xref="S3.SS2.SSS1.p4.7.m7.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS1.p4.7.m7.1.1.2.cmml" xref="S3.SS2.SSS1.p4.7.m7.1.1.2">14.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.7.m7.1c">14.9\%</annotation></semantics></math>, F:<math id="S3.SS2.SSS1.p4.8.m8.1" class="ltx_Math" alttext="35.9\%" display="inline"><semantics id="S3.SS2.SSS1.p4.8.m8.1a"><mrow id="S3.SS2.SSS1.p4.8.m8.1.1" xref="S3.SS2.SSS1.p4.8.m8.1.1.cmml"><mn id="S3.SS2.SSS1.p4.8.m8.1.1.2" xref="S3.SS2.SSS1.p4.8.m8.1.1.2.cmml">35.9</mn><mo id="S3.SS2.SSS1.p4.8.m8.1.1.1" xref="S3.SS2.SSS1.p4.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.8.m8.1b"><apply id="S3.SS2.SSS1.p4.8.m8.1.1.cmml" xref="S3.SS2.SSS1.p4.8.m8.1.1"><csymbol cd="latexml" id="S3.SS2.SSS1.p4.8.m8.1.1.1.cmml" xref="S3.SS2.SSS1.p4.8.m8.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS1.p4.8.m8.1.1.2.cmml" xref="S3.SS2.SSS1.p4.8.m8.1.1.2">35.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.8.m8.1c">35.9\%</annotation></semantics></math>). We extract the 10 central slices in the axial plane of the images to train and test our models.</p>
</div>
<div id="S3.SS2.SSS1.p5" class="ltx_para">
<p id="S3.SS2.SSS1.p5.1" class="ltx_p"><span id="S3.SS2.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Data distribution</span>:
There is no need for synthetic distribution for the ADNI dataset since it is already distributed. However, since CIFAR-10 and Fashion MNIST datasets are centralized, we partition them and distribute samples to different clients. Furthermore, because we want to see the effect of the distribution of the dataset in RQ1 and RQ2, we partition these datasets with different degrees of non-iid.
We include both iid (uniformly distributed between clients), and non-iid distributions since an iid distribution represents an ideal case. However, in real-world FL scenarios, the data tends to be more non-iid <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S3.SS2.SSS1.p6" class="ltx_para">
<p id="S3.SS2.SSS1.p6.1" class="ltx_p">To simulate different iid distributions, we use a method similar to the method used in the original FL paper <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite>, but we add multiple non-iid options. We first group the images based on the classes and then split each group’s samples into chunks of equal size (this size increases with the non-iid parameter and depends on the number of clients).
Then for each client, we select a subset of groups randomly, and from each group, we select a chunk randomly and assign it to that client.
This process finishes when all chunks are assigned to the clients.
The number of selected groups for each client depends on the non-iid degree, and with the increase of non-iid degree, the number of selected groups decreases. However, all clients receive the same amount of samples as chunks are larger for higher non-iid degrees.</p>
</div>
<div id="S3.SS2.SSS1.p7" class="ltx_para">
<p id="S3.SS2.SSS1.p7.1" class="ltx_p">For instance, all groups will be selected with a non-iid degree of 0, and clients will get data from all classes. However, in a high non-iid scenario, only a couple of groups will be selected, and each client will have samples from only some classes.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.T1.sf1" class="ltx_table ltx_figure_panel ltx_align_center">
<table id="S3.T1.sf1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.sf1.1.1.1" class="ltx_tr">
<th id="S3.T1.sf1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf1.1.1.1.1.1.1" class="ltx_p" style="width:69.4pt;">Layer</span>
</span>
</th>
<th id="S3.T1.sf1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Output Shape</th>
<th id="S3.T1.sf1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Param #</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.sf1.1.2.1" class="ltx_tr">
<td id="S3.T1.sf1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf1.1.2.1.1.1.1" class="ltx_p" style="width:69.4pt;">conv2d_0</span>
</span>
</td>
<td id="S3.T1.sf1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 26, 26, 32)</td>
<td id="S3.T1.sf1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">320</td>
</tr>
<tr id="S3.T1.sf1.1.3.2" class="ltx_tr">
<td id="S3.T1.sf1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf1.1.3.2.1.1.1" class="ltx_p" style="width:69.4pt;">max_pooling2d_0</span>
</span>
</td>
<td id="S3.T1.sf1.1.3.2.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 13, 13, 32)</td>
<td id="S3.T1.sf1.1.3.2.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
</tr>
<tr id="S3.T1.sf1.1.4.3" class="ltx_tr">
<td id="S3.T1.sf1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf1.1.4.3.1.1.1" class="ltx_p" style="width:69.4pt;">conv2d_1</span>
</span>
</td>
<td id="S3.T1.sf1.1.4.3.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 11, 11, 64)</td>
<td id="S3.T1.sf1.1.4.3.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">18496</td>
</tr>
<tr id="S3.T1.sf1.1.5.4" class="ltx_tr">
<td id="S3.T1.sf1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf1.1.5.4.1.1.1" class="ltx_p" style="width:69.4pt;">max_pooling2d_1</span>
</span>
</td>
<td id="S3.T1.sf1.1.5.4.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 5, 5, 64)</td>
<td id="S3.T1.sf1.1.5.4.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
</tr>
<tr id="S3.T1.sf1.1.6.5" class="ltx_tr">
<td id="S3.T1.sf1.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf1.1.6.5.1.1.1" class="ltx_p" style="width:69.4pt;">flatten_0</span>
</span>
</td>
<td id="S3.T1.sf1.1.6.5.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 1600)</td>
<td id="S3.T1.sf1.1.6.5.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
</tr>
<tr id="S3.T1.sf1.1.7.6" class="ltx_tr">
<td id="S3.T1.sf1.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf1.1.7.6.1.1.1" class="ltx_p" style="width:69.4pt;">dropout_0</span>
</span>
</td>
<td id="S3.T1.sf1.1.7.6.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 1600)</td>
<td id="S3.T1.sf1.1.7.6.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
</tr>
<tr id="S3.T1.sf1.1.8.7" class="ltx_tr">
<td id="S3.T1.sf1.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf1.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf1.1.8.7.1.1.1" class="ltx_p" style="width:69.4pt;">dense_0</span>
</span>
</td>
<td id="S3.T1.sf1.1.8.7.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 10)</td>
<td id="S3.T1.sf1.1.8.7.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">16010</td>
</tr>
<tr id="S3.T1.sf1.1.9.8" class="ltx_tr">
<td id="S3.T1.sf1.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf1.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf1.1.9.8.1.1.1" class="ltx_p" style="width:69.4pt;"><span id="S3.T1.sf1.1.9.8.1.1.1.1" class="ltx_text ltx_align_left ltx_inline-block" style="width:0.0pt;">Total params: 34,826</span></span>
</span>
</td>
<td id="S3.T1.sf1.1.9.8.2" class="ltx_td ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S3.T1.sf1.1.9.8.3" class="ltx_td ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr id="S3.T1.sf1.1.10.9" class="ltx_tr">
<td id="S3.T1.sf1.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf1.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf1.1.10.9.1.1.1" class="ltx_p" style="width:69.4pt;"><span id="S3.T1.sf1.1.10.9.1.1.1.1" class="ltx_text ltx_align_left ltx_inline-block" style="width:0.0pt;">Trainable params: 34,826</span></span>
</span>
</td>
<td id="S3.T1.sf1.1.10.9.2" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S3.T1.sf1.1.10.9.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr id="S3.T1.sf1.1.11.10" class="ltx_tr">
<td id="S3.T1.sf1.1.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf1.1.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf1.1.11.10.1.1.1" class="ltx_p" style="width:69.4pt;"><span id="S3.T1.sf1.1.11.10.1.1.1.1" class="ltx_text ltx_align_left ltx_inline-block" style="width:0.0pt;">Non-trainable params: 0</span></span>
</span>
</td>
<td id="S3.T1.sf1.1.11.10.2" class="ltx_td ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S3.T1.sf1.1.11.10.3" class="ltx_td ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(a) </span>Fashion MNIST</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.T1.sf2" class="ltx_table ltx_figure_panel ltx_align_center">
<table id="S3.T1.sf2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.sf2.1.1.1" class="ltx_tr">
<th id="S3.T1.sf2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf2.1.1.1.1.1.1" class="ltx_p" style="width:69.4pt;">Layer</span>
</span>
</th>
<th id="S3.T1.sf2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Output Shape</th>
<th id="S3.T1.sf2.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Param #</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.sf2.1.2.1" class="ltx_tr">
<td id="S3.T1.sf2.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf2.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf2.1.2.1.1.1.1" class="ltx_p" style="width:69.4pt;">conv2d_0</span>
</span>
</td>
<td id="S3.T1.sf2.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 32, 32, 32)</td>
<td id="S3.T1.sf2.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">896</td>
</tr>
<tr id="S3.T1.sf2.1.3.2" class="ltx_tr">
<td id="S3.T1.sf2.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf2.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf2.1.3.2.1.1.1" class="ltx_p" style="width:69.4pt;">conv2d_1</span>
</span>
</td>
<td id="S3.T1.sf2.1.3.2.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 30, 30, 32)</td>
<td id="S3.T1.sf2.1.3.2.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">9248</td>
</tr>
<tr id="S3.T1.sf2.1.4.3" class="ltx_tr">
<td id="S3.T1.sf2.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf2.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf2.1.4.3.1.1.1" class="ltx_p" style="width:69.4pt;">max_pooling2d_0</span>
</span>
</td>
<td id="S3.T1.sf2.1.4.3.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 15, 15, 32)</td>
<td id="S3.T1.sf2.1.4.3.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
</tr>
<tr id="S3.T1.sf2.1.5.4" class="ltx_tr">
<td id="S3.T1.sf2.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf2.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf2.1.5.4.1.1.1" class="ltx_p" style="width:69.4pt;">dropout_0</span>
</span>
</td>
<td id="S3.T1.sf2.1.5.4.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 15, 15, 32)</td>
<td id="S3.T1.sf2.1.5.4.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
</tr>
<tr id="S3.T1.sf2.1.6.5" class="ltx_tr">
<td id="S3.T1.sf2.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf2.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf2.1.6.5.1.1.1" class="ltx_p" style="width:69.4pt;">conv2d_2</span>
</span>
</td>
<td id="S3.T1.sf2.1.6.5.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 15, 15, 64)</td>
<td id="S3.T1.sf2.1.6.5.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">18496</td>
</tr>
<tr id="S3.T1.sf2.1.7.6" class="ltx_tr">
<td id="S3.T1.sf2.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf2.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf2.1.7.6.1.1.1" class="ltx_p" style="width:69.4pt;">conv2d_3</span>
</span>
</td>
<td id="S3.T1.sf2.1.7.6.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 13, 13, 64)</td>
<td id="S3.T1.sf2.1.7.6.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">36928</td>
</tr>
<tr id="S3.T1.sf2.1.8.7" class="ltx_tr">
<td id="S3.T1.sf2.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf2.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf2.1.8.7.1.1.1" class="ltx_p" style="width:69.4pt;">max_pooling2d_1</span>
</span>
</td>
<td id="S3.T1.sf2.1.8.7.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 6, 6, 64)</td>
<td id="S3.T1.sf2.1.8.7.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
</tr>
<tr id="S3.T1.sf2.1.9.8" class="ltx_tr">
<td id="S3.T1.sf2.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf2.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf2.1.9.8.1.1.1" class="ltx_p" style="width:69.4pt;">dropout_1</span>
</span>
</td>
<td id="S3.T1.sf2.1.9.8.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 6, 6, 64)</td>
<td id="S3.T1.sf2.1.9.8.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
</tr>
<tr id="S3.T1.sf2.1.10.9" class="ltx_tr">
<td id="S3.T1.sf2.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf2.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf2.1.10.9.1.1.1" class="ltx_p" style="width:69.4pt;">flatten_0</span>
</span>
</td>
<td id="S3.T1.sf2.1.10.9.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 2304)</td>
<td id="S3.T1.sf2.1.10.9.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
</tr>
<tr id="S3.T1.sf2.1.11.10" class="ltx_tr">
<td id="S3.T1.sf2.1.11.10.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf2.1.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf2.1.11.10.1.1.1" class="ltx_p" style="width:69.4pt;">dense_0</span>
</span>
</td>
<td id="S3.T1.sf2.1.11.10.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 512)</td>
<td id="S3.T1.sf2.1.11.10.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">1180160</td>
</tr>
<tr id="S3.T1.sf2.1.12.11" class="ltx_tr">
<td id="S3.T1.sf2.1.12.11.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf2.1.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf2.1.12.11.1.1.1" class="ltx_p" style="width:69.4pt;">dropout_2</span>
</span>
</td>
<td id="S3.T1.sf2.1.12.11.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 512)</td>
<td id="S3.T1.sf2.1.12.11.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">0</td>
</tr>
<tr id="S3.T1.sf2.1.13.12" class="ltx_tr">
<td id="S3.T1.sf2.1.13.12.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf2.1.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf2.1.13.12.1.1.1" class="ltx_p" style="width:69.4pt;">dense_1</span>
</span>
</td>
<td id="S3.T1.sf2.1.13.12.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(None, 10)</td>
<td id="S3.T1.sf2.1.13.12.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">5130</td>
</tr>
<tr id="S3.T1.sf2.1.14.13" class="ltx_tr">
<td id="S3.T1.sf2.1.14.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf2.1.14.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf2.1.14.13.1.1.1" class="ltx_p" style="width:69.4pt;"><span id="S3.T1.sf2.1.14.13.1.1.1.1" class="ltx_text ltx_align_left ltx_inline-block" style="width:0.0pt;">Total params: 1,250,858</span></span>
</span>
</td>
<td id="S3.T1.sf2.1.14.13.2" class="ltx_td ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S3.T1.sf2.1.14.13.3" class="ltx_td ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr id="S3.T1.sf2.1.15.14" class="ltx_tr">
<td id="S3.T1.sf2.1.15.14.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf2.1.15.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf2.1.15.14.1.1.1" class="ltx_p" style="width:69.4pt;"><span id="S3.T1.sf2.1.15.14.1.1.1.1" class="ltx_text ltx_align_left ltx_inline-block" style="width:0.0pt;">Trainable params: 1,250,858</span></span>
</span>
</td>
<td id="S3.T1.sf2.1.15.14.2" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S3.T1.sf2.1.15.14.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
<tr id="S3.T1.sf2.1.16.15" class="ltx_tr">
<td id="S3.T1.sf2.1.16.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S3.T1.sf2.1.16.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.sf2.1.16.15.1.1.1" class="ltx_p" style="width:69.4pt;"><span id="S3.T1.sf2.1.16.15.1.1.1.1" class="ltx_text ltx_align_left ltx_inline-block" style="width:0.0pt;">Non-trainable params: 0</span></span>
</span>
</td>
<td id="S3.T1.sf2.1.16.15.2" class="ltx_td ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S3.T1.sf2.1.16.15.3" class="ltx_td ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(b) </span>CIFAR-10</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Model summary for generic datasets.</figcaption>
</figure>
<div id="S3.SS2.SSS1.p8" class="ltx_para">
<p id="S3.SS2.SSS1.p8.1" class="ltx_p"><span id="S3.SS2.SSS1.p8.1.1" class="ltx_text ltx_font_bold">Models</span>:
Our models for generic datasets are simple convolutional neural networks with 12 and 7 layers for CIFAR-10 and Fashion MNIST, respectively, which are taken from Keras’ tutorials and examples <cite class="ltx_cite ltx_citemacro_citep">(cif, <a href="#bib.bib1" title="" class="ltx_ref">2021</a>; mni, <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>. Table <a href="#S3.T1" title="Table 1 ‣ 3.2.1 Datasets and models ‣ 3.2 Experiment design ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows a summary of the models used for Fashion MNIST and CIFAR-10 datasets.
For the ADNI dataset, we use a transfer learning (TL) approach using the VGG16 model <cite class="ltx_cite ltx_citemacro_citep">(Simonyan and Zisserman, <a href="#bib.bib34" title="" class="ltx_ref">2015</a>)</cite> pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Russakovsky et al., <a href="#bib.bib32" title="" class="ltx_ref">2015</a>)</cite>. Our classifier consists of one dense layer with 512 neurons and a rectified linear activation followed by a dense layer with two neurons and a softmax activation. The VGG16 weights were frozen during training.</p>
</div>
<div id="S3.SS2.SSS1.p9" class="ltx_para">
<p id="S3.SS2.SSS1.p9.1" class="ltx_p">An important note is that these model architectures have many different parameters that can be tuned to ensure higher robustness against attacks. However, our study focuses on aggregator techniques in FL to improve the overall robustness. Consequently, we use the models exactly as they were proposed.
Lastly, Our selected models for the experiments ensure the variety in our cases and help our results be generalizable.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Attacks and faults</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">We use attacks described in Section <a href="#S2.SS3" title="2.3 Attacks in Federated Learning ‣ 2 Background ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a> namely: Label Flip attack, Random Update attack, Sign Flip attack, and Backdoor attack.
Our choice of attacks contains both untargeted and targeted attacks. Moreover, we have attacks from both data poisoning and model poisoning categories, which again helps with the generalizability of our study.
We set the mean and standard deviation of Gaussian distribution for Random Update zero and two, respectively. We set the multiplier for sign attack to 10. We choose the pixel pattern technique discussed before for the Backdoor attack as it makes it generalizable for different datasets, and we set the update multiplier to 10. These numbers were selected based on empirical evaluations and how they were used in related works to ensure the attacks’ effectiveness <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>; Blanchard et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">To simulate faults, we choose all data-related mutations discussed in Section <a href="#S2.SS4" title="2.4 Mutation operators in Deep Learning ‣ 2 Background ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>. We set the delete percentage, unbalance percentage, and overlap percentage discussed in the mutations background section to 75%. Also, we set the sigma multiplier for the Noise mutator to one. Since these mutators were not tested on FL before, we show the reason behind these choices in the RQ1 section.
Moreover, since we are investigating FL-related (by this term, we mean faults that can happen in FL and are not exclusive to FL) faults, we need to consider what faults can be caused by clients’ mistakes. The choice of model-related parameters and mutation operators makes it more of a centralized choice, not a client choice.
Note that although the faults we cover are not exclusive to FL, they apply to FL since FL is a federated version of a regular learning system, so analyzing their impact in FL is important to see how they can affect the overall system quality, just like a normal learning system.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">Finally, to thoroughly investigate how much damage attacks and faults can cause in FL, we choose three different proportions of affected clients: 0.1, 0.3, and 0.5, and study the effect on the final model.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Aggregation methods</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">As discussed in Section <a href="#S2.SS2" title="2.2 Aggregation methods in Federated Learning ‣ 2 Background ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>, we select Federated Averaging as our baseline method and Krum, Median, and Trimmed Mean as the robust aggregation methods under study, to represent the most well-known aggregation methods from the literature. We set the hyperparameters for Krum and Trimmed Mean based on the expected number of malicious clients.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">To show the feasibility of an ensemble aggregator, in RQ4, we choose two attacks, Label Flip and Sign Flip, to have both data and model poisoning attacks (we discuss these choices more in the results section of RQ4). Our ensemble aggregator performs the aggregation with all four mentioned aggregators, then picks the aggregated update with the best validation accuracy. This process happens at each round independently until the training is complete. A note about this approach is that the ensemble does not create a weighted average of all the aggregators. Rather, it selects the result of the best one. Thus, it does not require weight tuning.</p>
</div>
</section>
<section id="S3.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4 </span>Federated setup</h4>

<div id="S3.SS2.SSS4.p1" class="ltx_para">
<p id="S3.SS2.SSS4.p1.1" class="ltx_p">For the RQ1 and RQ2, we follow federated settings in <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite>. We distribute data to 100 clients and select 10 of them randomly each round to simulate a cross-device setting. Local epoch and batch size are set to 5 and 10, respectively. Furthermore, we repeat experiments for each dataset with three levels of non-iid: 0, 0.4, and 0.7. The number of training rounds depends on the dataset and how fast the model can converge. For Fashion MNIST, it is set to 100, and for CIFAR-10, it is set to 1,000.</p>
</div>
<div id="S3.SS2.SSS4.p2" class="ltx_para">
<p id="S3.SS2.SSS4.p2.1" class="ltx_p">For the RQ3, since we have six centers in the ADNI dataset and a cross-silo setting, we select all clients at each round for training. Moreover, we set the batch size to 32, the local epoch is set to one, and the number of training rounds is 100.</p>
</div>
<div id="S3.SS2.SSS4.p3" class="ltx_para">
<p id="S3.SS2.SSS4.p3.1" class="ltx_p">In RQ4, we conduct experiments on the CIFAR-10 (with the non-iid degree of 0.4) and ADNI datasets to compare our proposed aggregator with existing ones.</p>
</div>
<div id="S3.SS2.SSS4.p4" class="ltx_para">
<p id="S3.SS2.SSS4.p4.1" class="ltx_p">Moreover, in all of the comparisons, we compare the accuracies and also run non-parametric statistical significance tests (Mann–Whitney U test with p_value less than 0.05) on the 10 runs of each configuration per aggregator to show that the results are not due to chance.</p>
</div>
<div id="S3.SS2.SSS4.p5" class="ltx_para">
<p id="S3.SS2.SSS4.p5.1" class="ltx_p">Finally, in all RQs, to eliminate the randomness introduced in different steps of the experiment, we run all configurations 10 times and always report the <span id="S3.SS2.SSS4.p5.1.1" class="ltx_text ltx_font_italic">median</span> of 10 runs. So all accuracy values in figures are median accuracies. The reason behind this choice is that the median is better at filtering the outlier values that might happen in the 10 runs, which will result in more consistent results.</p>
</div>
<div id="S3.SS2.SSS4.p6" class="ltx_para">
<p id="S3.SS2.SSS4.p6.1" class="ltx_p">Furthermore, the mean and std values reported in the tables are taken across different configurations (e.g., non-iid degree) described in their related sections. The mean is taken over different configurations instead of the median since we want to consider all configurations equally in the final result. These mean and std values should not be confused with the median taken across different runs (mean and std are applied to the results of different configurations reported in different figures.). Also, other metrics like the median of different configurations and also the raw results for each configuration can be found in our replication package.</p>
</div>
</section>
<section id="S3.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.5 </span>Threat model</h4>

<div id="S3.SS2.SSS5.p1" class="ltx_para">
<p id="S3.SS2.SSS5.p1.1" class="ltx_p">We assume the attacker has complete control of a proportion of clients; thus, they can alter their model updates sent to the server and poison the training data. Furthermore, the devices can collude to make attacks more powerful like <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>; Zhao et al., <a href="#bib.bib42" title="" class="ltx_ref">2021</a>; Blanchard et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>. However, the attacker does not know the server’s aggregation method, which is the same assumption used by previous works <cite class="ltx_cite ltx_citemacro_citep">(Blanchard et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>; Fang et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S3.SS2.SSS5.p2" class="ltx_para">
<p id="S3.SS2.SSS5.p2.1" class="ltx_p">In our opinion, some aggregators like Krum need to know the number of attacked clients, which is an unrealistic assumption. However, given that it has been used extensively before, <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib25" title="" class="ltx_ref">2021</a>; Fang et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>, we also include this configuration in our study.
It might be possible to make Krum work without knowing the exact number of attackers (using an estimation approach), but that is not the focus of our study.</p>
</div>
<div id="S3.SS2.SSS5.p3" class="ltx_para">
<p id="S3.SS2.SSS5.p3.1" class="ltx_p">Lastly, the server and the aggregation techniques are assumed to be uncompromised following previous studies’ settings <cite class="ltx_cite ltx_citemacro_citep">(Fung et al., <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS2.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.6 </span>Execution setup and environment</h4>

<div id="S3.SS2.SSS6.p1" class="ltx_para">
<p id="S3.SS2.SSS6.p1.1" class="ltx_p">All experiments are done on Compute Canada cluster nodes with 4 CPU (Intel Gold 6148 Skylake @ 2.4 GHz) cores, a GPU (NVidia V100SXM2 with 16GB memory and 125 TFLOPS computation power), and 64GB of RAM running on CentOS 7. We use TensorFlow 2.4 <cite class="ltx_cite ltx_citemacro_citep">(Abadi et al., <a href="#bib.bib3" title="" class="ltx_ref">2015</a>)</cite> as our DL framework and Python 3.8 and simulate FL on a single machine.</p>
</div>
</section>
<section id="S3.SS2.SSS7" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.7 </span>Evaluation metrics</h4>

<div id="S3.SS2.SSS7.p1" class="ltx_para">
<p id="S3.SS2.SSS7.p1.1" class="ltx_p">We use the model’s prediction accuracy on test split as our metric for all attacks and mutators except the Backdoor attack.
In the Backdoor attack, we use the accuracy of the backdoor task as the metric and not the accuracy of the main task. The reason is that the backdoor attack is trying to fool the model on the backdoor task and not the main task so the main task accuracy is virtually the same as a clean scenario. Furthermore, we choose this metric to be consistent with related works that study the backdoor task <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>; Bhagoji et al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>; Sun et al., <a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>. In contrast to other attacks, the Backdoor attack’s effectiveness directly correlates with this metric.</p>
</div>
<div id="S3.SS2.SSS7.p2" class="ltx_para">
<p id="S3.SS2.SSS7.p2.1" class="ltx_p">Finally, whenever we mention accuracy in the rest of the study, we mean prediction accuracy. Also, when we want to talk about the accuracy of the backdoor task, we use the term backdoor accuracy.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Experiment results</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In this section, we discuss the results for RQ1 to RQ4.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">To avoid confusion, we first look at the effect of faults (mutators) in all sections, then we discuss the attacks on FL.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span><span id="S3.SS3.SSS1.1.1" class="ltx_text ltx_font_bold">RQ1 results (effect of attacks and faults on Federated Averaging):</span>
</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">The mutators, as discussed in <a href="#S2.SS4" title="2.4 Mutation operators in Deep Learning ‣ 2 Background ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a> have different parameters that can determine how effective they can be. To ensure we have chosen reasonable parameters, we do some preliminary experiments on CIFAR-10 with the non-iid level of 0.4 while the proportion of the affected clients is set to 0.3.
We test three different values for the mutators that alter a percentage of the data: 25%, 50%, and 75%. We choose 0.1, 0.5, and 1 as the candidates for the Noise mutator’s sigma multiplier.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-params-deletedata.png" id="S3.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="149" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Delete Mutator</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-params-unbalancedata.png" id="S3.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="240" height="149" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Unbalance Mutator</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-params-overlapdata.png" id="S3.F4.sf3.g1" class="ltx_graphics ltx_img_landscape" width="240" height="149" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Overlap Mutator</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F4.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-params-noisedata.png" id="S3.F4.sf4.g1" class="ltx_graphics ltx_img_landscape" width="240" height="149" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Noise Mutator</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Effect of mutator parameters on CIFAR-10 datset with non-iid level of 0.4 and affected clients proportion of 0.3.</figcaption>
</figure>
<div id="S3.SS3.SSS1.p2" class="ltx_para">
<p id="S3.SS3.SSS1.p2.1" class="ltx_p">The results are reported in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
As shown in Figures <a href="#S3.F4.sf1" title="In Figure 4 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4a</span></a> to <a href="#S3.F4.sf3" title="In Figure 4 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4c</span></a>, increasing the percentage parameter of Delete, Unbalance, and Overlap mutators does not have a significant impact on the final accuracy. However, in all cases, 75% gives the best results. So for these mutators, we set the percentage parameter to 75%.</p>
</div>
<div id="S3.SS3.SSS1.p3" class="ltx_para">
<p id="S3.SS3.SSS1.p3.1" class="ltx_p">Nevertheless, increasing the sigma multiplier seems to have a more noticeable effect on the Noise mutator, as shown in <a href="#S3.F4.sf4" title="In Figure 4 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4d</span></a>. Given these results, we set the sigma multiplier to be one in the remainder of our experiments.</p>
</div>
<div id="S3.SS3.SSS1.p4" class="ltx_para">
<p id="S3.SS3.SSS1.p4.1" class="ltx_p">However, unlike the other mutators that could have a max percentage of 100%, the sigma multiplier does not have a max sealing here. So a valid question might be why we do not set the multiplier even higher?</p>
</div>
<figure id="S3.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S3.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-sigma-0.png" id="S3.F5.sf1.g1" class="ltx_graphics ltx_img_square" width="120" height="120" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Original image</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S3.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-sigma-0.1.png" id="S3.F5.sf2.g1" class="ltx_graphics ltx_img_square" width="120" height="120" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>multiplier=0.1</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S3.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-sigma-0.5.png" id="S3.F5.sf3.g1" class="ltx_graphics ltx_img_square" width="120" height="120" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>multiplier=0.5</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S3.F5.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-sigma-1.png" id="S3.F5.sf4.g1" class="ltx_graphics ltx_img_square" width="120" height="120" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>multiplier=1</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S3.F5.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-sigma-2.png" id="S3.F5.sf5.g1" class="ltx_graphics ltx_img_square" width="120" height="120" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>multiplier=2</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Effect of the Noise mutator’s sigma multiplier on a sample from CIFAR-10 datset.</figcaption>
</figure>
<div id="S3.SS3.SSS1.p5" class="ltx_para">
<p id="S3.SS3.SSS1.p5.1" class="ltx_p">To address this concern, we show how the image is affected when it gets mutated by the Noise mutator with different sigma multipliers in Figure <a href="#S3.F5" title="Figure 5 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. As the sigma multiplier increases, the image gets noisier as expected. However, if we increase it too much, the image gets unrecognizable, like in Figure <a href="#S3.F5.sf5" title="In Figure 5 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5e</span></a>. Since the Noise mutator is supposed to simulate faults, extreme values for the sigma multiplier become unacceptable, and this is the reason that we set this parameter to be one in our experiments.</p>
</div>
<div id="S3.SS3.SSS1.p6" class="ltx_para">
<p id="S3.SS3.SSS1.p6.1" class="ltx_p"><span id="S3.SS3.SSS1.p6.1.1" class="ltx_text ltx_font_bold">Federated Averaging’s performance under simulated faults:</span></p>
</div>
<figure id="S3.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-cifar-fedavg-deletedata.png" id="S3.F6.sf1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Delete Mutator</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-fedavg-unbalancedata.png" id="S3.F6.sf2.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Unbalance Mutator</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-fedavg-overlapdata.png" id="S3.F6.sf3.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Overlap Mutator</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-fedavg-noisedata.png" id="S3.F6.sf4.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Noise Mutator</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>CIFAR-10 - Federated Averaging performance under simulated faults.</figcaption>
</figure>
<figure id="S3.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-mnist-fedavg-deletedata.png" id="S3.F7.sf1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="132" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Delete Mutator</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-fedavg-unbalancedata.png" id="S3.F7.sf2.g1" class="ltx_graphics ltx_img_landscape" width="240" height="132" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Unbalance Mutator</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F7.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-fedavg-overlapdata.png" id="S3.F7.sf3.g1" class="ltx_graphics ltx_img_landscape" width="240" height="132" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Overlap Mutator</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F7.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-fedavg-noisedata.png" id="S3.F7.sf4.g1" class="ltx_graphics ltx_img_landscape" width="240" height="132" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Noise Mutator</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Fashion MNIST - Federated Averaging performance under simulated faults.</figcaption>
</figure>
<div id="S3.SS3.SSS1.p7" class="ltx_para">
<p id="S3.SS3.SSS1.p7.1" class="ltx_p">First, we discuss the results for faults simulated by mutation operators.
The results for the CIFAR-10 and Fashion MNIST datasets are reported in Figures <a href="#S3.F6" title="Figure 6 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#S3.F7" title="Figure 7 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, respectively.
Our first observation is that data mutators do not significantly impact the final accuracy of the model. Another interesting observation is that an increase in the proportion of affected clients does not noticeably decrease the accuracy. However, among these mutators, as Figures <a href="#S3.F6.sf4" title="In Figure 6 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6d</span></a> and <a href="#S3.F7.sf4" title="In Figure 7 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7d</span></a> show, the Noise mutator is more effective, and it gets slightly more powerful as the proportion increases. Therefore, a noisy dataset is a possible human fault that can cause problems in FL.</p>
</div>
<div id="S3.SS3.SSS1.p8" class="ltx_para">
<p id="S3.SS3.SSS1.p8.1" class="ltx_p">Moreover, a general and somewhat expected observation is that the accuracy slightly decreases as the dataset becomes more non-iid. This can be seen in different bar colors in Figures <a href="#S3.F6" title="Figure 6 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#S3.F7" title="Figure 7 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>The accuracy change (between clean and 0.5 proportion configs) of Federated Averaging, per mutator (averaged over all non-iid configurations).</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">Accuracy change</th>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<td id="S3.T2.1.2.2.1" class="ltx_td"></td>
<th id="S3.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2">CIFAR-10</th>
<th id="S3.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2">Fashion MNIST</th>
</tr>
<tr id="S3.T2.1.3.3" class="ltx_tr">
<th id="S3.T2.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Mutator</th>
<th id="S3.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Mean</th>
<th id="S3.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Std</th>
<th id="S3.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Mean</th>
<th id="S3.T2.1.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Std</th>
</tr>
<tr id="S3.T2.1.4.4" class="ltx_tr">
<td id="S3.T2.1.4.4.1" class="ltx_td ltx_align_center ltx_border_tt">Delete</td>
<td id="S3.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_tt">-4.3</td>
<td id="S3.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_tt">0.7</td>
<td id="S3.T2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_tt">-0.1</td>
<td id="S3.T2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_tt">1.24</td>
</tr>
<tr id="S3.T2.1.5.5" class="ltx_tr">
<td id="S3.T2.1.5.5.1" class="ltx_td ltx_align_center">Overlap</td>
<td id="S3.T2.1.5.5.2" class="ltx_td ltx_align_center">-0.71</td>
<td id="S3.T2.1.5.5.3" class="ltx_td ltx_align_center">3.29</td>
<td id="S3.T2.1.5.5.4" class="ltx_td ltx_align_center">-1.3</td>
<td id="S3.T2.1.5.5.5" class="ltx_td ltx_align_center">4.11</td>
</tr>
<tr id="S3.T2.1.6.6" class="ltx_tr">
<td id="S3.T2.1.6.6.1" class="ltx_td ltx_align_center">Unbalance</td>
<td id="S3.T2.1.6.6.2" class="ltx_td ltx_align_center">-0.64</td>
<td id="S3.T2.1.6.6.3" class="ltx_td ltx_align_center">1.01</td>
<td id="S3.T2.1.6.6.4" class="ltx_td ltx_align_center">-0.21</td>
<td id="S3.T2.1.6.6.5" class="ltx_td ltx_align_center">1.75</td>
</tr>
<tr id="S3.T2.1.7.7" class="ltx_tr">
<td id="S3.T2.1.7.7.1" class="ltx_td ltx_align_center ltx_border_bb">Noise</td>
<td id="S3.T2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_bb">-5.66</td>
<td id="S3.T2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_bb">1.05</td>
<td id="S3.T2.1.7.7.4" class="ltx_td ltx_align_center ltx_border_bb">-2.84</td>
<td id="S3.T2.1.7.7.5" class="ltx_td ltx_align_center ltx_border_bb">1.04</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS3.SSS1.p9" class="ltx_para">
<p id="S3.SS3.SSS1.p9.1" class="ltx_p">We report a summary of faults’ effect on Federated Averaging in Table <a href="#S3.T2" title="Table 2 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The change values reported are the amount of accuracy change between a clean scenario and the case where half of the clients are affected by the mutators. As the results show, the mutators do not significantly impact the accuracy. However, out of the mutators, the Noise mutator has the most impact on the final model, which is less than 6% and is insignificant. Note that the statistical test results here show that in 20% of the cases, these differences are statistically the same as well, but in 80%, they are statistically different. However, the amount of difference itself is not actually significant (less than 6%).</p>
</div>
<div id="S3.SS3.SSS1.p10" class="ltx_para">
<p id="S3.SS3.SSS1.p10.1" class="ltx_p"><span id="S3.SS3.SSS1.p10.1.1" class="ltx_text ltx_font_bold">Federated Averaging’s performance under attacks:</span></p>
</div>
<figure id="S3.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-cifar-fedavg-labelflip.png" id="S3.F8.sf1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>CIFAR-10 - Label Flip</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-fedavg-labelflip.png" id="S3.F8.sf2.g1" class="ltx_graphics ltx_img_landscape" width="240" height="132" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Fashion MNIST - Label Flip</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-fedavg-randomupdate.png" id="S3.F8.sf3.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>CIFAR-10 - Random Update</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-fedavg-randomupdate.png" id="S3.F8.sf4.g1" class="ltx_graphics ltx_img_landscape" width="240" height="132" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Fashion MNIST - Random Update</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-fedavg-signflip.png" id="S3.F8.sf5.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>CIFAR-10 - Sign Flip</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-fedavg-signflip.png" id="S3.F8.sf6.g1" class="ltx_graphics ltx_img_landscape" width="240" height="132" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>Fashion MNIST - Sign Flip</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Federated Averaging performance under untargeted attacks.</figcaption>
</figure>
<div id="S3.SS3.SSS1.p11" class="ltx_para">
<p id="S3.SS3.SSS1.p11.1" class="ltx_p">Figure <a href="#S3.F8" title="Figure 8 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the results of the untargeted attacks used in our experiments. As shown in Figures <a href="#S3.F8.sf1" title="In Figure 8 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8a</span></a> and <a href="#S3.F8.sf2" title="In Figure 8 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8b</span></a>, we can see that Label Flip is effective, and its effect is more noticeable in higher proportions like 0.3 and 0.5 (especially compared to the mutators discussed previously). Furthermore, a non-iid dataset can cause more damage as the proportion increases. On CIFAR-10, we see a 13% decrease of accuracy (due to non-iid distribution) in 0.5 proportion compared to 5% in 0.1 proportion. We see the same pattern on Fashion MNIST, and in 0.5 proportion, accuracy decreases by 8%, but in 0.1 proportion, the decrease is around 3%. Considering that technically this is also a data mutator, human error and mislabelling samples can result in problems in FL.</p>
</div>
<div id="S3.SS3.SSS1.p12" class="ltx_para">
<p id="S3.SS3.SSS1.p12.1" class="ltx_p">Additionally, the results show that model poisoning attacks are much more powerful than data attacks and faults. As Figures <a href="#S3.F8.sf3" title="In Figure 8 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8c</span></a> and <a href="#S3.F8.sf5" title="In Figure 8 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8e</span></a> show, even with a small proportion of affected clients, Federated Averaging loses its effectiveness completely, and the model classifies all test cases as a single class. As a result, accuracy reaches 10%, which is similar to random guessing. The same can be seen from Figures <a href="#S3.F8.sf4" title="In Figure 8 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8d</span></a> and <a href="#S3.F8.sf6" title="In Figure 8 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8f</span></a> for the Fashion MNIST dataset, and the model is essentially guessing randomly.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>The accuracy change (between clean and 0.5 proportion configs) of Federated Averaging, per attack (averaged over all non-iid configurations).</figcaption>
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">Accuracy change</td>
</tr>
<tr id="S3.T3.1.2.2" class="ltx_tr">
<td id="S3.T3.1.2.2.1" class="ltx_td"></td>
<td id="S3.T3.1.2.2.2" class="ltx_td ltx_align_center" colspan="2">CIFAR-10</td>
<td id="S3.T3.1.2.2.3" class="ltx_td ltx_align_center" colspan="2">Fashion MNIST</td>
</tr>
<tr id="S3.T3.1.3.3" class="ltx_tr">
<td id="S3.T3.1.3.3.1" class="ltx_td ltx_align_center">Mutator</td>
<td id="S3.T3.1.3.3.2" class="ltx_td ltx_align_center">Mean</td>
<td id="S3.T3.1.3.3.3" class="ltx_td ltx_align_center">Std</td>
<td id="S3.T3.1.3.3.4" class="ltx_td ltx_align_center">Mean</td>
<td id="S3.T3.1.3.3.5" class="ltx_td ltx_align_center">Std</td>
</tr>
<tr id="S3.T3.1.4.4" class="ltx_tr">
<td id="S3.T3.1.4.4.1" class="ltx_td ltx_align_center ltx_border_tt">Label Flip</td>
<td id="S3.T3.1.4.4.2" class="ltx_td ltx_align_center ltx_border_tt">-39.02</td>
<td id="S3.T3.1.4.4.3" class="ltx_td ltx_align_center ltx_border_tt">7.81</td>
<td id="S3.T3.1.4.4.4" class="ltx_td ltx_align_center ltx_border_tt">-21.26</td>
<td id="S3.T3.1.4.4.5" class="ltx_td ltx_align_center ltx_border_tt">9.6</td>
</tr>
<tr id="S3.T3.1.5.5" class="ltx_tr">
<td id="S3.T3.1.5.5.1" class="ltx_td ltx_align_center">Random Update</td>
<td id="S3.T3.1.5.5.2" class="ltx_td ltx_align_center">-66.13</td>
<td id="S3.T3.1.5.5.3" class="ltx_td ltx_align_center">1.67</td>
<td id="S3.T3.1.5.5.4" class="ltx_td ltx_align_center">-77.81</td>
<td id="S3.T3.1.5.5.5" class="ltx_td ltx_align_center">1.53</td>
</tr>
<tr id="S3.T3.1.6.6" class="ltx_tr">
<td id="S3.T3.1.6.6.1" class="ltx_td ltx_align_center ltx_border_bb">Sign Flip</td>
<td id="S3.T3.1.6.6.2" class="ltx_td ltx_align_center ltx_border_bb">-66.13</td>
<td id="S3.T3.1.6.6.3" class="ltx_td ltx_align_center ltx_border_bb">1.67</td>
<td id="S3.T3.1.6.6.4" class="ltx_td ltx_align_center ltx_border_bb">-77.81</td>
<td id="S3.T3.1.6.6.5" class="ltx_td ltx_align_center ltx_border_bb">1.53</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS3.SSS1.p13" class="ltx_para">
<p id="S3.SS3.SSS1.p13.1" class="ltx_p">Like the faults section, we report a summary of attacks against Federated Averaging in Table <a href="#S3.T3" title="Table 3 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The change values reported are the amount of accuracy change between a clean scenario and the case where half of the clients are under attack.
Among the untargeted attacks, Random Update and Sign Flip are the most effective attacks against Federated Averaging, with around 70% accuracy change. Furthermore, all the attacks are much more effective than mutators as they decrease the accuracy by at least 21.26% (the best mutator did not even reach 6%). Furthermore, the statistical test shows that all these changes are statistically different as well.</p>
</div>
<div id="S3.SS3.SSS1.p14" class="ltx_para">
<p id="S3.SS3.SSS1.p14.1" class="ltx_p">Regarding the Backdoor attack, we showed how it is applied in the background section in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.3 Attacks in Federated Learning ‣ 2 Background ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
In the CIFAR-10 dataset, images will be misclassified as a car with that specific pixel pattern. Also, in the Fashion MNIST dataset, they will be misclassified as a trouser.</p>
</div>
<figure id="S3.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F9.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-cifar-fedavg-backdoor.png" id="S3.F9.sf1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="131" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>CIFAR-10</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F9.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-fedavg-backdoor.png" id="S3.F9.sf2.g1" class="ltx_graphics ltx_img_landscape" width="240" height="131" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Fashion MNIST</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Federated Averaging performance under the Backdoor attack.</figcaption>
</figure>
<figure id="S3.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F10.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-mnist-backdoor-0.1.png" id="S3.F10.sf1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="151" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Proportion=0.1</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F10.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-backdoor-0.5.png" id="S3.F10.sf2.g1" class="ltx_graphics ltx_img_landscape" width="240" height="153" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Proportion=0.5</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Fashion MNIST - Backdoor task accuracy per round plots(iid distribution).</figcaption>
</figure>
<div id="S3.SS3.SSS1.p15" class="ltx_para">
<p id="S3.SS3.SSS1.p15.1" class="ltx_p">Finally, the results of the Backdoor attack are reported in Figure <a href="#S3.F9" title="Figure 9 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. Note that the Backdoor results are reported separately from other attacks to avoid confusion. This is because other attack results report the accuracy of the main task, which the attack is trying to decrease. In contrast, Backdoor results report the backdoor task accuracy, which the attack is trying to increase.</p>
</div>
<div id="S3.SS3.SSS1.p16" class="ltx_para">
<p id="S3.SS3.SSS1.p16.1" class="ltx_p">Figure <a href="#S3.F9.sf1" title="In Figure 9 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9a</span></a> confirms that the Backdoor attack is highly effective on CIFAR-10.
As with even 0.1 proportion of malicious clients, the backdoor task reaches above 90% accuracy in all non-iid cases.
This is more obvious in Figure <a href="#S3.F9.sf2" title="In Figure 9 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9b</span></a> where even with 0.1 proportion, the attacker reaches 100% accuracy in all non-iid cases.
To clarify the Fashion MNIST case, we show the accuracy per round plots of Fashion MNIST in an iid scenario for 0.1 and 0.5 proportions in Figure <a href="#S3.F10" title="Figure 10 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. As we see, in a high proportion, the backdoor task accuracy starts from a very high value and converges to 100% very fast, but in a small proportion, it starts from around 50% and fluctuates a lot more before it converges. So the proportion has an effect here, but since the Fashion MNIST task is relatively simple, the backdoor task converges to maximum accuracy even in a small proportion in the end.</p>
</div>
<div id="S3.SS3.SSS1.p17" class="ltx_para">
<p id="S3.SS3.SSS1.p17.1" class="ltx_p">Also, in Figure <a href="#S3.F9.sf1" title="In Figure 9 ‣ 3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9a</span></a> for the CIFAR-10 dataset, we see that in the tiniest proportion, a more non-iid distribution increases the backdoor accuracy and makes it more powerful. This is not obvious in higher proportions as the backdoor reaches the highest feasible (considering the attack method and dataset) accuracy with an iid distribution. So, a more non-iid distribution cannot increase it. For the same reason, non-iid does not change the backdoor accuracy for the Fashion MNIST dataset as the backdoor accuracy is already 100%.</p>
</div>
<div id="S3.SS3.SSS1.p18" class="ltx_para ltx_noindent">
<svg id="S3.SS3.SSS1.p18.pic1" class="ltx_picture" height="122.88" overflow="visible" version="1.1" width="600"><g transform="translate(0,122.88) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 116.98 C 0 120.24 2.64 122.88 5.91 122.88 L 594.09 122.88 C 597.36 122.88 600 120.24 600 116.98 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 116.98 C 1.97 119.15 3.73 120.91 5.91 120.91 L 594.09 120.91 C 596.27 120.91 598.03 119.15 598.03 116.98 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="95.32" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S3.SS3.SSS1.p18.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S3.SS3.SSS1.p18.pic1.1.1.1.1.1.1" class="ltx_p"><span id="S3.SS3.SSS1.p18.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Answer to RQ1:</span> In generic image datasets (e.g., CIFAR-10 and Fashion MNIST), Federated Averaging is not robust against any attacks, and the FL process faces quality issues, as shown by the final model accuracy Also, generally, model attacks are more powerful than data attacks. Furthermore, data mutators do not significantly impact Federated Averaging, but noisy data leaves a bigger mark. Lastly, the non-iid distribution has a more detrimental impact on the quality of FL when more clients are attacked.</span>
</span></foreignObject></g></g></svg>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span><span id="S3.SS3.SSS2.1.1" class="ltx_text ltx_font_bold">RQ2 results (comparison of aggregators/defense mechanisms):</span>
</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">We divide this RQ into two parts: first, we compare the aggregators with no attack, then compare them under attacks.</p>
</div>
<figure id="S3.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F11.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-cifar-clean.png" id="S3.F11.sf1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="149" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>CIFAR10</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F11.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-clean.png" id="S3.F11.sf2.g1" class="ltx_graphics ltx_img_landscape" width="240" height="146" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Fashion MNIST</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Aggregators accuracy with no attack applied.</figcaption>
</figure>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<p id="S3.SS3.SSS2.p2.1" class="ltx_p">We show the results for the first part in Figures <a href="#S3.F11.sf1" title="In Figure 11 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11a</span></a> and <a href="#S3.F11.sf2" title="In Figure 11 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11b</span></a> for CIFAR-10 and Fashion MNIST, respectively. Results show that Federated Averaging and Trimmed Mean perform similarly, and the non-iid does not significantly impact their accuracy. Median performs almost similarly to Federated Averaging and Trimmed Mean in a more iid setting, but it loses its accuracy slightly in a high non-iid situation.</p>
</div>
<div id="S3.SS3.SSS2.p3" class="ltx_para">
<p id="S3.SS3.SSS2.p3.1" class="ltx_p">In contrast, Krum’s accuracy is noticeably lower than the other aggregators in an iid scenario. As the dataset becomes more non-iid, the difference between Krum and others becomes more significant.</p>
</div>
<div id="S3.SS3.SSS2.p4" class="ltx_para">
<p id="S3.SS3.SSS2.p4.1" class="ltx_p">Lastly, comparing two datasets shows that non-iid distribution decreases accuracy in CIFAR-10 more than Fashion MNIST, which can be because the former is more challenging than the latter.</p>
</div>
<figure id="S3.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F12.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-cifar-defences-0-labelflip.png" id="S3.F12.sf1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Label Flip, non-iid=0</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F12.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-defences-0-randomupdate.png" id="S3.F12.sf2.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Random Update, non-iid=0</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F12.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-defences-0.4-labelflip.png" id="S3.F12.sf3.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Label Flip, non-iid=0.4</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F12.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-defences-0.4-randomupdate.png" id="S3.F12.sf4.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Random Update, non-iid=0.4</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F12.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-defences-0.7-labelflip.png" id="S3.F12.sf5.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>Label Flip, non-iid=0.7</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F12.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-defences-0.7-randomupdate.png" id="S3.F12.sf6.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>Random Update, non-iid=0.7</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F12.sf7" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-defences-0-signflip.png" id="S3.F12.sf7.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>Sign Flip, non-iid=0</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F12.sf8" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-defences-0.4-signflip.png" id="S3.F12.sf8.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>Sign Flip, non-iid=0.4</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F12.sf9" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-defences-0.7-signflip.png" id="S3.F12.sf9.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(i) </span>Sign Flip, non-iid=0.7</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>CIFAR-10 - Aggregators performance under untargeted attacks.</figcaption>
</figure>
<figure id="S3.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F13.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-mnist-defences-0-labelflip.png" id="S3.F13.sf1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="132" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Label Flip, non-iid=0</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F13.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-defences-0-randomupdate.png" id="S3.F13.sf2.g1" class="ltx_graphics ltx_img_landscape" width="240" height="133" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Random Update, non-iid=0</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F13.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-defences-0.4-labelflip.png" id="S3.F13.sf3.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Label Flip, non-iid=0.4</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F13.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-defences-0.4-randomupdate.png" id="S3.F13.sf4.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Random Update, non-iid=0.4</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F13.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-defences-0.7-labelflip.png" id="S3.F13.sf5.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>Label Flip, non-iid=0.7</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F13.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-defences-0.7-randomupdate.png" id="S3.F13.sf6.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>Random Update, non-iid=0.7</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F13.sf7" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-defences-0-signflip.png" id="S3.F13.sf7.g1" class="ltx_graphics ltx_img_landscape" width="240" height="133" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(g) </span>Sign Flip, non-iid=0</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F13.sf8" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-defences-0.4-signflip.png" id="S3.F13.sf8.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(h) </span>Sign Flip, non-iid=0.4</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F13.sf9" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-defences-0.7-signflip.png" id="S3.F13.sf9.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(i) </span>Sign Flip, non-iid=0.7</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Fashion MNIST - Aggregators performance under untargeted attacks.</figcaption>
</figure>
<div id="S3.SS3.SSS2.p5" class="ltx_para">
<p id="S3.SS3.SSS2.p5.1" class="ltx_p">We compare the aggregators under various attacks for the second part of this research question. Since in RQ1, we saw that data mutators are not very effective against the baseline (Federated Averaging), we exclude them from this part.
Figures <a href="#S3.F12" title="Figure 12 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> and <a href="#S3.F13" title="Figure 13 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> show the results of untargeted attacks for the CIFAR-10 and Fashion MNIST, respectively. For the reasons discussed in RQ1, separate Backdoor attack results from untargeted attack results.</p>
</div>
<div id="S3.SS3.SSS2.p6" class="ltx_para">
<p id="S3.SS3.SSS2.p6.1" class="ltx_p">As Figure <a href="#S3.F12.sf1" title="In Figure 12 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12a</span></a> shows, in the Label Flip attack on CIFAR-10, when we consider an iid situation, Federated Averaging, Median, and Trimmed Mean perform very similarly in smaller attack proportions. However, Trimmed Mean falls behind when half of the clients are byzantine. However, Krum does not perform well in the Label Flip attack and roughly achieves 10% lower accuracy in smaller proportions than other methods; this gets much worse when half of the clients are attacked.</p>
</div>
<div id="S3.SS3.SSS2.p7" class="ltx_para">
<p id="S3.SS3.SSS2.p7.1" class="ltx_p">Figure <a href="#S3.F13.sf1" title="In Figure 13 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13a</span></a> shows similar patterns on the Fashion MNIST dataset in an iid case. We see that Federated Averaging, Median, and Trimmed Mean are very similar in small proportions, but in 0.5 proportion, Federated Averaging is noticeably more robust. Like CIFAR-10, Krum is the worst aggregator by far, and its results are even worse than CIFAR-10 as it reaches as low as 2% accuracy in some cases.</p>
</div>
<div id="S3.SS3.SSS2.p8" class="ltx_para">
<p id="S3.SS3.SSS2.p8.1" class="ltx_p">Looking at Figures <a href="#S3.F12.sf3" title="In Figure 12 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12c</span></a> and <a href="#S3.F12.sf5" title="In Figure 12 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12e</span></a> for the CIFAR-10 dataset, as the non-iid degree increases, Federated Averaging outperforms other techniques in almost all the cases, and its advantage is more noticeable, with 30% of malicious clients.
We also see that Trimmed Mean gets better results than the Median in almost all cases, making it a better choice than the Median in a non-iid scenario.
Krum gets even worse when data becomes more non-iid, and even with 0.1 malicious clients, it works way worse than when it was not attacked.
Consequently, although Federated Averaging is not robust against this attack, it still is the best choice and generally works better than the others.</p>
</div>
<div id="S3.SS3.SSS2.p9" class="ltx_para">
<p id="S3.SS3.SSS2.p9.1" class="ltx_p">On the Fashion MNIST dataset, Figures <a href="#S3.F13.sf3" title="In Figure 13 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13c</span></a> and <a href="#S3.F13.sf5" title="In Figure 13 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13e</span></a> show similar findings, and Federated Averaging and Trimmed Mean are the best choices.</p>
</div>
<div id="S3.SS3.SSS2.p10" class="ltx_para">
<p id="S3.SS3.SSS2.p10.1" class="ltx_p">To summarize the Label Flip attack, generally, robust aggregators perform worse than Federated Averaging in a non-iid case when they are under attack. This might be because the robust aggregators cannot distinguish what is causing the difference between the updates (high non-iid or the attack itself), which results in poor performance.</p>
</div>
<div id="S3.SS3.SSS2.p11" class="ltx_para">
<p id="S3.SS3.SSS2.p11.1" class="ltx_p">Looking at the untargeted model poisoning attacks (Random Update and Sign Flip) for the CIFAR-10 dataset. The first remark is that even with the smallest proportion of affected clients in an iid case (Figures <a href="#S3.F12.sf2" title="In Figure 12 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12b</span></a> and <a href="#S3.F12.sf7" title="In Figure 12 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12g</span></a>), Federated Averaging and Trimmed Mean are entirely ineffective, and the model is guessing randomly. Moreover, when a small proportion of clients are affected, Median works best in all non-iid degrees and achieves the highest accuracy (Figures <a href="#S3.F12.sf4" title="In Figure 12 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12d</span></a>, <a href="#S3.F12.sf6" title="In Figure 12 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12f</span></a> and <a href="#S3.F12.sf8" title="In Figure 12 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12h</span></a>, <a href="#S3.F12.sf9" title="In Figure 12 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12i</span></a>). Nevertheless, when more clients become byzantine, it too loses its effectiveness and surrenders to attackers like Federated Averaging and Trimmed Mean. In contrast to the label attack where Krum struggled, here is where Krum shines. In non-iid degrees of 0 and 0.4, Krum shows incredible robustness against the attacks. No matter the proportion of affected clients, it performs excellently and achieves results as if it were not under attack. However, in the highest non-iid case, Krum relatively loses its robustness when half of the clients become byzantine.</p>
</div>
<div id="S3.SS3.SSS2.p12" class="ltx_para">
<p id="S3.SS3.SSS2.p12.1" class="ltx_p">The observations are a bit different on the Fashion MNSIT dataset. Firstly, we see that Trimmed Mean is still viable against Random Update attack in all non-iid cases under small proportions (Figures <a href="#S3.F13.sf2" title="In Figure 13 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13b</span></a>, <a href="#S3.F13.sf4" title="In Figure 13 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13d</span></a>, <a href="#S3.F13.sf6" title="In Figure 13 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13f</span></a>). This is because the Fashion MNIST task is less complex compared to CIFAR-10. However, in the Sign Flip attack, Trimmed Mean is guessing randomly in more cases compared to the Random Update attack (Figures <a href="#S3.F13.sf7" title="In Figure 13 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13g</span></a>, <a href="#S3.F13.sf8" title="In Figure 13 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13h</span></a>, <a href="#S3.F13.sf9" title="In Figure 13 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13i</span></a>), which confirms the fact that the Sign Flip attack is more powerful. Everything else is similar to CIFAR-10, and Median and Krum are still the best options.</p>
</div>
<div id="S3.SS3.SSS2.p13" class="ltx_para">
<p id="S3.SS3.SSS2.p13.1" class="ltx_p">To sum up, in untargeted attacks, generally speaking, Krum is the most robust technique and works very reliably in all cases. However, if the proportion of affected clients is small, the Median is better because it has better convergence speed and accuracy than Krum.</p>
</div>
<figure id="S3.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F14.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-cifar-defences-0-backdoor.png" id="S3.F14.sf1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="131" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>CIFAR-10, non-iid=0</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F14.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-defences-0-backdoor.png" id="S3.F14.sf2.g1" class="ltx_graphics ltx_img_landscape" width="240" height="131" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Fashion MNIST, non-iid=0</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F14.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-defences-0.4-backdoor.png" id="S3.F14.sf3.g1" class="ltx_graphics ltx_img_landscape" width="240" height="131" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>CIFAR-10, non-iid=0.4</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F14.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-defences-0.4-backdoor.png" id="S3.F14.sf4.g1" class="ltx_graphics ltx_img_landscape" width="240" height="131" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Fashion MNIST, non-iid=0.4</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F14.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-defences-0.7-backdoor.png" id="S3.F14.sf5.g1" class="ltx_graphics ltx_img_landscape" width="240" height="131" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(e) </span>CIFAR-10, non-iid=0.7</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F14.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-mnist-defences-0.7-backdoor.png" id="S3.F14.sf6.g1" class="ltx_graphics ltx_img_landscape" width="240" height="131" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(f) </span>Fashion MNIST, non-iid=0.7</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Aggregators performance under the Backdoor attack.</figcaption>
</figure>
<div id="S3.SS3.SSS2.p14" class="ltx_para">
<p id="S3.SS3.SSS2.p14.1" class="ltx_p">The results for the Backdoor attack are shown in Figure <a href="#S3.F14" title="Figure 14 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>. As it can be seen, Krum is quite robust in all cases. Moreover, with the increase of attackers or the non-iid degree, the final model does not get fooled, and backdoor accuracy is close to zero. On the other hand, Federated Averaging gets entirely fooled in all cases and is not robust. Median and Trimmed Mean show some resilience against the Backdoor attack. The former is slightly superior when data is more iid, and the proportion of affected clients is small. However, they are not even close to Krum, and as the proportion increases, they become ineffective like Federated Averaging, and the backdoor goal is almost always achieved.
In this case, Krum is undoubtedly the best and most robust technique.</p>
</div>
<figure id="S3.T4" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Aggregators summary in terms of their accuracy (averaged across attacks, non-iid configurations, and proportion of affected clients) and the number of times the aggregator is the best choice among all aggregators under study (Number of times achieving the Top rank) – The shaded cells mark the best techniques.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.T4.sf1" class="ltx_table ltx_figure_panel ltx_align_center">
<table id="S3.T4.sf1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T4.sf1.1.1.1" class="ltx_tr">
<td id="S3.T4.sf1.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S3.T4.sf1.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S3.T4.sf1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Accuracy</th>
<th id="S3.T4.sf1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S3.T4.sf1.1.1.1.4.1" class="ltx_text">
<span id="S3.T4.sf1.1.1.1.4.1.1" class="ltx_inline-block">
<span id="S3.T4.sf1.1.1.1.4.1.1.1" class="ltx_p">Top rank</span>
<span id="S3.T4.sf1.1.1.1.4.1.1.2" class="ltx_p">frequency</span>
</span></span></th>
</tr>
<tr id="S3.T4.sf1.1.2.2" class="ltx_tr">
<th id="S3.T4.sf1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Dataset</th>
<th id="S3.T4.sf1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Aggregator</th>
<th id="S3.T4.sf1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Mean</th>
<th id="S3.T4.sf1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Std</th>
</tr>
<tr id="S3.T4.sf1.1.3.3" class="ltx_tr">
<td id="S3.T4.sf1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_tt" rowspan="4"><span id="S3.T4.sf1.1.3.3.1.1" class="ltx_text">CIFAR-10</span></td>
<td id="S3.T4.sf1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_tt">FedAvg</td>
<td id="S3.T4.sf1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">26.31</td>
<td id="S3.T4.sf1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_tt">24.88</td>
<td id="S3.T4.sf1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_tt">8</td>
</tr>
<tr id="S3.T4.sf1.1.4.4" class="ltx_tr">
<td id="S3.T4.sf1.1.4.4.1" class="ltx_td ltx_align_center">Krum</td>
<td id="S3.T4.sf1.1.4.4.2" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T4.sf1.1.4.4.2.1" class="ltx_text" style="background-color:#DFDFDF;">51.09</span></td>
<td id="S3.T4.sf1.1.4.4.3" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T4.sf1.1.4.4.3.1" class="ltx_text" style="background-color:#DFDFDF;">16.66</span></td>
<td id="S3.T4.sf1.1.4.4.4" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T4.sf1.1.4.4.4.1" class="ltx_text" style="background-color:#DFDFDF;">12</span></td>
</tr>
<tr id="S3.T4.sf1.1.5.5" class="ltx_tr">
<td id="S3.T4.sf1.1.5.5.1" class="ltx_td ltx_align_center">Median</td>
<td id="S3.T4.sf1.1.5.5.2" class="ltx_td ltx_align_center">37.04</td>
<td id="S3.T4.sf1.1.5.5.3" class="ltx_td ltx_align_center">27.32</td>
<td id="S3.T4.sf1.1.5.5.4" class="ltx_td ltx_align_center">6</td>
</tr>
<tr id="S3.T4.sf1.1.6.6" class="ltx_tr">
<td id="S3.T4.sf1.1.6.6.1" class="ltx_td ltx_align_center">Tri-mean</td>
<td id="S3.T4.sf1.1.6.6.2" class="ltx_td ltx_align_center">24.61</td>
<td id="S3.T4.sf1.1.6.6.3" class="ltx_td ltx_align_center">23.2</td>
<td id="S3.T4.sf1.1.6.6.4" class="ltx_td ltx_align_center">1</td>
</tr>
<tr id="S3.T4.sf1.1.7.7" class="ltx_tr">
<td id="S3.T4.sf1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="4"><span id="S3.T4.sf1.1.7.7.1.1" class="ltx_text">
<span id="S3.T4.sf1.1.7.7.1.1.1" class="ltx_inline-block">
<span id="S3.T4.sf1.1.7.7.1.1.1.1" class="ltx_p">Fashion</span>
<span id="S3.T4.sf1.1.7.7.1.1.1.2" class="ltx_p">MNIST</span>
</span></span></td>
<td id="S3.T4.sf1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t">FedAvg</td>
<td id="S3.T4.sf1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t">33.14</td>
<td id="S3.T4.sf1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t">33.29</td>
<td id="S3.T4.sf1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t">5</td>
</tr>
<tr id="S3.T4.sf1.1.8.8" class="ltx_tr">
<td id="S3.T4.sf1.1.8.8.1" class="ltx_td ltx_align_center">Krum</td>
<td id="S3.T4.sf1.1.8.8.2" class="ltx_td ltx_align_center">51.32</td>
<td id="S3.T4.sf1.1.8.8.3" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T4.sf1.1.8.8.3.1" class="ltx_text" style="background-color:#DFDFDF;">30.27</span></td>
<td id="S3.T4.sf1.1.8.8.4" class="ltx_td ltx_align_center">9</td>
</tr>
<tr id="S3.T4.sf1.1.9.9" class="ltx_tr">
<td id="S3.T4.sf1.1.9.9.1" class="ltx_td ltx_align_center">Median</td>
<td id="S3.T4.sf1.1.9.9.2" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T4.sf1.1.9.9.2.1" class="ltx_text" style="background-color:#DFDFDF;">58.43</span></td>
<td id="S3.T4.sf1.1.9.9.3" class="ltx_td ltx_align_center">31.93</td>
<td id="S3.T4.sf1.1.9.9.4" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T4.sf1.1.9.9.4.1" class="ltx_text" style="background-color:#DFDFDF;">10</span></td>
</tr>
<tr id="S3.T4.sf1.1.10.10" class="ltx_tr">
<td id="S3.T4.sf1.1.10.10.1" class="ltx_td ltx_align_center ltx_border_bb">Tri-mean</td>
<td id="S3.T4.sf1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_bb">53.69</td>
<td id="S3.T4.sf1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_bb">32.25</td>
<td id="S3.T4.sf1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_bb">3</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(a) </span>Results for untargeted attacks (higher accuracy is better)</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.T4.sf2" class="ltx_table ltx_figure_panel ltx_align_center">
<table id="S3.T4.sf2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T4.sf2.1.1.1" class="ltx_tr">
<td id="S3.T4.sf2.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S3.T4.sf2.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S3.T4.sf2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Backdoor accuracy</th>
<th id="S3.T4.sf2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S3.T4.sf2.1.1.1.4.1" class="ltx_text">
<span id="S3.T4.sf2.1.1.1.4.1.1" class="ltx_inline-block">
<span id="S3.T4.sf2.1.1.1.4.1.1.1" class="ltx_p">Top rank</span>
<span id="S3.T4.sf2.1.1.1.4.1.1.2" class="ltx_p">frequency</span>
</span></span></th>
</tr>
<tr id="S3.T4.sf2.1.2.2" class="ltx_tr">
<th id="S3.T4.sf2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Dataset</th>
<th id="S3.T4.sf2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Aggregator</th>
<th id="S3.T4.sf2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Mean</th>
<th id="S3.T4.sf2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Std</th>
</tr>
<tr id="S3.T4.sf2.1.3.3" class="ltx_tr">
<td id="S3.T4.sf2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_tt" rowspan="4"><span id="S3.T4.sf2.1.3.3.1.1" class="ltx_text">CIFAR-10</span></td>
<td id="S3.T4.sf2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_tt">FedAvg</td>
<td id="S3.T4.sf2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">95.16</td>
<td id="S3.T4.sf2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_tt" style="background-color:#DFDFDF;"><span id="S3.T4.sf2.1.3.3.4.1" class="ltx_text" style="background-color:#DFDFDF;">1.78</span></td>
<td id="S3.T4.sf2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_tt">0</td>
</tr>
<tr id="S3.T4.sf2.1.4.4" class="ltx_tr">
<td id="S3.T4.sf2.1.4.4.1" class="ltx_td ltx_align_center">Krum</td>
<td id="S3.T4.sf2.1.4.4.2" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T4.sf2.1.4.4.2.1" class="ltx_text" style="background-color:#DFDFDF;">4.26</span></td>
<td id="S3.T4.sf2.1.4.4.3" class="ltx_td ltx_align_center">1.86</td>
<td id="S3.T4.sf2.1.4.4.4" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T4.sf2.1.4.4.4.1" class="ltx_text" style="background-color:#DFDFDF;">9</span></td>
</tr>
<tr id="S3.T4.sf2.1.5.5" class="ltx_tr">
<td id="S3.T4.sf2.1.5.5.1" class="ltx_td ltx_align_center">Median</td>
<td id="S3.T4.sf2.1.5.5.2" class="ltx_td ltx_align_center">90.72</td>
<td id="S3.T4.sf2.1.5.5.3" class="ltx_td ltx_align_center">6.65</td>
<td id="S3.T4.sf2.1.5.5.4" class="ltx_td ltx_align_center">0</td>
</tr>
<tr id="S3.T4.sf2.1.6.6" class="ltx_tr">
<td id="S3.T4.sf2.1.6.6.1" class="ltx_td ltx_align_center">Tri-mean</td>
<td id="S3.T4.sf2.1.6.6.2" class="ltx_td ltx_align_center">93.56</td>
<td id="S3.T4.sf2.1.6.6.3" class="ltx_td ltx_align_center">3.71</td>
<td id="S3.T4.sf2.1.6.6.4" class="ltx_td ltx_align_center">0</td>
</tr>
<tr id="S3.T4.sf2.1.7.7" class="ltx_tr">
<td id="S3.T4.sf2.1.7.7.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="4"><span id="S3.T4.sf2.1.7.7.1.1" class="ltx_text">
<span id="S3.T4.sf2.1.7.7.1.1.1" class="ltx_inline-block">
<span id="S3.T4.sf2.1.7.7.1.1.1.1" class="ltx_p">Fashion</span>
<span id="S3.T4.sf2.1.7.7.1.1.1.2" class="ltx_p">MNIST</span>
</span></span></td>
<td id="S3.T4.sf2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t">FedAvg</td>
<td id="S3.T4.sf2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t">99.94</td>
<td id="S3.T4.sf2.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#DFDFDF;"><span id="S3.T4.sf2.1.7.7.4.1" class="ltx_text" style="background-color:#DFDFDF;">0.08</span></td>
<td id="S3.T4.sf2.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t">0</td>
</tr>
<tr id="S3.T4.sf2.1.8.8" class="ltx_tr">
<td id="S3.T4.sf2.1.8.8.1" class="ltx_td ltx_align_center">Krum</td>
<td id="S3.T4.sf2.1.8.8.2" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T4.sf2.1.8.8.2.1" class="ltx_text" style="background-color:#DFDFDF;">1.71</span></td>
<td id="S3.T4.sf2.1.8.8.3" class="ltx_td ltx_align_center">2.27</td>
<td id="S3.T4.sf2.1.8.8.4" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T4.sf2.1.8.8.4.1" class="ltx_text" style="background-color:#DFDFDF;">9</span></td>
</tr>
<tr id="S3.T4.sf2.1.9.9" class="ltx_tr">
<td id="S3.T4.sf2.1.9.9.1" class="ltx_td ltx_align_center">Median</td>
<td id="S3.T4.sf2.1.9.9.2" class="ltx_td ltx_align_center">82.39</td>
<td id="S3.T4.sf2.1.9.9.3" class="ltx_td ltx_align_center">24.79</td>
<td id="S3.T4.sf2.1.9.9.4" class="ltx_td ltx_align_center">0</td>
</tr>
<tr id="S3.T4.sf2.1.10.10" class="ltx_tr">
<td id="S3.T4.sf2.1.10.10.1" class="ltx_td ltx_align_center ltx_border_bb">Tri-mean</td>
<td id="S3.T4.sf2.1.10.10.2" class="ltx_td ltx_align_center ltx_border_bb">99.73</td>
<td id="S3.T4.sf2.1.10.10.3" class="ltx_td ltx_align_center ltx_border_bb">0.23</td>
<td id="S3.T4.sf2.1.10.10.4" class="ltx_td ltx_align_center ltx_border_bb">0</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(b) </span>Results for the Backdoor attack (lower backdoor accuracy is better)</figcaption>
</figure>
</div>
</div>
</figure>
<div id="S3.SS3.SSS2.p15" class="ltx_para">
<p id="S3.SS3.SSS2.p15.1" class="ltx_p">We report a summary of aggregators for CIFAR-10 and Fashion MNIST in Table <a href="#S3.T4" title="Table 4 ‣ 3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> based on the average accuracy of the final model on the test data and the number of times each aggregator was the most robust one. Note that only attacks are selected here since data mutators were ineffective and untargeted attacks and backdoor attacks were split to avoid confusion. As it can be seen, Krum achieves the best accuracy on average and achieves the top rank the most in the CIFAR-10 dataset, so all in all, it is the most robust aggregation method for that dataset. For Fashion MNIST, Median is the most robust aggregator. However, considering the first rank count of aggregators, Krum comes in a close second, but its mean accuracy is less than of the Median. This is because it achieves far worse results in the Label Flip attack.</p>
</div>
<div id="S3.SS3.SSS2.p16" class="ltx_para">
<p id="S3.SS3.SSS2.p16.1" class="ltx_p">Lastly, we report the statistical tests. We have 2 datasets, 4 attacks, 3 proportions, and 3 non-iid degrees, which results in 72 cases. Based on the previous discussions, we consider Krum to be the best option overall and compare it to the second-best aggregator (per case) in all the cases.
The results show that in 38 cases, Krum is the best aggregator with a statistically significant difference. In one case, it is the best, but the difference is insignificant. It is not the best in two cases, but the difference is not significant either. Finally, in 31 cases, Krum is worse with a significant difference. As a result, we can say that Krum is the best choice in 57% of the cases.
Doing the same test for Median, Trimmed Mean, and Federated Averaging will result in 52%, 45.8%, and 50%. These results confirm that Krum is the best aggregator overall.</p>
</div>
<div id="S3.SS3.SSS2.p17" class="ltx_para ltx_noindent">
<svg id="S3.SS3.SSS2.p17.pic1" class="ltx_picture" height="156.09" overflow="visible" version="1.1" width="600"><g transform="translate(0,156.09) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 150.18 C 0 153.45 2.64 156.09 5.91 156.09 L 594.09 156.09 C 597.36 156.09 600 153.45 600 150.18 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 150.18 C 1.97 152.36 3.73 154.12 5.91 154.12 L 594.09 154.12 C 596.27 154.12 598.03 152.36 598.03 150.18 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="128.53" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S3.SS3.SSS2.p17.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S3.SS3.SSS2.p17.pic1.1.1.1.1.1.1" class="ltx_p"><span id="S3.SS3.SSS2.p17.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Answer to RQ2:</span>
In 57% of the cases, Krum is the most robust FL aggregator, and Median comes in second when there is no prior knowledge of the type of attack or fault (which is often the case). However, if attacks were known, the best choice for the Label Flip attack would still be Federated Averaging. Also, if the model was under untargeted model attacks and the number of attackers was small, Median would be a better choice than Krum. That being said, Krum’s main drawback is its problem with non-iid distribution, which struggles to maintain the same accuracy as the iid degree increases. So, in short, there is no best defense technique for all situations to ensure quality.</span>
</span></foreignObject></g></g></svg>
</div>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span><span id="S3.SS3.SSS3.1.1" class="ltx_text ltx_font_bold">RQ3 results (the case of a real federated dataset):</span>
</h4>

<figure id="S3.F15" class="ltx_figure"><img src="/html/2201.01409/assets/images-adni-clean.png" id="S3.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="240" height="135" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>ADNI - Aggregators accuracy with no attack.</figcaption>
</figure>
<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p id="S3.SS3.SSS3.p1.1" class="ltx_p">Like RQ2, we first report the results where all clients are benign in Figure <a href="#S3.F15" title="Figure 15 ‣ 3.3.3 RQ3 results (the case of a real federated dataset): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>. As it can be seen, all aggregators perform similarly and achieve the same accuracy of 75% except Krum. This is much like the results we saw for general datasets in RQ2. However, there is a significant difference in the ADNI dataset for Krum. Here Krum does not perform reliably and shows different behavior with different runs.
Moreover, Krum converges to very different results, and there is a 15% difference in the final accuracy for Krum’s worst and best run. Whereas in CIFAR-10, the variation between the results is less than 5%. This inconsistency might be because this dataset is naturally non-iid and unbalanced.</p>
</div>
<figure id="S3.F16" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F16.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-adni-krum-0.png" id="S3.F16.sf1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="154" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>ADNI - Worst</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F16.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-adni-krum-3.png" id="S3.F16.sf2.g1" class="ltx_graphics ltx_img_landscape" width="240" height="154" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>ADNI - Best</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F16.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-krum-3.png" id="S3.F16.sf3.g1" class="ltx_graphics ltx_img_landscape" width="240" height="154" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>CIFAR-10 - Worst (non-iid=0.4)</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F16.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-cifar-krum-4.png" id="S3.F16.sf4.g1" class="ltx_graphics ltx_img_landscape" width="240" height="154" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>CIFAR-10 - Best (non-iid=0.4)</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Convergence plots of Krum on different runs.</figcaption>
</figure>
<div id="S3.SS3.SSS3.p2" class="ltx_para">
<p id="S3.SS3.SSS3.p2.1" class="ltx_p">We saw in RQ2 that Krum performed worse compared to others in synthetic non-iid cases, and here we see a similar problem but more severe.
To illustrate this better, Figure <a href="#S3.F16" title="Figure 16 ‣ 3.3.3 RQ3 results (the case of a real federated dataset): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> shows Krum’s test accuracy convergence trend over the communication rounds for ADNI and CIFAR-10 datasets on worst and best runs. As seen in Figures <a href="#S3.F16.sf1" title="In Figure 16 ‣ 3.3.3 RQ3 results (the case of a real federated dataset): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16a</span></a> and <a href="#S3.F16.sf2" title="In Figure 16 ‣ 3.3.3 RQ3 results (the case of a real federated dataset): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16b</span></a>, Krum does not work reliably here, and it converges to very different results, and we see a 15% difference in the final accuracy. Whereas in CIFAR-10, the results difference is less than 5%.
One explanation for this inconsistency is that this RQ is investigating a cross-silo FL case, meaning clients are not selected randomly. Thus, Krum always sees updates from the same clients and chooses one which might be good or bad for the final test.
Since different centers have images from different MRI machines, the selected client update by Krum might not be a generally good solution, and final results are dependent on random factors like the data shuffle method, weight initialization, and other factors. Consequently, Krum is not guaranteed to converge to the best solution in this dataset, and its results are unreliable and quite random.</p>
</div>
<div id="S3.SS3.SSS3.p3" class="ltx_para">
<p id="S3.SS3.SSS3.p3.1" class="ltx_p">Note that the high fluctuations visible in Figures <a href="#S3.F16.sf4" title="In Figure 16 ‣ 3.3.3 RQ3 results (the case of a real federated dataset): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16d</span></a> and <a href="#S3.F16.sf3" title="In Figure 16 ‣ 3.3.3 RQ3 results (the case of a real federated dataset): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16c</span></a> are because Krum only selects one client’s update at each round, and the setting shown here is non-iid and cross-device, resulting in the fluctuations.</p>
</div>
<figure id="S3.F17" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F17.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-adni-defences-deletedata.png" id="S3.F17.sf1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Delete Mutator</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F17.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-adni-defences-unbalancedata.png" id="S3.F17.sf2.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Unbalance Mutator</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F17.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-adni-defences-overlapdata.png" id="S3.F17.sf3.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Overlap Mutator</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F17.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-adni-defences-noisedata.png" id="S3.F17.sf4.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>Noise Mutator</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>ADNI - Aggregators performance under simulated faults.</figcaption>
</figure>
<div id="S3.SS3.SSS3.p4" class="ltx_para">
<p id="S3.SS3.SSS3.p4.1" class="ltx_p"><span id="S3.SS3.SSS3.p4.1.1" class="ltx_text ltx_font_bold">Effect of mutators on ADNI dataset:</span></p>
</div>
<div id="S3.SS3.SSS3.p5" class="ltx_para">
<p id="S3.SS3.SSS3.p5.1" class="ltx_p">To see how aggregation methods work here and what the differences are compared to generic image datasets, we report the mutator results in Figure <a href="#S3.F17" title="Figure 17 ‣ 3.3.3 RQ3 results (the case of a real federated dataset): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>.</p>
</div>
<div id="S3.SS3.SSS3.p6" class="ltx_para">
<p id="S3.SS3.SSS3.p6.1" class="ltx_p">In most data mutators, the results are similar to the RQ1, and none of them significantly impact any of the aggregators. As seen before, the Noise mutator reduces the quality of the model slightly. However, interestingly, in the ADNI dataset, the Overlap mutator has a significant impact on Federated Averaging. As the proportion of affected clients increases, it performs even worse and hits 46% accuracy. This can be because this dataset has only two classes, and this mutator overlaps only two classes (which are all of the classes here), as discussed in Section <a href="#S2.SS4" title="2.4 Mutation operators in Deep Learning ‣ 2 Background ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>.</p>
</div>
<div id="S3.SS3.SSS3.p7" class="ltx_para">
<p id="S3.SS3.SSS3.p7.1" class="ltx_p">On the other hand, Median and Trimmed Mean show excellent robustness against this mutator, and in smaller proportions, they perform as if there was no attack. Although Median is slightly less resilient in smaller proportions, it performs better when half of the clients are byzantine. Krum is also more robust than Federated Averaging, but because of its problems with the ADNI dataset almost always performs worse than Median and Trimmed Mean.</p>
</div>
<div id="S3.SS3.SSS3.p8" class="ltx_para">
<p id="S3.SS3.SSS3.p8.1" class="ltx_p">An exciting conclusion is that robust aggregation methods can mitigate faults that hurt Federated Averaging, like when similar samples have different labels.</p>
</div>
<figure id="S3.F18" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F18.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-adni-defences-labelflip.png" id="S3.F18.sf1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="135" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Label Flip</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F18.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-adni-defences-randomupdate.png" id="S3.F18.sf2.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Random Update</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F18.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2201.01409/assets/images-no-legend-adni-defences-signflip.png" id="S3.F18.sf3.g1" class="ltx_graphics ltx_img_landscape" width="240" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Sign Flip</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>ADNI - Aggregators performance under untargeted attacks.</figcaption>
</figure>
<div id="S3.SS3.SSS3.p9" class="ltx_para">
<p id="S3.SS3.SSS3.p9.1" class="ltx_p"><span id="S3.SS3.SSS3.p9.1.1" class="ltx_text ltx_font_bold">Effect of attacks on ADNI dataset:</span></p>
</div>
<div id="S3.SS3.SSS3.p10" class="ltx_para">
<p id="S3.SS3.SSS3.p10.1" class="ltx_p">According to Figure <a href="#S3.F18.sf1" title="In Figure 18 ‣ 3.3.3 RQ3 results (the case of a real federated dataset): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18a</span></a>, in the Label Flip attack, we see that in a small proportion of affected clients, all aggregators perform very similarly except Krum, which is worse, much like the previous RQ. As the proportion increases, we see that Median and Trimmed Mean get slightly better results. However, when half of the clients are under attack, they are less robust than Federated Averaging.</p>
</div>
<div id="S3.SS3.SSS3.p11" class="ltx_para">
<p id="S3.SS3.SSS3.p11.1" class="ltx_p">In untargeted model attacks (Figures <a href="#S3.F18.sf2" title="In Figure 18 ‣ 3.3.3 RQ3 results (the case of a real federated dataset): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18b</span></a> and <a href="#S3.F18.sf3" title="In Figure 18 ‣ 3.3.3 RQ3 results (the case of a real federated dataset): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18c</span></a>), we see again that Federated Averaging is performing very poorly. In the Random Update, it achieves an average accuracy of 64%. However, in the Sign Flip attack, it is even worse as models are guessing randomly and since the data is unbalanced final accuracy is 34%. This shows that Sign Flip can cause more damage, and it makes sense because it is trying to guide the model in the opposite direction rather than a random direction.
In contrast, Trimmed Mean and Median are robust, and in the Random Update attack with smaller proportions, they even get close to no attack accuracy of 75%. However, since Sign Flip is more powerful than Random Update, both aggregators get lower accuracy in the Sign Flip case and even completely fail when half of the clients are malicious.</p>
</div>
<div id="S3.SS3.SSS3.p12" class="ltx_para">
<p id="S3.SS3.SSS3.p12.1" class="ltx_p">Although Krum did not always converge to the best solutions, it is still more robust than Federated Averaging and is closely behind Median and Trimmed Mean. In some cases, like 0.5 proportion, it even works better than those two.</p>
</div>
<div id="S3.SS3.SSS3.p13" class="ltx_para">
<p id="S3.SS3.SSS3.p13.1" class="ltx_p">As a result, Trimmed Mean and Median for these attacks are the best and reasonably robust choices.
Furthermore, Median and Trimmed Mean aggregators show more robustness in higher proportions than RQ2, which can be because we are using transfer learning and many layers of the model are not trainable. Hence, model attacks are less effective than they were in RQ2.</p>
</div>
<figure id="S3.F19" class="ltx_figure"><img src="/html/2201.01409/assets/images-adni-defences-backdoor.png" id="S3.F19.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="240" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>ADNI - Aggregators performance under Backdoor attack.</figcaption>
</figure>
<div id="S3.SS3.SSS3.p14" class="ltx_para">
<p id="S3.SS3.SSS3.p14.1" class="ltx_p">Figure <a href="#S3.F19" title="Figure 19 ‣ 3.3.3 RQ3 results (the case of a real federated dataset): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a> shows the results for the Backdoor attack. The first observation is that Krum still is the most robust approach in the Backdoor attack with only 50% backdoor task accuracy in small proportions. Even in the highest proportion, it still has 60% backdoor accuracy, which is much better than the other aggregators. However, Krum struggles with the main task (as discussed before), so Krum has a trade-off between main and backdoor task accuracy.</p>
</div>
<div id="S3.SS3.SSS3.p15" class="ltx_para">
<p id="S3.SS3.SSS3.p15.1" class="ltx_p">Note that Krum’s backdoor task accuracy in this dataset is much higher than in generic datasets used in RQ2 (more than 50% in this question compared to less than 10% in RQ2). This is because there are only two classes in this dataset, and the main task accuracy of the dataset does not exceed 75% (for Krum, it is even lower). This means some samples are already misclassified to the attackers’ target label, making the backdoor task more successful.</p>
</div>
<div id="S3.SS3.SSS3.p16" class="ltx_para">
<p id="S3.SS3.SSS3.p16.1" class="ltx_p">Moreover, the Median shows slight robustness in the smallest attack proportion against the backdoor objective, but it is inadequate. Trimmed Mean performs worse than Median, which is not good enough either. Federated Averaging is the worst of all, and its backdoor task is always successful in all cases. These patterns are much like what we observed in RQ2.</p>
</div>
<figure id="S3.T5" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>
ADNI - Aggregators summary in terms of their accuracy (averaged across attacks and proportion of affected clients) and the number of times the aggregator is the best choice among all aggregators under study (Number of times achieving the Top rank) – The shaded cells mark the best techniques.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.T5.sf1" class="ltx_table ltx_figure_panel ltx_align_center">
<table id="S3.T5.sf1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T5.sf1.1.1.1" class="ltx_tr">
<td id="S3.T5.sf1.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S3.T5.sf1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Accuracy</th>
<th id="S3.T5.sf1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S3.T5.sf1.1.1.1.3.1" class="ltx_text">
<span id="S3.T5.sf1.1.1.1.3.1.1" class="ltx_inline-block">
<span id="S3.T5.sf1.1.1.1.3.1.1.1" class="ltx_p">Top rank</span>
<span id="S3.T5.sf1.1.1.1.3.1.1.2" class="ltx_p">frequency</span>
</span></span></th>
</tr>
<tr id="S3.T5.sf1.1.2.2" class="ltx_tr">
<th id="S3.T5.sf1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Aggregator</th>
<th id="S3.T5.sf1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Mean</th>
<th id="S3.T5.sf1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Std</th>
</tr>
<tr id="S3.T5.sf1.1.3.3" class="ltx_tr">
<td id="S3.T5.sf1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_tt">FedAvg</td>
<td id="S3.T5.sf1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_tt">52.89</td>
<td id="S3.T5.sf1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">12.66</td>
<td id="S3.T5.sf1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_tt">1</td>
</tr>
<tr id="S3.T5.sf1.1.4.4" class="ltx_tr">
<td id="S3.T5.sf1.1.4.4.1" class="ltx_td ltx_align_center">Krum</td>
<td id="S3.T5.sf1.1.4.4.2" class="ltx_td ltx_align_center">63.23</td>
<td id="S3.T5.sf1.1.4.4.3" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T5.sf1.1.4.4.3.1" class="ltx_text" style="background-color:#DFDFDF;">6.24</span></td>
<td id="S3.T5.sf1.1.4.4.4" class="ltx_td ltx_align_center">2</td>
</tr>
<tr id="S3.T5.sf1.1.5.5" class="ltx_tr">
<td id="S3.T5.sf1.1.5.5.1" class="ltx_td ltx_align_center">Median</td>
<td id="S3.T5.sf1.1.5.5.2" class="ltx_td ltx_align_center">64.97</td>
<td id="S3.T5.sf1.1.5.5.3" class="ltx_td ltx_align_center">11.87</td>
<td id="S3.T5.sf1.1.5.5.4" class="ltx_td ltx_align_center">4</td>
</tr>
<tr id="S3.T5.sf1.1.6.6" class="ltx_tr">
<td id="S3.T5.sf1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_bb">Tri-mean</td>
<td id="S3.T5.sf1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DFDFDF;"><span id="S3.T5.sf1.1.6.6.2.1" class="ltx_text" style="background-color:#DFDFDF;">65.16</span></td>
<td id="S3.T5.sf1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_bb">11.72</td>
<td id="S3.T5.sf1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DFDFDF;"><span id="S3.T5.sf1.1.6.6.4.1" class="ltx_text" style="background-color:#DFDFDF;">5</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(a) </span>Results for untargeted attacks and Overlap mutator (higher accuracy is better)</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.T5.sf2" class="ltx_table ltx_figure_panel ltx_align_center">
<table id="S3.T5.sf2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T5.sf2.1.1.1" class="ltx_tr">
<td id="S3.T5.sf2.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S3.T5.sf2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Backdoor accuracy</th>
<th id="S3.T5.sf2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S3.T5.sf2.1.1.1.3.1" class="ltx_text">
<span id="S3.T5.sf2.1.1.1.3.1.1" class="ltx_inline-block">
<span id="S3.T5.sf2.1.1.1.3.1.1.1" class="ltx_p">Top rank</span>
<span id="S3.T5.sf2.1.1.1.3.1.1.2" class="ltx_p">frequency</span>
</span></span></th>
</tr>
<tr id="S3.T5.sf2.1.2.2" class="ltx_tr">
<th id="S3.T5.sf2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Aggregator</th>
<th id="S3.T5.sf2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Mean</th>
<th id="S3.T5.sf2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Std</th>
</tr>
<tr id="S3.T5.sf2.1.3.3" class="ltx_tr">
<td id="S3.T5.sf2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_tt">FedAvg</td>
<td id="S3.T5.sf2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_tt">99.47</td>
<td id="S3.T5.sf2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_tt" style="background-color:#DFDFDF;"><span id="S3.T5.sf2.1.3.3.3.1" class="ltx_text" style="background-color:#DFDFDF;">0.75</span></td>
<td id="S3.T5.sf2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_tt">0</td>
</tr>
<tr id="S3.T5.sf2.1.4.4" class="ltx_tr">
<td id="S3.T5.sf2.1.4.4.1" class="ltx_td ltx_align_center">Krum</td>
<td id="S3.T5.sf2.1.4.4.2" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T5.sf2.1.4.4.2.1" class="ltx_text" style="background-color:#DFDFDF;">53.34</span></td>
<td id="S3.T5.sf2.1.4.4.3" class="ltx_td ltx_align_center">5.05</td>
<td id="S3.T5.sf2.1.4.4.4" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T5.sf2.1.4.4.4.1" class="ltx_text" style="background-color:#DFDFDF;">3</span></td>
</tr>
<tr id="S3.T5.sf2.1.5.5" class="ltx_tr">
<td id="S3.T5.sf2.1.5.5.1" class="ltx_td ltx_align_center">Median</td>
<td id="S3.T5.sf2.1.5.5.2" class="ltx_td ltx_align_center">96.9</td>
<td id="S3.T5.sf2.1.5.5.3" class="ltx_td ltx_align_center">3.41</td>
<td id="S3.T5.sf2.1.5.5.4" class="ltx_td ltx_align_center">0</td>
</tr>
<tr id="S3.T5.sf2.1.6.6" class="ltx_tr">
<td id="S3.T5.sf2.1.6.6.1" class="ltx_td ltx_align_center ltx_border_bb">Tri-mean</td>
<td id="S3.T5.sf2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_bb">97.92</td>
<td id="S3.T5.sf2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_bb">2.09</td>
<td id="S3.T5.sf2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_bb">0</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(b) </span>Results for Backdoor attack (lower accuracy is better)</figcaption>
</figure>
</div>
</div>
</figure>
<div id="S3.SS3.SSS3.p17" class="ltx_para">
<p id="S3.SS3.SSS3.p17.1" class="ltx_p">Following what we did in RQ2, we report the summary of aggregators for this dataset in Table <a href="#S3.T5" title="Table 5 ‣ 3.3.3 RQ3 results (the case of a real federated dataset): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Unlike RQ2, the Overlap mutator is also included here since it showed to be effective for this dataset.</p>
</div>
<div id="S3.SS3.SSS3.p18" class="ltx_para">
<p id="S3.SS3.SSS3.p18.1" class="ltx_p">According to the table, Trimmed Mean gets the best results for untargeted attacks and mutators, and Median comes in second with a negligible difference. All aggregators lose the competition to the attacker for the Backdoor attack except Krum, which shows decent robustness, but its main task accuracy can still be problematic.</p>
</div>
<div id="S3.SS3.SSS3.p19" class="ltx_para">
<p id="S3.SS3.SSS3.p19.1" class="ltx_p">Lastly, we report the statistical tests. We have 4 attacks, 1 mutator that is effective(Overlap) proportions, and 3 proportions which result in 15 cases. Since Trimmed Mean and Median were close, we discuss their statistical test results in detail and compare them to the second-best aggregator (per case) in all the cases.</p>
</div>
<div id="S3.SS3.SSS3.p20" class="ltx_para">
<p id="S3.SS3.SSS3.p20.1" class="ltx_p">For Trimmed Mean, results show that it is significantly better in three cases. In five cases, it is better, but the difference is insignificant. In five cases, it is worse, but again, not significantly. Lastly, it is significantly worse in two cases. So in 86% of cases, Trimmed Mean is the best choice.</p>
</div>
<div id="S3.SS3.SSS3.p21" class="ltx_para">
<p id="S3.SS3.SSS3.p21.1" class="ltx_p">For Median, results show that it is significantly better in three cases. In four cases, it is better, but the difference is not significant. In six cases, it is worse, again not significantly. Finally, in two cases, it is significantly worse. So just like Trimmed Mean, in 86% of cases, Median is the best choice.
Also, this number for Krum and Federated Averaging is 60% and 40% respectively, which confirms that Median and Trimmed Mean are the best choices here.</p>
</div>
<div id="S3.SS3.SSS3.p22" class="ltx_para ltx_noindent">
<svg id="S3.SS3.SSS3.p22.pic1" class="ltx_picture" height="139.48" overflow="visible" version="1.1" width="600"><g transform="translate(0,139.48) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 133.58 C 0 136.84 2.64 139.48 5.91 139.48 L 594.09 139.48 C 597.36 139.48 600 136.84 600 133.58 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 133.58 C 1.97 135.75 3.73 137.52 5.91 137.52 L 594.09 137.52 C 596.27 137.52 598.03 135.75 598.03 133.58 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="111.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S3.SS3.SSS3.p22.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S3.SS3.SSS3.p22.pic1.1.1.1.1.1.1" class="ltx_p"><span id="S3.SS3.SSS3.p22.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Answer to RQ3:</span>
Unlike RQ2, where Krum showed to be the best aggregator, overall, here, Trimmed Mean and Median are the most robust ones in 86% of cases, and they can better maintain the quality of the FL process. However, Krum is still the best aggregator for the Backdoor attack (on the backdoor task, to be more precise). Moreover, Krum shows problems in the cross-silo FL with non-iid data, and it is not consistent like the other aggregators. Finally, even in a case-by-case situation in untargeted attacks, Trimmed Mean and Median are still a great choice of aggregation method.</span>
</span></foreignObject></g></g></svg>
</div>
</section>
<section id="S3.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4 </span><span id="S3.SS3.SSS4.1.1" class="ltx_text ltx_font_bold">RQ4 results (an ensemble of aggregators):</span>
</h4>

<figure id="S3.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Comparison of available aggregators with the ensemble aggregator.</figcaption>
<table id="S3.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T6.1.1.1" class="ltx_tr">
<th id="S3.T6.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt"></th>
<th id="S3.T6.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt"></th>
<th id="S3.T6.1.1.1.3" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S3.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5">Aggregator</th>
</tr>
<tr id="S3.T6.1.2.2" class="ltx_tr">
<th id="S3.T6.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row">Dataset</th>
<th id="S3.T6.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row">Attack</th>
<th id="S3.T6.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r">portion</th>
<th id="S3.T6.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">FedAvg</th>
<th id="S3.T6.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Krum</th>
<th id="S3.T6.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">Median</th>
<th id="S3.T6.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">Tri-Mean</th>
<th id="S3.T6.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column">Ensemble</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T6.1.3.1" class="ltx_tr">
<th id="S3.T6.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" rowspan="8"><span id="S3.T6.1.3.1.1.1" class="ltx_text">CIFAR-10</span></th>
<th id="S3.T6.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" rowspan="3"><span id="S3.T6.1.3.1.2.1" class="ltx_text">Label Flip</span></th>
<th id="S3.T6.1.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">0.1</th>
<td id="S3.T6.1.3.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="background-color:#DFDFDF;"><span id="S3.T6.1.3.1.4.1" class="ltx_text" style="background-color:#DFDFDF;">74.58</span></td>
<td id="S3.T6.1.3.1.5" class="ltx_td ltx_align_center ltx_border_tt">49.49</td>
<td id="S3.T6.1.3.1.6" class="ltx_td ltx_align_center ltx_border_tt">70.34</td>
<td id="S3.T6.1.3.1.7" class="ltx_td ltx_align_center ltx_border_tt">73.2</td>
<td id="S3.T6.1.3.1.8" class="ltx_td ltx_align_center ltx_border_tt">73.31</td>
</tr>
<tr id="S3.T6.1.4.2" class="ltx_tr">
<th id="S3.T6.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.3</th>
<td id="S3.T6.1.4.2.2" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T6.1.4.2.2.1" class="ltx_text" style="background-color:#DFDFDF;">66.81</span></td>
<td id="S3.T6.1.4.2.3" class="ltx_td ltx_align_center">41.02</td>
<td id="S3.T6.1.4.2.4" class="ltx_td ltx_align_center">54.13</td>
<td id="S3.T6.1.4.2.5" class="ltx_td ltx_align_center">59.01</td>
<td id="S3.T6.1.4.2.6" class="ltx_td ltx_align_center">64.85</td>
</tr>
<tr id="S3.T6.1.5.3" class="ltx_tr">
<th id="S3.T6.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.5</th>
<td id="S3.T6.1.5.3.2" class="ltx_td ltx_align_center">36.4</td>
<td id="S3.T6.1.5.3.3" class="ltx_td ltx_align_center">29.41</td>
<td id="S3.T6.1.5.3.4" class="ltx_td ltx_align_center">27.59</td>
<td id="S3.T6.1.5.3.5" class="ltx_td ltx_align_center">31.84</td>
<td id="S3.T6.1.5.3.6" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T6.1.5.3.6.1" class="ltx_text" style="background-color:#DFDFDF;">60.53</span></td>
</tr>
<tr id="S3.T6.1.6.4" class="ltx_tr">
<th id="S3.T6.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S3.T6.1.6.4.1.1" class="ltx_text">Sign Flip</span></th>
<th id="S3.T6.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">0.1</th>
<td id="S3.T6.1.6.4.3" class="ltx_td ltx_align_center ltx_border_t">10.0</td>
<td id="S3.T6.1.6.4.4" class="ltx_td ltx_align_center ltx_border_t">61.0</td>
<td id="S3.T6.1.6.4.5" class="ltx_td ltx_align_center ltx_border_t">70.96</td>
<td id="S3.T6.1.6.4.6" class="ltx_td ltx_align_center ltx_border_t">10.0</td>
<td id="S3.T6.1.6.4.7" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#DFDFDF;"><span id="S3.T6.1.6.4.7.1" class="ltx_text" style="background-color:#DFDFDF;">72.08</span></td>
</tr>
<tr id="S3.T6.1.7.5" class="ltx_tr">
<th id="S3.T6.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.3</th>
<td id="S3.T6.1.7.5.2" class="ltx_td ltx_align_center">10.0</td>
<td id="S3.T6.1.7.5.3" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T6.1.7.5.3.1" class="ltx_text" style="background-color:#DFDFDF;">60.62</span></td>
<td id="S3.T6.1.7.5.4" class="ltx_td ltx_align_center">10.0</td>
<td id="S3.T6.1.7.5.5" class="ltx_td ltx_align_center">10.0</td>
<td id="S3.T6.1.7.5.6" class="ltx_td ltx_align_center">59.77</td>
</tr>
<tr id="S3.T6.1.8.6" class="ltx_tr">
<th id="S3.T6.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.5</th>
<td id="S3.T6.1.8.6.2" class="ltx_td ltx_align_center">10.0</td>
<td id="S3.T6.1.8.6.3" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T6.1.8.6.3.1" class="ltx_text" style="background-color:#DFDFDF;">59.13</span></td>
<td id="S3.T6.1.8.6.4" class="ltx_td ltx_align_center">10.0</td>
<td id="S3.T6.1.8.6.5" class="ltx_td ltx_align_center">10.0</td>
<td id="S3.T6.1.8.6.6" class="ltx_td ltx_align_center">10.0</td>
</tr>
<tr id="S3.T6.1.9.7" class="ltx_tr">
<th id="S3.T6.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="2">Average</th>
<td id="S3.T6.1.9.7.2" class="ltx_td ltx_align_center ltx_border_t">34.63</td>
<td id="S3.T6.1.9.7.3" class="ltx_td ltx_align_center ltx_border_t">50.11</td>
<td id="S3.T6.1.9.7.4" class="ltx_td ltx_align_center ltx_border_t">40.5</td>
<td id="S3.T6.1.9.7.5" class="ltx_td ltx_align_center ltx_border_t">32.34</td>
<td id="S3.T6.1.9.7.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#DFDFDF;"><span id="S3.T6.1.9.7.6.1" class="ltx_text" style="background-color:#DFDFDF;">56.75</span></td>
</tr>
<tr id="S3.T6.1.10.8" class="ltx_tr">
<th id="S3.T6.1.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="2">Std</th>
<td id="S3.T6.1.10.8.2" class="ltx_td ltx_align_center">27.25</td>
<td id="S3.T6.1.10.8.3" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T6.1.10.8.3.1" class="ltx_text" style="background-color:#DFDFDF;">11.70</span></td>
<td id="S3.T6.1.10.8.4" class="ltx_td ltx_align_center">25.92</td>
<td id="S3.T6.1.10.8.5" class="ltx_td ltx_align_center">25.42</td>
<td id="S3.T6.1.10.8.6" class="ltx_td ltx_align_center">21.54</td>
</tr>
<tr id="S3.T6.1.11.9" class="ltx_tr">
<th id="S3.T6.1.11.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="8"><span id="S3.T6.1.11.9.1.1" class="ltx_text">ADNI</span></th>
<th id="S3.T6.1.11.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S3.T6.1.11.9.2.1" class="ltx_text">Label Flip</span></th>
<th id="S3.T6.1.11.9.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">0.1</th>
<td id="S3.T6.1.11.9.4" class="ltx_td ltx_align_center ltx_border_t">69.09</td>
<td id="S3.T6.1.11.9.5" class="ltx_td ltx_align_center ltx_border_t">61.07</td>
<td id="S3.T6.1.11.9.6" class="ltx_td ltx_align_center ltx_border_t">69.68</td>
<td id="S3.T6.1.11.9.7" class="ltx_td ltx_align_center ltx_border_t">69.1</td>
<td id="S3.T6.1.11.9.8" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#DFDFDF;"><span id="S3.T6.1.11.9.8.1" class="ltx_text" style="background-color:#DFDFDF;">70.03</span></td>
</tr>
<tr id="S3.T6.1.12.10" class="ltx_tr">
<th id="S3.T6.1.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.3</th>
<td id="S3.T6.1.12.10.2" class="ltx_td ltx_align_center">58.2</td>
<td id="S3.T6.1.12.10.3" class="ltx_td ltx_align_center">59.28</td>
<td id="S3.T6.1.12.10.4" class="ltx_td ltx_align_center">61.01</td>
<td id="S3.T6.1.12.10.5" class="ltx_td ltx_align_center">60.59</td>
<td id="S3.T6.1.12.10.6" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T6.1.12.10.6.1" class="ltx_text" style="background-color:#DFDFDF;">65.46</span></td>
</tr>
<tr id="S3.T6.1.13.11" class="ltx_tr">
<th id="S3.T6.1.13.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.5</th>
<td id="S3.T6.1.13.11.2" class="ltx_td ltx_align_center">54.02</td>
<td id="S3.T6.1.13.11.3" class="ltx_td ltx_align_center">45.39</td>
<td id="S3.T6.1.13.11.4" class="ltx_td ltx_align_center">46.13</td>
<td id="S3.T6.1.13.11.5" class="ltx_td ltx_align_center">48.41</td>
<td id="S3.T6.1.13.11.6" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T6.1.13.11.6.1" class="ltx_text" style="background-color:#DFDFDF;">65.83</span></td>
</tr>
<tr id="S3.T6.1.14.12" class="ltx_tr">
<th id="S3.T6.1.14.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S3.T6.1.14.12.1.1" class="ltx_text">Sign Flip</span></th>
<th id="S3.T6.1.14.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">0.1</th>
<td id="S3.T6.1.14.12.3" class="ltx_td ltx_align_center ltx_border_t">34.49</td>
<td id="S3.T6.1.14.12.4" class="ltx_td ltx_align_center ltx_border_t">63.46</td>
<td id="S3.T6.1.14.12.5" class="ltx_td ltx_align_center ltx_border_t">71.61</td>
<td id="S3.T6.1.14.12.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#DFDFDF;"><span id="S3.T6.1.14.12.6.1" class="ltx_text" style="background-color:#DFDFDF;">72.9</span></td>
<td id="S3.T6.1.14.12.7" class="ltx_td ltx_align_center ltx_border_t">71.5</td>
</tr>
<tr id="S3.T6.1.15.13" class="ltx_tr">
<th id="S3.T6.1.15.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.3</th>
<td id="S3.T6.1.15.13.2" class="ltx_td ltx_align_center">34.49</td>
<td id="S3.T6.1.15.13.3" class="ltx_td ltx_align_center">66.17</td>
<td id="S3.T6.1.15.13.4" class="ltx_td ltx_align_center">67.75</td>
<td id="S3.T6.1.15.13.5" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T6.1.15.13.5.1" class="ltx_text" style="background-color:#DFDFDF;">69.12</span></td>
<td id="S3.T6.1.15.13.6" class="ltx_td ltx_align_center">65.69</td>
</tr>
<tr id="S3.T6.1.16.14" class="ltx_tr">
<th id="S3.T6.1.16.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.5</th>
<td id="S3.T6.1.16.14.2" class="ltx_td ltx_align_center">34.49</td>
<td id="S3.T6.1.16.14.3" class="ltx_td ltx_align_center" style="background-color:#DFDFDF;"><span id="S3.T6.1.16.14.3.1" class="ltx_text" style="background-color:#DFDFDF;">67.46</span></td>
<td id="S3.T6.1.16.14.4" class="ltx_td ltx_align_center">34.49</td>
<td id="S3.T6.1.16.14.5" class="ltx_td ltx_align_center">34.49</td>
<td id="S3.T6.1.16.14.6" class="ltx_td ltx_align_center">49.34</td>
</tr>
<tr id="S3.T6.1.17.15" class="ltx_tr">
<th id="S3.T6.1.17.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="2">Average</th>
<td id="S3.T6.1.17.15.2" class="ltx_td ltx_align_center ltx_border_t">47.46</td>
<td id="S3.T6.1.17.15.3" class="ltx_td ltx_align_center ltx_border_t">60.47</td>
<td id="S3.T6.1.17.15.4" class="ltx_td ltx_align_center ltx_border_t">58.45</td>
<td id="S3.T6.1.17.15.5" class="ltx_td ltx_align_center ltx_border_t">59.1</td>
<td id="S3.T6.1.17.15.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#DFDFDF;"><span id="S3.T6.1.17.15.6.1" class="ltx_text" style="background-color:#DFDFDF;">64.64</span></td>
</tr>
<tr id="S3.T6.1.18.16" class="ltx_tr">
<th id="S3.T6.1.18.16.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" colspan="2">Std</th>
<td id="S3.T6.1.18.16.2" class="ltx_td ltx_align_center ltx_border_bb">13.73</td>
<td id="S3.T6.1.18.16.3" class="ltx_td ltx_align_center ltx_border_bb">7.30</td>
<td id="S3.T6.1.18.16.4" class="ltx_td ltx_align_center ltx_border_bb">13.65</td>
<td id="S3.T6.1.18.16.5" class="ltx_td ltx_align_center ltx_border_bb">13.62</td>
<td id="S3.T6.1.18.16.6" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#DFDFDF;"><span id="S3.T6.1.18.16.6.1" class="ltx_text" style="background-color:#DFDFDF;">7.23</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS3.SSS4.p1" class="ltx_para">
<p id="S3.SS3.SSS4.p1.1" class="ltx_p">As discussed before, in this RQ, we evaluate our ensemble method on the CIFAR-10 (with non-iid=0.4) and ADNI datasets and test the new method on Label Flip and Sign Flip attacks.
We choose these attacks to have attacks from data and model poisoning categories. Furthermore, we select the Sign Flip attack over the Random update from the category of model poisoning attacks since it is more effective as it directs the training process in the opposite direction of the updates rather than just a random direction (we confirmed this in RQ2–3 results).
Given that the Fashion MNIST dataset is less challenging than CIFAR-10, based on RQ1–2 results, we only focus on these two datasets for the sake of space.</p>
</div>
<div id="S3.SS3.SSS4.p2" class="ltx_para">
<p id="S3.SS3.SSS4.p2.1" class="ltx_p">We report the results in Table <a href="#S3.T6" title="Table 6 ‣ 3.3.4 RQ4 results (an ensemble of aggregators): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Out of these 12 configurations, the ensemble aggregator is the best option in Five cases, according to the median accuracies. Running statistical tests (Mann-Whitney U Test) shows that the ensemble is significantly better than the second technique in three cases out of five. However, in the remaining two cases, there is no significant difference. Furthermore, this difference is insignificant in four cases out of the seven remaining cases where the ensemble is not the best. As a result, in 75% of cases, the ensemble aggregator is the most reasonable choice. If were run the same test for other aggregators, Federated Averaging gets 25%, and the rest will achieve 33.3%, which clearly shows the ensemble method is superior.
Lastly, our ensemble technique achieves the highest mean accuracy compared to other aggregators.</p>
</div>
<div id="S3.SS3.SSS4.p3" class="ltx_para">
<p id="S3.SS3.SSS4.p3.1" class="ltx_p">One caveat with our ensemble approach is that it takes longer to train. On average we see around 50% increased training time which is not too bad considering the results. Furthermore, as the overhead is on the server, and servers are not limited like clients, servers can easily make up for this by utilizing more GPU resources. Lastly, this method only has an overhead in the training phase, so clients will not notice any difference in the testing phase (this phase is repetitive and is where users interact with the model).</p>
</div>
<div id="S3.SS3.SSS4.p4" class="ltx_para ltx_noindent">
<svg id="S3.SS3.SSS4.p4.pic1" class="ltx_picture" height="91.21" overflow="visible" version="1.1" width="600"><g transform="translate(0,91.21) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 85.3 C 0 88.57 2.64 91.21 5.91 91.21 L 594.09 91.21 C 597.36 91.21 600 88.57 600 85.3 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 85.3 C 1.97 87.48 3.73 89.24 5.91 89.24 L 594.09 89.24 C 596.27 89.24 598.03 87.48 598.03 85.3 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="63.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S3.SS3.SSS4.p4.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S3.SS3.SSS4.p4.pic1.1.1.1.1.1.1" class="ltx_p"><span id="S3.SS3.SSS4.p4.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Answer to RQ4:</span>
It is possible to offer an ensemble aggregator that is more generalizable (the best choice in 75% of the cases) than any of its constituent aggregators. Also, it shows decent robustness in both data (Label Flip was a representative of this category) and model poisoning (Sign Flip was a representative of this category) attacks without prior knowledge.</span>
</span></foreignObject></g></g></svg>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Discussions</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">As discussed, one of the main challenges that can jeopardize the quality of the FL is byzantine attacks. Our results in Section <a href="#S3.SS3.SSS1" title="3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.1</span></a> confirm this problem and show that attacks and, to some extent, faults can degrade the overall quality of the FL. In Section <a href="#S3.SS3.SSS2" title="3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.2</span></a> we saw that having robust aggregation techniques instead of the basic Federated Averaging can indeed improve the robustness and, consequently, the quality of training.
Although robust aggregators are effective, another factor that challenges the quality of the FL process is the proportion of byzantine clients as shown in Section <a href="#S3.SS3.SSS2" title="3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>. The aggregators typically have a breaking point regarding the number of byzantine clients, and the aggregator might not work past that as intended. For instance, Median’s breaking point is when half of the clients are byzantine <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al., <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>. However, the number of byzantine clients might not be below the aggregators’ breaking point in a real scenario. As a result, there could still be serious quality issues in practice.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Moreover, the choice of the aggregator is non-trivial, and it is one of the most significant challenges we currently face in this domain. Our experiments in Sections <a href="#S3.SS3.SSS2" title="3.3.2 RQ2 results (comparison of aggregators/defense mechanisms): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.2</span></a> and <a href="#S3.SS3.SSS3" title="3.3.3 RQ3 results (the case of a real federated dataset): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.3</span></a> show that there is no perfect aggregator for all cases, and it is imperative to consider the context and settings when applying a specific aggregator.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">Even though selecting aggregators based on the settings seems like a reasonable tactic, it is usually not practical. Since in a real case, essential factors like the dataset and attack type are unknown to the server (consequently the aggregator), i.e., there is no way to know which aggregator would work best. However, in Section <a href="#S3.SS3.SSS4" title="3.3.4 RQ4 results (an ensemble of aggregators): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.4</span></a> results confirm that an ensemble of all aggregators can indeed result in a better and more general aggregator.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">Moreover, as discussed in Section <a href="#S3.SS2.SSS5" title="3.2.5 Threat model ‣ 3.2 Experiment design ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.5</span></a>, some aggregators need extra information to perform decently, which might not be available. For instance, Krum and Trimmed Mean need to know how many byzantine clients are at each round, which is not a realistic assumption. So, using them in an actual application would be problematic unless they use some estimation technique to estimate the number of byzantine clients. Although this seems a solid solution, the choice of estimation technique itself is significant and needs extensive study.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p">An important point to notice is that model attacks, which are far more powerful than poisoning attacks (Shown in Section <a href="#S3.SS3.SSS1" title="3.3.1 RQ1 results (effect of attacks and faults on Federated Averaging): ‣ 3.3 Experiment results ‣ 3 Empirical study ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>), are only possible if the attackers can access and alter the client code. Consequently, it would be ideal to improve the client code security and stop these attacks before they happen. So it would be interesting to see which techniques can be applied here to make it harder for the attacker to implement model poisoning attacks. One idea would be cryptography which is already being used on secure aggregators <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al., <a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S3.SS4.p6" class="ltx_para">
<p id="S3.SS4.p6.1" class="ltx_p">Another exciting follow-up research opportunity in this domain is the client selection strategy in a cross-device FL. The baseline for client selection is choosing clients randomly, at each round <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite>. Recently, new client selection techniques have been introduced to make the overall FL process faster by considering devices’ hardware heterogeneity <cite class="ltx_cite ltx_citemacro_citep">(Nishio and Yonetani, <a href="#bib.bib30" title="" class="ltx_ref">2019</a>; Chai et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>; Lai et al., <a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>. It would be interesting to see how a byzantine-aware client selection technique could improve the overall quality alone and how it would work with a robust aggregation technique.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Limitations and threats to validity</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">One of the limitations of our study is that in RQ4, our proposed technique is not applicable in the Backdoor attack where the backdoor task is unknown. We will consider this as future work and an extension of this study.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">In terms of internal validity, we reused well-known attacks and defenses (robust aggregations) from the existing literature and their replication packages. Two co-authors of the papers carefully went through the implementations. Furthermore, an important point is that we did not use FL-specific frameworks like Tensorflow Federated (TFF). The reason is that many of the aggregators used in our study are not available in FL frameworks. Also, integrating them into the frameworks is not a straightforward task, and it is impossible in some cases due to restricted APIs (e.g., implementation of Krum in TFF).
Furthermore, client-server communications are actually very important and critical features, when one wants to deploy FL. However, in our context and to compare the attacks and defenses, a single machine simulation is enough and the deployment communication features will not affect the aggregator or attacks.
The last question that may arise here is whether a simulation approach like ours is going to have the same performance (in terms of the model accuracy) as a framework with a distributed deployment. The answer is that if factors like the DL framework and its version, the python version, and the hardware are identical and communications are reliable, the results should be the same.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">Concerning external validity, although we have broad systematic coverage of many existing approaches, the choice of dataset and models and the number of attacks, mutators, and aggregators can be questioned.
To mitigate this problem, regarding datasets, we used well-known generic Fashion MNIST and CIFAR-10 datasets and an application-specific ADNI dataset, which is a real-world federated dataset, to improve the generalizability of the findings. In terms of the models, although like many related works <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib22" title="" class="ltx_ref">2019</a>, <a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite> and to make the systematic experiments with all combinations manageable, we used only one model per dataset, we used a diverse set of models across datasets (from simple CNNs to more complex VGG16).
We tried to pick well-established FL untargeted and targeted attacks from both data and model categories regarding the selected attacks. Additionally, we used previously studied mutation operators that were applicable to FL and were based on real faults.
Furthermore, concerning the aggregators, we studied four of the most well-studied techniques in the published FL literature that come with replication packages.
Finally, a question may arise regarding the ensemble method used in RQ4, like why is the technique not considering identifying and filtering the byzantine clients instead? The answer is that aggregators like Krum are doing the same thing, and the proposed method should include a new technique to make up for instances where single aggregators do not perform as expected. Thus, an ensemble will be helpful, and that is why we chose it.</p>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.1" class="ltx_p">Regarding construct validity, we used prediction accuracy as the metric for all the untargeted attacks and faults. Also, for the Backdoor attack, we used the backdoor task accuracy to be consistent with related studies.</p>
</div>
<div id="S3.SS5.p5" class="ltx_para">
<p id="S3.SS5.p5.1" class="ltx_p">In terms of conclusion validity, one potential threat could be the random factors of the study, like client selection for training and selection of byzantine clients. We tried to mitigate this issue by running the experiments 10 times and reporting the median of the results to exclude the outliers. Also, we ran statistical significance tests to show that the observations were not due to chance.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related work</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Robust Federated Learning:</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In recent years, many techniques have been proposed to increase the robustness of FL against the byzantine attacks. These techniques fall into two main categories that we will discuss.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Aggregation techniques:</span>
The first category is aggregation techniques which include the techniques used in our study like Krum, Median, and Trimmed-Mean <cite class="ltx_cite ltx_citemacro_citep">(Blanchard et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>; Yin et al., <a href="#bib.bib41" title="" class="ltx_ref">2018</a>)</cite>. This is the category that is the focus of this study.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">However, other techniques have been proposed like RSA that ensures the robustness in heterogeneous cases (non-iid data) <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>. RSA is different from other robust aggregations discussed in this paper as it is not comparing updates together. Instead, it compares the received models (not the updates) at the server with the global model. It is a norm-based approach that penalizes models that deviate too much from the global model by limiting their contribution to the aggregated model. We could not use this technique since its incomplete replication package was not well-documented.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Another technique introduced recently is called attack-adaptive aggregation, which works with attention models <cite class="ltx_cite ltx_citemacro_citep">(Wan and Chen, <a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite>. The objective of the proposed aggregation model is to assign a weight to each local update in a way that byzantine updates get excluded (they get zero weight assigned to them) and do a weighted averaging at the end. The main drawback of this attention-based aggregation model is that it first needs to be trained on the data collected during a normal FL process (the data is the local updates sent to the server). After the model is trained, it can be used in another FL process for aggregation. Moreover, this technique is neither scalable nor generalizable to different cases. Since our study was comprehensive and this adaptive technique was not generalizable and had an incomplete replication package, we excluded this technique from our experiments.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">Furthermore, another aggregation has been proposed called SEAR, which works in secure environments <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite>. Since secure aggregation uses cryptography, it stops the server from seeing the updates and makes these robust aggregation techniques impossible to use. They proposed a new secure aggregation called SEAR, which relies on Intel Software Guard Extensions (SGX) for secure aggregation. They also proposed a new aggregation technique that works best with the SGX environment. However, their final results regarding final model accuracy are not that different from the aggregators studied in this paper. Since their main contribution is to the security and performance of robust aggregators in the secure environment (and their aggregator alone is not that powerful), we omitted SEAR from our experiments.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">There are also techniques proposed online <cite class="ltx_cite ltx_citemacro_citep">(Fu et al., <a href="#bib.bib11" title="" class="ltx_ref">2021</a>; Pillutla et al., <a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite> which are not published in any peer-reviewed venue yet. Thus we have not included them in this study.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p"><span id="S4.SS1.p7.1.1" class="ltx_text ltx_font_bold">Optimization techniques:</span>
Other than aggregation techniques, new optimization techniques have been proposed to improve the overall quality of FL. The first and most popular technique in this category is FedProx <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib24" title="" class="ltx_ref">2020b</a>)</cite>. The main idea of FedProx is to change the learning rates of clients based on their data distribution to make the convergence faster. FedProx is not designed to resist attacks. Instead, its goal is to improve Federated averaging in non-iid cases.
However, a similar idea has been used in another work called FoolsGold to make FL more robust against Sybil attacks. <cite class="ltx_cite ltx_citemacro_citep">(Fung et al., <a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>. Their approach works based on model updates received at the server. FoolsGold first compares the update vectors based on their similarity and identifies the problematic updates and, consequently, the byzantine clients. Afterward, FoolsGold changes the learning rate of the byzantine clients to make their updates have less contribution in the final model. However, this approach does not work in several cases, like model poisoning attacks. If the attacker sends random updates, the learning has no effect in the final update, and in other cases, clients can multiply their updates without restriction to make up for the learning rate. Furthermore, in model poisoning attacks, the attacker has full control of the byzantine clients; thus, it can set its learning rate to make its attack more effective <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, nullifying what FoolsGold was trying to achieve.</p>
</div>
<div id="S4.SS1.p8" class="ltx_para">
<p id="S4.SS1.p8.1" class="ltx_p">More recently, a new scheme has been proposed called Ditto <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite>. Unlike previous techniques that only change the learning rate, Ditto introduces a completely new optimization algorithm that follows a particular objective to ensure fairness and robustness.</p>
</div>
<div id="S4.SS1.p9" class="ltx_para">
<p id="S4.SS1.p9.1" class="ltx_p">Since these optimization techniques were not the main focus of our study, we did not include them in our experiments.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Empirical evaluation of FL attacks and defenses</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The effect of attacks and aggregators on FL quality is an important topic, and it has been studied extensively.
<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib10" title="" class="ltx_ref">Fang et al.</a></cite> conducted a study on FL aggregators and attacks. They used Label Flip, Random Update, and their proposed attack with Krum, Median, and Trimmed Mean aggregators <cite class="ltx_cite ltx_citemacro_citep">(Fang et al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>. They also have studied the effects of non-iid distribution and the proportion of affected clients but only for one dataset. However, our study has some key differences from their study. Firstly, they omitted Federated Averaging, the baseline of FL, and as we showed before, is better than the other aggregators in the Label Flip attack. Secondly, they only used untargeted attacks in their evaluations, whereas we also studied targeted attacks and mutators to simulate faults. Lastly, they only used naturally centralized datasets in a cross-device setting; in contrast, we also did a comprehensive study on a federated dataset and cross-silo FL setting.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Like the previous related study, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib5" title="" class="ltx_ref">Bhagoji et al.</a></cite> studied attacks in FL (more specifically model poisoning) and introduced a new attack technique <cite class="ltx_cite ltx_citemacro_citep">(Bhagoji et al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>. They introduced a targeted model poisoning attack and showed its effectiveness against Federated Averaging on the Fashion MNIST dataset. They also compared Krum and Median using their attack. This study is more focused on a variation of the targeted attacks. In contrast, we conducted a large-scale study on both untargeted and targeted attacks, mutators, and more aggregation techniques on all attacks. We also considered three datasets, one of which was naturally federated, and studied the effect of different distributions and the proportion of attacked clients.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Another work has surveyed attacks and defenses in FL, which is more theoretical than the previously discussed studies <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al., <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>. They studied the attacks and defenses and compared them using their theoretical details and limitations. Furthermore, they studied different privacy techniques used in FL. Consequently, our empirical study adds a practical value to their theoretical study and explores mentioned limitations in practice.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">A summary of this paper’s contributions compared to some of the more related works is reported in Table <a href="#S4.T7" title="Table 7 ‣ 4.2 Empirical evaluation of FL attacks and defenses ‣ 4 Related work ‣ Towards Understanding Quality Challenges of the Federated Learning for Neural Networks: A First Look from the Lens of Robustness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Contributions of this study compared to related works.</figcaption>
<table id="S4.T7.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.1.1.1" class="ltx_tr">
<th id="S4.T7.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" style="width:60.7pt;"></th>
<th id="S4.T7.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" style="width:43.4pt;">
<span id="S4.T7.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.1.1.2.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite></span>
</span>
</th>
<th id="S4.T7.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" style="width:43.4pt;">
<span id="S4.T7.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.1.1.3.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Wan and Chen (<a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite></span>
</span>
</th>
<th id="S4.T7.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" style="width:43.4pt;">
<span id="S4.T7.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.1.1.4.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Fang et al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite></span>
</span>
</th>
<th id="S4.T7.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" style="width:43.4pt;">
<span id="S4.T7.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.1.1.5.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Bhagoji et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite></span>
</span>
</th>
<th id="S4.T7.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" style="width:43.4pt;">
<span id="S4.T7.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.1.1.6.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Lyu et al. (<a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite></span>
</span>
</th>
<th id="S4.T7.1.1.1.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" style="width:43.4pt;">
<span id="S4.T7.1.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.1.1.7.1.1" class="ltx_p">Our study</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.1.2.1" class="ltx_tr">
<td id="S4.T7.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:60.7pt;">
<span id="S4.T7.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.2.1.1.1.1" class="ltx_p">Aggregators</span>
</span>
</td>
<td id="S4.T7.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:43.4pt;">
<span id="S4.T7.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.2.1.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:43.4pt;">
<span id="S4.T7.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.2.1.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:43.4pt;">
<span id="S4.T7.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.2.1.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.2.1.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:43.4pt;">
<span id="S4.T7.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.2.1.5.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.2.1.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:43.4pt;">
<span id="S4.T7.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.2.1.6.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.2.1.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" style="width:43.4pt;">
<span id="S4.T7.1.2.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.2.1.7.1.1" class="ltx_p">✓</span>
</span>
</td>
</tr>
<tr id="S4.T7.1.3.2" class="ltx_tr">
<td id="S4.T7.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:60.7pt;">
<span id="S4.T7.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.3.2.1.1.1" class="ltx_p">Untargeted</span>
<span id="S4.T7.1.3.2.1.1.2" class="ltx_p">attacks</span>
</span>
</td>
<td id="S4.T7.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.3.2.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.3.2.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.3.2.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.3.2.5.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.3.2.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.3.2.6.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.3.2.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.3.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.3.2.7.1.1" class="ltx_p">✓</span>
</span>
</td>
</tr>
<tr id="S4.T7.1.4.3" class="ltx_tr">
<td id="S4.T7.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:60.7pt;">
<span id="S4.T7.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.4.3.1.1.1" class="ltx_p">Targeted</span>
<span id="S4.T7.1.4.3.1.1.2" class="ltx_p">attacks</span>
</span>
</td>
<td id="S4.T7.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.4.3.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.4.3.5.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.4.3.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.4.3.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.4.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.4.3.7.1.1" class="ltx_p">✓</span>
</span>
</td>
</tr>
<tr id="S4.T7.1.5.4" class="ltx_tr">
<td id="S4.T7.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:60.7pt;">
<span id="S4.T7.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.5.4.1.1.1" class="ltx_p">Faults</span>
</span>
</td>
<td id="S4.T7.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.5.4.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.5.4.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.5.4.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.5.4.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.5.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.5.4.7.1.1" class="ltx_p">✓</span>
</span>
</td>
</tr>
<tr id="S4.T7.1.6.5" class="ltx_tr">
<td id="S4.T7.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:60.7pt;">
<span id="S4.T7.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.6.5.1.1.1" class="ltx_p">Cross-device</span>
</span>
</td>
<td id="S4.T7.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.6.5.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.6.5.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.6.5.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.6.5.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.6.5.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.6.5.5.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.6.5.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.6.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.6.5.6.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.6.5.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.6.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.6.5.7.1.1" class="ltx_p">✓</span>
</span>
</td>
</tr>
<tr id="S4.T7.1.7.6" class="ltx_tr">
<td id="S4.T7.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:60.7pt;">
<span id="S4.T7.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.7.6.1.1.1" class="ltx_p">Cross-silo</span>
</span>
</td>
<td id="S4.T7.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.7.6.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.7.6.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.7.6.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.7.6.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.7.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.7.6.7.1.1" class="ltx_p">✓</span>
</span>
</td>
</tr>
<tr id="S4.T7.1.8.7" class="ltx_tr">
<td id="S4.T7.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:60.7pt;">
<span id="S4.T7.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.8.7.1.1.1" class="ltx_p">Multiple distributions</span>
</span>
</td>
<td id="S4.T7.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.8.7.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.8.7.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.8.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.8.7.4.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.8.7.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.8.7.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.8.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.8.7.6.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.8.7.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.8.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.8.7.7.1.1" class="ltx_p">✓</span>
</span>
</td>
</tr>
<tr id="S4.T7.1.9.8" class="ltx_tr">
<td id="S4.T7.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:60.7pt;">
<span id="S4.T7.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.9.8.1.1.1" class="ltx_p">New aggregation</span>
<span id="S4.T7.1.9.8.1.1.2" class="ltx_p">technique</span>
</span>
</td>
<td id="S4.T7.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.9.8.2.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.9.8.3.1.1" class="ltx_p">✓</span>
</span>
</td>
<td id="S4.T7.1.9.8.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.9.8.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.9.8.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;"></td>
<td id="S4.T7.1.9.8.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.9.8.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.9.8.7.1.1" class="ltx_p">✓</span>
</span>
</td>
</tr>
<tr id="S4.T7.1.10.9" class="ltx_tr">
<td id="S4.T7.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:60.7pt;">
<span id="S4.T7.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.10.9.1.1.1" class="ltx_p">Limitations</span>
</span>
</td>
<td id="S4.T7.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.10.9.2.1.1" class="ltx_p">Technique needs tuning for different cases</span>
</span>
</td>
<td id="S4.T7.1.10.9.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.10.9.3.1.1" class="ltx_p">Not generaizable and scalable</span>
</span>
</td>
<td id="S4.T7.1.10.9.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.10.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.10.9.4.1.1" class="ltx_p">Federated Averaging is not considered</span>
</span>
</td>
<td id="S4.T7.1.10.9.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.10.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.10.9.5.1.1" class="ltx_p">Does not consider different proportion of attackers</span>
</span>
</td>
<td id="S4.T7.1.10.9.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.10.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.10.9.6.1.1" class="ltx_p">The study is purely theoretical</span>
</span>
</td>
<td id="S4.T7.1.10.9.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" style="width:43.4pt;">
<span id="S4.T7.1.10.9.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.10.9.7.1.1" class="ltx_p">New technique works only on untargeted attacks</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and future work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we conducted a large-scale empirical study on the effect of faults and attacks on FL aggregators. We performed our experiments on two generic image datasets, each with three different distributions, one federated medical dataset, eight attacks and mutators, and four aggregation techniques resulting in 496 configurations.
Results show that the Sign Flip and Backdoor attacks are the most effective attacks. Moreover, mutators do not significantly impact FL’s quality, except for the Overlap data mutator, which can affect Federated Averaging in the ADNI dataset. In addition, our study shows that there is no single best robust aggregator, and their accuracy depends on factors such as attack type, dataset, and data distribution. For instance, Krum is most robust in model poisoning attacks, but it is not acceptable in the Label Flip attacks. Inspired by the results of different aggregators, we show that an ensemble of these aggregators can be more robust than (or as good as) any single aggregator to improve the FL process quality in 75% of cases where the attacks and data distribution are unknown to the aggregator.
In the future, we plan to study different attack scenarios, e.g., a case where the attacker is aware of the aggregator used on the server. Also, we want to extend this study to other FL-exclusive issues, such as clients’ machines capabilities, learning frameworks, network failures, and arithmetic computation precision issues. Lastly, we want to extend the ensemble technique to make it effective against targeted attacks like the Backdoor attack and more efficient by using different heuristics while selecting the best aggregator in each round.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
Data collection and sharing for this project were funded by the Alzheimer’s Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions.

</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">cif (2021)</span>
<span class="ltx_bibblock">
(2021) keras/cifar10_cnn.py at master · keras-team/keras · github.
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py</span>

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">mni (2021)</span>
<span class="ltx_bibblock">
(2021) Simple mnist convnet.
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://keras.io/examples/vision/mnist_convnet/</span>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadi et al. (2015)</span>
<span class="ltx_bibblock">
Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, Corrado GS, Davis A,
Dean J, Devin M, Ghemawat S, Goodfellow I, Harp A, Irving G, Isard M, Jia Y,
Jozefowicz R, Kaiser L, Kudlur M, Levenberg J, Mané D, Monga R, Moore S,
Murray D, Olah C, Schuster M, Shlens J, Steiner B, Sutskever I, Talwar K,
Tucker P, Vanhoucke V, Vasudevan V, Viégas F, Vinyals O, Warden P,
Wattenberg M, Wicke M, Yu Y, Zheng X (2015) TensorFlow: Large-scale machine
learning on heterogeneous systems.
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.tensorflow.org/</span>, software available from
tensorflow.org

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bagdasaryan et al. (2020)</span>
<span class="ltx_bibblock">
Bagdasaryan E, Veit A, Hua Y, Estrin D, Shmatikov V (2020) How to backdoor
federated learning. In: Chiappa S, Calandra R (eds) Proceedings of the Twenty
Third International Conference on Artificial Intelligence and Statistics,
PMLR, Proceedings of Machine Learning Research, vol 108, pp 2938–2948,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://proceedings.mlr.press/v108/bagdasaryan20a.html</span>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhagoji et al. (2019)</span>
<span class="ltx_bibblock">
Bhagoji AN, Chakraborty S, Mittal P, Calo SB (2019) Analyzing federated
learning through an adversarial lens. In: Chaudhuri K, Salakhutdinov R (eds)
Proceedings of the 36th International Conference on Machine Learning, ICML
2019, 9-15 June 2019, Long Beach, California, USA, PMLR, Proceedings of
Machine Learning Research, vol 97, pp 634–643,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://proceedings.mlr.press/v97/bhagoji19a.html</span>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blanchard et al. (2017)</span>
<span class="ltx_bibblock">
Blanchard P, El Mhamdi EM, Guerraoui R, Stainer J (2017) Machine learning with
adversaries: Byzantine tolerant gradient descent. In: Guyon I, Luxburg UV,
Bengio S, Wallach H, Fergus R, Vishwanathan S, Garnett R (eds) Advances in
Neural Information Processing Systems, Curran Associates, Inc., vol 30,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://proceedings.neurips.cc/paper/2017/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf</span>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. (2017)</span>
<span class="ltx_bibblock">
Bonawitz K, Ivanov V, Kreuter B, Marcedone A, McMahan HB, Patel S, Ramage D,
Segal A, Seth K (2017) Practical secure aggregation for privacy-preserving
machine learning. In: Proceedings of the 2017 ACM SIGSAC Conference on
Computer and Communications Security, Association for Computing Machinery,
New York, NY, USA, CCS ’17, p 1175–1191, DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3133956.3133982</span>,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3133956.3133982</span>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chai et al. (2020)</span>
<span class="ltx_bibblock">
Chai Z, Ali A, Zawad S, Truex S, Anwar A, Baracaldo N, Zhou Y, Ludwig H, Yan F,
Cheng Y (2020) Tifl: A tier-based federated learning system. In: Proceedings
of the 29th International Symposium on High-Performance Parallel and
Distributed Computing, Association for Computing Machinery, New York, NY,
USA, HPDC ’20, p 125–136, DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3369583.3392686</span>,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3369583.3392686</span>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2017)</span>
<span class="ltx_bibblock">
Chen X, Liu C, Li B, Lu K, Song D (2017) Targeted backdoor attacks on deep
learning systems using data poisoning. <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">1712.05526</span>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2020)</span>
<span class="ltx_bibblock">
Fang M, Cao X, Jia J, Gong NZ (2020) Local model poisoning attacks to
byzantine-robust federated learning. In: Capkun S, Roesner F (eds) 29th
USENIX Security Symposium, USENIX Security 2020, August 12-14, 2020,
USENIX Association, pp 1605–1622,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.usenix.org/conference/usenixsecurity20/presentation/fang</span>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. (2021)</span>
<span class="ltx_bibblock">
Fu S, Xie C, Li B, Chen Q (2021) Attack-resistant federated learning with
residual-based reweighting. <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">1912.11464</span>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fung et al. (2020)</span>
<span class="ltx_bibblock">
Fung C, Yoon CJM, Beschastnikh I (2020) The Limitations of Federated Learning
in Sybil Settings. In: Symposium on Research in Attacks, Intrusion, and
Defenses, RAID

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muñoz González et al. (2017)</span>
<span class="ltx_bibblock">
Muñoz González L, Biggio B, Demontis A, Paudice A, Wongrassamee V, Lupu
EC, Roli F (2017) Towards poisoning of deep learning algorithms with
back-gradient optimization. In: Proceedings of the 10th ACM Workshop on
Artificial Intelligence and Security, Association for Computing Machinery,
New York, NY, USA, AISec ’17, p 27–38, DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3128572.3140451</span>,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3128572.3140451</span>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et al. (2015)</span>
<span class="ltx_bibblock">
Goodfellow IJ, Shlens J, Szegedy C (2015) Explaining and harnessing adversarial
examples. In: Bengio Y, LeCun Y (eds) 3rd International Conference on
Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings, URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://arxiv.org/abs/1412.6572</span>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. (2019)</span>
<span class="ltx_bibblock">
Gu T, Liu K, Dolan-Gavitt B, Garg S (2019) Badnets: Evaluating backdooring
attacks on deep neural networks. IEEE Access 7:47230–47244,
DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ACCESS.2019.2909068</span>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2019)</span>
<span class="ltx_bibblock">
Hu Q, Ma L, Xie X, Yu B, Liu Y, Zhao J (2019) Deepmutation++: A mutation
testing framework for deep learning systems. In: 2019 34th IEEE/ACM
International Conference on Automated Software Engineering (ASE), pp
1158–1161, DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ASE.2019.00126</span>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Humbatova et al. (2021)</span>
<span class="ltx_bibblock">
Humbatova N, Jahangirova G, Tonella P (2021) Deepcrime: Mutation testing of
deep learning systems based on real faults. In: Proceedings of the 30th ACM
SIGSOFT International Symposium on Software Testing and Analysis, Association
for Computing Machinery, New York, NY, USA, ISSTA 2021, p 67–78,
DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3460319.3464825</span>,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3460319.3464825</span>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al. (2021)</span>
<span class="ltx_bibblock">
Kairouz P, McMahan HB, Avent B, Bellet A, Bennis M, Bhagoji AN, Bonawitz K,
Charles Z, Cormode G, Cummings R, D’Oliveira RGL, Eichner H, Rouayheb SE,
Evans D, Gardner J, Garrett Z, Gascón A, Ghazi B, Gibbons PB, Gruteser M,
Harchaoui Z, He C, He L, Huo Z, Hutchinson B, Hsu J, Jaggi M, Javidi T, Joshi
G, Khodak M, Konecný J, Korolova A, Koushanfar F, Koyejo S, Lepoint T, Liu
Y, Mittal P, Mohri M, Nock R, Özgür A, Pagh R, Qi H, Ramage D, Raskar R,
Raykova M, Song D, Song W, Stich SU, Sun Z, Suresh AT, Tramèr F, Vepakomma
P, Wang J, Xiong L, Xu Z, Yang Q, Yu FX, Yu H, Zhao S (2021) Advances and
open problems in federated learning. Foundations and Trends® in Machine
Learning 14(1–2):1–210, DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1561/2200000083</span>,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://dx.doi.org/10.1561/2200000083</span>

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky et al. (2009)</span>
<span class="ltx_bibblock">
Krizhevsky A, Hinton G, et al. (2009) Learning multiple layers of features from
tiny images

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al. (2021)</span>
<span class="ltx_bibblock">
Lai F, Zhu X, Madhyastha HV, Chowdhury M (2021) Oort: Efficient federated
learning via guided participant selection. In: Brown AD, Lorch JR (eds) 15th
USENIX Symposium on Operating Systems Design and Implementation, OSDI
2021, July 14-16, 2021, USENIX Association, pp 19–35,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.usenix.org/conference/osdi21/presentation/lai</span>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lamport et al. (2019)</span>
<span class="ltx_bibblock">
Lamport L, Shostak R, Pease M (2019) The Byzantine Generals Problem,
Association for Computing Machinery, New York, NY, USA, p 203–226.
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1145/3335772.3335936</span>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Li L, Xu W, Chen T, Giannakis GB, Ling Q (2019) RSA: byzantine-robust
stochastic aggregation methods for distributed learning from heterogeneous
datasets. In: The Thirty-Third AAAI Conference on Artificial Intelligence,
AAAI 2019, The Thirty-First Innovative Applications of Artificial
Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on
Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu,
Hawaii, USA, January 27 - February 1, 2019, AAAI Press, pp 1544–1551,
DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1609/aaai.v33i01.33011544</span>,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1609/aaai.v33i01.33011544</span>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020a)</span>
<span class="ltx_bibblock">
Li T, Sahu AK, Talwalkar A, Smith V (2020a) Federated learning:
Challenges, methods, and future directions. IEEE Signal Processing Magazine
37(3):50–60, DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/MSP.2020.2975749</span>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020b)</span>
<span class="ltx_bibblock">
Li T, Sahu AK, Zaheer M, Sanjabi M, Talwalkar A, Smith V (2020b)
Federated optimization in heterogeneous networks. In: Dhillon IS,
Papailiopoulos DS, Sze V (eds) Proceedings of Machine Learning and Systems
2020, MLSys 2020, Austin, TX, USA, March 2-4, 2020, mlsys.org,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://proceedings.mlsys.org/book/316.pdf</span>

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Li T, Hu S, Beirami A, Smith V (2021) Ditto: Fair and robust federated learning
through personalization. In: Meila M, Zhang T (eds) Proceedings of the 38th
International Conference on Machine Learning, PMLR, Proceedings of Machine
Learning Research, vol 139, pp 6357–6368,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://proceedings.mlr.press/v139/li21h.html</span>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2020)</span>
<span class="ltx_bibblock">
Lyu L, Yu H, Ma X, Sun L, Zhao J, Yang Q, Yu PS (2020) Privacy and robustness
in federated learning: Attacks and defenses. <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">2012.06337</span>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madry et al. (2018)</span>
<span class="ltx_bibblock">
Madry A, Makelov A, Schmidt L, Tsipras D, Vladu A (2018) Towards deep learning
models resistant to adversarial attacks. In: 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May
3, 2018, Conference Track Proceedings, OpenReview.net,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://openreview.net/forum?id=rJzIBfZAb</span>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017)</span>
<span class="ltx_bibblock">
McMahan B, Moore E, Ramage D, Hampson S, y Arcas BA (2017)
Communication-efficient learning of deep networks from decentralized data.
In: Singh A, Zhu XJ (eds) Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017,
Fort Lauderdale, FL, USA, PMLR, Proceedings of Machine Learning Research,
vol 54, pp 1273–1282,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://proceedings.mlr.press/v54/mcmahan17a.html</span>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mueller et al. (2005)</span>
<span class="ltx_bibblock">
Mueller SG, Weiner MW, Thal LJ, Petersen RC, Jack C, Jagust W, Trojanowski JQ,
Toga AW, Beckett L (2005) The alzheimer’s disease neuroimaging initiative.
Neuroimaging Clinics of North America 15(4):869

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nishio and Yonetani (2019)</span>
<span class="ltx_bibblock">
Nishio T, Yonetani R (2019) Client selection for federated learning with
heterogeneous resources in mobile edge. In: 2019 IEEE International
Conference on Communications, ICC 2019, Shanghai, China, May 20-24, 2019,
IEEE, pp 1–7, DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICC.2019.8761315</span>,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.1109/ICC.2019.8761315</span>

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pillutla et al. (2019)</span>
<span class="ltx_bibblock">
Pillutla K, Kakade SM, Harchaoui Z (2019) Robust aggregation for federated
learning. <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">1912.13445</span>

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russakovsky et al. (2015)</span>
<span class="ltx_bibblock">
Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, Huang Z, Karpathy A,
Khosla A, Bernstein M, Berg AC, Fei-Fei L (2015) ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer Vision (IJCV)
115(3):211–252, DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/s11263-015-0816-y</span>

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2018)</span>
<span class="ltx_bibblock">
Shen W, Wan J, Chen Z (2018) Munn: Mutation analysis of neural networks. In:
2018 IEEE International Conference on Software Quality, Reliability and
Security Companion (QRS-C), pp 108–115, DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/QRS-C.2018.00032</span>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman (2015)</span>
<span class="ltx_bibblock">
Simonyan K, Zisserman A (2015) Very deep convolutional networks for large-scale
image recognition. In: Bengio Y, LeCun Y (eds) 3rd International Conference
on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings, URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://arxiv.org/abs/1409.1556</span>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2019)</span>
<span class="ltx_bibblock">
Sun Z, Kairouz P, Suresh AT, McMahan HB (2019) Can you really backdoor
federated learning? <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">1911.07963</span>

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan and Chen (2021)</span>
<span class="ltx_bibblock">
Wan CP, Chen Q (2021) Robust federated learning with attack-adaptive
aggregation. <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">2102.05257</span>

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Wang H, Yurochkin M, Sun Y, Papailiopoulos DS, Khazaeni Y (2020) Federated
learning with matched averaging. In: 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020,
OpenReview.net, URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://openreview.net/forum?id=BkluqlSFDS</span>

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019)</span>
<span class="ltx_bibblock">
Wang J, Dong G, Sun J, Wang X, Zhang P (2019) Adversarial sample detection for
deep neural network through model mutation testing. In: 2019 IEEE/ACM 41st
International Conference on Software Engineering (ICSE), pp 1245–1256,
DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICSE.2019.00126</span>

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al. (2021)</span>
<span class="ltx_bibblock">
Xia Q, Ye W, Tao Z, Wu J, Li Q (2021) A survey of federated learning for edge
computing: Research problems and solutions. High-Confidence Computing
1(1):100008, DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">https://doi.org/10.1016/j.hcc.2021.100008</span>,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.sciencedirect.com/science/article/pii/S266729522100009X</span>

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2017)</span>
<span class="ltx_bibblock">
Xiao H, Rasul K, Vollgraf R (2017) Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">cs.LG/1708.07747</span>

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2018)</span>
<span class="ltx_bibblock">
Yin D, Chen Y, Kannan R, Bartlett P (2018) Byzantine-robust distributed
learning: Towards optimal statistical rates. In: Dy J, Krause A (eds)
Proceedings of the 35th International Conference on Machine Learning, PMLR,
Proceedings of Machine Learning Research, vol 80, pp 5650–5659,
URL <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://proceedings.mlr.press/v80/yin18a.html</span>

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2021)</span>
<span class="ltx_bibblock">
Zhao L, Jiang J, Feng B, Wang Q, Shen C, Li Q (2021) Sear: Secure and efficient
aggregation for byzantine-robust federated learning. IEEE Transactions on
Dependable and Secure Computing pp 1–1, DOI <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/TDSC.2021.3093711</span>

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2201.01408" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2201.01409" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2201.01409">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2201.01409" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2201.01410" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 19:58:51 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
