<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.10254] Personalized Federated Learning for Statistical Heterogeneity</title><meta property="og:description" content="The popularity of federated learning (FL) is on the rise, along with growing concerns about data privacy in artificial intelligence applications. FL facilitates collaborative multi-party model learning while simultaneo…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Personalized Federated Learning for Statistical Heterogeneity">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Personalized Federated Learning for Statistical Heterogeneity">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.10254">

<!--Generated on Tue Mar  5 18:43:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
federated learning,  personalization,  non-IID,  statistical heterogeneity,  privacy-preserving
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Personalized Federated Learning for Statistical Heterogeneity
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">1<sup id="id1.1.id1" class="ltx_sup">st</sup>Muhammad Firdaus
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.2.id1" class="ltx_text ltx_font_italic">Department of AI Convergence</span>
<br class="ltx_break"><span id="id3.3.id2" class="ltx_text ltx_font_italic">Pukyong National University
<br class="ltx_break"></span>Busan, Republic of Korea 
<br class="ltx_break">0000-0003-0104-848X
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">2<sup id="id4.1.id1" class="ltx_sup">nd</sup> Kyung-Hyune Rhee
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.2.id1" class="ltx_text ltx_font_italic">Division of Computer Engineering and AI</span>
<br class="ltx_break"><span id="id6.3.id2" class="ltx_text ltx_font_italic">Pukyong National University
<br class="ltx_break"></span>Busan, Republic of Korea 
<br class="ltx_break">0000-0003-0466-8254
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">The popularity of federated learning (FL) is on the rise, along with growing concerns about data privacy in artificial intelligence applications. FL facilitates collaborative multi-party model learning while simultaneously ensuring the preservation of data confidentiality. Nevertheless, the problem of statistical heterogeneity caused by the presence of diverse client data distributions gives rise to certain challenges, such as inadequate personalization and slow convergence. In order to address the above issues, this paper offers a brief summary of the current research progress in the field of personalized federated learning (PFL). It outlines the PFL concept, examines related techniques, and highlights current endeavors. Furthermore, this paper also discusses potential further research and obstacles associated with PFL.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
federated learning, personalization, non-IID, statistical heterogeneity, privacy-preserving

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recently, we have observed favorable advancements through the utilization of machine learning (ML) support. ML facilitates model training by consolidating and aggregating client data on a central server. However, these techniques encounter significant privacy concerns, including the potential for sensitive data exposure, the risk of single points of failure (SPoF), and the substantial resource burdens associated with collecting and storing training data. Moreover, due to the introduction of data privacy regulations like the Health Insurance Portability and Accountability Act (HIPAA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and the General Data Protection Regulation (GDPR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, there is a growing demand for safeguarding privacy in AI, as highlighted by Cheng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
To tackle these challenges, the concept of federated learning (FL) emerged as a promising approach. FL allows distributed mobile devices to collectively train models without centralizing the training data, thus maintaining local data on the client’s devices. Furthermore, FL showcases its effectiveness and privacy preservation by enabling collaborative local training and shared updates to the machine learning model, all while safeguarding the confidentiality of individual datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, despite their advantages, the existing methods of FL encounter several difficulties. These challenges include statistical heterogeneity, resulting in poor convergence and a lack of personalization due to the influence of non-independent and identically distributed (non-IID) data distribution across clients. As a result, these challenges have a detrimental impact on the overall performance of the global FL model when applied to individual clients with eminent performance outputs. Consequently, some clients are reluctant or even unwilling to participate in and contribute to the collaborative FL training. In order to address these issues, the concept of personalized federated learning (PFL) has been introduced <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
This paper provides an overview of the current research landscape in PFL, highlighting its high-level aspects. Additionally, the paper briefly outlines the core concept of PFL and delves into the associated methodologies and ongoing research. Lastly, we also discuss observations and challenges in the field of PFL.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The paper’s structure is organized in the following manner: Section 2 provides an explanation of the background knowledge pertaining to FL and the impact of non-IID data. Section 3 provides a concise overview of the concept of PFL. Subsequently, Section 4 provides an overview of the survey conducted on pertinent methodologies and ongoing research efforts pertaining to PFL. In Section 5, we examine several observations and concerns. Finally, this work is concluded in Section 6.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Preliminaries</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Federated Averaging (FedAvg)</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In the context of the client-server architecture in traditional ML, the process of training models is consistently executed on the server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Clients solely perform as data providers, whereas the server also accomplishes data training and aggregation. However, several concerns are associated with the classical ML strategy, especially regarding user privacy. Therefore, Google introduced federated averaging (FedAvg) as a new communication-efficient optimization algorithm for FL. FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is an effective approach for training models in a distributed manner. This method enables mobile devices to collaborate in training the models without the need to centralize the training data, hence allowing the local data to be stored on the respective mobile devices.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2402.10254/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="230" height="282" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The illustration of FedAvg procedures</figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The illustration of the FedAvg processes is depicted in Figure <a href="#S2.F1" title="Figure 1 ‣ II-A Federated Averaging (FedAvg) ‣ II Preliminaries ‣ Personalized Federated Learning for Statistical Heterogeneity" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Initially, the server, acting as a provider of models, transmits the global model to the clients. Participating clients obtain the global model from the aggregator server, train the current global model on their local data, and then upload the resulting model to the aggregator server. In its role as an aggregator server, the central server collects and consolidates all the model updates from the clients in order to generate an updated global model for the following iteration. Therefore, the FedAvg algorithm efficiently enhances the privacy of clients by preventing some attacks that aim to get unauthorized access to the local training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Additionally, it demonstrates improved communication efficiency compared to traditional distributed stochastic gradient descent (SGD) methods by reducing the number of communication rounds. Moreover, the efficacy of FedAvg is evident in various domains of learning, including predictive models in health <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, low latency vehicle-to-vehicle communication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, vocabulary estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, and next word prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. In each of these instances, FedAvg consistently demonstrates enhanced performance.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Statistical Heterogeneity: The Effect of Non-IID Data</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">While FL offers numerous benefits for various real-world use cases, it also presents several challenges due to the diverse data distribution among clients. This diversity leads to a deficiency in personalization and poor convergence. The data distribution among clients is notably dissimilar, resulting in a situation where the data distribution is mostly non-IID. Consequently, this statistical disparity complicates the training of a single model capable of performing effectively for all clients. Furthermore, the presence of non-IID data can substantially impact the accuracy of the FedAvg algorithm. The local objectives of each client do not align with the global optimum due to the significant differences in data distribution between individual local datasets and the overarching global distribution. As a consequence, a drift occurs in the updated local models<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. In other words, each model is updated during the local training phase towards its specific local optima, which could be far away from the global optima. Primarily, when numerous noteworthy local updates take place (indicating a considerable count of local epochs), the resultant aggregated model may potentially drift from the global optima <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>,<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. As a result, the convergence of the global model is notably less precise compared to the scenario where data distribution is independent and identical (IID).</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Personalized Federated Learning (PFL) Concept</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The concept of PFL comes from shortcomings created by statistical heterogeneity and the non-IID data distribution that force the need for personalization methods in the current FL setting. The accuracy of FedAvg-based techniques suffers a substantial decrease when training non-IID data, mainly due to client drift. The study conducted by Tan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> presents two distinct approaches for achieving PFL. The approaches encompass the global model personalization strategy, which involves the training of a singular global model, and the learning personalized models strategy, which entails the individual training of PFL models. The majority of personalization techniques for a global model FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> often involve two separate procedures: the initial stage entails the collaborative development of a global model, followed by the subsequent phase, which entails utilizing the client’s private data to personalize the global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. In summary, the PFL technique commonly adopts FedAvg as the de facto method for training in broad FL environments. The difference is that after the training process, the trained global model is personalized through additional training and local adaptation steps tailored to the local dataset of the client. The objective of PFL is stated as follows.</p>
</div>
<div id="S3.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\min_{w\in\mathbb{R}^{d}}F\left(w\right)=\frac{1}{M}\sum_{m=1}^{M}f_{m}({w_{m}}+\frac{\mu}{\alpha_{i}})" display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mrow id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml"><mrow id="S3.E1.m1.2.2.3.2" xref="S3.E1.m1.2.2.3.2.cmml"><munder id="S3.E1.m1.2.2.3.2.1" xref="S3.E1.m1.2.2.3.2.1.cmml"><mi id="S3.E1.m1.2.2.3.2.1.2" xref="S3.E1.m1.2.2.3.2.1.2.cmml">min</mi><mrow id="S3.E1.m1.2.2.3.2.1.3" xref="S3.E1.m1.2.2.3.2.1.3.cmml"><mi id="S3.E1.m1.2.2.3.2.1.3.2" xref="S3.E1.m1.2.2.3.2.1.3.2.cmml">w</mi><mo id="S3.E1.m1.2.2.3.2.1.3.1" xref="S3.E1.m1.2.2.3.2.1.3.1.cmml">∈</mo><msup id="S3.E1.m1.2.2.3.2.1.3.3" xref="S3.E1.m1.2.2.3.2.1.3.3.cmml"><mi id="S3.E1.m1.2.2.3.2.1.3.3.2" xref="S3.E1.m1.2.2.3.2.1.3.3.2.cmml">ℝ</mi><mi id="S3.E1.m1.2.2.3.2.1.3.3.3" xref="S3.E1.m1.2.2.3.2.1.3.3.3.cmml">d</mi></msup></mrow></munder><mo lspace="0.167em" id="S3.E1.m1.2.2.3.2a" xref="S3.E1.m1.2.2.3.2.cmml">⁡</mo><mi id="S3.E1.m1.2.2.3.2.2" xref="S3.E1.m1.2.2.3.2.2.cmml">F</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.3.1" xref="S3.E1.m1.2.2.3.1.cmml">​</mo><mrow id="S3.E1.m1.2.2.3.3.2" xref="S3.E1.m1.2.2.3.cmml"><mo id="S3.E1.m1.2.2.3.3.2.1" xref="S3.E1.m1.2.2.3.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">w</mi><mo id="S3.E1.m1.2.2.3.3.2.2" xref="S3.E1.m1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml">=</mo><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.cmml"><mfrac id="S3.E1.m1.2.2.1.3" xref="S3.E1.m1.2.2.1.3.cmml"><mn id="S3.E1.m1.2.2.1.3.2" xref="S3.E1.m1.2.2.1.3.2.cmml">1</mn><mi id="S3.E1.m1.2.2.1.3.3" xref="S3.E1.m1.2.2.1.3.3.cmml">M</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.2.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><munderover id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml"><mo movablelimits="false" id="S3.E1.m1.2.2.1.1.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E1.m1.2.2.1.1.2.2.3" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml"><mi id="S3.E1.m1.2.2.1.1.2.2.3.2" xref="S3.E1.m1.2.2.1.1.2.2.3.2.cmml">m</mi><mo id="S3.E1.m1.2.2.1.1.2.2.3.1" xref="S3.E1.m1.2.2.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E1.m1.2.2.1.1.2.2.3.3" xref="S3.E1.m1.2.2.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.2.2.1.1.2.3" xref="S3.E1.m1.2.2.1.1.2.3.cmml">M</mi></munderover><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.3.2.cmml">f</mi><mi id="S3.E1.m1.2.2.1.1.1.3.3" xref="S3.E1.m1.2.2.1.1.1.3.3.cmml">m</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.2.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.2.cmml">w</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.2.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.3.cmml">m</mi></msub><mo id="S3.E1.m1.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml">+</mo><mfrac id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.2.cmml">μ</mi><msub id="S3.E1.m1.2.2.1.1.1.1.1.1.3.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.3.3.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.3.2.cmml">α</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.3.3.cmml">i</mi></msub></mfrac></mrow><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><eq id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"></eq><apply id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3"><times id="S3.E1.m1.2.2.3.1.cmml" xref="S3.E1.m1.2.2.3.1"></times><apply id="S3.E1.m1.2.2.3.2.cmml" xref="S3.E1.m1.2.2.3.2"><apply id="S3.E1.m1.2.2.3.2.1.cmml" xref="S3.E1.m1.2.2.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.3.2.1.1.cmml" xref="S3.E1.m1.2.2.3.2.1">subscript</csymbol><min id="S3.E1.m1.2.2.3.2.1.2.cmml" xref="S3.E1.m1.2.2.3.2.1.2"></min><apply id="S3.E1.m1.2.2.3.2.1.3.cmml" xref="S3.E1.m1.2.2.3.2.1.3"><in id="S3.E1.m1.2.2.3.2.1.3.1.cmml" xref="S3.E1.m1.2.2.3.2.1.3.1"></in><ci id="S3.E1.m1.2.2.3.2.1.3.2.cmml" xref="S3.E1.m1.2.2.3.2.1.3.2">𝑤</ci><apply id="S3.E1.m1.2.2.3.2.1.3.3.cmml" xref="S3.E1.m1.2.2.3.2.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.3.2.1.3.3.1.cmml" xref="S3.E1.m1.2.2.3.2.1.3.3">superscript</csymbol><ci id="S3.E1.m1.2.2.3.2.1.3.3.2.cmml" xref="S3.E1.m1.2.2.3.2.1.3.3.2">ℝ</ci><ci id="S3.E1.m1.2.2.3.2.1.3.3.3.cmml" xref="S3.E1.m1.2.2.3.2.1.3.3.3">𝑑</ci></apply></apply></apply><ci id="S3.E1.m1.2.2.3.2.2.cmml" xref="S3.E1.m1.2.2.3.2.2">𝐹</ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑤</ci></apply><apply id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.2.2.1"><times id="S3.E1.m1.2.2.1.2.cmml" xref="S3.E1.m1.2.2.1.2"></times><apply id="S3.E1.m1.2.2.1.3.cmml" xref="S3.E1.m1.2.2.1.3"><divide id="S3.E1.m1.2.2.1.3.1.cmml" xref="S3.E1.m1.2.2.1.3"></divide><cn type="integer" id="S3.E1.m1.2.2.1.3.2.cmml" xref="S3.E1.m1.2.2.1.3.2">1</cn><ci id="S3.E1.m1.2.2.1.3.3.cmml" xref="S3.E1.m1.2.2.1.3.3">𝑀</ci></apply><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1"><apply id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2">superscript</csymbol><apply id="S3.E1.m1.2.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2">subscript</csymbol><sum id="S3.E1.m1.2.2.1.1.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2"></sum><apply id="S3.E1.m1.2.2.1.1.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.3"><eq id="S3.E1.m1.2.2.1.1.2.2.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2.3.1"></eq><ci id="S3.E1.m1.2.2.1.1.2.2.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.3.2">𝑚</ci><cn type="integer" id="S3.E1.m1.2.2.1.1.2.2.3.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.3">𝑀</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.2"></times><apply id="S3.E1.m1.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.2">𝑓</ci><ci id="S3.E1.m1.2.2.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3.3">𝑚</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1"><plus id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1"></plus><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.2">𝑤</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.3">𝑚</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3"><divide id="S3.E1.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3"></divide><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.2">𝜇</ci><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.3.2">𝛼</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\min_{w\in\mathbb{R}^{d}}F\left(w\right)=\frac{1}{M}\sum_{m=1}^{M}f_{m}({w_{m}}+\frac{\mu}{\alpha_{i}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.7" class="ltx_p">where <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="F_{m}" display="inline"><semantics id="S3.p3.1.m1.1a"><msub id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mi id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">F</mi><mi id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">𝐹</ci><ci id="S3.p3.1.m1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">F_{m}</annotation></semantics></math> is a loss function associated with the <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="m-" display="inline"><semantics id="S3.p3.2.m2.1a"><mrow id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml"><mi id="S3.p3.2.m2.1.1.2" xref="S3.p3.2.m2.1.1.2.cmml">m</mi><mo id="S3.p3.2.m2.1.1.3" xref="S3.p3.2.m2.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><apply id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1"><csymbol cd="latexml" id="S3.p3.2.m2.1.1.1.cmml" xref="S3.p3.2.m2.1.1">limit-from</csymbol><ci id="S3.p3.2.m2.1.1.2.cmml" xref="S3.p3.2.m2.1.1.2">𝑚</ci><minus id="S3.p3.2.m2.1.1.3.cmml" xref="S3.p3.2.m2.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">m-</annotation></semantics></math>th clients; <math id="S3.p3.3.m3.1" class="ltx_Math" alttext="w\in\mathbb{R}^{d}" display="inline"><semantics id="S3.p3.3.m3.1a"><mrow id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml"><mi id="S3.p3.3.m3.1.1.2" xref="S3.p3.3.m3.1.1.2.cmml">w</mi><mo id="S3.p3.3.m3.1.1.1" xref="S3.p3.3.m3.1.1.1.cmml">∈</mo><msup id="S3.p3.3.m3.1.1.3" xref="S3.p3.3.m3.1.1.3.cmml"><mi id="S3.p3.3.m3.1.1.3.2" xref="S3.p3.3.m3.1.1.3.2.cmml">ℝ</mi><mi id="S3.p3.3.m3.1.1.3.3" xref="S3.p3.3.m3.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><apply id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1"><in id="S3.p3.3.m3.1.1.1.cmml" xref="S3.p3.3.m3.1.1.1"></in><ci id="S3.p3.3.m3.1.1.2.cmml" xref="S3.p3.3.m3.1.1.2">𝑤</ci><apply id="S3.p3.3.m3.1.1.3.cmml" xref="S3.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.p3.3.m3.1.1.3.1.cmml" xref="S3.p3.3.m3.1.1.3">superscript</csymbol><ci id="S3.p3.3.m3.1.1.3.2.cmml" xref="S3.p3.3.m3.1.1.3.2">ℝ</ci><ci id="S3.p3.3.m3.1.1.3.3.cmml" xref="S3.p3.3.m3.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">w\in\mathbb{R}^{d}</annotation></semantics></math> encodes the parameters of local model <math id="S3.p3.4.m4.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">w</annotation></semantics></math>, <math id="S3.p3.5.m5.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S3.p3.5.m5.1a"><mi id="S3.p3.5.m5.1.1" xref="S3.p3.5.m5.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.1b"><ci id="S3.p3.5.m5.1.1.cmml" xref="S3.p3.5.m5.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.1c">\mu</annotation></semantics></math> is the regularization parameter, and <math id="S3.p3.6.m6.1" class="ltx_Math" alttext="\alpha_{p}" display="inline"><semantics id="S3.p3.6.m6.1a"><msub id="S3.p3.6.m6.1.1" xref="S3.p3.6.m6.1.1.cmml"><mi id="S3.p3.6.m6.1.1.2" xref="S3.p3.6.m6.1.1.2.cmml">α</mi><mi id="S3.p3.6.m6.1.1.3" xref="S3.p3.6.m6.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.6.m6.1b"><apply id="S3.p3.6.m6.1.1.cmml" xref="S3.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p3.6.m6.1.1.1.cmml" xref="S3.p3.6.m6.1.1">subscript</csymbol><ci id="S3.p3.6.m6.1.1.2.cmml" xref="S3.p3.6.m6.1.1.2">𝛼</ci><ci id="S3.p3.6.m6.1.1.3.cmml" xref="S3.p3.6.m6.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.6.m6.1c">\alpha_{p}</annotation></semantics></math> represents the local adaptation steps in iteration <math id="S3.p3.7.m7.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.p3.7.m7.1a"><mi id="S3.p3.7.m7.1.1" xref="S3.p3.7.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p3.7.m7.1b"><ci id="S3.p3.7.m7.1.1.cmml" xref="S3.p3.7.m7.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.7.m7.1c">i</annotation></semantics></math>.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">On the other hand, the learning personalized model strategy aims to apply different learning algorithms and modify the process of FL aggregation to achieve PFL by classifying it into architecture-based and similarity methods. Moreover, the objective of PFL is to collaboratively train personalized models for a particular group of clients by leveraging the non-IID nature of their respective data distribution along with simultaneously protecting their private information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Further, the three following goals must all be addressed simultaneously rather than separately in improving a more valuable PFL for practical concerns <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Attaining swift model convergence within a limited set of training iterations.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Enhancing personalized models that are advantageous to a significant portion of clients.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Formulating more precise overarching models that aid clients possessing restricted private data in achieving personalization.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Survey of Related Methods and Current Works for PFL</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we survey the related methods and current works of PFL by separating them into three classifications (i.e., based on data, model, and similarity) for manageable knowledge. Moreover, we provide a classification diagram of PFL that can be seen in Figure 2. Finally, Table 1 summarizes the state-of-the-art methods for the PFL research area.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2402.10254/assets/fig2.png" id="S4.F2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="424" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The classification diagram of PFL</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Data-based Methods</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Data-based methods desire to diminish the client data distributions’ statistical heterogeneity, which is encouraged by the client drift problem. This method consists of capability-based client selection and data augmentation that supports improving the global FL model’s generalization performance.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Capability-based Client Selection</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">: In each FL communication round, these methods optimize the subset of involved clients to enhance the model generalization performance and facilitate sampling from a more uniform distribution of client’s data. The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> proposed FAVOR, a client selection algorithm based on deep Q-learning formulation that aims to minimize the number of communication rounds while maximizing accuracy. To mitigate the impact of non-IID data, FAVOR was designed to selectively choose a certain number of clients to participate in each training round. Nevertheless, deep Q-learning demands computationally higher algorithms that lead to increased computation overhead.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data Augmentation</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">: These approaches are simple to implement in the conventional FL setting that is offered to improve data statistical homogeneity and reduce data inequality. In the study conducted by Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, a data sharing approach was introduced. This technique aims to disseminate a proportionate subset of global data while addressing the issue of imbalanced allocation of client data classes among individual clients. However, potential privacy leakage is the main challenge since the client’s data distribution is frequently shared during training.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Model-based Methods</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Model-based methods obey the prevailing FL training setting that involves training using a single global model. This method consists of meta-learning, transfer learning, regularization with fine-tuning, knowledge distillation, and parameter decoupling.</p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Meta-learning</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">: This method strives to solve new tasks with only a few training examples by generating highly-adaptable models and applying training on multiple learning tasks that can further learn. Moreover, the meta-learning method, also known as initialization-based learning, provides fast adaptation to new tasks with better generalization. Authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> highlight the relationship between FedAvg and model agnostic meta-learning (MAML). The MAML algorithm is executed in two distinct phases, namely meta-testing and meta-training. They note that solely optimizing for global model accuracy may not lead to strong personalization results.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Summary of the state-of-the-art methods for the PFL research.</figcaption>
<div id="S4.T1.1" class="ltx_inline-block ltx_transformed_outer" style="width:527.3pt;height:436.1pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S4.T1.1.1" class="ltx_p"><span id="S4.T1.1.1.1" class="ltx_text">
<span id="S4.T1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T1.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T1.1.1.1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.1.1.1.1.1" class="ltx_p" style="width:65.0pt;"><span id="S4.T1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Paper Ref.</span></span>
</span></span>
<span id="S4.T1.1.1.1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.1.1.2.1.1" class="ltx_p" style="width:30.4pt;"><span id="S4.T1.1.1.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Year</span></span>
</span></span>
<span id="S4.T1.1.1.1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.1.1.3.1.1" class="ltx_p" style="width:56.4pt;"><span id="S4.T1.1.1.1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">PFL Protocols</span></span>
</span></span>
<span id="S4.T1.1.1.1.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.1.1.4.1.1" class="ltx_p" style="width:65.0pt;"><span id="S4.T1.1.1.1.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">PFL Methods</span></span>
</span></span>
<span id="S4.T1.1.1.1.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.1.1.5.1.1" class="ltx_p" style="width:130.1pt;"><span id="S4.T1.1.1.1.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Key Contributions</span></span>
</span></span>
<span id="S4.T1.1.1.1.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.1.1.6.1.1" class="ltx_p" style="width:108.4pt;"><span id="S4.T1.1.1.1.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold">Remarks</span></span>
</span></span></span>
<span id="S4.T1.1.1.1.1.2.2" class="ltx_tr">
<span id="S4.T1.1.1.1.1.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.2.2.1.1.1" class="ltx_p" style="width:65.0pt;">Wang, et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite></span>
</span></span>
<span id="S4.T1.1.1.1.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.2.2.2.1.1" class="ltx_p" style="width:30.4pt;">2020</span>
</span></span>
<span id="S4.T1.1.1.1.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.2.2.3.1.1" class="ltx_p" style="width:56.4pt;">FAVOR</span>
</span></span>
<span id="S4.T1.1.1.1.1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.2.2.4.1.1" class="ltx_p" style="width:65.0pt;">Capability-based client selection</span>
</span></span>
<span id="S4.T1.1.1.1.1.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.2.2.5.1.1" class="ltx_p" style="width:130.1pt;">Enhancing the model generalization performance.</span>
</span></span>
<span id="S4.T1.1.1.1.1.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.2.2.6.1.1" class="ltx_p" style="width:108.4pt;">High communication and computation cost</span>
</span></span></span>
<span id="S4.T1.1.1.1.1.3.3" class="ltx_tr">
<span id="S4.T1.1.1.1.1.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.3.3.1.1.1" class="ltx_p" style="width:65.0pt;">Zhao, et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></span>
</span></span>
<span id="S4.T1.1.1.1.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.3.3.2.1.1" class="ltx_p" style="width:30.4pt;">2018</span>
</span></span>
<span id="S4.T1.1.1.1.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.3.3.3.1.1" class="ltx_p" style="width:56.4pt;">Based on FedAvg</span>
</span></span>
<span id="S4.T1.1.1.1.1.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.3.3.4.1.1" class="ltx_p" style="width:65.0pt;">Data augmentation</span>
</span></span>
<span id="S4.T1.1.1.1.1.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.3.3.5.1.1" class="ltx_p" style="width:130.1pt;">These methods are easy to enforce in the standard FL setting.</span>
</span></span>
<span id="S4.T1.1.1.1.1.3.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.3.3.6.1.1" class="ltx_p" style="width:108.4pt;">Potential privacy leakage is the main challenge</span>
</span></span></span>
<span id="S4.T1.1.1.1.1.4.4" class="ltx_tr">
<span id="S4.T1.1.1.1.1.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.4.4.1.1.1" class="ltx_p" style="width:65.0pt;">Jiang, et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite></span>
</span></span>
<span id="S4.T1.1.1.1.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.4.4.2.1.1" class="ltx_p" style="width:30.4pt;">2019</span>
</span></span>
<span id="S4.T1.1.1.1.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.4.4.3.1.1" class="ltx_p" style="width:56.4pt;">MAML on FedAvg</span>
</span></span>
<span id="S4.T1.1.1.1.1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.4.4.4.1.1" class="ltx_p" style="width:65.0pt;">Meta-learning</span>
</span></span>
<span id="S4.T1.1.1.1.1.4.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.4.4.5.1.1" class="ltx_p" style="width:130.1pt;">Highlight the relationship between model agnostic meta-learning (MAML) and FedAvg.</span>
</span></span>
<span id="S4.T1.1.1.1.1.4.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.4.4.6.1.1" class="ltx_p" style="width:108.4pt;">A single global model setting with centralized approach</span>
</span></span></span>
<span id="S4.T1.1.1.1.1.5.5" class="ltx_tr">
<span id="S4.T1.1.1.1.1.5.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.5.5.1.1.1" class="ltx_p" style="width:65.0pt;">Chen, et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite></span>
</span></span>
<span id="S4.T1.1.1.1.1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.5.5.2.1.1" class="ltx_p" style="width:30.4pt;">2020</span>
</span></span>
<span id="S4.T1.1.1.1.1.5.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.5.5.3.1.1" class="ltx_p" style="width:56.4pt;">FedHealth</span>
</span></span>
<span id="S4.T1.1.1.1.1.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.5.5.4.1.1" class="ltx_p" style="width:65.0pt;">Transfer learning</span>
</span></span>
<span id="S4.T1.1.1.1.1.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.5.5.5.1.1" class="ltx_p" style="width:130.1pt;">Reducing the domain dissimilarity between the local models and global model to improve personalization.</span>
</span></span>
<span id="S4.T1.1.1.1.1.5.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.5.5.6.1.1" class="ltx_p" style="width:108.4pt;">A single global model setting and potential attacks considerations</span>
</span></span></span>
<span id="S4.T1.1.1.1.1.6.6" class="ltx_tr">
<span id="S4.T1.1.1.1.1.6.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.6.6.1.1.1" class="ltx_p" style="width:65.0pt;">Li, et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite></span>
</span></span>
<span id="S4.T1.1.1.1.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.6.6.2.1.1" class="ltx_p" style="width:30.4pt;">2020</span>
</span></span>
<span id="S4.T1.1.1.1.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.6.6.3.1.1" class="ltx_p" style="width:56.4pt;">FedProx</span>
</span></span>
<span id="S4.T1.1.1.1.1.6.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.6.6.4.1.1" class="ltx_p" style="width:65.0pt;">Regularization with fine-tuning</span>
</span></span>
<span id="S4.T1.1.1.1.1.6.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.6.6.5.1.1" class="ltx_p" style="width:130.1pt;">Evaluates the dissimilarity between the global FL model and local models.</span>
</span></span>
<span id="S4.T1.1.1.1.1.6.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.6.6.6.1.1" class="ltx_p" style="width:108.4pt;">A single global model setting with centralized approach</span>
</span></span></span>
<span id="S4.T1.1.1.1.1.7.7" class="ltx_tr">
<span id="S4.T1.1.1.1.1.7.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.7.7.1.1.1" class="ltx_p" style="width:65.0pt;">Li and wang, et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite></span>
</span></span>
<span id="S4.T1.1.1.1.1.7.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.7.7.2.1.1" class="ltx_p" style="width:30.4pt;">2019</span>
</span></span>
<span id="S4.T1.1.1.1.1.7.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.7.7.3.1.1" class="ltx_p" style="width:56.4pt;">FedMD</span>
</span></span>
<span id="S4.T1.1.1.1.1.7.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.7.7.4.1.1" class="ltx_p" style="width:65.0pt;">Knowledge distillation</span>
</span></span>
<span id="S4.T1.1.1.1.1.7.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.7.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.7.7.5.1.1" class="ltx_p" style="width:130.1pt;">Supports resource heterogeneity for each client with communication-efficient.</span>
</span></span>
<span id="S4.T1.1.1.1.1.7.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.7.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.7.7.6.1.1" class="ltx_p" style="width:108.4pt;">Requires the representative proxy datasets and benchmark</span>
</span></span></span>
<span id="S4.T1.1.1.1.1.8.8" class="ltx_tr">
<span id="S4.T1.1.1.1.1.8.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.8.8.1.1.1" class="ltx_p" style="width:65.0pt;">Arivazhagan, et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite></span>
</span></span>
<span id="S4.T1.1.1.1.1.8.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.8.8.2.1.1" class="ltx_p" style="width:30.4pt;">2019</span>
</span></span>
<span id="S4.T1.1.1.1.1.8.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.8.8.3.1.1" class="ltx_p" style="width:56.4pt;">FedPer</span>
</span></span>
<span id="S4.T1.1.1.1.1.8.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.8.8.4.1.1" class="ltx_p" style="width:65.0pt;">Parameter decoupling</span>
</span></span>
<span id="S4.T1.1.1.1.1.8.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.8.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.8.8.5.1.1" class="ltx_p" style="width:130.1pt;">Simple formulation and allows some model layers stored locally while the remaining layers are trained using FL.</span>
</span></span>
<span id="S4.T1.1.1.1.1.8.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.8.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.8.8.6.1.1" class="ltx_p" style="width:108.4pt;">Difficult to determine optimal system and requires the representative proxy datasets and benchmark</span>
</span></span></span>
<span id="S4.T1.1.1.1.1.9.9" class="ltx_tr">
<span id="S4.T1.1.1.1.1.9.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.9.9.1.1.1" class="ltx_p" style="width:65.0pt;">Sattler, et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite></span>
</span></span>
<span id="S4.T1.1.1.1.1.9.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.9.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.9.9.2.1.1" class="ltx_p" style="width:30.4pt;">2020</span>
</span></span>
<span id="S4.T1.1.1.1.1.9.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.9.9.3.1.1" class="ltx_p" style="width:56.4pt;">FMTL</span>
</span></span>
<span id="S4.T1.1.1.1.1.9.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.9.9.4.1.1" class="ltx_p" style="width:65.0pt;">Context-based client clustering</span>
</span></span>
<span id="S4.T1.1.1.1.1.9.9.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.9.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.9.9.5.1.1" class="ltx_p" style="width:130.1pt;">Clustering approaches are beneficial when client partitions are inherently present.</span>
</span></span>
<span id="S4.T1.1.1.1.1.9.9.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.9.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.9.9.6.1.1" class="ltx_p" style="width:108.4pt;">Requires higher computational overhead and potential attacks considerations</span>
</span></span></span>
<span id="S4.T1.1.1.1.1.10.10" class="ltx_tr">
<span id="S4.T1.1.1.1.1.10.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.10.10.1.1.1" class="ltx_p" style="width:65.0pt;">Smith, et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite></span>
</span></span>
<span id="S4.T1.1.1.1.1.10.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.10.10.2.1.1" class="ltx_p" style="width:30.4pt;">2017</span>
</span></span>
<span id="S4.T1.1.1.1.1.10.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.10.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.10.10.3.1.1" class="ltx_p" style="width:56.4pt;">MOCHA</span>
</span></span>
<span id="S4.T1.1.1.1.1.10.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.10.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.10.10.4.1.1" class="ltx_p" style="width:65.0pt;">Multi-task learning</span>
</span></span>
<span id="S4.T1.1.1.1.1.10.10.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.10.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.10.10.5.1.1" class="ltx_p" style="width:130.1pt;">Utilize client relationships in pairs to discover models that are comparable for similar clients.</span>
</span></span>
<span id="S4.T1.1.1.1.1.10.10.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.1.1.1.10.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.10.10.6.1.1" class="ltx_p" style="width:108.4pt;">Requires the representative proxy datasets and benchmark</span>
</span></span></span>
<span id="S4.T1.1.1.1.1.11.11" class="ltx_tr">
<span id="S4.T1.1.1.1.1.11.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_tt">
<span id="S4.T1.1.1.1.1.11.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.11.11.1.1.1" class="ltx_p" style="width:65.0pt;">Peterson, et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite></span>
</span></span>
<span id="S4.T1.1.1.1.1.11.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_tt">
<span id="S4.T1.1.1.1.1.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.11.11.2.1.1" class="ltx_p" style="width:30.4pt;">2019</span>
</span></span>
<span id="S4.T1.1.1.1.1.11.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_tt">
<span id="S4.T1.1.1.1.1.11.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.11.11.3.1.1" class="ltx_p" style="width:56.4pt;">Not defined</span>
</span></span>
<span id="S4.T1.1.1.1.1.11.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_tt">
<span id="S4.T1.1.1.1.1.11.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.11.11.4.1.1" class="ltx_p" style="width:65.0pt;">Interpolation-based Mixture Model</span>
</span></span>
<span id="S4.T1.1.1.1.1.11.11.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_tt">
<span id="S4.T1.1.1.1.1.11.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.11.11.5.1.1" class="ltx_p" style="width:130.1pt;">leverages a mixture of local models and global model.</span>
</span></span>
<span id="S4.T1.1.1.1.1.11.11.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_tt">
<span id="S4.T1.1.1.1.1.11.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.11.11.6.1.1" class="ltx_p" style="width:108.4pt;">A single global model with centralized approach</span>
</span></span></span>
</span>
</span></span></p>
</span></div>
</figure>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Transfer Learning</h4>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">: This method transfers the trained model parameters, called knowledge, from a source to a destination to avoid the necessity to construct models from scratch. Moreover, it lowers the domain dissimilarity between the local and global models to improve personalization. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, the authors propose FedHealth, a novel federated transfer learning framework tailored for wearable healthcare, to address these challenges. They transfer the formerly trained global model to each device after first training a global model using standard FL. Additionally, FedHealth leverages FL for data aggregation and employs transfer learning to build relatively personalized models.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Regularization with Fine-tuning</h4>

<div id="S4.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p1.1" class="ltx_p">: Model regularization is a standard method to improve convergence and prevent overfitting during model training. In order to tackle client drift, regularization is implemented between the local models and global model. The work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> proposed FedProx to alter the effect of updated models by considering the disparity between the local models and the global FL model. FedProx is a generalized and re-parametrized version of FedAvg, designed to address heterogeneity in federated networks.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Knowledge Distillation</h4>

<div id="S4.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px4.p1.1" class="ltx_p">: The primary purpose of this method is to support resource heterogeneity for each client with communication-efficient. The authors in<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> proposed FedMD protocol to design a high degree of flexibility for clients through PFL that facilitates creating various models utilizing their private data. This framework accommodates scenarios where each participant designs their own model due to intellectual property concerns and varying tasks and data.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Parameter Decoupling</h4>

<div id="S4.SS2.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px5.p1.1" class="ltx_p">: This method seeks to gain PFL from the global FL model parameters by decoupling the local private model parameters. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, the authors suggested that some model layers can be stored locally while the remaining layers are trained using FL. In this sense, this approach includes a base model along with a personalization layer to address the challenge of statistical heterogeneity in FL.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Similarity-based Methods</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Similarity-based methods seek to accomplish PFL by mapping client associations. Every client learns a personalized model, while the associated clients learn similar models. PFL has investigated many client relationship types, including clustering takes into account group-level client relationships, whereas model interpolation and multi-task learning take into account pairwise client ties.</p>
</div>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Context-based Client Clustering</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">: When client partitions are inherently present, clustering approaches are beneficial. It is more appropriate to use a multi-model method where an FL model is trained for every similar group of clients. Hierarchical clustering has been added to FL in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> as a postprocessing step. The clients in FL are grouped together using an optimal partitioning technique that relies on the cosine similarity of the gradient updates generated by the clients.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Multi-task Learning</h4>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p">: Smith et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> introduced MOCHA, an algorithm designed to address the challenge of federated multitask learning. This method aims to learn both user task settings and a similarity matrix concurrently. The distributed multitask MOCHA algorithm tackles several learning challenges, such as communication restrictions, stragglers, and fault tolerance. They concentrate on the convex case, and it is opaque how they involve deep learning models that are not convex when powerful duality is no longer assured.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Interpolation-based Mixture Model</h4>

<div id="S4.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px3.p1.1" class="ltx_p">: This method is a simple formulation that leverages a mixture of global model and local models. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, the authors suggested using methods from various expert literature. Their method of model interpolation learns an interpolation weight depending on features to improve model accuracy for all users. They validate their approach using both real and synthetic data, highlighting its effectiveness in enhancing FL performance.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Discussion and Remarks</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Recently, PFL research has been garnering attention due to its potential to tackle the fundamental challenges of current FL, including statistical heterogeneity that leads to poor convergence and a lack of personalization because of the influence of non-IID data distribution across clients. However, based on the explanation in Table <a href="#S4.T1" title="TABLE I ‣ Meta-learning ‣ IV-B Model-based Methods ‣ IV Survey of Related Methods and Current Works for PFL ‣ Personalized Federated Learning for Statistical Heterogeneity" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> about the survey on PFL methods, we can see that several remarks need to be considered for future directions, as follows:</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reliable computation with multiple models</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">: The existing PFL methods still rely on a centralized strategy where the server orchestrates all processes, demanding high communication and incurring computational costs. Furthermore, most methods employ a single global model, which is not suitable and poses challenges for achieving PFL due to notable dissimilarities in data distribution among clients (i.e., non-IID). To address the challenges mentioned, a solution that combines edge computing with a multi-model approach should be considered. This approach aims to reduce communication overhead, adapt models to client-specific data distributions, and enhance the overall efficiency and effectiveness of PFL.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Privacy-preserving</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">: While the PFL framework shows potential as a more favorable approach in contrast to the traditional centralized model training framework, it encounters notable difficulties, particularly concerning the selection of users for the model training procedure and the technique of augmenting data. Moreover, the problem shifts even worse when malicious clients exist and conduct various adversarial activities, such as providing incorrect information messages known as false data injection attacks [14] during collaborative training. Therefore, several privacy techniques, such as secure multiparty computation, homomorphic encryption, differential privacy, and trusted execution environment (TEE), might be leveraged for PFL in further research.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Trustworthy PFL</h4>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p1.1" class="ltx_p">: Recent studies have shown FL flaws that may possibly endanger client security and privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Hence, it is of utmost importance to study attack methods on FL and formulate protective strategies to counteract them, thereby guaranteeing the security of the FL system. In order to develop resilient PFL approaches, additional research is necessary to delve into various forms of attacks and corresponding safeguards, especially as more intricate PFL protocols and structures are being introduced. Additionally, designing incentive mechanisms is a viable study area for achieving these goals to maintain fairness and motivate clients’ contributions. Blockchain, as a distributed ledger technology, may be used to construct decentralized incentive systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> . These systems can play a crucial role in promoting the development of future collaborative PFL scenarios</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Representative Datasets and Benchmark</h4>

<div id="S5.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p1.1" class="ltx_p">: The growth of the PFL field depends on representative datasets. Additional datasets encompassing a wider range of modalities like sensor data, video, and audio, and encompassing a more diverse set of machine learning tasks pertinent to real-world use cases, are necessary to underpin PFL investigations. Moreover, conducting performance benchmarking holds equal importance as a pivotal factor in fostering the continuous advancement of the PFL research domain in the long run.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The notion of personalized federated learning is introduced to tackle statistical heterogeneity that leads to poor convergence and a lack of personalization due to the influence of non-IID data distribution among clients. This paper comprehensively summarizes the present research landscape in personalized federated learning (PFL). It also offers a concise explanation of the PFL concept and investigates associated techniques and ongoing efforts, encompassing data-based, model-based, and similarity-based methods. Lastly, this paper also discusses the need for further research and development in the field of privacy-preserving and trustworthy PFL. Additionally, it highlights the need to use representative datasets and benchmarks to ensure the effectiveness and reliability of PFL methods.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research was supported by the Republic of Korea’s MSIT (Ministry of Science and ICT), under the ICT Convergence Industry Innovation Technology Development Project (2022-0-00614) supervised by the IITP and partially supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (No. 2021R1I1A3046590).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> G. J. Annas, “Hipaa regulations: A new era of medicalrecord privacy?” New England Journal of Medicine, vol. 348, p. 1486, 2003

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> P. Voigt and A. Von dem Bussche, “The eu general data protection regulation (gdpr),” A Practical Guide, 1st Ed., Cham: Springer International Publishing, vol. 10, no. 3152676, pp. 10–5555, 2017

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> Y. Cheng, Y. Liu, T. Chen, and Q. Yang, “Federated learning for privacy-preserving ai,” Communications of the ACM, vol. 63, no. 12, pp. 33–36, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> J. Posner, L. Tseng, M. Aloqaily, and Y. Jararweh, “Federated learning in vehicular networks: Opportunities and solutions,” IEEE Network, vol. 35, no. 2, pp. 152–159, 2021

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> M. Firdaus, S. Noh, Z. Qian, H. T. Larasati, and K.-H. Rhee, “Personalized federated learning for heterogeneous data: A distributed edge clustering approach,” Mathematical Biosciences and Engineering, vol. 20, no. 6, pp. 10 725–10 740, 2023

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> H. T. Larasati, M. Firdaus, and H. Kim, “Quantum federated learning: Remarks and challenges,” in 2022 IEEE 9th International Conference on Cyber Security and Cloud Computing (CSCloud)/2022 IEEE 8th International Conference on Edge Computing and Scalable Cloud (EdgeCom), IEEE, 2022, pp. 1–5

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"> B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efficient learning of deep networks from decentralized data,” in Artificial intelligence and statistics, PMLR, 2017, pp. 1273–1282.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> X. Zhu, H. Li, and Y. Yu, “Blockchain-based privacy preserving deep learning,” in International Conference on Information Security and Cryptology, Springer, 2018, pp. 370–383

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> T. S. Brisimi, R. Chen, T. Mela, A. Olshevsky, I. C. Paschalidis, and W. Shi, “Federated learning of predictive models from federated electronic health records,” International journal of medical informatics, vol. 112, pp. 59–67, 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> S. Samarakoon, M. Bennis, W. Saad, and M. Debbah, “Federated learning for ultra-reliable low-latency v2v communications,” in 2018 IEEE Global Communications Conference (GLOBECOM), IEEE, 2018, pp. 1–7.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"> M. Chen, R. Mathews, T. Ouyang, and F. Beaufays, “Federated learning of out-of-vocabulary words,” arXiv preprint arXiv:1903.10635, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"> A. Hard, K. Rao, R. Mathews, et al., “Federated learning for mobile keyboard prediction,” arXiv preprint arXiv:1811.03604, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"> S. P. Karimireddy, S. Kale, M. Mohri, S. J. Reddi, S. U. Stich, and A. T. Suresh, “Scaffold: Stochastic controlled averaging for on-device federated learning.,” 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"> T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith, “Federated optimization in heterogeneous networks,” Proceedings of Machine Learning and Systems, vol. 2, pp. 429–450, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"> J. Wang, Q. Liu, H. Liang, G. Joshi, and H. V. Poor, “Tackling the objective inconsistency problem in heterogeneous federated optimization,” Advances in neural information processing systems, vol. 33, pp. 7611–7623, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"> A. Z. Tan, H. Yu, L. Cui, and Q. Yang, “Towards personalized federated learning,” IEEE Transactions on Neural Networks and Learning Systems, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"> K. C. Sim, P. Zadrazil, and F. Beaufays, “An investigation into on-device personalization of end-to-end automatic speech recognition models,” arXiv preprint arXiv:1909.06678, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"> V. Kulkarni, M. Kulkarni, and A. Pant, “Survey of personalization techniques for federated learning,” in 2020 Fourth World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4), IEEE, 2020, pp. 794–797.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"> M. Firdaus, S. Noh, Z. Qian, and K.-H. Rhee, “Bpfl: Blockchain-enabled distributed edge cluster for personalized federated learning,” in International Conference on Computer Science and its Applications and the International Conference on Ubiquitous Information Technologies and Applications, Springer, 2022, pp. 431–437

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"> Y. Jiang, J. Konecn ˇ y, K. Rush, and S. Kannan, “Improving federated learning personalization via model agnostic meta learning,” arXiv preprint arXiv:1909.12488, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"> H. Wang, Z. Kaplan, D. Niu, and B. Li, “Optimizing
federated learning on non-iid data with reinforcement learning,” in IEEE INFOCOM 2020-IEEE Conference on Computer Communications, IEEE, 2020, pp. 1698– 1707.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"> Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Federated learning with non-iid data,” arXiv preprint arXiv:1806.00582, 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"> Y. Chen, X. Qin, J. Wang, C. Yu, and W. Gao, “Fedhealth: A federated transfer learning framework for wearable healthcare,” IEEE Intelligent Systems, vol. 35, no. 4, pp. 83–93, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"> D. Li and J. Wang, “Fedmd: Heterogenous federated learning via model distillation,” arXiv preprint arXiv:1910.03581, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"> M. G. Arivazhagan, V. Aggarwal, A. K. Singh, and S. Choudhary, “Federated learning with personalization layers,” arXiv preprint arXiv:1912.00818, 2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"> F. Sattler, K.-R. Muller, and W. Samek, “Clustered ¨ federated learning: Model-agnostic distributed multitask optimization under privacy constraints,” IEEE transactions on neural networks and learning systems, vol. 32, no. 8, pp. 3710–3722, 2020.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"> V. Smith, C.-K. Chiang, M. Sanjabi, and A. S. Talwalkar, “Federated multi-task learning,” Advances in neural information processing systems, vol. 30, 2017.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"> D. Peterson, P. Kanani, and V. J. Marathe, “Private federated learning with domain adaptation,” arXiv preprint arXiv:1912.06733, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"> L. Lyu, H. Yu, X. Ma, et al., “Privacy and robustness
in federated learning: Attacks and defenses,” arXiv preprint arXiv:2012.06337, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"> M. Firdaus and K.-H. Rhee, “A joint framework to privacy-preserving edge intelligence in vehicular networks,” in International Conference on Information Security Applications, Springer, 2022, pp. 156–167.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"> Firdaus, M., and Rhee, K. H. (2023, December). Towards Trustworthy Collaborative Healthcare Data Sharing. In 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) (pp. 4059-4064). IEEE.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.10253" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.10254" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.10254">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.10254" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.10255" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 18:43:27 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
