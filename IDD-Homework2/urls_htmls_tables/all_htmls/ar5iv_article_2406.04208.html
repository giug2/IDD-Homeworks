<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  Aligning Agents like Large Language Models
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Adam Jelley
    <sup class="ltx_sup" id="id3.3.id1">
     †
    </sup>
    <br class="ltx_break"/>
    University of Edinburgh
&amp;Yuhan Cao
    <br class="ltx_break"/>
    Microsoft Research
&amp;Dave Bignell
    <br class="ltx_break"/>
    Microsoft Research
Sam Devlin
    <br class="ltx_break"/>
    Microsoft Research
&amp;Tabish Rashid
    <br class="ltx_break"/>
    Microsoft Research
   </span>
   <span class="ltx_author_notes">
    <sup class="ltx_sup" id="id4.4.id1">
     †
    </sup>
    Work done at Microsoft Research Cambridge. Correspondence to:
    <span class="ltx_text ltx_font_typewriter" id="id5.5.id2">
     adam.jelley@ed.ac.uk
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id6.id1">
   Training agents to behave as desired in complex 3D environments from high-dimensional sensory information is challenging. Imitation learning from diverse human behavior provides a scalable approach for training an agent with a sensible behavioral prior, but such an agent may not perform the specific behaviors of interest when deployed. To address this issue, we draw an analogy between the undesirable behaviors of imitation learning agents and the unhelpful responses of unaligned large language models (LLMs). We then investigate how the procedure for aligning LLMs can be applied to aligning agents in a 3D environment from pixels. For our analysis, we utilize an academically illustrative part of a modern console game in which the human behavior distribution is multi-modal, but we want our agent to imitate a single mode of this behavior. We demonstrate that we can align our agent to consistently perform the desired mode, while providing insights and advice for successfully applying this approach to training agents. Project webpage at:
   <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://adamjelley.github.io/aligning-agents-like-llms/" target="_blank" title="">
    https://adamjelley.github.io/aligning-agents-like-llms/
   </a>
   .
  </p>
 </div>
 <figure class="ltx_figure" id="S0.F1">
  <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="225" id="S0.F1.g1" src="/html/2406.04208/assets/x1.png" width="360"/>
  <figcaption class="ltx_caption ltx_centering">
   <span class="ltx_tag ltx_tag_figure">
    <span class="ltx_text" id="S0.F1.2.1.1" style="font-size:90%;">
     Figure 1
    </span>
    :
   </span>
   <span class="ltx_text" id="S0.F1.3.2" style="font-size:90%;">
    Illustration of our approach for aligning a generally capable agent with a game designer’s intended goals or preferences. A general agent pre-trained to imitate a large diverse dataset of human gameplay provides a base agent which can be more easily fine-tuned to imitate a smaller task or demonstration dataset. This agent can then be further fine-tuned with reinforcement learning using a reward model learned from preferences to reliably achieve the target behavior with the desired style. This approach is analogous to the alignment procedure used for recent large language models.
   </span>
  </figcaption>
 </figure>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    The optimal approach for training generally competent agents to act in complex 3D environments without carefully crafted reward functions is an open question. Many modern console games provide such 3D environments, where the state space, action space and temporal horizons are large enough that learning from scratch is usually infeasible, even with a clearly defined goal. In these environments, a natural approach is to leverage a large dataset of general human behavior to pre-train an agent with imitation learning. This provides an agent with a general understanding of player behavior, and there is evidence of generalization benefits from scaling up data and compute
    <cite class="ltx_cite ltx_citemacro_citep">
     (Lee et al.,
     <a class="ltx_ref" href="#bib.bib32" title="">
      2022
     </a>
     ; Baker et al.,
     <a class="ltx_ref" href="#bib.bib7" title="">
      2022
     </a>
     ; Reed et al.,
     <a class="ltx_ref" href="#bib.bib45" title="">
      2022
     </a>
     ; Brohan et al.,
     <a class="ltx_ref" href="#bib.bib13" title="">
      2023
     </a>
     ; Bousmalis et al.,
     <a class="ltx_ref" href="#bib.bib11" title="">
      2023
     </a>
     ; SIMA-Team et al.,
     <a class="ltx_ref" href="#bib.bib49" title="">
      2024
     </a>
     )
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    However, such an agent will inevitably learn to imitate all behaviors found within the human gameplay, including undesirable behaviors of novice or malicious players, analogous to unhelpful or toxic responses of unaligned large language models
    <cite class="ltx_cite ltx_citemacro_citep">
     (Ziegler et al.,
     <a class="ltx_ref" href="#bib.bib58" title="">
      2020
     </a>
     )
    </cite>
    . Additionally, game designers may have preferences for agents to act with a certain style or strategy, for which it may be difficult to codify a suitable reward function
    <cite class="ltx_cite ltx_citemacro_citep">
     (Aytemiz et al.,
     <a class="ltx_ref" href="#bib.bib4" title="">
      2021
     </a>
     ; Devlin et al.,
     <a class="ltx_ref" href="#bib.bib19" title="">
      2021
     </a>
     )
    </cite>
    . There is a parallel between training useful agents and aligning large language models. In the same way that LLMs have been aligned to serve as customer service chatbots
    <cite class="ltx_cite ltx_citemacro_citep">
     (Banerjee et al.,
     <a class="ltx_ref" href="#bib.bib8" title="">
      2023
     </a>
     )
    </cite>
    , search engines
    <cite class="ltx_cite ltx_citemacro_citep">
     (Spatharioti et al.,
     <a class="ltx_ref" href="#bib.bib50" title="">
      2023
     </a>
     )
    </cite>
    and code generation assistants
    <cite class="ltx_cite ltx_citemacro_citep">
     (Chen et al.,
     <a class="ltx_ref" href="#bib.bib16" title="">
      2021b
     </a>
     )
    </cite>
    , we can imagine aligning a large imitation learning agent with various objectives within a game. For example, we may wish to train agents that can act as allies, opponents
    <cite class="ltx_cite ltx_citemacro_citep">
     (Vinyals et al.,
     <a class="ltx_ref" href="#bib.bib54" title="">
      2019
     </a>
     )
    </cite>
    , non-player-characters (NPCs)
    <cite class="ltx_cite ltx_citemacro_citep">
     (Alonso et al.,
     <a class="ltx_ref" href="#bib.bib3" title="">
      2020
     </a>
     )
    </cite>
    , or even for quality assurance during development
    <cite class="ltx_cite ltx_citemacro_citep">
     (Bergdahl et al.,
     <a class="ltx_ref" href="#bib.bib10" title="">
      2021
     </a>
     )
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    In this work, we take inspiration from the current procedure for training large language models (LLMs) and investigate applying this procedure to train agents. We consider a situation where the human behavior distribution is distinctly multi-modal over non-negligible time horizons (of the order of
    <math alttext="\sim 10" class="ltx_Math" display="inline" id="S1.p3.1.m1.1">
     <semantics id="S1.p3.1.m1.1a">
      <mrow id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">
       <mi id="S1.p3.1.m1.1.1.2" xref="S1.p3.1.m1.1.1.2.cmml">
       </mi>
       <mo id="S1.p3.1.m1.1.1.1" xref="S1.p3.1.m1.1.1.1.cmml">
        ∼
       </mo>
       <mn id="S1.p3.1.m1.1.1.3" xref="S1.p3.1.m1.1.1.3.cmml">
        10
       </mn>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b">
       <apply id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">
        <csymbol cd="latexml" id="S1.p3.1.m1.1.1.1.cmml" xref="S1.p3.1.m1.1.1.1">
         similar-to
        </csymbol>
        <csymbol cd="latexml" id="S1.p3.1.m1.1.1.2.cmml" xref="S1.p3.1.m1.1.1.2">
         absent
        </csymbol>
        <cn id="S1.p3.1.m1.1.1.3.cmml" type="integer" xref="S1.p3.1.m1.1.1.3">
         10
        </cn>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">
       \sim 10
      </annotation>
     </semantics>
    </math>
    seconds), but we desire our agent to imitate a single mode of this distribution. This particular mode of behavior may be difficult to learn from scratch with reinforcement learning since there is no clear reward function, but an imitation learning agent pre-trained on general gameplay data would only sometimes sample the desired behavior mode. At the same time, it may be infeasible to obtain sufficient clean demonstrations for robust imitation learning. To isolate this problem, we focus on an academically illustrative part of a modern console game where players must navigate from a randomly selected spawn point to one of three jumppads.
We find that a base imitation learning agent reaches all three jumppads in similar proportions to the human data.
We then demonstrate that we can align this agent to consistently reach a single preferred jumppad, using synthetic preference labelling to train a reward model followed by online reinforcement learning using this reward model.
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    Our contributions include: 1) An analysis of the importance and potential difficulties of applying each stage of the current LLM training pipeline to agents, including unsupervised pre-training, supervised fine-tuning, preference modelling and online alignment, 2) Evidence of representation transfer between imitation learning and preference learning that highlights the benefits of using the base agent to initialise the reward model when learning preferences from pixels, and 3) The introduction of an additional training stage,
    <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">
     preference fine-tuning
    </span>
    , to substantially improve alignment efficiency. We perform the entire procedure from visual inputs with full controller action output, maintaining the generality of our approach to other game environments, and demonstrating that the modern LLM training paradigm can be successfully applied to aligning agents on a real 3D game.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Game Environment and Alignment Goal
  </h2>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    Environment
   </h3>
   <div class="ltx_para" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     For this paper, we utilize the video game Bleeding Edge
     <span class="ltx_note ltx_role_footnote" id="footnote1">
      <sup class="ltx_note_mark">
       1
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         1
        </sup>
        <span class="ltx_tag ltx_tag_note">
         1
        </span>
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bleedingedge.com/en" target="_blank" title="">
         https://www.bleedingedge.com/en
        </a>
       </span>
      </span>
     </span>
     , which was launched in 2020 for Xbox One
     <span class="ltx_note ltx_role_footnote" id="footnote2">
      <sup class="ltx_note_mark">
       2
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         2
        </sup>
        <span class="ltx_tag ltx_tag_note">
         2
        </span>
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pcgamingwiki.com/wiki/Bleeding_Edge" target="_blank" title="">
         https://www.pcgamingwiki.com/wiki/Bleeding_Edge
        </a>
       </span>
      </span>
     </span>
     . It is a team-based 4v4 online multi-player video game. Players select from thirteen possible heroes, each with different abilities. The game is played with a third-person view, with the camera angle controlled by the player, so the environment is partially observable. Here we focus on a single map, called Skygarden. This map is spread over three islands each with multiple elevation levels, including a main island and two launch islands (one for each team).
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2
    </span>
    Alignment Objective
   </h3>
   <div class="ltx_para" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     At the beginning of the game, players spawn at one of four nearby points on their team’s launch island. From the spawn location a player must navigate across the launch island to one of three jumppads that launch the player from their launch island onto the main island. Depending on the jumppad selected, the player will be launched onto different areas of the main island, so players may wish to take different jumppads during a match depending on the location of opposing team players. For the purpose of this work, we aim to train an agent to consistently navigate across the launch island to a single one of the three jumppads. Since this navigation task requires around 10 seconds to complete for an optimal agent, a random agent will rarely leave the spawn area. Additionally, without access to privileged information such as the agent location (we only provide visual input as below) and an externally shaped reward function, it would be difficult to train an agent to complete this task. On the other hand, an imitation learning agent will learn to reach all three of the jumppads, as in the human distribution. Therefore while the task is simple, it provides a clear motivation for alignment and a setting which we can quantitatively analyse.
    </p>
   </div>
   <figure class="ltx_figure" id="S2.F2">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_3">
      <figure class="ltx_figure ltx_figure_panel" id="S2.F2.sf1">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="276" id="S2.F2.sf1.g1" src="/html/2406.04208/assets/x2.png" width="461"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S2.F2.sf1.2.1.1" style="font-size:90%;">
          (a)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_3">
      <figure class="ltx_figure ltx_figure_panel" id="S2.F2.sf2">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="353" id="S2.F2.sf2.g1" src="/html/2406.04208/assets/figures/Screenshot_Jumppad_New.png" width="598"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S2.F2.sf2.2.1.1" style="font-size:90%;">
          (b)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_3">
      <figure class="ltx_figure ltx_figure_panel" id="S2.F2.sf3">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="273" id="S2.F2.sf3.g1" src="/html/2406.04208/assets/x3.png" width="461"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         <span class="ltx_text" id="S2.F2.sf3.2.1.1" style="font-size:90%;">
          (c)
         </span>
        </span>
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">
       Figure 2
      </span>
      :
     </span>
     <span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">
      Screenshots of the agent at a spawn point (
      <a class="ltx_ref" href="#S2.F2.sf1" title="In Figure 2 ‣ 2.2 Alignment Objective ‣ 2 Game Environment and Alignment Goal ‣ Aligning Agents like Large Language Models">
       <span class="ltx_text ltx_ref_tag">
        2(a)
       </span>
      </a>
      ), heading towards the middle jumppad (
      <a class="ltx_ref" href="#S2.F2.sf2" title="In Figure 2 ‣ 2.2 Alignment Objective ‣ 2 Game Environment and Alignment Goal ‣ Aligning Agents like Large Language Models">
       <span class="ltx_text ltx_ref_tag">
        2(b)
       </span>
      </a>
      ), and looking back at the launch island after having launched from the right jumppad (
      <a class="ltx_ref" href="#S2.F2.sf3" title="In Figure 2 ‣ 2.2 Alignment Objective ‣ 2 Game Environment and Alignment Goal ‣ Aligning Agents like Large Language Models">
       <span class="ltx_text ltx_ref_tag">
        2(c)
       </span>
      </a>
      ).
     </span>
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.3
    </span>
    Observation and Action Spaces
   </h3>
   <div class="ltx_para" id="S2.SS3.p1">
    <p class="ltx_p" id="S2.SS3.p1.4">
     To maintain the generality of our approach to any 3D environment, we take visual gameplay observations
     <math alttext="\mathbf{o}_{t}\in\mathbb{R}^{H\times W\times 3}" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1.1">
      <semantics id="S2.SS3.p1.1.m1.1a">
       <mrow id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">
        <msub id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">
         <mi id="S2.SS3.p1.1.m1.1.1.2.2" xref="S2.SS3.p1.1.m1.1.1.2.2.cmml">
          𝐨
         </mi>
         <mi id="S2.SS3.p1.1.m1.1.1.2.3" xref="S2.SS3.p1.1.m1.1.1.2.3.cmml">
          t
         </mi>
        </msub>
        <mo id="S2.SS3.p1.1.m1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.cmml">
         ∈
        </mo>
        <msup id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml">
         <mi id="S2.SS3.p1.1.m1.1.1.3.2" xref="S2.SS3.p1.1.m1.1.1.3.2.cmml">
          ℝ
         </mi>
         <mrow id="S2.SS3.p1.1.m1.1.1.3.3" xref="S2.SS3.p1.1.m1.1.1.3.3.cmml">
          <mi id="S2.SS3.p1.1.m1.1.1.3.3.2" xref="S2.SS3.p1.1.m1.1.1.3.3.2.cmml">
           H
          </mi>
          <mo id="S2.SS3.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS3.p1.1.m1.1.1.3.3.1.cmml">
           ×
          </mo>
          <mi id="S2.SS3.p1.1.m1.1.1.3.3.3" xref="S2.SS3.p1.1.m1.1.1.3.3.3.cmml">
           W
          </mi>
          <mo id="S2.SS3.p1.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S2.SS3.p1.1.m1.1.1.3.3.1.cmml">
           ×
          </mo>
          <mn id="S2.SS3.p1.1.m1.1.1.3.3.4" xref="S2.SS3.p1.1.m1.1.1.3.3.4.cmml">
           3
          </mn>
         </mrow>
        </msup>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b">
        <apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">
         <in id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1">
         </in>
         <apply id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2">
          <csymbol cd="ambiguous" id="S2.SS3.p1.1.m1.1.1.2.1.cmml" xref="S2.SS3.p1.1.m1.1.1.2">
           subscript
          </csymbol>
          <ci id="S2.SS3.p1.1.m1.1.1.2.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2.2">
           𝐨
          </ci>
          <ci id="S2.SS3.p1.1.m1.1.1.2.3.cmml" xref="S2.SS3.p1.1.m1.1.1.2.3">
           𝑡
          </ci>
         </apply>
         <apply id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3">
          <csymbol cd="ambiguous" id="S2.SS3.p1.1.m1.1.1.3.1.cmml" xref="S2.SS3.p1.1.m1.1.1.3">
           superscript
          </csymbol>
          <ci id="S2.SS3.p1.1.m1.1.1.3.2.cmml" xref="S2.SS3.p1.1.m1.1.1.3.2">
           ℝ
          </ci>
          <apply id="S2.SS3.p1.1.m1.1.1.3.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3.3">
           <times id="S2.SS3.p1.1.m1.1.1.3.3.1.cmml" xref="S2.SS3.p1.1.m1.1.1.3.3.1">
           </times>
           <ci id="S2.SS3.p1.1.m1.1.1.3.3.2.cmml" xref="S2.SS3.p1.1.m1.1.1.3.3.2">
            𝐻
           </ci>
           <ci id="S2.SS3.p1.1.m1.1.1.3.3.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3.3.3">
            𝑊
           </ci>
           <cn id="S2.SS3.p1.1.m1.1.1.3.3.4.cmml" type="integer" xref="S2.SS3.p1.1.m1.1.1.3.3.4">
            3
           </cn>
          </apply>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">
        \mathbf{o}_{t}\in\mathbb{R}^{H\times W\times 3}
       </annotation>
      </semantics>
     </math>
     as input, sampled at 10Hz. We do not provide any privileged information, so the agent only has access to input information available to human players. The action space consists of 12 buttons and 2 joysticks.
The left joystick controls the movement of the agent, while the right joystick controls the camera angle. We decompose each joystick into an
     <math alttext="x" class="ltx_Math" display="inline" id="S2.SS3.p1.2.m2.1">
      <semantics id="S2.SS3.p1.2.m2.1a">
       <mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">
        x
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b">
        <ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">
         𝑥
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">
        x
       </annotation>
      </semantics>
     </math>
     and a
     <math alttext="y" class="ltx_Math" display="inline" id="S2.SS3.p1.3.m3.1">
      <semantics id="S2.SS3.p1.3.m3.1a">
       <mi id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml">
        y
       </mi>
       <annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b">
        <ci id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1">
         𝑦
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">
        y
       </annotation>
      </semantics>
     </math>
     component which are independently discretised into 11 buckets. This creates a
     <math alttext="12+2+2=16" class="ltx_Math" display="inline" id="S2.SS3.p1.4.m4.1">
      <semantics id="S2.SS3.p1.4.m4.1a">
       <mrow id="S2.SS3.p1.4.m4.1.1" xref="S2.SS3.p1.4.m4.1.1.cmml">
        <mrow id="S2.SS3.p1.4.m4.1.1.2" xref="S2.SS3.p1.4.m4.1.1.2.cmml">
         <mn id="S2.SS3.p1.4.m4.1.1.2.2" xref="S2.SS3.p1.4.m4.1.1.2.2.cmml">
          12
         </mn>
         <mo id="S2.SS3.p1.4.m4.1.1.2.1" xref="S2.SS3.p1.4.m4.1.1.2.1.cmml">
          +
         </mo>
         <mn id="S2.SS3.p1.4.m4.1.1.2.3" xref="S2.SS3.p1.4.m4.1.1.2.3.cmml">
          2
         </mn>
         <mo id="S2.SS3.p1.4.m4.1.1.2.1a" xref="S2.SS3.p1.4.m4.1.1.2.1.cmml">
          +
         </mo>
         <mn id="S2.SS3.p1.4.m4.1.1.2.4" xref="S2.SS3.p1.4.m4.1.1.2.4.cmml">
          2
         </mn>
        </mrow>
        <mo id="S2.SS3.p1.4.m4.1.1.1" xref="S2.SS3.p1.4.m4.1.1.1.cmml">
         =
        </mo>
        <mn id="S2.SS3.p1.4.m4.1.1.3" xref="S2.SS3.p1.4.m4.1.1.3.cmml">
         16
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S2.SS3.p1.4.m4.1b">
        <apply id="S2.SS3.p1.4.m4.1.1.cmml" xref="S2.SS3.p1.4.m4.1.1">
         <eq id="S2.SS3.p1.4.m4.1.1.1.cmml" xref="S2.SS3.p1.4.m4.1.1.1">
         </eq>
         <apply id="S2.SS3.p1.4.m4.1.1.2.cmml" xref="S2.SS3.p1.4.m4.1.1.2">
          <plus id="S2.SS3.p1.4.m4.1.1.2.1.cmml" xref="S2.SS3.p1.4.m4.1.1.2.1">
          </plus>
          <cn id="S2.SS3.p1.4.m4.1.1.2.2.cmml" type="integer" xref="S2.SS3.p1.4.m4.1.1.2.2">
           12
          </cn>
          <cn id="S2.SS3.p1.4.m4.1.1.2.3.cmml" type="integer" xref="S2.SS3.p1.4.m4.1.1.2.3">
           2
          </cn>
          <cn id="S2.SS3.p1.4.m4.1.1.2.4.cmml" type="integer" xref="S2.SS3.p1.4.m4.1.1.2.4">
           2
          </cn>
         </apply>
         <cn id="S2.SS3.p1.4.m4.1.1.3.cmml" type="integer" xref="S2.SS3.p1.4.m4.1.1.3">
          16
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S2.SS3.p1.4.m4.1c">
        12+2+2=16
       </annotation>
      </semantics>
     </math>
     dimensional discrete action space in total, although the joystick dimensions are of most relevance to our objective.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Implementation and Analysis
  </h2>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Training a Base Imitation Learning Model
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     We now follow the general procedure outlined in Figure
     <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     and Appendix
     <a class="ltx_ref" href="#A1" title="Appendix A Discussion of General Procedure for Aligning Agents ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       A
      </span>
     </a>
     and describe our specific implementation of this procedure for training an agent aligned to reliably reach a preferred jumppad.
We begin by performing imitation learning from visual observations to gamepad actions on general gameplay data to provide a behavioral prior with a general understanding of human gameplay.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">
      Dataset:
     </span>
     For general pre-training, a dataset was extracted from recorded human gameplay, as described in Appendix
     <a class="ltx_ref" href="#A2" title="Appendix B Bleeding Edge Game Human Data Collection ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       B
      </span>
     </a>
     . This large unfiltered dataset consists of 71,940 individual player trajectories from 8788 matches recorded between 09-02-2020 and 10-19-2022, which amounts to 9875 hours (1.12 years) of individual gameplay.
For the purposes of this work, we use an agent that was trained for roughly one epoch on this dataset. Considering the success of recent foundation models, we note that given our unified observation and action spaces, it may also be possible to train across games to obtain a base model, as explored in previous work
     <cite class="ltx_cite ltx_citemacro_citep">
      (Reed et al.,
      <a class="ltx_ref" href="#bib.bib45" title="">
       2022
      </a>
      ; SIMA-Team et al.,
      <a class="ltx_ref" href="#bib.bib49" title="">
       2024
      </a>
      )
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p3">
    <p class="ltx_p" id="S3.SS1.p3.4">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.4.1">
      Architecture and Training:
     </span>
     For the policy, we use a GPT-2
     <cite class="ltx_cite ltx_citemacro_citep">
      (Radford et al.,
      <a class="ltx_ref" href="#bib.bib43" title="">
       2019
      </a>
      )
     </cite>
     causal transformer architecture with 103M parameters, similar to that used by VPT
     <cite class="ltx_cite ltx_citemacro_citep">
      (Baker et al.,
      <a class="ltx_ref" href="#bib.bib7" title="">
       2022
      </a>
      )
     </cite>
     . Observations from the human gameplay
     <math alttext="\mathbf{o}_{t}\in\mathbb{R}^{H\times W\times 3}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1">
      <semantics id="S3.SS1.p3.1.m1.1a">
       <mrow id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">
        <msub id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">
         <mi id="S3.SS1.p3.1.m1.1.1.2.2" xref="S3.SS1.p3.1.m1.1.1.2.2.cmml">
          𝐨
         </mi>
         <mi id="S3.SS1.p3.1.m1.1.1.2.3" xref="S3.SS1.p3.1.m1.1.1.2.3.cmml">
          t
         </mi>
        </msub>
        <mo id="S3.SS1.p3.1.m1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.cmml">
         ∈
        </mo>
        <msup id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">
         <mi id="S3.SS1.p3.1.m1.1.1.3.2" xref="S3.SS1.p3.1.m1.1.1.3.2.cmml">
          ℝ
         </mi>
         <mrow id="S3.SS1.p3.1.m1.1.1.3.3" xref="S3.SS1.p3.1.m1.1.1.3.3.cmml">
          <mi id="S3.SS1.p3.1.m1.1.1.3.3.2" xref="S3.SS1.p3.1.m1.1.1.3.3.2.cmml">
           H
          </mi>
          <mo id="S3.SS1.p3.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p3.1.m1.1.1.3.3.1.cmml">
           ×
          </mo>
          <mi id="S3.SS1.p3.1.m1.1.1.3.3.3" xref="S3.SS1.p3.1.m1.1.1.3.3.3.cmml">
           W
          </mi>
          <mo id="S3.SS1.p3.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p3.1.m1.1.1.3.3.1.cmml">
           ×
          </mo>
          <mn id="S3.SS1.p3.1.m1.1.1.3.3.4" xref="S3.SS1.p3.1.m1.1.1.3.3.4.cmml">
           3
          </mn>
         </mrow>
        </msup>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b">
        <apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">
         <in id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1">
         </in>
         <apply id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">
          <csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.2.1.cmml" xref="S3.SS1.p3.1.m1.1.1.2">
           subscript
          </csymbol>
          <ci id="S3.SS1.p3.1.m1.1.1.2.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2.2">
           𝐨
          </ci>
          <ci id="S3.SS1.p3.1.m1.1.1.2.3.cmml" xref="S3.SS1.p3.1.m1.1.1.2.3">
           𝑡
          </ci>
         </apply>
         <apply id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">
          <csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.3.1.cmml" xref="S3.SS1.p3.1.m1.1.1.3">
           superscript
          </csymbol>
          <ci id="S3.SS1.p3.1.m1.1.1.3.2.cmml" xref="S3.SS1.p3.1.m1.1.1.3.2">
           ℝ
          </ci>
          <apply id="S3.SS1.p3.1.m1.1.1.3.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3">
           <times id="S3.SS1.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3.1">
           </times>
           <ci id="S3.SS1.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3.2">
            𝐻
           </ci>
           <ci id="S3.SS1.p3.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3.3">
            𝑊
           </ci>
           <cn id="S3.SS1.p3.1.m1.1.1.3.3.4.cmml" type="integer" xref="S3.SS1.p3.1.m1.1.1.3.3.4">
            3
           </cn>
          </apply>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">
        \mathbf{o}_{t}\in\mathbb{R}^{H\times W\times 3}
       </annotation>
      </semantics>
     </math>
     are taken directly as input to a convolutional encoder to give observation embeddings
     <math alttext="\mathbf{z}_{t}" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1">
      <semantics id="S3.SS1.p3.2.m2.1a">
       <msub id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">
        <mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">
         𝐳
        </mi>
        <mi id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">
         t
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b">
        <apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">
         <csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">
          𝐳
         </ci>
         <ci id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">
          𝑡
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">
        \mathbf{z}_{t}
       </annotation>
      </semantics>
     </math>
     .
The transformer is trained
with a context window of
     <math alttext="H=32" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1">
      <semantics id="S3.SS1.p3.3.m3.1a">
       <mrow id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">
        <mi id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">
         H
        </mi>
        <mo id="S3.SS1.p3.3.m3.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.cmml">
         =
        </mo>
        <mn id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">
         32
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b">
        <apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">
         <eq id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1">
         </eq>
         <ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">
          𝐻
         </ci>
         <cn id="S3.SS1.p3.3.m3.1.1.3.cmml" type="integer" xref="S3.SS1.p3.3.m3.1.1.3">
          32
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">
        H=32
       </annotation>
      </semantics>
     </math>
     timesteps (corresponding to around 3s of gameplay given the 10Hz sampling). The context window is important since the game is partially observable: as the context window is increased, the agent is able to better capture the state of the environment and take more informed actions (i.e. with a more Markovian state) at the cost of computational complexity. The output corresponds to the 16 discrete action dimensions. The transformer and convolutional encoder are both trained end to end with a cross-entropy loss over all output action components to provide a policy
     <math alttext="\pi(a_{t}|o_{t},...o_{t-H})" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m4.1">
      <semantics id="S3.SS1.p3.4.m4.1a">
       <mrow id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">
        <mi id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml">
         π
        </mi>
        <mo id="S3.SS1.p3.4.m4.1.1.2" lspace="0em" rspace="0em" xref="S3.SS1.p3.4.m4.1.1.2.cmml">
         ​
        </mo>
        <mrow id="S3.SS1.p3.4.m4.1.1.1.1" xref="S3.SS1.p3.4.m4.1.1.1.1.1.cmml">
         <mo id="S3.SS1.p3.4.m4.1.1.1.1.2" stretchy="false" xref="S3.SS1.p3.4.m4.1.1.1.1.1.cmml">
          (
         </mo>
         <mrow id="S3.SS1.p3.4.m4.1.1.1.1.1" xref="S3.SS1.p3.4.m4.1.1.1.1.1.cmml">
          <msub id="S3.SS1.p3.4.m4.1.1.1.1.1.4" xref="S3.SS1.p3.4.m4.1.1.1.1.1.4.cmml">
           <mi id="S3.SS1.p3.4.m4.1.1.1.1.1.4.2" xref="S3.SS1.p3.4.m4.1.1.1.1.1.4.2.cmml">
            a
           </mi>
           <mi id="S3.SS1.p3.4.m4.1.1.1.1.1.4.3" xref="S3.SS1.p3.4.m4.1.1.1.1.1.4.3.cmml">
            t
           </mi>
          </msub>
          <mo fence="false" id="S3.SS1.p3.4.m4.1.1.1.1.1.3" xref="S3.SS1.p3.4.m4.1.1.1.1.1.3.cmml">
           |
          </mo>
          <mrow id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.3.cmml">
           <msub id="S3.SS1.p3.4.m4.1.1.1.1.1.1.1.1" xref="S3.SS1.p3.4.m4.1.1.1.1.1.1.1.1.cmml">
            <mi id="S3.SS1.p3.4.m4.1.1.1.1.1.1.1.1.2" xref="S3.SS1.p3.4.m4.1.1.1.1.1.1.1.1.2.cmml">
             o
            </mi>
            <mi id="S3.SS1.p3.4.m4.1.1.1.1.1.1.1.1.3" xref="S3.SS1.p3.4.m4.1.1.1.1.1.1.1.1.3.cmml">
             t
            </mi>
           </msub>
           <mo id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.3" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.3.cmml">
            ,
           </mo>
           <mrow id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.cmml">
            <mi id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.2" mathvariant="normal" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.2.cmml">
             …
            </mi>
            <mo id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.1" lspace="0em" rspace="0em" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.1.cmml">
             ​
            </mo>
            <msub id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.cmml">
             <mi id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.2" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.2.cmml">
              o
             </mi>
             <mrow id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.3" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.3.cmml">
              <mi id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.3.2" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.3.2.cmml">
               t
              </mi>
              <mo id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.3.1" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.3.1.cmml">
               −
              </mo>
              <mi id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.3.3" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.3.3.cmml">
               H
              </mi>
             </mrow>
            </msub>
           </mrow>
          </mrow>
         </mrow>
         <mo id="S3.SS1.p3.4.m4.1.1.1.1.3" stretchy="false" xref="S3.SS1.p3.4.m4.1.1.1.1.1.cmml">
          )
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b">
        <apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">
         <times id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2">
         </times>
         <ci id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3">
          𝜋
         </ci>
         <apply id="S3.SS1.p3.4.m4.1.1.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1">
          <csymbol cd="latexml" id="S3.SS1.p3.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.3">
           conditional
          </csymbol>
          <apply id="S3.SS1.p3.4.m4.1.1.1.1.1.4.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.4">
           <csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.1.1.4.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.4">
            subscript
           </csymbol>
           <ci id="S3.SS1.p3.4.m4.1.1.1.1.1.4.2.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.4.2">
            𝑎
           </ci>
           <ci id="S3.SS1.p3.4.m4.1.1.1.1.1.4.3.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.4.3">
            𝑡
           </ci>
          </apply>
          <list id="S3.SS1.p3.4.m4.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2">
           <apply id="S3.SS1.p3.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.1.1.1">
            <csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.1.1.1">
             subscript
            </csymbol>
            <ci id="S3.SS1.p3.4.m4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.1.1.1.2">
             𝑜
            </ci>
            <ci id="S3.SS1.p3.4.m4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.1.1.1.3">
             𝑡
            </ci>
           </apply>
           <apply id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2">
            <times id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.1">
            </times>
            <ci id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.2.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.2">
             …
            </ci>
            <apply id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3">
             <csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3">
              subscript
             </csymbol>
             <ci id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.2">
              𝑜
             </ci>
             <apply id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.3">
              <minus id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.3.1.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.3.1">
              </minus>
              <ci id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.3.2.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.3.2">
               𝑡
              </ci>
              <ci id="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.3.3.cmml" xref="S3.SS1.p3.4.m4.1.1.1.1.1.2.2.2.3.3.3">
               𝐻
              </ci>
             </apply>
            </apply>
           </apply>
          </list>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">
        \pi(a_{t}|o_{t},...o_{t-H})
       </annotation>
      </semantics>
     </math>
     . Full architecture and training details are provided in Appendix
     <a class="ltx_ref" href="#A3" title="Appendix C Architectures and Training Details ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       C
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p4">
    <p class="ltx_p" id="S3.SS1.p4.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">
      Evaluation:
     </span>
     Once trained, we ran our base pre-trained agent online in the game environment and recorded which jumppad was reached for 1000 episodes. To run the agent online, we initialized the
     <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.2">
      Ninja
     </span>
     character with an empty context buffer at one of the spawn points at random. We ran the agent until it reached one of the jumppads or timed out. The jumppad distribution reached is demonstrated in Figure
     <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3.1 Training a Base Imitation Learning Model ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     below.
    </p>
   </div>
   <figure class="ltx_figure ltx_align_floatright" id="S3.F3">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="129" id="S3.F3.1.g1" src="/html/2406.04208/assets/x4.png" width="221"/>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S3.F3.3.1.1" style="font-size:90%;">
       Figure 3
      </span>
      :
     </span>
     <span class="ltx_text" id="S3.F3.4.2" style="font-size:90%;">
      Distribution of jumppads reached by the base imitation learning agent.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S3.SS1.p5">
    <p class="ltx_p" id="S3.SS1.p5.2">
     We find that our base model reaches a jumppad
     <math alttext="56\%" class="ltx_Math" display="inline" id="S3.SS1.p5.1.m1.1">
      <semantics id="S3.SS1.p5.1.m1.1a">
       <mrow id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">
        <mn id="S3.SS1.p5.1.m1.1.1.2" xref="S3.SS1.p5.1.m1.1.1.2.cmml">
         56
        </mn>
        <mo id="S3.SS1.p5.1.m1.1.1.1" xref="S3.SS1.p5.1.m1.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b">
        <apply id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">
         <csymbol cd="latexml" id="S3.SS1.p5.1.m1.1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1.1">
          percent
         </csymbol>
         <cn id="S3.SS1.p5.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.p5.1.m1.1.1.2">
          56
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">
        56\%
       </annotation>
      </semantics>
     </math>
     of the time, and has a bias towards the middle jumppad, reflecting human behavior. Our base model therefore provides a reasonable behavioral prior that is much better than random exploration of the launch island (which has a
     <math alttext="0\%" class="ltx_Math" display="inline" id="S3.SS1.p5.2.m2.1">
      <semantics id="S3.SS1.p5.2.m2.1a">
       <mrow id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml">
        <mn id="S3.SS1.p5.2.m2.1.1.2" xref="S3.SS1.p5.2.m2.1.1.2.cmml">
         0
        </mn>
        <mo id="S3.SS1.p5.2.m2.1.1.1" xref="S3.SS1.p5.2.m2.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b">
        <apply id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">
         <csymbol cd="latexml" id="S3.SS1.p5.2.m2.1.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1.1">
          percent
         </csymbol>
         <cn id="S3.SS1.p5.2.m2.1.1.2.cmml" type="integer" xref="S3.SS1.p5.2.m2.1.1.2">
          0
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">
        0\%
       </annotation>
      </semantics>
     </math>
     success rate). However, the success rate could be improved, which is likely due to distribution shift between the offline data and the online environment. For example, the base model was trained on data containing all thirteen possible characters, while our online evaluation only uses the default
     <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.2.1">
      Ninja
     </span>
     character. Additionally, we initialize our agents online at the spawn point with an empty context buffer, while in training the agent will have access to context including a spawn animation and prior gameplay. While measures can be taken to avoid distribution shift, offline only learning is inherently challenging
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ostrovski et al.,
      <a class="ltx_ref" href="#bib.bib39" title="">
       2021
      </a>
      )
     </cite>
     and some distribution shift is usually inevitable, as we discuss further in Appendix
     <a class="ltx_ref" href="#A4" title="Appendix D Discussion of Offline to Online Distribution Shift ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       D
      </span>
     </a>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Supervised Fine-Tuning on Task Relevant Dataset
   </h3>
   <figure class="ltx_figure ltx_align_floatright" id="S3.F4">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="129" id="S3.F4.1.g1" src="/html/2406.04208/assets/x5.png" width="221"/>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S3.F4.3.1.1" style="font-size:90%;">
       Figure 4
      </span>
      :
     </span>
     <span class="ltx_text" id="S3.F4.4.2" style="font-size:90%;">
      Distribution of jumppads reached by the fine-tuned imitation learning agent.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     Following the LLM pipeline, we now fine-tune our agent on a smaller curated dataset. Here we fine-tune on a high quality subset of the pre-training data. However, more generally this could consist of fine-tuning on demonstrations of desireable behavior by the game designer.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p2">
    <p class="ltx_p" id="S3.SS2.p2.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">
      Dataset:
     </span>
     We curated 300 successful trajectories (100 per jumppad) for fine-tuning, all of which involved the character Ninja and were filtered to contain only the first part of the trajectory until a jumppad was reached. We fine-tune until convergence as described in Appendix
     <a class="ltx_ref" href="#A3" title="Appendix C Architectures and Training Details ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       C
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p3">
    <p class="ltx_p" id="S3.SS2.p3.3">
     <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.3.1">
      Results:
     </span>
     We evaluate our agent following the same procedure as before. We see in Figure
     <a class="ltx_ref" href="#S3.F4" title="Figure 4 ‣ 3.2 Supervised Fine-Tuning on Task Relevant Dataset ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     that the success rate of reaching a jumppad is now substantially higher (
     <math alttext="89\%" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1">
      <semantics id="S3.SS2.p3.1.m1.1a">
       <mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">
        <mn id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">
         89
        </mn>
        <mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b">
        <apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">
         <csymbol cd="latexml" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1">
          percent
         </csymbol>
         <cn id="S3.SS2.p3.1.m1.1.1.2.cmml" type="integer" xref="S3.SS2.p3.1.m1.1.1.2">
          89
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">
        89\%
       </annotation>
      </semantics>
     </math>
     ), and that the agent more evenly reaches all three jumppads (corresponding to the balanced fine-tuning dataset). At this stage, it is natural to ask whether the general pre-training was beneficial or whether the model could have been trained from scratch on our task relevant dataset directly. We find that the agent without pre-training has a less diverse jumppad distribution, and a higher failure rate (around
     <math alttext="20\%" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1">
      <semantics id="S3.SS2.p3.2.m2.1a">
       <mrow id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">
        <mn id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">
         20
        </mn>
        <mo id="S3.SS2.p3.2.m2.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b">
        <apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">
         <csymbol cd="latexml" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1">
          percent
         </csymbol>
         <cn id="S3.SS2.p3.2.m2.1.1.2.cmml" type="integer" xref="S3.SS2.p3.2.m2.1.1.2">
          20
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">
        20\%
       </annotation>
      </semantics>
     </math>
     of trajectories fail to reach a jumppad compared to only around
     <math alttext="10\%" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1">
      <semantics id="S3.SS2.p3.3.m3.1a">
       <mrow id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">
        <mn id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">
         10
        </mn>
        <mo id="S3.SS2.p3.3.m3.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b">
        <apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">
         <csymbol cd="latexml" id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1">
          percent
         </csymbol>
         <cn id="S3.SS2.p3.3.m3.1.1.2.cmml" type="integer" xref="S3.SS2.p3.3.m3.1.1.2">
          10
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">
        10\%
       </annotation>
      </semantics>
     </math>
     for the pre-trained agent). Interestingly, from inspecting agent behaviors we found that the pre-trained agent appeared to be more robust since it can often better ‘self-correct’ its trajectories if it drifts out-of-distribution of the fine-tuning dataset. Full details of our pre-training ablation and an example of this self-correction behavior are provided in Appendix
     <a class="ltx_ref" href="#A5.SS1" title="E.1 Ablation of Unsupervised Pre-Training ‣ Appendix E Ablation of Unsupervised Pre-Training and Model Scaling ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       E.1
      </span>
     </a>
     . These results corroborate similar findings for LLMs, in which pre-training transformer models has been shown to enable more effective fine-tuning, effectively increasing the size of the fine-tuning data compared to training from scratch
     <cite class="ltx_cite ltx_citemacro_citep">
      (Hernandez et al.,
      <a class="ltx_ref" href="#bib.bib24" title="">
       2021
      </a>
      )
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p4">
    <p class="ltx_p" id="S3.SS2.p4.1">
     We also perform a preliminary investigation of model scaling by training additional models from scratch on this smaller task-specific dataset in Appendix
     <a class="ltx_ref" href="#A5.SS2" title="E.2 Preliminary Model Scaling Analysis ‣ Appendix E Ablation of Unsupervised Pre-Training and Model Scaling ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       E.2
      </span>
     </a>
     . We find that smaller models have a higher failure rate, even trained only on this relatively small fine-tuning dataset. While larger models than we consider may perform even better (particularly when incorporating pre-training), this would increase the inference cost which has a substantial impact on online alignment speed (and the potential for the final agent to behave in real-time), so we proceed with the 103M parameter model described above.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.3
    </span>
    Obtaining Preference Data on Online Rollouts
   </h3>
   <div class="ltx_para" id="S3.SS3.p1">
    <p class="ltx_p" id="S3.SS3.p1.1">
     We now deploy the fine-tuned agent in the environment to collect trajectories for preference labelling. This is analogous to generating multiple LLM responses to a prompt, but in this context the prompt becomes the initial observation and optional context of any previous observations and actions provided.
Similarly to LLMs, there are tradeoffs to be made in the diversity, quality and quantity of preferences when provided by humans (discussed further in Appendix
     <a class="ltx_ref" href="#A9" title="Appendix I Limitations and Further Work ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       I
      </span>
     </a>
     ), but here we avoid such issues by utilizing synthetic preferences to isolate the effect of quantity.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS3.p2">
    <p class="ltx_p" id="S3.SS3.p2.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">
      Online Rollouts:
     </span>
     We initialize the Ninja character with an empty context buffer at a random spawn point, equivalent to the evaluation procedure above. We repeated this procedure to generate a video dataset of 2400 on-policy trajectories, divided into 1000 trajectories for training and 1400 for evaluation. Similarly to LLMs, the temperature of the softmax sampling of the policy for action selection can be increased to generate more diverse behaviors from the agent for easier comparison, but here we found the behavior to be diverse enough with a default temperature parameter of 1.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS3.p3">
    <p class="ltx_p" id="S3.SS3.p3.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS3.p3.1.1">
      Generating Preferences:
     </span>
     We utilize synthetic preferences based on the primary criteria:
     <br class="ltx_break"/>
    </p>
   </div>
   <div class="ltx_para" id="S3.SS3.p4">
    <p class="ltx_p ltx_align_center" id="S3.SS3.p4.2">
     <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.2.2">
      Preferred Jumppad Reached
      <math alttext="&gt;" class="ltx_Math" display="inline" id="S3.SS3.p4.1.1.1.m1.1">
       <semantics id="S3.SS3.p4.1.1.1.m1.1a">
        <mo id="S3.SS3.p4.1.1.1.m1.1.1" xref="S3.SS3.p4.1.1.1.m1.1.1.cmml">
         &gt;
        </mo>
        <annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.1.1.m1.1b">
         <gt id="S3.SS3.p4.1.1.1.m1.1.1.cmml" xref="S3.SS3.p4.1.1.1.m1.1.1">
         </gt>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS3.p4.1.1.1.m1.1c">
         &gt;
        </annotation>
       </semantics>
      </math>
      Other Jumppad Reached
      <math alttext="&gt;" class="ltx_Math" display="inline" id="S3.SS3.p4.2.2.2.m2.1">
       <semantics id="S3.SS3.p4.2.2.2.m2.1a">
        <mo id="S3.SS3.p4.2.2.2.m2.1.1" xref="S3.SS3.p4.2.2.2.m2.1.1.cmml">
         &gt;
        </mo>
        <annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.2.2.m2.1b">
         <gt id="S3.SS3.p4.2.2.2.m2.1.1.cmml" xref="S3.SS3.p4.2.2.2.m2.1.1">
         </gt>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS3.p4.2.2.2.m2.1c">
         &gt;
        </annotation>
       </semantics>
      </math>
      No Jumppad Reached
     </span>
    </p>
   </div>
   <div class="ltx_para" id="S3.SS3.p5">
    <p class="ltx_p" id="S3.SS3.p5.3">
     Within each of these primary categories, we further rank trajectories by their duration, with shorter trajectories being preferred.
By selecting subsets of trajectories in the training dataset, we are able to investigate how reward model performance scales with number of comparisons (which is a proxy for the human labelling time requirement, often the main bottleneck for reinforcement learning from human feedback, or RLHF).
A preference dataset
     <math alttext="P" class="ltx_Math" display="inline" id="S3.SS3.p5.1.m1.1">
      <semantics id="S3.SS3.p5.1.m1.1a">
       <mi id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml">
        P
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b">
        <ci id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">
         𝑃
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">
        P
       </annotation>
      </semantics>
     </math>
     is then created by considering all pairwise comparisons of trajectories
     <math alttext="\tau" class="ltx_Math" display="inline" id="S3.SS3.p5.2.m2.1">
      <semantics id="S3.SS3.p5.2.m2.1a">
       <mi id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml">
        τ
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b">
        <ci id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">
         𝜏
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">
        \tau
       </annotation>
      </semantics>
     </math>
     using the criteria above, i.e.
     <math alttext="(\tau_{A}\succ\tau_{B})\in P\ \forall\ \tau_{A},\tau_{B}\in\tau\text{ if }\tau_{A}\succ\tau_{B}" class="ltx_Math" display="inline" id="S3.SS3.p5.3.m3.2">
      <semantics id="S3.SS3.p5.3.m3.2a">
       <mrow id="S3.SS3.p5.3.m3.2.2.2" xref="S3.SS3.p5.3.m3.2.2.3.cmml">
        <mrow id="S3.SS3.p5.3.m3.1.1.1.1" xref="S3.SS3.p5.3.m3.1.1.1.1.cmml">
         <mrow id="S3.SS3.p5.3.m3.1.1.1.1.1.1" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.cmml">
          <mo id="S3.SS3.p5.3.m3.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.cmml">
           (
          </mo>
          <mrow id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.cmml">
           <msub id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.2" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.2.cmml">
            <mi id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.2.2" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.2.2.cmml">
             τ
            </mi>
            <mi id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.2.3" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.2.3.cmml">
             A
            </mi>
           </msub>
           <mo id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.1" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.1.cmml">
            ≻
           </mo>
           <msub id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.3" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.3.cmml">
            <mi id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.3.2" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.3.2.cmml">
             τ
            </mi>
            <mi id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.3.3" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.3.3.cmml">
             B
            </mi>
           </msub>
          </mrow>
          <mo id="S3.SS3.p5.3.m3.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.cmml">
           )
          </mo>
         </mrow>
         <mo id="S3.SS3.p5.3.m3.1.1.1.1.2" xref="S3.SS3.p5.3.m3.1.1.1.1.2.cmml">
          ∈
         </mo>
         <mrow id="S3.SS3.p5.3.m3.1.1.1.1.3" xref="S3.SS3.p5.3.m3.1.1.1.1.3.cmml">
          <mi id="S3.SS3.p5.3.m3.1.1.1.1.3.2" xref="S3.SS3.p5.3.m3.1.1.1.1.3.2.cmml">
           P
          </mi>
          <mo id="S3.SS3.p5.3.m3.1.1.1.1.3.1" lspace="0.667em" rspace="0em" xref="S3.SS3.p5.3.m3.1.1.1.1.3.1.cmml">
           ​
          </mo>
          <mrow id="S3.SS3.p5.3.m3.1.1.1.1.3.3" xref="S3.SS3.p5.3.m3.1.1.1.1.3.3.cmml">
           <mo id="S3.SS3.p5.3.m3.1.1.1.1.3.3.1" rspace="0.667em" xref="S3.SS3.p5.3.m3.1.1.1.1.3.3.1.cmml">
            ∀
           </mo>
           <msub id="S3.SS3.p5.3.m3.1.1.1.1.3.3.2" xref="S3.SS3.p5.3.m3.1.1.1.1.3.3.2.cmml">
            <mi id="S3.SS3.p5.3.m3.1.1.1.1.3.3.2.2" xref="S3.SS3.p5.3.m3.1.1.1.1.3.3.2.2.cmml">
             τ
            </mi>
            <mi id="S3.SS3.p5.3.m3.1.1.1.1.3.3.2.3" xref="S3.SS3.p5.3.m3.1.1.1.1.3.3.2.3.cmml">
             A
            </mi>
           </msub>
          </mrow>
         </mrow>
        </mrow>
        <mo id="S3.SS3.p5.3.m3.2.2.2.3" xref="S3.SS3.p5.3.m3.2.2.3a.cmml">
         ,
        </mo>
        <mrow id="S3.SS3.p5.3.m3.2.2.2.2" xref="S3.SS3.p5.3.m3.2.2.2.2.cmml">
         <msub id="S3.SS3.p5.3.m3.2.2.2.2.2" xref="S3.SS3.p5.3.m3.2.2.2.2.2.cmml">
          <mi id="S3.SS3.p5.3.m3.2.2.2.2.2.2" xref="S3.SS3.p5.3.m3.2.2.2.2.2.2.cmml">
           τ
          </mi>
          <mi id="S3.SS3.p5.3.m3.2.2.2.2.2.3" xref="S3.SS3.p5.3.m3.2.2.2.2.2.3.cmml">
           B
          </mi>
         </msub>
         <mo id="S3.SS3.p5.3.m3.2.2.2.2.3" xref="S3.SS3.p5.3.m3.2.2.2.2.3.cmml">
          ∈
         </mo>
         <mrow id="S3.SS3.p5.3.m3.2.2.2.2.4" xref="S3.SS3.p5.3.m3.2.2.2.2.4.cmml">
          <mi id="S3.SS3.p5.3.m3.2.2.2.2.4.2" xref="S3.SS3.p5.3.m3.2.2.2.2.4.2.cmml">
           τ
          </mi>
          <mo id="S3.SS3.p5.3.m3.2.2.2.2.4.1" lspace="0em" rspace="0em" xref="S3.SS3.p5.3.m3.2.2.2.2.4.1.cmml">
           ​
          </mo>
          <mtext id="S3.SS3.p5.3.m3.2.2.2.2.4.3" xref="S3.SS3.p5.3.m3.2.2.2.2.4.3a.cmml">
           if
          </mtext>
          <mo id="S3.SS3.p5.3.m3.2.2.2.2.4.1a" lspace="0em" rspace="0em" xref="S3.SS3.p5.3.m3.2.2.2.2.4.1.cmml">
           ​
          </mo>
          <msub id="S3.SS3.p5.3.m3.2.2.2.2.4.4" xref="S3.SS3.p5.3.m3.2.2.2.2.4.4.cmml">
           <mi id="S3.SS3.p5.3.m3.2.2.2.2.4.4.2" xref="S3.SS3.p5.3.m3.2.2.2.2.4.4.2.cmml">
            τ
           </mi>
           <mi id="S3.SS3.p5.3.m3.2.2.2.2.4.4.3" xref="S3.SS3.p5.3.m3.2.2.2.2.4.4.3.cmml">
            A
           </mi>
          </msub>
         </mrow>
         <mo id="S3.SS3.p5.3.m3.2.2.2.2.5" xref="S3.SS3.p5.3.m3.2.2.2.2.5.cmml">
          ≻
         </mo>
         <msub id="S3.SS3.p5.3.m3.2.2.2.2.6" xref="S3.SS3.p5.3.m3.2.2.2.2.6.cmml">
          <mi id="S3.SS3.p5.3.m3.2.2.2.2.6.2" xref="S3.SS3.p5.3.m3.2.2.2.2.6.2.cmml">
           τ
          </mi>
          <mi id="S3.SS3.p5.3.m3.2.2.2.2.6.3" xref="S3.SS3.p5.3.m3.2.2.2.2.6.3.cmml">
           B
          </mi>
         </msub>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS3.p5.3.m3.2b">
        <apply id="S3.SS3.p5.3.m3.2.2.3.cmml" xref="S3.SS3.p5.3.m3.2.2.2">
         <csymbol cd="ambiguous" id="S3.SS3.p5.3.m3.2.2.3a.cmml" xref="S3.SS3.p5.3.m3.2.2.2.3">
          formulae-sequence
         </csymbol>
         <apply id="S3.SS3.p5.3.m3.1.1.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1">
          <in id="S3.SS3.p5.3.m3.1.1.1.1.2.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.2">
          </in>
          <apply id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1">
           <csymbol cd="latexml" id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.1">
            succeeds
           </csymbol>
           <apply id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.2">
            <csymbol cd="ambiguous" id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.2">
             subscript
            </csymbol>
            <ci id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.2.2">
             𝜏
            </ci>
            <ci id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.2.3">
             𝐴
            </ci>
           </apply>
           <apply id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.3">
            <csymbol cd="ambiguous" id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.3">
             subscript
            </csymbol>
            <ci id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.3.2">
             𝜏
            </ci>
            <ci id="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.1.1.1.3.3">
             𝐵
            </ci>
           </apply>
          </apply>
          <apply id="S3.SS3.p5.3.m3.1.1.1.1.3.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.3">
           <times id="S3.SS3.p5.3.m3.1.1.1.1.3.1.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.3.1">
           </times>
           <ci id="S3.SS3.p5.3.m3.1.1.1.1.3.2.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.3.2">
            𝑃
           </ci>
           <apply id="S3.SS3.p5.3.m3.1.1.1.1.3.3.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.3.3">
            <csymbol cd="latexml" id="S3.SS3.p5.3.m3.1.1.1.1.3.3.1.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.3.3.1">
             for-all
            </csymbol>
            <apply id="S3.SS3.p5.3.m3.1.1.1.1.3.3.2.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.3.3.2">
             <csymbol cd="ambiguous" id="S3.SS3.p5.3.m3.1.1.1.1.3.3.2.1.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.3.3.2">
              subscript
             </csymbol>
             <ci id="S3.SS3.p5.3.m3.1.1.1.1.3.3.2.2.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.3.3.2.2">
              𝜏
             </ci>
             <ci id="S3.SS3.p5.3.m3.1.1.1.1.3.3.2.3.cmml" xref="S3.SS3.p5.3.m3.1.1.1.1.3.3.2.3">
              𝐴
             </ci>
            </apply>
           </apply>
          </apply>
         </apply>
         <apply id="S3.SS3.p5.3.m3.2.2.2.2.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2">
          <and id="S3.SS3.p5.3.m3.2.2.2.2a.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2">
          </and>
          <apply id="S3.SS3.p5.3.m3.2.2.2.2b.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2">
           <in id="S3.SS3.p5.3.m3.2.2.2.2.3.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.3">
           </in>
           <apply id="S3.SS3.p5.3.m3.2.2.2.2.2.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.2">
            <csymbol cd="ambiguous" id="S3.SS3.p5.3.m3.2.2.2.2.2.1.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.2">
             subscript
            </csymbol>
            <ci id="S3.SS3.p5.3.m3.2.2.2.2.2.2.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.2.2">
             𝜏
            </ci>
            <ci id="S3.SS3.p5.3.m3.2.2.2.2.2.3.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.2.3">
             𝐵
            </ci>
           </apply>
           <apply id="S3.SS3.p5.3.m3.2.2.2.2.4.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.4">
            <times id="S3.SS3.p5.3.m3.2.2.2.2.4.1.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.4.1">
            </times>
            <ci id="S3.SS3.p5.3.m3.2.2.2.2.4.2.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.4.2">
             𝜏
            </ci>
            <ci id="S3.SS3.p5.3.m3.2.2.2.2.4.3a.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.4.3">
             <mtext id="S3.SS3.p5.3.m3.2.2.2.2.4.3.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.4.3">
              if
             </mtext>
            </ci>
            <apply id="S3.SS3.p5.3.m3.2.2.2.2.4.4.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.4.4">
             <csymbol cd="ambiguous" id="S3.SS3.p5.3.m3.2.2.2.2.4.4.1.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.4.4">
              subscript
             </csymbol>
             <ci id="S3.SS3.p5.3.m3.2.2.2.2.4.4.2.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.4.4.2">
              𝜏
             </ci>
             <ci id="S3.SS3.p5.3.m3.2.2.2.2.4.4.3.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.4.4.3">
              𝐴
             </ci>
            </apply>
           </apply>
          </apply>
          <apply id="S3.SS3.p5.3.m3.2.2.2.2c.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2">
           <csymbol cd="latexml" id="S3.SS3.p5.3.m3.2.2.2.2.5.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.5">
            succeeds
           </csymbol>
           <share href="#S3.SS3.p5.3.m3.2.2.2.2.4.cmml" id="S3.SS3.p5.3.m3.2.2.2.2d.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2">
           </share>
           <apply id="S3.SS3.p5.3.m3.2.2.2.2.6.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.6">
            <csymbol cd="ambiguous" id="S3.SS3.p5.3.m3.2.2.2.2.6.1.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.6">
             subscript
            </csymbol>
            <ci id="S3.SS3.p5.3.m3.2.2.2.2.6.2.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.6.2">
             𝜏
            </ci>
            <ci id="S3.SS3.p5.3.m3.2.2.2.2.6.3.cmml" xref="S3.SS3.p5.3.m3.2.2.2.2.6.3">
             𝐵
            </ci>
           </apply>
          </apply>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS3.p5.3.m3.2c">
        (\tau_{A}\succ\tau_{B})\in P\ \forall\ \tau_{A},\tau_{B}\in\tau\text{ if }\tau_{A}\succ\tau_{B}
       </annotation>
      </semantics>
     </math>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.4
    </span>
    Training a Reward Model on Preferences
   </h3>
   <div class="ltx_para" id="S3.SS4.p1">
    <p class="ltx_p" id="S3.SS4.p1.1">
     A reward model is then trained on this dataset to predict rewards in accordance with preferences as described below. Previous work on RLHF for agents has generally relied on simple (often linear) reward models
     <cite class="ltx_cite ltx_citemacro_citep">
      (Knox &amp; Stone,
      <a class="ltx_ref" href="#bib.bib30" title="">
       2008
      </a>
      ; Christiano et al.,
      <a class="ltx_ref" href="#bib.bib17" title="">
       2017
      </a>
      )
     </cite>
     . However recent work on LLMs has demonstrated that reward models that utilize the pre-trained or fine-tuned policy model with the action classification head replaced with a scalar regression head generally perform better
     <cite class="ltx_cite ltx_citemacro_citep">
      (Stiennon et al.,
      <a class="ltx_ref" href="#bib.bib51" title="">
       2020
      </a>
      )
     </cite>
     , and also improve with scale
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ouyang et al.,
      <a class="ltx_ref" href="#bib.bib40" title="">
       2022
      </a>
      ; Touvron et al.,
      <a class="ltx_ref" href="#bib.bib52" title="">
       2023
      </a>
      )
     </cite>
     . It is hypothesised that this is because this allows the reward model to benefit from the pre-training, which helps to the reward model to better distinguish behaviors for reward assignment, since the reward model shares the agent’s knowledge of the behavior space
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ziegler et al.,
      <a class="ltx_ref" href="#bib.bib58" title="">
       2020
      </a>
      )
     </cite>
     . We investigate this potential for representation transfer in the context of agents in this section.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS4.p2">
    <p class="ltx_p" id="S3.SS4.p2.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS4.p2.1.1">
      Procedure:
     </span>
     We follow the standard Bradley-Terry
     <cite class="ltx_cite ltx_citemacro_citep">
      (Bradley &amp; Terry,
      <a class="ltx_ref" href="#bib.bib12" title="">
       1952
      </a>
      )
     </cite>
     model procedure to train a reward model
     <math alttext="\hat{r}" class="ltx_Math" display="inline" id="S3.SS4.p2.1.m1.1">
      <semantics id="S3.SS4.p2.1.m1.1a">
       <mover accent="true" id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">
        <mi id="S3.SS4.p2.1.m1.1.1.2" xref="S3.SS4.p2.1.m1.1.1.2.cmml">
         r
        </mi>
        <mo id="S3.SS4.p2.1.m1.1.1.1" xref="S3.SS4.p2.1.m1.1.1.1.cmml">
         ^
        </mo>
       </mover>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b">
        <apply id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">
         <ci id="S3.SS4.p2.1.m1.1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1.1">
          ^
         </ci>
         <ci id="S3.SS4.p2.1.m1.1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.1.2">
          𝑟
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">
        \hat{r}
       </annotation>
      </semantics>
     </math>
     from these pairwise preferences, as introduced by
     <cite class="ltx_cite ltx_citemacro_citet">
      Christiano et al. (
      <a class="ltx_ref" href="#bib.bib17" title="">
       2017
      </a>
      )
     </cite>
     . Specifically, we interpret trajectory rewards as preference rankings analogous to Elo ratings
     <cite class="ltx_cite ltx_citemacro_citep">
      (Elo,
      <a class="ltx_ref" href="#bib.bib20" title="">
       1978
      </a>
      )
     </cite>
     developed for chess, such that the annotator’s probability of preferring a trajectory depends exponentially on the trajectory reward difference.
We can then fit the reward model by minimising the cross-entropy between these probabilities and the preference labels, which gives the loss function:
    </p>
    <table class="ltx_equation ltx_eqn_table" id="S3.E1">
     <tbody>
      <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
       <td class="ltx_eqn_cell ltx_eqn_center_padleft">
       </td>
       <td class="ltx_eqn_cell ltx_align_center">
        <math alttext="\mathcal{L}(\hat{r})=\sum_{(\tau_{w},\tau_{l})\in P}-\log\bigg{(}\sigma\big{(}\hat{r}(\tau_{w})-\hat{r}(\tau_{l})\big{)}\bigg{)}" class="ltx_Math" display="block" id="S3.E1.m1.5">
         <semantics id="S3.E1.m1.5a">
          <mrow id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">
           <mrow id="S3.E1.m1.5.5.3" xref="S3.E1.m1.5.5.3.cmml">
            <mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.5.5.3.2" xref="S3.E1.m1.5.5.3.2.cmml">
             ℒ
            </mi>
            <mo id="S3.E1.m1.5.5.3.1" lspace="0em" rspace="0em" xref="S3.E1.m1.5.5.3.1.cmml">
             ​
            </mo>
            <mrow id="S3.E1.m1.5.5.3.3.2" xref="S3.E1.m1.3.3.cmml">
             <mo id="S3.E1.m1.5.5.3.3.2.1" stretchy="false" xref="S3.E1.m1.3.3.cmml">
              (
             </mo>
             <mover accent="true" id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">
              <mi id="S3.E1.m1.3.3.2" xref="S3.E1.m1.3.3.2.cmml">
               r
              </mi>
              <mo id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.cmml">
               ^
              </mo>
             </mover>
             <mo id="S3.E1.m1.5.5.3.3.2.2" stretchy="false" xref="S3.E1.m1.3.3.cmml">
              )
             </mo>
            </mrow>
           </mrow>
           <mo id="S3.E1.m1.5.5.2" rspace="0.111em" xref="S3.E1.m1.5.5.2.cmml">
            =
           </mo>
           <mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.cmml">
            <munder id="S3.E1.m1.5.5.1.3" xref="S3.E1.m1.5.5.1.3.cmml">
             <mo id="S3.E1.m1.5.5.1.3.2" movablelimits="false" rspace="0em" xref="S3.E1.m1.5.5.1.3.2.cmml">
              ∑
             </mo>
             <mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml">
              <mrow id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.3.cmml">
               <mo id="S3.E1.m1.2.2.2.2.2.3" stretchy="false" xref="S3.E1.m1.2.2.2.2.3.cmml">
                (
               </mo>
               <msub id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">
                <mi id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">
                 τ
                </mi>
                <mi id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">
                 w
                </mi>
               </msub>
               <mo id="S3.E1.m1.2.2.2.2.2.4" xref="S3.E1.m1.2.2.2.2.3.cmml">
                ,
               </mo>
               <msub id="S3.E1.m1.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.cmml">
                <mi id="S3.E1.m1.2.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.cmml">
                 τ
                </mi>
                <mi id="S3.E1.m1.2.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.2.3.cmml">
                 l
                </mi>
               </msub>
               <mo id="S3.E1.m1.2.2.2.2.2.5" stretchy="false" xref="S3.E1.m1.2.2.2.2.3.cmml">
                )
               </mo>
              </mrow>
              <mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">
               ∈
              </mo>
              <mi id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.4.cmml">
               P
              </mi>
             </mrow>
            </munder>
            <mo id="S3.E1.m1.5.5.1.2" lspace="0em" xref="S3.E1.m1.5.5.1.2.cmml">
             −
            </mo>
            <mrow id="S3.E1.m1.5.5.1.1.1" xref="S3.E1.m1.5.5.1.1.2.cmml">
             <mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">
              log
             </mi>
             <mo id="S3.E1.m1.5.5.1.1.1a" xref="S3.E1.m1.5.5.1.1.2.cmml">
              ⁡
             </mo>
             <mrow id="S3.E1.m1.5.5.1.1.1.1" xref="S3.E1.m1.5.5.1.1.2.cmml">
              <mo id="S3.E1.m1.5.5.1.1.1.1.2" maxsize="210%" minsize="210%" xref="S3.E1.m1.5.5.1.1.2.cmml">
               (
              </mo>
              <mrow id="S3.E1.m1.5.5.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.cmml">
               <mi id="S3.E1.m1.5.5.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.3.cmml">
                σ
               </mi>
               <mo id="S3.E1.m1.5.5.1.1.1.1.1.2" lspace="0em" rspace="0em" xref="S3.E1.m1.5.5.1.1.1.1.1.2.cmml">
                ​
               </mo>
               <mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.cmml">
                <mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.2" maxsize="120%" minsize="120%" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.cmml">
                 (
                </mo>
                <mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.cmml">
                 <mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml">
                  <mover accent="true" id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.cmml">
                   <mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.2.cmml">
                    r
                   </mi>
                   <mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.1.cmml">
                    ^
                   </mo>
                  </mover>
                  <mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2" lspace="0em" rspace="0em" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml">
                   ​
                  </mo>
                  <mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.cmml">
                   <mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.cmml">
                    (
                   </mo>
                   <msub id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.cmml">
                    <mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">
                     τ
                    </mi>
                    <mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">
                     w
                    </mi>
                   </msub>
                   <mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.cmml">
                    )
                   </mo>
                  </mrow>
                 </mrow>
                 <mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.3.cmml">
                  −
                 </mo>
                 <mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.cmml">
                  <mover accent="true" id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.3.cmml">
                   <mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.3.2.cmml">
                    r
                   </mi>
                   <mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.3.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.3.1.cmml">
                    ^
                   </mo>
                  </mover>
                  <mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.2" lspace="0em" rspace="0em" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.2.cmml">
                   ​
                  </mo>
                  <mrow id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.cmml">
                   <mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.cmml">
                    (
                   </mo>
                   <msub id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.cmml">
                    <mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.2.cmml">
                     τ
                    </mi>
                    <mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.3.cmml">
                     l
                    </mi>
                   </msub>
                   <mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.cmml">
                    )
                   </mo>
                  </mrow>
                 </mrow>
                </mrow>
                <mo id="S3.E1.m1.5.5.1.1.1.1.1.1.1.3" maxsize="120%" minsize="120%" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.cmml">
                 )
                </mo>
               </mrow>
              </mrow>
              <mo id="S3.E1.m1.5.5.1.1.1.1.3" maxsize="210%" minsize="210%" xref="S3.E1.m1.5.5.1.1.2.cmml">
               )
              </mo>
             </mrow>
            </mrow>
           </mrow>
          </mrow>
          <annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b">
           <apply id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5">
            <eq id="S3.E1.m1.5.5.2.cmml" xref="S3.E1.m1.5.5.2">
            </eq>
            <apply id="S3.E1.m1.5.5.3.cmml" xref="S3.E1.m1.5.5.3">
             <times id="S3.E1.m1.5.5.3.1.cmml" xref="S3.E1.m1.5.5.3.1">
             </times>
             <ci id="S3.E1.m1.5.5.3.2.cmml" xref="S3.E1.m1.5.5.3.2">
              ℒ
             </ci>
             <apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.5.5.3.3.2">
              <ci id="S3.E1.m1.3.3.1.cmml" xref="S3.E1.m1.3.3.1">
               ^
              </ci>
              <ci id="S3.E1.m1.3.3.2.cmml" xref="S3.E1.m1.3.3.2">
               𝑟
              </ci>
             </apply>
            </apply>
            <apply id="S3.E1.m1.5.5.1.cmml" xref="S3.E1.m1.5.5.1">
             <minus id="S3.E1.m1.5.5.1.2.cmml" xref="S3.E1.m1.5.5.1.2">
             </minus>
             <apply id="S3.E1.m1.5.5.1.3.cmml" xref="S3.E1.m1.5.5.1.3">
              <csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.3.1.cmml" xref="S3.E1.m1.5.5.1.3">
               subscript
              </csymbol>
              <sum id="S3.E1.m1.5.5.1.3.2.cmml" xref="S3.E1.m1.5.5.1.3.2">
              </sum>
              <apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2">
               <in id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3">
               </in>
               <interval closure="open" id="S3.E1.m1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2">
                <apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">
                 <csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">
                  subscript
                 </csymbol>
                 <ci id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">
                  𝜏
                 </ci>
                 <ci id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">
                  𝑤
                 </ci>
                </apply>
                <apply id="S3.E1.m1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2">
                 <csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2">
                  subscript
                 </csymbol>
                 <ci id="S3.E1.m1.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2">
                  𝜏
                 </ci>
                 <ci id="S3.E1.m1.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3">
                  𝑙
                 </ci>
                </apply>
               </interval>
               <ci id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4">
                𝑃
               </ci>
              </apply>
             </apply>
             <apply id="S3.E1.m1.5.5.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1">
              <log id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">
              </log>
              <apply id="S3.E1.m1.5.5.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1">
               <times id="S3.E1.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.2">
               </times>
               <ci id="S3.E1.m1.5.5.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.3">
                𝜎
               </ci>
               <apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1">
                <minus id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.3">
                </minus>
                <apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1">
                 <times id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.2">
                 </times>
                 <apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3">
                  <ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.1">
                   ^
                  </ci>
                  <ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.3.2">
                   𝑟
                  </ci>
                 </apply>
                 <apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1">
                  <csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1">
                   subscript
                  </csymbol>
                  <ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.2">
                   𝜏
                  </ci>
                  <ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.3">
                   𝑤
                  </ci>
                 </apply>
                </apply>
                <apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2">
                 <times id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.2">
                 </times>
                 <apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.3">
                  <ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.3.1">
                   ^
                  </ci>
                  <ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.3.2">
                   𝑟
                  </ci>
                 </apply>
                 <apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1">
                  <csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1">
                   subscript
                  </csymbol>
                  <ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.2">
                   𝜏
                  </ci>
                  <ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.3">
                   𝑙
                  </ci>
                 </apply>
                </apply>
               </apply>
              </apply>
             </apply>
            </apply>
           </apply>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.E1.m1.5c">
           \mathcal{L}(\hat{r})=\sum_{(\tau_{w},\tau_{l})\in P}-\log\bigg{(}\sigma\big{(}\hat{r}(\tau_{w})-\hat{r}(\tau_{l})\big{)}\bigg{)}
          </annotation>
         </semantics>
        </math>
       </td>
       <td class="ltx_eqn_cell ltx_eqn_center_padright">
       </td>
       <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1">
        <span class="ltx_tag ltx_tag_equation ltx_align_right">
         (1)
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <p class="ltx_p" id="S3.SS4.p2.7">
     where
     <math alttext="\sigma" class="ltx_Math" display="inline" id="S3.SS4.p2.2.m1.1">
      <semantics id="S3.SS4.p2.2.m1.1a">
       <mi id="S3.SS4.p2.2.m1.1.1" xref="S3.SS4.p2.2.m1.1.1.cmml">
        σ
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m1.1b">
        <ci id="S3.SS4.p2.2.m1.1.1.cmml" xref="S3.SS4.p2.2.m1.1.1">
         𝜎
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p2.2.m1.1c">
        \sigma
       </annotation>
      </semantics>
     </math>
     is the sigmoid function and
     <math alttext="(\tau_{w},\tau_{l})" class="ltx_Math" display="inline" id="S3.SS4.p2.3.m2.2">
      <semantics id="S3.SS4.p2.3.m2.2a">
       <mrow id="S3.SS4.p2.3.m2.2.2.2" xref="S3.SS4.p2.3.m2.2.2.3.cmml">
        <mo id="S3.SS4.p2.3.m2.2.2.2.3" stretchy="false" xref="S3.SS4.p2.3.m2.2.2.3.cmml">
         (
        </mo>
        <msub id="S3.SS4.p2.3.m2.1.1.1.1" xref="S3.SS4.p2.3.m2.1.1.1.1.cmml">
         <mi id="S3.SS4.p2.3.m2.1.1.1.1.2" xref="S3.SS4.p2.3.m2.1.1.1.1.2.cmml">
          τ
         </mi>
         <mi id="S3.SS4.p2.3.m2.1.1.1.1.3" xref="S3.SS4.p2.3.m2.1.1.1.1.3.cmml">
          w
         </mi>
        </msub>
        <mo id="S3.SS4.p2.3.m2.2.2.2.4" xref="S3.SS4.p2.3.m2.2.2.3.cmml">
         ,
        </mo>
        <msub id="S3.SS4.p2.3.m2.2.2.2.2" xref="S3.SS4.p2.3.m2.2.2.2.2.cmml">
         <mi id="S3.SS4.p2.3.m2.2.2.2.2.2" xref="S3.SS4.p2.3.m2.2.2.2.2.2.cmml">
          τ
         </mi>
         <mi id="S3.SS4.p2.3.m2.2.2.2.2.3" xref="S3.SS4.p2.3.m2.2.2.2.2.3.cmml">
          l
         </mi>
        </msub>
        <mo id="S3.SS4.p2.3.m2.2.2.2.5" stretchy="false" xref="S3.SS4.p2.3.m2.2.2.3.cmml">
         )
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m2.2b">
        <interval closure="open" id="S3.SS4.p2.3.m2.2.2.3.cmml" xref="S3.SS4.p2.3.m2.2.2.2">
         <apply id="S3.SS4.p2.3.m2.1.1.1.1.cmml" xref="S3.SS4.p2.3.m2.1.1.1.1">
          <csymbol cd="ambiguous" id="S3.SS4.p2.3.m2.1.1.1.1.1.cmml" xref="S3.SS4.p2.3.m2.1.1.1.1">
           subscript
          </csymbol>
          <ci id="S3.SS4.p2.3.m2.1.1.1.1.2.cmml" xref="S3.SS4.p2.3.m2.1.1.1.1.2">
           𝜏
          </ci>
          <ci id="S3.SS4.p2.3.m2.1.1.1.1.3.cmml" xref="S3.SS4.p2.3.m2.1.1.1.1.3">
           𝑤
          </ci>
         </apply>
         <apply id="S3.SS4.p2.3.m2.2.2.2.2.cmml" xref="S3.SS4.p2.3.m2.2.2.2.2">
          <csymbol cd="ambiguous" id="S3.SS4.p2.3.m2.2.2.2.2.1.cmml" xref="S3.SS4.p2.3.m2.2.2.2.2">
           subscript
          </csymbol>
          <ci id="S3.SS4.p2.3.m2.2.2.2.2.2.cmml" xref="S3.SS4.p2.3.m2.2.2.2.2.2">
           𝜏
          </ci>
          <ci id="S3.SS4.p2.3.m2.2.2.2.2.3.cmml" xref="S3.SS4.p2.3.m2.2.2.2.2.3">
           𝑙
          </ci>
         </apply>
        </interval>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p2.3.m2.2c">
        (\tau_{w},\tau_{l})
       </annotation>
      </semantics>
     </math>
     are the trajectories being compared, with
     <math alttext="\tau_{w}" class="ltx_Math" display="inline" id="S3.SS4.p2.4.m3.1">
      <semantics id="S3.SS4.p2.4.m3.1a">
       <msub id="S3.SS4.p2.4.m3.1.1" xref="S3.SS4.p2.4.m3.1.1.cmml">
        <mi id="S3.SS4.p2.4.m3.1.1.2" xref="S3.SS4.p2.4.m3.1.1.2.cmml">
         τ
        </mi>
        <mi id="S3.SS4.p2.4.m3.1.1.3" xref="S3.SS4.p2.4.m3.1.1.3.cmml">
         w
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m3.1b">
        <apply id="S3.SS4.p2.4.m3.1.1.cmml" xref="S3.SS4.p2.4.m3.1.1">
         <csymbol cd="ambiguous" id="S3.SS4.p2.4.m3.1.1.1.cmml" xref="S3.SS4.p2.4.m3.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS4.p2.4.m3.1.1.2.cmml" xref="S3.SS4.p2.4.m3.1.1.2">
          𝜏
         </ci>
         <ci id="S3.SS4.p2.4.m3.1.1.3.cmml" xref="S3.SS4.p2.4.m3.1.1.3">
          𝑤
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p2.4.m3.1c">
        \tau_{w}
       </annotation>
      </semantics>
     </math>
     being the winning (preferred) trajectory and
     <math alttext="\tau_{l}" class="ltx_Math" display="inline" id="S3.SS4.p2.5.m4.1">
      <semantics id="S3.SS4.p2.5.m4.1a">
       <msub id="S3.SS4.p2.5.m4.1.1" xref="S3.SS4.p2.5.m4.1.1.cmml">
        <mi id="S3.SS4.p2.5.m4.1.1.2" xref="S3.SS4.p2.5.m4.1.1.2.cmml">
         τ
        </mi>
        <mi id="S3.SS4.p2.5.m4.1.1.3" xref="S3.SS4.p2.5.m4.1.1.3.cmml">
         l
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p2.5.m4.1b">
        <apply id="S3.SS4.p2.5.m4.1.1.cmml" xref="S3.SS4.p2.5.m4.1.1">
         <csymbol cd="ambiguous" id="S3.SS4.p2.5.m4.1.1.1.cmml" xref="S3.SS4.p2.5.m4.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS4.p2.5.m4.1.1.2.cmml" xref="S3.SS4.p2.5.m4.1.1.2">
          𝜏
         </ci>
         <ci id="S3.SS4.p2.5.m4.1.1.3.cmml" xref="S3.SS4.p2.5.m4.1.1.3">
          𝑙
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p2.5.m4.1c">
        \tau_{l}
       </annotation>
      </semantics>
     </math>
     being the losing trajectory. Since the reward model is only trained on comparisons, the scale of the predicted rewards is arbitrary. As a result, we found the need to apply a small amount of
     <math alttext="L2" class="ltx_Math" display="inline" id="S3.SS4.p2.6.m5.1">
      <semantics id="S3.SS4.p2.6.m5.1a">
       <mrow id="S3.SS4.p2.6.m5.1.1" xref="S3.SS4.p2.6.m5.1.1.cmml">
        <mi id="S3.SS4.p2.6.m5.1.1.2" xref="S3.SS4.p2.6.m5.1.1.2.cmml">
         L
        </mi>
        <mo id="S3.SS4.p2.6.m5.1.1.1" lspace="0em" rspace="0em" xref="S3.SS4.p2.6.m5.1.1.1.cmml">
         ​
        </mo>
        <mn id="S3.SS4.p2.6.m5.1.1.3" xref="S3.SS4.p2.6.m5.1.1.3.cmml">
         2
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p2.6.m5.1b">
        <apply id="S3.SS4.p2.6.m5.1.1.cmml" xref="S3.SS4.p2.6.m5.1.1">
         <times id="S3.SS4.p2.6.m5.1.1.1.cmml" xref="S3.SS4.p2.6.m5.1.1.1">
         </times>
         <ci id="S3.SS4.p2.6.m5.1.1.2.cmml" xref="S3.SS4.p2.6.m5.1.1.2">
          𝐿
         </ci>
         <cn id="S3.SS4.p2.6.m5.1.1.3.cmml" type="integer" xref="S3.SS4.p2.6.m5.1.1.3">
          2
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p2.6.m5.1c">
        L2
       </annotation>
      </semantics>
     </math>
     regularisation to prevent the scale of the rewards becoming overly large for the best and worst trajectories (overfitting). We further empirically normalise the reward model after training using the max and min of predicted rewards over the training trajectories to scale our reward model output to be in the range
     <math alttext="\hat{r}\in[0,1]" class="ltx_Math" display="inline" id="S3.SS4.p2.7.m6.2">
      <semantics id="S3.SS4.p2.7.m6.2a">
       <mrow id="S3.SS4.p2.7.m6.2.3" xref="S3.SS4.p2.7.m6.2.3.cmml">
        <mover accent="true" id="S3.SS4.p2.7.m6.2.3.2" xref="S3.SS4.p2.7.m6.2.3.2.cmml">
         <mi id="S3.SS4.p2.7.m6.2.3.2.2" xref="S3.SS4.p2.7.m6.2.3.2.2.cmml">
          r
         </mi>
         <mo id="S3.SS4.p2.7.m6.2.3.2.1" xref="S3.SS4.p2.7.m6.2.3.2.1.cmml">
          ^
         </mo>
        </mover>
        <mo id="S3.SS4.p2.7.m6.2.3.1" xref="S3.SS4.p2.7.m6.2.3.1.cmml">
         ∈
        </mo>
        <mrow id="S3.SS4.p2.7.m6.2.3.3.2" xref="S3.SS4.p2.7.m6.2.3.3.1.cmml">
         <mo id="S3.SS4.p2.7.m6.2.3.3.2.1" stretchy="false" xref="S3.SS4.p2.7.m6.2.3.3.1.cmml">
          [
         </mo>
         <mn id="S3.SS4.p2.7.m6.1.1" xref="S3.SS4.p2.7.m6.1.1.cmml">
          0
         </mn>
         <mo id="S3.SS4.p2.7.m6.2.3.3.2.2" xref="S3.SS4.p2.7.m6.2.3.3.1.cmml">
          ,
         </mo>
         <mn id="S3.SS4.p2.7.m6.2.2" xref="S3.SS4.p2.7.m6.2.2.cmml">
          1
         </mn>
         <mo id="S3.SS4.p2.7.m6.2.3.3.2.3" stretchy="false" xref="S3.SS4.p2.7.m6.2.3.3.1.cmml">
          ]
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p2.7.m6.2b">
        <apply id="S3.SS4.p2.7.m6.2.3.cmml" xref="S3.SS4.p2.7.m6.2.3">
         <in id="S3.SS4.p2.7.m6.2.3.1.cmml" xref="S3.SS4.p2.7.m6.2.3.1">
         </in>
         <apply id="S3.SS4.p2.7.m6.2.3.2.cmml" xref="S3.SS4.p2.7.m6.2.3.2">
          <ci id="S3.SS4.p2.7.m6.2.3.2.1.cmml" xref="S3.SS4.p2.7.m6.2.3.2.1">
           ^
          </ci>
          <ci id="S3.SS4.p2.7.m6.2.3.2.2.cmml" xref="S3.SS4.p2.7.m6.2.3.2.2">
           𝑟
          </ci>
         </apply>
         <interval closure="closed" id="S3.SS4.p2.7.m6.2.3.3.1.cmml" xref="S3.SS4.p2.7.m6.2.3.3.2">
          <cn id="S3.SS4.p2.7.m6.1.1.cmml" type="integer" xref="S3.SS4.p2.7.m6.1.1">
           0
          </cn>
          <cn id="S3.SS4.p2.7.m6.2.2.cmml" type="integer" xref="S3.SS4.p2.7.m6.2.2">
           1
          </cn>
         </interval>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p2.7.m6.2c">
        \hat{r}\in[0,1]
       </annotation>
      </semantics>
     </math>
     (a crucial but often overlooked detail).
    </p>
   </div>
   <div class="ltx_para" id="S3.SS4.p3">
    <p class="ltx_p" id="S3.SS4.p3.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS4.p3.1.1">
      Architecture:
     </span>
     Following the LLM procedure to use the agent model for the reward model initialization, we take the outputs of the fine-tuned agent’s transformer as embeddings for each timestep, which are fed into an MLP reward model to produce a scalar return for the trajectory which is then input to the loss function in Equation
     <a class="ltx_ref" href="#S3.E1" title="In 3.4 Training a Reward Model on Preferences ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     .
As an ablation, to investigate whether these observation representations used by the agent for imitation learning are also beneficial for reward modelling (i.e. predicting preferences), we consider a reward model with an equivalent architecture but with a randomly initialized linear encoder to provide random projections as embeddings for each timestep. We also investigate how reward model performance scales with number of comparisons, to obtain an estimate of the human time required to provide sufficient feedback for training the reward model.
    </p>
   </div>
   <figure class="ltx_figure ltx_align_floatright" id="S3.F5">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="182" id="S3.F5.1.g1" src="/html/2406.04208/assets/x6.png" width="207"/>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S3.F5.3.1.1" style="font-size:90%;">
       Figure 5
      </span>
      :
     </span>
     <span class="ltx_text" id="S3.F5.4.2" style="font-size:90%;">
      Reward model test performances.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S3.SS4.p4">
    <p class="ltx_p" id="S3.SS4.p4.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS4.p4.1.1">
      Results:
     </span>
     To measure the performance of our reward models, we apply each reward model to our held-out test trajectories. We compute pairwise preferences according to the reward models, and compare to the ground truth pairwise preferences to obtain a test preference accuracy. Reward model performances shown in Figure
     <a class="ltx_ref" href="#S3.F5" title="Figure 5 ‣ 3.4 Training a Reward Model on Preferences ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para" id="S3.SS4.p5">
    <p class="ltx_p" id="S3.SS4.p5.1">
     We find that reward model accuracy generally increases with number of comparisons, although the random projection reward models have high variance. Most strikingly, the reward models utilizing the agent model perform better than the reward models using random projections across the full range of comparison sizes, suggesting that the imitation learning agent’s representations of the observations contains information beneficial to predicting preferences. We further find that when using this encoder it is possible to train a reward model from visual input to achieve over 90% preference accuracy with only
     <math alttext="\sim 100" class="ltx_Math" display="inline" id="S3.SS4.p5.1.m1.1">
      <semantics id="S3.SS4.p5.1.m1.1a">
       <mrow id="S3.SS4.p5.1.m1.1.1" xref="S3.SS4.p5.1.m1.1.1.cmml">
        <mi id="S3.SS4.p5.1.m1.1.1.2" xref="S3.SS4.p5.1.m1.1.1.2.cmml">
        </mi>
        <mo id="S3.SS4.p5.1.m1.1.1.1" xref="S3.SS4.p5.1.m1.1.1.1.cmml">
         ∼
        </mo>
        <mn id="S3.SS4.p5.1.m1.1.1.3" xref="S3.SS4.p5.1.m1.1.1.3.cmml">
         100
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS4.p5.1.m1.1b">
        <apply id="S3.SS4.p5.1.m1.1.1.cmml" xref="S3.SS4.p5.1.m1.1.1">
         <csymbol cd="latexml" id="S3.SS4.p5.1.m1.1.1.1.cmml" xref="S3.SS4.p5.1.m1.1.1.1">
          similar-to
         </csymbol>
         <csymbol cd="latexml" id="S3.SS4.p5.1.m1.1.1.2.cmml" xref="S3.SS4.p5.1.m1.1.1.2">
          absent
         </csymbol>
         <cn id="S3.SS4.p5.1.m1.1.1.3.cmml" type="integer" xref="S3.SS4.p5.1.m1.1.1.3">
          100
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS4.p5.1.m1.1c">
        \sim 100
       </annotation>
      </semantics>
     </math>
     comparisons. While we use synthetic preferences, we note that this would correspond to less than 1 hour of labelling time for our task.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS5">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.5
    </span>
    Aligning the Fine-tuned Model with the Reward Model
   </h3>
   <div class="ltx_para" id="S3.SS5.p1">
    <p class="ltx_p" id="S3.SS5.p1.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS5.p1.1.1">
      Procedure:
     </span>
     Finally, we align our agent with our preferences as captured by our reward models. We run our fine-tuned agent in the environment as described in Section
     <a class="ltx_ref" href="#S2" title="2 Game Environment and Alignment Goal ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     to generate online trajectories. After each trajectory is complete, we apply our reward model to the trajectory to generate a reward corresponding to that trajectory. We use this reward as the return for that trajectory and update our agent policy
     <math alttext="\pi" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1.1">
      <semantics id="S3.SS5.p1.1.m1.1a">
       <mi id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">
        π
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b">
        <ci id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">
         𝜋
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">
        \pi
       </annotation>
      </semantics>
     </math>
     using an undiscounted REINFORCE
     <cite class="ltx_cite ltx_citemacro_citep">
      (Williams,
      <a class="ltx_ref" href="#bib.bib55" title="">
       1992
      </a>
      )
     </cite>
     loss. While much of the RLHF literature uses PPO
     <cite class="ltx_cite ltx_citemacro_citep">
      (Schulman et al.,
      <a class="ltx_ref" href="#bib.bib47" title="">
       2017
      </a>
      )
     </cite>
     , we note that concurrent work on LLMs has found PPO to be unnecessarily complex for RLHF
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ahmadian et al.,
      <a class="ltx_ref" href="#bib.bib2" title="">
       2024
      </a>
      )
     </cite>
     . This is due to the fact that in the standard LLM framework the reward is only provided at the end of the trajectory, meaning that we generally just want to reinforce entire trajectories rather than attempt credit assignment at each timestep using an advantage function. Additionally,
     <cite class="ltx_cite ltx_citemacro_citet">
      Ahmadian et al. (
      <a class="ltx_ref" href="#bib.bib2" title="">
       2024
      </a>
      )
     </cite>
     found that clipping rarely occurs in practice since the initial fine-tuned policy is close to the final aligned policy. This is reflected by the use of REINFORCE in recent state-of-the-art LLMs
     <cite class="ltx_cite ltx_citemacro_citep">
      (Google Deepmind,
      <a class="ltx_ref" href="#bib.bib21" title="">
       2024
      </a>
      )
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para" id="S3.SS5.p2">
    <p class="ltx_p" id="S3.SS5.p2.1">
     For our experiments we ran our agent online for 9600 episodes (corresponding to around 1 day of real time gameplay), using a batch size of 16 to give 600 parameter updates. We fine-tune the last layer only since we want to maintain the general pre-trained representations, but note that approaches such as Low-Rank Adaption (LoRA)
     <cite class="ltx_cite ltx_citemacro_citep">
      (Hu et al.,
      <a class="ltx_ref" href="#bib.bib26" title="">
       2021
      </a>
      )
     </cite>
     used for fine-tuning LLMs could also be used here.
We also investigated the use of an additional KL divergence term to regularise the optimized policy towards the initial fine-tuned policy, as is commonly used in LLM alignment
     <cite class="ltx_cite ltx_citemacro_citep">
      (Touvron et al.,
      <a class="ltx_ref" href="#bib.bib52" title="">
       2023
      </a>
      ; Bai et al.,
      <a class="ltx_ref" href="#bib.bib6" title="">
       2022
      </a>
      )
     </cite>
     , but found it unnecessary. This is likely due to the fact that we fine-tune only the last layer rather than the entire network, which mitigates policy divergence.
    </p>
   </div>
   <div class="ltx_pagination ltx_role_newpage">
   </div>
   <section class="ltx_subsubsection" id="S3.SS5.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.5.1
     </span>
     Aligning Agent Towards Left Jumppad
    </h4>
    <figure class="ltx_figure ltx_align_floatright" id="S3.F6">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="274" id="S3.F6.g1" src="/html/2406.04208/assets/figures/Test_Align_Left_PrefV2_FineTuned_AllRMs.png" width="269"/>
     <figcaption class="ltx_caption">
      <span class="ltx_tag ltx_tag_figure">
       <span class="ltx_text" id="S3.F6.2.1.1" style="font-size:90%;">
        Figure 6
       </span>
       :
      </span>
      <span class="ltx_text" id="S3.F6.3.2" style="font-size:90%;">
       Left jumppad success rate during online alignment. Standard errors shaded, and number of seeds shown in legend. We see that: 1) Higher accuracy reward models (darker) generally lead to better alignment, 2) Preference fine-tuning (blue) on preferred trajectories before online alignment improves performance for all reward models.
      </span>
     </figcaption>
    </figure>
    <div class="ltx_para" id="S3.SS5.SSS1.p1">
     <p class="ltx_p" id="S3.SS5.SSS1.p1.1">
      We begin by focusing on aligning our agent towards the left jumppad. We plot the average success rate of reaching the left jumppad against the number of episodes with reward models that have been trained on 100 up to 500k comparisons in Figure
      <a class="ltx_ref" href="#S3.F6" title="Figure 6 ‣ 3.5.1 Aligning Agent Towards Left Jumppad ‣ 3.5 Aligning the Fine-tuned Model with the Reward Model ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
       <span class="ltx_text ltx_ref_tag">
        6
       </span>
      </a>
      . We see that all of our reward models are sufficient to align our agent to consistently reach the left jumppad, with reward models trained on more data (with higher test performances) generally leading to better alignment.
     </p>
    </div>
    <div class="ltx_para" id="S3.SS5.SSS1.p2">
     <p class="ltx_p" id="S3.SS5.SSS1.p2.1">
      However, we see that the agents take most of the 600 updates to fully align, corresponding over a day of real time training. To improve this efficiency, we consider the addition of a
      <span class="ltx_text ltx_font_italic" id="S3.SS5.SSS1.p2.1.1">
       preference fine-tuning
      </span>
      phase in which we first further fine-tune on the preferred trajectories.
Specifically, we apply the reward model to the training trajectories and take the top 20% of trajectories by reward (corresponding to the preferred trajectories) and perform additional fine-tuning on these trajectories. This is similar to Reinforced Self-Training (ReST)
      <cite class="ltx_cite ltx_citemacro_citep">
       (Gulcehre et al.,
       <a class="ltx_ref" href="#bib.bib22" title="">
        2023
       </a>
       )
      </cite>
      , recently introduced to improve the efficiency of language model alignment. We find that this improves online performance for all reward models, shown in Figure
      <a class="ltx_ref" href="#S3.F6" title="Figure 6 ‣ 3.5.1 Aligning Agent Towards Left Jumppad ‣ 3.5 Aligning the Fine-tuned Model with the Reward Model ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
       <span class="ltx_text ltx_ref_tag">
        6
       </span>
      </a>
      .
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="S3.SS5.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      3.5.2
     </span>
     Aligning Agent Towards Right Jumppad
    </h4>
    <div class="ltx_para" id="S3.SS5.SSS2.p1">
     <p class="ltx_p" id="S3.SS5.SSS2.p1.1">
      We now consider aligning our agent towards the right jumppad.
Since this task is seemingly of identical difficulty, we would expect to be able to align the agent similarly.
However, we found that our agents did not align as quickly towards the right jumppad, and in the absence of preference fine-tuning only reached around a
      <math alttext="\sim 40\%" class="ltx_Math" display="inline" id="S3.SS5.SSS2.p1.1.m1.1">
       <semantics id="S3.SS5.SSS2.p1.1.m1.1a">
        <mrow id="S3.SS5.SSS2.p1.1.m1.1.1" xref="S3.SS5.SSS2.p1.1.m1.1.1.cmml">
         <mi id="S3.SS5.SSS2.p1.1.m1.1.1.2" xref="S3.SS5.SSS2.p1.1.m1.1.1.2.cmml">
         </mi>
         <mo id="S3.SS5.SSS2.p1.1.m1.1.1.1" xref="S3.SS5.SSS2.p1.1.m1.1.1.1.cmml">
          ∼
         </mo>
         <mrow id="S3.SS5.SSS2.p1.1.m1.1.1.3" xref="S3.SS5.SSS2.p1.1.m1.1.1.3.cmml">
          <mn id="S3.SS5.SSS2.p1.1.m1.1.1.3.2" xref="S3.SS5.SSS2.p1.1.m1.1.1.3.2.cmml">
           40
          </mn>
          <mo id="S3.SS5.SSS2.p1.1.m1.1.1.3.1" xref="S3.SS5.SSS2.p1.1.m1.1.1.3.1.cmml">
           %
          </mo>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S3.SS5.SSS2.p1.1.m1.1b">
         <apply id="S3.SS5.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS5.SSS2.p1.1.m1.1.1">
          <csymbol cd="latexml" id="S3.SS5.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS5.SSS2.p1.1.m1.1.1.1">
           similar-to
          </csymbol>
          <csymbol cd="latexml" id="S3.SS5.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS5.SSS2.p1.1.m1.1.1.2">
           absent
          </csymbol>
          <apply id="S3.SS5.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS5.SSS2.p1.1.m1.1.1.3">
           <csymbol cd="latexml" id="S3.SS5.SSS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS5.SSS2.p1.1.m1.1.1.3.1">
            percent
           </csymbol>
           <cn id="S3.SS5.SSS2.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S3.SS5.SSS2.p1.1.m1.1.1.3.2">
            40
           </cn>
          </apply>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS5.SSS2.p1.1.m1.1c">
         \sim 40\%
        </annotation>
       </semantics>
      </math>
      success rate after one day of training, as shown in Figure
      <a class="ltx_ref" href="#S3.F8" title="Figure 8 ‣ 3.5.2 Aligning Agent Towards Right Jumppad ‣ 3.5 Aligning the Fine-tuned Model with the Reward Model ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
       <span class="ltx_text ltx_ref_tag">
        8
       </span>
      </a>
      .
     </p>
    </div>
    <figure class="ltx_figure" id="S3.F8">
     <div class="ltx_flex_figure">
      <div class="ltx_flex_cell ltx_flex_size_2">
       <figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S3.F8.1" style="width:199.5pt;">
        <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="547" id="S3.F8.1.g1" src="/html/2406.04208/assets/figures/Test_Align_Right_PrefV2_FineTuned_AllRMs.png" width="538"/>
        <figcaption class="ltx_caption">
         <span class="ltx_tag ltx_tag_figure">
          <span class="ltx_text" id="S3.F8.1.1.1.1" style="font-size:90%;">
           Figure 7
          </span>
          :
         </span>
         <span class="ltx_text" id="S3.F8.1.2.2" style="font-size:90%;">
          Right jumppad success rate during online alignment. Standard errors shaded, and number of seeds in legend. Similar trends hold to Figure
          <a class="ltx_ref" href="#S3.F6" title="Figure 6 ‣ 3.5.1 Aligning Agent Towards Left Jumppad ‣ 3.5 Aligning the Fine-tuned Model with the Reward Model ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
           <span class="ltx_text ltx_ref_tag">
            6
           </span>
          </a>
          , but alignment is less effective.
         </span>
        </figcaption>
       </figure>
      </div>
      <div class="ltx_flex_cell ltx_flex_size_2">
       <figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S3.F8.3" style="width:208.1pt;">
        <div class="ltx_flex_figure">
         <div class="ltx_flex_cell ltx_flex_size_1">
          <img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_portrait" height="428" id="S3.F8.2.g1" src="/html/2406.04208/assets/figures/Train_FineTuned_JP1_trajectories.png" width="269"/>
         </div>
         <div class="ltx_flex_break">
         </div>
         <div class="ltx_flex_cell ltx_flex_size_1">
          <img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_portrait" height="428" id="S3.F8.3.g2" src="/html/2406.04208/assets/figures/Train_FineTuned_JP3_trajectories.png" width="269"/>
         </div>
        </div>
        <figcaption class="ltx_caption">
         <span class="ltx_tag ltx_tag_figure">
          <span class="ltx_text" id="S3.F8.3.1.1.1" style="font-size:90%;">
           Figure 8
          </span>
          :
         </span>
         <span class="ltx_text" id="S3.F8.3.2.2" style="font-size:90%;">
          Fine-tuned agent trajectories which reached the left (red) or right (green) jumppads. While fine-tuning enables more efficient preference labelling, it reduces diversity, potentially limiting alignment.
         </span>
        </figcaption>
       </figure>
      </div>
     </div>
    </figure>
    <div class="ltx_para" id="S3.SS5.SSS2.p2">
     <p class="ltx_p" id="S3.SS5.SSS2.p2.1">
      In order to investigate this discrepancy, we analyzed the rollouts of the fine-tuned agent, shown in Figure
      <a class="ltx_ref" href="#S3.F8" title="Figure 8 ‣ 3.5.2 Aligning Agent Towards Right Jumppad ‣ 3.5 Aligning the Fine-tuned Model with the Reward Model ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
       <span class="ltx_text ltx_ref_tag">
        8
       </span>
      </a>
      .
We see that the trajectories that reach the left jumppad start relatively evenly from the 4 spawn locations.
However, of the trajectories that successfully reach the right jumppad, only 1% originated from the spawns on the right hand side. Therefore it is unsurprising that it is more difficult for the agent to learn to go right, given that in half the episodes (from the right spawn locations) the agent rarely navigates to the right jumppad to receive reward to reinforce its behavior.
To confirm this hypothesis, we instead attempted to align the base agent (before fine-tuning), and found that it can indeed be aligned more symmetrically, although with a lower final success rate, as discussed in Appendix
      <a class="ltx_ref" href="#A7.F17" title="Figure 17 ‣ Appendix G Alignment of Base Model for Comparison with Fine-Tuned Model ‣ Aligning Agents like Large Language Models">
       <span class="ltx_text ltx_ref_tag">
        17
       </span>
      </a>
      . The addition of preference fine-tuning before online alignment helps to mitigate this imbalance, as shown in Figure
      <a class="ltx_ref" href="#S3.F8" title="Figure 8 ‣ 3.5.2 Aligning Agent Towards Right Jumppad ‣ 3.5 Aligning the Fine-tuned Model with the Reward Model ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
       <span class="ltx_text ltx_ref_tag">
        8
       </span>
      </a>
      , since it modifies the agent to produce more of the preferred behaviors that achieve reward before online alignment. This enabled us to completely align the agent right with the same training budget, as shown by the final jumppad distributions in Appendix
      <a class="ltx_ref" href="#A8.F19" title="Figure 19 ‣ Appendix H Final Aligned Agent Jumppad Distributions ‣ Aligning Agents like Large Language Models">
       <span class="ltx_text ltx_ref_tag">
        19
       </span>
      </a>
      . This further demonstrates the benefits of our proposed additional stage of preference fine-tuning.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS6">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.6
    </span>
    Summary of Agent Alignment
   </h3>
   <figure class="ltx_figure" id="S3.F9">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="190" id="S3.F9.g1" src="/html/2406.04208/assets/x7.png" width="369"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="S3.F9.2.1.1" style="font-size:90%;">
       Figure 9
      </span>
      :
     </span>
     <span class="ltx_text" id="S3.F9.3.2" style="font-size:90%;">
      Heatmap of agent trajectories at each stage of our pipeline. Each stage shows 1000 rollouts.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="S3.SS6.p1">
    <p class="ltx_p" id="S3.SS6.p1.1">
     To visualise the gradual alignment of our agent, we provide a heatmap of the agent trajectories at each stage of our alignment pipeline above in Figure
     <a class="ltx_ref" href="#S3.F9" title="Figure 9 ‣ 3.6 Summary of Agent Alignment ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       9
      </span>
     </a>
     . We see that the base agent trajectories are relatively diverse, capturing a variety of human behaviors, including those that do not reach a jumppad, or take an indirect route to do so. The fine-tuned agent which has been refined on curated task-specific/demonstration trajectories shows more direct trajectories to the jumppads, but still does not incorporate any preference regarding the the jumppad which we would like our agent to reach. Finally, preference fine-tuning and subsequent online reinforcement learning enable us to align our agent to reliably perform the desired behavior; in this case to reach the left or the right jumppad. Videos of the behavior of our agent at each stage of this alignment pipeline are provided on the project webpage at:
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://adamjelley.github.io/aligning-agents-like-llms/" target="_blank" title="">
      https://adamjelley.github.io/aligning-agents-like-llms/
     </a>
     .
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Related Work
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    <span class="ltx_text ltx_font_bold" id="S4.p1.1.1">
     Large Scale Imitation Learning:
    </span>
    Imitation learning and offline reinforcement learning
    <cite class="ltx_cite ltx_citemacro_citep">
     (Levine et al.,
     <a class="ltx_ref" href="#bib.bib33" title="">
      2020
     </a>
     )
    </cite>
    have recently gained popularity with the aim of demonstrating similar scaling laws to LLMs
    <cite class="ltx_cite ltx_citemacro_citep">
     (Kaplan et al.,
     <a class="ltx_ref" href="#bib.bib29" title="">
      2020
     </a>
     ; Hernandez et al.,
     <a class="ltx_ref" href="#bib.bib24" title="">
      2021
     </a>
     )
    </cite>
    to obtain more general agents. Decision Transformer
    <cite class="ltx_cite ltx_citemacro_citep">
     (Chen et al.,
     <a class="ltx_ref" href="#bib.bib15" title="">
      2021a
     </a>
     )
    </cite>
    proposed imitation learning with a transformer policy that can be conditioned on a desired return. Multi-Game Decison Transformers
    <cite class="ltx_cite ltx_citemacro_citep">
     (Lee et al.,
     <a class="ltx_ref" href="#bib.bib32" title="">
      2022
     </a>
     )
    </cite>
    extended this approach to learn multiple game policies with a single model. GATO
    <cite class="ltx_cite ltx_citemacro_citep">
     (Reed et al.,
     <a class="ltx_ref" href="#bib.bib45" title="">
      2022
     </a>
     )
    </cite>
    and RoboCat
    <cite class="ltx_cite ltx_citemacro_citep">
     (Bousmalis et al.,
     <a class="ltx_ref" href="#bib.bib11" title="">
      2023
     </a>
     )
    </cite>
    demonstrated large scale multi-task imitation learning could enable a single model to learn and improve on hundreds of diverse tasks. More recently, SIMA
    <cite class="ltx_cite ltx_citemacro_citep">
     (SIMA-Team et al.,
     <a class="ltx_ref" href="#bib.bib49" title="">
      2024
     </a>
     )
    </cite>
    is a general agent with language input and keyboard output which shows promising generalization to new video games, and further demonstrates the potential for scaling imitation learning to large, diverse datasets.
   </p>
  </div>
  <div class="ltx_para" id="S4.p2">
   <p class="ltx_p" id="S4.p2.1">
    <span class="ltx_text ltx_font_bold" id="S4.p2.1.1">
     Fine-tuning with Reinforcement Learning:
    </span>
    The use of imitation learning as pre-training for reinforcement learning was first investigated to improve the sample efficiency of deep RL algorithms by reducing the exploration space
    <cite class="ltx_cite ltx_citemacro_citep">
     (Hester et al.,
     <a class="ltx_ref" href="#bib.bib25" title="">
      2017
     </a>
     ; Vecerik et al.,
     <a class="ltx_ref" href="#bib.bib53" title="">
      2018
     </a>
     )
    </cite>
    . AlphaGo
    <cite class="ltx_cite ltx_citemacro_citep">
     (Silver et al.,
     <a class="ltx_ref" href="#bib.bib48" title="">
      2016
     </a>
     )
    </cite>
    and subsequently AlphaStar
    <cite class="ltx_cite ltx_citemacro_citep">
     (Vinyals et al.,
     <a class="ltx_ref" href="#bib.bib54" title="">
      2019
     </a>
     )
    </cite>
    demonstrated that imitation learning on human data could provide a strong behavior prior for environments like StarCraft where reinforcement learning from scratch is infeasible. VPT
    <cite class="ltx_cite ltx_citemacro_citep">
     (Baker et al.,
     <a class="ltx_ref" href="#bib.bib7" title="">
      2022
     </a>
     )
    </cite>
    extended this paradigm to web-scale data by performing imitation learning on 70k hours of Minecraft videos labelled by an inverse dynamics model, before fine-tuning with reinforcement learning on task-specific rewards. While these works demonstrate the potential of fine-tuning large imitation models, they use hard-coded reward functions to maximise agent performance rather than align an agent’s behavior with subjective preferences.
   </p>
  </div>
  <div class="ltx_para" id="S4.p3">
   <p class="ltx_p" id="S4.p3.1">
    <span class="ltx_text ltx_font_bold" id="S4.p3.1.1">
     Reinforcement Learning from Human Feedback (RLHF):
    </span>
    Training agents with human preferences has a long history, as reviewed by
    <cite class="ltx_cite ltx_citemacro_citet">
     Wirth et al. (
     <a class="ltx_ref" href="#bib.bib56" title="">
      2017
     </a>
     )
    </cite>
    and
    <cite class="ltx_cite ltx_citemacro_citet">
     Zhang et al. (
     <a class="ltx_ref" href="#bib.bib57" title="">
      2021
     </a>
     )
    </cite>
    (including notably
    <cite class="ltx_cite ltx_citemacro_citet">
     Bennett et al. (
     <a class="ltx_ref" href="#bib.bib9" title="">
      2009
     </a>
     )
    </cite>
    and
    <cite class="ltx_cite ltx_citemacro_citet">
     Knox &amp; Stone (
     <a class="ltx_ref" href="#bib.bib30" title="">
      2008
     </a>
     )
    </cite>
    ), leading to the popular modern formulation of RLHF for deep learning proposed by
    <cite class="ltx_cite ltx_citemacro_citet">
     Christiano et al. (
     <a class="ltx_ref" href="#bib.bib17" title="">
      2017
     </a>
     )
    </cite>
    that we use in our work (see Section
    <a class="ltx_ref" href="#S3.SS4" title="3.4 Training a Reward Model on Preferences ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      3.4
     </span>
    </a>
    ). This idea was extended by
    <cite class="ltx_cite ltx_citemacro_citet">
     Ibarz et al. (
     <a class="ltx_ref" href="#bib.bib28" title="">
      2018
     </a>
     )
    </cite>
    to include imitation learning as pre-training to improve the efficiency of preference learning. PEBBLE
    <cite class="ltx_cite ltx_citemacro_citep">
     (Lee et al.,
     <a class="ltx_ref" href="#bib.bib31" title="">
      2021
     </a>
     )
    </cite>
    instead utilises unsupervised pre-training to increase the diversity of initial behaviors.
    <cite class="ltx_cite ltx_citemacro_citet">
     Abramson et al. (
     <a class="ltx_ref" href="#bib.bib1" title="">
      2022
     </a>
     )
    </cite>
    and
    <cite class="ltx_cite ltx_citemacro_citet">
     Milani et al. (
     <a class="ltx_ref" href="#bib.bib36" title="">
      2023
     </a>
     )
    </cite>
    are closest to our work in demonstrating the potential of RLHF for improving the performance of large imitation agents in 3D simulated worlds, but do not analyze the full LLM training pipeline.
   </p>
  </div>
  <div class="ltx_para" id="S4.p4">
   <p class="ltx_p" id="S4.p4.1">
    <span class="ltx_text ltx_font_bold" id="S4.p4.1.1">
     Large Language Models:
    </span>
    <cite class="ltx_cite ltx_citemacro_citet">
     Radford et al. (
     <a class="ltx_ref" href="#bib.bib42" title="">
      2018
     </a>
     )
    </cite>
    demonstrated that pre-training with an unsupervised generative task (next token prediction) on a large diverse corpus of text, followed by fine-tuning on a specific task provided benefits compared fine-tuning alone.
    <cite class="ltx_cite ltx_citemacro_citet">
     Stiennon et al. (
     <a class="ltx_ref" href="#bib.bib51" title="">
      2020
     </a>
     )
    </cite>
    (following
    <cite class="ltx_cite ltx_citemacro_citet">
     Ziegler et al. (
     <a class="ltx_ref" href="#bib.bib58" title="">
      2020
     </a>
     )
    </cite>
    ) then popularised the use of RLHF to further fine-tune these models to align their responses with human preferences. This procedure led to InstructGPT
    <cite class="ltx_cite ltx_citemacro_citep">
     (Ouyang et al.,
     <a class="ltx_ref" href="#bib.bib40" title="">
      2022
     </a>
     )
    </cite>
    , Chat-GPT
    <cite class="ltx_cite ltx_citemacro_citep">
     (OpenAI,
     <a class="ltx_ref" href="#bib.bib37" title="">
      2022
     </a>
     )
    </cite>
    and GPT-4
    <cite class="ltx_cite ltx_citemacro_citep">
     (OpenAI,
     <a class="ltx_ref" href="#bib.bib38" title="">
      2023
     </a>
     )
    </cite>
    as well as open-source models, such as Claude
    <cite class="ltx_cite ltx_citemacro_citep">
     (Bai et al.,
     <a class="ltx_ref" href="#bib.bib6" title="">
      2022
     </a>
     )
    </cite>
    , Llama 2 and 3
    <cite class="ltx_cite ltx_citemacro_citep">
     (Touvron et al.,
     <a class="ltx_ref" href="#bib.bib52" title="">
      2023
     </a>
     )
    </cite>
    and Gemma
    <cite class="ltx_cite ltx_citemacro_citep">
     (Google Deepmind,
     <a class="ltx_ref" href="#bib.bib21" title="">
      2024
     </a>
     )
    </cite>
    . The success of these models suggests that it is worth investigating whether their training procedure can be transferred to other domains. While components of our procedure have been applied both individually and in combinations to agents, our work is the first to investigate the benefits of applying the full LLM training procedure to agents trained end to end from visual inputs to a unified action space.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Limitations and Broader Impact
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    One limitation of our work is that we assume access to a large amount of human data on the game of interest to pre-train a base model. However, large amounts of human data may not be available.
A potential solution may be to use a ‘gaming foundation model’ trained on similar games (with pixel input and action controller output), as investigated by other works
    <cite class="ltx_cite ltx_citemacro_citep">
     (Lee et al.,
     <a class="ltx_ref" href="#bib.bib32" title="">
      2022
     </a>
     ; Reed et al.,
     <a class="ltx_ref" href="#bib.bib45" title="">
      2022
     </a>
     ; SIMA-Team et al.,
     <a class="ltx_ref" href="#bib.bib49" title="">
      2024
     </a>
     )
    </cite>
    .
Another limitation is our use of synthetic preferences. While this may be possible for some games, real human feedback from a game designer will be required in general, and may introduce label noise. A final limitation is that running agents online in modern games to collect preferences and for fine-tuning can be expensive, especially since these games must often be run in real-time in order to be rendered. Preference fine-tuning, as we introduced, provides one way to improve the efficiency of alignment. Other developments in the use of RLHF for language models could also be applied to address these challenges, as discussed in Appendix
    <a class="ltx_ref" href="#A9" title="Appendix I Limitations and Further Work ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      I
     </span>
    </a>
    , and we believe could provide many promising research directions to improve the efficiency of agent alignment.
   </p>
  </div>
  <div class="ltx_para" id="S5.p2">
   <p class="ltx_p" id="S5.p2.1">
    More generally, research into aligning agents with preferences is important to ensure that agents are helpful and harmless. However, this procedure has various known open problems and limitations
    <cite class="ltx_cite ltx_citemacro_citep">
     (Casper et al.,
     <a class="ltx_ref" href="#bib.bib14" title="">
      2023
     </a>
     )
    </cite>
    . Video games therefore provide an important test bed for such research, helping to mitigate risks and provide insights in a safe environment that may generalize to other more real-world applications such as robotics.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    In this paper, we have drawn an analogy between training agents and training large language models. We demonstrated that the current LLM training procedure can be used to align agents to reliably perform desired behaviors in complex environments on a modern console game. This enabled us to train an agent to achieve specific modes of behavior in the game that would be difficult to achieve reliably with imitation learning, reinforcement learning or RLHF alone. Our analysis shows that many of the recent developments in the current procedure for training LLMs, such as general pre-training and initializing reward models from the pre-trained agent, can be applied and have similar benefits for training agents. We also demonstrated improved alignment efficiency through the inclusion of an additional preference fine-tuning stage. We hope that our work encourages further communication and collaboration between the LLM and gaming communities, to enable shared insights and provide a path towards practical and reliable deployment of reinforcement learning agents in modern games.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="Sx1">
  <h2 class="ltx_title ltx_title_section">
   Acknowledgments and Disclosure of Funding
  </h2>
  <div class="ltx_para" id="Sx1.p1">
   <p class="ltx_p" id="Sx1.p1.1">
    This project was done during an internship with the Game Intelligence Team at Microsoft Research Cambridge. We are grateful to Ninja Theory and Microsoft for their support with the project. Thank you to Yuhan Cao and Dave Bignell for implementation and infrastructure support, Sam Devlin for advice and guidance, and particularly Tabish Rashid for supervising the internship and seeing the project through to completion. In addition to the authors, thank you to the whole team, including Anssi Kanervisto, Gunshi Gupta, Katja Hofmann, Lukas Schäfer, Raluca Georgescu, Sergio Valcarcel Macua, Shanzheng Tan, Tarun Gupta, Tim Pearce and Eloi Alonso for a great internship. Thank you also to Amos Storkey, Tom Lee, Trevor McInroe and Rich Turner for insightful discussions. Outside of the internship, Adam Jelley is supported by Microsoft Research and EPSRC through Microsoft’s PhD Scholarship Programme.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Abramson et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Josh Abramson, Arun Ahuja, Federico Carnevale, Petko Georgiev, Alex Goldin, Alden Hung, Jessica Landon, Jirka Lhotka, Timothy Lillicrap, Alistair Muldal, George Powell, Adam Santoro, Guy Scully, Sanjana Srivastava, Tamara von Glehn, Greg Wayne, Nathaniel Wong, Chen Yan, and Rui Zhu.
    </span>
    <span class="ltx_bibblock">
     Improving Multimodal Interactive Agents with Reinforcement Learning from Human Feedback, November 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2211.11602" target="_blank" title="">
      http://arxiv.org/abs/2211.11602
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2211.11602 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ahmadian et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Ahmet Üstün, and Sara Hooker.
    </span>
    <span class="ltx_bibblock">
     Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs, February 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.14740" target="_blank" title="">
      http://arxiv.org/abs/2402.14740
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2402.14740 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Alonso et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Eloi Alonso, Maxim Peter, David Goumard, and Joshua Romoff.
    </span>
    <span class="ltx_bibblock">
     Deep Reinforcement Learning for Navigation in AAA Video Games, November 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2011.04764" target="_blank" title="">
      http://arxiv.org/abs/2011.04764
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2011.04764 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Aytemiz et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Batu Aytemiz, Mikhail Jacob, and Sam Devlin.
    </span>
    <span class="ltx_bibblock">
     Acting with Style: Towards Designer-centred Reinforcement Learning for the Video Games Industry.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      CHI 2021 Workshop on Reinforcement Learning for Humans, Computer, and Interaction (RL4HCI)
     </em>
     . Association for Computing Machinery (ACM), May 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.microsoft.com/en-us/research/publication/acting-with-style-towards-designer-centred-reinforcement-learning-for-the-video-games-industry/" target="_blank" title="">
      https://www.microsoft.com/en-us/research/publication/acting-with-style-towards-designer-centred-reinforcement-learning-for-the-video-games-industry/
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ba et al. (2016)
    </span>
    <span class="ltx_bibblock">
     Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.
    </span>
    <span class="ltx_bibblock">
     Layer Normalization, July 2016.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1607.06450" target="_blank" title="">
      http://arxiv.org/abs/1607.06450
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1607.06450 [cs, stat].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bai et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.
    </span>
    <span class="ltx_bibblock">
     Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, April 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2204.05862" target="_blank" title="">
      http://arxiv.org/abs/2204.05862
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2204.05862 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Baker et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune.
    </span>
    <span class="ltx_bibblock">
     Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos, June 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2206.11795" target="_blank" title="">
      http://arxiv.org/abs/2206.11795
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2206.11795 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Banerjee et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Debarag Banerjee, Pooja Singh, Arjun Avadhanam, and Saksham Srivastava.
    </span>
    <span class="ltx_bibblock">
     Benchmarking LLM powered Chatbots: Methods and Metrics, August 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2308.04624" target="_blank" title="">
      http://arxiv.org/abs/2308.04624
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2308.04624 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bennett et al. (2009)
    </span>
    <span class="ltx_bibblock">
     James Bennett, Stanley Lanning, and Netflix Netflix.
    </span>
    <span class="ltx_bibblock">
     The Netflix Prize.
    </span>
    <span class="ltx_bibblock">
     January 2009.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bergdahl et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Joakim Bergdahl, Camilo Gordillo, Konrad Tollmar, and Linus Gisslén.
    </span>
    <span class="ltx_bibblock">
     Augmenting Automated Game Testing with Deep Reinforcement Learning, March 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2103.15819" target="_blank" title="">
      http://arxiv.org/abs/2103.15819
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2103.15819 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bousmalis et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X. Lee, Maria Bauza, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, Antoine Laurens, Claudio Fantacci, Valentin Dalibard, Martina Zambelli, Murilo Martins, Rugile Pevceviciute, Michiel Blokzijl, Misha Denil, Nathan Batchelor, Thomas Lampe, Emilio Parisotto, Konrad Żołna, Scott Reed, Sergio Gómez Colmenarejo, Jon Scholz, Abbas Abdolmaleki, Oliver Groth, Jean-Baptiste Regli, Oleg Sushkov, Tom Rothörl, José Enrique Chen, Yusuf Aytar, Dave Barker, Joy Ortiz, Martin Riedmiller, Jost Tobias Springenberg, Raia Hadsell, Francesco Nori, and Nicolas Heess.
    </span>
    <span class="ltx_bibblock">
     RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation, December 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2306.11706" target="_blank" title="">
      http://arxiv.org/abs/2306.11706
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2306.11706 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bradley &amp; Terry (1952)
    </span>
    <span class="ltx_bibblock">
     Ralph Allan Bradley and Milton E. Terry.
    </span>
    <span class="ltx_bibblock">
     Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      Biometrika
     </em>
     , 39(3/4):324–345, 1952.
    </span>
    <span class="ltx_bibblock">
     ISSN 0006-3444.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.2307/2334029
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.jstor.org/stable/2334029" target="_blank" title="">
      https://www.jstor.org/stable/2334029
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Publisher: [Oxford University Press, Biometrika Trust].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brohan et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich.
    </span>
    <span class="ltx_bibblock">
     RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, July 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2307.15818" target="_blank" title="">
      http://arxiv.org/abs/2307.15818
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2307.15818 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Casper et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell.
    </span>
    <span class="ltx_bibblock">
     Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback, July 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2307.15217" target="_blank" title="">
      http://arxiv.org/abs/2307.15217
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2307.15217 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2021a)
    </span>
    <span class="ltx_bibblock">
     Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
    </span>
    <span class="ltx_bibblock">
     Decision Transformer: Reinforcement Learning via Sequence Modeling.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      arXiv:2106.01345 [cs]
     </em>
     , June 2021a.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2106.01345" target="_blank" title="">
      http://arxiv.org/abs/2106.01345
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv: 2106.01345.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2021b)
    </span>
    <span class="ltx_bibblock">
     Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
    </span>
    <span class="ltx_bibblock">
     Evaluating Large Language Models Trained on Code, July 2021b.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2107.03374" target="_blank" title="">
      http://arxiv.org/abs/2107.03374
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2107.03374 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Christiano et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.
    </span>
    <span class="ltx_bibblock">
     Deep reinforcement learning from human preferences, July 2017.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1706.03741" target="_blank" title="">
      http://arxiv.org/abs/1706.03741
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1706.03741 [cs, stat].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chung et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.
    </span>
    <span class="ltx_bibblock">
     Scaling Instruction-Finetuned Language Models, December 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2210.11416" target="_blank" title="">
      http://arxiv.org/abs/2210.11416
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2210.11416 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Devlin et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Sam Devlin, Raluca Georgescu, Ida Momennejad, Jaroslaw Rzepecki, Evelyn Zuniga, Gavin Costello, Guy Leroy, Ali Shaw, and Katja Hofmann.
    </span>
    <span class="ltx_bibblock">
     Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation, July 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2105.09637" target="_blank" title="">
      http://arxiv.org/abs/2105.09637
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2105.09637.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Elo (1978)
    </span>
    <span class="ltx_bibblock">
     Arpad E. Elo.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      The Rating of Chessplayers, Past and Present
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     Arco Pub., 1978.
    </span>
    <span class="ltx_bibblock">
     ISBN 978-0-668-04721-0.
    </span>
    <span class="ltx_bibblock">
     Google-Books-ID: 8pMnAQAAMAAJ.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Google Deepmind (2024)
    </span>
    <span class="ltx_bibblock">
     Gemma Team Google Deepmind.
    </span>
    <span class="ltx_bibblock">
     Gemma: Open Models Based on Gemini Research and Technology.
    </span>
    <span class="ltx_bibblock">
     Technical report, February 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf" target="_blank" title="">
      https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gulcehre et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and Nando de Freitas.
    </span>
    <span class="ltx_bibblock">
     Reinforced Self-Training (ReST) for Language Modeling, August 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2308.08998" target="_blank" title="">
      http://arxiv.org/abs/2308.08998
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2308.08998 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hendrycks &amp; Gimpel (2023)
    </span>
    <span class="ltx_bibblock">
     Dan Hendrycks and Kevin Gimpel.
    </span>
    <span class="ltx_bibblock">
     Gaussian Error Linear Units (GELUs), June 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1606.08415" target="_blank" title="">
      http://arxiv.org/abs/1606.08415
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1606.08415 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hernandez et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish.
    </span>
    <span class="ltx_bibblock">
     Scaling Laws for Transfer, February 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2102.01293" target="_blank" title="">
      http://arxiv.org/abs/2102.01293
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2102.01293 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hester et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John Agapiou, Joel Z. Leibo, and Audrunas Gruslys.
    </span>
    <span class="ltx_bibblock">
     Deep Q-learning from Demonstrations, November 2017.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1704.03732" target="_blank" title="">
      http://arxiv.org/abs/1704.03732
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1704.03732 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hu et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
    </span>
    <span class="ltx_bibblock">
     LoRA: Low-Rank Adaptation of Large Language Models, October 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2106.09685" target="_blank" title="">
      http://arxiv.org/abs/2106.09685
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2106.09685 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Jian Hu, Li Tao, June Yang, and Chandler Zhou.
    </span>
    <span class="ltx_bibblock">
     Aligning Language Models with Offline Reinforcement Learning from Human Feedback, August 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2308.12050" target="_blank" title="">
      http://arxiv.org/abs/2308.12050
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2308.12050 [cs] version: 1.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ibarz et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei.
    </span>
    <span class="ltx_bibblock">
     Reward learning from human preferences and demonstrations in Atari.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , volume 31. Curran Associates, Inc., 2018.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2018/hash/8cbe9ce23f42628c98f80fa0fac8b19a-Abstract.html" target="_blank" title="">
      https://proceedings.neurips.cc/paper/2018/hash/8cbe9ce23f42628c98f80fa0fac8b19a-Abstract.html
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kaplan et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    </span>
    <span class="ltx_bibblock">
     Scaling Laws for Neural Language Models, January 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2001.08361" target="_blank" title="">
      http://arxiv.org/abs/2001.08361
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2001.08361 [cs, stat].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Knox &amp; Stone (2008)
    </span>
    <span class="ltx_bibblock">
     Bradley Knox and Peter Stone.
    </span>
    <span class="ltx_bibblock">
     TAMER: Training an Agent Manually via Evaluative Reinforcement.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">
      2008 7th IEEE International Conference on Development and Learning
     </em>
     , pp.  292–297, Monterey, CA, August 2008. IEEE.
    </span>
    <span class="ltx_bibblock">
     ISBN 978-1-4244-2661-4.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.1109/DEVLRN.2008.4640845
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://ieeexplore.ieee.org/document/4640845/" target="_blank" title="">
      http://ieeexplore.ieee.org/document/4640845/
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lee et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Kimin Lee, Laura Smith, and Pieter Abbeel.
    </span>
    <span class="ltx_bibblock">
     PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training, June 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2106.05091" target="_blank" title="">
      http://arxiv.org/abs/2106.05091
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2106.05091 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lee et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie Xu, Sergio Guadarrama, Ian Fischer, Eric Jang, Henryk Michalewski, and Igor Mordatch.
    </span>
    <span class="ltx_bibblock">
     Multi-Game Decision Transformers, October 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2205.15241" target="_blank" title="">
      http://arxiv.org/abs/2205.15241
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2205.15241.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Levine et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
    </span>
    <span class="ltx_bibblock">
     Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems, November 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2005.01643" target="_blank" title="">
      http://arxiv.org/abs/2005.01643
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2005.01643 [cs, stat].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
    </span>
    <span class="ltx_bibblock">
     A ConvNet for the 2020s, March 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2201.03545" target="_blank" title="">
      http://arxiv.org/abs/2201.03545
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2201.03545 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Loshchilov &amp; Hutter (2019)
    </span>
    <span class="ltx_bibblock">
     Ilya Loshchilov and Frank Hutter.
    </span>
    <span class="ltx_bibblock">
     Decoupled Weight Decay Regularization, January 2019.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1711.05101" target="_blank" title="">
      http://arxiv.org/abs/1711.05101
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1711.05101 [cs, math].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Milani et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Stephanie Milani, Anssi Kanervisto, Karolis Ramanauskas, Sander Schulhoff, Brandon Houghton, Sharada Mohanty, Byron Galbraith, Ke Chen, Yan Song, Tianze Zhou, Bingquan Yu, He Liu, Kai Guan, Yujing Hu, Tangjie Lv, Federico Malato, Florian Leopold, Amogh Raut, Ville Hautamäki, Andrew Melnik, Shu Ishida, João F. Henriques, Robert Klassert, Walter Laurito, Ellen Novoseller, Vinicius G. Goecks, Nicholas Waytowich, David Watkins, Josh Miller, and Rohin Shah.
    </span>
    <span class="ltx_bibblock">
     Towards Solving Fuzzy Tasks with Human Feedback: A Retrospective of the MineRL BASALT 2022 Competition, March 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2303.13512" target="_blank" title="">
      http://arxiv.org/abs/2303.13512
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2303.13512 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     OpenAI (2022)
    </span>
    <span class="ltx_bibblock">
     OpenAI.
    </span>
    <span class="ltx_bibblock">
     Introducing ChatGPT, 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt" target="_blank" title="">
      https://openai.com/blog/chatgpt
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     OpenAI (2023)
    </span>
    <span class="ltx_bibblock">
     OpenAI.
    </span>
    <span class="ltx_bibblock">
     GPT-4 Technical Report, March 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2303.08774" target="_blank" title="">
      http://arxiv.org/abs/2303.08774
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2303.08774 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ostrovski et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Georg Ostrovski, Pablo Samuel Castro, and Will Dabney.
    </span>
    <span class="ltx_bibblock">
     The Difficulty of Passive Learning in Deep Reinforcement Learning, October 2021.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2110.14020" target="_blank" title="">
      http://arxiv.org/abs/2110.14020
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2110.14020 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ouyang et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.
    </span>
    <span class="ltx_bibblock">
     Training language models to follow instructions with human feedback, March 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2203.02155" target="_blank" title="">
      http://arxiv.org/abs/2203.02155
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2203.02155 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Pomerleau (1991)
    </span>
    <span class="ltx_bibblock">
     Dean A. Pomerleau.
    </span>
    <span class="ltx_bibblock">
     Efficient Training of Artificial Neural Networks for Autonomous Navigation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">
      Neural Computation
     </em>
     , 3(1):88–97, March 1991.
    </span>
    <span class="ltx_bibblock">
     ISSN 0899-7667.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.1162/neco.1991.3.1.88
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     Conference Name: Neural Computation.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Radford et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
    </span>
    <span class="ltx_bibblock">
     Improving Language Understanding by Generative Pre-Training.
    </span>
    <span class="ltx_bibblock">
     2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Radford et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
    </span>
    <span class="ltx_bibblock">
     Language Models are Unsupervised Multitask Learners.
    </span>
    <span class="ltx_bibblock">
     February 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rafailov et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn.
    </span>
    <span class="ltx_bibblock">
     Direct Preference Optimization: Your Language Model is Secretly a Reward Model, May 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.18290" target="_blank" title="">
      http://arxiv.org/abs/2305.18290
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2305.18290 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Reed et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas.
    </span>
    <span class="ltx_bibblock">
     A Generalist Agent.
    </span>
    <span class="ltx_bibblock">
     Technical Report arXiv:2205.06175, arXiv, May 2022.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2205.06175" target="_blank" title="">
      http://arxiv.org/abs/2205.06175
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2205.06175 [cs] type: article.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ross et al. (2011)
    </span>
    <span class="ltx_bibblock">
     Stephane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell.
    </span>
    <span class="ltx_bibblock">
     A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning, March 2011.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1011.0686" target="_blank" title="">
      http://arxiv.org/abs/1011.0686
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1011.0686 [cs, stat].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schulman et al. (2017)
    </span>
    <span class="ltx_bibblock">
     John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
    </span>
    <span class="ltx_bibblock">
     Proximal Policy Optimization Algorithms.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">
      arXiv:1707.06347 [cs]
     </em>
     , August 2017.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1707.06347" target="_blank" title="">
      http://arxiv.org/abs/1707.06347
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv: 1707.06347.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib48">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Silver et al. (2016)
    </span>
    <span class="ltx_bibblock">
     David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis.
    </span>
    <span class="ltx_bibblock">
     Mastering the game of Go with deep neural networks and tree search.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">
      Nature
     </em>
     , 529(7587):484–489, January 2016.
    </span>
    <span class="ltx_bibblock">
     ISSN 1476-4687.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.1038/nature16961
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/nature16961" target="_blank" title="">
      https://www.nature.com/articles/nature16961
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     Bandiera_abtest: a Cg_type: Nature Research Journals Number: 7587 Primary_atype: Research Publisher: Nature Publishing Group Subject_term: Computational science;Computer science;Reward Subject_term_id: computational-science;computer-science;reward.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib49">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     SIMA-Team et al. (2024)
    </span>
    <span class="ltx_bibblock">
     SIMA-Team, Maria Abi Raad, Arun Ahuja, Catarina Barros, Frederic Besse, Andrew Bolt, Adrian Bolton, Bethanie Brownfield, Gavin Buttimore, Max Cant, Sarah Chakera, Stephanie C. Y. Chan, Jeff Clune, Adrian Collister, Vikki Copeman, Alex Cullum, Ishita Dasgupta, Dario de Cesare, Julia Di Trapani, Yani Donchev, Emma Dunleavy, Martin Engelcke, Ryan Faulkner, Frankie Garcia, Charles Gbadamosi, Zhitao Gong, Lucy Gonzales, Kshitij Gupta, Karol Gregor, Arne Olav Hallingstad, Tim Harley, Sam Haves, Felix Hill, Ed Hirst, Drew A. Hudson, Jony Hudson, Steph Hughes-Fitt, Danilo J. Rezende, Mimi Jasarevic, Laura Kampis, Rosemary Ke, Thomas Keck, Junkyung Kim, Oscar Knagg, Kavya Kopparapu, Andrew Lampinen, Shane Legg, Alexander Lerchner, Marjorie Limont, Yulan Liu, Maria Loks-Thompson, Joseph Marino, Kathryn Martin Cussons, Loic Matthey, Siobhan Mcloughlin, Piermaria Mendolicchio, Hamza Merzic, Anna Mitenkova, Alexandre Moufarek, Valeria Oliveira, Yanko Oliveira, Hannah Openshaw, Renke Pan, Aneesh Pappu, Alex Platonov, Ollie
Purkiss, David Reichert, John Reid, Pierre Harvey Richemond, Tyson Roberts, Giles Ruscoe, Jaume Sanchez Elias, Tasha Sandars, Daniel P. Sawyer, Tim Scholtes, Guy Simmons, Daniel Slater, Hubert Soyer, Heiko Strathmann, Peter Stys, Allison C. Tam, Denis Teplyashin, Tayfun Terzi, Davide Vercelli, Bojan Vujatovic, Marcus Wainwright, Jane X. Wang, Zhengdong Wang, Daan Wierstra, Duncan Williams, Nathaniel Wong, Sarah York, and Nick Young.
    </span>
    <span class="ltx_bibblock">
     Scaling Instructable Agents Across Many Simulated Worlds, April 2024.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2404.10179" target="_blank" title="">
      http://arxiv.org/abs/2404.10179
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2404.10179 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib50">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Spatharioti et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Sofia Eleni Spatharioti, David M. Rothschild, Daniel G. Goldstein, and Jake M. Hofman.
    </span>
    <span class="ltx_bibblock">
     Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment, July 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2307.03744" target="_blank" title="">
      http://arxiv.org/abs/2307.03744
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2307.03744 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib51">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Stiennon et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano.
    </span>
    <span class="ltx_bibblock">
     Learning to summarize with human feedback.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , volume 33, pp.  3008–3021. Curran Associates, Inc., 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html" target="_blank" title="">
      https://proceedings.neurips.cc/paper_files/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib52">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom.
    </span>
    <span class="ltx_bibblock">
     Llama 2: Open Foundation and Fine-Tuned Chat Models, July 2023.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2307.09288" target="_blank" title="">
      http://arxiv.org/abs/2307.09288
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2307.09288 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib53">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vecerik et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Rothörl, Thomas Lampe, and Martin Riedmiller.
    </span>
    <span class="ltx_bibblock">
     Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards, October 2018.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1707.08817" target="_blank" title="">
      http://arxiv.org/abs/1707.08817
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1707.08817 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib54">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vinyals et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S. Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver.
    </span>
    <span class="ltx_bibblock">
     Grandmaster level in StarCraft II using multi-agent reinforcement learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">
      Nature
     </em>
     , 575(7782):350–354, November 2019.
    </span>
    <span class="ltx_bibblock">
     ISSN 0028-0836, 1476-4687.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.1038/s41586-019-1724-z
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.nature.com/articles/s41586-019-1724-z" target="_blank" title="">
      http://www.nature.com/articles/s41586-019-1724-z
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib55">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Williams (1992)
    </span>
    <span class="ltx_bibblock">
     Ronald J. Williams.
    </span>
    <span class="ltx_bibblock">
     Simple statistical gradient-following algorithms for connectionist reinforcement learning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">
      Machine Learning
     </em>
     , 8(3):229–256, May 1992.
    </span>
    <span class="ltx_bibblock">
     ISSN 1573-0565.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.1007/BF00992696
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/BF00992696" target="_blank" title="">
      https://doi.org/10.1007/BF00992696
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib56">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wirth et al. (2017)
    </span>
    <span class="ltx_bibblock">
     Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes Fürnkranz.
    </span>
    <span class="ltx_bibblock">
     A Survey of Preference-Based Reinforcement Learning Methods.
    </span>
    <span class="ltx_bibblock">
     2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib57">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Ruohan Zhang, Faraz Torabi, Garrett Warnell, and Peter Stone.
    </span>
    <span class="ltx_bibblock">
     Recent Advances in Leveraging Human Guidance for Sequential Decision-Making Tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">
      Autonomous Agents and Multi-Agent Systems
     </em>
     , 35(2):31, October 2021.
    </span>
    <span class="ltx_bibblock">
     ISSN 1387-2532, 1573-7454.
    </span>
    <span class="ltx_bibblock">
     doi:
     <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">
      10.1007/s10458-021-09514-w
     </span>
     .
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2107.05825" target="_blank" title="">
      http://arxiv.org/abs/2107.05825
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:2107.05825 [cs].
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib58">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ziegler et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving.
    </span>
    <span class="ltx_bibblock">
     Fine-Tuning Language Models from Human Preferences, January 2020.
    </span>
    <span class="ltx_bibblock">
     URL
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1909.08593" target="_blank" title="">
      http://arxiv.org/abs/1909.08593
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     arXiv:1909.08593 [cs, stat].
    </span>
   </li>
  </ul>
 </section>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   Discussion of General Procedure for Aligning Agents
  </h2>
  <div class="ltx_para" id="A1.p1">
   <p class="ltx_p" id="A1.p1.1">
    We break down our procedure for aligning agents with preferences (see Figure
    <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    ) into five steps:
   </p>
  </div>
  <div class="ltx_para" id="A1.p2">
   <ol class="ltx_enumerate" id="A1.I1">
    <li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      1.
     </span>
     <div class="ltx_para" id="A1.I1.i1.p1">
      <p class="ltx_p" id="A1.I1.i1.p1.1">
       <span class="ltx_text ltx_font_bold" id="A1.I1.i1.p1.1.1">
        Train a Base Imitation Learning Policy
        <br class="ltx_break"/>
       </span>
       The first ingredient for training large language models is to train a large, scalable transformer architecture with self-supervised next-token prediction on a diverse dataset of human text, to obtain a language prior. In the context of agents on modern console games, we interpret this as imitation learning to predict the next action taken in human gameplay data, to obtain a behavioural prior. Specifically this involves training the transformer autoregressively to learn a policy
       <math alttext="p(a_{t}|o_{t},...,o_{t-H})" class="ltx_Math" display="inline" id="A1.I1.i1.p1.1.m1.2">
        <semantics id="A1.I1.i1.p1.1.m1.2a">
         <mrow id="A1.I1.i1.p1.1.m1.2.2" xref="A1.I1.i1.p1.1.m1.2.2.cmml">
          <mi id="A1.I1.i1.p1.1.m1.2.2.3" xref="A1.I1.i1.p1.1.m1.2.2.3.cmml">
           p
          </mi>
          <mo id="A1.I1.i1.p1.1.m1.2.2.2" lspace="0em" rspace="0em" xref="A1.I1.i1.p1.1.m1.2.2.2.cmml">
           ​
          </mo>
          <mrow id="A1.I1.i1.p1.1.m1.2.2.1.1" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.cmml">
           <mo id="A1.I1.i1.p1.1.m1.2.2.1.1.2" stretchy="false" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.cmml">
            (
           </mo>
           <mrow id="A1.I1.i1.p1.1.m1.2.2.1.1.1" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.cmml">
            <msub id="A1.I1.i1.p1.1.m1.2.2.1.1.1.4" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.4.cmml">
             <mi id="A1.I1.i1.p1.1.m1.2.2.1.1.1.4.2" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.4.2.cmml">
              a
             </mi>
             <mi id="A1.I1.i1.p1.1.m1.2.2.1.1.1.4.3" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.4.3.cmml">
              t
             </mi>
            </msub>
            <mo fence="false" id="A1.I1.i1.p1.1.m1.2.2.1.1.1.3" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.3.cmml">
             |
            </mo>
            <mrow id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.3.cmml">
             <msub id="A1.I1.i1.p1.1.m1.2.2.1.1.1.1.1.1" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.1.1.1.cmml">
              <mi id="A1.I1.i1.p1.1.m1.2.2.1.1.1.1.1.1.2" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.1.1.1.2.cmml">
               o
              </mi>
              <mi id="A1.I1.i1.p1.1.m1.2.2.1.1.1.1.1.1.3" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.1.1.1.3.cmml">
               t
              </mi>
             </msub>
             <mo id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.3" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.3.cmml">
              ,
             </mo>
             <mi id="A1.I1.i1.p1.1.m1.1.1" mathvariant="normal" xref="A1.I1.i1.p1.1.m1.1.1.cmml">
              …
             </mi>
             <mo id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.4" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.3.cmml">
              ,
             </mo>
             <msub id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.cmml">
              <mi id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.2" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.2.cmml">
               o
              </mi>
              <mrow id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.3" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.3.cmml">
               <mi id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.3.2" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.3.2.cmml">
                t
               </mi>
               <mo id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.3.1" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.3.1.cmml">
                −
               </mo>
               <mi id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.3.3" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.3.3.cmml">
                H
               </mi>
              </mrow>
             </msub>
            </mrow>
           </mrow>
           <mo id="A1.I1.i1.p1.1.m1.2.2.1.1.3" stretchy="false" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.cmml">
            )
           </mo>
          </mrow>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="A1.I1.i1.p1.1.m1.2b">
          <apply id="A1.I1.i1.p1.1.m1.2.2.cmml" xref="A1.I1.i1.p1.1.m1.2.2">
           <times id="A1.I1.i1.p1.1.m1.2.2.2.cmml" xref="A1.I1.i1.p1.1.m1.2.2.2">
           </times>
           <ci id="A1.I1.i1.p1.1.m1.2.2.3.cmml" xref="A1.I1.i1.p1.1.m1.2.2.3">
            𝑝
           </ci>
           <apply id="A1.I1.i1.p1.1.m1.2.2.1.1.1.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1">
            <csymbol cd="latexml" id="A1.I1.i1.p1.1.m1.2.2.1.1.1.3.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.3">
             conditional
            </csymbol>
            <apply id="A1.I1.i1.p1.1.m1.2.2.1.1.1.4.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.4">
             <csymbol cd="ambiguous" id="A1.I1.i1.p1.1.m1.2.2.1.1.1.4.1.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.4">
              subscript
             </csymbol>
             <ci id="A1.I1.i1.p1.1.m1.2.2.1.1.1.4.2.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.4.2">
              𝑎
             </ci>
             <ci id="A1.I1.i1.p1.1.m1.2.2.1.1.1.4.3.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.4.3">
              𝑡
             </ci>
            </apply>
            <list id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.3.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2">
             <apply id="A1.I1.i1.p1.1.m1.2.2.1.1.1.1.1.1.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.1.1.1">
              <csymbol cd="ambiguous" id="A1.I1.i1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.1.1.1">
               subscript
              </csymbol>
              <ci id="A1.I1.i1.p1.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.1.1.1.2">
               𝑜
              </ci>
              <ci id="A1.I1.i1.p1.1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.1.1.1.3">
               𝑡
              </ci>
             </apply>
             <ci id="A1.I1.i1.p1.1.m1.1.1.cmml" xref="A1.I1.i1.p1.1.m1.1.1">
              …
             </ci>
             <apply id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2">
              <csymbol cd="ambiguous" id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.1.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2">
               subscript
              </csymbol>
              <ci id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.2.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.2">
               𝑜
              </ci>
              <apply id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.3.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.3">
               <minus id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.3.1.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.3.1">
               </minus>
               <ci id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.3.2.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.3.2">
                𝑡
               </ci>
               <ci id="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.3.3.cmml" xref="A1.I1.i1.p1.1.m1.2.2.1.1.1.2.2.2.3.3">
                𝐻
               </ci>
              </apply>
             </apply>
            </list>
           </apply>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="A1.I1.i1.p1.1.m1.2c">
          p(a_{t}|o_{t},...,o_{t-H})
         </annotation>
        </semantics>
       </math>
       . For the purpose of this work, we consider an agent trained on diverse data within a particular game, but note that given our unified observation and action spaces (visual observations and gamepad actions), it would also be possible to train across games, as explored in previous work
       <cite class="ltx_cite ltx_citemacro_citep">
        (Reed et al.,
        <a class="ltx_ref" href="#bib.bib45" title="">
         2022
        </a>
        ; Lee et al.,
        <a class="ltx_ref" href="#bib.bib32" title="">
         2022
        </a>
        )
       </cite>
       .
      </p>
     </div>
    </li>
    <li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      2.
     </span>
     <div class="ltx_para" id="A1.I1.i2.p1">
      <p class="ltx_p" id="A1.I1.i2.p1.1">
       <span class="ltx_text ltx_font_bold" id="A1.I1.i2.p1.1.1">
        Supervised Fine-Tune on a Task Relevant Dataset
        <br class="ltx_break"/>
       </span>
       The next step in the current LLM pipeline is to fine-tune the pre-trained ‘foundation’ model on task relevant data, such as instruction data
       <cite class="ltx_cite ltx_citemacro_citep">
        (Chung et al.,
        <a class="ltx_ref" href="#bib.bib18" title="">
         2022
        </a>
        )
       </cite>
       . Pre-trained transformer models have been shown to fine-tune more effectively, essentially increasing the size of the fine-tuning data compared to training from scratch
       <cite class="ltx_cite ltx_citemacro_citep">
        (Hernandez et al.,
        <a class="ltx_ref" href="#bib.bib24" title="">
         2021
        </a>
        )
       </cite>
       . For decision-making agents this involves fine-tuning by imitation learning on the task (or game) of interest. For specific behaviours, this could also consist of fine-tuning on demonstrations of the desired behaviour by the game designer. The training procedure is equivalent to step 1.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      3.
     </span>
     <div class="ltx_para" id="A1.I1.i3.p1">
      <p class="ltx_p" id="A1.I1.i3.p1.1">
       <span class="ltx_text ltx_font_bold" id="A1.I1.i3.p1.1.1">
        Generate Preference Data on Online Trajectories
        <br class="ltx_break"/>
       </span>
       Fine-tuned LLMs are subsequently prompted to generate multiple responses which are compared by human labellers to provide preferences. In the context of agents, the prompt becomes the initial observation (and optionally the context of previous observations and actions). The agent is then rolled out from a given initial start state multiple times to collect multiple trajectories. Similarly to LLMs, the temperature of the softmax sampling of the policy for action selection can be increased to generate more diverse behaviours from the agent for easier comparison. A human (e.g. game designer) then provides preferences on these trajectories, such as preferring trajectories where the agent plays in a certain style.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      4.
     </span>
     <div class="ltx_para" id="A1.I1.i4.p1">
      <p class="ltx_p" id="A1.I1.i4.p1.1">
       <span class="ltx_text ltx_font_bold" id="A1.I1.i4.p1.1.1">
        Train a Reward Model on Preferences
        <br class="ltx_break"/>
       </span>
       A reward model is then trained on these online trajectories, commonly using a Bradley Terry model
       <cite class="ltx_cite ltx_citemacro_citep">
        (Bradley &amp; Terry,
        <a class="ltx_ref" href="#bib.bib12" title="">
         1952
        </a>
        )
       </cite>
       for pairwise comparisons, so that the reward model provides higher reward for preferred trajectories. While this reward model is usually trained from scratch in the context of agents, the modern LLM procedure utilises the pre-trained or fine-tuned policy model by replacing the action classification head with a scalar regression head. This enables the reward model to also benefit from the pre-training, and share the same knowledge as the agent to reduce out-of-distribution behaviours
       <cite class="ltx_cite ltx_citemacro_citep">
        (Ziegler et al.,
        <a class="ltx_ref" href="#bib.bib58" title="">
         2020
        </a>
        )
       </cite>
       .
      </p>
     </div>
    </li>
    <li class="ltx_item" id="A1.I1.i5" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      5.
     </span>
     <div class="ltx_para" id="A1.I1.i5.p1">
      <p class="ltx_p" id="A1.I1.i5.p1.1">
       <span class="ltx_text ltx_font_bold" id="A1.I1.i5.p1.1.1">
        Align the Fine-tuned Model with the Reward Model
        <br class="ltx_break"/>
       </span>
       Finally the agent can be trained with online reinforcement learning, to maximise the reward provided by the reward model, thereby aligning the agent with the game designer’s preferences. Since online reinforcement learning can be inefficient, we find that additional
       <span class="ltx_text ltx_font_italic" id="A1.I1.i5.p1.1.2">
        preference fine-tuning
       </span>
       can be performed on the high reward trajectories before deploying it online to get the agent closer to the desired behaviour. A common failure mode at this stage is reward model over-optimisation, where the agent performs behaviours that maximise the reward model output but are not aligned with preferences (also known as reward hacking). If this occurs, regularisation towards the original policy can be added or steps 3-5 can be repeated to generate new preferences on the reward hacking behaviour which can be used to re-train the reward model. This may take multiple iterations (e.g. 5 reward model iterations were used for Llama2
       <cite class="ltx_cite ltx_citemacro_citep">
        (Touvron et al.,
        <a class="ltx_ref" href="#bib.bib52" title="">
         2023
        </a>
        )
       </cite>
       ), to eventually obtain an aligned agent.
      </p>
     </div>
    </li>
   </ol>
  </div>
  <div class="ltx_para" id="A1.p3">
   <p class="ltx_p" id="A1.p3.1">
    This procedure combines the benefits of large scale pre-training to obtain a widely generalisable agent, with the benefits of reinforcement learning from preferences to obtain an aligned agent. Furthermore, it also combines the benefits of offline learning to avoid the well-known sample inefficiency of online reinforcement learning, with online fine-tuning to alleviate the well-known problems associated with imitation learning agents going out of distribution
    <cite class="ltx_cite ltx_citemacro_citep">
     (Ross et al.,
     <a class="ltx_ref" href="#bib.bib46" title="">
      2011
     </a>
     )
    </cite>
    by enabling active online learning
    <cite class="ltx_cite ltx_citemacro_citep">
     (Ostrovski et al.,
     <a class="ltx_ref" href="#bib.bib39" title="">
      2021
     </a>
     )
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="A1.p4">
   <p class="ltx_p" id="A1.p4.1">
    Our procedure for aligning agents with preferences can also be summarised in algorithmic form:
   </p>
  </div>
  <figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_float">
     <span class="ltx_text ltx_font_bold" id="alg1.2.1.1">
      Algorithm 1
     </span>
    </span>
    Aligning Agents with Preferred Behaviours
   </figcaption>
   <div class="ltx_listing ltx_listing" id="alg1.3">
    <div class="ltx_listingline" id="alg1.l1">
     <span class="ltx_tag ltx_tag_listingline">
      <span class="ltx_text" id="alg1.l1.1.1.1" style="font-size:80%;">
       1:
      </span>
     </span>
     Large diverse dataset
     <math alttext="D" class="ltx_Math" display="inline" id="alg1.l1.m1.1">
      <semantics id="alg1.l1.m1.1a">
       <mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">
        D
       </mi>
       <annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b">
        <ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">
         𝐷
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="alg1.l1.m1.1c">
        D
       </annotation>
      </semantics>
     </math>
     , task specific dataset
     <math alttext="T" class="ltx_Math" display="inline" id="alg1.l1.m2.1">
      <semantics id="alg1.l1.m2.1a">
       <mi id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml">
        T
       </mi>
       <annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b">
        <ci id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1">
         𝑇
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="alg1.l1.m2.1c">
        T
       </annotation>
      </semantics>
     </math>
     , environment
     <math alttext="E" class="ltx_Math" display="inline" id="alg1.l1.m3.1">
      <semantics id="alg1.l1.m3.1a">
       <mi id="alg1.l1.m3.1.1" xref="alg1.l1.m3.1.1.cmml">
        E
       </mi>
       <annotation-xml encoding="MathML-Content" id="alg1.l1.m3.1b">
        <ci id="alg1.l1.m3.1.1.cmml" xref="alg1.l1.m3.1.1">
         𝐸
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="alg1.l1.m3.1c">
        E
       </annotation>
      </semantics>
     </math>
     , transformer policy
     <math alttext="\pi" class="ltx_Math" display="inline" id="alg1.l1.m4.1">
      <semantics id="alg1.l1.m4.1a">
       <mi id="alg1.l1.m4.1.1" xref="alg1.l1.m4.1.1.cmml">
        π
       </mi>
       <annotation-xml encoding="MathML-Content" id="alg1.l1.m4.1b">
        <ci id="alg1.l1.m4.1.1.cmml" xref="alg1.l1.m4.1.1">
         𝜋
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="alg1.l1.m4.1c">
        \pi
       </annotation>
      </semantics>
     </math>
    </div>
    <div class="ltx_listingline" id="alg1.l2">
     <span class="ltx_tag ltx_tag_listingline">
      <span class="ltx_text" id="alg1.l2.1.1.1" style="font-size:80%;">
       2:
      </span>
     </span>
     Train policy
     <math alttext="\pi" class="ltx_Math" display="inline" id="alg1.l2.m1.1">
      <semantics id="alg1.l2.m1.1a">
       <mi id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml">
        π
       </mi>
       <annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b">
        <ci id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1">
         𝜋
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="alg1.l2.m1.1c">
        \pi
       </annotation>
      </semantics>
     </math>
     on large diverse dataset
     <math alttext="D" class="ltx_Math" display="inline" id="alg1.l2.m2.1">
      <semantics id="alg1.l2.m2.1a">
       <mi id="alg1.l2.m2.1.1" xref="alg1.l2.m2.1.1.cmml">
        D
       </mi>
       <annotation-xml encoding="MathML-Content" id="alg1.l2.m2.1b">
        <ci id="alg1.l2.m2.1.1.cmml" xref="alg1.l2.m2.1.1">
         𝐷
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="alg1.l2.m2.1c">
        D
       </annotation>
      </semantics>
     </math>
     with imitation learning
     <cite class="ltx_cite ltx_citemacro_citep">
      (Pomerleau,
      <a class="ltx_ref" href="#bib.bib41" title="">
       1991
      </a>
      )
     </cite>
     <table class="ltx_equation ltx_eqn_table" id="A1.Ex1">
      <tbody>
       <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
        <td class="ltx_eqn_cell ltx_eqn_center_padleft">
        </td>
        <td class="ltx_eqn_cell ltx_align_center">
         <math alttext="\mathcal{L}_{U}(\pi)=\mathbb{E}_{D}\left[-\log\pi(a^{\tau}_{t}|o^{\tau}_{t},o^{\tau}_{t-1},...)\right]" class="ltx_Math" display="block" id="A1.Ex1.m1.3">
          <semantics id="A1.Ex1.m1.3a">
           <mrow id="A1.Ex1.m1.3.3" xref="A1.Ex1.m1.3.3.cmml">
            <mrow id="A1.Ex1.m1.3.3.3" xref="A1.Ex1.m1.3.3.3.cmml">
             <msub id="A1.Ex1.m1.3.3.3.2" xref="A1.Ex1.m1.3.3.3.2.cmml">
              <mi class="ltx_font_mathcaligraphic" id="A1.Ex1.m1.3.3.3.2.2" xref="A1.Ex1.m1.3.3.3.2.2.cmml">
               ℒ
              </mi>
              <mi id="A1.Ex1.m1.3.3.3.2.3" xref="A1.Ex1.m1.3.3.3.2.3.cmml">
               U
              </mi>
             </msub>
             <mo id="A1.Ex1.m1.3.3.3.1" lspace="0em" rspace="0em" xref="A1.Ex1.m1.3.3.3.1.cmml">
              ​
             </mo>
             <mrow id="A1.Ex1.m1.3.3.3.3.2" xref="A1.Ex1.m1.3.3.3.cmml">
              <mo id="A1.Ex1.m1.3.3.3.3.2.1" stretchy="false" xref="A1.Ex1.m1.3.3.3.cmml">
               (
              </mo>
              <mi id="A1.Ex1.m1.1.1" xref="A1.Ex1.m1.1.1.cmml">
               π
              </mi>
              <mo id="A1.Ex1.m1.3.3.3.3.2.2" stretchy="false" xref="A1.Ex1.m1.3.3.3.cmml">
               )
              </mo>
             </mrow>
            </mrow>
            <mo id="A1.Ex1.m1.3.3.2" xref="A1.Ex1.m1.3.3.2.cmml">
             =
            </mo>
            <mrow id="A1.Ex1.m1.3.3.1" xref="A1.Ex1.m1.3.3.1.cmml">
             <msub id="A1.Ex1.m1.3.3.1.3" xref="A1.Ex1.m1.3.3.1.3.cmml">
              <mi id="A1.Ex1.m1.3.3.1.3.2" xref="A1.Ex1.m1.3.3.1.3.2.cmml">
               𝔼
              </mi>
              <mi id="A1.Ex1.m1.3.3.1.3.3" xref="A1.Ex1.m1.3.3.1.3.3.cmml">
               D
              </mi>
             </msub>
             <mo id="A1.Ex1.m1.3.3.1.2" lspace="0em" rspace="0em" xref="A1.Ex1.m1.3.3.1.2.cmml">
              ​
             </mo>
             <mrow id="A1.Ex1.m1.3.3.1.1.1" xref="A1.Ex1.m1.3.3.1.1.2.cmml">
              <mo id="A1.Ex1.m1.3.3.1.1.1.2" xref="A1.Ex1.m1.3.3.1.1.2.1.cmml">
               [
              </mo>
              <mrow id="A1.Ex1.m1.3.3.1.1.1.1" xref="A1.Ex1.m1.3.3.1.1.1.1.cmml">
               <mo id="A1.Ex1.m1.3.3.1.1.1.1a" rspace="0.167em" xref="A1.Ex1.m1.3.3.1.1.1.1.cmml">
                −
               </mo>
               <mrow id="A1.Ex1.m1.3.3.1.1.1.1.1" xref="A1.Ex1.m1.3.3.1.1.1.1.1.cmml">
                <mrow id="A1.Ex1.m1.3.3.1.1.1.1.1.3" xref="A1.Ex1.m1.3.3.1.1.1.1.1.3.cmml">
                 <mi id="A1.Ex1.m1.3.3.1.1.1.1.1.3.1" xref="A1.Ex1.m1.3.3.1.1.1.1.1.3.1.cmml">
                  log
                 </mi>
                 <mo id="A1.Ex1.m1.3.3.1.1.1.1.1.3a" lspace="0.167em" xref="A1.Ex1.m1.3.3.1.1.1.1.1.3.cmml">
                  ⁡
                 </mo>
                 <mi id="A1.Ex1.m1.3.3.1.1.1.1.1.3.2" xref="A1.Ex1.m1.3.3.1.1.1.1.1.3.2.cmml">
                  π
                 </mi>
                </mrow>
                <mo id="A1.Ex1.m1.3.3.1.1.1.1.1.2" lspace="0em" rspace="0em" xref="A1.Ex1.m1.3.3.1.1.1.1.1.2.cmml">
                 ​
                </mo>
                <mrow id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.cmml">
                 <mo id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.2" stretchy="false" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.cmml">
                  (
                 </mo>
                 <mrow id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.cmml">
                  <msubsup id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.cmml">
                   <mi id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.2.2" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.2.2.cmml">
                    a
                   </mi>
                   <mi id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.3" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.3.cmml">
                    t
                   </mi>
                   <mi id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.2.3" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.2.3.cmml">
                    τ
                   </mi>
                  </msubsup>
                  <mo fence="false" id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.3" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">
                   |
                  </mo>
                  <mrow id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml">
                   <msubsup id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">
                    <mi id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">
                     o
                    </mi>
                    <mi id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml">
                     t
                    </mi>
                    <mi id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">
                     τ
                    </mi>
                   </msubsup>
                   <mo id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.3" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml">
                    ,
                   </mo>
                   <msubsup id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.cmml">
                    <mi id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.2" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml">
                     o
                    </mi>
                    <mrow id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.cmml">
                     <mi id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.2" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml">
                      t
                     </mi>
                     <mo id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.1" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml">
                      −
                     </mo>
                     <mn id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.3" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml">
                      1
                     </mn>
                    </mrow>
                    <mi id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.3" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml">
                     τ
                    </mi>
                   </msubsup>
                   <mo id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.4" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml">
                    ,
                   </mo>
                   <mi id="A1.Ex1.m1.2.2" mathvariant="normal" xref="A1.Ex1.m1.2.2.cmml">
                    …
                   </mi>
                  </mrow>
                 </mrow>
                 <mo id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.3" stretchy="false" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.cmml">
                  )
                 </mo>
                </mrow>
               </mrow>
              </mrow>
              <mo id="A1.Ex1.m1.3.3.1.1.1.3" xref="A1.Ex1.m1.3.3.1.1.2.1.cmml">
               ]
              </mo>
             </mrow>
            </mrow>
           </mrow>
           <annotation-xml encoding="MathML-Content" id="A1.Ex1.m1.3b">
            <apply id="A1.Ex1.m1.3.3.cmml" xref="A1.Ex1.m1.3.3">
             <eq id="A1.Ex1.m1.3.3.2.cmml" xref="A1.Ex1.m1.3.3.2">
             </eq>
             <apply id="A1.Ex1.m1.3.3.3.cmml" xref="A1.Ex1.m1.3.3.3">
              <times id="A1.Ex1.m1.3.3.3.1.cmml" xref="A1.Ex1.m1.3.3.3.1">
              </times>
              <apply id="A1.Ex1.m1.3.3.3.2.cmml" xref="A1.Ex1.m1.3.3.3.2">
               <csymbol cd="ambiguous" id="A1.Ex1.m1.3.3.3.2.1.cmml" xref="A1.Ex1.m1.3.3.3.2">
                subscript
               </csymbol>
               <ci id="A1.Ex1.m1.3.3.3.2.2.cmml" xref="A1.Ex1.m1.3.3.3.2.2">
                ℒ
               </ci>
               <ci id="A1.Ex1.m1.3.3.3.2.3.cmml" xref="A1.Ex1.m1.3.3.3.2.3">
                𝑈
               </ci>
              </apply>
              <ci id="A1.Ex1.m1.1.1.cmml" xref="A1.Ex1.m1.1.1">
               𝜋
              </ci>
             </apply>
             <apply id="A1.Ex1.m1.3.3.1.cmml" xref="A1.Ex1.m1.3.3.1">
              <times id="A1.Ex1.m1.3.3.1.2.cmml" xref="A1.Ex1.m1.3.3.1.2">
              </times>
              <apply id="A1.Ex1.m1.3.3.1.3.cmml" xref="A1.Ex1.m1.3.3.1.3">
               <csymbol cd="ambiguous" id="A1.Ex1.m1.3.3.1.3.1.cmml" xref="A1.Ex1.m1.3.3.1.3">
                subscript
               </csymbol>
               <ci id="A1.Ex1.m1.3.3.1.3.2.cmml" xref="A1.Ex1.m1.3.3.1.3.2">
                𝔼
               </ci>
               <ci id="A1.Ex1.m1.3.3.1.3.3.cmml" xref="A1.Ex1.m1.3.3.1.3.3">
                𝐷
               </ci>
              </apply>
              <apply id="A1.Ex1.m1.3.3.1.1.2.cmml" xref="A1.Ex1.m1.3.3.1.1.1">
               <csymbol cd="latexml" id="A1.Ex1.m1.3.3.1.1.2.1.cmml" xref="A1.Ex1.m1.3.3.1.1.1.2">
                delimited-[]
               </csymbol>
               <apply id="A1.Ex1.m1.3.3.1.1.1.1.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1">
                <minus id="A1.Ex1.m1.3.3.1.1.1.1.2.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1">
                </minus>
                <apply id="A1.Ex1.m1.3.3.1.1.1.1.1.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1">
                 <times id="A1.Ex1.m1.3.3.1.1.1.1.1.2.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.2">
                 </times>
                 <apply id="A1.Ex1.m1.3.3.1.1.1.1.1.3.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.3">
                  <log id="A1.Ex1.m1.3.3.1.1.1.1.1.3.1.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.3.1">
                  </log>
                  <ci id="A1.Ex1.m1.3.3.1.1.1.1.1.3.2.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.3.2">
                   𝜋
                  </ci>
                 </apply>
                 <apply id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1">
                  <csymbol cd="latexml" id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.3">
                   conditional
                  </csymbol>
                  <apply id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4">
                   <csymbol cd="ambiguous" id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.1.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4">
                    subscript
                   </csymbol>
                   <apply id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.2.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4">
                    <csymbol cd="ambiguous" id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.2.1.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4">
                     superscript
                    </csymbol>
                    <ci id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.2.2.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.2.2">
                     𝑎
                    </ci>
                    <ci id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.2.3.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.2.3">
                     𝜏
                    </ci>
                   </apply>
                   <ci id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.3.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.4.3">
                    𝑡
                   </ci>
                  </apply>
                  <list id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2">
                   <apply id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1">
                    <csymbol cd="ambiguous" id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1">
                     subscript
                    </csymbol>
                    <apply id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1">
                     <csymbol cd="ambiguous" id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1">
                      superscript
                     </csymbol>
                     <ci id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2">
                      𝑜
                     </ci>
                     <ci id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3">
                      𝜏
                     </ci>
                    </apply>
                    <ci id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3">
                     𝑡
                    </ci>
                   </apply>
                   <apply id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2">
                    <csymbol cd="ambiguous" id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2">
                     subscript
                    </csymbol>
                    <apply id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2">
                     <csymbol cd="ambiguous" id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2">
                      superscript
                     </csymbol>
                     <ci id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.2">
                      𝑜
                     </ci>
                     <ci id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.3">
                      𝜏
                     </ci>
                    </apply>
                    <apply id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3">
                     <minus id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.1">
                     </minus>
                     <ci id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.2">
                      𝑡
                     </ci>
                     <cn id="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml" type="integer" xref="A1.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.3">
                      1
                     </cn>
                    </apply>
                   </apply>
                   <ci id="A1.Ex1.m1.2.2.cmml" xref="A1.Ex1.m1.2.2">
                    …
                   </ci>
                  </list>
                 </apply>
                </apply>
               </apply>
              </apply>
             </apply>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A1.Ex1.m1.3c">
            \mathcal{L}_{U}(\pi)=\mathbb{E}_{D}\left[-\log\pi(a^{\tau}_{t}|o^{\tau}_{t},o^{\tau}_{t-1},...)\right]
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_eqn_cell ltx_eqn_center_padright">
        </td>
       </tr>
      </tbody>
     </table>
    </div>
    <div class="ltx_listingline" id="alg1.l3">
     <span class="ltx_tag ltx_tag_listingline">
      <span class="ltx_text" id="alg1.l3.1.1.1" style="font-size:80%;">
       3:
      </span>
     </span>
     Fine-tune policy
     <math alttext="\pi" class="ltx_Math" display="inline" id="alg1.l3.m1.1">
      <semantics id="alg1.l3.m1.1a">
       <mi id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml">
        π
       </mi>
       <annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b">
        <ci id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1">
         𝜋
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="alg1.l3.m1.1c">
        \pi
       </annotation>
      </semantics>
     </math>
     on task specific dataset or demonstrations
     <math alttext="T" class="ltx_Math" display="inline" id="alg1.l3.m2.1">
      <semantics id="alg1.l3.m2.1a">
       <mi id="alg1.l3.m2.1.1" xref="alg1.l3.m2.1.1.cmml">
        T
       </mi>
       <annotation-xml encoding="MathML-Content" id="alg1.l3.m2.1b">
        <ci id="alg1.l3.m2.1.1.cmml" xref="alg1.l3.m2.1.1">
         𝑇
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="alg1.l3.m2.1c">
        T
       </annotation>
      </semantics>
     </math>
     <table class="ltx_equation ltx_eqn_table" id="A1.Ex2">
      <tbody>
       <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
        <td class="ltx_eqn_cell ltx_eqn_center_padleft">
        </td>
        <td class="ltx_eqn_cell ltx_align_center">
         <math alttext="\mathcal{L}_{FT}(\pi)=\mathbb{E}_{T}\left[-\log\pi(a^{\tau}_{t}|o^{\tau}_{t},o^{\tau}_{t-1},...)\right]" class="ltx_Math" display="block" id="A1.Ex2.m1.3">
          <semantics id="A1.Ex2.m1.3a">
           <mrow id="A1.Ex2.m1.3.3" xref="A1.Ex2.m1.3.3.cmml">
            <mrow id="A1.Ex2.m1.3.3.3" xref="A1.Ex2.m1.3.3.3.cmml">
             <msub id="A1.Ex2.m1.3.3.3.2" xref="A1.Ex2.m1.3.3.3.2.cmml">
              <mi class="ltx_font_mathcaligraphic" id="A1.Ex2.m1.3.3.3.2.2" xref="A1.Ex2.m1.3.3.3.2.2.cmml">
               ℒ
              </mi>
              <mrow id="A1.Ex2.m1.3.3.3.2.3" xref="A1.Ex2.m1.3.3.3.2.3.cmml">
               <mi id="A1.Ex2.m1.3.3.3.2.3.2" xref="A1.Ex2.m1.3.3.3.2.3.2.cmml">
                F
               </mi>
               <mo id="A1.Ex2.m1.3.3.3.2.3.1" lspace="0em" rspace="0em" xref="A1.Ex2.m1.3.3.3.2.3.1.cmml">
                ​
               </mo>
               <mi id="A1.Ex2.m1.3.3.3.2.3.3" xref="A1.Ex2.m1.3.3.3.2.3.3.cmml">
                T
               </mi>
              </mrow>
             </msub>
             <mo id="A1.Ex2.m1.3.3.3.1" lspace="0em" rspace="0em" xref="A1.Ex2.m1.3.3.3.1.cmml">
              ​
             </mo>
             <mrow id="A1.Ex2.m1.3.3.3.3.2" xref="A1.Ex2.m1.3.3.3.cmml">
              <mo id="A1.Ex2.m1.3.3.3.3.2.1" stretchy="false" xref="A1.Ex2.m1.3.3.3.cmml">
               (
              </mo>
              <mi id="A1.Ex2.m1.1.1" xref="A1.Ex2.m1.1.1.cmml">
               π
              </mi>
              <mo id="A1.Ex2.m1.3.3.3.3.2.2" stretchy="false" xref="A1.Ex2.m1.3.3.3.cmml">
               )
              </mo>
             </mrow>
            </mrow>
            <mo id="A1.Ex2.m1.3.3.2" xref="A1.Ex2.m1.3.3.2.cmml">
             =
            </mo>
            <mrow id="A1.Ex2.m1.3.3.1" xref="A1.Ex2.m1.3.3.1.cmml">
             <msub id="A1.Ex2.m1.3.3.1.3" xref="A1.Ex2.m1.3.3.1.3.cmml">
              <mi id="A1.Ex2.m1.3.3.1.3.2" xref="A1.Ex2.m1.3.3.1.3.2.cmml">
               𝔼
              </mi>
              <mi id="A1.Ex2.m1.3.3.1.3.3" xref="A1.Ex2.m1.3.3.1.3.3.cmml">
               T
              </mi>
             </msub>
             <mo id="A1.Ex2.m1.3.3.1.2" lspace="0em" rspace="0em" xref="A1.Ex2.m1.3.3.1.2.cmml">
              ​
             </mo>
             <mrow id="A1.Ex2.m1.3.3.1.1.1" xref="A1.Ex2.m1.3.3.1.1.2.cmml">
              <mo id="A1.Ex2.m1.3.3.1.1.1.2" xref="A1.Ex2.m1.3.3.1.1.2.1.cmml">
               [
              </mo>
              <mrow id="A1.Ex2.m1.3.3.1.1.1.1" xref="A1.Ex2.m1.3.3.1.1.1.1.cmml">
               <mo id="A1.Ex2.m1.3.3.1.1.1.1a" rspace="0.167em" xref="A1.Ex2.m1.3.3.1.1.1.1.cmml">
                −
               </mo>
               <mrow id="A1.Ex2.m1.3.3.1.1.1.1.1" xref="A1.Ex2.m1.3.3.1.1.1.1.1.cmml">
                <mrow id="A1.Ex2.m1.3.3.1.1.1.1.1.3" xref="A1.Ex2.m1.3.3.1.1.1.1.1.3.cmml">
                 <mi id="A1.Ex2.m1.3.3.1.1.1.1.1.3.1" xref="A1.Ex2.m1.3.3.1.1.1.1.1.3.1.cmml">
                  log
                 </mi>
                 <mo id="A1.Ex2.m1.3.3.1.1.1.1.1.3a" lspace="0.167em" xref="A1.Ex2.m1.3.3.1.1.1.1.1.3.cmml">
                  ⁡
                 </mo>
                 <mi id="A1.Ex2.m1.3.3.1.1.1.1.1.3.2" xref="A1.Ex2.m1.3.3.1.1.1.1.1.3.2.cmml">
                  π
                 </mi>
                </mrow>
                <mo id="A1.Ex2.m1.3.3.1.1.1.1.1.2" lspace="0em" rspace="0em" xref="A1.Ex2.m1.3.3.1.1.1.1.1.2.cmml">
                 ​
                </mo>
                <mrow id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.cmml">
                 <mo id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.2" stretchy="false" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.cmml">
                  (
                 </mo>
                 <mrow id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.cmml">
                  <msubsup id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.cmml">
                   <mi id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.2.2" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.2.2.cmml">
                    a
                   </mi>
                   <mi id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.3" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.3.cmml">
                    t
                   </mi>
                   <mi id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.2.3" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.2.3.cmml">
                    τ
                   </mi>
                  </msubsup>
                  <mo fence="false" id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.3" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">
                   |
                  </mo>
                  <mrow id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml">
                   <msubsup id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">
                    <mi id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">
                     o
                    </mi>
                    <mi id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml">
                     t
                    </mi>
                    <mi id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">
                     τ
                    </mi>
                   </msubsup>
                   <mo id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.3" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml">
                    ,
                   </mo>
                   <msubsup id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.cmml">
                    <mi id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.2" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml">
                     o
                    </mi>
                    <mrow id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.cmml">
                     <mi id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.2" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml">
                      t
                     </mi>
                     <mo id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.1" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml">
                      −
                     </mo>
                     <mn id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.3" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml">
                      1
                     </mn>
                    </mrow>
                    <mi id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.3" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml">
                     τ
                    </mi>
                   </msubsup>
                   <mo id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.4" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml">
                    ,
                   </mo>
                   <mi id="A1.Ex2.m1.2.2" mathvariant="normal" xref="A1.Ex2.m1.2.2.cmml">
                    …
                   </mi>
                  </mrow>
                 </mrow>
                 <mo id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.3" stretchy="false" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.cmml">
                  )
                 </mo>
                </mrow>
               </mrow>
              </mrow>
              <mo id="A1.Ex2.m1.3.3.1.1.1.3" xref="A1.Ex2.m1.3.3.1.1.2.1.cmml">
               ]
              </mo>
             </mrow>
            </mrow>
           </mrow>
           <annotation-xml encoding="MathML-Content" id="A1.Ex2.m1.3b">
            <apply id="A1.Ex2.m1.3.3.cmml" xref="A1.Ex2.m1.3.3">
             <eq id="A1.Ex2.m1.3.3.2.cmml" xref="A1.Ex2.m1.3.3.2">
             </eq>
             <apply id="A1.Ex2.m1.3.3.3.cmml" xref="A1.Ex2.m1.3.3.3">
              <times id="A1.Ex2.m1.3.3.3.1.cmml" xref="A1.Ex2.m1.3.3.3.1">
              </times>
              <apply id="A1.Ex2.m1.3.3.3.2.cmml" xref="A1.Ex2.m1.3.3.3.2">
               <csymbol cd="ambiguous" id="A1.Ex2.m1.3.3.3.2.1.cmml" xref="A1.Ex2.m1.3.3.3.2">
                subscript
               </csymbol>
               <ci id="A1.Ex2.m1.3.3.3.2.2.cmml" xref="A1.Ex2.m1.3.3.3.2.2">
                ℒ
               </ci>
               <apply id="A1.Ex2.m1.3.3.3.2.3.cmml" xref="A1.Ex2.m1.3.3.3.2.3">
                <times id="A1.Ex2.m1.3.3.3.2.3.1.cmml" xref="A1.Ex2.m1.3.3.3.2.3.1">
                </times>
                <ci id="A1.Ex2.m1.3.3.3.2.3.2.cmml" xref="A1.Ex2.m1.3.3.3.2.3.2">
                 𝐹
                </ci>
                <ci id="A1.Ex2.m1.3.3.3.2.3.3.cmml" xref="A1.Ex2.m1.3.3.3.2.3.3">
                 𝑇
                </ci>
               </apply>
              </apply>
              <ci id="A1.Ex2.m1.1.1.cmml" xref="A1.Ex2.m1.1.1">
               𝜋
              </ci>
             </apply>
             <apply id="A1.Ex2.m1.3.3.1.cmml" xref="A1.Ex2.m1.3.3.1">
              <times id="A1.Ex2.m1.3.3.1.2.cmml" xref="A1.Ex2.m1.3.3.1.2">
              </times>
              <apply id="A1.Ex2.m1.3.3.1.3.cmml" xref="A1.Ex2.m1.3.3.1.3">
               <csymbol cd="ambiguous" id="A1.Ex2.m1.3.3.1.3.1.cmml" xref="A1.Ex2.m1.3.3.1.3">
                subscript
               </csymbol>
               <ci id="A1.Ex2.m1.3.3.1.3.2.cmml" xref="A1.Ex2.m1.3.3.1.3.2">
                𝔼
               </ci>
               <ci id="A1.Ex2.m1.3.3.1.3.3.cmml" xref="A1.Ex2.m1.3.3.1.3.3">
                𝑇
               </ci>
              </apply>
              <apply id="A1.Ex2.m1.3.3.1.1.2.cmml" xref="A1.Ex2.m1.3.3.1.1.1">
               <csymbol cd="latexml" id="A1.Ex2.m1.3.3.1.1.2.1.cmml" xref="A1.Ex2.m1.3.3.1.1.1.2">
                delimited-[]
               </csymbol>
               <apply id="A1.Ex2.m1.3.3.1.1.1.1.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1">
                <minus id="A1.Ex2.m1.3.3.1.1.1.1.2.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1">
                </minus>
                <apply id="A1.Ex2.m1.3.3.1.1.1.1.1.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1">
                 <times id="A1.Ex2.m1.3.3.1.1.1.1.1.2.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.2">
                 </times>
                 <apply id="A1.Ex2.m1.3.3.1.1.1.1.1.3.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.3">
                  <log id="A1.Ex2.m1.3.3.1.1.1.1.1.3.1.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.3.1">
                  </log>
                  <ci id="A1.Ex2.m1.3.3.1.1.1.1.1.3.2.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.3.2">
                   𝜋
                  </ci>
                 </apply>
                 <apply id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1">
                  <csymbol cd="latexml" id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.3">
                   conditional
                  </csymbol>
                  <apply id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4">
                   <csymbol cd="ambiguous" id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.1.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4">
                    subscript
                   </csymbol>
                   <apply id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.2.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4">
                    <csymbol cd="ambiguous" id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.2.1.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4">
                     superscript
                    </csymbol>
                    <ci id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.2.2.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.2.2">
                     𝑎
                    </ci>
                    <ci id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.2.3.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.2.3">
                     𝜏
                    </ci>
                   </apply>
                   <ci id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.3.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.4.3">
                    𝑡
                   </ci>
                  </apply>
                  <list id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2">
                   <apply id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1">
                    <csymbol cd="ambiguous" id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1">
                     subscript
                    </csymbol>
                    <apply id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1">
                     <csymbol cd="ambiguous" id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1">
                      superscript
                     </csymbol>
                     <ci id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2">
                      𝑜
                     </ci>
                     <ci id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3">
                      𝜏
                     </ci>
                    </apply>
                    <ci id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3">
                     𝑡
                    </ci>
                   </apply>
                   <apply id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2">
                    <csymbol cd="ambiguous" id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2">
                     subscript
                    </csymbol>
                    <apply id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2">
                     <csymbol cd="ambiguous" id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2">
                      superscript
                     </csymbol>
                     <ci id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.2">
                      𝑜
                     </ci>
                     <ci id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.2.3">
                      𝜏
                     </ci>
                    </apply>
                    <apply id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3">
                     <minus id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.1">
                     </minus>
                     <ci id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.2">
                      𝑡
                     </ci>
                     <cn id="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml" type="integer" xref="A1.Ex2.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.3.3">
                      1
                     </cn>
                    </apply>
                   </apply>
                   <ci id="A1.Ex2.m1.2.2.cmml" xref="A1.Ex2.m1.2.2">
                    …
                   </ci>
                  </list>
                 </apply>
                </apply>
               </apply>
              </apply>
             </apply>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A1.Ex2.m1.3c">
            \mathcal{L}_{FT}(\pi)=\mathbb{E}_{T}\left[-\log\pi(a^{\tau}_{t}|o^{\tau}_{t},o^{\tau}_{t-1},...)\right]
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_eqn_cell ltx_eqn_center_padright">
        </td>
       </tr>
      </tbody>
     </table>
    </div>
    <div class="ltx_listingline" id="alg1.l4">
     <span class="ltx_tag ltx_tag_listingline">
      <span class="ltx_text" id="alg1.l4.1.1.1" style="font-size:80%;">
       4:
      </span>
     </span>
     Run fine-tuned policy
     <math alttext="\pi" class="ltx_Math" display="inline" id="alg1.l4.m1.1">
      <semantics id="alg1.l4.m1.1a">
       <mi id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml">
        π
       </mi>
       <annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b">
        <ci id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1">
         𝜋
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="alg1.l4.m1.1c">
        \pi
       </annotation>
      </semantics>
     </math>
     in environment and provide preferences on pairwise comparisons on online trajectories to obtain preference data
     <math alttext="(\tau_{A}\succ\tau_{B})\in P" class="ltx_Math" display="inline" id="alg1.l4.m2.1">
      <semantics id="alg1.l4.m2.1a">
       <mrow id="alg1.l4.m2.1.1" xref="alg1.l4.m2.1.1.cmml">
        <mrow id="alg1.l4.m2.1.1.1.1" xref="alg1.l4.m2.1.1.1.1.1.cmml">
         <mo id="alg1.l4.m2.1.1.1.1.2" stretchy="false" xref="alg1.l4.m2.1.1.1.1.1.cmml">
          (
         </mo>
         <mrow id="alg1.l4.m2.1.1.1.1.1" xref="alg1.l4.m2.1.1.1.1.1.cmml">
          <msub id="alg1.l4.m2.1.1.1.1.1.2" xref="alg1.l4.m2.1.1.1.1.1.2.cmml">
           <mi id="alg1.l4.m2.1.1.1.1.1.2.2" xref="alg1.l4.m2.1.1.1.1.1.2.2.cmml">
            τ
           </mi>
           <mi id="alg1.l4.m2.1.1.1.1.1.2.3" xref="alg1.l4.m2.1.1.1.1.1.2.3.cmml">
            A
           </mi>
          </msub>
          <mo id="alg1.l4.m2.1.1.1.1.1.1" xref="alg1.l4.m2.1.1.1.1.1.1.cmml">
           ≻
          </mo>
          <msub id="alg1.l4.m2.1.1.1.1.1.3" xref="alg1.l4.m2.1.1.1.1.1.3.cmml">
           <mi id="alg1.l4.m2.1.1.1.1.1.3.2" xref="alg1.l4.m2.1.1.1.1.1.3.2.cmml">
            τ
           </mi>
           <mi id="alg1.l4.m2.1.1.1.1.1.3.3" xref="alg1.l4.m2.1.1.1.1.1.3.3.cmml">
            B
           </mi>
          </msub>
         </mrow>
         <mo id="alg1.l4.m2.1.1.1.1.3" stretchy="false" xref="alg1.l4.m2.1.1.1.1.1.cmml">
          )
         </mo>
        </mrow>
        <mo id="alg1.l4.m2.1.1.2" xref="alg1.l4.m2.1.1.2.cmml">
         ∈
        </mo>
        <mi id="alg1.l4.m2.1.1.3" xref="alg1.l4.m2.1.1.3.cmml">
         P
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="alg1.l4.m2.1b">
        <apply id="alg1.l4.m2.1.1.cmml" xref="alg1.l4.m2.1.1">
         <in id="alg1.l4.m2.1.1.2.cmml" xref="alg1.l4.m2.1.1.2">
         </in>
         <apply id="alg1.l4.m2.1.1.1.1.1.cmml" xref="alg1.l4.m2.1.1.1.1">
          <csymbol cd="latexml" id="alg1.l4.m2.1.1.1.1.1.1.cmml" xref="alg1.l4.m2.1.1.1.1.1.1">
           succeeds
          </csymbol>
          <apply id="alg1.l4.m2.1.1.1.1.1.2.cmml" xref="alg1.l4.m2.1.1.1.1.1.2">
           <csymbol cd="ambiguous" id="alg1.l4.m2.1.1.1.1.1.2.1.cmml" xref="alg1.l4.m2.1.1.1.1.1.2">
            subscript
           </csymbol>
           <ci id="alg1.l4.m2.1.1.1.1.1.2.2.cmml" xref="alg1.l4.m2.1.1.1.1.1.2.2">
            𝜏
           </ci>
           <ci id="alg1.l4.m2.1.1.1.1.1.2.3.cmml" xref="alg1.l4.m2.1.1.1.1.1.2.3">
            𝐴
           </ci>
          </apply>
          <apply id="alg1.l4.m2.1.1.1.1.1.3.cmml" xref="alg1.l4.m2.1.1.1.1.1.3">
           <csymbol cd="ambiguous" id="alg1.l4.m2.1.1.1.1.1.3.1.cmml" xref="alg1.l4.m2.1.1.1.1.1.3">
            subscript
           </csymbol>
           <ci id="alg1.l4.m2.1.1.1.1.1.3.2.cmml" xref="alg1.l4.m2.1.1.1.1.1.3.2">
            𝜏
           </ci>
           <ci id="alg1.l4.m2.1.1.1.1.1.3.3.cmml" xref="alg1.l4.m2.1.1.1.1.1.3.3">
            𝐵
           </ci>
          </apply>
         </apply>
         <ci id="alg1.l4.m2.1.1.3.cmml" xref="alg1.l4.m2.1.1.3">
          𝑃
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="alg1.l4.m2.1c">
        (\tau_{A}\succ\tau_{B})\in P
       </annotation>
      </semantics>
     </math>
    </div>
    <div class="ltx_listingline" id="alg1.l5">
     <span class="ltx_tag ltx_tag_listingline">
      <span class="ltx_text" id="alg1.l5.1.1.1" style="font-size:80%;">
       5:
      </span>
     </span>
     Initialise additional reward model
     <math alttext="\hat{r}" class="ltx_Math" display="inline" id="alg1.l5.m1.1">
      <semantics id="alg1.l5.m1.1a">
       <mover accent="true" id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml">
        <mi id="alg1.l5.m1.1.1.2" xref="alg1.l5.m1.1.1.2.cmml">
         r
        </mi>
        <mo id="alg1.l5.m1.1.1.1" xref="alg1.l5.m1.1.1.1.cmml">
         ^
        </mo>
       </mover>
       <annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b">
        <apply id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1">
         <ci id="alg1.l5.m1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1">
          ^
         </ci>
         <ci id="alg1.l5.m1.1.1.2.cmml" xref="alg1.l5.m1.1.1.2">
          𝑟
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="alg1.l5.m1.1c">
        \hat{r}
       </annotation>
      </semantics>
     </math>
     from
     <math alttext="\pi" class="ltx_Math" display="inline" id="alg1.l5.m2.1">
      <semantics id="alg1.l5.m2.1a">
       <mi id="alg1.l5.m2.1.1" xref="alg1.l5.m2.1.1.cmml">
        π
       </mi>
       <annotation-xml encoding="MathML-Content" id="alg1.l5.m2.1b">
        <ci id="alg1.l5.m2.1.1.cmml" xref="alg1.l5.m2.1.1">
         𝜋
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="alg1.l5.m2.1c">
        \pi
       </annotation>
      </semantics>
     </math>
     and train on preferences
     <math alttext="P" class="ltx_Math" display="inline" id="alg1.l5.m3.1">
      <semantics id="alg1.l5.m3.1a">
       <mi id="alg1.l5.m3.1.1" xref="alg1.l5.m3.1.1.cmml">
        P
       </mi>
       <annotation-xml encoding="MathML-Content" id="alg1.l5.m3.1b">
        <ci id="alg1.l5.m3.1.1.cmml" xref="alg1.l5.m3.1.1">
         𝑃
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="alg1.l5.m3.1c">
        P
       </annotation>
      </semantics>
     </math>
     using Bradley-Terry model
     <cite class="ltx_cite ltx_citemacro_citep">
      (Bradley &amp; Terry,
      <a class="ltx_ref" href="#bib.bib12" title="">
       1952
      </a>
      )
     </cite>
     <table class="ltx_equation ltx_eqn_table" id="A1.Ex3">
      <tbody>
       <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
        <td class="ltx_eqn_cell ltx_eqn_center_padleft">
        </td>
        <td class="ltx_eqn_cell ltx_align_center">
         <math alttext="\mathcal{L}(\hat{r})=\sum_{(\tau_{w},\tau_{l})\in P}-\log\bigg{(}\sigma\big{(}\hat{r}(\tau_{w})-\hat{r}(\tau_{l})\big{)}\bigg{)}" class="ltx_Math" display="block" id="A1.Ex3.m1.5">
          <semantics id="A1.Ex3.m1.5a">
           <mrow id="A1.Ex3.m1.5.5" xref="A1.Ex3.m1.5.5.cmml">
            <mrow id="A1.Ex3.m1.5.5.3" xref="A1.Ex3.m1.5.5.3.cmml">
             <mi class="ltx_font_mathcaligraphic" id="A1.Ex3.m1.5.5.3.2" xref="A1.Ex3.m1.5.5.3.2.cmml">
              ℒ
             </mi>
             <mo id="A1.Ex3.m1.5.5.3.1" lspace="0em" rspace="0em" xref="A1.Ex3.m1.5.5.3.1.cmml">
              ​
             </mo>
             <mrow id="A1.Ex3.m1.5.5.3.3.2" xref="A1.Ex3.m1.3.3.cmml">
              <mo id="A1.Ex3.m1.5.5.3.3.2.1" stretchy="false" xref="A1.Ex3.m1.3.3.cmml">
               (
              </mo>
              <mover accent="true" id="A1.Ex3.m1.3.3" xref="A1.Ex3.m1.3.3.cmml">
               <mi id="A1.Ex3.m1.3.3.2" xref="A1.Ex3.m1.3.3.2.cmml">
                r
               </mi>
               <mo id="A1.Ex3.m1.3.3.1" xref="A1.Ex3.m1.3.3.1.cmml">
                ^
               </mo>
              </mover>
              <mo id="A1.Ex3.m1.5.5.3.3.2.2" stretchy="false" xref="A1.Ex3.m1.3.3.cmml">
               )
              </mo>
             </mrow>
            </mrow>
            <mo id="A1.Ex3.m1.5.5.2" rspace="0.111em" xref="A1.Ex3.m1.5.5.2.cmml">
             =
            </mo>
            <mrow id="A1.Ex3.m1.5.5.1" xref="A1.Ex3.m1.5.5.1.cmml">
             <munder id="A1.Ex3.m1.5.5.1.3" xref="A1.Ex3.m1.5.5.1.3.cmml">
              <mo id="A1.Ex3.m1.5.5.1.3.2" movablelimits="false" rspace="0em" xref="A1.Ex3.m1.5.5.1.3.2.cmml">
               ∑
              </mo>
              <mrow id="A1.Ex3.m1.2.2.2" xref="A1.Ex3.m1.2.2.2.cmml">
               <mrow id="A1.Ex3.m1.2.2.2.2.2" xref="A1.Ex3.m1.2.2.2.2.3.cmml">
                <mo id="A1.Ex3.m1.2.2.2.2.2.3" stretchy="false" xref="A1.Ex3.m1.2.2.2.2.3.cmml">
                 (
                </mo>
                <msub id="A1.Ex3.m1.1.1.1.1.1.1" xref="A1.Ex3.m1.1.1.1.1.1.1.cmml">
                 <mi id="A1.Ex3.m1.1.1.1.1.1.1.2" xref="A1.Ex3.m1.1.1.1.1.1.1.2.cmml">
                  τ
                 </mi>
                 <mi id="A1.Ex3.m1.1.1.1.1.1.1.3" xref="A1.Ex3.m1.1.1.1.1.1.1.3.cmml">
                  w
                 </mi>
                </msub>
                <mo id="A1.Ex3.m1.2.2.2.2.2.4" xref="A1.Ex3.m1.2.2.2.2.3.cmml">
                 ,
                </mo>
                <msub id="A1.Ex3.m1.2.2.2.2.2.2" xref="A1.Ex3.m1.2.2.2.2.2.2.cmml">
                 <mi id="A1.Ex3.m1.2.2.2.2.2.2.2" xref="A1.Ex3.m1.2.2.2.2.2.2.2.cmml">
                  τ
                 </mi>
                 <mi id="A1.Ex3.m1.2.2.2.2.2.2.3" xref="A1.Ex3.m1.2.2.2.2.2.2.3.cmml">
                  l
                 </mi>
                </msub>
                <mo id="A1.Ex3.m1.2.2.2.2.2.5" stretchy="false" xref="A1.Ex3.m1.2.2.2.2.3.cmml">
                 )
                </mo>
               </mrow>
               <mo id="A1.Ex3.m1.2.2.2.3" xref="A1.Ex3.m1.2.2.2.3.cmml">
                ∈
               </mo>
               <mi id="A1.Ex3.m1.2.2.2.4" xref="A1.Ex3.m1.2.2.2.4.cmml">
                P
               </mi>
              </mrow>
             </munder>
             <mo id="A1.Ex3.m1.5.5.1.2" lspace="0em" xref="A1.Ex3.m1.5.5.1.2.cmml">
              −
             </mo>
             <mrow id="A1.Ex3.m1.5.5.1.1.1" xref="A1.Ex3.m1.5.5.1.1.2.cmml">
              <mi id="A1.Ex3.m1.4.4" xref="A1.Ex3.m1.4.4.cmml">
               log
              </mi>
              <mo id="A1.Ex3.m1.5.5.1.1.1a" xref="A1.Ex3.m1.5.5.1.1.2.cmml">
               ⁡
              </mo>
              <mrow id="A1.Ex3.m1.5.5.1.1.1.1" xref="A1.Ex3.m1.5.5.1.1.2.cmml">
               <mo id="A1.Ex3.m1.5.5.1.1.1.1.2" maxsize="210%" minsize="210%" xref="A1.Ex3.m1.5.5.1.1.2.cmml">
                (
               </mo>
               <mrow id="A1.Ex3.m1.5.5.1.1.1.1.1" xref="A1.Ex3.m1.5.5.1.1.1.1.1.cmml">
                <mi id="A1.Ex3.m1.5.5.1.1.1.1.1.3" xref="A1.Ex3.m1.5.5.1.1.1.1.1.3.cmml">
                 σ
                </mi>
                <mo id="A1.Ex3.m1.5.5.1.1.1.1.1.2" lspace="0em" rspace="0em" xref="A1.Ex3.m1.5.5.1.1.1.1.1.2.cmml">
                 ​
                </mo>
                <mrow id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.cmml">
                 <mo id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.2" maxsize="120%" minsize="120%" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.cmml">
                  (
                 </mo>
                 <mrow id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.cmml">
                  <mrow id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.cmml">
                   <mover accent="true" id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.3" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.3.cmml">
                    <mi id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.3.2" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.3.2.cmml">
                     r
                    </mi>
                    <mo id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.3.1" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.3.1.cmml">
                     ^
                    </mo>
                   </mover>
                   <mo id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.2" lspace="0em" rspace="0em" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml">
                    ​
                   </mo>
                   <mrow id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.cmml">
                    <mo id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.cmml">
                     (
                    </mo>
                    <msub id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.cmml">
                     <mi id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">
                      τ
                     </mi>
                     <mi id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">
                      w
                     </mi>
                    </msub>
                    <mo id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.cmml">
                     )
                    </mo>
                   </mrow>
                  </mrow>
                  <mo id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.3" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.3.cmml">
                   −
                  </mo>
                  <mrow id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.cmml">
                   <mover accent="true" id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.3" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.3.cmml">
                    <mi id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.3.2" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.3.2.cmml">
                     r
                    </mi>
                    <mo id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.3.1" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.3.1.cmml">
                     ^
                    </mo>
                   </mover>
                   <mo id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.2" lspace="0em" rspace="0em" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.2.cmml">
                    ​
                   </mo>
                   <mrow id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.cmml">
                    <mo id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.2" stretchy="false" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.cmml">
                     (
                    </mo>
                    <msub id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.cmml">
                     <mi id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.2" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.2.cmml">
                      τ
                     </mi>
                     <mi id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.3" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.3.cmml">
                      l
                     </mi>
                    </msub>
                    <mo id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.3" stretchy="false" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.cmml">
                     )
                    </mo>
                   </mrow>
                  </mrow>
                 </mrow>
                 <mo id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.3" maxsize="120%" minsize="120%" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.cmml">
                  )
                 </mo>
                </mrow>
               </mrow>
               <mo id="A1.Ex3.m1.5.5.1.1.1.1.3" maxsize="210%" minsize="210%" xref="A1.Ex3.m1.5.5.1.1.2.cmml">
                )
               </mo>
              </mrow>
             </mrow>
            </mrow>
           </mrow>
           <annotation-xml encoding="MathML-Content" id="A1.Ex3.m1.5b">
            <apply id="A1.Ex3.m1.5.5.cmml" xref="A1.Ex3.m1.5.5">
             <eq id="A1.Ex3.m1.5.5.2.cmml" xref="A1.Ex3.m1.5.5.2">
             </eq>
             <apply id="A1.Ex3.m1.5.5.3.cmml" xref="A1.Ex3.m1.5.5.3">
              <times id="A1.Ex3.m1.5.5.3.1.cmml" xref="A1.Ex3.m1.5.5.3.1">
              </times>
              <ci id="A1.Ex3.m1.5.5.3.2.cmml" xref="A1.Ex3.m1.5.5.3.2">
               ℒ
              </ci>
              <apply id="A1.Ex3.m1.3.3.cmml" xref="A1.Ex3.m1.5.5.3.3.2">
               <ci id="A1.Ex3.m1.3.3.1.cmml" xref="A1.Ex3.m1.3.3.1">
                ^
               </ci>
               <ci id="A1.Ex3.m1.3.3.2.cmml" xref="A1.Ex3.m1.3.3.2">
                𝑟
               </ci>
              </apply>
             </apply>
             <apply id="A1.Ex3.m1.5.5.1.cmml" xref="A1.Ex3.m1.5.5.1">
              <minus id="A1.Ex3.m1.5.5.1.2.cmml" xref="A1.Ex3.m1.5.5.1.2">
              </minus>
              <apply id="A1.Ex3.m1.5.5.1.3.cmml" xref="A1.Ex3.m1.5.5.1.3">
               <csymbol cd="ambiguous" id="A1.Ex3.m1.5.5.1.3.1.cmml" xref="A1.Ex3.m1.5.5.1.3">
                subscript
               </csymbol>
               <sum id="A1.Ex3.m1.5.5.1.3.2.cmml" xref="A1.Ex3.m1.5.5.1.3.2">
               </sum>
               <apply id="A1.Ex3.m1.2.2.2.cmml" xref="A1.Ex3.m1.2.2.2">
                <in id="A1.Ex3.m1.2.2.2.3.cmml" xref="A1.Ex3.m1.2.2.2.3">
                </in>
                <interval closure="open" id="A1.Ex3.m1.2.2.2.2.3.cmml" xref="A1.Ex3.m1.2.2.2.2.2">
                 <apply id="A1.Ex3.m1.1.1.1.1.1.1.cmml" xref="A1.Ex3.m1.1.1.1.1.1.1">
                  <csymbol cd="ambiguous" id="A1.Ex3.m1.1.1.1.1.1.1.1.cmml" xref="A1.Ex3.m1.1.1.1.1.1.1">
                   subscript
                  </csymbol>
                  <ci id="A1.Ex3.m1.1.1.1.1.1.1.2.cmml" xref="A1.Ex3.m1.1.1.1.1.1.1.2">
                   𝜏
                  </ci>
                  <ci id="A1.Ex3.m1.1.1.1.1.1.1.3.cmml" xref="A1.Ex3.m1.1.1.1.1.1.1.3">
                   𝑤
                  </ci>
                 </apply>
                 <apply id="A1.Ex3.m1.2.2.2.2.2.2.cmml" xref="A1.Ex3.m1.2.2.2.2.2.2">
                  <csymbol cd="ambiguous" id="A1.Ex3.m1.2.2.2.2.2.2.1.cmml" xref="A1.Ex3.m1.2.2.2.2.2.2">
                   subscript
                  </csymbol>
                  <ci id="A1.Ex3.m1.2.2.2.2.2.2.2.cmml" xref="A1.Ex3.m1.2.2.2.2.2.2.2">
                   𝜏
                  </ci>
                  <ci id="A1.Ex3.m1.2.2.2.2.2.2.3.cmml" xref="A1.Ex3.m1.2.2.2.2.2.2.3">
                   𝑙
                  </ci>
                 </apply>
                </interval>
                <ci id="A1.Ex3.m1.2.2.2.4.cmml" xref="A1.Ex3.m1.2.2.2.4">
                 𝑃
                </ci>
               </apply>
              </apply>
              <apply id="A1.Ex3.m1.5.5.1.1.2.cmml" xref="A1.Ex3.m1.5.5.1.1.1">
               <log id="A1.Ex3.m1.4.4.cmml" xref="A1.Ex3.m1.4.4">
               </log>
               <apply id="A1.Ex3.m1.5.5.1.1.1.1.1.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1">
                <times id="A1.Ex3.m1.5.5.1.1.1.1.1.2.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.2">
                </times>
                <ci id="A1.Ex3.m1.5.5.1.1.1.1.1.3.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.3">
                 𝜎
                </ci>
                <apply id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1">
                 <minus id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.3.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.3">
                 </minus>
                 <apply id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1">
                  <times id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.2.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.2">
                  </times>
                  <apply id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.3.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.3">
                   <ci id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.3.1">
                    ^
                   </ci>
                   <ci id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.3.2">
                    𝑟
                   </ci>
                  </apply>
                  <apply id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1">
                   <csymbol cd="ambiguous" id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1">
                    subscript
                   </csymbol>
                   <ci id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.2">
                    𝜏
                   </ci>
                   <ci id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.1.3">
                    𝑤
                   </ci>
                  </apply>
                 </apply>
                 <apply id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2">
                  <times id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.2.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.2">
                  </times>
                  <apply id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.3.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.3">
                   <ci id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.3.1">
                    ^
                   </ci>
                   <ci id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.3.2">
                    𝑟
                   </ci>
                  </apply>
                  <apply id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1">
                   <csymbol cd="ambiguous" id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1">
                    subscript
                   </csymbol>
                   <ci id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.2.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.2">
                    𝜏
                   </ci>
                   <ci id="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.3.cmml" xref="A1.Ex3.m1.5.5.1.1.1.1.1.1.1.1.2.1.1.1.3">
                    𝑙
                   </ci>
                  </apply>
                 </apply>
                </apply>
               </apply>
              </apply>
             </apply>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A1.Ex3.m1.5c">
            \mathcal{L}(\hat{r})=\sum_{(\tau_{w},\tau_{l})\in P}-\log\bigg{(}\sigma\big{(}\hat{r}(\tau_{w})-\hat{r}(\tau_{l})\big{)}\bigg{)}
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_eqn_cell ltx_eqn_center_padright">
        </td>
       </tr>
      </tbody>
     </table>
    </div>
    <div class="ltx_listingline" id="alg1.l6">
     <span class="ltx_tag ltx_tag_listingline">
      <span class="ltx_text" id="alg1.l6.1.1.1" style="font-size:80%;">
       6:
      </span>
     </span>
     Train fine-tuned policy
     <math alttext="\pi_{RL}" class="ltx_Math" display="inline" id="alg1.l6.m1.1">
      <semantics id="alg1.l6.m1.1a">
       <msub id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml">
        <mi id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml">
         π
        </mi>
        <mrow id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml">
         <mi id="alg1.l6.m1.1.1.3.2" xref="alg1.l6.m1.1.1.3.2.cmml">
          R
         </mi>
         <mo id="alg1.l6.m1.1.1.3.1" lspace="0em" rspace="0em" xref="alg1.l6.m1.1.1.3.1.cmml">
          ​
         </mo>
         <mi id="alg1.l6.m1.1.1.3.3" xref="alg1.l6.m1.1.1.3.3.cmml">
          L
         </mi>
        </mrow>
       </msub>
       <annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b">
        <apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1">
         <csymbol cd="ambiguous" id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1">
          subscript
         </csymbol>
         <ci id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2">
          𝜋
         </ci>
         <apply id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3">
          <times id="alg1.l6.m1.1.1.3.1.cmml" xref="alg1.l6.m1.1.1.3.1">
          </times>
          <ci id="alg1.l6.m1.1.1.3.2.cmml" xref="alg1.l6.m1.1.1.3.2">
           𝑅
          </ci>
          <ci id="alg1.l6.m1.1.1.3.3.cmml" xref="alg1.l6.m1.1.1.3.3">
           𝐿
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="alg1.l6.m1.1c">
        \pi_{RL}
       </annotation>
      </semantics>
     </math>
     using online reinforcement learning, using reward model
     <math alttext="\hat{r}" class="ltx_Math" display="inline" id="alg1.l6.m2.1">
      <semantics id="alg1.l6.m2.1a">
       <mover accent="true" id="alg1.l6.m2.1.1" xref="alg1.l6.m2.1.1.cmml">
        <mi id="alg1.l6.m2.1.1.2" xref="alg1.l6.m2.1.1.2.cmml">
         r
        </mi>
        <mo id="alg1.l6.m2.1.1.1" xref="alg1.l6.m2.1.1.1.cmml">
         ^
        </mo>
       </mover>
       <annotation-xml encoding="MathML-Content" id="alg1.l6.m2.1b">
        <apply id="alg1.l6.m2.1.1.cmml" xref="alg1.l6.m2.1.1">
         <ci id="alg1.l6.m2.1.1.1.cmml" xref="alg1.l6.m2.1.1.1">
          ^
         </ci>
         <ci id="alg1.l6.m2.1.1.2.cmml" xref="alg1.l6.m2.1.1.2">
          𝑟
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="alg1.l6.m2.1c">
        \hat{r}
       </annotation>
      </semantics>
     </math>
     to provide return for online trajectory
     <math alttext="\tau" class="ltx_Math" display="inline" id="alg1.l6.m3.1">
      <semantics id="alg1.l6.m3.1a">
       <mi id="alg1.l6.m3.1.1" xref="alg1.l6.m3.1.1.cmml">
        τ
       </mi>
       <annotation-xml encoding="MathML-Content" id="alg1.l6.m3.1b">
        <ci id="alg1.l6.m3.1.1.cmml" xref="alg1.l6.m3.1.1">
         𝜏
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="alg1.l6.m3.1c">
        \tau
       </annotation>
      </semantics>
     </math>
     to align agent with preferences. For example, using REINFORCE
     <cite class="ltx_cite ltx_citemacro_citep">
      (Williams,
      <a class="ltx_ref" href="#bib.bib55" title="">
       1992
      </a>
      )
     </cite>
     :
     <table class="ltx_equation ltx_eqn_table" id="A1.Ex4">
      <tbody>
       <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
        <td class="ltx_eqn_cell ltx_eqn_center_padleft">
        </td>
        <td class="ltx_eqn_cell ltx_align_center">
         <math alttext="\mathcal{L}_{RL}(\pi_{RL})=-\mathbb{E}_{\pi_{RL}}\left[\sum_{t=1}^{T}\gamma^{t}\hat{r}(\tau)\log\pi_{RL}(a_{t}|o_{t},...)\right]" class="ltx_Math" display="block" id="A1.Ex4.m1.4">
          <semantics id="A1.Ex4.m1.4a">
           <mrow id="A1.Ex4.m1.4.4" xref="A1.Ex4.m1.4.4.cmml">
            <mrow id="A1.Ex4.m1.3.3.1" xref="A1.Ex4.m1.3.3.1.cmml">
             <msub id="A1.Ex4.m1.3.3.1.3" xref="A1.Ex4.m1.3.3.1.3.cmml">
              <mi class="ltx_font_mathcaligraphic" id="A1.Ex4.m1.3.3.1.3.2" xref="A1.Ex4.m1.3.3.1.3.2.cmml">
               ℒ
              </mi>
              <mrow id="A1.Ex4.m1.3.3.1.3.3" xref="A1.Ex4.m1.3.3.1.3.3.cmml">
               <mi id="A1.Ex4.m1.3.3.1.3.3.2" xref="A1.Ex4.m1.3.3.1.3.3.2.cmml">
                R
               </mi>
               <mo id="A1.Ex4.m1.3.3.1.3.3.1" lspace="0em" rspace="0em" xref="A1.Ex4.m1.3.3.1.3.3.1.cmml">
                ​
               </mo>
               <mi id="A1.Ex4.m1.3.3.1.3.3.3" xref="A1.Ex4.m1.3.3.1.3.3.3.cmml">
                L
               </mi>
              </mrow>
             </msub>
             <mo id="A1.Ex4.m1.3.3.1.2" lspace="0em" rspace="0em" xref="A1.Ex4.m1.3.3.1.2.cmml">
              ​
             </mo>
             <mrow id="A1.Ex4.m1.3.3.1.1.1" xref="A1.Ex4.m1.3.3.1.1.1.1.cmml">
              <mo id="A1.Ex4.m1.3.3.1.1.1.2" stretchy="false" xref="A1.Ex4.m1.3.3.1.1.1.1.cmml">
               (
              </mo>
              <msub id="A1.Ex4.m1.3.3.1.1.1.1" xref="A1.Ex4.m1.3.3.1.1.1.1.cmml">
               <mi id="A1.Ex4.m1.3.3.1.1.1.1.2" xref="A1.Ex4.m1.3.3.1.1.1.1.2.cmml">
                π
               </mi>
               <mrow id="A1.Ex4.m1.3.3.1.1.1.1.3" xref="A1.Ex4.m1.3.3.1.1.1.1.3.cmml">
                <mi id="A1.Ex4.m1.3.3.1.1.1.1.3.2" xref="A1.Ex4.m1.3.3.1.1.1.1.3.2.cmml">
                 R
                </mi>
                <mo id="A1.Ex4.m1.3.3.1.1.1.1.3.1" lspace="0em" rspace="0em" xref="A1.Ex4.m1.3.3.1.1.1.1.3.1.cmml">
                 ​
                </mo>
                <mi id="A1.Ex4.m1.3.3.1.1.1.1.3.3" xref="A1.Ex4.m1.3.3.1.1.1.1.3.3.cmml">
                 L
                </mi>
               </mrow>
              </msub>
              <mo id="A1.Ex4.m1.3.3.1.1.1.3" stretchy="false" xref="A1.Ex4.m1.3.3.1.1.1.1.cmml">
               )
              </mo>
             </mrow>
            </mrow>
            <mo id="A1.Ex4.m1.4.4.3" xref="A1.Ex4.m1.4.4.3.cmml">
             =
            </mo>
            <mrow id="A1.Ex4.m1.4.4.2" xref="A1.Ex4.m1.4.4.2.cmml">
             <mo id="A1.Ex4.m1.4.4.2a" xref="A1.Ex4.m1.4.4.2.cmml">
              −
             </mo>
             <mrow id="A1.Ex4.m1.4.4.2.1" xref="A1.Ex4.m1.4.4.2.1.cmml">
              <msub id="A1.Ex4.m1.4.4.2.1.3" xref="A1.Ex4.m1.4.4.2.1.3.cmml">
               <mi id="A1.Ex4.m1.4.4.2.1.3.2" xref="A1.Ex4.m1.4.4.2.1.3.2.cmml">
                𝔼
               </mi>
               <msub id="A1.Ex4.m1.4.4.2.1.3.3" xref="A1.Ex4.m1.4.4.2.1.3.3.cmml">
                <mi id="A1.Ex4.m1.4.4.2.1.3.3.2" xref="A1.Ex4.m1.4.4.2.1.3.3.2.cmml">
                 π
                </mi>
                <mrow id="A1.Ex4.m1.4.4.2.1.3.3.3" xref="A1.Ex4.m1.4.4.2.1.3.3.3.cmml">
                 <mi id="A1.Ex4.m1.4.4.2.1.3.3.3.2" xref="A1.Ex4.m1.4.4.2.1.3.3.3.2.cmml">
                  R
                 </mi>
                 <mo id="A1.Ex4.m1.4.4.2.1.3.3.3.1" lspace="0em" rspace="0em" xref="A1.Ex4.m1.4.4.2.1.3.3.3.1.cmml">
                  ​
                 </mo>
                 <mi id="A1.Ex4.m1.4.4.2.1.3.3.3.3" xref="A1.Ex4.m1.4.4.2.1.3.3.3.3.cmml">
                  L
                 </mi>
                </mrow>
               </msub>
              </msub>
              <mo id="A1.Ex4.m1.4.4.2.1.2" lspace="0em" rspace="0em" xref="A1.Ex4.m1.4.4.2.1.2.cmml">
               ​
              </mo>
              <mrow id="A1.Ex4.m1.4.4.2.1.1.1" xref="A1.Ex4.m1.4.4.2.1.1.2.cmml">
               <mo id="A1.Ex4.m1.4.4.2.1.1.1.2" xref="A1.Ex4.m1.4.4.2.1.1.2.1.cmml">
                [
               </mo>
               <mrow id="A1.Ex4.m1.4.4.2.1.1.1.1" xref="A1.Ex4.m1.4.4.2.1.1.1.1.cmml">
                <munderover id="A1.Ex4.m1.4.4.2.1.1.1.1.2" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2.cmml">
                 <mo id="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.2" lspace="0em" movablelimits="false" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.2.cmml">
                  ∑
                 </mo>
                 <mrow id="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.3" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.3.cmml">
                  <mi id="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.3.2" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.3.2.cmml">
                   t
                  </mi>
                  <mo id="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.3.1" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.3.1.cmml">
                   =
                  </mo>
                  <mn id="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.3.3" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.3.3.cmml">
                   1
                  </mn>
                 </mrow>
                 <mi id="A1.Ex4.m1.4.4.2.1.1.1.1.2.3" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2.3.cmml">
                  T
                 </mi>
                </munderover>
                <mrow id="A1.Ex4.m1.4.4.2.1.1.1.1.1" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.cmml">
                 <msup id="A1.Ex4.m1.4.4.2.1.1.1.1.1.3" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.3.cmml">
                  <mi id="A1.Ex4.m1.4.4.2.1.1.1.1.1.3.2" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.3.2.cmml">
                   γ
                  </mi>
                  <mi id="A1.Ex4.m1.4.4.2.1.1.1.1.1.3.3" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.3.3.cmml">
                   t
                  </mi>
                 </msup>
                 <mo id="A1.Ex4.m1.4.4.2.1.1.1.1.1.2" lspace="0em" rspace="0em" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.2.cmml">
                  ​
                 </mo>
                 <mover accent="true" id="A1.Ex4.m1.4.4.2.1.1.1.1.1.4" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.4.cmml">
                  <mi id="A1.Ex4.m1.4.4.2.1.1.1.1.1.4.2" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.4.2.cmml">
                   r
                  </mi>
                  <mo id="A1.Ex4.m1.4.4.2.1.1.1.1.1.4.1" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.4.1.cmml">
                   ^
                  </mo>
                 </mover>
                 <mo id="A1.Ex4.m1.4.4.2.1.1.1.1.1.2a" lspace="0em" rspace="0em" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.2.cmml">
                  ​
                 </mo>
                 <mrow id="A1.Ex4.m1.4.4.2.1.1.1.1.1.5.2" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.cmml">
                  <mo id="A1.Ex4.m1.4.4.2.1.1.1.1.1.5.2.1" stretchy="false" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.cmml">
                   (
                  </mo>
                  <mi id="A1.Ex4.m1.1.1" xref="A1.Ex4.m1.1.1.cmml">
                   τ
                  </mi>
                  <mo id="A1.Ex4.m1.4.4.2.1.1.1.1.1.5.2.2" stretchy="false" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.cmml">
                   )
                  </mo>
                 </mrow>
                 <mo id="A1.Ex4.m1.4.4.2.1.1.1.1.1.2b" lspace="0.167em" rspace="0em" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.2.cmml">
                  ​
                 </mo>
                 <mrow id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.cmml">
                  <mi id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.1" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.1.cmml">
                   log
                  </mi>
                  <mo id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6a" lspace="0.167em" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.cmml">
                   ⁡
                  </mo>
                  <msub id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.cmml">
                   <mi id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.2" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.2.cmml">
                    π
                   </mi>
                   <mrow id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.3" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.3.cmml">
                    <mi id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.3.2" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.3.2.cmml">
                     R
                    </mi>
                    <mo id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.3.1" lspace="0em" rspace="0em" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.3.1.cmml">
                     ​
                    </mo>
                    <mi id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.3.3" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.3.3.cmml">
                     L
                    </mi>
                   </mrow>
                  </msub>
                 </mrow>
                 <mo id="A1.Ex4.m1.4.4.2.1.1.1.1.1.2c" lspace="0em" rspace="0em" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.2.cmml">
                  ​
                 </mo>
                 <mrow id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.cmml">
                  <mo id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.2" stretchy="false" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.cmml">
                   (
                  </mo>
                  <mrow id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.cmml">
                   <msub id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.3" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.3.cmml">
                    <mi id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.3.2" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.3.2.cmml">
                     a
                    </mi>
                    <mi id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.3.3" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.3.3.cmml">
                     t
                    </mi>
                   </msub>
                   <mo fence="false" id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.2" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.2.cmml">
                    |
                   </mo>
                   <mrow id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.2.cmml">
                    <msub id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1.1" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1.1.cmml">
                     <mi id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">
                      o
                     </mi>
                     <mi id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">
                      t
                     </mi>
                    </msub>
                    <mo id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1.2" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.2.cmml">
                     ,
                    </mo>
                    <mi id="A1.Ex4.m1.2.2" mathvariant="normal" xref="A1.Ex4.m1.2.2.cmml">
                     …
                    </mi>
                   </mrow>
                  </mrow>
                  <mo id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.3" stretchy="false" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.cmml">
                   )
                  </mo>
                 </mrow>
                </mrow>
               </mrow>
               <mo id="A1.Ex4.m1.4.4.2.1.1.1.3" xref="A1.Ex4.m1.4.4.2.1.1.2.1.cmml">
                ]
               </mo>
              </mrow>
             </mrow>
            </mrow>
           </mrow>
           <annotation-xml encoding="MathML-Content" id="A1.Ex4.m1.4b">
            <apply id="A1.Ex4.m1.4.4.cmml" xref="A1.Ex4.m1.4.4">
             <eq id="A1.Ex4.m1.4.4.3.cmml" xref="A1.Ex4.m1.4.4.3">
             </eq>
             <apply id="A1.Ex4.m1.3.3.1.cmml" xref="A1.Ex4.m1.3.3.1">
              <times id="A1.Ex4.m1.3.3.1.2.cmml" xref="A1.Ex4.m1.3.3.1.2">
              </times>
              <apply id="A1.Ex4.m1.3.3.1.3.cmml" xref="A1.Ex4.m1.3.3.1.3">
               <csymbol cd="ambiguous" id="A1.Ex4.m1.3.3.1.3.1.cmml" xref="A1.Ex4.m1.3.3.1.3">
                subscript
               </csymbol>
               <ci id="A1.Ex4.m1.3.3.1.3.2.cmml" xref="A1.Ex4.m1.3.3.1.3.2">
                ℒ
               </ci>
               <apply id="A1.Ex4.m1.3.3.1.3.3.cmml" xref="A1.Ex4.m1.3.3.1.3.3">
                <times id="A1.Ex4.m1.3.3.1.3.3.1.cmml" xref="A1.Ex4.m1.3.3.1.3.3.1">
                </times>
                <ci id="A1.Ex4.m1.3.3.1.3.3.2.cmml" xref="A1.Ex4.m1.3.3.1.3.3.2">
                 𝑅
                </ci>
                <ci id="A1.Ex4.m1.3.3.1.3.3.3.cmml" xref="A1.Ex4.m1.3.3.1.3.3.3">
                 𝐿
                </ci>
               </apply>
              </apply>
              <apply id="A1.Ex4.m1.3.3.1.1.1.1.cmml" xref="A1.Ex4.m1.3.3.1.1.1">
               <csymbol cd="ambiguous" id="A1.Ex4.m1.3.3.1.1.1.1.1.cmml" xref="A1.Ex4.m1.3.3.1.1.1">
                subscript
               </csymbol>
               <ci id="A1.Ex4.m1.3.3.1.1.1.1.2.cmml" xref="A1.Ex4.m1.3.3.1.1.1.1.2">
                𝜋
               </ci>
               <apply id="A1.Ex4.m1.3.3.1.1.1.1.3.cmml" xref="A1.Ex4.m1.3.3.1.1.1.1.3">
                <times id="A1.Ex4.m1.3.3.1.1.1.1.3.1.cmml" xref="A1.Ex4.m1.3.3.1.1.1.1.3.1">
                </times>
                <ci id="A1.Ex4.m1.3.3.1.1.1.1.3.2.cmml" xref="A1.Ex4.m1.3.3.1.1.1.1.3.2">
                 𝑅
                </ci>
                <ci id="A1.Ex4.m1.3.3.1.1.1.1.3.3.cmml" xref="A1.Ex4.m1.3.3.1.1.1.1.3.3">
                 𝐿
                </ci>
               </apply>
              </apply>
             </apply>
             <apply id="A1.Ex4.m1.4.4.2.cmml" xref="A1.Ex4.m1.4.4.2">
              <minus id="A1.Ex4.m1.4.4.2.2.cmml" xref="A1.Ex4.m1.4.4.2">
              </minus>
              <apply id="A1.Ex4.m1.4.4.2.1.cmml" xref="A1.Ex4.m1.4.4.2.1">
               <times id="A1.Ex4.m1.4.4.2.1.2.cmml" xref="A1.Ex4.m1.4.4.2.1.2">
               </times>
               <apply id="A1.Ex4.m1.4.4.2.1.3.cmml" xref="A1.Ex4.m1.4.4.2.1.3">
                <csymbol cd="ambiguous" id="A1.Ex4.m1.4.4.2.1.3.1.cmml" xref="A1.Ex4.m1.4.4.2.1.3">
                 subscript
                </csymbol>
                <ci id="A1.Ex4.m1.4.4.2.1.3.2.cmml" xref="A1.Ex4.m1.4.4.2.1.3.2">
                 𝔼
                </ci>
                <apply id="A1.Ex4.m1.4.4.2.1.3.3.cmml" xref="A1.Ex4.m1.4.4.2.1.3.3">
                 <csymbol cd="ambiguous" id="A1.Ex4.m1.4.4.2.1.3.3.1.cmml" xref="A1.Ex4.m1.4.4.2.1.3.3">
                  subscript
                 </csymbol>
                 <ci id="A1.Ex4.m1.4.4.2.1.3.3.2.cmml" xref="A1.Ex4.m1.4.4.2.1.3.3.2">
                  𝜋
                 </ci>
                 <apply id="A1.Ex4.m1.4.4.2.1.3.3.3.cmml" xref="A1.Ex4.m1.4.4.2.1.3.3.3">
                  <times id="A1.Ex4.m1.4.4.2.1.3.3.3.1.cmml" xref="A1.Ex4.m1.4.4.2.1.3.3.3.1">
                  </times>
                  <ci id="A1.Ex4.m1.4.4.2.1.3.3.3.2.cmml" xref="A1.Ex4.m1.4.4.2.1.3.3.3.2">
                   𝑅
                  </ci>
                  <ci id="A1.Ex4.m1.4.4.2.1.3.3.3.3.cmml" xref="A1.Ex4.m1.4.4.2.1.3.3.3.3">
                   𝐿
                  </ci>
                 </apply>
                </apply>
               </apply>
               <apply id="A1.Ex4.m1.4.4.2.1.1.2.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1">
                <csymbol cd="latexml" id="A1.Ex4.m1.4.4.2.1.1.2.1.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.2">
                 delimited-[]
                </csymbol>
                <apply id="A1.Ex4.m1.4.4.2.1.1.1.1.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1">
                 <apply id="A1.Ex4.m1.4.4.2.1.1.1.1.2.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2">
                  <csymbol cd="ambiguous" id="A1.Ex4.m1.4.4.2.1.1.1.1.2.1.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2">
                   superscript
                  </csymbol>
                  <apply id="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2">
                   <csymbol cd="ambiguous" id="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.1.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2">
                    subscript
                   </csymbol>
                   <sum id="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.2.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.2">
                   </sum>
                   <apply id="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.3.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.3">
                    <eq id="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.3.1.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.3.1">
                    </eq>
                    <ci id="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.3.2.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.3.2">
                     𝑡
                    </ci>
                    <cn id="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.3.3.cmml" type="integer" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2.2.3.3">
                     1
                    </cn>
                   </apply>
                  </apply>
                  <ci id="A1.Ex4.m1.4.4.2.1.1.1.1.2.3.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.2.3">
                   𝑇
                  </ci>
                 </apply>
                 <apply id="A1.Ex4.m1.4.4.2.1.1.1.1.1.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1">
                  <times id="A1.Ex4.m1.4.4.2.1.1.1.1.1.2.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.2">
                  </times>
                  <apply id="A1.Ex4.m1.4.4.2.1.1.1.1.1.3.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.3">
                   <csymbol cd="ambiguous" id="A1.Ex4.m1.4.4.2.1.1.1.1.1.3.1.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.3">
                    superscript
                   </csymbol>
                   <ci id="A1.Ex4.m1.4.4.2.1.1.1.1.1.3.2.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.3.2">
                    𝛾
                   </ci>
                   <ci id="A1.Ex4.m1.4.4.2.1.1.1.1.1.3.3.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.3.3">
                    𝑡
                   </ci>
                  </apply>
                  <apply id="A1.Ex4.m1.4.4.2.1.1.1.1.1.4.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.4">
                   <ci id="A1.Ex4.m1.4.4.2.1.1.1.1.1.4.1.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.4.1">
                    ^
                   </ci>
                   <ci id="A1.Ex4.m1.4.4.2.1.1.1.1.1.4.2.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.4.2">
                    𝑟
                   </ci>
                  </apply>
                  <ci id="A1.Ex4.m1.1.1.cmml" xref="A1.Ex4.m1.1.1">
                   𝜏
                  </ci>
                  <apply id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6">
                   <log id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.1.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.1">
                   </log>
                   <apply id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2">
                    <csymbol cd="ambiguous" id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.1.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2">
                     subscript
                    </csymbol>
                    <ci id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.2.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.2">
                     𝜋
                    </ci>
                    <apply id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.3.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.3">
                     <times id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.3.1.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.3.1">
                     </times>
                     <ci id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.3.2.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.3.2">
                      𝑅
                     </ci>
                     <ci id="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.3.3.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.6.2.3.3">
                      𝐿
                     </ci>
                    </apply>
                   </apply>
                  </apply>
                  <apply id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1">
                   <csymbol cd="latexml" id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.2.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.2">
                    conditional
                   </csymbol>
                   <apply id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.3.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.3">
                    <csymbol cd="ambiguous" id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.3.1.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.3">
                     subscript
                    </csymbol>
                    <ci id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.3.2.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.3.2">
                     𝑎
                    </ci>
                    <ci id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.3.3.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.3.3">
                     𝑡
                    </ci>
                   </apply>
                   <list id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1">
                    <apply id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1.1">
                     <csymbol cd="ambiguous" id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1.1">
                      subscript
                     </csymbol>
                     <ci id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1.1.2">
                      𝑜
                     </ci>
                     <ci id="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="A1.Ex4.m1.4.4.2.1.1.1.1.1.1.1.1.1.1.1.3">
                      𝑡
                     </ci>
                    </apply>
                    <ci id="A1.Ex4.m1.2.2.cmml" xref="A1.Ex4.m1.2.2">
                     …
                    </ci>
                   </list>
                  </apply>
                 </apply>
                </apply>
               </apply>
              </apply>
             </apply>
            </apply>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A1.Ex4.m1.4c">
            \mathcal{L}_{RL}(\pi_{RL})=-\mathbb{E}_{\pi_{RL}}\left[\sum_{t=1}^{T}\gamma^{t}\hat{r}(\tau)\log\pi_{RL}(a_{t}|o_{t},...)\right]
           </annotation>
          </semantics>
         </math>
        </td>
        <td class="ltx_eqn_cell ltx_eqn_center_padright">
        </td>
       </tr>
      </tbody>
     </table>
    </div>
   </div>
  </figure>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   Bleeding Edge Game Human Data Collection
  </h2>
  <div class="ltx_para" id="A2.p1">
   <p class="ltx_p" id="A2.p1.1">
    Human gameplay data was recorded as part of the regular gameplay process, in order to enable in-game functionality as well as to support research. In game, recordings allow players to view their past games to improve their skills and for entertainment. Games were recorded on the servers that hosted the games in the form of replay files, which include a representation of the internal game state and controller actions of all players.
   </p>
  </div>
  <div class="ltx_para" id="A2.p2">
   <p class="ltx_p" id="A2.p2.1">
    Data collection was covered by an End User License Agreement (EULA) to which players agreed when logging in to play the game for the first time. Our use of the recorded human gameplay data for this specific research was governed by a data sharing agreement with the game studio, and approved by our institution’s IRB. To minimize risks regarding data privacy, any personally identifiable information (Xbox user ID) was removed when extracting the data used for this study from the original replays.
   </p>
  </div>
 </section>
 <section class="ltx_appendix" id="A3">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix C
   </span>
   Architectures and Training Details
  </h2>
  <section class="ltx_subsection" id="A3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     C.1
    </span>
    Base Model
   </h3>
   <div class="ltx_para" id="A3.SS1.p1">
    <p class="ltx_p" id="A3.SS1.p1.4">
     For our base model (
     <math alttext="\sim 103" class="ltx_Math" display="inline" id="A3.SS1.p1.1.m1.1">
      <semantics id="A3.SS1.p1.1.m1.1a">
       <mrow id="A3.SS1.p1.1.m1.1.1" xref="A3.SS1.p1.1.m1.1.1.cmml">
        <mi id="A3.SS1.p1.1.m1.1.1.2" xref="A3.SS1.p1.1.m1.1.1.2.cmml">
        </mi>
        <mo id="A3.SS1.p1.1.m1.1.1.1" xref="A3.SS1.p1.1.m1.1.1.1.cmml">
         ∼
        </mo>
        <mn id="A3.SS1.p1.1.m1.1.1.3" xref="A3.SS1.p1.1.m1.1.1.3.cmml">
         103
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p1.1.m1.1b">
        <apply id="A3.SS1.p1.1.m1.1.1.cmml" xref="A3.SS1.p1.1.m1.1.1">
         <csymbol cd="latexml" id="A3.SS1.p1.1.m1.1.1.1.cmml" xref="A3.SS1.p1.1.m1.1.1.1">
          similar-to
         </csymbol>
         <csymbol cd="latexml" id="A3.SS1.p1.1.m1.1.1.2.cmml" xref="A3.SS1.p1.1.m1.1.1.2">
          absent
         </csymbol>
         <cn id="A3.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="A3.SS1.p1.1.m1.1.1.3">
          103
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p1.1.m1.1c">
        \sim 103
       </annotation>
      </semantics>
     </math>
     M parameters) we use a GPT-2 causal transformer architecture with
     <math alttext="8" class="ltx_Math" display="inline" id="A3.SS1.p1.2.m2.1">
      <semantics id="A3.SS1.p1.2.m2.1a">
       <mn id="A3.SS1.p1.2.m2.1.1" xref="A3.SS1.p1.2.m2.1.1.cmml">
        8
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p1.2.m2.1b">
        <cn id="A3.SS1.p1.2.m2.1.1.cmml" type="integer" xref="A3.SS1.p1.2.m2.1.1">
         8
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p1.2.m2.1c">
        8
       </annotation>
      </semantics>
     </math>
     layers with
     <math alttext="1024" class="ltx_Math" display="inline" id="A3.SS1.p1.3.m3.1">
      <semantics id="A3.SS1.p1.3.m3.1a">
       <mn id="A3.SS1.p1.3.m3.1.1" xref="A3.SS1.p1.3.m3.1.1.cmml">
        1024
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p1.3.m3.1b">
        <cn id="A3.SS1.p1.3.m3.1.1.cmml" type="integer" xref="A3.SS1.p1.3.m3.1.1">
         1024
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p1.3.m3.1c">
        1024
       </annotation>
      </semantics>
     </math>
     hidden dim. Each attention layer has 8 heads, and the feedforward layers have a hidden dim of
     <math alttext="4096" class="ltx_Math" display="inline" id="A3.SS1.p1.4.m4.1">
      <semantics id="A3.SS1.p1.4.m4.1a">
       <mn id="A3.SS1.p1.4.m4.1.1" xref="A3.SS1.p1.4.m4.1.1.cmml">
        4096
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p1.4.m4.1b">
        <cn id="A3.SS1.p1.4.m4.1.1.cmml" type="integer" xref="A3.SS1.p1.4.m4.1.1">
         4096
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p1.4.m4.1c">
        4096
       </annotation>
      </semantics>
     </math>
     .
    </p>
   </div>
   <div class="ltx_para" id="A3.SS1.p2">
    <p class="ltx_p" id="A3.SS1.p2.3">
     Each image is resized to be of shape
     <math alttext="128\times 128\times 3" class="ltx_Math" display="inline" id="A3.SS1.p2.1.m1.1">
      <semantics id="A3.SS1.p2.1.m1.1a">
       <mrow id="A3.SS1.p2.1.m1.1.1" xref="A3.SS1.p2.1.m1.1.1.cmml">
        <mn id="A3.SS1.p2.1.m1.1.1.2" xref="A3.SS1.p2.1.m1.1.1.2.cmml">
         128
        </mn>
        <mo id="A3.SS1.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A3.SS1.p2.1.m1.1.1.1.cmml">
         ×
        </mo>
        <mn id="A3.SS1.p2.1.m1.1.1.3" xref="A3.SS1.p2.1.m1.1.1.3.cmml">
         128
        </mn>
        <mo id="A3.SS1.p2.1.m1.1.1.1a" lspace="0.222em" rspace="0.222em" xref="A3.SS1.p2.1.m1.1.1.1.cmml">
         ×
        </mo>
        <mn id="A3.SS1.p2.1.m1.1.1.4" xref="A3.SS1.p2.1.m1.1.1.4.cmml">
         3
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p2.1.m1.1b">
        <apply id="A3.SS1.p2.1.m1.1.1.cmml" xref="A3.SS1.p2.1.m1.1.1">
         <times id="A3.SS1.p2.1.m1.1.1.1.cmml" xref="A3.SS1.p2.1.m1.1.1.1">
         </times>
         <cn id="A3.SS1.p2.1.m1.1.1.2.cmml" type="integer" xref="A3.SS1.p2.1.m1.1.1.2">
          128
         </cn>
         <cn id="A3.SS1.p2.1.m1.1.1.3.cmml" type="integer" xref="A3.SS1.p2.1.m1.1.1.3">
          128
         </cn>
         <cn id="A3.SS1.p2.1.m1.1.1.4.cmml" type="integer" xref="A3.SS1.p2.1.m1.1.1.4">
          3
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p2.1.m1.1c">
        128\times 128\times 3
       </annotation>
      </semantics>
     </math>
     , divided by 255 to put its values in
     <math alttext="[0,1]" class="ltx_Math" display="inline" id="A3.SS1.p2.2.m2.2">
      <semantics id="A3.SS1.p2.2.m2.2a">
       <mrow id="A3.SS1.p2.2.m2.2.3.2" xref="A3.SS1.p2.2.m2.2.3.1.cmml">
        <mo id="A3.SS1.p2.2.m2.2.3.2.1" stretchy="false" xref="A3.SS1.p2.2.m2.2.3.1.cmml">
         [
        </mo>
        <mn id="A3.SS1.p2.2.m2.1.1" xref="A3.SS1.p2.2.m2.1.1.cmml">
         0
        </mn>
        <mo id="A3.SS1.p2.2.m2.2.3.2.2" xref="A3.SS1.p2.2.m2.2.3.1.cmml">
         ,
        </mo>
        <mn id="A3.SS1.p2.2.m2.2.2" xref="A3.SS1.p2.2.m2.2.2.cmml">
         1
        </mn>
        <mo id="A3.SS1.p2.2.m2.2.3.2.3" stretchy="false" xref="A3.SS1.p2.2.m2.2.3.1.cmml">
         ]
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p2.2.m2.2b">
        <interval closure="closed" id="A3.SS1.p2.2.m2.2.3.1.cmml" xref="A3.SS1.p2.2.m2.2.3.2">
         <cn id="A3.SS1.p2.2.m2.1.1.cmml" type="integer" xref="A3.SS1.p2.2.m2.1.1">
          0
         </cn>
         <cn id="A3.SS1.p2.2.m2.2.2.cmml" type="integer" xref="A3.SS1.p2.2.m2.2.2">
          1
         </cn>
        </interval>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p2.2.m2.2c">
        [0,1]
       </annotation>
      </semantics>
     </math>
     , and is then fed into a convolutional encoder to map it to a
     <math alttext="1024" class="ltx_Math" display="inline" id="A3.SS1.p2.3.m3.1">
      <semantics id="A3.SS1.p2.3.m3.1a">
       <mn id="A3.SS1.p2.3.m3.1.1" xref="A3.SS1.p2.3.m3.1.1.cmml">
        1024
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p2.3.m3.1b">
        <cn id="A3.SS1.p2.3.m3.1.1.cmml" type="integer" xref="A3.SS1.p2.3.m3.1.1">
         1024
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p2.3.m3.1c">
        1024
       </annotation>
      </semantics>
     </math>
     dimensional vector.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS1.p3">
    <p class="ltx_p" id="A3.SS1.p3.7">
     The first layer of the conv net has kernels of shape
     <math alttext="8\times 8" class="ltx_Math" display="inline" id="A3.SS1.p3.1.m1.1">
      <semantics id="A3.SS1.p3.1.m1.1a">
       <mrow id="A3.SS1.p3.1.m1.1.1" xref="A3.SS1.p3.1.m1.1.1.cmml">
        <mn id="A3.SS1.p3.1.m1.1.1.2" xref="A3.SS1.p3.1.m1.1.1.2.cmml">
         8
        </mn>
        <mo id="A3.SS1.p3.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A3.SS1.p3.1.m1.1.1.1.cmml">
         ×
        </mo>
        <mn id="A3.SS1.p3.1.m1.1.1.3" xref="A3.SS1.p3.1.m1.1.1.3.cmml">
         8
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p3.1.m1.1b">
        <apply id="A3.SS1.p3.1.m1.1.1.cmml" xref="A3.SS1.p3.1.m1.1.1">
         <times id="A3.SS1.p3.1.m1.1.1.1.cmml" xref="A3.SS1.p3.1.m1.1.1.1">
         </times>
         <cn id="A3.SS1.p3.1.m1.1.1.2.cmml" type="integer" xref="A3.SS1.p3.1.m1.1.1.2">
          8
         </cn>
         <cn id="A3.SS1.p3.1.m1.1.1.3.cmml" type="integer" xref="A3.SS1.p3.1.m1.1.1.3">
          8
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p3.1.m1.1c">
        8\times 8
       </annotation>
      </semantics>
     </math>
     , with a stride of
     <math alttext="4" class="ltx_Math" display="inline" id="A3.SS1.p3.2.m2.1">
      <semantics id="A3.SS1.p3.2.m2.1a">
       <mn id="A3.SS1.p3.2.m2.1.1" xref="A3.SS1.p3.2.m2.1.1.cmml">
        4
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p3.2.m2.1b">
        <cn id="A3.SS1.p3.2.m2.1.1.cmml" type="integer" xref="A3.SS1.p3.2.m2.1.1">
         4
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p3.2.m2.1c">
        4
       </annotation>
      </semantics>
     </math>
     , and a padding of
     <math alttext="3" class="ltx_Math" display="inline" id="A3.SS1.p3.3.m3.1">
      <semantics id="A3.SS1.p3.3.m3.1a">
       <mn id="A3.SS1.p3.3.m3.1.1" xref="A3.SS1.p3.3.m3.1.1.cmml">
        3
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p3.3.m3.1b">
        <cn id="A3.SS1.p3.3.m3.1.1.cmml" type="integer" xref="A3.SS1.p3.3.m3.1.1">
         3
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p3.3.m3.1c">
        3
       </annotation>
      </semantics>
     </math>
     and maps to
     <math alttext="16" class="ltx_Math" display="inline" id="A3.SS1.p3.4.m4.1">
      <semantics id="A3.SS1.p3.4.m4.1a">
       <mn id="A3.SS1.p3.4.m4.1.1" xref="A3.SS1.p3.4.m4.1.1.cmml">
        16
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p3.4.m4.1b">
        <cn id="A3.SS1.p3.4.m4.1.1.cmml" type="integer" xref="A3.SS1.p3.4.m4.1.1">
         16
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p3.4.m4.1c">
        16
       </annotation>
      </semantics>
     </math>
     channel dimension.
This is followed by 4 lots of ConvNext
     <cite class="ltx_cite ltx_citemacro_citep">
      (Liu et al.,
      <a class="ltx_ref" href="#bib.bib34" title="">
       2022
      </a>
      )
     </cite>
     and downsampling blocks (kernel of shape
     <math alttext="3\times 3" class="ltx_Math" display="inline" id="A3.SS1.p3.5.m5.1">
      <semantics id="A3.SS1.p3.5.m5.1a">
       <mrow id="A3.SS1.p3.5.m5.1.1" xref="A3.SS1.p3.5.m5.1.1.cmml">
        <mn id="A3.SS1.p3.5.m5.1.1.2" xref="A3.SS1.p3.5.m5.1.1.2.cmml">
         3
        </mn>
        <mo id="A3.SS1.p3.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="A3.SS1.p3.5.m5.1.1.1.cmml">
         ×
        </mo>
        <mn id="A3.SS1.p3.5.m5.1.1.3" xref="A3.SS1.p3.5.m5.1.1.3.cmml">
         3
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p3.5.m5.1b">
        <apply id="A3.SS1.p3.5.m5.1.1.cmml" xref="A3.SS1.p3.5.m5.1.1">
         <times id="A3.SS1.p3.5.m5.1.1.1.cmml" xref="A3.SS1.p3.5.m5.1.1.1">
         </times>
         <cn id="A3.SS1.p3.5.m5.1.1.2.cmml" type="integer" xref="A3.SS1.p3.5.m5.1.1.2">
          3
         </cn>
         <cn id="A3.SS1.p3.5.m5.1.1.3.cmml" type="integer" xref="A3.SS1.p3.5.m5.1.1.3">
          3
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p3.5.m5.1c">
        3\times 3
       </annotation>
      </semantics>
     </math>
     , stride of
     <math alttext="2" class="ltx_Math" display="inline" id="A3.SS1.p3.6.m6.1">
      <semantics id="A3.SS1.p3.6.m6.1a">
       <mn id="A3.SS1.p3.6.m6.1.1" xref="A3.SS1.p3.6.m6.1.1.cmml">
        2
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p3.6.m6.1b">
        <cn id="A3.SS1.p3.6.m6.1.1.cmml" type="integer" xref="A3.SS1.p3.6.m6.1.1">
         2
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p3.6.m6.1c">
        2
       </annotation>
      </semantics>
     </math>
     , padding of
     <math alttext="1" class="ltx_Math" display="inline" id="A3.SS1.p3.7.m7.1">
      <semantics id="A3.SS1.p3.7.m7.1a">
       <mn id="A3.SS1.p3.7.m7.1.1" xref="A3.SS1.p3.7.m7.1.1.cmml">
        1
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p3.7.m7.1b">
        <cn id="A3.SS1.p3.7.m7.1.1.cmml" type="integer" xref="A3.SS1.p3.7.m7.1.1">
         1
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p3.7.m7.1c">
        1
       </annotation>
      </semantics>
     </math>
     , doubling the channel dimension).
Finally, a LayerNorm
     <cite class="ltx_cite ltx_citemacro_citep">
      (Ba et al.,
      <a class="ltx_ref" href="#bib.bib5" title="">
       2016
      </a>
      )
     </cite>
     is applied to the output.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS1.p4">
    <p class="ltx_p" id="A3.SS1.p4.1">
     The transformer operates on sequences of 32 timesteps using learnt positional encodings. The output of the transformer is layernormed, and then fed into an MLP with a single hidden layer of
     <math alttext="1024" class="ltx_Math" display="inline" id="A3.SS1.p4.1.m1.1">
      <semantics id="A3.SS1.p4.1.m1.1a">
       <mn id="A3.SS1.p4.1.m1.1.1" xref="A3.SS1.p4.1.m1.1.1.cmml">
        1024
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p4.1.m1.1b">
        <cn id="A3.SS1.p4.1.m1.1.1.cmml" type="integer" xref="A3.SS1.p4.1.m1.1.1">
         1024
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p4.1.m1.1c">
        1024
       </annotation>
      </semantics>
     </math>
     dimensions with a GELU
     <cite class="ltx_cite ltx_citemacro_citep">
      (Hendrycks &amp; Gimpel,
      <a class="ltx_ref" href="#bib.bib23" title="">
       2023
      </a>
      )
     </cite>
     non-linearity.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS1.p5">
    <p class="ltx_p" id="A3.SS1.p5.3">
     For our optimiser we use AdamW
     <cite class="ltx_cite ltx_citemacro_citep">
      (Loshchilov &amp; Hutter,
      <a class="ltx_ref" href="#bib.bib35" title="">
       2019
      </a>
      )
     </cite>
     with a learning rate of
     <math alttext="1e-4" class="ltx_Math" display="inline" id="A3.SS1.p5.1.m1.1">
      <semantics id="A3.SS1.p5.1.m1.1a">
       <mrow id="A3.SS1.p5.1.m1.1.1" xref="A3.SS1.p5.1.m1.1.1.cmml">
        <mrow id="A3.SS1.p5.1.m1.1.1.2" xref="A3.SS1.p5.1.m1.1.1.2.cmml">
         <mn id="A3.SS1.p5.1.m1.1.1.2.2" xref="A3.SS1.p5.1.m1.1.1.2.2.cmml">
          1
         </mn>
         <mo id="A3.SS1.p5.1.m1.1.1.2.1" lspace="0em" rspace="0em" xref="A3.SS1.p5.1.m1.1.1.2.1.cmml">
          ​
         </mo>
         <mi id="A3.SS1.p5.1.m1.1.1.2.3" xref="A3.SS1.p5.1.m1.1.1.2.3.cmml">
          e
         </mi>
        </mrow>
        <mo id="A3.SS1.p5.1.m1.1.1.1" xref="A3.SS1.p5.1.m1.1.1.1.cmml">
         −
        </mo>
        <mn id="A3.SS1.p5.1.m1.1.1.3" xref="A3.SS1.p5.1.m1.1.1.3.cmml">
         4
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p5.1.m1.1b">
        <apply id="A3.SS1.p5.1.m1.1.1.cmml" xref="A3.SS1.p5.1.m1.1.1">
         <minus id="A3.SS1.p5.1.m1.1.1.1.cmml" xref="A3.SS1.p5.1.m1.1.1.1">
         </minus>
         <apply id="A3.SS1.p5.1.m1.1.1.2.cmml" xref="A3.SS1.p5.1.m1.1.1.2">
          <times id="A3.SS1.p5.1.m1.1.1.2.1.cmml" xref="A3.SS1.p5.1.m1.1.1.2.1">
          </times>
          <cn id="A3.SS1.p5.1.m1.1.1.2.2.cmml" type="integer" xref="A3.SS1.p5.1.m1.1.1.2.2">
           1
          </cn>
          <ci id="A3.SS1.p5.1.m1.1.1.2.3.cmml" xref="A3.SS1.p5.1.m1.1.1.2.3">
           𝑒
          </ci>
         </apply>
         <cn id="A3.SS1.p5.1.m1.1.1.3.cmml" type="integer" xref="A3.SS1.p5.1.m1.1.1.3">
          4
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p5.1.m1.1c">
        1e-4
       </annotation>
      </semantics>
     </math>
     and a weight decay of
     <math alttext="1e-4" class="ltx_Math" display="inline" id="A3.SS1.p5.2.m2.1">
      <semantics id="A3.SS1.p5.2.m2.1a">
       <mrow id="A3.SS1.p5.2.m2.1.1" xref="A3.SS1.p5.2.m2.1.1.cmml">
        <mrow id="A3.SS1.p5.2.m2.1.1.2" xref="A3.SS1.p5.2.m2.1.1.2.cmml">
         <mn id="A3.SS1.p5.2.m2.1.1.2.2" xref="A3.SS1.p5.2.m2.1.1.2.2.cmml">
          1
         </mn>
         <mo id="A3.SS1.p5.2.m2.1.1.2.1" lspace="0em" rspace="0em" xref="A3.SS1.p5.2.m2.1.1.2.1.cmml">
          ​
         </mo>
         <mi id="A3.SS1.p5.2.m2.1.1.2.3" xref="A3.SS1.p5.2.m2.1.1.2.3.cmml">
          e
         </mi>
        </mrow>
        <mo id="A3.SS1.p5.2.m2.1.1.1" xref="A3.SS1.p5.2.m2.1.1.1.cmml">
         −
        </mo>
        <mn id="A3.SS1.p5.2.m2.1.1.3" xref="A3.SS1.p5.2.m2.1.1.3.cmml">
         4
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p5.2.m2.1b">
        <apply id="A3.SS1.p5.2.m2.1.1.cmml" xref="A3.SS1.p5.2.m2.1.1">
         <minus id="A3.SS1.p5.2.m2.1.1.1.cmml" xref="A3.SS1.p5.2.m2.1.1.1">
         </minus>
         <apply id="A3.SS1.p5.2.m2.1.1.2.cmml" xref="A3.SS1.p5.2.m2.1.1.2">
          <times id="A3.SS1.p5.2.m2.1.1.2.1.cmml" xref="A3.SS1.p5.2.m2.1.1.2.1">
          </times>
          <cn id="A3.SS1.p5.2.m2.1.1.2.2.cmml" type="integer" xref="A3.SS1.p5.2.m2.1.1.2.2">
           1
          </cn>
          <ci id="A3.SS1.p5.2.m2.1.1.2.3.cmml" xref="A3.SS1.p5.2.m2.1.1.2.3">
           𝑒
          </ci>
         </apply>
         <cn id="A3.SS1.p5.2.m2.1.1.3.cmml" type="integer" xref="A3.SS1.p5.2.m2.1.1.3">
          4
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p5.2.m2.1c">
        1e-4
       </annotation>
      </semantics>
     </math>
     .
We use a batch size of 256 with a learning rate warmup period of 1000 updates and a gradient clipping value of
     <math alttext="1" class="ltx_Math" display="inline" id="A3.SS1.p5.3.m3.1">
      <semantics id="A3.SS1.p5.3.m3.1a">
       <mn id="A3.SS1.p5.3.m3.1.1" xref="A3.SS1.p5.3.m3.1.1.cmml">
        1
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS1.p5.3.m3.1b">
        <cn id="A3.SS1.p5.3.m3.1.1.cmml" type="integer" xref="A3.SS1.p5.3.m3.1.1">
         1
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS1.p5.3.m3.1c">
        1
       </annotation>
      </semantics>
     </math>
     . We train with the same image augmentations as used by
     <cite class="ltx_cite ltx_citemacro_citep">
      (Baker et al.,
      <a class="ltx_ref" href="#bib.bib7" title="">
       2022
      </a>
      )
     </cite>
     , and filter out all no-op actions.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS1.p6">
    <p class="ltx_p" id="A3.SS1.p6.1">
     We used 8 32GB Nvidia V100 GPUs for approximately 4 days to train the base model.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     C.2
    </span>
    Fine-Tuning of Base Model
   </h3>
   <div class="ltx_para" id="A3.SS2.p1">
    <p class="ltx_p" id="A3.SS2.p1.2">
     We train for 1500 batches of size 128 with a learning rate of
     <math alttext="1e-6" class="ltx_Math" display="inline" id="A3.SS2.p1.1.m1.1">
      <semantics id="A3.SS2.p1.1.m1.1a">
       <mrow id="A3.SS2.p1.1.m1.1.1" xref="A3.SS2.p1.1.m1.1.1.cmml">
        <mrow id="A3.SS2.p1.1.m1.1.1.2" xref="A3.SS2.p1.1.m1.1.1.2.cmml">
         <mn id="A3.SS2.p1.1.m1.1.1.2.2" xref="A3.SS2.p1.1.m1.1.1.2.2.cmml">
          1
         </mn>
         <mo id="A3.SS2.p1.1.m1.1.1.2.1" lspace="0em" rspace="0em" xref="A3.SS2.p1.1.m1.1.1.2.1.cmml">
          ​
         </mo>
         <mi id="A3.SS2.p1.1.m1.1.1.2.3" xref="A3.SS2.p1.1.m1.1.1.2.3.cmml">
          e
         </mi>
        </mrow>
        <mo id="A3.SS2.p1.1.m1.1.1.1" xref="A3.SS2.p1.1.m1.1.1.1.cmml">
         −
        </mo>
        <mn id="A3.SS2.p1.1.m1.1.1.3" xref="A3.SS2.p1.1.m1.1.1.3.cmml">
         6
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A3.SS2.p1.1.m1.1b">
        <apply id="A3.SS2.p1.1.m1.1.1.cmml" xref="A3.SS2.p1.1.m1.1.1">
         <minus id="A3.SS2.p1.1.m1.1.1.1.cmml" xref="A3.SS2.p1.1.m1.1.1.1">
         </minus>
         <apply id="A3.SS2.p1.1.m1.1.1.2.cmml" xref="A3.SS2.p1.1.m1.1.1.2">
          <times id="A3.SS2.p1.1.m1.1.1.2.1.cmml" xref="A3.SS2.p1.1.m1.1.1.2.1">
          </times>
          <cn id="A3.SS2.p1.1.m1.1.1.2.2.cmml" type="integer" xref="A3.SS2.p1.1.m1.1.1.2.2">
           1
          </cn>
          <ci id="A3.SS2.p1.1.m1.1.1.2.3.cmml" xref="A3.SS2.p1.1.m1.1.1.2.3">
           𝑒
          </ci>
         </apply>
         <cn id="A3.SS2.p1.1.m1.1.1.3.cmml" type="integer" xref="A3.SS2.p1.1.m1.1.1.3">
          6
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS2.p1.1.m1.1c">
        1e-6
       </annotation>
      </semantics>
     </math>
     with the same image augmentations and no-op filtering as for pre-training with
     <math alttext="200" class="ltx_Math" display="inline" id="A3.SS2.p1.2.m2.1">
      <semantics id="A3.SS2.p1.2.m2.1a">
       <mn id="A3.SS2.p1.2.m2.1.1" xref="A3.SS2.p1.2.m2.1.1.cmml">
        200
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS2.p1.2.m2.1b">
        <cn id="A3.SS2.p1.2.m2.1.1.cmml" type="integer" xref="A3.SS2.p1.2.m2.1.1">
         200
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS2.p1.2.m2.1c">
        200
       </annotation>
      </semantics>
     </math>
     warmup steps.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS2.p2">
    <p class="ltx_p" id="A3.SS2.p2.1">
     We used 4 48GB Nvidia A6000 GPUs for less than 1 hour for fine-tuning.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     C.3
    </span>
    Reward Models
   </h3>
   <div class="ltx_para" id="A3.SS3.p1">
    <p class="ltx_p" id="A3.SS3.p1.1">
     Each trajectory is padded up to the maximum length of 100 (with black images) before being fed into the reward models.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS3.p2">
    <p class="ltx_p" id="A3.SS3.p2.1">
     For the
     <span class="ltx_text ltx_font_typewriter" id="A3.SS3.p2.1.1">
      Random Encoder
     </span>
     model we randomly initialise a linear layer to randomly project the flattened values of the image to a
     <math alttext="512" class="ltx_Math" display="inline" id="A3.SS3.p2.1.m1.1">
      <semantics id="A3.SS3.p2.1.m1.1a">
       <mn id="A3.SS3.p2.1.m1.1.1" xref="A3.SS3.p2.1.m1.1.1.cmml">
        512
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p2.1.m1.1b">
        <cn id="A3.SS3.p2.1.m1.1.1.cmml" type="integer" xref="A3.SS3.p2.1.m1.1.1">
         512
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS3.p2.1.m1.1c">
        512
       </annotation>
      </semantics>
     </math>
     dimensional vector. This linear layer is not trained.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS3.p3">
    <p class="ltx_p" id="A3.SS3.p3.3">
     For the
     <span class="ltx_text ltx_font_typewriter" id="A3.SS3.p3.3.1">
      Agent Encoder
     </span>
     model, we feed the trajectory into the fine-tuned agent and take the layernormed output of the transformer, corresponding to timesteps
     <math alttext="0" class="ltx_Math" display="inline" id="A3.SS3.p3.1.m1.1">
      <semantics id="A3.SS3.p3.1.m1.1a">
       <mn id="A3.SS3.p3.1.m1.1.1" xref="A3.SS3.p3.1.m1.1.1.cmml">
        0
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p3.1.m1.1b">
        <cn id="A3.SS3.p3.1.m1.1.1.cmml" type="integer" xref="A3.SS3.p3.1.m1.1.1">
         0
        </cn>
       </annotation-xml>
      </semantics>
     </math>
     up to
     <math alttext="100" class="ltx_Math" display="inline" id="A3.SS3.p3.2.m2.1">
      <semantics id="A3.SS3.p3.2.m2.1a">
       <mn id="A3.SS3.p3.2.m2.1.1" xref="A3.SS3.p3.2.m2.1.1.cmml">
        100
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p3.2.m2.1b">
        <cn id="A3.SS3.p3.2.m2.1.1.cmml" type="integer" xref="A3.SS3.p3.2.m2.1.1">
         100
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS3.p3.2.m2.1c">
        100
       </annotation>
      </semantics>
     </math>
     , as
     <math alttext="1024" class="ltx_Math" display="inline" id="A3.SS3.p3.3.m3.1">
      <semantics id="A3.SS3.p3.3.m3.1a">
       <mn id="A3.SS3.p3.3.m3.1.1" xref="A3.SS3.p3.3.m3.1.1.cmml">
        1024
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p3.3.m3.1b">
        <cn id="A3.SS3.p3.3.m3.1.1.cmml" type="integer" xref="A3.SS3.p3.3.m3.1.1">
         1024
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS3.p3.3.m3.1c">
        1024
       </annotation>
      </semantics>
     </math>
     dimensional embeddings. This is larger since it must capture the 32 context steps rather than just a single observation. The parameters of the fine-tuned agent are not trained.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS3.p4">
    <p class="ltx_p" id="A3.SS3.p4.4">
     For both models we then feed these vectors into an MLP with a GELU non-linearity and a hidden layer of
     <math alttext="256" class="ltx_Math" display="inline" id="A3.SS3.p4.1.m1.1">
      <semantics id="A3.SS3.p4.1.m1.1a">
       <mn id="A3.SS3.p4.1.m1.1.1" xref="A3.SS3.p4.1.m1.1.1.cmml">
        256
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p4.1.m1.1b">
        <cn id="A3.SS3.p4.1.m1.1.1.cmml" type="integer" xref="A3.SS3.p4.1.m1.1.1">
         256
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS3.p4.1.m1.1c">
        256
       </annotation>
      </semantics>
     </math>
     and and output dimension of
     <math alttext="3" class="ltx_Math" display="inline" id="A3.SS3.p4.2.m2.1">
      <semantics id="A3.SS3.p4.2.m2.1a">
       <mn id="A3.SS3.p4.2.m2.1.1" xref="A3.SS3.p4.2.m2.1.1.cmml">
        3
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p4.2.m2.1b">
        <cn id="A3.SS3.p4.2.m2.1.1.cmml" type="integer" xref="A3.SS3.p4.2.m2.1.1">
         3
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS3.p4.2.m2.1c">
        3
       </annotation>
      </semantics>
     </math>
     .
Each of the 100 3-dimensional vectors are concatenated together and then fed into another MLP with a GELU,
     <math alttext="256" class="ltx_Math" display="inline" id="A3.SS3.p4.3.m3.1">
      <semantics id="A3.SS3.p4.3.m3.1a">
       <mn id="A3.SS3.p4.3.m3.1.1" xref="A3.SS3.p4.3.m3.1.1.cmml">
        256
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p4.3.m3.1b">
        <cn id="A3.SS3.p4.3.m3.1.1.cmml" type="integer" xref="A3.SS3.p4.3.m3.1.1">
         256
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS3.p4.3.m3.1c">
        256
       </annotation>
      </semantics>
     </math>
     hidden dimension, and an output of
     <math alttext="1" class="ltx_Math" display="inline" id="A3.SS3.p4.4.m4.1">
      <semantics id="A3.SS3.p4.4.m4.1a">
       <mn id="A3.SS3.p4.4.m4.1.1" xref="A3.SS3.p4.4.m4.1.1.cmml">
        1
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p4.4.m4.1b">
        <cn id="A3.SS3.p4.4.m4.1.1.cmml" type="integer" xref="A3.SS3.p4.4.m4.1.1">
         1
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS3.p4.4.m4.1c">
        1
       </annotation>
      </semantics>
     </math>
     .
    </p>
   </div>
   <div class="ltx_para" id="A3.SS3.p5">
    <p class="ltx_p" id="A3.SS3.p5.7">
     To train the reward models we use a minibatch of size
     <math alttext="2048" class="ltx_Math" display="inline" id="A3.SS3.p5.1.m1.1">
      <semantics id="A3.SS3.p5.1.m1.1a">
       <mn id="A3.SS3.p5.1.m1.1.1" xref="A3.SS3.p5.1.m1.1.1.cmml">
        2048
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p5.1.m1.1b">
        <cn id="A3.SS3.p5.1.m1.1.1.cmml" type="integer" xref="A3.SS3.p5.1.m1.1.1">
         2048
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS3.p5.1.m1.1c">
        2048
       </annotation>
      </semantics>
     </math>
     , learning rate of
     <math alttext="1e-4" class="ltx_Math" display="inline" id="A3.SS3.p5.2.m2.1">
      <semantics id="A3.SS3.p5.2.m2.1a">
       <mrow id="A3.SS3.p5.2.m2.1.1" xref="A3.SS3.p5.2.m2.1.1.cmml">
        <mrow id="A3.SS3.p5.2.m2.1.1.2" xref="A3.SS3.p5.2.m2.1.1.2.cmml">
         <mn id="A3.SS3.p5.2.m2.1.1.2.2" xref="A3.SS3.p5.2.m2.1.1.2.2.cmml">
          1
         </mn>
         <mo id="A3.SS3.p5.2.m2.1.1.2.1" lspace="0em" rspace="0em" xref="A3.SS3.p5.2.m2.1.1.2.1.cmml">
          ​
         </mo>
         <mi id="A3.SS3.p5.2.m2.1.1.2.3" xref="A3.SS3.p5.2.m2.1.1.2.3.cmml">
          e
         </mi>
        </mrow>
        <mo id="A3.SS3.p5.2.m2.1.1.1" xref="A3.SS3.p5.2.m2.1.1.1.cmml">
         −
        </mo>
        <mn id="A3.SS3.p5.2.m2.1.1.3" xref="A3.SS3.p5.2.m2.1.1.3.cmml">
         4
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p5.2.m2.1b">
        <apply id="A3.SS3.p5.2.m2.1.1.cmml" xref="A3.SS3.p5.2.m2.1.1">
         <minus id="A3.SS3.p5.2.m2.1.1.1.cmml" xref="A3.SS3.p5.2.m2.1.1.1">
         </minus>
         <apply id="A3.SS3.p5.2.m2.1.1.2.cmml" xref="A3.SS3.p5.2.m2.1.1.2">
          <times id="A3.SS3.p5.2.m2.1.1.2.1.cmml" xref="A3.SS3.p5.2.m2.1.1.2.1">
          </times>
          <cn id="A3.SS3.p5.2.m2.1.1.2.2.cmml" type="integer" xref="A3.SS3.p5.2.m2.1.1.2.2">
           1
          </cn>
          <ci id="A3.SS3.p5.2.m2.1.1.2.3.cmml" xref="A3.SS3.p5.2.m2.1.1.2.3">
           𝑒
          </ci>
         </apply>
         <cn id="A3.SS3.p5.2.m2.1.1.3.cmml" type="integer" xref="A3.SS3.p5.2.m2.1.1.3">
          4
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS3.p5.2.m2.1c">
        1e-4
       </annotation>
      </semantics>
     </math>
     , and an
     <math alttext="L_{2}" class="ltx_Math" display="inline" id="A3.SS3.p5.3.m3.1">
      <semantics id="A3.SS3.p5.3.m3.1a">
       <msub id="A3.SS3.p5.3.m3.1.1" xref="A3.SS3.p5.3.m3.1.1.cmml">
        <mi id="A3.SS3.p5.3.m3.1.1.2" xref="A3.SS3.p5.3.m3.1.1.2.cmml">
         L
        </mi>
        <mn id="A3.SS3.p5.3.m3.1.1.3" xref="A3.SS3.p5.3.m3.1.1.3.cmml">
         2
        </mn>
       </msub>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p5.3.m3.1b">
        <apply id="A3.SS3.p5.3.m3.1.1.cmml" xref="A3.SS3.p5.3.m3.1.1">
         <csymbol cd="ambiguous" id="A3.SS3.p5.3.m3.1.1.1.cmml" xref="A3.SS3.p5.3.m3.1.1">
          subscript
         </csymbol>
         <ci id="A3.SS3.p5.3.m3.1.1.2.cmml" xref="A3.SS3.p5.3.m3.1.1.2">
          𝐿
         </ci>
         <cn id="A3.SS3.p5.3.m3.1.1.3.cmml" type="integer" xref="A3.SS3.p5.3.m3.1.1.3">
          2
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS3.p5.3.m3.1c">
        L_{2}
       </annotation>
      </semantics>
     </math>
     regularisation penalty of
     <math alttext="0.1" class="ltx_Math" display="inline" id="A3.SS3.p5.4.m4.1">
      <semantics id="A3.SS3.p5.4.m4.1a">
       <mn id="A3.SS3.p5.4.m4.1.1" xref="A3.SS3.p5.4.m4.1.1.cmml">
        0.1
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p5.4.m4.1b">
        <cn id="A3.SS3.p5.4.m4.1.1.cmml" type="float" xref="A3.SS3.p5.4.m4.1.1">
         0.1
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS3.p5.4.m4.1c">
        0.1
       </annotation>
      </semantics>
     </math>
     .
We train all models for
     <math alttext="200" class="ltx_Math" display="inline" id="A3.SS3.p5.5.m5.1">
      <semantics id="A3.SS3.p5.5.m5.1a">
       <mn id="A3.SS3.p5.5.m5.1.1" xref="A3.SS3.p5.5.m5.1.1.cmml">
        200
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p5.5.m5.1b">
        <cn id="A3.SS3.p5.5.m5.1.1.cmml" type="integer" xref="A3.SS3.p5.5.m5.1.1">
         200
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS3.p5.5.m5.1c">
        200
       </annotation>
      </semantics>
     </math>
     epochs, except for the largest training set size of
     <math alttext="1000" class="ltx_Math" display="inline" id="A3.SS3.p5.6.m6.1">
      <semantics id="A3.SS3.p5.6.m6.1a">
       <mn id="A3.SS3.p5.6.m6.1.1" xref="A3.SS3.p5.6.m6.1.1.cmml">
        1000
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p5.6.m6.1b">
        <cn id="A3.SS3.p5.6.m6.1.1.cmml" type="integer" xref="A3.SS3.p5.6.m6.1.1">
         1000
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS3.p5.6.m6.1c">
        1000
       </annotation>
      </semantics>
     </math>
     trajectories which we train for
     <math alttext="50" class="ltx_Math" display="inline" id="A3.SS3.p5.7.m7.1">
      <semantics id="A3.SS3.p5.7.m7.1a">
       <mn id="A3.SS3.p5.7.m7.1.1" xref="A3.SS3.p5.7.m7.1.1.cmml">
        50
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p5.7.m7.1b">
        <cn id="A3.SS3.p5.7.m7.1.1.cmml" type="integer" xref="A3.SS3.p5.7.m7.1.1">
         50
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS3.p5.7.m7.1c">
        50
       </annotation>
      </semantics>
     </math>
     epochs.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS3.p6">
    <p class="ltx_p" id="A3.SS3.p6.1">
     After training, we compute the minimum and maximum outputs of the reward model on the training set.
These are then used to normalise the output of the reward model to lie within
     <math alttext="[0,1]" class="ltx_Math" display="inline" id="A3.SS3.p6.1.m1.2">
      <semantics id="A3.SS3.p6.1.m1.2a">
       <mrow id="A3.SS3.p6.1.m1.2.3.2" xref="A3.SS3.p6.1.m1.2.3.1.cmml">
        <mo id="A3.SS3.p6.1.m1.2.3.2.1" stretchy="false" xref="A3.SS3.p6.1.m1.2.3.1.cmml">
         [
        </mo>
        <mn id="A3.SS3.p6.1.m1.1.1" xref="A3.SS3.p6.1.m1.1.1.cmml">
         0
        </mn>
        <mo id="A3.SS3.p6.1.m1.2.3.2.2" xref="A3.SS3.p6.1.m1.2.3.1.cmml">
         ,
        </mo>
        <mn id="A3.SS3.p6.1.m1.2.2" xref="A3.SS3.p6.1.m1.2.2.cmml">
         1
        </mn>
        <mo id="A3.SS3.p6.1.m1.2.3.2.3" stretchy="false" xref="A3.SS3.p6.1.m1.2.3.1.cmml">
         ]
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p6.1.m1.2b">
        <interval closure="closed" id="A3.SS3.p6.1.m1.2.3.1.cmml" xref="A3.SS3.p6.1.m1.2.3.2">
         <cn id="A3.SS3.p6.1.m1.1.1.cmml" type="integer" xref="A3.SS3.p6.1.m1.1.1">
          0
         </cn>
         <cn id="A3.SS3.p6.1.m1.2.2.cmml" type="integer" xref="A3.SS3.p6.1.m1.2.2">
          1
         </cn>
        </interval>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS3.p6.1.m1.2c">
        [0,1]
       </annotation>
      </semantics>
     </math>
     .
    </p>
   </div>
   <div class="ltx_para" id="A3.SS3.p7">
    <p class="ltx_p" id="A3.SS3.p7.1">
     We used 4 48GB Nvidia A6000 GPUs for a few hours in total for training all reward models.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A3.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     C.4
    </span>
    Alignment Training
   </h3>
   <div class="ltx_para" id="A3.SS4.p1">
    <p class="ltx_p" id="A3.SS4.p1.4">
     <span class="ltx_text ltx_font_bold" id="A3.SS4.p1.4.1">
      Preference Fine-Tuning:
     </span>
     after training the reward model on its dataset of
     <math alttext="M" class="ltx_Math" display="inline" id="A3.SS4.p1.1.m1.1">
      <semantics id="A3.SS4.p1.1.m1.1a">
       <mi id="A3.SS4.p1.1.m1.1.1" xref="A3.SS4.p1.1.m1.1.1.cmml">
        M
       </mi>
       <annotation-xml encoding="MathML-Content" id="A3.SS4.p1.1.m1.1b">
        <ci id="A3.SS4.p1.1.m1.1.1.cmml" xref="A3.SS4.p1.1.m1.1.1">
         𝑀
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS4.p1.1.m1.1c">
        M
       </annotation>
      </semantics>
     </math>
     trajectories (which result in a dataset of up to
     <math alttext="N=(M)(M-1)/2" class="ltx_Math" display="inline" id="A3.SS4.p1.2.m2.2">
      <semantics id="A3.SS4.p1.2.m2.2a">
       <mrow id="A3.SS4.p1.2.m2.2.2" xref="A3.SS4.p1.2.m2.2.2.cmml">
        <mi id="A3.SS4.p1.2.m2.2.2.3" xref="A3.SS4.p1.2.m2.2.2.3.cmml">
         N
        </mi>
        <mo id="A3.SS4.p1.2.m2.2.2.2" xref="A3.SS4.p1.2.m2.2.2.2.cmml">
         =
        </mo>
        <mrow id="A3.SS4.p1.2.m2.2.2.1" xref="A3.SS4.p1.2.m2.2.2.1.cmml">
         <mrow id="A3.SS4.p1.2.m2.2.2.1.1" xref="A3.SS4.p1.2.m2.2.2.1.1.cmml">
          <mrow id="A3.SS4.p1.2.m2.2.2.1.1.3.2" xref="A3.SS4.p1.2.m2.2.2.1.1.cmml">
           <mo id="A3.SS4.p1.2.m2.2.2.1.1.3.2.1" stretchy="false" xref="A3.SS4.p1.2.m2.2.2.1.1.cmml">
            (
           </mo>
           <mi id="A3.SS4.p1.2.m2.1.1" xref="A3.SS4.p1.2.m2.1.1.cmml">
            M
           </mi>
           <mo id="A3.SS4.p1.2.m2.2.2.1.1.3.2.2" stretchy="false" xref="A3.SS4.p1.2.m2.2.2.1.1.cmml">
            )
           </mo>
          </mrow>
          <mo id="A3.SS4.p1.2.m2.2.2.1.1.2" lspace="0em" rspace="0em" xref="A3.SS4.p1.2.m2.2.2.1.1.2.cmml">
           ​
          </mo>
          <mrow id="A3.SS4.p1.2.m2.2.2.1.1.1.1" xref="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.cmml">
           <mo id="A3.SS4.p1.2.m2.2.2.1.1.1.1.2" stretchy="false" xref="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.cmml">
            (
           </mo>
           <mrow id="A3.SS4.p1.2.m2.2.2.1.1.1.1.1" xref="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.cmml">
            <mi id="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.2" xref="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.2.cmml">
             M
            </mi>
            <mo id="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.1" xref="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.1.cmml">
             −
            </mo>
            <mn id="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.3" xref="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.3.cmml">
             1
            </mn>
           </mrow>
           <mo id="A3.SS4.p1.2.m2.2.2.1.1.1.1.3" stretchy="false" xref="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.cmml">
            )
           </mo>
          </mrow>
         </mrow>
         <mo id="A3.SS4.p1.2.m2.2.2.1.2" xref="A3.SS4.p1.2.m2.2.2.1.2.cmml">
          /
         </mo>
         <mn id="A3.SS4.p1.2.m2.2.2.1.3" xref="A3.SS4.p1.2.m2.2.2.1.3.cmml">
          2
         </mn>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A3.SS4.p1.2.m2.2b">
        <apply id="A3.SS4.p1.2.m2.2.2.cmml" xref="A3.SS4.p1.2.m2.2.2">
         <eq id="A3.SS4.p1.2.m2.2.2.2.cmml" xref="A3.SS4.p1.2.m2.2.2.2">
         </eq>
         <ci id="A3.SS4.p1.2.m2.2.2.3.cmml" xref="A3.SS4.p1.2.m2.2.2.3">
          𝑁
         </ci>
         <apply id="A3.SS4.p1.2.m2.2.2.1.cmml" xref="A3.SS4.p1.2.m2.2.2.1">
          <divide id="A3.SS4.p1.2.m2.2.2.1.2.cmml" xref="A3.SS4.p1.2.m2.2.2.1.2">
          </divide>
          <apply id="A3.SS4.p1.2.m2.2.2.1.1.cmml" xref="A3.SS4.p1.2.m2.2.2.1.1">
           <times id="A3.SS4.p1.2.m2.2.2.1.1.2.cmml" xref="A3.SS4.p1.2.m2.2.2.1.1.2">
           </times>
           <ci id="A3.SS4.p1.2.m2.1.1.cmml" xref="A3.SS4.p1.2.m2.1.1">
            𝑀
           </ci>
           <apply id="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.cmml" xref="A3.SS4.p1.2.m2.2.2.1.1.1.1">
            <minus id="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.1.cmml" xref="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.1">
            </minus>
            <ci id="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.2.cmml" xref="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.2">
             𝑀
            </ci>
            <cn id="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.3.cmml" type="integer" xref="A3.SS4.p1.2.m2.2.2.1.1.1.1.1.3">
             1
            </cn>
           </apply>
          </apply>
          <cn id="A3.SS4.p1.2.m2.2.2.1.3.cmml" type="integer" xref="A3.SS4.p1.2.m2.2.2.1.3">
           2
          </cn>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS4.p1.2.m2.2c">
        N=(M)(M-1)/2
       </annotation>
      </semantics>
     </math>
     comparisions), we compute the reward for each of these
     <math alttext="M" class="ltx_Math" display="inline" id="A3.SS4.p1.3.m3.1">
      <semantics id="A3.SS4.p1.3.m3.1a">
       <mi id="A3.SS4.p1.3.m3.1.1" xref="A3.SS4.p1.3.m3.1.1.cmml">
        M
       </mi>
       <annotation-xml encoding="MathML-Content" id="A3.SS4.p1.3.m3.1b">
        <ci id="A3.SS4.p1.3.m3.1.1.cmml" xref="A3.SS4.p1.3.m3.1.1">
         𝑀
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS4.p1.3.m3.1c">
        M
       </annotation>
      </semantics>
     </math>
     trajectories.
We then sort them by magnitude, and take the top
     <math alttext="20\%" class="ltx_Math" display="inline" id="A3.SS4.p1.4.m4.1">
      <semantics id="A3.SS4.p1.4.m4.1a">
       <mrow id="A3.SS4.p1.4.m4.1.1" xref="A3.SS4.p1.4.m4.1.1.cmml">
        <mn id="A3.SS4.p1.4.m4.1.1.2" xref="A3.SS4.p1.4.m4.1.1.2.cmml">
         20
        </mn>
        <mo id="A3.SS4.p1.4.m4.1.1.1" xref="A3.SS4.p1.4.m4.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A3.SS4.p1.4.m4.1b">
        <apply id="A3.SS4.p1.4.m4.1.1.cmml" xref="A3.SS4.p1.4.m4.1.1">
         <csymbol cd="latexml" id="A3.SS4.p1.4.m4.1.1.1.cmml" xref="A3.SS4.p1.4.m4.1.1.1">
          percent
         </csymbol>
         <cn id="A3.SS4.p1.4.m4.1.1.2.cmml" type="integer" xref="A3.SS4.p1.4.m4.1.1.2">
          20
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS4.p1.4.m4.1c">
        20\%
       </annotation>
      </semantics>
     </math>
     of these as a smaller dataset to perform behaviour cloning on.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS4.p2">
    <p class="ltx_p" id="A3.SS4.p2.3">
     For this final step of BC, we use a learning rate of
     <math alttext="1e-5" class="ltx_Math" display="inline" id="A3.SS4.p2.1.m1.1">
      <semantics id="A3.SS4.p2.1.m1.1a">
       <mrow id="A3.SS4.p2.1.m1.1.1" xref="A3.SS4.p2.1.m1.1.1.cmml">
        <mrow id="A3.SS4.p2.1.m1.1.1.2" xref="A3.SS4.p2.1.m1.1.1.2.cmml">
         <mn id="A3.SS4.p2.1.m1.1.1.2.2" xref="A3.SS4.p2.1.m1.1.1.2.2.cmml">
          1
         </mn>
         <mo id="A3.SS4.p2.1.m1.1.1.2.1" lspace="0em" rspace="0em" xref="A3.SS4.p2.1.m1.1.1.2.1.cmml">
          ​
         </mo>
         <mi id="A3.SS4.p2.1.m1.1.1.2.3" xref="A3.SS4.p2.1.m1.1.1.2.3.cmml">
          e
         </mi>
        </mrow>
        <mo id="A3.SS4.p2.1.m1.1.1.1" xref="A3.SS4.p2.1.m1.1.1.1.cmml">
         −
        </mo>
        <mn id="A3.SS4.p2.1.m1.1.1.3" xref="A3.SS4.p2.1.m1.1.1.3.cmml">
         5
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A3.SS4.p2.1.m1.1b">
        <apply id="A3.SS4.p2.1.m1.1.1.cmml" xref="A3.SS4.p2.1.m1.1.1">
         <minus id="A3.SS4.p2.1.m1.1.1.1.cmml" xref="A3.SS4.p2.1.m1.1.1.1">
         </minus>
         <apply id="A3.SS4.p2.1.m1.1.1.2.cmml" xref="A3.SS4.p2.1.m1.1.1.2">
          <times id="A3.SS4.p2.1.m1.1.1.2.1.cmml" xref="A3.SS4.p2.1.m1.1.1.2.1">
          </times>
          <cn id="A3.SS4.p2.1.m1.1.1.2.2.cmml" type="integer" xref="A3.SS4.p2.1.m1.1.1.2.2">
           1
          </cn>
          <ci id="A3.SS4.p2.1.m1.1.1.2.3.cmml" xref="A3.SS4.p2.1.m1.1.1.2.3">
           𝑒
          </ci>
         </apply>
         <cn id="A3.SS4.p2.1.m1.1.1.3.cmml" type="integer" xref="A3.SS4.p2.1.m1.1.1.3">
          5
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS4.p2.1.m1.1c">
        1e-5
       </annotation>
      </semantics>
     </math>
     with
     <math alttext="1000" class="ltx_Math" display="inline" id="A3.SS4.p2.2.m2.1">
      <semantics id="A3.SS4.p2.2.m2.1a">
       <mn id="A3.SS4.p2.2.m2.1.1" xref="A3.SS4.p2.2.m2.1.1.cmml">
        1000
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS4.p2.2.m2.1b">
        <cn id="A3.SS4.p2.2.m2.1.1.cmml" type="integer" xref="A3.SS4.p2.2.m2.1.1">
         1000
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS4.p2.2.m2.1c">
        1000
       </annotation>
      </semantics>
     </math>
     updates on minibatches of size
     <math alttext="256" class="ltx_Math" display="inline" id="A3.SS4.p2.3.m3.1">
      <semantics id="A3.SS4.p2.3.m3.1a">
       <mn id="A3.SS4.p2.3.m3.1.1" xref="A3.SS4.p2.3.m3.1.1.cmml">
        256
       </mn>
       <annotation-xml encoding="MathML-Content" id="A3.SS4.p2.3.m3.1b">
        <cn id="A3.SS4.p2.3.m3.1.1.cmml" type="integer" xref="A3.SS4.p2.3.m3.1.1">
         256
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS4.p2.3.m3.1c">
        256
       </annotation>
      </semantics>
     </math>
     .
We only train the parameters of the MLP after the transformer layers.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS4.p3">
    <p class="ltx_p" id="A3.SS4.p3.1">
     We again used 4 48GB Nvidia A6000 GPUs for less than an hour for preference fine-tuning.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS4.p4">
    <p class="ltx_p" id="A3.SS4.p4.1">
     <span class="ltx_text ltx_font_bold" id="A3.SS4.p4.1.1">
      Reinforcement Learning:
     </span>
     in our experiments we use an undiscounted REINFORCE loss on batches of 16 episodes of up to 100 timesteps. We use a learning rate of
     <math alttext="1e-4" class="ltx_Math" display="inline" id="A3.SS4.p4.1.m1.1">
      <semantics id="A3.SS4.p4.1.m1.1a">
       <mrow id="A3.SS4.p4.1.m1.1.1" xref="A3.SS4.p4.1.m1.1.1.cmml">
        <mrow id="A3.SS4.p4.1.m1.1.1.2" xref="A3.SS4.p4.1.m1.1.1.2.cmml">
         <mn id="A3.SS4.p4.1.m1.1.1.2.2" xref="A3.SS4.p4.1.m1.1.1.2.2.cmml">
          1
         </mn>
         <mo id="A3.SS4.p4.1.m1.1.1.2.1" lspace="0em" rspace="0em" xref="A3.SS4.p4.1.m1.1.1.2.1.cmml">
          ​
         </mo>
         <mi id="A3.SS4.p4.1.m1.1.1.2.3" xref="A3.SS4.p4.1.m1.1.1.2.3.cmml">
          e
         </mi>
        </mrow>
        <mo id="A3.SS4.p4.1.m1.1.1.1" xref="A3.SS4.p4.1.m1.1.1.1.cmml">
         −
        </mo>
        <mn id="A3.SS4.p4.1.m1.1.1.3" xref="A3.SS4.p4.1.m1.1.1.3.cmml">
         4
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A3.SS4.p4.1.m1.1b">
        <apply id="A3.SS4.p4.1.m1.1.1.cmml" xref="A3.SS4.p4.1.m1.1.1">
         <minus id="A3.SS4.p4.1.m1.1.1.1.cmml" xref="A3.SS4.p4.1.m1.1.1.1">
         </minus>
         <apply id="A3.SS4.p4.1.m1.1.1.2.cmml" xref="A3.SS4.p4.1.m1.1.1.2">
          <times id="A3.SS4.p4.1.m1.1.1.2.1.cmml" xref="A3.SS4.p4.1.m1.1.1.2.1">
          </times>
          <cn id="A3.SS4.p4.1.m1.1.1.2.2.cmml" type="integer" xref="A3.SS4.p4.1.m1.1.1.2.2">
           1
          </cn>
          <ci id="A3.SS4.p4.1.m1.1.1.2.3.cmml" xref="A3.SS4.p4.1.m1.1.1.2.3">
           𝑒
          </ci>
         </apply>
         <cn id="A3.SS4.p4.1.m1.1.1.3.cmml" type="integer" xref="A3.SS4.p4.1.m1.1.1.3">
          4
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS4.p4.1.m1.1c">
        1e-4
       </annotation>
      </semantics>
     </math>
     and once again only train the parameters of the MLP after the transformer layers.
If an error occurs during an episode’s rollout, we simply drop the samples from that episode and subsequently use a smaller batch size for the update.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS4.p5">
    <p class="ltx_p" id="A3.SS4.p5.1">
     We used 16 Standard NV12 machines (each with 2 Nvidia Tesla M60 GPUs for game rendering) on Azure Batch, requiring one machine around one day per seed for agent alignment.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS4.p6">
    <p class="ltx_p" id="A3.SS4.p6.1">
     The total compute used for the entire research project therefore represents of the order of weeks of total compute time, with preliminary experiments not included of the order of days of compute.
    </p>
   </div>
   <div class="ltx_pagination ltx_role_newpage">
   </div>
  </section>
 </section>
 <section class="ltx_appendix" id="A4">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix D
   </span>
   Discussion of Offline to Online Distribution Shift
  </h2>
  <div class="ltx_para" id="A4.p1">
   <p class="ltx_p" id="A4.p1.1">
    As we mention in Section
    <a class="ltx_ref" href="#S3" title="3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    , the nature of using a real AAA video game is such that there is significant offline to online distribution shift between our offline training data and our online evaluation. One such source is due to character selection and customisable visual modifications, as shown below in Figure
    <a class="ltx_ref" href="#A4.F10" title="Figure 10 ‣ Appendix D Discussion of Offline to Online Distribution Shift ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      10
     </span>
    </a>
    .
   </p>
  </div>
  <figure class="ltx_figure" id="A4.F10">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_many">
     <figure class="ltx_figure ltx_figure_panel" id="A4.F10.sf1">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="598" id="A4.F10.sf1.g1" src="/html/2406.04208/assets/figures/appendix_figures/character_1.png" width="598"/>
      <figcaption class="ltx_caption">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="A4.F10.sf1.2.1.1" style="font-size:90%;">
         (a)
        </span>
       </span>
      </figcaption>
     </figure>
    </div>
    <div class="ltx_flex_cell ltx_flex_size_many">
     <figure class="ltx_figure ltx_figure_panel" id="A4.F10.sf2">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="598" id="A4.F10.sf2.g1" src="/html/2406.04208/assets/figures/appendix_figures/character_2.png" width="598"/>
      <figcaption class="ltx_caption">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="A4.F10.sf2.2.1.1" style="font-size:90%;">
         (b)
        </span>
       </span>
      </figcaption>
     </figure>
    </div>
    <div class="ltx_flex_cell ltx_flex_size_many">
     <figure class="ltx_figure ltx_figure_panel" id="A4.F10.sf3">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="598" id="A4.F10.sf3.g1" src="/html/2406.04208/assets/figures/appendix_figures/character_3.png" width="598"/>
      <figcaption class="ltx_caption">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="A4.F10.sf3.2.1.1" style="font-size:90%;">
         (c)
        </span>
       </span>
      </figcaption>
     </figure>
    </div>
    <div class="ltx_flex_cell ltx_flex_size_many">
     <figure class="ltx_figure ltx_figure_panel" id="A4.F10.sf4">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="598" id="A4.F10.sf4.g1" src="/html/2406.04208/assets/figures/appendix_figures/character_4.png" width="598"/>
      <figcaption class="ltx_caption">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="A4.F10.sf4.2.1.1" style="font-size:90%;">
         (d)
        </span>
       </span>
      </figcaption>
     </figure>
    </div>
    <div class="ltx_flex_cell ltx_flex_size_many">
     <figure class="ltx_figure ltx_figure_panel" id="A4.F10.sf5">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="598" id="A4.F10.sf5.g1" src="/html/2406.04208/assets/figures/appendix_figures/character_ours.png" width="598"/>
      <figcaption class="ltx_caption">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="A4.F10.sf5.2.1.1" style="font-size:90%;">
         (e)
        </span>
       </span>
      </figcaption>
     </figure>
    </div>
   </div>
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="A4.F10.6.3.1" style="font-size:90%;">
      Figure 10
     </span>
     :
    </span>
    <span class="ltx_text" id="A4.F10.4.2" style="font-size:90%;">
     Screenshots of various characters with visual modifications contained within the general pre-training data. All are completely representative of the input provided to our agent (only
     <math alttext="256\times 256" class="ltx_Math" display="inline" id="A4.F10.3.1.m1.1">
      <semantics id="A4.F10.3.1.m1.1b">
       <mrow id="A4.F10.3.1.m1.1.1" xref="A4.F10.3.1.m1.1.1.cmml">
        <mn id="A4.F10.3.1.m1.1.1.2" xref="A4.F10.3.1.m1.1.1.2.cmml">
         256
        </mn>
        <mo id="A4.F10.3.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A4.F10.3.1.m1.1.1.1.cmml">
         ×
        </mo>
        <mn id="A4.F10.3.1.m1.1.1.3" xref="A4.F10.3.1.m1.1.1.3.cmml">
         256
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A4.F10.3.1.m1.1c">
        <apply id="A4.F10.3.1.m1.1.1.cmml" xref="A4.F10.3.1.m1.1.1">
         <times id="A4.F10.3.1.m1.1.1.1.cmml" xref="A4.F10.3.1.m1.1.1.1">
         </times>
         <cn id="A4.F10.3.1.m1.1.1.2.cmml" type="integer" xref="A4.F10.3.1.m1.1.1.2">
          256
         </cn>
         <cn id="A4.F10.3.1.m1.1.1.3.cmml" type="integer" xref="A4.F10.3.1.m1.1.1.3">
          256
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A4.F10.3.1.m1.1d">
        256\times 256
       </annotation>
      </semantics>
     </math>
     rather than
     <math alttext="128\times 128" class="ltx_Math" display="inline" id="A4.F10.4.2.m2.1">
      <semantics id="A4.F10.4.2.m2.1b">
       <mrow id="A4.F10.4.2.m2.1.1" xref="A4.F10.4.2.m2.1.1.cmml">
        <mn id="A4.F10.4.2.m2.1.1.2" xref="A4.F10.4.2.m2.1.1.2.cmml">
         128
        </mn>
        <mo id="A4.F10.4.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="A4.F10.4.2.m2.1.1.1.cmml">
         ×
        </mo>
        <mn id="A4.F10.4.2.m2.1.1.3" xref="A4.F10.4.2.m2.1.1.3.cmml">
         128
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A4.F10.4.2.m2.1c">
        <apply id="A4.F10.4.2.m2.1.1.cmml" xref="A4.F10.4.2.m2.1.1">
         <times id="A4.F10.4.2.m2.1.1.1.cmml" xref="A4.F10.4.2.m2.1.1.1">
         </times>
         <cn id="A4.F10.4.2.m2.1.1.2.cmml" type="integer" xref="A4.F10.4.2.m2.1.1.2">
          128
         </cn>
         <cn id="A4.F10.4.2.m2.1.1.3.cmml" type="integer" xref="A4.F10.4.2.m2.1.1.3">
          128
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A4.F10.4.2.m2.1d">
        128\times 128
       </annotation>
      </semantics>
     </math>
     resolution). Figure (e) demonstrates the agent we use for online evaluation.
    </span>
   </figcaption>
  </figure>
  <div class="ltx_para" id="A4.p2">
   <p class="ltx_p" id="A4.p2.1">
    It is well-known that offline imitation learning often suffers from online distribution drift, particularly when learning from pixels in a continuous partially observable environment, due to accumulating errors
    <cite class="ltx_cite ltx_citemacro_citep">
     (Ross et al.,
     <a class="ltx_ref" href="#bib.bib46" title="">
      2011
     </a>
     )
    </cite>
    . The visual distribution shift in our environment shown in Figure
    <a class="ltx_ref" href="#A4.F10" title="Figure 10 ‣ Appendix D Discussion of Offline to Online Distribution Shift ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      10
     </span>
    </a>
    only exacerbates these issues. As a result, imitation learning agents may not consistently perform imitated behaviours online even if the loss has converged on the offline data. This challenge is fundamental to offline learning
    <cite class="ltx_cite ltx_citemacro_citep">
     (Ostrovski et al.,
     <a class="ltx_ref" href="#bib.bib39" title="">
      2021
     </a>
     )
    </cite>
    . This motivates the need to have some online fine-tuning (in our case from preferences) to refine the policy to consistently perform the desired behaviour online.
   </p>
  </div>
  <div class="ltx_para" id="A4.p3">
   <p class="ltx_p" id="A4.p3.1">
    Figure
    <a class="ltx_ref" href="#S3.F4" title="Figure 4 ‣ 3.2 Supervised Fine-Tuning on Task Relevant Dataset ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      4
     </span>
    </a>
    demonstrates that even after the supervised fine-tuning stage on a dataset of trajectories that always reach a jumppad and involve the same character, a significant proportion of trajectories (
    <math alttext="\sim 10\%" class="ltx_Math" display="inline" id="A4.p3.1.m1.1">
     <semantics id="A4.p3.1.m1.1a">
      <mrow id="A4.p3.1.m1.1.1" xref="A4.p3.1.m1.1.1.cmml">
       <mi id="A4.p3.1.m1.1.1.2" xref="A4.p3.1.m1.1.1.2.cmml">
       </mi>
       <mo id="A4.p3.1.m1.1.1.1" xref="A4.p3.1.m1.1.1.1.cmml">
        ∼
       </mo>
       <mrow id="A4.p3.1.m1.1.1.3" xref="A4.p3.1.m1.1.1.3.cmml">
        <mn id="A4.p3.1.m1.1.1.3.2" xref="A4.p3.1.m1.1.1.3.2.cmml">
         10
        </mn>
        <mo id="A4.p3.1.m1.1.1.3.1" xref="A4.p3.1.m1.1.1.3.1.cmml">
         %
        </mo>
       </mrow>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A4.p3.1.m1.1b">
       <apply id="A4.p3.1.m1.1.1.cmml" xref="A4.p3.1.m1.1.1">
        <csymbol cd="latexml" id="A4.p3.1.m1.1.1.1.cmml" xref="A4.p3.1.m1.1.1.1">
         similar-to
        </csymbol>
        <csymbol cd="latexml" id="A4.p3.1.m1.1.1.2.cmml" xref="A4.p3.1.m1.1.1.2">
         absent
        </csymbol>
        <apply id="A4.p3.1.m1.1.1.3.cmml" xref="A4.p3.1.m1.1.1.3">
         <csymbol cd="latexml" id="A4.p3.1.m1.1.1.3.1.cmml" xref="A4.p3.1.m1.1.1.3.1">
          percent
         </csymbol>
         <cn id="A4.p3.1.m1.1.1.3.2.cmml" type="integer" xref="A4.p3.1.m1.1.1.3.2">
          10
         </cn>
        </apply>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A4.p3.1.m1.1c">
       \sim 10\%
      </annotation>
     </semantics>
    </math>
    ) do not reach any jumppad. However, after online fine-tuning, we are able to increase the overall success rate to nearly 100% as shown in Figure
    <a class="ltx_ref" href="#A8.F19" title="Figure 19 ‣ Appendix H Final Aligned Agent Jumppad Distributions ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      19
     </span>
    </a>
    .
   </p>
  </div>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
 <section class="ltx_appendix" id="A5">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix E
   </span>
   Ablation of Unsupervised Pre-Training and Model Scaling
  </h2>
  <section class="ltx_subsection" id="A5.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     E.1
    </span>
    Ablation of Unsupervised Pre-Training
   </h3>
   <div class="ltx_para" id="A5.SS1.p1">
    <p class="ltx_p" id="A5.SS1.p1.1">
     We see from the results in Sections
     <a class="ltx_ref" href="#S3.SS1" title="3.1 Training a Base Imitation Learning Model ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       3.1
      </span>
     </a>
     and
     <a class="ltx_ref" href="#S3.SS2" title="3.2 Supervised Fine-Tuning on Task Relevant Dataset ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       3.2
      </span>
     </a>
     that fine-tuning improves the task-specific performance of our pre-trained agent.
However, to determine whether fine-tuning our base model is beneficial over simply training a model from scratch on our curated task-specific dataset, we also ablate the unsupervised pre-training stage. We train our agent from scratch for 20k updates, using the same procedure as used previously for fine-tuning (see Appendix
     <a class="ltx_ref" href="#A3" title="Appendix C Architectures and Training Details ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       C
      </span>
     </a>
     ) until the loss appeared to converge. We subsequently evaluated the agent in the environment for 1000 episodes following the same procedure described in Section
     <a class="ltx_ref" href="#S3" title="3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     . The distribution of jumppads reached by the agent trained from scratch on the fine-tuning dataset is shown below (left) for comparison with the original pre-trained+fine-tuned agent (right) in Figure
     <a class="ltx_ref" href="#A5.F11" title="Figure 11 ‣ E.1 Ablation of Unsupervised Pre-Training ‣ Appendix E Ablation of Unsupervised Pre-Training and Model Scaling ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       11
      </span>
     </a>
     .
    </p>
   </div>
   <figure class="ltx_figure" id="A5.F11">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_2">
      <img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="129" id="A5.F11.g1" src="/html/2406.04208/assets/x8.png" width="221"/>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="129" id="A5.F11.g2" src="/html/2406.04208/assets/x9.png" width="221"/>
     </div>
    </div>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="A5.F11.2.1.1" style="font-size:90%;">
       Figure 11
      </span>
      :
     </span>
     <span class="ltx_text" id="A5.F11.3.2" style="font-size:90%;">
      Distribution of jumppads reached by an imitation learning agent trained from scratch on the task-specific dataset used for fine-tuning the base model (left) compared to the base agent that was fine-tuned on this dataset.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="A5.SS1.p2">
    <p class="ltx_p" id="A5.SS1.p2.1">
     We find that this agent has a greater failure rate (around 20% for the fine-tuned only agent fail to reach any jumppad compared to only around 10% for the pre-trained+fine-tuned agent) and a much narrower distribution of jumppads reached. This is surprising given the task-specific dataset consists of successful trajectories evenly split between the jumppads.
    </p>
   </div>
   <div class="ltx_para" id="A5.SS1.p3">
    <p class="ltx_p" id="A5.SS1.p3.1">
     As additional anecdotal evidence for the benefit of unsupervised pre-training, we also noticed fine-tuned agents miss a jumppad, hit the wall behind and then turn around to hit the jumppad. Since the fine-tuning dataset consists of only curated trajectories that directly hit the dataset, this behaviour was not present in the fine-tuning dataset, and agents trained from scratch on the fine-tuned dataset are found to continuously run into the wall if they miss the jumppad, as they have never seen that observation before (and since moving forward is the most likely action). This behavior is shown on our project webpage at:
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://adamjelley.github.io/aligning-agents-like-llms/" target="_blank" title="">
      https://adamjelley.github.io/aligning-agents-like-llms/
     </a>
     .
    </p>
   </div>
   <div class="ltx_para" id="A5.SS1.p4">
    <p class="ltx_p" id="A5.SS1.p4.1">
     This provides further anecdotal but intuitive evidence for the benefits of unsupervised pre-training.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A5.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     E.2
    </span>
    Preliminary Model Scaling Analysis
   </h3>
   <div class="ltx_para" id="A5.SS2.p1">
    <p class="ltx_p" id="A5.SS2.p1.1">
     To further justify the model size and investigate scaling properties of our transformer policy, we also trained smaller models of 4M and 25M parameters on our task-specific dataset, using the same procedure as above. The architecture of our 2 smaller models is identical to that of the base model described in Section
     <a class="ltx_ref" href="#A3" title="Appendix C Architectures and Training Details ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       C
      </span>
     </a>
     , except for:
    </p>
   </div>
   <div class="ltx_para" id="A5.SS2.p2">
    <ul class="ltx_itemize" id="A5.I1">
     <li class="ltx_item" id="A5.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A5.I1.i1.p1">
       <p class="ltx_p" id="A5.I1.i1.p1.4">
        <math alttext="\sim" class="ltx_Math" display="inline" id="A5.I1.i1.p1.1.m1.1">
         <semantics id="A5.I1.i1.p1.1.m1.1a">
          <mo id="A5.I1.i1.p1.1.m1.1.1" xref="A5.I1.i1.p1.1.m1.1.1.cmml">
           ∼
          </mo>
          <annotation-xml encoding="MathML-Content" id="A5.I1.i1.p1.1.m1.1b">
           <csymbol cd="latexml" id="A5.I1.i1.p1.1.m1.1.1.cmml" xref="A5.I1.i1.p1.1.m1.1.1">
            similar-to
           </csymbol>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="A5.I1.i1.p1.1.m1.1c">
           \sim
          </annotation>
         </semantics>
        </math>
        <span class="ltx_text ltx_font_bold" id="A5.I1.i1.p1.4.1">
         4M:
        </span>
        <math alttext="4" class="ltx_Math" display="inline" id="A5.I1.i1.p1.2.m2.1">
         <semantics id="A5.I1.i1.p1.2.m2.1a">
          <mn id="A5.I1.i1.p1.2.m2.1.1" xref="A5.I1.i1.p1.2.m2.1.1.cmml">
           4
          </mn>
          <annotation-xml encoding="MathML-Content" id="A5.I1.i1.p1.2.m2.1b">
           <cn id="A5.I1.i1.p1.2.m2.1.1.cmml" type="integer" xref="A5.I1.i1.p1.2.m2.1.1">
            4
           </cn>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="A5.I1.i1.p1.2.m2.1c">
           4
          </annotation>
         </semantics>
        </math>
        layers with
        <math alttext="256" class="ltx_Math" display="inline" id="A5.I1.i1.p1.3.m3.1">
         <semantics id="A5.I1.i1.p1.3.m3.1a">
          <mn id="A5.I1.i1.p1.3.m3.1.1" xref="A5.I1.i1.p1.3.m3.1.1.cmml">
           256
          </mn>
          <annotation-xml encoding="MathML-Content" id="A5.I1.i1.p1.3.m3.1b">
           <cn id="A5.I1.i1.p1.3.m3.1.1.cmml" type="integer" xref="A5.I1.i1.p1.3.m3.1.1">
            256
           </cn>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="A5.I1.i1.p1.3.m3.1c">
           256
          </annotation>
         </semantics>
        </math>
        hidden dims and
        <math alttext="4" class="ltx_Math" display="inline" id="A5.I1.i1.p1.4.m4.1">
         <semantics id="A5.I1.i1.p1.4.m4.1a">
          <mn id="A5.I1.i1.p1.4.m4.1.1" xref="A5.I1.i1.p1.4.m4.1.1.cmml">
           4
          </mn>
          <annotation-xml encoding="MathML-Content" id="A5.I1.i1.p1.4.m4.1b">
           <cn id="A5.I1.i1.p1.4.m4.1.1.cmml" type="integer" xref="A5.I1.i1.p1.4.m4.1.1">
            4
           </cn>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="A5.I1.i1.p1.4.m4.1c">
           4
          </annotation>
         </semantics>
        </math>
        heads for each attention layer.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A5.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A5.I1.i2.p1">
       <p class="ltx_p" id="A5.I1.i2.p1.4">
        <math alttext="\sim" class="ltx_Math" display="inline" id="A5.I1.i2.p1.1.m1.1">
         <semantics id="A5.I1.i2.p1.1.m1.1a">
          <mo id="A5.I1.i2.p1.1.m1.1.1" xref="A5.I1.i2.p1.1.m1.1.1.cmml">
           ∼
          </mo>
          <annotation-xml encoding="MathML-Content" id="A5.I1.i2.p1.1.m1.1b">
           <csymbol cd="latexml" id="A5.I1.i2.p1.1.m1.1.1.cmml" xref="A5.I1.i2.p1.1.m1.1.1">
            similar-to
           </csymbol>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="A5.I1.i2.p1.1.m1.1c">
           \sim
          </annotation>
         </semantics>
        </math>
        <span class="ltx_text ltx_font_bold" id="A5.I1.i2.p1.4.1">
         25M:
        </span>
        <math alttext="8" class="ltx_Math" display="inline" id="A5.I1.i2.p1.2.m2.1">
         <semantics id="A5.I1.i2.p1.2.m2.1a">
          <mn id="A5.I1.i2.p1.2.m2.1.1" xref="A5.I1.i2.p1.2.m2.1.1.cmml">
           8
          </mn>
          <annotation-xml encoding="MathML-Content" id="A5.I1.i2.p1.2.m2.1b">
           <cn id="A5.I1.i2.p1.2.m2.1.1.cmml" type="integer" xref="A5.I1.i2.p1.2.m2.1.1">
            8
           </cn>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="A5.I1.i2.p1.2.m2.1c">
           8
          </annotation>
         </semantics>
        </math>
        layers with
        <math alttext="512" class="ltx_Math" display="inline" id="A5.I1.i2.p1.3.m3.1">
         <semantics id="A5.I1.i2.p1.3.m3.1a">
          <mn id="A5.I1.i2.p1.3.m3.1.1" xref="A5.I1.i2.p1.3.m3.1.1.cmml">
           512
          </mn>
          <annotation-xml encoding="MathML-Content" id="A5.I1.i2.p1.3.m3.1b">
           <cn id="A5.I1.i2.p1.3.m3.1.1.cmml" type="integer" xref="A5.I1.i2.p1.3.m3.1.1">
            512
           </cn>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="A5.I1.i2.p1.3.m3.1c">
           512
          </annotation>
         </semantics>
        </math>
        hidden dims and
        <math alttext="8" class="ltx_Math" display="inline" id="A5.I1.i2.p1.4.m4.1">
         <semantics id="A5.I1.i2.p1.4.m4.1a">
          <mn id="A5.I1.i2.p1.4.m4.1.1" xref="A5.I1.i2.p1.4.m4.1.1.cmml">
           8
          </mn>
          <annotation-xml encoding="MathML-Content" id="A5.I1.i2.p1.4.m4.1b">
           <cn id="A5.I1.i2.p1.4.m4.1.1.cmml" type="integer" xref="A5.I1.i2.p1.4.m4.1.1">
            8
           </cn>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="A5.I1.i2.p1.4.m4.1c">
           8
          </annotation>
         </semantics>
        </math>
        heads for each attention layer.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="A5.I1.i3" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="A5.I1.i3.p1">
       <p class="ltx_p" id="A5.I1.i3.p1.4">
        (
        <math alttext="\sim" class="ltx_Math" display="inline" id="A5.I1.i3.p1.1.m1.1">
         <semantics id="A5.I1.i3.p1.1.m1.1a">
          <mo id="A5.I1.i3.p1.1.m1.1.1" xref="A5.I1.i3.p1.1.m1.1.1.cmml">
           ∼
          </mo>
          <annotation-xml encoding="MathML-Content" id="A5.I1.i3.p1.1.m1.1b">
           <csymbol cd="latexml" id="A5.I1.i3.p1.1.m1.1.1.cmml" xref="A5.I1.i3.p1.1.m1.1.1">
            similar-to
           </csymbol>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="A5.I1.i3.p1.1.m1.1c">
           \sim
          </annotation>
         </semantics>
        </math>
        <span class="ltx_text ltx_font_bold" id="A5.I1.i3.p1.4.1">
         103M:
        </span>
        <math alttext="8" class="ltx_Math" display="inline" id="A5.I1.i3.p1.2.m2.1">
         <semantics id="A5.I1.i3.p1.2.m2.1a">
          <mn id="A5.I1.i3.p1.2.m2.1.1" xref="A5.I1.i3.p1.2.m2.1.1.cmml">
           8
          </mn>
          <annotation-xml encoding="MathML-Content" id="A5.I1.i3.p1.2.m2.1b">
           <cn id="A5.I1.i3.p1.2.m2.1.1.cmml" type="integer" xref="A5.I1.i3.p1.2.m2.1.1">
            8
           </cn>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="A5.I1.i3.p1.2.m2.1c">
           8
          </annotation>
         </semantics>
        </math>
        layers with
        <math alttext="1024" class="ltx_Math" display="inline" id="A5.I1.i3.p1.3.m3.1">
         <semantics id="A5.I1.i3.p1.3.m3.1a">
          <mn id="A5.I1.i3.p1.3.m3.1.1" xref="A5.I1.i3.p1.3.m3.1.1.cmml">
           1024
          </mn>
          <annotation-xml encoding="MathML-Content" id="A5.I1.i3.p1.3.m3.1b">
           <cn id="A5.I1.i3.p1.3.m3.1.1.cmml" type="integer" xref="A5.I1.i3.p1.3.m3.1.1">
            1024
           </cn>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="A5.I1.i3.p1.3.m3.1c">
           1024
          </annotation>
         </semantics>
        </math>
        hidden dims and
        <math alttext="8" class="ltx_Math" display="inline" id="A5.I1.i3.p1.4.m4.1">
         <semantics id="A5.I1.i3.p1.4.m4.1a">
          <mn id="A5.I1.i3.p1.4.m4.1.1" xref="A5.I1.i3.p1.4.m4.1.1.cmml">
           8
          </mn>
          <annotation-xml encoding="MathML-Content" id="A5.I1.i3.p1.4.m4.1b">
           <cn id="A5.I1.i3.p1.4.m4.1.1.cmml" type="integer" xref="A5.I1.i3.p1.4.m4.1.1">
            8
           </cn>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="A5.I1.i3.p1.4.m4.1c">
           8
          </annotation>
         </semantics>
        </math>
        heads for each attention layer.)
       </p>
      </div>
     </li>
    </ul>
   </div>
   <figure class="ltx_figure" id="A5.F12">
    <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="258" id="A5.F12.g1" src="/html/2406.04208/assets/x10.png" width="461"/>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="A5.F12.2.1.1" style="font-size:90%;">
       Figure 12
      </span>
      :
     </span>
     <span class="ltx_text" id="A5.F12.3.2" style="font-size:90%;">
      Distribution of jumppads reached by various size models (4M, 25M and 103M parameters) trained from scratch on the task-specific dataset used for fine-tuning the base model. Importantly, we see that task failure rates (the grey bars on the left) decrease as the number of parameters increases, even given the same size of dataset and number of training updates.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="A5.SS2.p3">
    <p class="ltx_p" id="A5.SS2.p3.1">
     The jumppad distributions for these models are provided in Figure
     <a class="ltx_ref" href="#A5.F12" title="Figure 12 ‣ E.2 Preliminary Model Scaling Analysis ‣ Appendix E Ablation of Unsupervised Pre-Training and Model Scaling ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       12
      </span>
     </a>
     . We see that these smaller models have much greater failure rates that the larger 103M parameter models shown in Figure
     <a class="ltx_ref" href="#A5.F11" title="Figure 11 ‣ E.1 Ablation of Unsupervised Pre-Training ‣ Appendix E Ablation of Unsupervised Pre-Training and Model Scaling ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       11
      </span>
     </a>
     , demonstrating that larger models are beneficial for imitation learning from pixels even on this relatively small task-specific dataset of 300 trajectories. While larger models still may be beneficial (particularly with pre-training on our large unsupervised dataset described in Section
     <a class="ltx_ref" href="#S3" title="3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     ), larger models would further increase the crucial inference cost and we find that 103M parameters is sufficient for further alignment, as demonstrated in Figure
     <a class="ltx_ref" href="#A5.F11" title="Figure 11 ‣ E.1 Ablation of Unsupervised Pre-Training ‣ Appendix E Ablation of Unsupervised Pre-Training and Model Scaling ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       11
      </span>
     </a>
     .
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A5.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     E.3
    </span>
    Alignment of Models Trained from Scratch by Size
   </h3>
   <div class="ltx_para" id="A5.SS3.p1">
    <p class="ltx_p" id="A5.SS3.p1.1">
     To complete our ablation, we now investigate how pre-training and model size affects online alignment.
To do so we followed the procedure in Appendix
     <a class="ltx_ref" href="#A1" title="Appendix A Discussion of General Procedure for Aligning Agents ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       A
      </span>
     </a>
     to generated preferences as in Section
     <a class="ltx_ref" href="#S3" title="3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     .
We then trained corresponding reward models and compare aligning these models trained from scratch to our pre-trained and fine-tuned model, as shown below in Figure
     <a class="ltx_ref" href="#A5.F13" title="Figure 13 ‣ E.3 Alignment of Models Trained from Scratch by Size ‣ Appendix E Ablation of Unsupervised Pre-Training and Model Scaling ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       13
      </span>
     </a>
     .
    </p>
   </div>
   <figure class="ltx_figure" id="A5.F13">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_2">
      <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="274" id="A5.F13.g1" src="/html/2406.04208/assets/figures/appendix_figures/AblationFTOnlyAlignLeft.png" width="269"/>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="273" id="A5.F13.g2" src="/html/2406.04208/assets/figures/appendix_figures/AblationFTOnlyAlignRight.png" width="269"/>
     </div>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      <span class="ltx_text" id="A5.F13.2.1.1" style="font-size:90%;">
       Figure 13
      </span>
      :
     </span>
     <span class="ltx_text" id="A5.F13.3.2" style="font-size:90%;">
      Left and right jumppad success rate during online alignment for models of different sizes trained from scratch using corresponding reward models trained on 500k comparisons.
     </span>
    </figcaption>
   </figure>
   <div class="ltx_para" id="A5.SS3.p2">
    <p class="ltx_p" id="A5.SS3.p2.1">
     Overall, the results are relatively high variance due to the small dataset these results are based on. However, bearing that in mind, we can see some general trends emerging nonetheless. We see that our pre-trained model aligns significantly better than the equivalent size model trained from scratch when aligned left, but worse when aligned right (due to the lack of diversity in the pre-trained+fine-tuned model as explained in Section
     <a class="ltx_ref" href="#S3.SS5.SSS2" title="3.5.2 Aligning Agent Towards Right Jumppad ‣ 3.5 Aligning the Fine-tuned Model with the Reward Model ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       3.5.2
      </span>
     </a>
     ). We also see that larger models generally align better (although the bias of the 25M parameter model towards going right makes this trend less clear) since the success rate generally increases more quickly during alignment for the larger models.
    </p>
   </div>
   <div class="ltx_para" id="A5.SS3.p3">
    <p class="ltx_p" id="A5.SS3.p3.2">
     We also note that in this experiment we only considered using the reward models trained on 500k comparisons, in order to see whether it was possible to successfully align a model trained solely on our small task-specific dataset.
As noted in the previous section, these models have significantly less diversity in their behaviour than the model first pre-trained and then fine-tuned.
This makes it much more costly (in terms of human time) to generate sufficient labelled examples of the desired behaviour to properly train the corresponding reward model (for example, less than
     <math alttext="10\%" class="ltx_Math" display="inline" id="A5.SS3.p3.1.m1.1">
      <semantics id="A5.SS3.p3.1.m1.1a">
       <mrow id="A5.SS3.p3.1.m1.1.1" xref="A5.SS3.p3.1.m1.1.1.cmml">
        <mn id="A5.SS3.p3.1.m1.1.1.2" xref="A5.SS3.p3.1.m1.1.1.2.cmml">
         10
        </mn>
        <mo id="A5.SS3.p3.1.m1.1.1.1" xref="A5.SS3.p3.1.m1.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A5.SS3.p3.1.m1.1b">
        <apply id="A5.SS3.p3.1.m1.1.1.cmml" xref="A5.SS3.p3.1.m1.1.1">
         <csymbol cd="latexml" id="A5.SS3.p3.1.m1.1.1.1.cmml" xref="A5.SS3.p3.1.m1.1.1.1">
          percent
         </csymbol>
         <cn id="A5.SS3.p3.1.m1.1.1.2.cmml" type="integer" xref="A5.SS3.p3.1.m1.1.1.2">
          10
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A5.SS3.p3.1.m1.1c">
        10\%
       </annotation>
      </semantics>
     </math>
     of trajectories reach the left jumppad for the model trained from scratch compared to over
     <math alttext="30\%" class="ltx_Math" display="inline" id="A5.SS3.p3.2.m2.1">
      <semantics id="A5.SS3.p3.2.m2.1a">
       <mrow id="A5.SS3.p3.2.m2.1.1" xref="A5.SS3.p3.2.m2.1.1.cmml">
        <mn id="A5.SS3.p3.2.m2.1.1.2" xref="A5.SS3.p3.2.m2.1.1.2.cmml">
         30
        </mn>
        <mo id="A5.SS3.p3.2.m2.1.1.1" xref="A5.SS3.p3.2.m2.1.1.1.cmml">
         %
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="A5.SS3.p3.2.m2.1b">
        <apply id="A5.SS3.p3.2.m2.1.1.cmml" xref="A5.SS3.p3.2.m2.1.1">
         <csymbol cd="latexml" id="A5.SS3.p3.2.m2.1.1.1.cmml" xref="A5.SS3.p3.2.m2.1.1.1">
          percent
         </csymbol>
         <cn id="A5.SS3.p3.2.m2.1.1.2.cmml" type="integer" xref="A5.SS3.p3.2.m2.1.1.2">
          30
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A5.SS3.p3.2.m2.1c">
        30\%
       </annotation>
      </semantics>
     </math>
     for the pre-trained + fine-tuned model).
This means that while these results show an upper bound in terms of reward model performance, we would expect the larger and pre-trained models to have less degradation in their alignment when using reward models trained on smaller quantities of preference data.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_appendix" id="A6">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix F
   </span>
   Reward Model Performances by Jumppad
  </h2>
  <div class="ltx_para" id="A6.p1">
   <p class="ltx_p" id="A6.p1.1">
    Test reward model performances against number of comparisons used for training by jumppad are shown below in Figure
    <a class="ltx_ref" href="#A6.F14" title="Figure 14 ‣ Appendix F Reward Model Performances by Jumppad ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      14
     </span>
    </a>
    .
   </p>
  </div>
  <figure class="ltx_figure" id="A6.F14">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_2">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="178" id="A6.F14.g1" src="/html/2406.04208/assets/x11.png" width="226"/>
    </div>
    <div class="ltx_flex_cell ltx_flex_size_2">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="177" id="A6.F14.g2" src="/html/2406.04208/assets/x12.png" width="230"/>
    </div>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="A6.F14.2.1.1" style="font-size:90%;">
      Figure 14
     </span>
     :
    </span>
    <span class="ltx_text" id="A6.F14.3.2" style="font-size:90%;">
     Test reward model performances against number of training comparisons by jumppad.
    </span>
   </figcaption>
  </figure>
  <div class="ltx_para" id="A6.p2">
   <p class="ltx_p" id="A6.p2.5">
    Both jumppads show the same trend for the agent encoder, reaching
    <math alttext="\sim 90\%" class="ltx_Math" display="inline" id="A6.p2.1.m1.1">
     <semantics id="A6.p2.1.m1.1a">
      <mrow id="A6.p2.1.m1.1.1" xref="A6.p2.1.m1.1.1.cmml">
       <mi id="A6.p2.1.m1.1.1.2" xref="A6.p2.1.m1.1.1.2.cmml">
       </mi>
       <mo id="A6.p2.1.m1.1.1.1" xref="A6.p2.1.m1.1.1.1.cmml">
        ∼
       </mo>
       <mrow id="A6.p2.1.m1.1.1.3" xref="A6.p2.1.m1.1.1.3.cmml">
        <mn id="A6.p2.1.m1.1.1.3.2" xref="A6.p2.1.m1.1.1.3.2.cmml">
         90
        </mn>
        <mo id="A6.p2.1.m1.1.1.3.1" xref="A6.p2.1.m1.1.1.3.1.cmml">
         %
        </mo>
       </mrow>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A6.p2.1.m1.1b">
       <apply id="A6.p2.1.m1.1.1.cmml" xref="A6.p2.1.m1.1.1">
        <csymbol cd="latexml" id="A6.p2.1.m1.1.1.1.cmml" xref="A6.p2.1.m1.1.1.1">
         similar-to
        </csymbol>
        <csymbol cd="latexml" id="A6.p2.1.m1.1.1.2.cmml" xref="A6.p2.1.m1.1.1.2">
         absent
        </csymbol>
        <apply id="A6.p2.1.m1.1.1.3.cmml" xref="A6.p2.1.m1.1.1.3">
         <csymbol cd="latexml" id="A6.p2.1.m1.1.1.3.1.cmml" xref="A6.p2.1.m1.1.1.3.1">
          percent
         </csymbol>
         <cn id="A6.p2.1.m1.1.1.3.2.cmml" type="integer" xref="A6.p2.1.m1.1.1.3.2">
          90
         </cn>
        </apply>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A6.p2.1.m1.1c">
       \sim 90\%
      </annotation>
     </semantics>
    </math>
    performance for
    <math alttext="100" class="ltx_Math" display="inline" id="A6.p2.2.m2.1">
     <semantics id="A6.p2.2.m2.1a">
      <mn id="A6.p2.2.m2.1.1" xref="A6.p2.2.m2.1.1.cmml">
       100
      </mn>
      <annotation-xml encoding="MathML-Content" id="A6.p2.2.m2.1b">
       <cn id="A6.p2.2.m2.1.1.cmml" type="integer" xref="A6.p2.2.m2.1.1">
        100
       </cn>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A6.p2.2.m2.1c">
       100
      </annotation>
     </semantics>
    </math>
    comparisons and
    <math alttext="100\%" class="ltx_Math" display="inline" id="A6.p2.3.m3.1">
     <semantics id="A6.p2.3.m3.1a">
      <mrow id="A6.p2.3.m3.1.1" xref="A6.p2.3.m3.1.1.cmml">
       <mn id="A6.p2.3.m3.1.1.2" xref="A6.p2.3.m3.1.1.2.cmml">
        100
       </mn>
       <mo id="A6.p2.3.m3.1.1.1" xref="A6.p2.3.m3.1.1.1.cmml">
        %
       </mo>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A6.p2.3.m3.1b">
       <apply id="A6.p2.3.m3.1.1.cmml" xref="A6.p2.3.m3.1.1">
        <csymbol cd="latexml" id="A6.p2.3.m3.1.1.1.cmml" xref="A6.p2.3.m3.1.1.1">
         percent
        </csymbol>
        <cn id="A6.p2.3.m3.1.1.2.cmml" type="integer" xref="A6.p2.3.m3.1.1.2">
         100
        </cn>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A6.p2.3.m3.1c">
       100\%
      </annotation>
     </semantics>
    </math>
    performance by
    <math alttext="500\text{k}" class="ltx_Math" display="inline" id="A6.p2.4.m4.1">
     <semantics id="A6.p2.4.m4.1a">
      <mrow id="A6.p2.4.m4.1.1" xref="A6.p2.4.m4.1.1.cmml">
       <mn id="A6.p2.4.m4.1.1.2" xref="A6.p2.4.m4.1.1.2.cmml">
        500
       </mn>
       <mo id="A6.p2.4.m4.1.1.1" lspace="0em" rspace="0em" xref="A6.p2.4.m4.1.1.1.cmml">
        ​
       </mo>
       <mtext id="A6.p2.4.m4.1.1.3" xref="A6.p2.4.m4.1.1.3a.cmml">
        k
       </mtext>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A6.p2.4.m4.1b">
       <apply id="A6.p2.4.m4.1.1.cmml" xref="A6.p2.4.m4.1.1">
        <times id="A6.p2.4.m4.1.1.1.cmml" xref="A6.p2.4.m4.1.1.1">
        </times>
        <cn id="A6.p2.4.m4.1.1.2.cmml" type="integer" xref="A6.p2.4.m4.1.1.2">
         500
        </cn>
        <ci id="A6.p2.4.m4.1.1.3a.cmml" xref="A6.p2.4.m4.1.1.3">
         <mtext id="A6.p2.4.m4.1.1.3.cmml" xref="A6.p2.4.m4.1.1.3">
          k
         </mtext>
        </ci>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A6.p2.4.m4.1c">
       500\text{k}
      </annotation>
     </semantics>
    </math>
    comparisons. The random encoder is high variance, but generally increases in performance with training comparisons for the left jumppad, while decreasing in performance for the right jumppad. This surprising trend for the right jumppad could be because the reward model begins to overfit to the imbalanced trajectories discussed in Section
    <a class="ltx_ref" href="#S3.SS5.SSS2" title="3.5.2 Aligning Agent Towards Right Jumppad ‣ 3.5 Aligning the Fine-tuned Model with the Reward Model ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      3.5.2
     </span>
    </a>
    and shown in Figure
    <a class="ltx_ref" href="#S3.F8" title="Figure 8 ‣ 3.5.2 Aligning Agent Towards Right Jumppad ‣ 3.5 Aligning the Fine-tuned Model with the Reward Model ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      8
     </span>
    </a>
    . In other words, the reward model misinterprets the preferences and instead learns to prefer shorter trajectories that begin at the bottom left spawn point (from which 84% of the successful right-going trajectories arise) leading to the
    <math alttext="\sim 20\%" class="ltx_Math" display="inline" id="A6.p2.5.m5.1">
     <semantics id="A6.p2.5.m5.1a">
      <mrow id="A6.p2.5.m5.1.1" xref="A6.p2.5.m5.1.1.cmml">
       <mi id="A6.p2.5.m5.1.1.2" xref="A6.p2.5.m5.1.1.2.cmml">
       </mi>
       <mo id="A6.p2.5.m5.1.1.1" xref="A6.p2.5.m5.1.1.1.cmml">
        ∼
       </mo>
       <mrow id="A6.p2.5.m5.1.1.3" xref="A6.p2.5.m5.1.1.3.cmml">
        <mn id="A6.p2.5.m5.1.1.3.2" xref="A6.p2.5.m5.1.1.3.2.cmml">
         20
        </mn>
        <mo id="A6.p2.5.m5.1.1.3.1" xref="A6.p2.5.m5.1.1.3.1.cmml">
         %
        </mo>
       </mrow>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A6.p2.5.m5.1b">
       <apply id="A6.p2.5.m5.1.1.cmml" xref="A6.p2.5.m5.1.1">
        <csymbol cd="latexml" id="A6.p2.5.m5.1.1.1.cmml" xref="A6.p2.5.m5.1.1.1">
         similar-to
        </csymbol>
        <csymbol cd="latexml" id="A6.p2.5.m5.1.1.2.cmml" xref="A6.p2.5.m5.1.1.2">
         absent
        </csymbol>
        <apply id="A6.p2.5.m5.1.1.3.cmml" xref="A6.p2.5.m5.1.1.3">
         <csymbol cd="latexml" id="A6.p2.5.m5.1.1.3.1.cmml" xref="A6.p2.5.m5.1.1.3.1">
          percent
         </csymbol>
         <cn id="A6.p2.5.m5.1.1.3.2.cmml" type="integer" xref="A6.p2.5.m5.1.1.3.2">
          20
         </cn>
        </apply>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A6.p2.5.m5.1c">
       \sim 20\%
      </annotation>
     </semantics>
    </math>
    accuracy that we see for higher numbers of comparisons. The agent encoder based reward model is better able to distinguish these trajectories and infer the true underlying preference that better fits the data.
   </p>
  </div>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
 <section class="ltx_appendix" id="A7">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix G
   </span>
   Alignment of Base Model for Comparison with Fine-Tuned Model
  </h2>
  <div class="ltx_para" id="A7.p1">
   <p class="ltx_p" id="A7.p1.1">
    To understand how trajectory diversity affects alignment, we also consider aligning the base agent using the same reward models (trained on the fine-tuned agent). By plotting the successful trajectories that reach the left and right jumppads we see that the base agent (left) has a greater diversity than the fine-tuned agent (right). However, we note that the success rate for reaching the left and right jumppads is lower for the base model (as in Figures
    <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3.1 Training a Base Imitation Learning Model ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    and
    <a class="ltx_ref" href="#S3.F4" title="Figure 4 ‣ 3.2 Supervised Fine-Tuning on Task Relevant Dataset ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      4
     </span>
    </a>
    ), so there are fewer trajectories.
   </p>
  </div>
  <figure class="ltx_figure" id="A7.F15">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_4">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="228" id="A7.F15.g1" src="/html/2406.04208/assets/figures/appendix_figures/Train_Base_JP1_trajectories.png" width="144"/>
    </div>
    <div class="ltx_flex_cell ltx_flex_size_4">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="228" id="A7.F15.g2" src="/html/2406.04208/assets/figures/appendix_figures/Train_Base_JP3_trajectories.png" width="144"/>
    </div>
    <div class="ltx_flex_cell ltx_flex_size_4">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="228" id="A7.F15.g3" src="/html/2406.04208/assets/figures/Train_FineTuned_JP1_trajectories.png" width="144"/>
    </div>
    <div class="ltx_flex_cell ltx_flex_size_4">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" height="228" id="A7.F15.g4" src="/html/2406.04208/assets/figures/Train_FineTuned_JP3_trajectories.png" width="144"/>
    </div>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="A7.F15.2.1.1" style="font-size:90%;">
      Figure 15
     </span>
     :
    </span>
    <span class="ltx_text" id="A7.F15.3.2" style="font-size:90%;">
     Base agent (left) and fine-tuned agent (right) trajectories for which the agents successfully reached the left (red) or right (green) jumppads. While fine-tuning provides a greater success rate, enabling more efficient preference labelling, it also reduces diversity of trajectories.
    </span>
   </figcaption>
  </figure>
  <div class="ltx_para" id="A7.p2">
   <p class="ltx_p" id="A7.p2.1">
    We now align the base agent to reach the left and right jumppads using the reward models trained on the fine-tuned agent, as shown below. We find that the agent can be aligned more symmetrically with both the left and the right jumppads, although alignment with the left jumppad still has a slightly better performance. However, in comparison to alignment of the fine-tuned agent (Figures
    <a class="ltx_ref" href="#S3.F6" title="Figure 6 ‣ 3.5.1 Aligning Agent Towards Left Jumppad ‣ 3.5 Aligning the Fine-tuned Model with the Reward Model ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      6
     </span>
    </a>
    and
    <a class="ltx_ref" href="#S3.F8" title="Figure 8 ‣ 3.5.2 Aligning Agent Towards Right Jumppad ‣ 3.5 Aligning the Fine-tuned Model with the Reward Model ‣ 3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      8
     </span>
    </a>
    ), we see that the alignment is much slower, starting at a lower success rate, not increasing as quickly during training, and reaching a lower final success rate. As before, we see the same general trend that higher accuracy reward models (using more preference data) lead to better alignment.
   </p>
  </div>
  <figure class="ltx_figure" id="A7.F17">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_2">
     <figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A7.F17.1" style="width:212.5pt;">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="547" id="A7.F17.1.g1" src="/html/2406.04208/assets/figures/appendix_figures/Test_Align_Left_Base_AllRMs.png" width="538"/>
      <figcaption class="ltx_caption ltx_centering">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="A7.F17.1.1.1.1" style="font-size:90%;">
         Figure 16
        </span>
        :
       </span>
       <span class="ltx_text" id="A7.F17.1.2.2" style="font-size:90%;">
        Left jumppad success rate for base agent using fine-tuned agent reward models.
       </span>
      </figcaption>
     </figure>
    </div>
    <div class="ltx_flex_cell ltx_flex_size_2">
     <figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A7.F17.2" style="width:212.5pt;">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="547" id="A7.F17.2.g1" src="/html/2406.04208/assets/figures/appendix_figures/Test_Align_Right_Base_AllRMs.png" width="538"/>
      <figcaption class="ltx_caption ltx_centering">
       <span class="ltx_tag ltx_tag_figure">
        <span class="ltx_text" id="A7.F17.2.1.1.1" style="font-size:90%;">
         Figure 17
        </span>
        :
       </span>
       <span class="ltx_text" id="A7.F17.2.2.2" style="font-size:90%;">
        Right jumppad success rate for base agent using fine-tuned agent reward models
       </span>
      </figcaption>
     </figure>
    </div>
   </div>
  </figure>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
 <section class="ltx_appendix" id="A8">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix H
   </span>
   Final Aligned Agent Jumppad Distributions
  </h2>
  <div class="ltx_para" id="A8.p1">
   <p class="ltx_p" id="A8.p1.1">
    Jumppad distributions for our final agents aligned to go left and right using preference fine-tuning and online alignment with reward models trained on 500k preferences are shown below.
   </p>
  </div>
  <div class="ltx_para" id="A8.p2">
   <p class="ltx_p" id="A8.p2.1">
    First we partially align our agents with preference fine-tuning using our (500k comparison) reward models, so that the behaviour distributions are closer to the desired behaviour distributions to reduce the alignment required online, as shown in Figure
    <a class="ltx_ref" href="#A8.F18" title="Figure 18 ‣ Appendix H Final Aligned Agent Jumppad Distributions ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      18
     </span>
    </a>
    .
   </p>
  </div>
  <figure class="ltx_figure" id="A8.F18">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_2">
     <div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A8.F18.1" style="width:212.5pt;">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="262" id="A8.F18.1.g1" src="/html/2406.04208/assets/x13.png" width="461"/>
     </div>
    </div>
    <div class="ltx_flex_cell ltx_flex_size_2">
     <div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A8.F18.2" style="width:212.5pt;">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="258" id="A8.F18.2.g1" src="/html/2406.04208/assets/x14.png" width="461"/>
     </div>
    </div>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="A8.F18.4.1.1" style="font-size:90%;">
      Figure 18
     </span>
     :
    </span>
    <span class="ltx_text" id="A8.F18.5.2" style="font-size:90%;">
     Left and right jumppad success rate for agents partially aligned with preference fine-tuning with reward models trained on 500k preferences.
    </span>
   </figcaption>
  </figure>
  <div class="ltx_para" id="A8.p3">
   <p class="ltx_p" id="A8.p3.1">
    We see that preference fine-tuning starts to align our agents towards the desired behaviour, but does not fully align our agents. Therefore we then perform online reinforcement learning using our (500k comparison) reward models until our agents are fully aligned.
   </p>
  </div>
  <figure class="ltx_figure" id="A8.F19">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_2">
     <div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A8.F19.1" style="width:212.5pt;">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="266" id="A8.F19.1.g1" src="/html/2406.04208/assets/x15.png" width="461"/>
     </div>
    </div>
    <div class="ltx_flex_cell ltx_flex_size_2">
     <div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A8.F19.2" style="width:212.5pt;">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="265" id="A8.F19.2.g1" src="/html/2406.04208/assets/x16.png" width="461"/>
     </div>
    </div>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     <span class="ltx_text" id="A8.F19.4.1.1" style="font-size:90%;">
      Figure 19
     </span>
     :
    </span>
    <span class="ltx_text" id="A8.F19.5.2" style="font-size:90%;">
     Left and right jumppad success rate for our fully aligned fine-tuned agents using preference fine-tuning and online reinforcement learning with reward models trained on 500k preferences.
    </span>
   </figcaption>
  </figure>
  <div class="ltx_para" id="A8.p4">
   <p class="ltx_p" id="A8.p4.1">
    Final evaluation shows that our agents have now been effectively fully aligned with our desired behaviour.
   </p>
  </div>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
 <section class="ltx_appendix" id="A9">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix I
   </span>
   Limitations and Further Work
  </h2>
  <div class="ltx_para" id="A9.p1">
   <p class="ltx_p" id="A9.p1.1">
    This work provides a proof of concept of an approach for training agents to act as desired on a complex 3D environment. For our procedure to be applied in practice by game designers or in other domains such as robotics, the efficiency of this pipeline will be essential.
   </p>
  </div>
  <div class="ltx_para" id="A9.p2">
   <p class="ltx_p" id="A9.p2.14">
    One limitation of our approach that we discuss in Section
    <a class="ltx_ref" href="#S5" title="5 Limitations and Broader Impact ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      5
     </span>
    </a>
    is that we utilize synthetic preferences. While this may be a realistic option for many use cases, real human feedback from a game designer will be required in general. As well as potentially introducing noise to the preference labels, this process could be costly in terms of human time, especially for agents on modern console games which must often be run in real-time in order to be rendered. Therefore supervised fine-tuning on relevant behaviours to improve preference labelling efficiency could help to reduce this human time requirement. However, knowledge transfer from training LLMs could again be relevant here. Recent work on providing human feedback for LLMs has used a hybrid form of feedback that combines preference and evaluative feedback, in which responses are grouped into a batch of size
    <math alttext="N" class="ltx_Math" display="inline" id="A9.p2.1.m1.1">
     <semantics id="A9.p2.1.m1.1a">
      <mi id="A9.p2.1.m1.1.1" xref="A9.p2.1.m1.1.1.cmml">
       N
      </mi>
      <annotation-xml encoding="MathML-Content" id="A9.p2.1.m1.1b">
       <ci id="A9.p2.1.m1.1.1.cmml" xref="A9.p2.1.m1.1.1">
        𝑁
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A9.p2.1.m1.1c">
       N
      </annotation>
     </semantics>
    </math>
    and simultaneously compared on a preference scale of size
    <math alttext="P" class="ltx_Math" display="inline" id="A9.p2.2.m2.1">
     <semantics id="A9.p2.2.m2.1a">
      <mi id="A9.p2.2.m2.1.1" xref="A9.p2.2.m2.1.1.cmml">
       P
      </mi>
      <annotation-xml encoding="MathML-Content" id="A9.p2.2.m2.1b">
       <ci id="A9.p2.2.m2.1.1.cmml" xref="A9.p2.2.m2.1.1">
        𝑃
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A9.p2.2.m2.1c">
       P
      </annotation>
     </semantics>
    </math>
    , which enables larger numbers of comparisons to be extracted from a given number of responses, improving the time efficiency of providing feedback. For example, InstructGPT
    <cite class="ltx_cite ltx_citemacro_citep">
     (Ouyang et al.,
     <a class="ltx_ref" href="#bib.bib40" title="">
      2022
     </a>
     )
    </cite>
    uses
    <math alttext="4\leq N\leq 9" class="ltx_Math" display="inline" id="A9.p2.3.m3.1">
     <semantics id="A9.p2.3.m3.1a">
      <mrow id="A9.p2.3.m3.1.1" xref="A9.p2.3.m3.1.1.cmml">
       <mn id="A9.p2.3.m3.1.1.2" xref="A9.p2.3.m3.1.1.2.cmml">
        4
       </mn>
       <mo id="A9.p2.3.m3.1.1.3" xref="A9.p2.3.m3.1.1.3.cmml">
        ≤
       </mo>
       <mi id="A9.p2.3.m3.1.1.4" xref="A9.p2.3.m3.1.1.4.cmml">
        N
       </mi>
       <mo id="A9.p2.3.m3.1.1.5" xref="A9.p2.3.m3.1.1.5.cmml">
        ≤
       </mo>
       <mn id="A9.p2.3.m3.1.1.6" xref="A9.p2.3.m3.1.1.6.cmml">
        9
       </mn>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A9.p2.3.m3.1b">
       <apply id="A9.p2.3.m3.1.1.cmml" xref="A9.p2.3.m3.1.1">
        <and id="A9.p2.3.m3.1.1a.cmml" xref="A9.p2.3.m3.1.1">
        </and>
        <apply id="A9.p2.3.m3.1.1b.cmml" xref="A9.p2.3.m3.1.1">
         <leq id="A9.p2.3.m3.1.1.3.cmml" xref="A9.p2.3.m3.1.1.3">
         </leq>
         <cn id="A9.p2.3.m3.1.1.2.cmml" type="integer" xref="A9.p2.3.m3.1.1.2">
          4
         </cn>
         <ci id="A9.p2.3.m3.1.1.4.cmml" xref="A9.p2.3.m3.1.1.4">
          𝑁
         </ci>
        </apply>
        <apply id="A9.p2.3.m3.1.1c.cmml" xref="A9.p2.3.m3.1.1">
         <leq id="A9.p2.3.m3.1.1.5.cmml" xref="A9.p2.3.m3.1.1.5">
         </leq>
         <share href="#A9.p2.3.m3.1.1.4.cmml" id="A9.p2.3.m3.1.1d.cmml" xref="A9.p2.3.m3.1.1">
         </share>
         <cn id="A9.p2.3.m3.1.1.6.cmml" type="integer" xref="A9.p2.3.m3.1.1.6">
          9
         </cn>
        </apply>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A9.p2.3.m3.1c">
       4\leq N\leq 9
      </annotation>
     </semantics>
    </math>
    ,
    <math alttext="P=7" class="ltx_Math" display="inline" id="A9.p2.4.m4.1">
     <semantics id="A9.p2.4.m4.1a">
      <mrow id="A9.p2.4.m4.1.1" xref="A9.p2.4.m4.1.1.cmml">
       <mi id="A9.p2.4.m4.1.1.2" xref="A9.p2.4.m4.1.1.2.cmml">
        P
       </mi>
       <mo id="A9.p2.4.m4.1.1.1" xref="A9.p2.4.m4.1.1.1.cmml">
        =
       </mo>
       <mn id="A9.p2.4.m4.1.1.3" xref="A9.p2.4.m4.1.1.3.cmml">
        7
       </mn>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A9.p2.4.m4.1b">
       <apply id="A9.p2.4.m4.1.1.cmml" xref="A9.p2.4.m4.1.1">
        <eq id="A9.p2.4.m4.1.1.1.cmml" xref="A9.p2.4.m4.1.1.1">
        </eq>
        <ci id="A9.p2.4.m4.1.1.2.cmml" xref="A9.p2.4.m4.1.1.2">
         𝑃
        </ci>
        <cn id="A9.p2.4.m4.1.1.3.cmml" type="integer" xref="A9.p2.4.m4.1.1.3">
         7
        </cn>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A9.p2.4.m4.1c">
       P=7
      </annotation>
     </semantics>
    </math>
    while Llama2
    <cite class="ltx_cite ltx_citemacro_citep">
     (Touvron et al.,
     <a class="ltx_ref" href="#bib.bib52" title="">
      2023
     </a>
     )
    </cite>
    uses
    <math alttext="N=2" class="ltx_Math" display="inline" id="A9.p2.5.m5.1">
     <semantics id="A9.p2.5.m5.1a">
      <mrow id="A9.p2.5.m5.1.1" xref="A9.p2.5.m5.1.1.cmml">
       <mi id="A9.p2.5.m5.1.1.2" xref="A9.p2.5.m5.1.1.2.cmml">
        N
       </mi>
       <mo id="A9.p2.5.m5.1.1.1" xref="A9.p2.5.m5.1.1.1.cmml">
        =
       </mo>
       <mn id="A9.p2.5.m5.1.1.3" xref="A9.p2.5.m5.1.1.3.cmml">
        2
       </mn>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A9.p2.5.m5.1b">
       <apply id="A9.p2.5.m5.1.1.cmml" xref="A9.p2.5.m5.1.1">
        <eq id="A9.p2.5.m5.1.1.1.cmml" xref="A9.p2.5.m5.1.1.1">
        </eq>
        <ci id="A9.p2.5.m5.1.1.2.cmml" xref="A9.p2.5.m5.1.1.2">
         𝑁
        </ci>
        <cn id="A9.p2.5.m5.1.1.3.cmml" type="integer" xref="A9.p2.5.m5.1.1.3">
         2
        </cn>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A9.p2.5.m5.1c">
       N=2
      </annotation>
     </semantics>
    </math>
    ,
    <math alttext="P=4" class="ltx_Math" display="inline" id="A9.p2.6.m6.1">
     <semantics id="A9.p2.6.m6.1a">
      <mrow id="A9.p2.6.m6.1.1" xref="A9.p2.6.m6.1.1.cmml">
       <mi id="A9.p2.6.m6.1.1.2" xref="A9.p2.6.m6.1.1.2.cmml">
        P
       </mi>
       <mo id="A9.p2.6.m6.1.1.1" xref="A9.p2.6.m6.1.1.1.cmml">
        =
       </mo>
       <mn id="A9.p2.6.m6.1.1.3" xref="A9.p2.6.m6.1.1.3.cmml">
        4
       </mn>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A9.p2.6.m6.1b">
       <apply id="A9.p2.6.m6.1.1.cmml" xref="A9.p2.6.m6.1.1">
        <eq id="A9.p2.6.m6.1.1.1.cmml" xref="A9.p2.6.m6.1.1.1">
        </eq>
        <ci id="A9.p2.6.m6.1.1.2.cmml" xref="A9.p2.6.m6.1.1.2">
         𝑃
        </ci>
        <cn id="A9.p2.6.m6.1.1.3.cmml" type="integer" xref="A9.p2.6.m6.1.1.3">
         4
        </cn>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A9.p2.6.m6.1c">
       P=4
      </annotation>
     </semantics>
    </math>
    . As
    <math alttext="N" class="ltx_Math" display="inline" id="A9.p2.7.m7.1">
     <semantics id="A9.p2.7.m7.1a">
      <mi id="A9.p2.7.m7.1.1" xref="A9.p2.7.m7.1.1.cmml">
       N
      </mi>
      <annotation-xml encoding="MathML-Content" id="A9.p2.7.m7.1b">
       <ci id="A9.p2.7.m7.1.1.cmml" xref="A9.p2.7.m7.1.1">
        𝑁
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A9.p2.7.m7.1c">
       N
      </annotation>
     </semantics>
    </math>
    and
    <math alttext="P" class="ltx_Math" display="inline" id="A9.p2.8.m8.1">
     <semantics id="A9.p2.8.m8.1a">
      <mi id="A9.p2.8.m8.1.1" xref="A9.p2.8.m8.1.1.cmml">
       P
      </mi>
      <annotation-xml encoding="MathML-Content" id="A9.p2.8.m8.1b">
       <ci id="A9.p2.8.m8.1.1.cmml" xref="A9.p2.8.m8.1.1">
        𝑃
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A9.p2.8.m8.1c">
       P
      </annotation>
     </semantics>
    </math>
    increase, more information can be extracted from the provided human feedback. For example, with
    <math alttext="N=5" class="ltx_Math" display="inline" id="A9.p2.9.m9.1">
     <semantics id="A9.p2.9.m9.1a">
      <mrow id="A9.p2.9.m9.1.1" xref="A9.p2.9.m9.1.1.cmml">
       <mi id="A9.p2.9.m9.1.1.2" xref="A9.p2.9.m9.1.1.2.cmml">
        N
       </mi>
       <mo id="A9.p2.9.m9.1.1.1" xref="A9.p2.9.m9.1.1.1.cmml">
        =
       </mo>
       <mn id="A9.p2.9.m9.1.1.3" xref="A9.p2.9.m9.1.1.3.cmml">
        5
       </mn>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A9.p2.9.m9.1b">
       <apply id="A9.p2.9.m9.1.1.cmml" xref="A9.p2.9.m9.1.1">
        <eq id="A9.p2.9.m9.1.1.1.cmml" xref="A9.p2.9.m9.1.1.1">
        </eq>
        <ci id="A9.p2.9.m9.1.1.2.cmml" xref="A9.p2.9.m9.1.1.2">
         𝑁
        </ci>
        <cn id="A9.p2.9.m9.1.1.3.cmml" type="integer" xref="A9.p2.9.m9.1.1.3">
         5
        </cn>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A9.p2.9.m9.1c">
       N=5
      </annotation>
     </semantics>
    </math>
    ,
    <math alttext="P=5" class="ltx_Math" display="inline" id="A9.p2.10.m10.1">
     <semantics id="A9.p2.10.m10.1a">
      <mrow id="A9.p2.10.m10.1.1" xref="A9.p2.10.m10.1.1.cmml">
       <mi id="A9.p2.10.m10.1.1.2" xref="A9.p2.10.m10.1.1.2.cmml">
        P
       </mi>
       <mo id="A9.p2.10.m10.1.1.1" xref="A9.p2.10.m10.1.1.1.cmml">
        =
       </mo>
       <mn id="A9.p2.10.m10.1.1.3" xref="A9.p2.10.m10.1.1.3.cmml">
        5
       </mn>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A9.p2.10.m10.1b">
       <apply id="A9.p2.10.m10.1.1.cmml" xref="A9.p2.10.m10.1.1">
        <eq id="A9.p2.10.m10.1.1.1.cmml" xref="A9.p2.10.m10.1.1.1">
        </eq>
        <ci id="A9.p2.10.m10.1.1.2.cmml" xref="A9.p2.10.m10.1.1.2">
         𝑃
        </ci>
        <cn id="A9.p2.10.m10.1.1.3.cmml" type="integer" xref="A9.p2.10.m10.1.1.3">
         5
        </cn>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A9.p2.10.m10.1c">
       P=5
      </annotation>
     </semantics>
    </math>
    and assuming no category duplication (no trajectories are considered equal),
    <math alttext="{5\choose 2}=10" class="ltx_Math" display="inline" id="A9.p2.11.m11.3">
     <semantics id="A9.p2.11.m11.3a">
      <mrow id="A9.p2.11.m11.3.4" xref="A9.p2.11.m11.3.4.cmml">
       <mrow id="A9.p2.11.m11.3.3.5" xref="A9.p2.11.m11.3.3.4.cmml">
        <mo id="A9.p2.11.m11.3.3.5.1" xref="A9.p2.11.m11.1.1.1.1.1.cmml">
         (
        </mo>
        <mfrac id="A9.p2.11.m11.3.3.3.3" linethickness="0pt" xref="A9.p2.11.m11.3.3.4.cmml">
         <mn id="A9.p2.11.m11.2.2.2.2.2" xref="A9.p2.11.m11.2.2.2.2.2.cmml">
          5
         </mn>
         <mn id="A9.p2.11.m11.3.3.3.3.3" xref="A9.p2.11.m11.3.3.3.3.3.cmml">
          2
         </mn>
        </mfrac>
        <mo id="A9.p2.11.m11.3.3.5.2" xref="A9.p2.11.m11.1.1.1.1.1.cmml">
         )
        </mo>
       </mrow>
       <mo id="A9.p2.11.m11.3.4.1" xref="A9.p2.11.m11.3.4.1.cmml">
        =
       </mo>
       <mn id="A9.p2.11.m11.3.4.2" xref="A9.p2.11.m11.3.4.2.cmml">
        10
       </mn>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A9.p2.11.m11.3b">
       <apply id="A9.p2.11.m11.3.4.cmml" xref="A9.p2.11.m11.3.4">
        <eq id="A9.p2.11.m11.3.4.1.cmml" xref="A9.p2.11.m11.3.4.1">
        </eq>
        <apply id="A9.p2.11.m11.3.3.4.cmml" xref="A9.p2.11.m11.3.3.5">
         <csymbol cd="latexml" id="A9.p2.11.m11.1.1.1.1.1.cmml" xref="A9.p2.11.m11.3.3.5.1">
          binomial
         </csymbol>
         <cn id="A9.p2.11.m11.2.2.2.2.2.cmml" type="integer" xref="A9.p2.11.m11.2.2.2.2.2">
          5
         </cn>
         <cn id="A9.p2.11.m11.3.3.3.3.3.cmml" type="integer" xref="A9.p2.11.m11.3.3.3.3.3">
          2
         </cn>
        </apply>
        <cn id="A9.p2.11.m11.3.4.2.cmml" type="integer" xref="A9.p2.11.m11.3.4.2">
         10
        </cn>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A9.p2.11.m11.3c">
       {5\choose 2}=10
      </annotation>
     </semantics>
    </math>
    comparisons can be extracted, which is equivalent to 2 bits of information per trajectory watched, compared to just 0.5 bits per trajectory for default pairwise comparisons with
    <math alttext="N=2" class="ltx_Math" display="inline" id="A9.p2.12.m12.1">
     <semantics id="A9.p2.12.m12.1a">
      <mrow id="A9.p2.12.m12.1.1" xref="A9.p2.12.m12.1.1.cmml">
       <mi id="A9.p2.12.m12.1.1.2" xref="A9.p2.12.m12.1.1.2.cmml">
        N
       </mi>
       <mo id="A9.p2.12.m12.1.1.1" xref="A9.p2.12.m12.1.1.1.cmml">
        =
       </mo>
       <mn id="A9.p2.12.m12.1.1.3" xref="A9.p2.12.m12.1.1.3.cmml">
        2
       </mn>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A9.p2.12.m12.1b">
       <apply id="A9.p2.12.m12.1.1.cmml" xref="A9.p2.12.m12.1.1">
        <eq id="A9.p2.12.m12.1.1.1.cmml" xref="A9.p2.12.m12.1.1.1">
        </eq>
        <ci id="A9.p2.12.m12.1.1.2.cmml" xref="A9.p2.12.m12.1.1.2">
         𝑁
        </ci>
        <cn id="A9.p2.12.m12.1.1.3.cmml" type="integer" xref="A9.p2.12.m12.1.1.3">
         2
        </cn>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A9.p2.12.m12.1c">
       N=2
      </annotation>
     </semantics>
    </math>
    ,
    <math alttext="P=2" class="ltx_Math" display="inline" id="A9.p2.13.m13.1">
     <semantics id="A9.p2.13.m13.1a">
      <mrow id="A9.p2.13.m13.1.1" xref="A9.p2.13.m13.1.1.cmml">
       <mi id="A9.p2.13.m13.1.1.2" xref="A9.p2.13.m13.1.1.2.cmml">
        P
       </mi>
       <mo id="A9.p2.13.m13.1.1.1" xref="A9.p2.13.m13.1.1.1.cmml">
        =
       </mo>
       <mn id="A9.p2.13.m13.1.1.3" xref="A9.p2.13.m13.1.1.3.cmml">
        2
       </mn>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A9.p2.13.m13.1b">
       <apply id="A9.p2.13.m13.1.1.cmml" xref="A9.p2.13.m13.1.1">
        <eq id="A9.p2.13.m13.1.1.1.cmml" xref="A9.p2.13.m13.1.1.1">
        </eq>
        <ci id="A9.p2.13.m13.1.1.2.cmml" xref="A9.p2.13.m13.1.1.2">
         𝑃
        </ci>
        <cn id="A9.p2.13.m13.1.1.3.cmml" type="integer" xref="A9.p2.13.m13.1.1.3">
         2
        </cn>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A9.p2.13.m13.1c">
       P=2
      </annotation>
     </semantics>
    </math>
    . This results in a
    <math alttext="4\times" class="ltx_math_unparsed" display="inline" id="A9.p2.14.m14.1">
     <semantics id="A9.p2.14.m14.1a">
      <mrow id="A9.p2.14.m14.1b">
       <mn id="A9.p2.14.m14.1.1">
        4
       </mn>
       <mo id="A9.p2.14.m14.1.2" lspace="0.222em">
        ×
       </mo>
      </mrow>
      <annotation encoding="application/x-tex" id="A9.p2.14.m14.1c">
       4\times
      </annotation>
     </semantics>
    </math>
    improvement in feedback efficiency for human labellers, at the cost of potential label noise due to the additional mental overhead required. Similar strategies could be applied to providing feedback to agents to make the process more time efficient.
   </p>
  </div>
  <div class="ltx_para" id="A9.p3">
   <p class="ltx_p" id="A9.p3.1">
    Another limitation we discuss is that running agents online in modern console games can be challenging. While our work demonstrated surprising efficiency for fine-tuning pre-trained agents with RL, for more complex behaviours and tasks this may become prohibitively expensive. Fortunately, developments in RLHF for language models can also be utilized here. We demonstrated in Section
    <a class="ltx_ref" href="#S3" title="3 Implementation and Analysis ‣ Aligning Agents like Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    that an intial step of preference fine-tuning could improve the efficiency of alignment, and explained that this corresponds to Reinforced Self-Training (ReST)
    <cite class="ltx_cite ltx_citemacro_citep">
     (Gulcehre et al.,
     <a class="ltx_ref" href="#bib.bib22" title="">
      2023
     </a>
     )
    </cite>
    with a single iteration. However, the full procedure using multiple iterations could be used to further improve the sample efficiency of aligning with preferences while maintaining the benefits of online exploration. Other recent work has investigated aligning models completely offline. Direct Preference Optimisation
    <cite class="ltx_cite ltx_citemacro_citep">
     (Rafailov et al.,
     <a class="ltx_ref" href="#bib.bib44" title="">
      2023
     </a>
     )
    </cite>
    provides an approach for optimising a model to align with preferences without the need for a reward model or online training, while
    <cite class="ltx_cite ltx_citemacro_citep">
     (Hu et al.,
     <a class="ltx_ref" href="#bib.bib27" title="">
      2023
     </a>
     )
    </cite>
    uses offline reinforcement learning with pre-generated samples and rewards. Additionally, efficient fine-tuning strategies such as LoRA
    <cite class="ltx_cite ltx_citemacro_citep">
     (Hu et al.,
     <a class="ltx_ref" href="#bib.bib26" title="">
      2021
     </a>
     )
    </cite>
    could be used to reduce the hardware requirements for the fine-tuning of a large base model for the individual applications of game designers. These new approaches provide promising directions for further work in the context of agents to improve the efficiency or potentially even remove the need for online training completely. However, the extent to which active online learning is required for true generality is still an open question.
   </p>
  </div>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
</article>
