<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Survey on Data Synthesis and Augmentation for Large Language Models</title>
<!--Generated on Wed Oct 16 16:10:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.12896v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S1" title="In A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S2" title="In A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Taxonomy</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S2.SS1" title="In 2. Taxonomy ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data Augmentation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S2.SS1.SSS1" title="In 2.1. Data Augmentation ‣ 2. Taxonomy ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span><span class="ltx_text ltx_font_bold">Data Labeling</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S2.SS1.SSS2" title="In 2.1. Data Augmentation ‣ 2. Taxonomy ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span><span class="ltx_text ltx_font_bold">Data Reformation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S2.SS1.SSS3" title="In 2.1. Data Augmentation ‣ 2. Taxonomy ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span><span class="ltx_text ltx_font_bold">Co-Annotation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S2.SS2" title="In 2. Taxonomy ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Data Synthesis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S2.SS2.SSS1" title="In 2.2. Data Synthesis ‣ 2. Taxonomy ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span><span class="ltx_text ltx_font_bold">General Model Distillation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S2.SS2.SSS2" title="In 2.2. Data Synthesis ‣ 2. Taxonomy ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span><span class="ltx_text ltx_font_bold">Domain Model Distillation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S2.SS2.SSS3" title="In 2.2. Data Synthesis ‣ 2. Taxonomy ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span><span class="ltx_text ltx_font_bold">Model Self-Improvement</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3" title="In A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Data Synthesis and Augmentation in the Full Lifecycle of LLM</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS1" title="In 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data Preparation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS1.SSS1" title="In 3.1. Data Preparation ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span><span class="ltx_text ltx_font_bold">General Model Distillation.</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS1.SSS2" title="In 3.1. Data Preparation ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span><span class="ltx_text ltx_font_bold">Data Augmentation.</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS2" title="In 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Pre-Training</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS2.SSS1" title="In 3.2. Pre-Training ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span><span class="ltx_text ltx_font_bold">Model Self-Improvement.</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS2.SSS2" title="In 3.2. Pre-Training ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span><span class="ltx_text ltx_font_bold">General Model Distillation.</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS2.SSS3" title="In 3.2. Pre-Training ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span><span class="ltx_text ltx_font_bold">Data Augmentation.</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS3" title="In 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Fine-Tuning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS3.SSS1" title="In 3.3. Fine-Tuning ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span><span class="ltx_text ltx_font_bold">Model Self-Improvement.</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS3.SSS2" title="In 3.3. Fine-Tuning ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span><span class="ltx_text ltx_font_bold">General Model Distillation.</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS3.SSS3" title="In 3.3. Fine-Tuning ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span><span class="ltx_text ltx_font_bold">Data Augmentation.</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS4" title="In 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Instruction-Tuning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS4.SSS1" title="In 3.4. Instruction-Tuning ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>General Model Distillation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS4.SSS2" title="In 3.4. Instruction-Tuning ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Model Self-Improvement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS4.SSS3" title="In 3.4. Instruction-Tuning ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>Data Augmentation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS5" title="In 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Preference Alignment</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS5.SSS1" title="In 3.5. Preference Alignment ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.1 </span><span class="ltx_text ltx_font_bold">General Model Distillation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS5.SSS2" title="In 3.5. Preference Alignment ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.2 </span><span class="ltx_text ltx_font_bold">Domain Model Distillation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS5.SSS3" title="In 3.5. Preference Alignment ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.3 </span><span class="ltx_text ltx_font_bold">Model Self-Improvement</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS5.SSS4" title="In 3.5. Preference Alignment ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.4 </span><span class="ltx_text ltx_font_bold">Data Augmentation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS6" title="In 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Applications</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS6.SSS1" title="In 3.6. Applications ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.1 </span><span class="ltx_text ltx_font_bold">Math</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS6.SSS2" title="In 3.6. Applications ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.2 </span><span class="ltx_text ltx_font_bold">Science</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS6.SSS3" title="In 3.6. Applications ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.3 </span><span class="ltx_text ltx_font_bold">Code</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS6.SSS4" title="In 3.6. Applications ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.4 </span><span class="ltx_text ltx_font_bold">Medical</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS6.SSS5" title="In 3.6. Applications ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.5 </span><span class="ltx_text ltx_font_bold">Law</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS6.SSS6" title="In 3.6. Applications ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.6 </span><span class="ltx_text ltx_font_bold">Others</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S4" title="In A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Functionality</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S4.SS1" title="In 4. Functionality ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Understanding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S4.SS2" title="In 4. Functionality ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Logic</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S4.SS3" title="In 4. Functionality ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Memory</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S4.SS4" title="In 4. Functionality ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S5" title="In A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Challenges and Limitations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S5.SS1" title="In 5. Challenges and Limitations ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Synthesizing and Augmenting Method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S5.SS2" title="In 5. Challenges and Limitations ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Data Quality</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S5.SS3" title="In 5. Challenges and Limitations ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Impact of Data Synthesis and Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S5.SS4" title="In 5. Challenges and Limitations ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Impact on Different Applications and Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S5.SS5" title="In 5. Challenges and Limitations ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Future Directions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S6" title="In A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">A Survey on Data Synthesis and Augmentation for Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ke Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:onecall@buaa.edu.cn">onecall@buaa.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Hangzhou Innovation Institute, Beihang University</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2"></span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiahui Zhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:zhujh224@buaa.edu.cn">zhujh224@buaa.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Hangzhou Innovation Institute, Beihang University</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2"></span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Minjie Ren
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:rmj%CB%99rmj@buaa.edu.cn">rmj˙rmj@buaa.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Hangzhou Innovation Institute, Beihang University</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2"></span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zeming Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:zmliu@buaa.edu.cn">zmliu@buaa.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">State Key Laboratory of Virtual Reality Technology and Systems, Beihang University</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2"></span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shiwei Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:shiweili93@buaa.edu.cn">shiweili93@buaa.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Hangzhou Innovation Institute, Beihang University</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2"></span><span class="ltx_text ltx_affiliation_country" id="id15.3.id3"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zongye Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:zhangzongye@buaa.edu.cn">zhangzongye@buaa.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id16.1.id1">State Key Laboratory of Virtual Reality Technology and Systems, Beihang University</span><span class="ltx_text ltx_affiliation_city" id="id17.2.id2"></span><span class="ltx_text ltx_affiliation_country" id="id18.3.id3"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenkai Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:zhangchenkai@buaa.edu.cn">zhangchenkai@buaa.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id19.1.id1">State Key Laboratory of Virtual Reality Technology and Systems, Beihang University</span><span class="ltx_text ltx_affiliation_city" id="id20.2.id2"></span><span class="ltx_text ltx_affiliation_country" id="id21.3.id3"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaoyu Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:zf2306113@buaa.edu.cn">zf2306113@buaa.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id22.1.id1">Hangzhou Innovation Institute, Beihang University</span><span class="ltx_text ltx_affiliation_city" id="id23.2.id2"></span><span class="ltx_text ltx_affiliation_country" id="id24.3.id3"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qiqi Zhan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:zhanqiqi@buaa.edu.cn">zhanqiqi@buaa.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id25.1.id1">State Key Laboratory of Virtual Reality Technology and Systems, Beihang University</span><span class="ltx_text ltx_affiliation_city" id="id26.2.id2"></span><span class="ltx_text ltx_affiliation_country" id="id27.3.id3"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qingjie Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:qingjie.liu@buaa.edu.cn">qingjie.liu@buaa.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id28.1.id1">State Key Laboratory of Virtual Reality Technology and Systems, Beihang University</span><span class="ltx_text ltx_affiliation_city" id="id29.2.id2"></span><span class="ltx_text ltx_affiliation_country" id="id30.3.id3"></span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yunhong Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yhwang@buaa.edu.cn">yhwang@buaa.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id31.1.id1">State Key Laboratory of Virtual Reality Technology and Systems, Beihang University</span><span class="ltx_text ltx_affiliation_city" id="id32.2.id2"></span><span class="ltx_text ltx_affiliation_country" id="id33.3.id3"></span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id34.id1">The success of Large Language Models (LLMs) is inherently linked to the availability of vast, diverse, and high-quality data for training and evaluation. However, the growth rate of high-quality data is significantly outpaced by the expansion of training datasets, leading to a looming data exhaustion crisis. This underscores the urgent need to enhance data efficiency and explore new data sources. In this context, synthetic data has emerged as a promising solution. Currently, data generation primarily consists of two major approaches: data augmentation and synthesis. This paper comprehensively reviews and summarizes data generation techniques throughout the lifecycle of LLMs, including data preparation, pre-training, fine-tuning, instruction-tuning, preference alignment, and applications. Furthermore, We discuss the current constraints faced by these methods and investigate potential pathways for future development and research. Our aspiration is to equip researchers with a clear understanding of these methodologies, enabling them to swiftly identify appropriate data generation strategies in the construction of LLMs, while providing valuable insights for future exploration.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, large language models (LLMs) have demonstrated unparalleled capabilities across a wide array of tasks <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib69" title="">2023</a>; Bang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib10" title="">2023</a>; Rathje et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib167" title="">2024</a>)</cite>, firmly establishing themselves as the backbone of general artificial intelligence (AI) systems. These models achieve significant improvements in natural language processing <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib235" title="">2023a</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib265" title="">2024c</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib263" title="">e</a>)</cite>, computer vision <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib208" title="">2024a</a>; Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib240" title="">2021</a>; Lai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib101" title="">2024</a>)</cite>, and other research fields <cite class="ltx_cite ltx_citemacro_citep">(Qiu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib164" title="">2023</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib230" title="">2023b</a>; Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib37" title="">2023a</a>)</cite>, consistently pushing the boundaries of what AI can achieve. The success of LLMs is largely attributed to their ability to capture intricate patterns and relationships within vast amounts of data, allowing them to perform complex tasks such as natural language inference <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib135" title="">2024a</a>; Dalal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib40" title="">2024</a>)</cite>, visual question answering <cite class="ltx_cite ltx_citemacro_citep">(Mañas et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib152" title="">2024</a>; Özdemir and Akagündüz, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib159" title="">2024</a>)</cite>, and vision-and-language navigation <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib126" title="">2024b</a>; Schumann et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib179" title="">2024</a>)</cite> with remarkable proficiency.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="289" id="S1.F1.g1" src="x1.png" width="722"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Statistics of the publications related to LLM-oriented data synthesis and augmentation technologies, grouped by the publication year and venue.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, the performance of LLMs is highly dependent on the quality and volume of the data they are trained on <cite class="ltx_cite ltx_citemacro_citep">(Gandhi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib59" title="">2024</a>; Achiam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib3" title="">2023</a>; Floridi and Chiriatti, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib58" title="">2020</a>)</cite>. With the exponential growth in model size — now reaching billions or even trillions of parameters <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib269" title="">2023b</a>; Le Scao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib106" title="">2023</a>; Ren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib169" title="">2023b</a>)</cite> — there is an increasing demand for large-scale, diverse, and high-quality data to ensure robust generalization across various tasks and domains. Obtaining such data poses significant challenges due to the high costs of data collection and the problems introduced by privacy concerns. Additionally, the growth rate of high-quality data lags far behind the rapidly increasing size of training datasets. If this trend continues, the available data will eventually be depleted, implying that without significant improvements in data efficiency or the discovery of new data sources, the growth of LLMs may slow down considerably. Given these impending limitations, data synthesis and augmentation techniques become essential to extending the lifespan and generalization of LLMs. Traditional data synthesis and augmentation techniques <cite class="ltx_cite ltx_citemacro_citep">(Takahashi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib195" title="">2019</a>; Krell and Kim, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib99" title="">2017</a>; Cubuk et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib35" title="">2019</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib136" title="">2020</a>)</cite>, such as image rotation, cropping, flipping, and rule-based natural language generation, have been widely used to address these data limitations. Although these approaches improve data diversity and address data scarcity to some extent, they still struggle to fully capture the complexities of real-world data <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib56" title="">2021</a>)</cite>, generate data at scale <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib234" title="">2022</a>)</cite>, and defend against adversarial examples <cite class="ltx_cite ltx_citemacro_citep">(Qiu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib163" title="">2020</a>)</cite>, limiting their effectiveness for training LLMs.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="306" id="S1.F2.g1" src="x2.png" width="714"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.2.1.1" style="font-size:90%;">Figure 2</span>. </span><span class="ltx_text" id="S1.F2.3.2" style="font-size:90%;">A comparison between existing surveys on data synthesis and augmentation techniques and our work. Previous surveys primarily focus on LLM-based data synthesis and augmentation methods aimed at supporting downstream tasks. In contrast, our work emphasizes LLM-oriented data synthesis and augmentation, systematically covering the full lifecycle of LLMs—from data preparation to applications—and addressing core LLM functions such as understanding and generation, with the ultimate goal of improving LLMs themselves through data-centric techniques.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To overcome these challenges, researchers have increasingly turned to LLM-oriented data synthesis and augmentation techniques, recognizing the ability of LLMs to model complex patterns from large datasets and generate synthetic data that closely mirror real-world distributions while introducing valuable variations <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib261" title="">2023b</a>; Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib38" title="">2023a</a>; Samuel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib176" title="">2023</a>)</cite>. These studies reduce the reliance on manually curated datasets and enable the generation of high-quality, diverse data that meets the evolving demands of LLMs throughout their lifecycle and functions. To capture the breadth of these efforts, we collected papers related to LLM-oriented data synthesis and augmentation by searching Google Scholar using keywords such as ”data synthesis,” ”data augmentation,” and ”large models.” Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the publication trends by year and venue, reflecting the increasing interest in this field. As of October 2024, we identified <math alttext="250" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mn id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">250</mn><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><cn id="S1.p3.1.m1.1.1.cmml" type="integer" xref="S1.p3.1.m1.1.1">250</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">250</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">250</annotation></semantics></math> unique publications covering diverse research topics and venues. Summarizing these efforts provides critical insights into the progress and challenges that remain, offering a foundation for future research. Despite these advancements, several key challenges remain in LLM-oriented data synthesis and augmentation. The misuse of synthetic data poses risks, particularly in spreading misinformation and raising ethical concerns around manipulating public opinion. Additionally, synthetic data often introduces ambiguity when aligning AI models with human values, potentially leading to biased outcomes. Evaluating models trained on synthetic data is also complex, as traditional benchmarks may not fully capture the nuances of this data. Ensuring reliability is another concern, as biases and inaccuracies from original datasets can persist in synthetic data, limiting its generalization across domains. Moreover, the computational demands of LLMs, along with challenges in handling less common languages or novel instructions, complicate broader applications. Finally, the lack of a unified framework for organizing and comparing the methods proposed in both academia and industry remains a barrier for researchers navigating this rapidly evolving field.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">This survey aims to address these gaps by providing a comprehensive overview of LLM-oriented data synthesis and augmentation techniques. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>, unlike previous surveys <cite class="ltx_cite ltx_citemacro_citep">(Long et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib141" title="">2024</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib148" title="">2024</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib272" title="">2024a</a>; Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib215" title="">2023b</a>; Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib44" title="">2024</a>)</cite>, which primarily focus on applying these methods to support specific downstream tasks or particular stages of LLMs, our work emphasizes the direct role of LLM-oriented techniques in improving the overall performance of LLMs across various stages of their lifecycle and core functions. In contrast to the work <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib138" title="">2024d</a>)</cite>, which focuses on practices for synthetic data generation to address challenges like data scarcity and privacy, our survey extends beyond practical guidance by categorizing methods aimed at improving LLM performance holistically. We examine not only data generation but also how these techniques enhance LLMs across all stages and functions, offering a more integrated, data-centric framework for advancing LLMs. Specifically, we systematically review and categorize existing research from two key perspectives: the lifecycle of LLMs (from pre-training to fine-tuning and application) and their core functions (understanding, logic, memory, and generation). By framing the discussion around these dual perspectives, we offer clearer insights into the development, interconnections, and practical applications of different approaches. Moreover, we identify critical challenges, explore emerging research directions, and highlight potential breakthroughs that could further drive advancements in LLM performance through data-centric methods.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The contributions of this survey are summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">First Survey:</span> To our knowledge, we present the first comprehensive survey focused on advancing LLMs through data synthesis and augmentation, systematically covering the entire lifecycle stages and core functions of LLMs. This survey provides an in-depth analysis of current methodologies and highlights the unique challenges at each stage.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">New taxonomy:</span> We introduce an innovative organizational framework that categorizes existing research from two key perspectives: the lifecycle stages of LLMs and their core functions. This taxonomy offers a clearer understanding of the progression, interconnections, and applicability of different approaches, providing valuable insights into both developmental and functional aspects of LLM-oriented data synthesis and augmentation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">New frontiers:</span> We identify critical challenges, and explore emerging research directions, and potential breakthroughs in LLM-oriented data synthesis and augmentation. This discussion aims to inspire future research and guide developments in data-centric techniques for LLM advancement.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.p1.1.1">Abundant resources:</span> We organize and maintain a dedicated repository to support ongoing research and collaboration in LLM-oriented data synthesis and augmentation. This resource includes a curated collection of related papers, multiple leaderboards tracking the latest advancements, and regular updates to foster innovation, guide future research directions, and accelerate breakthroughs in the field.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">By offering a comprehensive overview of LLM-oriented data synthesis and augmentation approaches, this survey aims to clarify the current state of the field and inspire future research directions that can further enhance LLM capabilities through data synthesis and augmentation methodologies.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">We organize the remainder of this survey as follows: Section 2 categorizes the primary areas of LLM-oriented data synthesis and augmentation, providing an overview of the foundational techniques. Section 3 discusses the current LLM-oriented data synthesis and augmentation methods from the perspective of the full lifecycle of LLMs, detailing how these techniques are employed at different stages of model development. In Section 4, we review these methods from the viewpoint of core LLM functions, exploring how data synthesis and augmentation enhance key capabilities such as understanding, logic, memory, and generation. Section 5 delves into the evaluation strategies for LLM-oriented data synthesis and augmentation, addressing benchmarks, evaluation metrics, and leaderboards used to assess and compare the effectiveness of existing approaches. Finally, Section 6 provides insights into challenges and emerging trends in LLM-oriented data synthesis and augmentation, offering recommendations for future research directions that can contribute to the continued advancement of LLMs through data synthesis and augmentation methodologies.</p>
</div>
<figure class="ltx_figure" id="S1.F3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.F3.1" style="width:433.6pt;height:122956.8pt;vertical-align:-122511.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-162.6pt,-219.1pt) scale(0.57147054889726,64.1067519791875) ;"><span class="ltx_ERROR undefined" id="S1.F3.1.2">{forest}</span>
<p class="ltx_p" id="S1.F3.1.1">for tree=
grow=east,
reversed=true,
anchor=base west,
parent anchor=east,
child anchor=west,
base=left,
font=,
rectangle,
draw=hidden-draw,
rounded corners,
align=left,
minimum width=4em,
edge+=darkgray, line width=1pt,
s sep=3pt,
inner xsep=2pt,
inner ysep=3pt,
edge path=
[draw, <span class="ltx_ERROR undefined" id="S1.F3.1.1.1">\forestoption</span>edge]
(!u.parent anchor) – ++(1.5mm,0) —- (.child anchor) <span class="ltx_ERROR undefined" id="S1.F3.1.1.2">\forestoption</span>edge label;,
ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center,
,
where level=1text width=7.2em,font=<span class="ltx_text" id="S1.F3.1.1.3" style="font-size:90%;">,</span>,
where level=2text width=8.0em,font=<span class="ltx_text" id="S1.F3.1.1.4" style="font-size:90%;">,</span>,
where level=3text width=10.0em,font=<span class="ltx_text" id="S1.F3.1.1.5" style="font-size:90%;">,</span>,
where level=4text width=9.0em,font=<span class="ltx_text" id="S1.F3.1.1.6" style="font-size:90%;">,</span>,
where level=5text width=6.4em,font=<span class="ltx_text" id="S1.F3.1.1.7" style="font-size:90%;">,</span>,
[
Data Synthesis and Augmentation for Large Language Models: A Survey, ver, color=bounding!100, fill=0!15, text=black, font=, text width=32.0em, text centered
[
Taxonomy (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S2" title="2. Taxonomy ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>), color=1!100, fill=1!80, text=black
[
Data Augmentation 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S2.SS1" title="2.1. Data Augmentation ‣ 2. Taxonomy ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">2.1</span></a>), color=1!100, fill=1!65, text=black
[
Data Labeling, color=1!100, fill=1_1!55, text=black
[
 T-SciQ <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib206" title="">2024d</a>)</cite>, ChatGPT-based <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib276" title="">2023b</a>; Gilardi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib64" title="">2023</a>; Alizadeh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib4" title="">2023</a>)</cite>, color=bounding!70, fill=1_1!20, text=black, text width=24.0em
]
]
[
Data Reformation, color=1!100, fill=1_1!55, text=black
[
 Mosaic<cite class="ltx_cite ltx_citemacro_citep">(Jocher, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib91" title="">2020</a>)</cite>, CORE <cite class="ltx_cite ltx_citemacro_citep">(Dixit et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib46" title="">2022</a>)</cite>, ALIA  <cite class="ltx_cite ltx_citemacro_citep">(Dunlap et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib52" title="">2023</a>)</cite>, ChatAug  <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib38" title="">2023a</a>)</cite>, color=bounding!70, fill=1_1!20, text=black, text width=24.0em
]
]
[
Co-Annotation, color=1!100, fill=1_1!55, text=black
[
 Co-annotating <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib117" title="">2023d</a>)</cite>, ToolCoder <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib260" title="">2023e</a>)</cite>, color=bounding!70, fill=1_1!20, text=black, text width=24.0em
]
]
]
[
Data Synthesis (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S2.SS2" title="2.2. Data Synthesis ‣ 2. Taxonomy ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">2.2</span></a>), color=1!100, fill=1!65, text=black
[
General Model 
<br class="ltx_break"/>Distillation, color=1!100, fill=1_1!55, text=black
[
 TinyStories<cite class="ltx_cite ltx_citemacro_citep">(Eldan and Li, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib54" title="">2023</a>)</cite>, Phi-1<cite class="ltx_cite ltx_citemacro_citep">(Gunasekar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib68" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib121" title="">2023b</a>)</cite>, Alpagasus <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib23" title="">2023b</a>)</cite>, WizardLM <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib224" title="">2023b</a>)</cite>, color=bounding!70, fill=1_1!20, text=black, text width=24.0em
]
]
[
Domain Model 
<br class="ltx_break"/>Distillation, color=1!100, fill=1_1!55, text=black
[
 Minerva<cite class="ltx_cite ltx_citemacro_citep">(Lewkowycz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib109" title="">2022</a>)</cite>, DeepSeek-Prover<cite class="ltx_cite ltx_citemacro_citep">(Xin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib221" title="">2024</a>)</cite>, WizardCoder<cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib147" title="">2024</a>)</cite>, color=bounding!70, fill=1_1!20, text=black, text width=24.0em
]
]
[
Model Self-Improvement, color=1!100, fill=1_1!55, text=black
[
 Rephrasing<cite class="ltx_cite ltx_citemacro_citep">(Maini et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib151" title="">2024</a>)</cite>, Self-instruct<cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib211" title="">2023b</a>)</cite>, SPIN <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib27" title="">2024a</a>)</cite>, SelTDA <cite class="ltx_cite ltx_citemacro_citep">(Khan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib95" title="">2023</a>)</cite>, color=bounding!70, fill=1_1!20, text=black, text width=24.0em
]
]
]
]
[
Full Lifecycle 
<br class="ltx_break"/>of LM (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3" title="3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>), color=2!100, fill=2!85, text=black
[
Data Preparation 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS1" title="3.1. Data Preparation ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">3.1</span></a>), color=orange!70, fill=orange!35, text=black
[
General Model Distillation, color=orange!70, fill=orange!18, text=black
[
 Dialogic <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib123" title="">2022a</a>)</cite>, MathInstruct <cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib245" title="">2023b</a>)</cite>, Genixer <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib267" title="">2023a</a>)</cite>, Magpie <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib228" title="">2024b</a>)</cite>, 
<br class="ltx_break"/>MMIQC <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib132" title="">2024f</a>)</cite>, Genie <cite class="ltx_cite ltx_citemacro_citep">(Yehudai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib237" title="">2024</a>)</cite>, Case2Code <cite class="ltx_cite ltx_citemacro_citep">(Shao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib181" title="">2024a</a>)</cite>, UltraChat <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib45" title="">2023</a>)</cite>, color=bounding!70, fill=orange!7, text=black, text width=24.0em
]
]
[
Data Augmentation, color=orange!70, fill=orange!18, text=black
[
 Disco <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib28" title="">2022</a>)</cite>, GPT3Mix <cite class="ltx_cite ltx_citemacro_citep">(Yoo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib238" title="">2021</a>)</cite>, CoAnnotating <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib117" title="">2023d</a>)</cite>, ALIA <cite class="ltx_cite ltx_citemacro_citep">(Dunlap et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib52" title="">2023</a>)</cite>, 
<br class="ltx_break"/>FullAnno <cite class="ltx_cite ltx_citemacro_citep">(Hao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib75" title="">2024</a>)</cite>, Dialgen <cite class="ltx_cite ltx_citemacro_citep">(Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib143" title="">2023</a>)</cite>, TinyGSM <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib129" title="">2023a</a>)</cite>, AMPS <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib78" title="">2021</a>)</cite>, color=bounding!70, fill=orange!7, text=black, text width=24.0em
]
]
]
[
Pretraining (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS2" title="3.2. Pre-Training ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">3.2</span></a>), color=blue!40, fill=blue!20, text=black
[
General Model Distillation, color=blue!40, fill=blue!10, text=black
[
 Phi-1 <cite class="ltx_cite ltx_citemacro_citep">(Gunasekar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib68" title="">2023</a>)</cite>, SciLitLLM <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib119" title="">2024c</a>)</cite>, TRAIT <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib124" title="">2024</a>)</cite>, AnyGPT <cite class="ltx_cite ltx_citemacro_citep">(Zhan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib252" title="">2024</a>)</cite>, 
<br class="ltx_break"/>Phi-1.5 <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib121" title="">2023b</a>)</cite>, TinyDialogues <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib57" title="">2024</a>)</cite>, color=bounding!70, fill=blue!5, text=black, text width=24.0em
]
]
[
Model Self-Improvement, color=blue!40, fill=blue!10, text=black
[
 VILA-2 <cite class="ltx_cite ltx_citemacro_citep">(Fang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib55" title="">2024</a>)</cite>, color=bounding!70, fill=blue!5, text=black, text width=24.0em
]
]
[
Data Augmentation, color=blue!40, fill=blue!10, text=black
[
 WRAP <cite class="ltx_cite ltx_citemacro_citep">(Maini et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib151" title="">2024</a>)</cite>, KMLM <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib134" title="">2021</a>)</cite>, bioR <cite class="ltx_cite ltx_citemacro_citep">(Zhu and Li, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib277" title="">2023</a>)</cite>, Physics-based <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib135" title="">2024a</a>)</cite>, color=bounding!70, fill=blue!5, text=black, text width=24.0em
]
]
]
[
Finetuning (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS3" title="3.3. Fine-Tuning ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">3.3</span></a>), color=purple!60, fill=purple!20, text=black
[
General Model Distillation, color=purple!60, fill=purple!10, text=black
[
 LAB <cite class="ltx_cite ltx_citemacro_citep">(Sudalairaj et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib192" title="">2024</a>)</cite>, LLM2LLM <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib108" title="">2024</a>)</cite>, GLAN <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib112" title="">2024b</a>)</cite>, Clingen <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib227" title="">2024a</a>)</cite>, 
<br class="ltx_break"/>Baize <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib223" title="">2023a</a>)</cite>, Evol-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib224" title="">2023b</a>)</cite>, HuaTuo <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib205" title="">2023c</a>)</cite>, NExT-GPT <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib220" title="">2023a</a>)</cite>, color=bounding!70, fill=purple!5, text=black, text width=24.0em
]
]
[
Model Self-Improvement, color=purple!60, fill=purple!10, text=black
[
 STaR <cite class="ltx_cite ltx_citemacro_citep">(Zelikman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib249" title="">2022</a>)</cite>, REST <cite class="ltx_cite ltx_citemacro_citep">(Gulcehre et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib67" title="">2023</a>)</cite>, Self-Translate <cite class="ltx_cite ltx_citemacro_citep">(Ri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib171" title="">2024</a>)</cite>, Self-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib211" title="">2023b</a>)</cite>, 
<br class="ltx_break"/>RFT <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib243" title="">2023</a>)</cite>, CodeRL <cite class="ltx_cite ltx_citemacro_citep">(Le et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib105" title="">2022</a>)</cite>, REST-EM <cite class="ltx_cite ltx_citemacro_citep">(Singh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib188" title="">2023</a>)</cite>, DeepSeekProver <cite class="ltx_cite ltx_citemacro_citep">(Xin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib221" title="">2024</a>)</cite>, color=bounding!70, fill=purple!5, text=black, text width=24.0em
]
]
[
Data Augmentation, color=purple!60, fill=purple!10, text=black
[
 MathGenie <cite class="ltx_cite ltx_citemacro_citep">(Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib145" title="">2024a</a>)</cite>, DISC-MedLLM <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib11" title="">2023</a>)</cite>, MetaMath <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib239" title="">2024</a>)</cite>, 
<br class="ltx_break"/>Symbol tuning <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib214" title="">2023a</a>)</cite>, Llama-3-UltraMedical <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib259" title="">2024g</a>)</cite>, Llemma <cite class="ltx_cite ltx_citemacro_citep">(Azerbayev et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib7" title="">2023</a>)</cite>, color=bounding!70, fill=purple!5, text=black, text width=24.0em
]
]
]
[
Instruction-Tuning
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS4" title="3.4. Instruction-Tuning ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">3.4</span></a>),
color=BlueGreen!150, fill=BlueGreen!90, text=black
[
General Model Distillation, color=BlueGreen!120, fill=BlueGreen!50, text=black
[
 Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib197" title="">2023</a>)</cite>, Vicuna <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib30" title="">2023</a>)</cite>, Orca  <cite class="ltx_cite ltx_citemacro_citep">(Mukherjee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib155" title="">2023</a>)</cite>,Baize <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib223" title="">2023a</a>)</cite>, LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib131" title="">2024c</a>)</cite>, color=bounding!70, fill=BlueGreen!30, text=black, text width=24.0em
]
]
[
Model Self-Improvement, color=BlueGreen!120, fill=BlueGreen!50, text=black
[
 Self-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib211" title="">2023b</a>)</cite>, SPIN <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib27" title="">2024a</a>)</cite>, CAI <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib9" title="">2022b</a>)</cite>, Toolformer <cite class="ltx_cite ltx_citemacro_citep">(Schick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib178" title="">2024</a>)</cite>, color=bounding!70, fill=BlueGreen!30, text=black, text width=24.0em
]
]
[
Data Augmentation, color=BlueGreen!120, fill=BlueGreen!50, text=black
[
 T-SciQ <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib206" title="">2024d</a>)</cite>, CORE <cite class="ltx_cite ltx_citemacro_citep">(Dixit et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib46" title="">2022</a>)</cite>, ChatAug  <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib38" title="">2023a</a>)</cite>, ToolCoder <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib260" title="">2023e</a>)</cite>, color=bounding!70, fill=BlueGreen!30, text=black, text width=24.0em
]
]
]
[
Preference 
<br class="ltx_break"/>Alignment(§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS5" title="3.5. Preference Alignment ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">3.5</span></a>) , color=BlueViolet!60, fill=BlueViolet!25, text=black
[
General Model Distillation, color=BlueViolet!60, fill=BlueViolet!15, text=black
[
 ULTRAFEEDBACK <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib36" title="">2023b</a>)</cite>, HelpSteer <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib213" title="">2023a</a>)</cite>, LEMA <cite class="ltx_cite ltx_citemacro_citep">(An et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib5" title="">2023</a>)</cite>,color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em
]
]
[
Domain Model Distillation, color=BlueViolet!60, fill=BlueViolet!15, text=black
[
 BAD  <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib226" title="">2021</a>)</cite>, BEAVERTAILS <cite class="ltx_cite ltx_citemacro_citep">(Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib86" title="">2024</a>)</cite>, PRM800K <cite class="ltx_cite ltx_citemacro_citep">(Lightman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib125" title="">2023</a>)</cite>, WebGPT <cite class="ltx_cite ltx_citemacro_citep">(Nakano et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib157" title="">2021</a>)</cite>,color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em
]
]
[
Model Self-Improvement, color=BlueViolet!60, fill=BlueViolet!15, text=black
[
 OAIF <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib70" title="">2024</a>)</cite>, SELF-JUDGE <cite class="ltx_cite ltx_citemacro_citep">(Ye and Ng, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib236" title="">2024</a>)</cite>, SALMON <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib194" title="">2024b</a>)</cite>, SteerLM <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib48" title="">2023</a>)</cite>, color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em
]
]
[
Data Augmentation, color=BlueViolet!60, fill=BlueViolet!15, text=black
[
  Starling-7B <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib274" title="">2023a</a>)</cite>, UltraInteract  <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib241" title="">2024a</a>)</cite>, CriticBench <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib127" title="">2024a</a>)</cite>, color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em
]
]
]
[
Applications (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.SS6" title="3.6. Applications ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">3.6</span></a>), color=red!40, fill=red!25, text=black
[
Math, color=4!80, fill=4_1!70, text=black
[
 MetaMath <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib239" title="">2024</a>)</cite>, MammoTH <cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib245" title="">2023b</a>)</cite>, STaR <cite class="ltx_cite ltx_citemacro_citep">(Zelikman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib249" title="">2022</a>)</cite>, Galactia <cite class="ltx_cite ltx_citemacro_citep">(Taylor et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib198" title="">2022</a>)</cite>, 
<br class="ltx_break"/> DeepSeekProver <cite class="ltx_cite ltx_citemacro_citep">(Xin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib221" title="">2024</a>)</cite>, WizardMath <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib146" title="">2023</a>)</cite>, color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em
]
]
[
Science, color=4!80, fill=4_1!70, text=black
[
 SciLitLLM <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib119" title="">2024c</a>)</cite>, ChemLLM <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib255" title="">2024f</a>)</cite>, SciGLM <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib254" title="">2024d</a>)</cite>, Galactia <cite class="ltx_cite ltx_citemacro_citep">(Taylor et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib198" title="">2022</a>)</cite>, color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em
]
]
[
Code, color=4!80, fill=4_1!70, text=black
[
 WizardCoder <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib147" title="">2024</a>)</cite>, MagicCoder <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib216" title="">2024</a>)</cite>, Code Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Chaudhary, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib18" title="">2023</a>)</cite>, 
<br class="ltx_break"/> Code LLama <cite class="ltx_cite ltx_citemacro_citep">(Rozière et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib174" title="">2023</a>)</cite>, Phi-1 <cite class="ltx_cite ltx_citemacro_citep">(Gunasekar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib68" title="">2023</a>)</cite>, Phi-1.5 <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib121" title="">2023b</a>)</cite>, color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em
]
]
[
Medical, color=4!80, fill=4_1!70, text=black
[
 DISC-MedLLM <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib11" title="">2023</a>)</cite>, HuatuoGPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib257" title="">2023a</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib21" title="">1 16</a>)</cite>, ChatCounselor <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib133" title="">9 27</a>)</cite>,
<br class="ltx_break"/> ClinGen <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib227" title="">2024a</a>)</cite>, color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em
]
]
[
Law, color=4!80, fill=4_1!70, text=black
[
 DISC-LawLLM <cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib244" title="">2023a</a>)</cite>, LawyerLLaMA <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib83" title="">2023</a>)</cite>, LawGPT <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib273" title="">2024b</a>)</cite>, 
<br class="ltx_break"/> WisdomInterrogatory <cite class="ltx_cite ltx_citemacro_citep">(zhihaiLLM, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib271" title="">847Z</a>)</cite>, color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em
]
]
]
]
[
Functionality (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S4" title="4. Functionality ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>), color=SlateBlue!80, fill=SlateBlue!40, text=black
[
Understanding (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S4.SS1" title="4.1. Understanding ‣ 4. Functionality ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">4.1</span></a>), color=brown!60, fill=brown!30, text=black
[
 Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib197" title="">2023</a>)</cite>, WizardLM <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib224" title="">2023b</a>)</cite>, WRAP<cite class="ltx_cite ltx_citemacro_citep">(Maini et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib151" title="">2024</a>)</cite>, LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib131" title="">2024c</a>)</cite>, ChartLlama<cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib74" title="">2023</a>)</cite>,Genixer<cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib267" title="">2023a</a>)</cite>, color=bounding!70, fill=brown!10, text=black, text width=35.8em
]
]
[
Logic (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S4.SS2" title="4.2. Logic ‣ 4. Functionality ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">4.2</span></a>), color=2_2_2!100, fill=2_2_2!60, text=black
[
 ReST<sup class="ltx_sup" id="S1.F3.1.1.8"><span class="ltx_text ltx_font_italic" id="S1.F3.1.1.8.1">EM</span></sup> <cite class="ltx_cite ltx_citemacro_citep">(Singh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib188" title="">2023</a>)</cite>, Case2Code<cite class="ltx_cite ltx_citemacro_citep">(Shao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib181" title="">2024a</a>)</cite>, MathInstruct<cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib245" title="">2023b</a>)</cite>, MMIQC<cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib132" title="">2024f</a>)</cite>, STaR<cite class="ltx_cite ltx_citemacro_citep">(Zelikman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib249" title="">2022</a>)</cite>,SelTDA <cite class="ltx_cite ltx_citemacro_citep">(Khan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib95" title="">2023</a>)</cite>, color=bounding!70, fill=2_2_2!20, text=black, text width=35.8em
]
]
[
Memory (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S4.SS3" title="4.3. Memory ‣ 4. Functionality ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">4.3</span></a>), color=LightGreen!100, fill=LightGreen!40, text=black
[
 Quiet-STaR <cite class="ltx_cite ltx_citemacro_citep">(Zelikman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib248" title="">2024</a>)</cite>, AutoKG <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib275" title="">2024</a>)</cite>, Persona Hub <cite class="ltx_cite ltx_citemacro_citep">(Chan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib17" title="">2024</a>)</cite>, AceCoder <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib114" title="">2023g</a>)</cite>, RepoCoder <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib256" title="">2023c</a>)</cite>, color=bounding!70, fill=LightGreen!10, text=black, text width=35.8em
]
]
[
Generation (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S4.SS4" title="4.4. Generation ‣ 4. Functionality ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">4.4</span></a>), color=Turquoise!80, fill=Turquoise!30, text=black
[
 Genie<cite class="ltx_cite ltx_citemacro_citep">(Yehudai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib237" title="">2024</a>)</cite>, UltraMedical<cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib259" title="">2024g</a>)</cite>, HuaTuo<cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib205" title="">2023c</a>)</cite>, TinyStories<cite class="ltx_cite ltx_citemacro_citep">(Eldan and Li, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib54" title="">2023</a>)</cite>, DIALOGIC<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib123" title="">2022a</a>)</cite>, ALIA <cite class="ltx_cite ltx_citemacro_citep">(Dunlap et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib52" title="">2023</a>)</cite>, color=bounding!70, fill=Turquoise!10, text=black, text width=35.8em
]
]
]
[
Challenges and 
<br class="ltx_break"/>Limitations (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S5" title="5. Challenges and Limitations ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">5</span></a>), color=6!100, fill=6!50, text=black
[
Synthesizing and 
<br class="ltx_break"/>Augmenting Method 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S5.SS1" title="5.1. Synthesizing and Augmenting Method ‣ 5. Challenges and Limitations ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">5.1</span></a>), color=6!100, fill=6!35, text=black
[
 d-RLAIF<cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib107" title="">2023</a>)</cite>, LLM2LLM<cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib108" title="">2024</a>)</cite>, Wizardmath<cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib146" title="">2023</a>)</cite>, STaR<cite class="ltx_cite ltx_citemacro_citep">(Zelikman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib249" title="">2022</a>)</cite>, SciGLM<cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib254" title="">2024d</a>)</cite>, ChemLLM<cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib255" title="">2024f</a>)</cite>
, color=bounding!70, fill=6!20, text=black, text width=35.8em
]
]
[
Data Quality (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S5.SS2" title="5.2. Data Quality ‣ 5. Challenges and Limitations ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">5.2</span></a>), color=6!100, fill=6!35, text=black
[
  LLMs4Synthesis<cite class="ltx_cite ltx_citemacro_citep">(Giglou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib63" title="">2024</a>)</cite>, CoRAL<cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib218" title="">2024a</a>)</cite>, FORD<cite class="ltx_cite ltx_citemacro_citep">(Xiong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib222" title="">2023</a>)</cite>,LTGC<cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib268" title="">2024a</a>)</cite>, color=bounding!70, fill=6!20, text=black, text width=35.8em
]
]
[
Impact of Data 
<br class="ltx_break"/>Synthesis and 
<br class="ltx_break"/>Augmentation (§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S5.SS3" title="5.3. Impact of Data Synthesis and Augmentation ‣ 5. Challenges and Limitations ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">5.3</span></a>), color=6!100, fill=6!35, text=black
[
  DataDreamer<cite class="ltx_cite ltx_citemacro_citep">(Patel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib160" title="">2024</a>)</cite>,HARMONIC<cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib210" title="">2024c</a>)</cite>, color=bounding!70, fill=6!20, text=black, text width=35.8em
]
]
[
Impact on Different 
<br class="ltx_break"/>Applications and 
<br class="ltx_break"/>Tasks (§<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:sec:Impact_on_Different_Applications_and_Tasks</span>), color=6!100, fill=6!35, text=black
[
 PANDA<cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib128" title="">2024e</a>)</cite>,REGA<cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib207" title="">2024e</a>)</cite>, color=bounding!70, fill=6!20, text=black, text width=35.8em
]
]
[
Future Directions 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S5.SS5" title="5.5. Future Directions ‣ 5. Challenges and Limitations ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">5.5</span></a>), color=6!100, fill=6!35, text=black
[
 TabSynDex<cite class="ltx_cite ltx_citemacro_citep">(Chundawat et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib33" title="">2022</a>)</cite>,CoLa-Diff<cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib88" title="">2023b</a>)</cite>,WizardCoder<cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib147" title="">2024</a>)</cite>, WebGPT<cite class="ltx_cite ltx_citemacro_citep">(Nakano et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib157" title="">2021</a>)</cite>, color=bounding!70, fill=6!20, text=black, text width=35.8em
]
]
]
]</p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F3.3.1.1" style="font-size:90%;">Figure 3</span>. </span><span class="ltx_text" id="S1.F3.4.2" style="font-size:90%;">The main content flow and categorization of this survey.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Taxonomy</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Data generation methods play a pivotal role in addressing data scarcity and imbalance, thereby improving model performance and generalization.
As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S2.F4" title="Figure 4 ‣ 2.2.3. Model Self-Improvement ‣ 2.2. Data Synthesis ‣ 2. Taxonomy ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>, we summarize the development and evolution of data augmentation and synthesis techniques in recent years.
This section primarily introduces the current classification of data generation methods, distinguishing between <span class="ltx_text ltx_font_bold" id="S2.p1.1.1">data augmentation</span>, which enhances existing data samples through transformations, and <span class="ltx_text ltx_font_bold" id="S2.p1.1.2">data synthesis</span>, which creates entirely new samples from scratch or based on generative models. Both methods differ in their approach to acquiring data but aim to expand datasets. Furthermore, data augmentation and synthesis methods can be categorized into subclasses from multiple dimensions.
Each approach has unique strengths and applications, enabling researchers to tailor their data generation strategies to specific needs and goals.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Data Augmentation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Data augmentation, a type of generation approach from data to data, generally involves manipulating the original data to increase its diversity and quantity without significantly altering its essential characteristics. Techniques used in data augmentation are designed to enhance the richness of existing data samples through transformations or perturbations. Across different modalities, data augmentation techniques often exhibit similarities. For instance, in image data, augmentation operations encompass mosaic<cite class="ltx_cite ltx_citemacro_citep">(Jocher, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib91" title="">2020</a>)</cite>, flipping<cite class="ltx_cite ltx_citemacro_citep">(Shorten and Khoshgoftaar, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib185" title="">2019</a>)</cite>, copy-pasting<cite class="ltx_cite ltx_citemacro_citep">(Ghiasi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib62" title="">2021</a>)</cite>, adding noise<cite class="ltx_cite ltx_citemacro_citep">(Maharana et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib150" title="">2022</a>)</cite>, pairing<cite class="ltx_cite ltx_citemacro_citep">(Inoue, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib85" title="">2018</a>)</cite> and so forth. Similarly, in text data, augmentation operations involve synonym replacement<cite class="ltx_cite ltx_citemacro_citep">(Kobayashi, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib96" title="">2018</a>)</cite>, copy-pasting<cite class="ltx_cite ltx_citemacro_citep">(Shorten et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib186" title="">2021</a>)</cite>, etc. Moreover, to cater to the demands of multimodal learning, existing research has addressed cross-modal information alignment during data augmentation. MixGen<cite class="ltx_cite ltx_citemacro_citep">(Hao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib76" title="">2023</a>)</cite> generates new training samples by linearly interpolating images and concatenating text sequences from two existing image-text pairs. The semantic relationship within the newly generated image-text pair remains consistent and matched. Recently, in the rapidly advancing landscape of LLMs, data augmentation has emerged as a cornerstone for bolstering model performance through the diversification of training exemplars, circumventing the necessity for extensive additional data gathering. From a data-centric perspective, we systematically categorize existing research on data augmentation into three distinct categories: <span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.1">data labeling<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.1.1.1">(</span>Liu et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.1.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib137" title="">2022</a>; Törnberg<span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.1.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib199" title="">2023</a>; Zhu et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.1.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib276" title="">2023b</a>; Gilardi et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.1.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib64" title="">2023</a>; Alizadeh et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.1.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib4" title="">2023</a>; Khan et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.1.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib95" title="">2023</a><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.1.3.3">)</span></cite></span>, <span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.2">data reformation<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.2.1.1">(</span>Yoo et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.2.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib238" title="">2021</a>; Dixit et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.2.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib46" title="">2022</a>; Dunlap et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.2.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib52" title="">2023</a>; Lu et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.2.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib144" title="">2024b</a><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.2.3.3">)</span></cite></span>, and <span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.3">co-annotation<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.3.1.1">(</span>Li et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.3.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib117" title="">2023d</a>; Bertaglia et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.3.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib12" title="">2023</a>; Ding et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.3.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib44" title="">2024</a><span class="ltx_text ltx_font_medium" id="S2.SS1.p1.1.3.3.3">)</span></cite></span>.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1. </span><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.1.1">Data Labeling</span>
</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">Data labeling endeavors to leverage the comprehensive language understanding capabilities of LLMs to annotate vast unlabeled datasets. This methodology is particularly beneficial in fields that possess a substantial unlabeled data corpus, encompassing domains such as cross-lingual processing and multimodal learning<cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib276" title="">2023b</a>; Gilardi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib64" title="">2023</a>; Alizadeh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib4" title="">2023</a>)</cite>, where the automation of annotation can significantly expedite the data preparation process. Recent research studies the zero-shot annotation ability of LLMs, such as GPT-4 on labeling political Twitter<cite class="ltx_cite ltx_citemacro_citep">(Törnberg, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib199" title="">2023</a>)</cite>. Moreover, Khan et al. <cite class="ltx_cite ltx_citemacro_citep">(Khan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib95" title="">2023</a>)</cite> focus on Visual question answering (VQA) tasks by generating pseudo-label data from unlabeled images by utilizing the SelTDA framework.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2. </span><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS2.1.1">Data Reformation</span>
</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">Data reformation involves transforming and restructuring existing data into a broader spectrum of variations, thereby facilitating more fine-grained data augmentation<cite class="ltx_cite ltx_citemacro_citep">(Dixit et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib46" title="">2022</a>; Dunlap et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib52" title="">2023</a>)</cite>. This approach aims to enrich the training landscape with diverse yet pertinent examples, enhancing the model’s robustness and generalization capabilities. Classic methods such as rotation<cite class="ltx_cite ltx_citemacro_citep">(Kalra et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib93" title="">2021</a>)</cite>, color channel transformation<cite class="ltx_cite ltx_citemacro_citep">(Gillespie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib65" title="">1987</a>)</cite>, and synonym replacement<cite class="ltx_cite ltx_citemacro_citep">(Kobayashi, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib96" title="">2018</a>)</cite> are commonly used. Recently, approaches utilizing LLMs have also emerged. For example, Chen et al.<cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib28" title="">2022</a>)</cite> propose Disco, an approach that harnesses LLMs to produce large-scale, high-quality counterfactual data.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3. </span><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS3.1.1">Co-Annotation</span>
</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">Co-annotation designates the collaborative effort between human annotators and LLMs in the annotation process<cite class="ltx_cite ltx_citemacro_citep">(Bertaglia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib12" title="">2023</a>)</cite>. By integrating the strengths of both annotation methodologies, co-annotation not only mitigates annotation costs but also concurrently enhances annotation performance, fostering a more efficient and effective approach to data annotation. Li et al.<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib117" title="">2023d</a>)</cite> introduce CoAnnotating, a framework that strategically assigns data points for annotation either to humans or to LLMs, based on an assessment of the LLM’s annotation uncertainty.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Data Synthesis</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Data synthesis, on the other hand, aims to create entirely new data from scratch or based on generative models, which are similar to the distribution of real data. In recent years, with the explosion and advancements in generative AI<cite class="ltx_cite ltx_citemacro_citep">(Ho et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib79" title="">2020</a>; Dhariwal and Nichol, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib43" title="">2021</a>; Rezende and Mohamed, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib170" title="">2015</a>; Brown, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib14" title="">2020</a>; Devlin, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib42" title="">2018</a>; Liu, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib140" title="">2019</a>; Pinaya et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib162" title="">2023</a>)</cite>, there have been significant strides in the quality and generation efficiency of synthetic data. Based on the requirements of LMs, this paper categorizes data synthesis methods into three main types: <span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.1">general model distillation<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.1.1.1">(</span>Eldan and Li<span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.1.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib54" title="">2023</a>; Li et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.1.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib121" title="">2023b</a>; Chen et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.1.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib23" title="">2023b</a>; Zhao et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.1.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib267" title="">2023a</a>; Zhang et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.1.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib264" title="">2024b</a><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.1.3.3">)</span></cite></span>, <span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.2">domain model distillation<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.2.1.1">(</span>Luo et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.2.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib146" title="">2023</a>; Wei et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.2.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib216" title="">2024</a>; Luo et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.2.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib147" title="">2024</a>; Lewkowycz et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.2.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib109" title="">2022</a><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.2.3.3">)</span></cite></span>, and <span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.3">model self-improvement<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.3.1.1">(</span>Maini et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.3.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib151" title="">2024</a>; Wang et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.3.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib211" title="">2023b</a>; Fang et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.3.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib55" title="">2024</a>; Zelikman et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.3.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib249" title="">2022</a><span class="ltx_text ltx_font_medium" id="S2.SS2.p1.1.3.3.3">)</span></cite></span>.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1. </span><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.1.1">General Model Distillation</span>
</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">Among these, general model distillation involves leveraging powerful general models, typically featuring larger parameters and superior performance, such as StableVicuna, ChatGPT, and GPT-4, to generate datasets that can enhance the capabilities of weaker models. There are various ways to employ these powerful models, such as using predefined templates to generate tiny stories<cite class="ltx_cite ltx_citemacro_citep">(Eldan and Li, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib54" title="">2023</a>)</cite> and leveraging the LLMs themselves to evaluate the quality of the generated data. Phi-1 and its series <cite class="ltx_cite ltx_citemacro_citep">(Gunasekar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib68" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib121" title="">2023b</a>)</cite>have demonstrated that a small amount of high-quality data can also train a powerful model, by leveraging the comprehensive generation of textbooks and exercises from GPT-3.5. Some other methods have also achieved performance improvements by generating instruction datasets and fine-tuning models after improving the quality of these datasets<cite class="ltx_cite ltx_citemacro_citep">(Honovich et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib81" title="">2022</a>; Taori et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib197" title="">2023</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib23" title="">2023b</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2. </span><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS2.1.1">Domain Model Distillation</span>
</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">Domain model distillation pertains to the utilization of models that are tailored to generate data within a particular domain. This approach is often necessary when general models fail to meet the specific needs of industry applications. For instance, in the context of code programming, domain model distillation can be employed to generate instructional data tailored to specific coding tasks<cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib216" title="">2024</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib147" title="">2024</a>)</cite>. In the realm of mathematics, methods such as Minerva<cite class="ltx_cite ltx_citemacro_citep">(Lewkowycz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib109" title="">2022</a>)</cite> and DeepSeekMath<cite class="ltx_cite ltx_citemacro_citep">(Xin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib221" title="">2024</a>)</cite> are designed to generate solutions to mathematical problems while ensuring their accuracy and diversity. Additionally, the realm of industry data often presents barriers, such as limited data scales and the inaccessibility of data within specific enterprises within the domain. These factors necessitate the adoption of domain-specific models that can effectively address the unique challenges posed by these scenarios.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3. </span><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS3.1.1">Model Self-Improvement</span>
</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">Model self-improvement refers to the process where a model generates higher-quality data to enhance its capabilities. For instance, leveraging existing instructions to adjust the model and prompting it to paraphrase documents on the web in specific styles, such as Wikipedia-style or QA-style, can be used to jointly pre-train LLMs for both authentic and synthetic paraphrasing tasks<cite class="ltx_cite ltx_citemacro_citep">(Maini et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib151" title="">2024</a>)</cite>. Self-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib211" title="">2023b</a>)</cite>enhances LMs themselves by autogenerating and refining instructional data, boosting performance with minimal human intervention.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="510" id="S2.F4.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.2.1.1" style="font-size:90%;">Figure 4</span>. </span><span class="ltx_text" id="S2.F4.3.2" style="font-size:90%;">Illustration of the evolutionary steps in the development of data synthesis and augmentation techniques for large models.</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Data Synthesis and Augmentation in the Full Lifecycle of LLM</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">From the perspective of the full lifecycle of LLM, We divide the existing investigations into six stages, including data preparation, pre-training, fine-tuning, instruction-tuning, preference alignment, and applications. The present section introduces relevant research in each stage.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Data Preparation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In the data preparation phase, data synthesis and augmentation aim to generate diverse and high-quality datasets for the training of LLMs, addressing the challenge of the scarcity of real-world data.
According to the taxonomy discussed in Section 2, We divide the present subsection into general model distillation and data augmentation.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.2.1.1" style="font-size:90%;">Table 1</span>. </span><span class="ltx_text" id="S3.T1.3.2" style="font-size:90%;">Data synthesis and augmentation in data preparation. In the table, method outlines the techniques presented by each research. Data source and synthetic data indicate the original data used to generate synthetic data and the synthetic data created for training purposes, respectively. A dash (-) in any cell denotes that the respective content was not mentioned in the cited literature.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.4.1.1">
<td class="ltx_td ltx_border_tt" id="S3.T1.4.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.2.1" style="font-size:70%;">Category</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.3.1" style="font-size:70%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.4.1" style="font-size:70%;">Data Source</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.4.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.5.1" style="font-size:70%;">Synthetic Data</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.4.1.1.6"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.6.1" style="font-size:70%;">Date</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.2.2.1" rowspan="18"><span class="ltx_text" id="S3.T1.4.2.2.1.1" style="font-size:70%;">General model distillation</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.2.2.2" rowspan="12"><span class="ltx_text" id="S3.T1.4.2.2.2.1" style="font-size:70%;">Uni-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.2.2.3"><span class="ltx_text" id="S3.T1.4.2.2.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.2.2.4"><span class="ltx_text" id="S3.T1.4.2.2.4.1" style="font-size:70%;">1500 basic words</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.2.2.5">
<span class="ltx_text" id="S3.T1.4.2.2.5.1" style="font-size:70%;">TinyStories</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.2.2.5.2.1" style="font-size:70%;">(</span>Eldan and Li<span class="ltx_text" id="S3.T1.4.2.2.5.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib54" title="">2023</a><span class="ltx_text" id="S3.T1.4.2.2.5.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.2.2.6"><span class="ltx_text" id="S3.T1.4.2.2.6.1" style="font-size:70%;">05/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.3.3">
<td class="ltx_td ltx_align_center" id="S3.T1.4.3.3.1">
<span class="ltx_text" id="S3.T1.4.3.3.1.1" style="font-size:70%;">Genie</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.3.3.1.2.1" style="font-size:70%;">(</span>Yehudai et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.3.3.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib237" title="">2024</a><span class="ltx_text" id="S3.T1.4.3.3.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.3.3.2"><span class="ltx_text" id="S3.T1.4.3.3.2.1" style="font-size:70%;">ELI5, NQ and ASQA</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.3.3.3"><span class="ltx_text" id="S3.T1.4.3.3.3.1" style="font-size:70%;">Wish-QA, etc.</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.3.3.4"><span class="ltx_text" id="S3.T1.4.3.3.4.1" style="font-size:70%;">01/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.4.4">
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.4.1">
<span class="ltx_text" id="S3.T1.4.4.4.1.1" style="font-size:70%;">Self-Instruct</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.4.4.1.2.1" style="font-size:70%;">(</span>Wang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.4.4.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib211" title="">2023b</a><span class="ltx_text" id="S3.T1.4.4.4.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.4.2"><span class="ltx_text" id="S3.T1.4.4.4.2.1" style="font-size:70%;">175 human-written samples</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.4.3"><span class="ltx_text" id="S3.T1.4.4.4.3.1" style="font-size:70%;">52k instructions</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4"><span class="ltx_text" id="S3.T1.4.4.4.4.1" style="font-size:70%;">12/2022</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.5.5">
<td class="ltx_td ltx_align_center" id="S3.T1.4.5.5.1">
<span class="ltx_text" id="S3.T1.4.5.5.1.1" style="font-size:70%;">Evol-Instruct</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.5.5.1.2.1" style="font-size:70%;">(</span>Xu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.5.5.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib224" title="">2023b</a><span class="ltx_text" id="S3.T1.4.5.5.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.5.5.2"><span class="ltx_text" id="S3.T1.4.5.5.2.1" style="font-size:70%;">Alpaca</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.5.5.3"><span class="ltx_text" id="S3.T1.4.5.5.3.1" style="font-size:70%;">Evol-instruct data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.5.5.4"><span class="ltx_text" id="S3.T1.4.5.5.4.1" style="font-size:70%;">04/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.6.6">
<td class="ltx_td ltx_align_center" id="S3.T1.4.6.6.1">
<span class="ltx_text" id="S3.T1.4.6.6.1.1" style="font-size:70%;">Iterative Question Composing</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.6.6.1.2.1" style="font-size:70%;">(</span>Liu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.6.6.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib132" title="">2024f</a><span class="ltx_text" id="S3.T1.4.6.6.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.6.6.2"><span class="ltx_text" id="S3.T1.4.6.6.2.1" style="font-size:70%;">MATH, MeTaMathQA</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.6.6.3"><span class="ltx_text" id="S3.T1.4.6.6.3.1" style="font-size:70%;">MMIQC</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.6.6.4"><span class="ltx_text" id="S3.T1.4.6.6.4.1" style="font-size:70%;">01/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.7.7">
<td class="ltx_td ltx_align_center" id="S3.T1.4.7.7.1"><span class="ltx_text" id="S3.T1.4.7.7.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.7.7.2"><span class="ltx_text" id="S3.T1.4.7.7.2.1" style="font-size:70%;">MATH, AQuA, etc.</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.7.7.3">
<span class="ltx_text" id="S3.T1.4.7.7.3.1" style="font-size:70%;">MathInstruct</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.7.7.3.2.1" style="font-size:70%;">(</span>Yue et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.7.7.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib245" title="">2023b</a><span class="ltx_text" id="S3.T1.4.7.7.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.7.7.4"><span class="ltx_text" id="S3.T1.4.7.7.4.1" style="font-size:70%;">09/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.8.8">
<td class="ltx_td ltx_align_center" id="S3.T1.4.8.8.1"><span class="ltx_text" id="S3.T1.4.8.8.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.8.8.2"><span class="ltx_text" id="S3.T1.4.8.8.2.1" style="font-size:70%;">Stack</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.8.8.3">
<span class="ltx_text" id="S3.T1.4.8.8.3.1" style="font-size:70%;">Case2Code</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.8.8.3.2.1" style="font-size:70%;">(</span>Shao et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.8.8.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib181" title="">2024a</a><span class="ltx_text" id="S3.T1.4.8.8.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.8.8.4"><span class="ltx_text" id="S3.T1.4.8.8.4.1" style="font-size:70%;">07/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.9.9">
<td class="ltx_td ltx_align_center" id="S3.T1.4.9.9.1">
<span class="ltx_text" id="S3.T1.4.9.9.1.1" style="font-size:70%;">OSS-Instruct</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.9.9.1.2.1" style="font-size:70%;">(</span>Wei et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.9.9.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib216" title="">2024</a><span class="ltx_text" id="S3.T1.4.9.9.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.9.9.2"><span class="ltx_text" id="S3.T1.4.9.9.2.1" style="font-size:70%;">starcoderdata</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.9.9.3"><span class="ltx_text" id="S3.T1.4.9.9.3.1" style="font-size:70%;">OSS-Instruct data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.9.9.4"><span class="ltx_text" id="S3.T1.4.9.9.4.1" style="font-size:70%;">12/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.10.10">
<td class="ltx_td ltx_align_center" id="S3.T1.4.10.10.1">
<span class="ltx_text" id="S3.T1.4.10.10.1.1" style="font-size:70%;">DIALOGIC</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.10.10.1.2.1" style="font-size:70%;">(</span>Li et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.10.10.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib123" title="">2022a</a><span class="ltx_text" id="S3.T1.4.10.10.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.10.10.2"><span class="ltx_text" id="S3.T1.4.10.10.2.1" style="font-size:70%;">MultiWOZ</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.10.10.3"><span class="ltx_text" id="S3.T1.4.10.10.3.1" style="font-size:70%;">task-oriented dialogues</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.10.10.4"><span class="ltx_text" id="S3.T1.4.10.10.4.1" style="font-size:70%;">10/2022</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.11.11">
<td class="ltx_td ltx_align_center" id="S3.T1.4.11.11.1"><span class="ltx_text" id="S3.T1.4.11.11.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.11.11.2"><span class="ltx_text" id="S3.T1.4.11.11.2.1" style="font-size:70%;">Meta topics, materials, etc.</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.11.11.3">
<span class="ltx_text" id="S3.T1.4.11.11.3.1" style="font-size:70%;">UltraChat</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.11.11.3.2.1" style="font-size:70%;">(</span>Ding et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.11.11.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib45" title="">2023</a><span class="ltx_text" id="S3.T1.4.11.11.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.11.11.4"><span class="ltx_text" id="S3.T1.4.11.11.4.1" style="font-size:70%;">05/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.12.12">
<td class="ltx_td ltx_align_center" id="S3.T1.4.12.12.1">
<span class="ltx_text" id="S3.T1.4.12.12.1.1" style="font-size:70%;">MAGPIE</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.12.12.1.2.1" style="font-size:70%;">(</span>Xu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.12.12.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib228" title="">2024b</a><span class="ltx_text" id="S3.T1.4.12.12.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.12.12.2"><span class="ltx_text" id="S3.T1.4.12.12.2.1" style="font-size:70%;">predefined instruction template</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.12.12.3"><span class="ltx_text" id="S3.T1.4.12.12.3.1" style="font-size:70%;">MAGPIE-Air, etc.</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.12.12.4"><span class="ltx_text" id="S3.T1.4.12.12.4.1" style="font-size:70%;">06/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.13.13">
<td class="ltx_td ltx_align_center" id="S3.T1.4.13.13.1">
<span class="ltx_text" id="S3.T1.4.13.13.1.1" style="font-size:70%;">Generator prompts</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.13.13.1.2.1" style="font-size:70%;">(</span>Chen et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.13.13.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib20" title="">2024c</a><span class="ltx_text" id="S3.T1.4.13.13.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.13.13.2"><span class="ltx_text" id="S3.T1.4.13.13.2.1" style="font-size:70%;">hand-written meta-prompts</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.13.13.3"><span class="ltx_text" id="S3.T1.4.13.13.3.1" style="font-size:70%;">GenQA</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.13.13.4"><span class="ltx_text" id="S3.T1.4.13.13.4.1" style="font-size:70%;">06/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.14.14">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.14.14.1" rowspan="6"><span class="ltx_text" id="S3.T1.4.14.14.1.1" style="font-size:70%;">multi-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.14.14.2">
<span class="ltx_text" id="S3.T1.4.14.14.2.1" style="font-size:70%;">Genixer</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.14.14.2.2.1" style="font-size:70%;">(</span>Zhao et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.14.14.2.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib267" title="">2023a</a><span class="ltx_text" id="S3.T1.4.14.14.2.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.14.14.3"><span class="ltx_text" id="S3.T1.4.14.14.3.1" style="font-size:70%;">VQAv2, VG, etc.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.14.14.4"><span class="ltx_text" id="S3.T1.4.14.14.4.1" style="font-size:70%;">Genixer-915K</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.14.14.5"><span class="ltx_text" id="S3.T1.4.14.14.5.1" style="font-size:70%;">12/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.15.15">
<td class="ltx_td ltx_align_center" id="S3.T1.4.15.15.1">
<span class="ltx_text" id="S3.T1.4.15.15.1.1" style="font-size:70%;">Multimodal self-instruct</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.15.15.1.2.1" style="font-size:70%;">(</span>Zhang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.15.15.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib264" title="">2024b</a><span class="ltx_text" id="S3.T1.4.15.15.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.15.15.2"><span class="ltx_text" id="S3.T1.4.15.15.2.1" style="font-size:70%;">key word seeds</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.15.15.3"><span class="ltx_text" id="S3.T1.4.15.15.3.1" style="font-size:70%;">multimodal self-instruct dataset</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.15.15.4"><span class="ltx_text" id="S3.T1.4.15.15.4.1" style="font-size:70%;">07/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.16.16">
<td class="ltx_td ltx_align_center" id="S3.T1.4.16.16.1"><span class="ltx_text" id="S3.T1.4.16.16.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.16.16.2"><span class="ltx_text" id="S3.T1.4.16.16.2.1" style="font-size:70%;">specific characteristics</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.16.16.3">
<span class="ltx_text" id="S3.T1.4.16.16.3.1" style="font-size:70%;">ChartLlama</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.16.16.3.2.1" style="font-size:70%;">(</span>Han et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.16.16.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib74" title="">2023</a><span class="ltx_text" id="S3.T1.4.16.16.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.16.16.4"><span class="ltx_text" id="S3.T1.4.16.16.4.1" style="font-size:70%;">11/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.17.17">
<td class="ltx_td ltx_align_center" id="S3.T1.4.17.17.1"><span class="ltx_text" id="S3.T1.4.17.17.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.17.17.2"><span class="ltx_text" id="S3.T1.4.17.17.2.1" style="font-size:70%;">Flickr30k, Visual Genome</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.17.17.3">
<span class="ltx_text" id="S3.T1.4.17.17.3.1" style="font-size:70%;">ComVint</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.17.17.3.2.1" style="font-size:70%;">(</span>Du et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.17.17.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib50" title="">2023</a><span class="ltx_text" id="S3.T1.4.17.17.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.17.17.4"><span class="ltx_text" id="S3.T1.4.17.17.4.1" style="font-size:70%;">11/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.18.18">
<td class="ltx_td ltx_align_center" id="S3.T1.4.18.18.1">
<span class="ltx_text" id="S3.T1.4.18.18.1.1" style="font-size:70%;">AnyGPT</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.18.18.1.2.1" style="font-size:70%;">(</span>Zhan et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.18.18.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib252" title="">2024</a><span class="ltx_text" id="S3.T1.4.18.18.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.18.18.2"><span class="ltx_text" id="S3.T1.4.18.18.2.1" style="font-size:70%;">100 meta topics</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.18.18.3"><span class="ltx_text" id="S3.T1.4.18.18.3.1" style="font-size:70%;">AnyInstruct-108k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.18.18.4"><span class="ltx_text" id="S3.T1.4.18.18.4.1" style="font-size:70%;">02/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.19.19">
<td class="ltx_td ltx_align_center" id="S3.T1.4.19.19.1"><span class="ltx_text" id="S3.T1.4.19.19.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.19.19.2"><span class="ltx_text" id="S3.T1.4.19.19.2.1" style="font-size:70%;">100k images</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.19.19.3">
<span class="ltx_text" id="S3.T1.4.19.19.3.1" style="font-size:70%;">ShareGPT-4V</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.19.19.3.2.1" style="font-size:70%;">(</span>Chen et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.19.19.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib22" title="">2023a</a><span class="ltx_text" id="S3.T1.4.19.19.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.19.19.4"><span class="ltx_text" id="S3.T1.4.19.19.4.1" style="font-size:70%;">11/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.20.20">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.4.20.20.1" rowspan="14"><span class="ltx_text" id="S3.T1.4.20.20.1.1" style="font-size:70%;">Data augmentation</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.20.20.2" rowspan="13"><span class="ltx_text" id="S3.T1.4.20.20.2.1" style="font-size:70%;">Uni-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.20.20.3">
<span class="ltx_text" id="S3.T1.4.20.20.3.1" style="font-size:70%;">FullAnno</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.20.20.3.2.1" style="font-size:70%;">(</span>Hao et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.20.20.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib75" title="">2024</a><span class="ltx_text" id="S3.T1.4.20.20.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.20.20.4"><span class="ltx_text" id="S3.T1.4.20.20.4.1" style="font-size:70%;">Coco, Visual Genome</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.20.20.5"><span class="ltx_text" id="S3.T1.4.20.20.5.1" style="font-size:70%;">re-annotated COCO and Visual Genome</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.4.20.20.6"><span class="ltx_text" id="S3.T1.4.20.20.6.1" style="font-size:70%;">09/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.21.21">
<td class="ltx_td ltx_align_center" id="S3.T1.4.21.21.1"><span class="ltx_text" id="S3.T1.4.21.21.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.21.21.2"><span class="ltx_text" id="S3.T1.4.21.21.2.1" style="font-size:70%;">MultiWOZ, etc.</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.21.21.3">
<span class="ltx_text" id="S3.T1.4.21.21.3.1" style="font-size:70%;">MultiWOZ one-shot, etc.</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.21.21.3.2.1" style="font-size:70%;">(</span>Labruna et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.21.21.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib100" title="">2023</a><span class="ltx_text" id="S3.T1.4.21.21.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.21.21.4"><span class="ltx_text" id="S3.T1.4.21.21.4.1" style="font-size:70%;">05/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.22.22">
<td class="ltx_td ltx_align_center" id="S3.T1.4.22.22.1"><span class="ltx_text" id="S3.T1.4.22.22.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.22.22.2"><span class="ltx_text" id="S3.T1.4.22.22.2.1" style="font-size:70%;">GSM8k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.22.22.3">
<span class="ltx_text" id="S3.T1.4.22.22.3.1" style="font-size:70%;">TinyGSM</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.22.22.3.2.1" style="font-size:70%;">(</span>Liu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.22.22.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib129" title="">2023a</a><span class="ltx_text" id="S3.T1.4.22.22.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.22.22.4"><span class="ltx_text" id="S3.T1.4.22.22.4.1" style="font-size:70%;">12/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.23.23">
<td class="ltx_td ltx_align_center" id="S3.T1.4.23.23.1">
<span class="ltx_text" id="S3.T1.4.23.23.1.1" style="font-size:70%;">GPT3Mix</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.23.23.1.2.1" style="font-size:70%;">(</span>Yoo et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.23.23.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib238" title="">2021</a><span class="ltx_text" id="S3.T1.4.23.23.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.23.23.2"><span class="ltx_text" id="S3.T1.4.23.23.2.1" style="font-size:70%;">SST-2, CR, et.</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.23.23.3"><span class="ltx_text" id="S3.T1.4.23.23.3.1" style="font-size:70%;">RT20</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.23.23.4"><span class="ltx_text" id="S3.T1.4.23.23.4.1" style="font-size:70%;">04/2021</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.24.24">
<td class="ltx_td ltx_align_center" id="S3.T1.4.24.24.1">
<span class="ltx_text" id="S3.T1.4.24.24.1.1" style="font-size:70%;">CORE</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.24.24.1.2.1" style="font-size:70%;">(</span>Dixit et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.24.24.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib46" title="">2022</a><span class="ltx_text" id="S3.T1.4.24.24.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.24.24.2"><span class="ltx_text" id="S3.T1.4.24.24.2.1" style="font-size:70%;">IMDB</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.24.24.3"><span class="ltx_text" id="S3.T1.4.24.24.3.1" style="font-size:70%;">augmented IMDB</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.24.24.4"><span class="ltx_text" id="S3.T1.4.24.24.4.1" style="font-size:70%;">10/2022</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.25.25">
<td class="ltx_td ltx_align_center" id="S3.T1.4.25.25.1">
<span class="ltx_text" id="S3.T1.4.25.25.1.1" style="font-size:70%;">DISCO</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.25.25.1.2.1" style="font-size:70%;">(</span>Chen et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.25.25.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib28" title="">2022</a><span class="ltx_text" id="S3.T1.4.25.25.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.25.25.2"><span class="ltx_text" id="S3.T1.4.25.25.2.1" style="font-size:70%;">SNLI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.25.25.3"><span class="ltx_text" id="S3.T1.4.25.25.3.1" style="font-size:70%;">DISCO dataset</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.25.25.4"><span class="ltx_text" id="S3.T1.4.25.25.4.1" style="font-size:70%;">12/2022</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.26.26">
<td class="ltx_td ltx_align_center" id="S3.T1.4.26.26.1">
<span class="ltx_text" id="S3.T1.4.26.26.1.1" style="font-size:70%;">Co-Annotating</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.26.26.1.2.1" style="font-size:70%;">(</span>Li et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.26.26.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib117" title="">2023d</a><span class="ltx_text" id="S3.T1.4.26.26.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.26.26.2"><span class="ltx_text" id="S3.T1.4.26.26.2.1" style="font-size:70%;">six classification datasets</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.26.26.3"><span class="ltx_text" id="S3.T1.4.26.26.3.1" style="font-size:70%;">co-annotated dataset</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.26.26.4"><span class="ltx_text" id="S3.T1.4.26.26.4.1" style="font-size:70%;">10/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.27.27">
<td class="ltx_td ltx_align_center" id="S3.T1.4.27.27.1">
<span class="ltx_text" id="S3.T1.4.27.27.1.1" style="font-size:70%;">Dialgen</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.27.27.1.2.1" style="font-size:70%;">(</span>Lu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.27.27.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib143" title="">2023</a><span class="ltx_text" id="S3.T1.4.27.27.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.27.27.2"><span class="ltx_text" id="S3.T1.4.27.27.2.1" style="font-size:70%;">AIC</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.27.27.3"><span class="ltx_text" id="S3.T1.4.27.27.3.1" style="font-size:70%;">Dialgen-AIC</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.27.27.4"><span class="ltx_text" id="S3.T1.4.27.27.4.1" style="font-size:70%;">07/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.28.28">
<td class="ltx_td ltx_align_center" id="S3.T1.4.28.28.1">
<span class="ltx_text" id="S3.T1.4.28.28.1.1" style="font-size:70%;">ToolCoder</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.28.28.1.2.1" style="font-size:70%;">(</span>Zhang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.28.28.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib260" title="">2023e</a><span class="ltx_text" id="S3.T1.4.28.28.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.28.28.2"><span class="ltx_text" id="S3.T1.4.28.28.2.1" style="font-size:70%;">CodeSearchNet-Pyhthon</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.28.28.3"><span class="ltx_text" id="S3.T1.4.28.28.3.1" style="font-size:70%;">53k augmented data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.28.28.4"><span class="ltx_text" id="S3.T1.4.28.28.4.1" style="font-size:70%;">05/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.29.29">
<td class="ltx_td ltx_align_center" id="S3.T1.4.29.29.1"><span class="ltx_text" id="S3.T1.4.29.29.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.29.29.2"><span class="ltx_text" id="S3.T1.4.29.29.2.1" style="font-size:70%;">100 hand-designed scripts</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.29.29.3">
<span class="ltx_text" id="S3.T1.4.29.29.3.1" style="font-size:70%;">AMPS</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.29.29.3.2.1" style="font-size:70%;">(</span>Hendrycks et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.29.29.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib78" title="">2021</a><span class="ltx_text" id="S3.T1.4.29.29.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.29.29.4"><span class="ltx_text" id="S3.T1.4.29.29.4.1" style="font-size:70%;">03/2021</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.30.30">
<td class="ltx_td ltx_align_center" id="S3.T1.4.30.30.1">
<span class="ltx_text" id="S3.T1.4.30.30.1.1" style="font-size:70%;">Mind’s Eye</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.30.30.1.2.1" style="font-size:70%;">(</span>Liu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.30.30.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib137" title="">2022</a><span class="ltx_text" id="S3.T1.4.30.30.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.30.30.2"><span class="ltx_text" id="S3.T1.4.30.30.2.1" style="font-size:70%;">physical reasoning questions</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.30.30.3"><span class="ltx_text" id="S3.T1.4.30.30.3.1" style="font-size:70%;">UTOPIA</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.30.30.4"><span class="ltx_text" id="S3.T1.4.30.30.4.1" style="font-size:70%;">10/2022</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.31.31">
<td class="ltx_td ltx_align_center" id="S3.T1.4.31.31.1"><span class="ltx_text" id="S3.T1.4.31.31.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.31.31.2"><span class="ltx_text" id="S3.T1.4.31.31.2.1" style="font-size:70%;">scientific papers, web data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.31.31.3">
<span class="ltx_text" id="S3.T1.4.31.31.3.1" style="font-size:70%;">Proof-Pile-2</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.31.31.3.2.1" style="font-size:70%;">(</span>Azerbayev et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.31.31.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib7" title="">2023</a><span class="ltx_text" id="S3.T1.4.31.31.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.31.31.4"><span class="ltx_text" id="S3.T1.4.31.31.4.1" style="font-size:70%;">10/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.32.32">
<td class="ltx_td ltx_align_center" id="S3.T1.4.32.32.1">
<span class="ltx_text" id="S3.T1.4.32.32.1.1" style="font-size:70%;">data pruning</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.32.32.1.2.1" style="font-size:70%;">(</span>Tsai et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.32.32.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib201" title="">2024</a><span class="ltx_text" id="S3.T1.4.32.32.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.32.32.2"><span class="ltx_text" id="S3.T1.4.32.32.2.1" style="font-size:70%;">MBPP, HumanEval</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.32.32.3"><span class="ltx_text" id="S3.T1.4.32.32.3.1" style="font-size:70%;">pruning dataset</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.32.32.4"><span class="ltx_text" id="S3.T1.4.32.32.4.1" style="font-size:70%;">07/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.33.33">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.4.33.33.1"><span class="ltx_text" id="S3.T1.4.33.33.1.1" style="font-size:70%;">multi-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.4.33.33.2">
<span class="ltx_text" id="S3.T1.4.33.33.2.1" style="font-size:70%;">ALIA</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.4.33.33.2.2.1" style="font-size:70%;">(</span>Dunlap et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T1.4.33.33.2.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib52" title="">2023</a><span class="ltx_text" id="S3.T1.4.33.33.2.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.4.33.33.3"><span class="ltx_text" id="S3.T1.4.33.33.3.1" style="font-size:70%;">CUB</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.4.33.33.4"><span class="ltx_text" id="S3.T1.4.33.33.4.1" style="font-size:70%;">task-relevant synthetic images</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.4.33.33.5"><span class="ltx_text" id="S3.T1.4.33.33.5.1" style="font-size:70%;">05/2023</span></td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.1.1">General Model Distillation.</span>
</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">This way aims to leverage the powerful capabilities of general LLMs to distill high-quality data.
According to the approach and data modality, we further divided general model distillation into five categories: synthesize from seeds, synthesize reasoning steps, synthesize with controllability, synthesize from scratch, and synthesize multimodal data.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p2.1.1">Synthesize from Seeds.</span>
To synthesize datasets for specific tasks, prompting LLMs with a small number of relevant examples can effectively produce high-quality datasets at a low cost.
For instance, to investigate “how small can an LLM be to achieve certain capabilities”, TinyStories<cite class="ltx_cite ltx_citemacro_citep">(Eldan and Li, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib54" title="">2023</a>)</cite> is constructed by instructing an LLM to generate stories that combine three words randomly chosen from 1500 basic words, and it can be used to train and evaluate language models.
Based on the collected large-scale functions, Case2Code<cite class="ltx_cite ltx_citemacro_citep">(Shao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib181" title="">2024a</a>)</cite> incorporates LLMs to generate suitable inputs for these functions and utilizes the code interpreter to calculate their corresponding outputs.
Due to the potential insufficiency in quantity and diversity of single-round synthetic data, methods for iterative data synthesis are investigated.
For example, Self-Instruct<cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib211" title="">2023b</a>)</cite> can be repeated for many iterations to accumulate a substantial volume of tasks. In each iteration, an LLM is prompted to generate new instructions from a small seed set, then creates input-output instances for each instruction independently.
Similarly, Evol-Instruct<cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib224" title="">2023b</a>)</cite> can be conducted over multiple rounds to gather a sufficient dataset encompassing various complexities. In each evolution, in-depth and in-breadth evolving are employed to either enhance the basic instructions to more sophisticated ones or innovate entirely new directives.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p3.1.1">Synthesize Reasoning Steps.</span>
To enhance the reasoning capability of LLMs, additional reasoning steps are generated in the process of data synthesis.
The synthetic question-response pairs in MMIQC<cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib132" title="">2024f</a>)</cite> are iteratively constructed by augmenting the initial problems and adding additional reasoning steps without altering their intrinsic logical structure.
Similarly, an effective generation strategy is put forward in which an LLM is requested to synthesize chain-of-thought (CoT) answers after question generation and verification<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib110" title="">2024d</a>)</cite>.
Based on the generation of question-CoT pairs through Self-Instruct, MathInstruct<cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib245" title="">2023b</a>)</cite> further supplements the Program-of-Thought (PoT) rationale to simplify the math-solving process.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p4">
<p class="ltx_p" id="S3.SS1.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p4.1.1">Synthesize with Controllability.</span>
To control the quality of synthetic data, researches are conducted into techniques for data synthesis with controllability.
Driven by the goal of reducing the potential bias of synthetic data,
OSS-Instruct<cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib216" title="">2024</a>)</cite> utilizes open-source seed code snippets to prompt an LLM in generating coding problems and corresponding solutions. The seed snippets provide controllability of the generation and encourage the LLM to synthesize a variety of coding problems.
Similarly, Genie<cite class="ltx_cite ltx_citemacro_citep">(Yehudai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib237" title="">2024</a>)</cite> prompts an LLM with four content-example pairs to generate a synthetic example to match the extracted content with no example.
In addition, seeded with a few annotated dialogues, DIALOGIC<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib123" title="">2022a</a>)</cite> guides GPT-3 to synthesize annotated dialogues in a controllable way, in which an auxiliary generator and slot-value match filter are utilized to mitigate de-generation and over-generation issues, respectively.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p5">
<p class="ltx_p" id="S3.SS1.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p5.1.1">Synthesize from Scratch.</span>
Another approach avoids reliance on the seed dataset, synthesizing data from scratch.
For instance, UltraChat<cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib45" title="">2023</a>)</cite> is composed of questions about the world, creation and generation, and assistance on existing materials, among which questions about the world request ChatGPT to generate meta topics, subtopics, and questions about conceptions from scratch.
As aligned LLMs can generate user queries due to the auto-regression nature, Magpie<cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib228" title="">2024b</a>)</cite> directly constructs instruction data by prompting aligned LLMs with a pre-query template to generate instructions along with their corresponding responses.
Focusing on generating large instruction datasets from a single prompt, generator prompts<cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib20" title="">2024c</a>)</cite> boost the output diversity by asking an LLM to produce a long list of possible choices and select one of the candidates at random.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p6">
<p class="ltx_p" id="S3.SS1.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p6.1.1">Synthesize Multimodal Data.</span>
Similar to unimodal, Prompting powerful LLMs like GPT to synthesize data based on seed sets is also the most common method for multimodal data synthesis.
For instance, ShareGPT4V<cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib22" title="">2023a</a>)</cite> consists of 100K high-quality captions that are produced by employing data-specific prompts to guide GPT4-vision in generating comprehensive descriptions for supervised fine-tuning. Additionally, an alternative caption model is fine-tuned on the 100k high-quality captions, to expand the number of captions for pre-training.
Taking into account the possible issue of overly simplistic cross-modal instructions generated by LLMs, ComVint<cite class="ltx_cite ltx_citemacro_citep">(Du et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib50" title="">2023</a>)</cite>, when presented with an image that has available annotations, adopts a pipeline that includes synthesis, complication, and reformulation.
Standing distinct from the previous data synthesis approach, StableLlava<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib122" title="">2023f</a>)</cite> synchronously synthesizes images and dialogues.
The method first employs ChatGPT to craft prompts for image generation and develops content-rich dialogues, subsequently leverages StabeDiffusion to synthesize images from these prompts.
Multimodal data can also be synthesized from scratch.
To create an image and a corresponding instruction from scratch, Multi-modal Self-Instruct<cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib264" title="">2024b</a>)</cite> first instructs the LLM to conceive an original visual concept. Subsequently, it produces detailed code to visualize the idea. Once the desired image is synthesized, the LLMs self-instruct multiple sets of high-quality question-answer pairs tailored to the visual content.
Likewise, in the field of interpreting chart figures, ChartLlama<cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib74" title="">2023</a>)</cite> initially leverages the capabilities of GPT-4 to generate chart data by providing specific attributes like topics, distributions, and trends. Following this, GPT is further employed to generate both chart figure and the associated instruction-answer data.
AnyInstruct-108k<cite class="ltx_cite ltx_citemacro_citep">(Zhan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib252" title="">2024</a>)</cite> is constructed by a two-stage approach including the generation of text-based conversations incorporating multimodal elements from meta topics and text-to-multimodality conversion.
Additionally, there are methods that empowers MLLM to synthesize data rather than prompting GPT.
Genixer<cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib267" title="">2023a</a>)</cite>, a holistic data generation pipeline, consists of four key steps including instruction data collection, instruction template design, empowering MLLMs, and data generation and filtering.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.1.1">Data Augmentation.</span>
</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">Data augmentation aims to further process existing data to obtain a more diverse set of high-quality data. In the present survey, we divide the existing methods of data augmentation into four groups: data labeling, data reformation, co-annotation, and non-LLM-driven.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p2.1.1">Data Labeling.</span>
Data labeling aims to harness the language comprehension abilities of LLMs for annotating unlabeled datasets<cite class="ltx_cite ltx_citemacro_citep">(Alizadeh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib4" title="">2023</a>; Gilardi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib64" title="">2023</a>)</cite>.
Based on a balanced sample with 500 tweets from Republican and Democratic politicians, ChatGPT-4 is utilized to annotate the political affiliation<cite class="ltx_cite ltx_citemacro_citep">(Törnberg, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib199" title="">2023</a>)</cite>. The results indicate that LLM annotations display higher accuracy and lower bias than human classifiers.
To apply to distinctive annotation tasks, one approach<cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib276" title="">2023b</a>)</cite> begins by creating a generalized prompt template. This template is then utilized to generate a ChatGPT prompt aims at extracting labels in alignment with the dataset’s original annotation methodology.
Similarly, another method involves initially creating dialogues that closely match the content of a reference dialogue, subsequently prompting the LLM to label the generated conversation using the same annotation schema provided in the existing repository<cite class="ltx_cite ltx_citemacro_citep">(Labruna et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib100" title="">2023</a>)</cite>.
Further, scholars have studied methods to improve the quality of LLMs’ labeled data using additional information.
For instance, FullAnno<cite class="ltx_cite ltx_citemacro_citep">(Hao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib75" title="">2024</a>)</cite> instructs LLMs to acquire comprehensive annotations for images with prompts including the category and position of objects, region description, and text information within the image.
In the field of speech emotion recognition, one approach<cite class="ltx_cite ltx_citemacro_citep">(Latif et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib104" title="">2023</a>)</cite> first annotates samples based solely on text. Afterwards, it enhances the annotation by integrating audio features and gender information alongside textual data. In addition, a VQ-VAE is employed to produce a 64-dimensional discrete representation of the audio, which is supplied to the LLM.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p3.1.1">Data Reformation.</span>
Data reformation attempts to transform existing data into a wider array of variations, and it typically involves the utilization of prompt engineering to guide LLMs in generating reformatted data.
For example, TinyGSM<cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib129" title="">2023a</a>)</cite> is constructed by prompting an LLM to generate problem variants from GSM8K, subsequently filtering out low-quality instances.
Similarly, GPT3Mix<cite class="ltx_cite ltx_citemacro_citep">(Yoo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib238" title="">2021</a>)</cite> extracts augmentation from the generation of the LLM by constructing a task-specific prompt from the selected examples and meta-information about the dataset.
Data reformation is also widely applied in generating counterfactual augmented data.
Specifically, CORE<cite class="ltx_cite ltx_citemacro_citep">(Dixit et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib46" title="">2022</a>)</cite> initially learns to retrieve relevant text excerpts. Following this the retrieved excerpts along with instructions and demonstrations are supplied as a prompt to GPT-3 to counterfactually edit input text.
In addition, DISCO<cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib28" title="">2022</a>)</cite> firstly decomposes given task instances into spans using linguistic processing tools, and subsequently employs prompt engineering and in-context learning with an LLM to overgenerate a diverse set of perturbations for these instances.
For multi-modality, ALIA<cite class="ltx_cite ltx_citemacro_citep">(Dunlap et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib52" title="">2023</a>)</cite> first generates captions for each image, and summarizes the captions into a short list of domain descriptions with an LLM. Then uses these descriptions to generate edits of the training data with Stable Diffusion.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p4">
<p class="ltx_p" id="S3.SS1.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p4.1.1">Co-Annotation.</span>
Co-annotation refers to the process where humans and LLMs annotate unlabeled data together.
For instance, Toolcoder<cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib260" title="">2023e</a>)</cite> utilizes human-written input-output within the prompt to direct ChatGPT in annotating tool-augmented dataset.
To address the problem of low agreement between annotators, ChatGPT is used to augment the annotation process with phrases identified as relevant features and brief explanations<cite class="ltx_cite ltx_citemacro_citep">(Bertaglia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib12" title="">2023</a>)</cite>.
Investigations on allocating data within the same dataset to humans and ChatGPT can achieve a more efficient and more accurate annotation.
Specifically, the CoAnnotating<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib117" title="">2023d</a>)</cite> framework automatically decides whether each data instance should be annotated by humans or by the LLMs by computing the uncertainty level of the LLMs’ annotations.
There are also iterative co-annotation methods for data augmentation.
For example, initiated with a task prompt consisting of a description for desired dialogue and dialogue history, Dialgen<cite class="ltx_cite ltx_citemacro_citep">(Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib143" title="">2023</a>)</cite> first proposes a candidate subdialogue, subsequently validates, edits and annotates teh generated subdialogue by human reviewers before requesting continuation via an updated prompt to the LLM. The process can be repeated multiple times until the dialogue is complete.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p5">
<p class="ltx_p" id="S3.SS1.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p5.1.1">Non LLM-Driven.</span>
Some methods do not use LLMs to synthesize or filter high-quality data.
For exmaple, AMPS<cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib78" title="">2021</a>)</cite>, an extensive and varied corpus for mathematics pretraining, comprises over 5 million problems generated with Mathematica scripts, based on 100 meticulously crafted modules.
In the field of physics, Mind’s Eye<cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib137" title="">2022</a>)</cite> utilizes a computational physics engine to produce ground-truth answers for the UTOPIA multi-task physics alignment dataset, designed to assess the ability of LLMs to understand fundamental physical laws.
Besides, filtering and pruning strategy are utilized for data augmentation.
For instance, Proof-Pile-2<cite class="ltx_cite ltx_citemacro_citep">(Azerbayev et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib7" title="">2023</a>)</cite> is an extensive dataset with 55B tokens of mathematics and mathematical code, enriched by filtering high-quality data from publicly available resources.
Recognizing substantial redundancies in synthetic training data, an efficient and scalable pruning strategy<cite class="ltx_cite ltx_citemacro_citep">(Tsai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib201" title="">2024</a>)</cite> is proposed which encompasses encoding, dimensionality reduction, clustering, and pruning.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Pre-Training</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">During the stage of pre-training, data synthesis and augmentation can provide LLMs with abundant, diverse, and controllable training data cost-effectively and efficiently, thereby enhancing model performance and reducing bias.
We also discuss the existing methods from three perspective: model self-improvement, general model distillation, and data augmentation.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.2.1.1" style="font-size:90%;">Table 2</span>. </span><span class="ltx_text" id="S3.T2.3.2" style="font-size:90%;">Data synthesis and augmentation in pre-training. Method outlines the techniques presented by each research. Data source and synthetic data indicate the original data used to generate synthetic data and the synthetic data created for pre-training, respectively. Base model and pre-trained model indicate the foundational models and the models that have undergone pre-training, respectively. A dash (-) in any cell denotes that the respective content was not mentioned in the cited literature.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T2.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.4.1.1">
<td class="ltx_td ltx_border_tt" id="S3.T2.4.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.2.1" style="font-size:70%;">Category</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.3.1" style="font-size:70%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.4.1" style="font-size:70%;">Data Source</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.4.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.5.1" style="font-size:70%;">Synthetic Data</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.4.1.1.6"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.6.1" style="font-size:70%;">Base Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.4.1.1.7"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.7.1" style="font-size:70%;">Pre-trained Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.4.1.1.8"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.8.1" style="font-size:70%;">Date</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.2.2.1"><span class="ltx_text" id="S3.T2.4.2.2.1.1" style="font-size:70%;">Model self-improvement</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.2.2.2"><span class="ltx_text" id="S3.T2.4.2.2.2.1" style="font-size:70%;">Multi-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.2.2.3"><span class="ltx_text" id="S3.T2.4.2.2.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.2.2.4"><span class="ltx_text" id="S3.T2.4.2.2.4.1" style="font-size:70%;">MMC4, Coyo, ShareGPT4v</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.2.2.5"><span class="ltx_text" id="S3.T2.4.2.2.5.1" style="font-size:70%;">Images with re-captioned texts</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.2.2.6"><span class="ltx_text" id="S3.T2.4.2.2.6.1" style="font-size:70%;">VILA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.2.2.7">
<span class="ltx_text" id="S3.T2.4.2.2.7.1" style="font-size:70%;">VILA-2</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T2.4.2.2.7.2.1" style="font-size:70%;">(</span>Fang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.4.2.2.7.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib55" title="">2024</a><span class="ltx_text" id="S3.T2.4.2.2.7.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.2.2.8"><span class="ltx_text" id="S3.T2.4.2.2.8.1" style="font-size:70%;">07/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.3.3.1" rowspan="2"><span class="ltx_text" id="S3.T2.4.3.3.1.1" style="font-size:70%;">model</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.3.3.2" rowspan="4"><span class="ltx_text" id="S3.T2.4.3.3.2.1" style="font-size:70%;">Uni-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.3.3.3"><span class="ltx_text" id="S3.T2.4.3.3.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.3.3.4"><span class="ltx_text" id="S3.T2.4.3.3.4.1" style="font-size:70%;">code snippets</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.3.3.5"><span class="ltx_text" id="S3.T2.4.3.3.5.1" style="font-size:70%;">1B tokens Python textbooks</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.3.3.6"><span class="ltx_text" id="S3.T2.4.3.3.6.1" style="font-size:70%;">1.3B parameter model</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.3.3.7">
<span class="ltx_text" id="S3.T2.4.3.3.7.1" style="font-size:70%;">phi-1-Base</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T2.4.3.3.7.2.1" style="font-size:70%;">(</span>Gunasekar et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.4.3.3.7.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib68" title="">2023</a><span class="ltx_text" id="S3.T2.4.3.3.7.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.3.3.8"><span class="ltx_text" id="S3.T2.4.3.3.8.1" style="font-size:70%;">06/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.4.4">
<td class="ltx_td ltx_align_center" id="S3.T2.4.4.4.1"><span class="ltx_text" id="S3.T2.4.4.4.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.4.4.2"><span class="ltx_text" id="S3.T2.4.4.4.2.1" style="font-size:70%;">20k topics</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.4.4.3"><span class="ltx_text" id="S3.T2.4.4.4.3.1" style="font-size:70%;">20B tokens textbooks data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.4.4.4"><span class="ltx_text" id="S3.T2.4.4.4.4.1" style="font-size:70%;">1.3B parameter model</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.4.4.5">
<span class="ltx_text" id="S3.T2.4.4.4.5.1" style="font-size:70%;">phi-1.5</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T2.4.4.4.5.2.1" style="font-size:70%;">(</span>Li et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.4.4.4.5.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib121" title="">2023b</a><span class="ltx_text" id="S3.T2.4.4.4.5.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.4.4.6"><span class="ltx_text" id="S3.T2.4.4.4.6.1" style="font-size:70%;">09/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.5.5">
<td class="ltx_td ltx_align_center" id="S3.T2.4.5.5.1"><span class="ltx_text" id="S3.T2.4.5.5.1.1" style="font-size:70%;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.5.5.2"><span class="ltx_text" id="S3.T2.4.5.5.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.5.5.3"><span class="ltx_text" id="S3.T2.4.5.5.3.1" style="font-size:70%;">conversation seed</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.5.5.4">
<span class="ltx_text" id="S3.T2.4.5.5.4.1" style="font-size:70%;">TinyDialogues</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T2.4.5.5.4.2.1" style="font-size:70%;">(</span>Feng et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.4.5.5.4.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib57" title="">2024</a><span class="ltx_text" id="S3.T2.4.5.5.4.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.5.5.5"><span class="ltx_text" id="S3.T2.4.5.5.5.1" style="font-size:70%;">GPT-2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.5.5.6"><span class="ltx_text" id="S3.T2.4.5.5.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.5.5.7"><span class="ltx_text" id="S3.T2.4.5.5.7.1" style="font-size:70%;">08/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.6.6">
<td class="ltx_td" id="S3.T2.4.6.6.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.6.6.2"><span class="ltx_text" id="S3.T2.4.6.6.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.6.6.3"><span class="ltx_text" id="S3.T2.4.6.6.3.1" style="font-size:70%;">Scientific corpora</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.6.6.4"><span class="ltx_text" id="S3.T2.4.6.6.4.1" style="font-size:70%;">SciLitIns</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.6.6.5"><span class="ltx_text" id="S3.T2.4.6.6.5.1" style="font-size:70%;">Qwen2-Base</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.6.6.6">
<span class="ltx_text" id="S3.T2.4.6.6.6.1" style="font-size:70%;">SciLitLLM</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T2.4.6.6.6.2.1" style="font-size:70%;">(</span>Li et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.4.6.6.6.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib119" title="">2024c</a><span class="ltx_text" id="S3.T2.4.6.6.6.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.6.6.7"><span class="ltx_text" id="S3.T2.4.6.6.7.1" style="font-size:70%;">08/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.7.7">
<td class="ltx_td ltx_align_center" id="S3.T2.4.7.7.1"><span class="ltx_text" id="S3.T2.4.7.7.1.1" style="font-size:70%;">distillation</span></td>
<td class="ltx_td" id="S3.T2.4.7.7.2"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.7.7.3">
<span class="ltx_text" id="S3.T2.4.7.7.3.1" style="font-size:70%;">TRAIT</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T2.4.7.7.3.2.1" style="font-size:70%;">(</span>Liang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.4.7.7.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib124" title="">2024</a><span class="ltx_text" id="S3.T2.4.7.7.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.7.7.4"><span class="ltx_text" id="S3.T2.4.7.7.4.1" style="font-size:70%;">GSM8k, OpenWebMath</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.7.7.5"><span class="ltx_text" id="S3.T2.4.7.7.5.1" style="font-size:70%;">Task-oriented synthetic passages</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.7.7.6"><span class="ltx_text" id="S3.T2.4.7.7.6.1" style="font-size:70%;">Mistral</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.7.7.7"><span class="ltx_text" id="S3.T2.4.7.7.7.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.7.7.8"><span class="ltx_text" id="S3.T2.4.7.7.8.1" style="font-size:70%;">06/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.8.8">
<td class="ltx_td" id="S3.T2.4.8.8.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.8.8.2"><span class="ltx_text" id="S3.T2.4.8.8.2.1" style="font-size:70%;">Multi-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.8.8.3"><span class="ltx_text" id="S3.T2.4.8.8.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.8.8.4"><span class="ltx_text" id="S3.T2.4.8.8.4.1" style="font-size:70%;">100 meta topics</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.8.8.5"><span class="ltx_text" id="S3.T2.4.8.8.5.1" style="font-size:70%;">AnyInstruct-108k</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.8.8.6"><span class="ltx_text" id="S3.T2.4.8.8.6.1" style="font-size:70%;">Llama 2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.8.8.7">
<span class="ltx_text" id="S3.T2.4.8.8.7.1" style="font-size:70%;">AnyGPT</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T2.4.8.8.7.2.1" style="font-size:70%;">(</span>Zhan et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.4.8.8.7.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib252" title="">2024</a><span class="ltx_text" id="S3.T2.4.8.8.7.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.8.8.8"><span class="ltx_text" id="S3.T2.4.8.8.8.1" style="font-size:70%;">02/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.9.9">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.4.9.9.1" rowspan="4"><span class="ltx_text" id="S3.T2.4.9.9.1.1" style="font-size:70%;">Augmentation</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.4.9.9.2" rowspan="4"><span class="ltx_text" id="S3.T2.4.9.9.2.1" style="font-size:70%;">Uni-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.9.9.3"><span class="ltx_text" id="S3.T2.4.9.9.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.9.9.4"><span class="ltx_text" id="S3.T2.4.9.9.4.1" style="font-size:70%;">sub-molecule groups</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.9.9.5">
<span class="ltx_text" id="S3.T2.4.9.9.5.1" style="font-size:70%;">Physics-based synthetic data</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T2.4.9.9.5.2.1" style="font-size:70%;">(</span>Liu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.4.9.9.5.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib135" title="">2024a</a><span class="ltx_text" id="S3.T2.4.9.9.5.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.9.9.6"><span class="ltx_text" id="S3.T2.4.9.9.6.1" style="font-size:70%;">MoLFormer</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.9.9.7"><span class="ltx_text" id="S3.T2.4.9.9.7.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.9.9.8"><span class="ltx_text" id="S3.T2.4.9.9.8.1" style="font-size:70%;">07/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.10.10">
<td class="ltx_td ltx_align_center" id="S3.T2.4.10.10.1">
<span class="ltx_text" id="S3.T2.4.10.10.1.1" style="font-size:70%;">WRAP</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T2.4.10.10.1.2.1" style="font-size:70%;">(</span>Maini et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.4.10.10.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib151" title="">2024</a><span class="ltx_text" id="S3.T2.4.10.10.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.10.10.2"><span class="ltx_text" id="S3.T2.4.10.10.2.1" style="font-size:70%;">C4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.10.10.3"><span class="ltx_text" id="S3.T2.4.10.10.3.1" style="font-size:70%;">Re-phrased texts</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.10.10.4"><span class="ltx_text" id="S3.T2.4.10.10.4.1" style="font-size:70%;">decoder-only transformers</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.10.10.5"><span class="ltx_text" id="S3.T2.4.10.10.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.10.10.6"><span class="ltx_text" id="S3.T2.4.10.10.6.1" style="font-size:70%;">01/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.11.11">
<td class="ltx_td ltx_align_center" id="S3.T2.4.11.11.1"><span class="ltx_text" id="S3.T2.4.11.11.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.11.11.2"><span class="ltx_text" id="S3.T2.4.11.11.2.1" style="font-size:70%;">CC100</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.11.11.3"><span class="ltx_text" id="S3.T2.4.11.11.3.1" style="font-size:70%;">knowledge-intensive multilingual data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.11.11.4"><span class="ltx_text" id="S3.T2.4.11.11.4.1" style="font-size:70%;">mBERT, XLM-R</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.11.11.5">
<span class="ltx_text" id="S3.T2.4.11.11.5.1" style="font-size:70%;">KMLM</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T2.4.11.11.5.2.1" style="font-size:70%;">(</span>Liu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T2.4.11.11.5.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib134" title="">2021</a><span class="ltx_text" id="S3.T2.4.11.11.5.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.11.11.6"><span class="ltx_text" id="S3.T2.4.11.11.6.1" style="font-size:70%;">11/2021</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.12.12">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.4.12.12.1"><span class="ltx_text" id="S3.T2.4.12.12.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.4.12.12.2"><span class="ltx_text" id="S3.T2.4.12.12.2.1" style="font-size:70%;">bioS</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.4.12.12.3">
<span class="ltx_text" id="S3.T2.4.12.12.3.1" style="font-size:70%;">bioR</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T2.4.12.12.3.2.1" style="font-size:70%;">(</span>Zhu and Li<span class="ltx_text" id="S3.T2.4.12.12.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib277" title="">2023</a><span class="ltx_text" id="S3.T2.4.12.12.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.4.12.12.4"><span class="ltx_text" id="S3.T2.4.12.12.4.1" style="font-size:70%;">Llama</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.4.12.12.5"><span class="ltx_text" id="S3.T2.4.12.12.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.4.12.12.6"><span class="ltx_text" id="S3.T2.4.12.12.6.1" style="font-size:70%;">09/2023</span></td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.1.1">Model Self-Improvement.</span>
</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">In the pre-training phase, model self-improvement denotes synthesize data by an LLM, and further utilizes the synthetic data to pre-train the same LLM. For instance, VILA-2<cite class="ltx_cite ltx_citemacro_citep">(Fang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib55" title="">2024</a>)</cite> utilize a self-augmenting process where the present round of VILA is used to generate long and detailed captions with appropriate prompt choice and conversation template for the next round pretraining.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.1.1">General Model Distillation.</span>
</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">General model distillation denotes the utilization of a general LLM with a strong capability to distill high-quality data.
To demonstrate the power of high-quality data in breaking existing scaling laws, phi-1<cite class="ltx_cite ltx_citemacro_citep">(Gunasekar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib68" title="">2023</a>)</cite> is pre-trained on code dataset of “textbook quality”, both synthetically generated with GPT-3.5 and filtered from web sources. Following the method of phi-1 and extending the task to common sense reasoning in natural language, phi-1.5<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib121" title="">2023b</a>)</cite> is pre-trained on a combination of phi-1’s training data and newly created synthetic data.
Inspired by TinyStories, TinyDialogues<cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib57" title="">2024</a>)</cite> is created by prompting GPT-4 to generate realistic dialogues featuring children of various ages as the main participants.
In the continual pre-training stage of SciLitLLM<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib119" title="">2024c</a>)</cite>, Llama3-7B-Instruct is utilized to correct the errors introduced during the PDF parsing process followed by supervised transfer learning on a classifier to filter out low-quality texts from the dataset.
For multi-modality, AnyGPT<cite class="ltx_cite ltx_citemacro_citep">(Zhan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib252" title="">2024</a>)</cite>, pre-trained on a multimodal text-centric dataset, is an any-to-any multimodal language model capable of comprehending and producing diverse modalities. Utilizing the open-source text-to-image generation model GLIDE, it has been demonstrated that synthetic data significantly aids classifier learning and holds substantial promise for model pre-training<cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib77" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3. </span><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.1.1">Data Augmentation.</span>
</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">Data augmentation aims to further process existing data to obtain a more diverse dataset. In the pre-training stage, there are mainly two kinds of methods: data reformation and non-LLMs-driven method.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p2.1.1">Data Reformation.</span>
Data reformation transforms the original dataset to obtain a new dataset with diversity and quality.
For instance, WRAP<cite class="ltx_cite ltx_citemacro_citep">(Maini et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib151" title="">2024</a>)</cite> employs an off-the-shelf instruction-tuned model to paraphrase web documents in various styles, thereby pre-training LLMs on a combination of real and synthetic rephrases.
Similarly, the BIO dataset bioR<cite class="ltx_cite ltx_citemacro_citep">(Zhu and Li, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib277" title="">2023</a>)</cite> is constructed by rewriting the synthetic dataset of 100k biographies using Llama to make them close to real-life biography style.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p3">
<p class="ltx_p" id="S3.SS2.SSS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p3.1.1">Non LLMs-Driven.</span>
Other methods augment the original dataset without the utilization of LLMs.
For example, Code Llama undergoes further pretraining on Proof-Pile-2, a 55B-token augmented dataset consisting of scientific papers and web data, yielding LLEMMA<cite class="ltx_cite ltx_citemacro_citep">(Azerbayev et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib7" title="">2023</a>)</cite>.
KMLM<cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib134" title="">2021</a>)</cite> is pre-trained on massive multilingual knowledge graph triples which are constructed by converting the structured knowledge from knowledge graphs to sequential data.
To tackle the pathology of data scarcity, a physics-based modeling framework<cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib135" title="">2024a</a>)</cite> is proposed that generates a multitude of synthetic data to align the LLM to a physically consistent initial state.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Fine-Tuning</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In the fine-tuning phase, data synthesis and augmentation refer to the employment of the generated data to fine-tune LLMs. It has been proven that generated data can effectively contribute to the fine-tuning of LLMs<cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib211" title="">2023b</a>; Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib224" title="">2023b</a>)</cite>.
We discuss the existing methods from three perspectives: model self-improvement, general model distillation, and data augmentation.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.2.1.1" style="font-size:90%;">Table 3</span>. </span><span class="ltx_text" id="S3.T3.3.2" style="font-size:90%;">Data synthesis and augmentation in fine-tuning. In the table, method outlines the techniques presented by each research. Data source and synthetic data indicate the original data used to generate synthetic data and the synthetic data created for fine-tuning, respectively. Base model and fine-tuned model indicate the foundational models and the models that have undergone fine-tuning, respectively. A dash (-) in any cell denotes that the respective content was not mentioned in the cited literature.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T3.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.4.1.1">
<td class="ltx_td ltx_border_tt" id="S3.T3.4.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.4.1.1.2.1" style="font-size:70%;">Category</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.4.1.1.3.1" style="font-size:70%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T3.4.1.1.4.1" style="font-size:70%;">Data Source</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.4.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T3.4.1.1.5.1" style="font-size:70%;">Synthetic Data</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.4.1.1.6"><span class="ltx_text ltx_font_bold" id="S3.T3.4.1.1.6.1" style="font-size:70%;">Base Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.4.1.1.7"><span class="ltx_text ltx_font_bold" id="S3.T3.4.1.1.7.1" style="font-size:70%;">Fine-tuned Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.4.1.1.8"><span class="ltx_text ltx_font_bold" id="S3.T3.4.1.1.8.1" style="font-size:70%;">Date</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.2.2.1" rowspan="4"><span class="ltx_text" id="S3.T3.4.2.2.1.1" style="font-size:70%;">self-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.2.2.2" rowspan="11"><span class="ltx_text" id="S3.T3.4.2.2.2.1" style="font-size:70%;">Uni-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.2.2.3">
<span class="ltx_text" id="S3.T3.4.2.2.3.1" style="font-size:70%;">STaR</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.2.2.3.2.1" style="font-size:70%;">(</span>Zelikman et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.2.2.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib249" title="">2022</a><span class="ltx_text" id="S3.T3.4.2.2.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.2.2.4"><span class="ltx_text" id="S3.T3.4.2.2.4.1" style="font-size:70%;">Arithmetic, etc.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.2.2.5"><span class="ltx_text" id="S3.T3.4.2.2.5.1" style="font-size:70%;">A rationale dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.2.2.6"><span class="ltx_text" id="S3.T3.4.2.2.6.1" style="font-size:70%;">GPT-J</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.2.2.7"><span class="ltx_text" id="S3.T3.4.2.2.7.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.2.2.8"><span class="ltx_text" id="S3.T3.4.2.2.8.1" style="font-size:70%;">03/2022</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.3.3">
<td class="ltx_td ltx_align_center" id="S3.T3.4.3.3.1"><span class="ltx_text" id="S3.T3.4.3.3.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.3.3.2"><span class="ltx_text" id="S3.T3.4.3.3.2.1" style="font-size:70%;">GSM8k, etc.</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.3.3.3"><span class="ltx_text" id="S3.T3.4.3.3.3.1" style="font-size:70%;">reasoning dataset</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.3.3.4"><span class="ltx_text" id="S3.T3.4.3.3.4.1" style="font-size:70%;">PaLM-540B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.3.3.5">
<span class="ltx_text" id="S3.T3.4.3.3.5.1" style="font-size:70%;">LMSI</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.3.3.5.2.1" style="font-size:70%;">(</span>Huang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.3.3.5.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib82" title="">2022</a><span class="ltx_text" id="S3.T3.4.3.3.5.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.3.3.6"><span class="ltx_text" id="S3.T3.4.3.3.6.1" style="font-size:70%;">10/2022</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4.4">
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.4.1">
<span class="ltx_text" id="S3.T3.4.4.4.1.1" style="font-size:70%;">ReST</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.4.4.1.2.1" style="font-size:70%;">(</span>Gulcehre et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.4.4.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib67" title="">2023</a><span class="ltx_text" id="S3.T3.4.4.4.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.4.2"><span class="ltx_text" id="S3.T3.4.4.4.2.1" style="font-size:70%;">IWSLT 2014, etc.</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.4.3"><span class="ltx_text" id="S3.T3.4.4.4.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.4.4"><span class="ltx_text" id="S3.T3.4.4.4.4.1" style="font-size:70%;">Transformer</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.4.5"><span class="ltx_text" id="S3.T3.4.4.4.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.4.6"><span class="ltx_text" id="S3.T3.4.4.4.6.1" style="font-size:70%;">08/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.5.5">
<td class="ltx_td ltx_align_center" id="S3.T3.4.5.5.1">
<span class="ltx_text" id="S3.T3.4.5.5.1.1" style="font-size:70%;">ReST-EM</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.5.5.1.2.1" style="font-size:70%;">(</span>Singh et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.5.5.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib188" title="">2023</a><span class="ltx_text" id="S3.T3.4.5.5.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.5.5.2"><span class="ltx_text" id="S3.T3.4.5.5.2.1" style="font-size:70%;">MATH, APPS</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.5.5.3"><span class="ltx_text" id="S3.T3.4.5.5.3.1" style="font-size:70%;">synthetic math and code</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.5.5.4"><span class="ltx_text" id="S3.T3.4.5.5.4.1" style="font-size:70%;">PaLM 2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.5.5.5"><span class="ltx_text" id="S3.T3.4.5.5.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.5.5.6"><span class="ltx_text" id="S3.T3.4.5.5.6.1" style="font-size:70%;">12/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.6.6">
<td class="ltx_td ltx_align_center" id="S3.T3.4.6.6.1"><span class="ltx_text" id="S3.T3.4.6.6.1.1" style="font-size:70%;">Model</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.6.6.2"><span class="ltx_text" id="S3.T3.4.6.6.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.6.6.3"><span class="ltx_text" id="S3.T3.4.6.6.3.1" style="font-size:70%;">RLHF V5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.6.6.4"><span class="ltx_text" id="S3.T3.4.6.6.4.1" style="font-size:70%;">Self-instruction dataset</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.6.6.5"><span class="ltx_text" id="S3.T3.4.6.6.5.1" style="font-size:70%;">Llama 2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.6.6.6">
<span class="ltx_text" id="S3.T3.4.6.6.6.1" style="font-size:70%;">Code Llama-Instruct</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.6.6.6.2.1" style="font-size:70%;">(</span>Rozière et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.6.6.6.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib175" title="">2024</a><span class="ltx_text" id="S3.T3.4.6.6.6.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.6.6.7"><span class="ltx_text" id="S3.T3.4.6.6.7.1" style="font-size:70%;">08/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.7.7">
<td class="ltx_td" id="S3.T3.4.7.7.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.7.2"><span class="ltx_text" id="S3.T3.4.7.7.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.7.3"><span class="ltx_text" id="S3.T3.4.7.7.3.1" style="font-size:70%;">miniF2F, FIMO</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.7.4"><span class="ltx_text" id="S3.T3.4.7.7.4.1" style="font-size:70%;">theorem-proof pairs</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.7.5"><span class="ltx_text" id="S3.T3.4.7.7.5.1" style="font-size:70%;">DeepSeek math</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.7.6">
<span class="ltx_text" id="S3.T3.4.7.7.6.1" style="font-size:70%;">DeepSeek Prover</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.7.7.6.2.1" style="font-size:70%;">(</span>Xin et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.7.7.6.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib221" title="">2024</a><span class="ltx_text" id="S3.T3.4.7.7.6.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.7.7"><span class="ltx_text" id="S3.T3.4.7.7.7.1" style="font-size:70%;">05/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.8.8">
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.8.1"><span class="ltx_text" id="S3.T3.4.8.8.1.1" style="font-size:70%;">improvement</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.8.2">
<span class="ltx_text" id="S3.T3.4.8.8.2.1" style="font-size:70%;">Self-Translate</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.8.8.2.2.1" style="font-size:70%;">(</span>Ri et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.8.8.2.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib171" title="">2024</a><span class="ltx_text" id="S3.T3.4.8.8.2.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.8.3"><span class="ltx_text" id="S3.T3.4.8.8.3.1" style="font-size:70%;">SQuAD, multiNLI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.8.4"><span class="ltx_text" id="S3.T3.4.8.8.4.1" style="font-size:70%;">Translated synthetic data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.8.5"><span class="ltx_text" id="S3.T3.4.8.8.5.1" style="font-size:70%;">Llama 2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.8.6"><span class="ltx_text" id="S3.T3.4.8.8.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.8.7"><span class="ltx_text" id="S3.T3.4.8.8.7.1" style="font-size:70%;">06/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.9.9">
<td class="ltx_td" id="S3.T3.4.9.9.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.9.2">
<span class="ltx_text" id="S3.T3.4.9.9.2.1" style="font-size:70%;">RFT</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.9.9.2.2.1" style="font-size:70%;">(</span>Yuan et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.9.9.2.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib243" title="">2023</a><span class="ltx_text" id="S3.T3.4.9.9.2.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.9.3"><span class="ltx_text" id="S3.T3.4.9.9.3.1" style="font-size:70%;">GSM8k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.9.4"><span class="ltx_text" id="S3.T3.4.9.9.4.1" style="font-size:70%;">Reject sampling samples</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.9.5"><span class="ltx_text" id="S3.T3.4.9.9.5.1" style="font-size:70%;">Llama</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.9.6"><span class="ltx_text" id="S3.T3.4.9.9.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.9.9.7"><span class="ltx_text" id="S3.T3.4.9.9.7.1" style="font-size:70%;">08/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.10.10">
<td class="ltx_td" id="S3.T3.4.10.10.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.10.10.2">
<span class="ltx_text" id="S3.T3.4.10.10.2.1" style="font-size:70%;">CodeRL</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.10.10.2.2.1" style="font-size:70%;">(</span>Le et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.10.10.2.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib105" title="">2022</a><span class="ltx_text" id="S3.T3.4.10.10.2.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.10.10.3"><span class="ltx_text" id="S3.T3.4.10.10.3.1" style="font-size:70%;">public code</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.10.10.4"><span class="ltx_text" id="S3.T3.4.10.10.4.1" style="font-size:70%;">synthetic samples</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.10.10.5"><span class="ltx_text" id="S3.T3.4.10.10.5.1" style="font-size:70%;">CodeT5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.10.10.6"><span class="ltx_text" id="S3.T3.4.10.10.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.10.10.7"><span class="ltx_text" id="S3.T3.4.10.10.7.1" style="font-size:70%;">07/2022</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.11.11">
<td class="ltx_td" id="S3.T3.4.11.11.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.11.11.2">
<span class="ltx_text" id="S3.T3.4.11.11.2.1" style="font-size:70%;">SPIN</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.11.11.2.2.1" style="font-size:70%;">(</span>Chen et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.11.11.2.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib27" title="">2024a</a><span class="ltx_text" id="S3.T3.4.11.11.2.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.11.11.3"><span class="ltx_text" id="S3.T3.4.11.11.3.1" style="font-size:70%;">Ultrachat200k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.11.11.4"><span class="ltx_text" id="S3.T3.4.11.11.4.1" style="font-size:70%;">100k synthetic dataset</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.11.11.5"><span class="ltx_text" id="S3.T3.4.11.11.5.1" style="font-size:70%;">zephyr</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.11.11.6"><span class="ltx_text" id="S3.T3.4.11.11.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.11.11.7"><span class="ltx_text" id="S3.T3.4.11.11.7.1" style="font-size:70%;">01/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.12.12">
<td class="ltx_td" id="S3.T3.4.12.12.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.12.12.2">
<span class="ltx_text" id="S3.T3.4.12.12.2.1" style="font-size:70%;">Self-Instruct</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.12.12.2.2.1" style="font-size:70%;">(</span>Wang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.12.12.2.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib211" title="">2023b</a><span class="ltx_text" id="S3.T3.4.12.12.2.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.12.12.3"><span class="ltx_text" id="S3.T3.4.12.12.3.1" style="font-size:70%;">175 human-written samples</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.12.12.4"><span class="ltx_text" id="S3.T3.4.12.12.4.1" style="font-size:70%;">52k instructions</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.12.12.5"><span class="ltx_text" id="S3.T3.4.12.12.5.1" style="font-size:70%;">GPT-3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.12.12.6"><span class="ltx_text" id="S3.T3.4.12.12.6.1" style="font-size:70%;">GPT-3-self-inst</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.12.12.7"><span class="ltx_text" id="S3.T3.4.12.12.7.1" style="font-size:70%;">12/2022</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.13.13">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.13.13.1" rowspan="8"><span class="ltx_text" id="S3.T3.4.13.13.1.1" style="font-size:70%;">model</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.13.13.2" rowspan="14"><span class="ltx_text" id="S3.T3.4.13.13.2.1" style="font-size:70%;">Uni-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.13.13.3">
<span class="ltx_text" id="S3.T3.4.13.13.3.1" style="font-size:70%;">Impossible Distillation</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.13.13.3.2.1" style="font-size:70%;">(</span>Jung et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.13.13.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib92" title="">2024</a><span class="ltx_text" id="S3.T3.4.13.13.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.13.13.4"><span class="ltx_text" id="S3.T3.4.13.13.4.1" style="font-size:70%;">contextual constraints</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.13.13.5"><span class="ltx_text" id="S3.T3.4.13.13.5.1" style="font-size:70%;">DIMPLE</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.13.13.6"><span class="ltx_text" id="S3.T3.4.13.13.6.1" style="font-size:70%;">T5-large</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.13.13.7"><span class="ltx_text" id="S3.T3.4.13.13.7.1" style="font-size:70%;">impossible-T5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.13.13.8"><span class="ltx_text" id="S3.T3.4.13.13.8.1" style="font-size:70%;">05/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.14.14">
<td class="ltx_td ltx_align_center" id="S3.T3.4.14.14.1"><span class="ltx_text" id="S3.T3.4.14.14.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.14.14.2"><span class="ltx_text" id="S3.T3.4.14.14.2.1" style="font-size:70%;">175 instruction-response pairs</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.14.14.3"><span class="ltx_text" id="S3.T3.4.14.14.3.1" style="font-size:70%;">instruction-following samples</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.14.14.4"><span class="ltx_text" id="S3.T3.4.14.14.4.1" style="font-size:70%;">Llama</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.14.14.5">
<span class="ltx_text" id="S3.T3.4.14.14.5.1" style="font-size:70%;">Alpaca</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.14.14.5.2.1" style="font-size:70%;">(</span>Taori et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.14.14.5.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib197" title="">2023</a><span class="ltx_text" id="S3.T3.4.14.14.5.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.14.14.6"><span class="ltx_text" id="S3.T3.4.14.14.6.1" style="font-size:70%;">2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.15.15">
<td class="ltx_td ltx_align_center" id="S3.T3.4.15.15.1">
<span class="ltx_text" id="S3.T3.4.15.15.1.1" style="font-size:70%;">LAB</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.15.15.1.2.1" style="font-size:70%;">(</span>Sudalairaj et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.15.15.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib192" title="">2024</a><span class="ltx_text" id="S3.T3.4.15.15.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.15.15.2"><span class="ltx_text" id="S3.T3.4.15.15.2.1" style="font-size:70%;">a taxonomy</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.15.15.3"><span class="ltx_text" id="S3.T3.4.15.15.3.1" style="font-size:70%;">synthetic instruction dataset</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.15.15.4"><span class="ltx_text" id="S3.T3.4.15.15.4.1" style="font-size:70%;">Llama-2, Mistral</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.15.15.5"><span class="ltx_text" id="S3.T3.4.15.15.5.1" style="font-size:70%;">Labradorite, Merlinite</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.15.15.6"><span class="ltx_text" id="S3.T3.4.15.15.6.1" style="font-size:70%;">03/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.16.16">
<td class="ltx_td ltx_align_center" id="S3.T3.4.16.16.1">
<span class="ltx_text" id="S3.T3.4.16.16.1.1" style="font-size:70%;">GLAN</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.16.16.1.2.1" style="font-size:70%;">(</span>Li et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.16.16.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib112" title="">2024b</a><span class="ltx_text" id="S3.T3.4.16.16.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.16.16.2"><span class="ltx_text" id="S3.T3.4.16.16.2.1" style="font-size:70%;">a taxonomy</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.16.16.3"><span class="ltx_text" id="S3.T3.4.16.16.3.1" style="font-size:70%;">synthetic instruction dataset</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.16.16.4"><span class="ltx_text" id="S3.T3.4.16.16.4.1" style="font-size:70%;">Mistral</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.16.16.5"><span class="ltx_text" id="S3.T3.4.16.16.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.16.16.6"><span class="ltx_text" id="S3.T3.4.16.16.6.1" style="font-size:70%;">02/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.17.17">
<td class="ltx_td ltx_align_center" id="S3.T3.4.17.17.1"><span class="ltx_text" id="S3.T3.4.17.17.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.17.17.2"><span class="ltx_text" id="S3.T3.4.17.17.2.1" style="font-size:70%;">Code Alpaca</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.17.17.3"><span class="ltx_text" id="S3.T3.4.17.17.3.1" style="font-size:70%;">instruction-following data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.17.17.4"><span class="ltx_text" id="S3.T3.4.17.17.4.1" style="font-size:70%;">StarCoder</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.17.17.5">
<span class="ltx_text" id="S3.T3.4.17.17.5.1" style="font-size:70%;">WizardCoder</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.17.17.5.2.1" style="font-size:70%;">(</span>Luo et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.17.17.5.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib147" title="">2024</a><span class="ltx_text" id="S3.T3.4.17.17.5.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.17.17.6"><span class="ltx_text" id="S3.T3.4.17.17.6.1" style="font-size:70%;">06/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.18.18">
<td class="ltx_td ltx_align_center" id="S3.T3.4.18.18.1">
<span class="ltx_text" id="S3.T3.4.18.18.1.1" style="font-size:70%;">CLINGEN</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.18.18.1.2.1" style="font-size:70%;">(</span>Xu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.18.18.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib227" title="">2024a</a><span class="ltx_text" id="S3.T3.4.18.18.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.18.18.2"><span class="ltx_text" id="S3.T3.4.18.18.2.1" style="font-size:70%;">clinically relevant knowledge</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.18.18.3"><span class="ltx_text" id="S3.T3.4.18.18.3.1" style="font-size:70%;">synthetic clinical data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.18.18.4"><span class="ltx_text" id="S3.T3.4.18.18.4.1" style="font-size:70%;">PubMedBERT</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.18.18.5"><span class="ltx_text" id="S3.T3.4.18.18.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.18.18.6"><span class="ltx_text" id="S3.T3.4.18.18.6.1" style="font-size:70%;">11/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.19.19">
<td class="ltx_td ltx_align_center" id="S3.T3.4.19.19.1"><span class="ltx_text" id="S3.T3.4.19.19.1.1" style="font-size:70%;">Self-chat</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.19.19.2"><span class="ltx_text" id="S3.T3.4.19.19.2.1" style="font-size:70%;">MedQuAD</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.19.19.3"><span class="ltx_text" id="S3.T3.4.19.19.3.1" style="font-size:70%;">111.5k dialogues</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.19.19.4"><span class="ltx_text" id="S3.T3.4.19.19.4.1" style="font-size:70%;">Llama</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.19.19.5">
<span class="ltx_text" id="S3.T3.4.19.19.5.1" style="font-size:70%;">Baize</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.19.19.5.2.1" style="font-size:70%;">(</span>Xu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.19.19.5.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib223" title="">2023a</a><span class="ltx_text" id="S3.T3.4.19.19.5.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.19.19.6"><span class="ltx_text" id="S3.T3.4.19.19.6.1" style="font-size:70%;">04/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.20.20">
<td class="ltx_td ltx_align_center" id="S3.T3.4.20.20.1">
<span class="ltx_text" id="S3.T3.4.20.20.1.1" style="font-size:70%;">LLM2LLM</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.20.20.1.2.1" style="font-size:70%;">(</span>Lee et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.20.20.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib108" title="">2024</a><span class="ltx_text" id="S3.T3.4.20.20.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.20.20.2"><span class="ltx_text" id="S3.T3.4.20.20.2.1" style="font-size:70%;">GSM8k, etc.</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.20.20.3"><span class="ltx_text" id="S3.T3.4.20.20.3.1" style="font-size:70%;">task-specific datasets</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.20.20.4"><span class="ltx_text" id="S3.T3.4.20.20.4.1" style="font-size:70%;">Llama 2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.20.20.5"><span class="ltx_text" id="S3.T3.4.20.20.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.20.20.6"><span class="ltx_text" id="S3.T3.4.20.20.6.1" style="font-size:70%;">03/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.21.21">
<td class="ltx_td ltx_align_center" id="S3.T3.4.21.21.1"><span class="ltx_text" id="S3.T3.4.21.21.1.1" style="font-size:70%;">General</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.21.21.2"><span class="ltx_text" id="S3.T3.4.21.21.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.21.21.3"><span class="ltx_text" id="S3.T3.4.21.21.3.1" style="font-size:70%;">CMeKG</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.21.21.4"><span class="ltx_text" id="S3.T3.4.21.21.4.1" style="font-size:70%;">Question-answer instances</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.21.21.5"><span class="ltx_text" id="S3.T3.4.21.21.5.1" style="font-size:70%;">Llama</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.21.21.6">
<span class="ltx_text" id="S3.T3.4.21.21.6.1" style="font-size:70%;">HuaTuo</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.21.21.6.2.1" style="font-size:70%;">(</span>Wang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.21.21.6.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib205" title="">2023c</a><span class="ltx_text" id="S3.T3.4.21.21.6.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.21.21.7"><span class="ltx_text" id="S3.T3.4.21.21.7.1" style="font-size:70%;">04/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.22.22">
<td class="ltx_td" id="S3.T3.4.22.22.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.22.22.2"><span class="ltx_text" id="S3.T3.4.22.22.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.22.22.3"><span class="ltx_text" id="S3.T3.4.22.22.3.1" style="font-size:70%;">FLAN-v2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.22.22.4"><span class="ltx_text" id="S3.T3.4.22.22.4.1" style="font-size:70%;">Query-response pairs</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.22.22.5"><span class="ltx_text" id="S3.T3.4.22.22.5.1" style="font-size:70%;">Llama</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.22.22.6">
<span class="ltx_text" id="S3.T3.4.22.22.6.1" style="font-size:70%;">Orca</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.22.22.6.2.1" style="font-size:70%;">(</span>Mukherjee et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.22.22.6.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib155" title="">2023</a><span class="ltx_text" id="S3.T3.4.22.22.6.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.22.22.7"><span class="ltx_text" id="S3.T3.4.22.22.7.1" style="font-size:70%;">06/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.23.23">
<td class="ltx_td ltx_align_center" id="S3.T3.4.23.23.1"><span class="ltx_text" id="S3.T3.4.23.23.1.1" style="font-size:70%;">distillation</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.23.23.2"><span class="ltx_text" id="S3.T3.4.23.23.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.23.23.3"><span class="ltx_text" id="S3.T3.4.23.23.3.1" style="font-size:70%;">FLAN-v2, Orca 2 dataset</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.23.23.4"><span class="ltx_text" id="S3.T3.4.23.23.4.1" style="font-size:70%;">1 million data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.23.23.5"><span class="ltx_text" id="S3.T3.4.23.23.5.1" style="font-size:70%;">Llama 2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.23.23.6">
<span class="ltx_text" id="S3.T3.4.23.23.6.1" style="font-size:70%;">Orca 2</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.23.23.6.2.1" style="font-size:70%;">(</span>Mitra et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.23.23.6.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib153" title="">2023</a><span class="ltx_text" id="S3.T3.4.23.23.6.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.23.23.7"><span class="ltx_text" id="S3.T3.4.23.23.7.1" style="font-size:70%;">11/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.24.24">
<td class="ltx_td" id="S3.T3.4.24.24.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.24.24.2">
<span class="ltx_text" id="S3.T3.4.24.24.2.1" style="font-size:70%;">Evol-Instruct</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.24.24.2.2.1" style="font-size:70%;">(</span>Xu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.24.24.2.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib224" title="">2023b</a><span class="ltx_text" id="S3.T3.4.24.24.2.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.24.24.3"><span class="ltx_text" id="S3.T3.4.24.24.3.1" style="font-size:70%;">Alpaca</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.24.24.4"><span class="ltx_text" id="S3.T3.4.24.24.4.1" style="font-size:70%;">250k instructions</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.24.24.5"><span class="ltx_text" id="S3.T3.4.24.24.5.1" style="font-size:70%;">Llama</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.24.24.6"><span class="ltx_text" id="S3.T3.4.24.24.6.1" style="font-size:70%;">WizardLM</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.24.24.7"><span class="ltx_text" id="S3.T3.4.24.24.7.1" style="font-size:70%;">04/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.25.25">
<td class="ltx_td" id="S3.T3.4.25.25.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.25.25.2">
<span class="ltx_text" id="S3.T3.4.25.25.2.1" style="font-size:70%;">OSS-Instruct</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.25.25.2.2.1" style="font-size:70%;">(</span>Wei et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.25.25.2.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib216" title="">2024</a><span class="ltx_text" id="S3.T3.4.25.25.2.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.25.25.3"><span class="ltx_text" id="S3.T3.4.25.25.3.1" style="font-size:70%;">starcoderdata</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.25.25.4"><span class="ltx_text" id="S3.T3.4.25.25.4.1" style="font-size:70%;">coding instruction data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.25.25.5"><span class="ltx_text" id="S3.T3.4.25.25.5.1" style="font-size:70%;">CodeLlama, etc.</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.25.25.6"><span class="ltx_text" id="S3.T3.4.25.25.6.1" style="font-size:70%;">Magicoder</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.25.25.7"><span class="ltx_text" id="S3.T3.4.25.25.7.1" style="font-size:70%;">12/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.26.26">
<td class="ltx_td" id="S3.T3.4.26.26.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.26.26.2"><span class="ltx_text" id="S3.T3.4.26.26.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.26.26.3"><span class="ltx_text" id="S3.T3.4.26.26.3.1" style="font-size:70%;">MATH, etc.</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.26.26.4"><span class="ltx_text" id="S3.T3.4.26.26.4.1" style="font-size:70%;">MathInstruct</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.26.26.5"><span class="ltx_text" id="S3.T3.4.26.26.5.1" style="font-size:70%;">Llama</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.26.26.6">
<span class="ltx_text" id="S3.T3.4.26.26.6.1" style="font-size:70%;">MAmmoTH</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.26.26.6.2.1" style="font-size:70%;">(</span>Yue et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.26.26.6.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib245" title="">2023b</a><span class="ltx_text" id="S3.T3.4.26.26.6.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.26.26.7"><span class="ltx_text" id="S3.T3.4.26.26.7.1" style="font-size:70%;">09/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.27.27">
<td class="ltx_td" id="S3.T3.4.27.27.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.27.27.2" rowspan="5"><span class="ltx_text" id="S3.T3.4.27.27.2.1" style="font-size:70%;">Multi-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.27.27.3">
<span class="ltx_text" id="S3.T3.4.27.27.3.1" style="font-size:70%;">NExT-GPT</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.27.27.3.2.1" style="font-size:70%;">(</span>Wu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.27.27.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib220" title="">2023a</a><span class="ltx_text" id="S3.T3.4.27.27.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.27.27.4"><span class="ltx_text" id="S3.T3.4.27.27.4.1" style="font-size:70%;">multi-modal data</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.27.27.5"><span class="ltx_text" id="S3.T3.4.27.27.5.1" style="font-size:70%;">MosIT dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.27.27.6"><span class="ltx_text" id="S3.T3.4.27.27.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.27.27.7"><span class="ltx_text" id="S3.T3.4.27.27.7.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.27.27.8"><span class="ltx_text" id="S3.T3.4.27.27.8.1" style="font-size:70%;">09/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.28.28">
<td class="ltx_td" id="S3.T3.4.28.28.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.28.28.2"><span class="ltx_text" id="S3.T3.4.28.28.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.28.28.3"><span class="ltx_text" id="S3.T3.4.28.28.3.1" style="font-size:70%;">COCO</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.28.28.4"><span class="ltx_text" id="S3.T3.4.28.28.4.1" style="font-size:70%;">instruction-following data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.28.28.5"><span class="ltx_text" id="S3.T3.4.28.28.5.1" style="font-size:70%;">VIcuna</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.28.28.6">
<span class="ltx_text" id="S3.T3.4.28.28.6.1" style="font-size:70%;">LLava</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.28.28.6.2.1" style="font-size:70%;">(</span>Liu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.28.28.6.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib139" title="">2023b</a><span class="ltx_text" id="S3.T3.4.28.28.6.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.28.28.7"><span class="ltx_text" id="S3.T3.4.28.28.7.1" style="font-size:70%;">04/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.29.29">
<td class="ltx_td" id="S3.T3.4.29.29.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.29.29.2"><span class="ltx_text" id="S3.T3.4.29.29.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.29.29.3"><span class="ltx_text" id="S3.T3.4.29.29.3.1" style="font-size:70%;">PMC-15M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.29.29.4"><span class="ltx_text" id="S3.T3.4.29.29.4.1" style="font-size:70%;">instruction-following data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.29.29.5"><span class="ltx_text" id="S3.T3.4.29.29.5.1" style="font-size:70%;">LLaVA</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.29.29.6">
<span class="ltx_text" id="S3.T3.4.29.29.6.1" style="font-size:70%;">LLaVA-Med</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.29.29.6.2.1" style="font-size:70%;">(</span>Li et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.29.29.6.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib111" title="">2024e</a><span class="ltx_text" id="S3.T3.4.29.29.6.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.29.29.7"><span class="ltx_text" id="S3.T3.4.29.29.7.1" style="font-size:70%;">06/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.30.30">
<td class="ltx_td" id="S3.T3.4.30.30.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.30.30.2"><span class="ltx_text" id="S3.T3.4.30.30.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.30.30.3"><span class="ltx_text" id="S3.T3.4.30.30.3.1" style="font-size:70%;">specific characteristics</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.30.30.4"><span class="ltx_text" id="S3.T3.4.30.30.4.1" style="font-size:70%;">ChartLlama</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.30.30.5"><span class="ltx_text" id="S3.T3.4.30.30.5.1" style="font-size:70%;">LLaVA</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.30.30.6">
<span class="ltx_text" id="S3.T3.4.30.30.6.1" style="font-size:70%;">ChartLlama</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.30.30.6.2.1" style="font-size:70%;">(</span>Han et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.30.30.6.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib74" title="">2023</a><span class="ltx_text" id="S3.T3.4.30.30.6.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.30.30.7"><span class="ltx_text" id="S3.T3.4.30.30.7.1" style="font-size:70%;">11/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.31.31">
<td class="ltx_td" id="S3.T3.4.31.31.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.31.31.2"><span class="ltx_text" id="S3.T3.4.31.31.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.31.31.3"><span class="ltx_text" id="S3.T3.4.31.31.3.1" style="font-size:70%;">100k images</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.31.31.4"><span class="ltx_text" id="S3.T3.4.31.31.4.1" style="font-size:70%;">ShareGPT-4V</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.31.31.5"><span class="ltx_text" id="S3.T3.4.31.31.5.1" style="font-size:70%;">LLaVA</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.31.31.6">
<span class="ltx_text" id="S3.T3.4.31.31.6.1" style="font-size:70%;">ShareGPT4V</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.31.31.6.2.1" style="font-size:70%;">(</span>Chen et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.31.31.6.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib22" title="">2023a</a><span class="ltx_text" id="S3.T3.4.31.31.6.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.31.31.7"><span class="ltx_text" id="S3.T3.4.31.31.7.1" style="font-size:70%;">11/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.32.32">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.4.32.32.1" rowspan="9"><span class="ltx_text" id="S3.T3.4.32.32.1.1" style="font-size:70%;">Augmentation</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.32.32.2" rowspan="8"><span class="ltx_text" id="S3.T3.4.32.32.2.1" style="font-size:70%;">Uni-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.32.32.3"><span class="ltx_text" id="S3.T3.4.32.32.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.32.32.4"><span class="ltx_text" id="S3.T3.4.32.32.4.1" style="font-size:70%;">NCBI Disease, GAD</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.32.32.5">
<span class="ltx_text" id="S3.T3.4.32.32.5.1" style="font-size:70%;">structured information</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.32.32.5.2.1" style="font-size:70%;">(</span>Tang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.32.32.5.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib196" title="">2023</a><span class="ltx_text" id="S3.T3.4.32.32.5.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.32.32.6"><span class="ltx_text" id="S3.T3.4.32.32.6.1" style="font-size:70%;">BERT, etc.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.32.32.7"><span class="ltx_text" id="S3.T3.4.32.32.7.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.32.32.8"><span class="ltx_text" id="S3.T3.4.32.32.8.1" style="font-size:70%;">03/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.33.33">
<td class="ltx_td ltx_align_center" id="S3.T3.4.33.33.1"><span class="ltx_text" id="S3.T3.4.33.33.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.33.33.2"><span class="ltx_text" id="S3.T3.4.33.33.2.1" style="font-size:70%;">MedQA</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.33.33.3"><span class="ltx_text" id="S3.T3.4.33.33.3.1" style="font-size:70%;">UltraMedical</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.33.33.4"><span class="ltx_text" id="S3.T3.4.33.33.4.1" style="font-size:70%;">Llama 3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.33.33.5">
<span class="ltx_text" id="S3.T3.4.33.33.5.1" style="font-size:70%;">Llama-3-UltraMedical</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.33.33.5.2.1" style="font-size:70%;">(</span>Zhang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.33.33.5.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib259" title="">2024g</a><span class="ltx_text" id="S3.T3.4.33.33.5.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.33.33.6"><span class="ltx_text" id="S3.T3.4.33.33.6.1" style="font-size:70%;">06/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.34.34">
<td class="ltx_td ltx_align_center" id="S3.T3.4.34.34.1"><span class="ltx_text" id="S3.T3.4.34.34.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.34.34.2"><span class="ltx_text" id="S3.T3.4.34.34.2.1" style="font-size:70%;">GSM8k, MATH</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.34.34.3"><span class="ltx_text" id="S3.T3.4.34.34.3.1" style="font-size:70%;">MetaMathQA</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.34.34.4"><span class="ltx_text" id="S3.T3.4.34.34.4.1" style="font-size:70%;">Llama</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.34.34.5">
<span class="ltx_text" id="S3.T3.4.34.34.5.1" style="font-size:70%;">MetaMath</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.34.34.5.2.1" style="font-size:70%;">(</span>Yu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.34.34.5.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib239" title="">2024</a><span class="ltx_text" id="S3.T3.4.34.34.5.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.34.34.6"><span class="ltx_text" id="S3.T3.4.34.34.6.1" style="font-size:70%;">09/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.35.35">
<td class="ltx_td ltx_align_center" id="S3.T3.4.35.35.1">
<span class="ltx_text" id="S3.T3.4.35.35.1.1" style="font-size:70%;">Symbol tuning</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.35.35.1.2.1" style="font-size:70%;">(</span>Wei et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.35.35.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib214" title="">2023a</a><span class="ltx_text" id="S3.T3.4.35.35.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.35.35.2"><span class="ltx_text" id="S3.T3.4.35.35.2.1" style="font-size:70%;">NLP datasets</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.35.35.3"><span class="ltx_text" id="S3.T3.4.35.35.3.1" style="font-size:70%;">input-label pairs</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.35.35.4"><span class="ltx_text" id="S3.T3.4.35.35.4.1" style="font-size:70%;">Flan-PaLM</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.35.35.5"><span class="ltx_text" id="S3.T3.4.35.35.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.35.35.6"><span class="ltx_text" id="S3.T3.4.35.35.6.1" style="font-size:70%;">05/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.36.36">
<td class="ltx_td ltx_align_center" id="S3.T3.4.36.36.1"><span class="ltx_text" id="S3.T3.4.36.36.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.36.36.2"><span class="ltx_text" id="S3.T3.4.36.36.2.1" style="font-size:70%;">MedMCQA</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.36.36.3"><span class="ltx_text" id="S3.T3.4.36.36.3.1" style="font-size:70%;">DISC-Med-SFT</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.36.36.4"><span class="ltx_text" id="S3.T3.4.36.36.4.1" style="font-size:70%;">Baichuan-13B-Base</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.36.36.5">
<span class="ltx_text" id="S3.T3.4.36.36.5.1" style="font-size:70%;">DISC-MedLLM</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.36.36.5.2.1" style="font-size:70%;">(</span>Bao et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.36.36.5.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib11" title="">2023</a><span class="ltx_text" id="S3.T3.4.36.36.5.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.36.36.6"><span class="ltx_text" id="S3.T3.4.36.36.6.1" style="font-size:70%;">08/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.37.37">
<td class="ltx_td ltx_align_center" id="S3.T3.4.37.37.1">
<span class="ltx_text" id="S3.T3.4.37.37.1.1" style="font-size:70%;">MathGenie</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.37.37.1.2.1" style="font-size:70%;">(</span>Lu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.37.37.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib145" title="">2024a</a><span class="ltx_text" id="S3.T3.4.37.37.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.37.37.2"><span class="ltx_text" id="S3.T3.4.37.37.2.1" style="font-size:70%;">GSM9k, MATH</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.37.37.3"><span class="ltx_text" id="S3.T3.4.37.37.3.1" style="font-size:70%;">problem-solution pairs</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.37.37.4"><span class="ltx_text" id="S3.T3.4.37.37.4.1" style="font-size:70%;">Llama 2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.37.37.5"><span class="ltx_text" id="S3.T3.4.37.37.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.37.37.6"><span class="ltx_text" id="S3.T3.4.37.37.6.1" style="font-size:70%;">02/2024</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.38.38">
<td class="ltx_td ltx_align_center" id="S3.T3.4.38.38.1"><span class="ltx_text" id="S3.T3.4.38.38.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.38.38.2"><span class="ltx_text" id="S3.T3.4.38.38.2.1" style="font-size:70%;">scientific papers, web data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.38.38.3"><span class="ltx_text" id="S3.T3.4.38.38.3.1" style="font-size:70%;">Proof-Pile-2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.38.38.4"><span class="ltx_text" id="S3.T3.4.38.38.4.1" style="font-size:70%;">Code Llama</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.38.38.5">
<span class="ltx_text" id="S3.T3.4.38.38.5.1" style="font-size:70%;">Llemma</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.38.38.5.2.1" style="font-size:70%;">(</span>Azerbayev et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.38.38.5.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib7" title="">2023</a><span class="ltx_text" id="S3.T3.4.38.38.5.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.38.38.6"><span class="ltx_text" id="S3.T3.4.38.38.6.1" style="font-size:70%;">10/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.39.39">
<td class="ltx_td ltx_align_center" id="S3.T3.4.39.39.1"><span class="ltx_text" id="S3.T3.4.39.39.1.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.39.39.2"><span class="ltx_text" id="S3.T3.4.39.39.2.1" style="font-size:70%;">MedDialog-CN, IMCS-V2, etc.</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.39.39.3"><span class="ltx_text" id="S3.T3.4.39.39.3.1" style="font-size:70%;">BianQueCorpus</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.39.39.4"><span class="ltx_text" id="S3.T3.4.39.39.4.1" style="font-size:70%;">ChatGLM</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.39.39.5">
<span class="ltx_text" id="S3.T3.4.39.39.5.1" style="font-size:70%;">BianQue</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.39.39.5.2.1" style="font-size:70%;">(</span>Chen et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.39.39.5.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib26" title="">2023d</a><span class="ltx_text" id="S3.T3.4.39.39.5.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.39.39.6"><span class="ltx_text" id="S3.T3.4.39.39.6.1" style="font-size:70%;">10/2023</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.40.40">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.4.40.40.1"><span class="ltx_text" id="S3.T3.4.40.40.1.1" style="font-size:70%;">Multi-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.4.40.40.2">
<span class="ltx_text" id="S3.T3.4.40.40.2.1" style="font-size:70%;">SelTDA</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T3.4.40.40.2.2.1" style="font-size:70%;">(</span>Khan et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T3.4.40.40.2.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib95" title="">2023</a><span class="ltx_text" id="S3.T3.4.40.40.2.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.4.40.40.3"><span class="ltx_text" id="S3.T3.4.40.40.3.1" style="font-size:70%;">A-OKVQA, AQUA</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.4.40.40.4"><span class="ltx_text" id="S3.T3.4.40.40.4.1" style="font-size:70%;">question-answer pairs</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.4.40.40.5"><span class="ltx_text" id="S3.T3.4.40.40.5.1" style="font-size:70%;">ViT-B/16</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.4.40.40.6"><span class="ltx_text" id="S3.T3.4.40.40.6.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.4.40.40.7"><span class="ltx_text" id="S3.T3.4.40.40.7.1" style="font-size:70%;">2023</span></td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.1.1">Model Self-Improvement.</span>
</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">The model self-improvement approach enables the LLM to learn from its outputs through a feedback process, thus eliminating the need for external support. Based on whether the method uses iterative self-improvement and the modality of synthetic data, we group the existing self-improvement strategy into three categories: single-shot self-improvement, iterative self-improvement, and multi-modal self-improvement.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p2.1.1">Single-Shot Self-Improvement.</span>
Single-shot self-improvement denotes the process of synthesizing data through an LLM and then performing a single fine-tuning for the same LLM with the synthesized data.
One category of methods involves supplementing information to the training dataset.
For example, one approach<cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib82" title="">2022</a>)</cite> involves using an LLM to create ‘high-confidence’ rationale-augmented responses for unlabeled questions through Chain-of-Thought prompting and self-consistency checks.
Based on the observation that the model performance has a log-linear relation versus the supervised data, reject sampling fine-tuning (RFT)<cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib243" title="">2023</a>)</cite> utilizes the LLM’s capabilities to generate and compile accurate reasoning trajectories as an augmented dataset.
Similarly, Self-Translate-Train<cite class="ltx_cite ltx_citemacro_citep">(Ri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib171" title="">2024</a>)</cite> capitalizes on the LLM’s translation prowess to produce synthetic training datasets in the desired target language, subsequently employing this self-generated data for fine-tuning.
On the other hand, another category of methods synthesizes new samples based on existing seed data.
For instance, CodeRL<cite class="ltx_cite ltx_citemacro_citep">(Le et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib105" title="">2022</a>)</cite> introduces an actor-critic framework that employs an LLM as the actor network to produce synthetic samples, while a separate neural network serves as a critic model to assess the quality of these samples.
Further, Self-Instruct<cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib211" title="">2023b</a>)</cite> prompts an LLM to generate new instructions and corresponding instances which can be used for the instruction tuning of the LLM itself.
Based on the self-instruct, Code Llama-Instruct<cite class="ltx_cite ltx_citemacro_citep">(Rozière et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib175" title="">2024</a>)</cite> are enhanced through fine-tuning on a combination of proprietary instructional data and synthetic self-instruct dataset which is crafted by prompting Llama 2 for coding issues and Code Llama for corresponding unit tests and solutions.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p3">
<p class="ltx_p" id="S3.SS3.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p3.1.1">Iterative Self-Improvement.</span>
Furthermore, to improve the quality, diversity and amount of synthetic data, various approaches iteratively synthesize datasets and continuously fine-tune LLM in improved loops.
To this end, a self-improvement strategy<cite class="ltx_cite ltx_citemacro_citep">(Haluptzok et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib72" title="">2022b</a>)</cite> is proposed where the LLM generates its own synthetic puzzle-solution pairs which are filtered before being used to fine-tune the LLM itself.
Further, STaR<cite class="ltx_cite ltx_citemacro_citep">(Zelikman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib249" title="">2022</a>)</cite> constructs an augmented dataset by using the LLM’s rationale generation ability and justifying ground-truth answers to problems the model failed to solve.
Besides, ReST<cite class="ltx_cite ltx_citemacro_citep">(Gulcehre et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib67" title="">2023</a>)</cite> first augments the training dataset by generating multiple output predictions using an LLM, and then fine-tunes the same LLM on the filtered dataset with an offline reinforcement learning objective.
Following ReST, ReST-EM<cite class="ltx_cite ltx_citemacro_citep">(Singh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib188" title="">2023</a>)</cite> refrains from augmenting the dataset in the generate step with human-generated outputs and fine-tunes the base LLM instead of the model obtained from the previous iteration in the improve step.
In addition, DeepSeek-Prover<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">xin2024deepseek</span>)</cite> is consistently fine-tuned on the synthetic data which is generated through auto formalization, quality filtering, and statement proving, and the updated model is then utilized for the subsequent iteration.
In SPIN<cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib27" title="">2024a</a>)</cite>, a self-play strategy is implemented where an LLM is fine-tuned to distinguish the response of the opponent player (the LLM from the previous iteration) from the target data distribution, thereby iteratively aligning the LLM with the target data distribution.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2. </span><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.1.1">General Model Distillation.</span>
</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">General model distillation denotes distilling high-quality fine-tuning data from a powerful LLM.
In the present survey, we divide the existing method of general model distillation into five categories: synthesize with seeds, synthesize data iteratively, synthesize reasoning steps, taxonomy-driven synthesize, and synthesize multimodal data.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.p2.1.1">Synthesize with Seeds.</span>
Synthesizing data from existing instance examples or data seed is the most common way<cite class="ltx_cite ltx_citemacro_citep">(Taori et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib197" title="">2023</a>; Chaudhary, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib18" title="">2023</a>; Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib216" title="">2024</a>)</cite>.
For instance, Unnatural Instruction<cite class="ltx_cite ltx_citemacro_citep">(Honovich et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib81" title="">2022</a>)</cite> gathers examples by initially providing a language model with three seed examples to generate a fourth. The dataset is then expanded by prompting the model to rephrase each instruction.
By utilizing self-chat to generate a multi-turn chat corpus with ChatGPT, Baize<cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib223" title="">2023a</a>)</cite> is obtained through parameter-efficient tuning and self-distillation with feedback.
Moreover, CLINGEN<cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib227" title="">2024a</a>)</cite> capitalizes on clinical knowledge extraction to contextualize prompts. The strategy encompasses creating clinical topics from both knowledge graphs and LLMs, as well as extracting writing style recommendations from LLMs.
Similarly, HuaTuo<cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib205" title="">2023c</a>)</cite> is a Llama-based model that has undergone SFT using generated QA which is synthesized through extracting knowledge instances from a knowledge graph and creating additional instances with the help of ChatGPT.
In addition, Impossible Distillation<cite class="ltx_cite ltx_citemacro_citep">(Jung et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib92" title="">2024</a>)</cite> enhances small, low-quality language models by utilizing paraphrase proximity and critic-guided distillation to create a high-quality paraphrase dataset.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p3">
<p class="ltx_p" id="S3.SS3.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.p3.1.1">Synthesize Data Iteratively.</span>
To construct high-quality data with diversity, some approaches build frameworks that can be performed multiple times.
For instance, WizardLM<cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib224" title="">2023b</a>)</cite>, fine-tuned with 70k synthetic data generated by the Evol-Instruct method, achieves state-of-the-art results on high-complexity tasks and remains competitive on other metrics.
Evol-Instruct is further adapted to the code domain with modifications including refining evolutionary instruction, simplifying evolutionary prompts formats, incorporating code debugging and time-space complexity constraints<cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib147" title="">2024</a>)</cite>.
Moreover, LLM2LLM<cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib108" title="">2024</a>)</cite> fine-tunes a student model on an initial dataset, then identifies errors, and augments training data with synthetic examples from a teacher LLM based on those errors. The process repeats to train the student model on increasingly targeted data points.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p4">
<p class="ltx_p" id="S3.SS3.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.p4.1.1">Synthesize Reasoning Steps.</span>
Recent studies have concentrated on improving the performance of LLM by imitation learning, drawing with the outputs generated by large foundation models (LFM). However, the smaller language model tends to imitate the style, but not the reasoning process of LFM. To this end, GAIR-Abel<cite class="ltx_cite ltx_citemacro_citep">(Chern et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib29" title="">2023</a>)</cite> found that the structure of the augmented responses significantly impacts the overall performance, with answers that initiate with a paraphrasing of the question and proceed with step-by-step solution showing superior performance than those in standard format. Further, Orca<cite class="ltx_cite ltx_citemacro_citep">(Mukherjee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib155" title="">2023</a>)</cite> is explanation-tuned on augmented query-response pairs with detailed responses from GPT-4. The approach allows Orca to learn from the rich signal including explanation traces, step-by-step thought processes, and other complex instructions. In Orca 2<cite class="ltx_cite ltx_citemacro_citep">(Mitra et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib153" title="">2023</a>)</cite>, various reasoning techniques including step-by-step, recall then generate, recall-reason-generate and direct answer are investigated, which endows the model with better reasoning capability.
Additionally, MAmmoTH<cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib245" title="">2023b</a>)</cite>, a series of open-source LLMs specifically tailored for general math problem-solving, are fine-tuned on the hybrid instruction tuning dataset MathInstruct with CoT and PoT rationales.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p5">
<p class="ltx_p" id="S3.SS3.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.p5.1.1">Taxonomy-Driven Synthesize.</span>
The aforementioned methods are mostly based on synthesizing datasets from seeds, while recent research has adopted another novel approach to synthesizing datasets through a taxonomy-driven method.
To address the issue of tail phenomenon, LAB<cite class="ltx_cite ltx_citemacro_citep">(Sudalairaj et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib192" title="">2024</a>)</cite> replaced the random sampling in existing synthetic data generation methods with a taxonomy-driven approach to guide the sampling of synthetic data. Similar to LAB, GLAN<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib112" title="">2024b</a>)</cite> utilizes a semi-automatic approach to synthesize large-scale datasets which uses a human-curated taxonomy to generate instruction-tuning data from a teacher model.
Further, SciLitIns<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib119" title="">2024c</a>)</cite> is constructed by a three-step pipeline including collecting a probability table of domain keywords, compiling a list of task descriptions, and prompting GPT-4o to generate.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p6">
<p class="ltx_p" id="S3.SS3.SSS2.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.p6.1.1">Synthesize MultiModal Data.</span>
General model distillation also holds great potential for multimodal applications.
For instance, Visual instruction-tuning<cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib131" title="">2024c</a>)</cite> extends instruction-tuning to the language-image multimodal space by leveraging language-only GPT-4 for multimodal instruction-following data generation.
By fine-tuning LLaVA on multimodal synthetic data, ChartLlama<cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib74" title="">2023</a>)</cite> is developed with a broad spectrum of capabilities for chart understanding and generation capabilities.
Besides, A curriculum learning method<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib111" title="">2024e</a>)</cite> is introduced which involves first fine-tuning LLaVA to align biomedical vocabulary and continue training the model using generated instruction-following data by GPT-4.
Further, ShareGPT4V-7B<cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib22" title="">2023a</a>)</cite> is fine-tuned on the ShareGPT4V dataset and demonstrates impressive performance across various multi-modal benchmarks.
In addition, NExT-GPT<cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib220" title="">2023a</a>)</cite> leverages a modality-switching instruction tuning (MosIT) methodology which prompts GPT-4 to generate multimodal dialogues under various scenarios based on template dialogue examples.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3. </span><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS3.1.1">Data Augmentation.</span>
</h4>
<div class="ltx_para" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">Data augmentation involves enhancing the existing data through various techniques to create a more extensive and varied dataset. In the fine-tuning stage, there are mainly two kinds of methods: data labeling nad data reformation.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS3.p2">
<p class="ltx_p" id="S3.SS3.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS3.p2.1.1">Data Labeling.</span>
Data labeling denotes generating annotations for unlabeled data.
For instance, A pipeline<cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib196" title="">2023</a>)</cite> is proposed to generate a large volume of synthetic data with labels using LLMs and further eliminate low-quality or duplicated samples. The results demonstrate the effectiveness of the method compared to LLM’s zero-shot performance.
Futhermore, Llama-3-UltraMedical<cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib259" title="">2024g</a>)</cite> is obtained by supervised fine-tuning on the UltraMedical dataset, which includes instructions annotated with completions from various LLMs with preferences annotated by GPT-4.
For mutil-modality, SelTDA<cite class="ltx_cite ltx_citemacro_citep">(Khan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib95" title="">2023</a>)</cite> uses the vision language model and target dataset to build a teacher model that can generate question-answer pseudo labels directly conditioned on an image alone.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS3.p3">
<p class="ltx_p" id="S3.SS3.SSS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS3.p3.1.1">Data Reformation.</span>
Data reformation refers to transform the existing data into a more diverse form, thereby augmenting the data.
For example, Symbol tuning<cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib214" title="">2023a</a>)</cite> fine-tunes language model on input-label pairs presented in-context where natural language labels are remapped to arbitrary symbols.
Further, DISC-MedLLM<cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib11" title="">2023</a>)</cite> is fine-tuned on the SFT dataset which is constructed by utilizing medical knowledge-graphs, reconstructing real-world dialogues, and rephrasing human-guided preference.
Besides, MetaMath<cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib239" title="">2024</a>)</cite> is fine-tuned on the MetaMathQA dataset which is constructed by rewriting questions with both forward and backward reasoning paths through LLMs.
In addition, MathGenie<cite class="ltx_cite ltx_citemacro_citep">(Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib145" title="">2024a</a>)</cite> consists of three components including iterative solution augmentation, question back-translation, and verification-based solution filtering to create diverse and reliable data.
Moreover, BianQueCorpus<cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib26" title="">2023d</a>)</cite> is constructed through collecting real-world multi-turn health conversations, constructing a data automatic cleaning process, and using ChatGPT to polish the doctors’ suggestion of multi-turn conversations.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Instruction-Tuning</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">In the instruction tuning phase, data synthetic aims at exploring synthetic instruction or prompt contents to generate instruction-following high-quality data via LLMs. According to the way of synthetic data, they consist of the following categories: (1) general model distillation, (2) model self-improvement, and (3) data augmentation, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#S3.T4" title="Table 4 ‣ 3.4. Instruction-Tuning ‣ 3. Data Synthesis and Augmentation in the Full Lifecycle of LLM ‣ A Survey on Data Synthesis and Augmentation for Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T4.3.1.1" style="font-size:90%;">Table 4</span>. </span><span class="ltx_text" id="S3.T4.4.2" style="font-size:90%;">Data synthesis and augmentation in Instruction-Tuning. A dash (-) indicates no relevant content</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.1.2.1">
<td class="ltx_td ltx_border_tt" id="S3.T4.1.2.1.1" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.1.2.1.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.2.1.2.1" style="font-size:70%;">Category</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.1.2.1.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.2.1.3.1" style="font-size:70%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.1.2.1.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.2.1.4.1" style="font-size:70%;">Data Source</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.1.2.1.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.2.1.5.1" style="font-size:70%;">Synthetic Data</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.1.2.1.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.2.1.6.1" style="font-size:70%;">Base Mmodel</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.1.2.1.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.2.1.7.1" style="font-size:70%;">Target Model</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.3.2.1" rowspan="6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.3.2.1.1" style="font-size:70%;">General model distillation,</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.3.2.2" rowspan="5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.3.2.2.1" style="font-size:70%;">Uni-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.3.2.3" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.3.2.3.1" style="font-size:70%;">Alpaca </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.3.2.3.2.1" style="font-size:70%;">(</span>Taori et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.3.2.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib197" title="">2023</a><span class="ltx_text" id="S3.T4.1.3.2.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.3.2.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.3.2.4.1" style="font-size:70%;">175 instruction-response pairs</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.3.2.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.3.2.5.1" style="font-size:70%;">52k instruction-following samples</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.3.2.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.3.2.6.1" style="font-size:70%;">GPT-3.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.3.2.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.3.2.7.1" style="font-size:70%;">LLaMA</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.4.3">
<td class="ltx_td ltx_align_center" id="S3.T4.1.4.3.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.4.3.1.1" style="font-size:70%;">Vicuna </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.4.3.1.2.1" style="font-size:70%;">(</span>Chiang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.4.3.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib30" title="">2023</a><span class="ltx_text" id="S3.T4.1.4.3.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.4.3.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.4.3.2.1" style="font-size:70%;">175 instruction-response pairs</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.4.3.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.4.3.3.1" style="font-size:70%;">9k instruction-following samples</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.4.3.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.4.3.4.1" style="font-size:70%;">ChatGPT</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.4.3.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.4.3.5.1" style="font-size:70%;">LLaMA</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.5.4">
<td class="ltx_td ltx_align_center" id="S3.T4.1.5.4.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.5.4.1.1" style="font-size:70%;">WizardLM </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.5.4.1.2.1" style="font-size:70%;">(</span>Xu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.5.4.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib224" title="">2023b</a><span class="ltx_text" id="S3.T4.1.5.4.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.5.4.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.5.4.2.1" style="font-size:70%;">175 instruction-response pairs</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.5.4.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.5.4.3.1" style="font-size:70%;">250k instructions</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.5.4.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.5.4.4.1" style="font-size:70%;">ChatGPT</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.5.4.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.5.4.5.1" style="font-size:70%;">LLaMA</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.6.5">
<td class="ltx_td ltx_align_center" id="S3.T4.1.6.5.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.6.5.1.1" style="font-size:70%;">Orca </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.6.5.1.2.1" style="font-size:70%;">(</span>Mukherjee et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.6.5.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib155" title="">2023</a><span class="ltx_text" id="S3.T4.1.6.5.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.6.5.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.6.5.2.1" style="font-size:70%;">FLANv2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.6.5.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.6.5.3.1" style="font-size:70%;">5 million query-response pairs</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.6.5.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.6.5.4.1" style="font-size:70%;">GPT-4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.6.5.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.6.5.5.1" style="font-size:70%;">LLaMA</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.7.6">
<td class="ltx_td ltx_align_center" id="S3.T4.1.7.6.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.7.6.1.1" style="font-size:70%;">Orca 2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.7.6.1.2.1" style="font-size:70%;">(</span>Mitra et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.7.6.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib153" title="">2023</a><span class="ltx_text" id="S3.T4.1.7.6.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.7.6.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.7.6.2.1" style="font-size:70%;">FLAN-v2 &amp; Orca 2 dataset</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.7.6.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.7.6.3.1" style="font-size:70%;">1 million data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.7.6.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.7.6.4.1" style="font-size:70%;">GPT-4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.7.6.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.7.6.5.1" style="font-size:70%;">LLaMA</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.8.7.1" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.8.7.1.1" style="font-size:70%;">Multi-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.8.7.2" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.8.7.2.1" style="font-size:70%;">LLaVA </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.8.7.2.2.1" style="font-size:70%;">(</span>Liu et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.8.7.2.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib131" title="">2024c</a><span class="ltx_text" id="S3.T4.1.8.7.2.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.8.7.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.8.7.3.1" style="font-size:70%;">image-text pairs</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.8.7.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.8.7.4.1" style="font-size:70%;">text-only prompt-answer pairs</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.8.7.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.8.7.5.1" style="font-size:70%;">ChatGPT and GPT-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.8.7.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.8.7.6.1" style="font-size:70%;">Vicuna and CLIP</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.9.8.1" rowspan="6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.9.8.1.1" style="font-size:70%;">Model self-improvement</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.9.8.2" rowspan="5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.9.8.2.1" style="font-size:70%;">Uni-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.9.8.3" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.9.8.3.1" style="font-size:70%;">Self-Instruct </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.9.8.3.2.1" style="font-size:70%;">(</span>Wang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.9.8.3.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib211" title="">2023b</a><span class="ltx_text" id="S3.T4.1.9.8.3.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.9.8.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.9.8.4.1" style="font-size:70%;">175 human-written samples</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.9.8.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.9.8.5.1" style="font-size:70%;">52k instructions</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.9.8.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.9.8.6.1" style="font-size:70%;">GTP-3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.9.8.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.9.8.7.1" style="font-size:70%;">GTP-3</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.10.9">
<td class="ltx_td ltx_align_center" id="S3.T4.1.10.9.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.10.9.1.1" style="font-size:70%;">Backtranslation </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.10.9.1.2.1" style="font-size:70%;">(</span>Li et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.10.9.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib120" title="">2023e</a><span class="ltx_text" id="S3.T4.1.10.9.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.10.9.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.10.9.2.1" style="font-size:70%;">13.2k instruction examples</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.10.9.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.10.9.3.1" style="font-size:70%;">502k instruction-output pairs</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.10.9.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.10.9.4.1" style="font-size:70%;">LLAMA</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.10.9.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.10.9.5.1" style="font-size:70%;">LLAMA</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.11.10">
<td class="ltx_td ltx_align_center" id="S3.T4.1.11.10.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.11.10.1.1" style="font-size:70%;">SPIN </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.11.10.1.2.1" style="font-size:70%;">(</span>Chen et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.11.10.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib27" title="">2024a</a><span class="ltx_text" id="S3.T4.1.11.10.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.11.10.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.11.10.2.1" style="font-size:70%;">50k prompts from Ultrachat200k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.11.10.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.11.10.3.1" style="font-size:70%;">100k synthetic dataset</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.11.10.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.11.10.4.1" style="font-size:70%;">zephyr-7b-sft-full</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.11.10.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.11.10.5.1" style="font-size:70%;">zephyr-7b-sft-full</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.1">
<td class="ltx_td ltx_align_center" id="S3.T4.1.1.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.1.1.1" style="font-size:70%;">ReST</span><sup class="ltx_sup" id="S3.T4.1.1.1.2"><span class="ltx_text ltx_font_italic" id="S3.T4.1.1.1.2.1" style="font-size:70%;">EM</span></sup><span class="ltx_text" id="S3.T4.1.1.1.3" style="font-size:70%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.1.1.4.1" style="font-size:70%;">(</span>Singh et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.1.1.5.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib188" title="">2023</a><span class="ltx_text" id="S3.T4.1.1.1.6.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.1.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.1.2.1" style="font-size:70%;">Hendrycks’ MATH and APPS dataset</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.1.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.1.3.1" style="font-size:70%;">32 or 64 solutions per problem</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.1.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.1.4.1" style="font-size:70%;">PaLM 2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.1.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.1.5.1" style="font-size:70%;">PaLM 2</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.12.11">
<td class="ltx_td ltx_align_center" id="S3.T4.1.12.11.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.12.11.1.1" style="font-size:70%;">CAI </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.12.11.1.2.1" style="font-size:70%;">(</span>Bai et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.12.11.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib9" title="">2022b</a><span class="ltx_text" id="S3.T4.1.12.11.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.12.11.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.12.11.2.1" style="font-size:70%;">16 principles or instructions</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.12.11.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.12.11.3.1" style="font-size:70%;">318k harmful and helpful instructions</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.12.11.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.12.11.4.1" style="font-size:70%;">52B SL-CAI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.12.11.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.12.11.5.1" style="font-size:70%;">52B SL-CAI</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.13.12.1" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.13.12.1.1" style="font-size:70%;">Multi-modality</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.13.12.2" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.13.12.2.1" style="font-size:70%;">SelTDA </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.13.12.2.2.1" style="font-size:70%;">(</span>Khan et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.13.12.2.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib95" title="">2023</a><span class="ltx_text" id="S3.T4.1.13.12.2.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.13.12.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.13.12.3.1" style="font-size:70%;">unlabeled images</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.13.12.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.13.12.4.1" style="font-size:70%;">image-conditional question-answer pairs</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.13.12.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.13.12.5.1" style="font-size:70%;">ViT-B/16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.13.12.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.13.12.6.1" style="font-size:70%;">ViT-B/16</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.14.13">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T4.1.14.13.1" rowspan="9" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.14.13.1.1" style="font-size:70%;">Data augmentation</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.14.13.2" rowspan="3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.14.13.2.1" style="font-size:70%;">Data labeling</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.14.13.3" style="padding-left:2.8pt;padding-right:2.8pt;"><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.14.13.3.1.1" style="font-size:70%;">(</span>Törnberg<span class="ltx_text" id="S3.T4.1.14.13.3.2.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib199" title="">2023</a><span class="ltx_text" id="S3.T4.1.14.13.3.3.3" style="font-size:70%;">)</span></cite></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.14.13.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.14.13.4.1" style="font-size:70%;">political Twitter messages</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.14.13.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.14.13.5.1" style="font-size:70%;">Annotating political Twitter messages</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.14.13.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.14.13.6.1" style="font-size:70%;">Chatgpt-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.14.13.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.14.13.7.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.15.14">
<td class="ltx_td ltx_align_center" id="S3.T4.1.15.14.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.15.14.1.1" style="font-size:70%;">Machine Translation </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.15.14.1.2.1" style="font-size:70%;">(</span>Zhang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.15.14.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib253" title="">2023d</a><span class="ltx_text" id="S3.T4.1.15.14.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.15.14.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.15.14.2.1" style="font-size:70%;">monolingual data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.15.14.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.15.14.3.1" style="font-size:70%;">back-/forward-translation monolingual data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.15.14.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.15.14.4.1" style="font-size:70%;">GLM-130B</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.15.14.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.15.14.5.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.16.15">
<td class="ltx_td ltx_align_center" id="S3.T4.1.16.15.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.16.15.1.1" style="font-size:70%;">T-SciQ </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.16.15.1.2.1" style="font-size:70%;">(</span>Wang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.16.15.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib206" title="">2024d</a><span class="ltx_text" id="S3.T4.1.16.15.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.16.15.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.16.15.2.1" style="font-size:70%;">original question-answer data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.16.15.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.16.15.3.1" style="font-size:70%;">planning-based CoT rationales</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.16.15.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.16.15.4.1" style="font-size:70%;">GPT-3.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.16.15.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.16.15.5.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.17.16">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.17.16.1" rowspan="3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.17.16.1.1" style="font-size:70%;">Data reformation</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.17.16.2" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.17.16.2.1" style="font-size:70%;">CORE </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.17.16.2.2.1" style="font-size:70%;">(</span>Dixit et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.17.16.2.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib46" title="">2022</a><span class="ltx_text" id="S3.T4.1.17.16.2.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.17.16.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.17.16.3.1" style="font-size:70%;">task-related unlabeled texts</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.17.16.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.17.16.4.1" style="font-size:70%;">diverse counterfactual perturbations</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.17.16.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.17.16.5.1" style="font-size:70%;">GPT-3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.17.16.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.17.16.6.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.18.17">
<td class="ltx_td ltx_align_center" id="S3.T4.1.18.17.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.18.17.1.1" style="font-size:70%;">ALIA </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.18.17.1.2.1" style="font-size:70%;">(</span>Dunlap et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.18.17.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib52" title="">2023</a><span class="ltx_text" id="S3.T4.1.18.17.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.18.17.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.18.17.2.1" style="font-size:70%;">image data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.18.17.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.18.17.3.1" style="font-size:70%;">language-guided image editing</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.18.17.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.18.17.4.1" style="font-size:70%;">GPT-4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.18.17.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.18.17.5.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.19.18">
<td class="ltx_td ltx_align_center" id="S3.T4.1.19.18.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.19.18.1.1" style="font-size:70%;">ChatAug </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.19.18.1.2.1" style="font-size:70%;">(</span>Dai et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.19.18.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib38" title="">2023a</a><span class="ltx_text" id="S3.T4.1.19.18.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.19.18.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.19.18.2.1" style="font-size:70%;">text data</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.19.18.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.19.18.3.1" style="font-size:70%;">semantically similar sentences</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.19.18.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.19.18.4.1" style="font-size:70%;">ChatGPT</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.19.18.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.19.18.5.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.20.19">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T4.1.20.19.1" rowspan="3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.20.19.1.1" style="font-size:70%;">Co-annotation</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.20.19.2" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.20.19.2.1" style="font-size:70%;">CoAnnotating </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.20.19.2.2.1" style="font-size:70%;">(</span>Li et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.20.19.2.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib117" title="">2023d</a><span class="ltx_text" id="S3.T4.1.20.19.2.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.20.19.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.20.19.3.1" style="font-size:70%;">unstructured texts</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.20.19.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.20.19.4.1" style="font-size:70%;">responses via different prompt variations</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.20.19.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.20.19.5.1" style="font-size:70%;">ChatGPT</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.20.19.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.20.19.6.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.21.20">
<td class="ltx_td ltx_align_center" id="S3.T4.1.21.20.1" style="padding-left:2.8pt;padding-right:2.8pt;"><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.21.20.1.1.1" style="font-size:70%;">(</span>Bertaglia et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.21.20.1.2.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib12" title="">2023</a><span class="ltx_text" id="S3.T4.1.21.20.1.3.3" style="font-size:70%;">)</span></cite></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.21.20.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.21.20.2.1" style="font-size:70%;">sponsored content on social media</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.21.20.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.21.20.3.1" style="font-size:70%;">generated explanations</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.21.20.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.21.20.4.1" style="font-size:70%;">ChatGPT</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.21.20.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.21.20.5.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.22.21">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.22.21.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_text" id="S3.T4.1.22.21.1.1" style="font-size:70%;">ToolCoder </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T4.1.22.21.1.2.1" style="font-size:70%;">(</span>Zhang et al<span class="ltx_text">.</span><span class="ltx_text" id="S3.T4.1.22.21.1.3.2.1.1" style="font-size:70%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib260" title="">2023e</a><span class="ltx_text" id="S3.T4.1.22.21.1.4.3" style="font-size:70%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.22.21.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.22.21.2.1" style="font-size:70%;">source code</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.22.21.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.22.21.3.1" style="font-size:70%;">API-augmented code</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.22.21.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.22.21.4.1" style="font-size:70%;">ChatGPT</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.22.21.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S3.T4.1.22.21.5.1" style="font-size:70%;">-</span></td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1. </span>General Model Distillation</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">To obtain diverse data, a popular method adopts a stronger LLM to synthesize data and perform instruction-tuning for a weaker LLM <cite class="ltx_cite ltx_citemacro_citep">(Honovich et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib81" title="">2022</a>)</cite>, including uni-modal synthesis and multi-modal synthesis.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p2">
<p class="ltx_p" id="S3.SS4.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p2.1.1">Uni-Modality.</span> Uni-modality synthesizes a specific type of data via teacher LLMs.
Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib197" title="">2023</a>)</cite> first generates instruction-following demonstrations via GPT-3.5 (text-davinci-003) and then fine-tunes llama-7b to create a replicable instruction-following model.
Next, Alpagasus <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib23" title="">2023b</a>)</cite> discovers that the instruction-tuning dataset used by Alpaca contains many incorrect or irrelevant low-quality instances. In response, they design a quality filtering strategy that leverages powerful LLMs like ChatGPT to automatically identify and filter out low-quality data. The results demonstrated that a small amount of high-quality data was sufficient to train a model with even stronger performance.
Based on Alpaca, Vicuna <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib30" title="">2023</a>)</cite> gathers user-shared conversations from ShareGPT.com to build an open-Source chatbot.
Given that the production of high-complexity instructions may pose a challenge for humans, WizardLM <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib224" title="">2023b</a>)</cite> proposes an Evol-Instruct strategy to rewrite and produce more complex instructions. Evol-Instruct uses LLMs to automatically mass-produce various instructions at different levels, including two evolution strategies: in-depth evolution and in-breadth evolution.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p3">
<p class="ltx_p" id="S3.SS4.SSS1.p3.1">While these above models produce abundant data via LLMs, they often lack the ability of reasoning and comprehension skills displayed by LLMs.
To this end, Orca <cite class="ltx_cite ltx_citemacro_citep">(Mukherjee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib155" title="">2023</a>)</cite> and Orca2 <cite class="ltx_cite ltx_citemacro_citep">(Mitra et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib153" title="">2023</a>)</cite> imitate the reasoning process of stronger LLMs via explanation traces to output synthetic samples.
Compared to vanilla instruction tuning, Orca leverages system instructions to augment query-response pairs with detailed reasoning explanations.
Based on Orca, Orca2 further introduces various reasoning strategies, such as step-by-step and recall-reason-generate, to learn to determine the most effective solution strategy for each task.
Orca2 distills a synthetic dataset by collecting FLAN-v2 Collection <cite class="ltx_cite ltx_citemacro_citep">(Longpre et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib142" title="">2023</a>)</cite>, 55K Few-Shot dataset, Math dataset <cite class="ltx_cite ltx_citemacro_citep">(Saxton et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib177" title="">2019</a>)</cite>, and 2000 Doctor-Patient Conversations, creating cautious system instructions to achieve cautious reasoning.
Moreover, Baize <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib223" title="">2023a</a>)</cite> proposes an open-source chat model that generates a high-quality multi-round dialogue corpus by leveraging ChatGPT to engage in a conversation with itself.
Baize employs questions from Quora3 and Stack Overflow4 as seeds to generate 111.5k dialogues through self-chat.
LongForm <cite class="ltx_cite ltx_citemacro_citep">(Köksal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib97" title="">2023</a>)</cite> generates instructions via LLMs via reverse instructions and builds an instruction-following text dataset, offering a cost-effective and fast approach to perform instruction tuning and output high-quality synthetic data.
To fine-tune instructions to optimize Code Large Language Models (Code LLMs), WizardCoder <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib147" title="">2024</a>)</cite> enhances the fine-tuning of complex instructions for Code LLMs by adapting the Evol-Instruct method to the code domain.
WizardCoder produces intricate code instruction set to improve StarCoder <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib118" title="">2023a</a>)</cite> model through code-specific Evol-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib224" title="">2023b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p4">
<p class="ltx_p" id="S3.SS4.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p4.1.1">Multi-Modality.</span>
Multi-modality generates cross-modality data via LLMs <cite class="ltx_cite ltx_citemacro_citep">(Dubey et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib51" title="">2024</a>)</cite>.
As a typical method, LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib131" title="">2024c</a>)</cite> is the first attempt to extend instruction-tuning to the language-image multimodal domain. It uses ChatGPT and GPT-4 to convert image-text pairs into multimodal instruction-following data and then fine-tunes on the generated instructional vision-language data by combining the visual encoder CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib165" title="">2021</a>)</cite> and language decoder Vicuna <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib30" title="">2023</a>)</cite>.
On this basis, LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib130" title="">2024b</a>)</cite>, LLaVA-Plus <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib139" title="">2023b</a>)</cite>, LLaVA-Interactive <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib25" title="">2023c</a>)</cite>, and LLaVA-Med <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib111" title="">2024e</a>)</cite> further extend LLaVA to a variety of multimodal tasks and design specialized prompt templates for better fine-tuning.
For example, LLaVA-Plus is dedicated for tool and skill uses in human-AI interaction sessions by incorporating user instructions that request multimodal tools and their execution results into LLaVA.
LLaVA-Med instructs instruction-following data from the captions through GPT-4 to capture open-ended conversational semantics, building a vision-language conversational assistant to answer questions about biomedical images.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2. </span>Model Self-Improvement</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">Model self-improvement aims at bootstrapping synthetic data from the model itself, including uni-modal synthesis and multi-modal synthesis.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p2">
<p class="ltx_p" id="S3.SS4.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.p2.1.1">Uni-Modality.</span> This category generates uni-modality data to implement instruction-tuning via the LLM itself.
For example, Self-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib211" title="">2023b</a>)</cite> prompts an off-the-shelf GPT3 to generate both new instructions and corresponding instances. It enhances GPT3’s ability to follow instructions by leveraging its own generated outputs.
Motivated by backtranslation methods, Instruction Backtranslation <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib120" title="">2023e</a>)</cite> generates instructions from human-written “responses” from web documents, instead of generating responses from instructions. It adopts a self-curation step to select high-quality pairs and produce augmented instruction-response pairs.
In addition, SPIN <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib27" title="">2024a</a>)</cite> designs a self-play mechanism like generative adversarial networks (GAN) to achieve instruction tuning. It adopts instances of the same LLMs from different iterations to combine the player (discriminator) and the opponent (generator).
To beyond human data, ReST<sup class="ltx_sup" id="S3.SS4.SSS2.p2.1.2"><span class="ltx_text ltx_font_italic" id="S3.SS4.SSS2.p2.1.2.1">EM</span></sup> <cite class="ltx_cite ltx_citemacro_citep">(Singh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib188" title="">2023</a>)</cite> proposes a two-step self-training method via expectation-maximization for reinforcement learning. It first generates multiple samples for each instruction and filters them to create synthetic data through binary feedback, and then fine-tunes on model-generated synthetic data.
To explore the harmlessness from AI feedback, CAI <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib9" title="">2022b</a>)</cite> generates self-critiques and revised responses via designed principles or instructions (i.e., constitution) and then fine-tunes the original model to achieve self-improvement on harmlessness.
To better teach LLMs to use tools, Toolformer <cite class="ltx_cite ltx_citemacro_citep">(Schick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib178" title="">2024</a>)</cite> allows LLMS to automatically transform the original input into the input for API calls via the prompt. In this way, LLMS can teach themselves to use external tools via simple APIs in a self-supervised way.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p3">
<p class="ltx_p" id="S3.SS4.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.p3.1.1">Multi-Modality.</span>
The above designs various instruction samples to improve the LLM’s
alignment ability. However, these works are usually text-only. Another category synthesizes multi-modality data via the LLM itself.
To fine-tune the large Vision-Language Model (VLM) to improve Visual Question Answering (VQA), SelTDA <cite class="ltx_cite ltx_citemacro_citep">(Khan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib95" title="">2023</a>)</cite> designs a self-taught data augmentation strategy for finetuning large VLMs on small-scale VQA datasets.
SelTDA generates question-answer pseudo-labels directly conditioned on an image alone by prompting the BLIP <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib113" title="">2022b</a>)</cite> VLM to caption images, allowing us to pseudo-label unlabeled images.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3. </span>Data Augmentation</h4>
<div class="ltx_para" id="S3.SS4.SSS3.p1">
<p class="ltx_p" id="S3.SS4.SSS3.p1.1">Data augmentation aims at enhancing model performance by diversifying training samples without the requirement for extra data. It leverages high-quality instructions or prompts to generate augmented data that users expect and match the target tasks. There are three main types: data labeling, data reformation, and co-annotation.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS3.p2">
<p class="ltx_p" id="S3.SS4.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS3.p2.1.1">Data Labeling.</span> Data labeling employs the language comprehension abilities of LLMs to annotate unlabeled examples <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib276" title="">2023b</a>)</cite>.
For example, ChatGPT-4 is used to classify political Twitter messages through specific instructions <cite class="ltx_cite ltx_citemacro_citep">(Törnberg, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib199" title="">2023</a>)</cite>. Subsequently, some works reveal that these data augmentation ways derived from open-source LLMs are better than manual labeling in many annotating high-resource tasks <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib276" title="">2023b</a>; Alizadeh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib4" title="">2023</a>)</cite>.
An LLM-based prompt strategy evaluates various factors for prompt template and demonstration to augment machine translation data <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib253" title="">2023d</a>)</cite>. It adopts pseudo parallel prompts from monolingual data via zero-shot prompting to improve translation performance.
To achieve multi-modal data augmentation, T-SciQ <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib206" title="">2024d</a>)</cite> teaches science question answering with chain-of-thought (CoT) prompting format to generate high-quality CoT rationales. It is eventually used to train much smaller models to perform CoT reasoning in complex modalities.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS3.p3">
<p class="ltx_p" id="S3.SS4.SSS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS3.p3.1.1">Data Reformation.</span>
Data Reformation transforms the existing data into other variations to meet the data format requirements of the target task.
For instance, CORE <cite class="ltx_cite ltx_citemacro_citep">(Dixit et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib46" title="">2022</a>)</cite> proposes a retrieval-augmented generation approach by creating diverse counterfactual perturbations for counterfactual data augmentation.
It incorporates counterfactual excerpts into prompts to GPT-3 with few-shot learning abilities for counterfactual editing.
Moreover, ALIA <cite class="ltx_cite ltx_citemacro_citep">(Dunlap et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib52" title="">2023</a>)</cite> summarizes the captions of images into short descriptions by prompting GPT-4, and then performs language-guided image editing of the training data with Stable Diffusion <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib172" title="">2022</a>)</cite>.
In the NLP task, ChatAug <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib38" title="">2023a</a>)</cite> transforms every sentence within the training samples into numerous conceptually akin yet semantically distinct instances.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS3.p4">
<p class="ltx_p" id="S3.SS4.SSS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS3.p4.1.1">Co-annotation.</span>
Co-annotation aims to collaboratively annotate data derived from humans and LLMs.
As a representative method, Coannotating designs the human-LLM co-annotation paradigm by using variational prompts to generate responses, which utilizes uncertainty to estimate LLMs’ annotation abilities <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib117" title="">2023d</a>)</cite>.
To improve annotation accuracy, ChatGPT is used to augment annotation data with phrases identified as pertinent attributes and brief explanations <cite class="ltx_cite ltx_citemacro_citep">(Bertaglia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib12" title="">2023</a>)</cite>.
To fine-tune code generation models <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib24" title="">2021</a>; Nijkamp et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib158" title="">2022</a>)</cite> to assist in high-quality code generation, ToolCoder <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib260" title="">2023e</a>)</cite> develops an automated data annotation approach for incorporating tool usage information into the source codes through API-augmented prompts.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>Preference Alignment</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Preference alignment is achieved by systematically refining large models to match complex human preferences <cite class="ltx_cite ltx_citemacro_citep">(Calandriello et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib15" title="">2024</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib60" title="">2024</a>)</cite>. This process starts with General model distillation <cite class="ltx_cite ltx_citemacro_citep">(Askell et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib6" title="">2021</a>; Chiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib31" title="">2024</a>; Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib8" title="">2022a</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib107" title="">2023</a>; Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib36" title="">2023b</a>; Köpf et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib98" title="">2024</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib213" title="">2023a</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib115" title="">2024a</a>; An et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib5" title="">2023</a>; Madaan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib149" title="">2024</a>)</cite>, which synthesizes broad preference data, providing a foundational alignment across diverse tasks. Domain model distillation <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib226" title="">2021</a>; Gehman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib61" title="">2020</a>; Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib39" title="">2023b</a>; Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib86" title="">2024</a>; Stiennon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib190" title="">2020</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib147" title="">2024</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib146" title="">2023</a>)</cite> then optimizes models with specialized datasets, enhancing performance in specific domains. Model self-improvement <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib9" title="">2022b</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib107" title="">2023</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib70" title="">2024</a>; Ye and Ng, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib236" title="">2024</a>; Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib242" title="">2024b</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib194" title="">2024b</a>; Hong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib80" title="">2023</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib23" title="">2023b</a>)</cite> allows models to iteratively refine their capabilities with minimal human intervention, using self-generated feedback. Data augmentation further strengthens model generalization by expanding and diversifying the training data. These interconnected methods form a coherent framework for optimizing model alignment with both general and domain-specific human preferences.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.1. </span><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS1.1.1">General Model Distillation</span>
</h4>
<div class="ltx_para" id="S3.SS5.SSS1.p1">
<p class="ltx_p" id="S3.SS5.SSS1.p1.1">General model distillation aims to generate high-quality preference data by leveraging large language models (LLMs) and external tools to better align models with complex human preferences <cite class="ltx_cite ltx_citemacro_citep">(Askell et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib6" title="">2021</a>)</cite>. This process is crucial for improving LLM performance in practical applications, particularly in areas like safety, reliability, and ethical considerations <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib31" title="">2024</a>)</cite>. One of the primary challenges in this approach is the bias and limitations inherent in strong models <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib8" title="">2022a</a>)</cite>. To address this, distillation from multiple strong models, rather than relying on a single one, can be employed to reduce bias and increase the diversity of responses.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS1.p2">
<p class="ltx_p" id="S3.SS5.SSS1.p2.1">Building upon these strategies, several approaches have been developed to refine preference alignment and mitigate the aforementioned challenges. For instance, RLAIF <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib107" title="">2023</a>)</cite> synthesizes preference data using sources like Reddit TL;DR <cite class="ltx_cite ltx_citemacro_citep">(Völske et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib203" title="">2017</a>)</cite> for summarization tasks and aligns dialogue generation with human preferences through Anthropic’s Helpful and Harmless Human Preferences. Similarly, ULTRAFEEDBACK <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib36" title="">2023b</a>)</cite> utilizes GPT-4 to generate over a million feedback points. By employing techniques such as best-of-n sampling <cite class="ltx_cite ltx_citemacro_citep">(Nakano et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib157" title="">2021</a>)</cite> and Proximal Policy Optimization (PPO) <cite class="ltx_cite ltx_citemacro_citep">(Lambert et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib102" title="">2024</a>)</cite>, it enhances feedback quality and minimizes annotation bias.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS1.p3">
<p class="ltx_p" id="S3.SS5.SSS1.p3.1">In addition to these methods, large-scale datasets have been created to enhance model alignment through crowd-sourced annotations. For example, Open Assistant <cite class="ltx_cite ltx_citemacro_citep">(Köpf et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib98" title="">2024</a>)</cite> was developed with contributions from over 13,500 volunteers, resulting in more than 161,000 messages across 66,497 conversation trees. Each message is annotated for quality, creativity, and harmfulness. Furthermore, HelpSteer <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib213" title="">2023a</a>)</cite> enhances data quality by annotating 37,000 conversations for attributes like helpfulness, correctness, coherence, complexity, and verbosity.
Another crucial technique in improving model alignment is Selective Reflection-Tuning <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib115" title="">2024a</a>)</cite>, which refines responses by filtering the teacher model’s outputs before using them for distillation. The filtering is based on the student model’s r-IFD score, ensuring that only the most challenging and appropriate responses are retained for training. Additionally, models like LEMA <cite class="ltx_cite ltx_citemacro_citep">(An et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib5" title="">2023</a>)</cite> enhance the refinement process by using GPT-4 to identify and correct errors in the student model’s responses. These corrections are then used to fine-tune the student model, making the alignment process more accurate and effective.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS1.p4">
<p class="ltx_p" id="S3.SS5.SSS1.p4.1">The refinement and critique capabilities of LLMs are critical to improving alignment. SELF-REFINE <cite class="ltx_cite ltx_citemacro_citep">(Madaan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib149" title="">2024</a>)</cite> allows models to critique their own responses, generating improved outputs based on their own feedback. Furthermore, evaluation methods such as MetaCritique from the Critique of Critique <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib193" title="">2024a</a>)</cite> provide metrics to assess how effectively a model’s critique improves refinement. CriticBench <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib127" title="">2024a</a>)</cite> also explores the relationship between generative capacity and the ability to critique and correct responses, offering insights into model performance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.2. </span><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS2.1.1">Domain Model Distillation</span>
</h4>
<div class="ltx_para" id="S3.SS5.SSS2.p1">
<p class="ltx_p" id="S3.SS5.SSS2.p1.1">Domain model distillation focuses on optimizing models for specific tasks by training them on specialized and domain-specific datasets, often using reinforcement learning and preference modeling techniques. This approach enables models to perform well across various domains, enhancing their ability to handle complex, specialized tasks. Through this process, models are refined to meet the requirements of various fields, including Safety-oriented Scenarios <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib226" title="">2021</a>; Gehman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib61" title="">2020</a>; Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib39" title="">2023b</a>; Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib86" title="">2024</a>)</cite>, Summarization <cite class="ltx_cite ltx_citemacro_citep">(Stiennon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib190" title="">2020</a>)</cite>, Mathematical Problem Solving <cite class="ltx_cite ltx_citemacro_citep">(Lightman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib125" title="">2023</a>)</cite>, Search-based Question Answering <cite class="ltx_cite ltx_citemacro_citep">(Nakano et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib157" title="">2021</a>)</cite>, as well as Code Generation and Logical Reasoning <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib147" title="">2024</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib146" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS2.p2">
<p class="ltx_p" id="S3.SS5.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS2.p2.1.1">Safety-oriented Scenarios.</span>
In sensitive or adversarial environments, ensuring safe deployment is essential. BAD <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib226" title="">2021</a>)</cite> addresses this by collecting adversarial dialogue data where annotators intentionally provoke unsafe behaviors in chatbots, helping train models to detect and prevent harmful responses. Datasets like REALTOXICITYPROMPTS <cite class="ltx_cite ltx_citemacro_citep">(Gehman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib61" title="">2020</a>)</cite> with 100K sentence-level prompts annotated for toxicity. Inspired by Safe RLHF <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib39" title="">2023b</a>)</cite>, BEAVERTAILS <cite class="ltx_cite ltx_citemacro_citep">(Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib86" title="">2024</a>)</cite> synthesizes data from over 44,000 red-teaming prompts, generating QA pairs with safety meta-labels to reduce harmful content and improve safety alignment.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS2.p3">
<p class="ltx_p" id="S3.SS5.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS2.p3.1.1">Summarization.</span>
In text summarization, Stiennon et al. <cite class="ltx_cite ltx_citemacro_citep">(Stiennon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib190" title="">2020</a>)</cite> generate high-quality summaries by comparing pairs of summaries from the Reddit TL;DR <cite class="ltx_cite ltx_citemacro_citep">(Völske et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib203" title="">2017</a>)</cite>and CNN/DM datasets <cite class="ltx_cite ltx_citemacro_citep">(See et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib180" title="">2017</a>)</cite>. Human evaluators provide pairwise comparisons for these summaries, which are then used to train a reward model. This model is further fine-tuned using reinforcement learning, refining summarization outputs to align with human preferences for clarity and quality, particularly in domains like news and social media.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS2.p4">
<p class="ltx_p" id="S3.SS5.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS2.p4.1.1">Mathematical Problem Solving.</span>
For mathematical reasoning tasks, PRM800K <cite class="ltx_cite ltx_citemacro_citep">(Lightman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib125" title="">2023</a>)</cite> offers a large dataset of 800,000 step-level labels across 75,000 solutions to 12,000 math problems. Labelers assign positive, negative, or neutral labels to each step, allowing models to focus on logical consistency and correctness. This approach reinforces the model’s ability to solve complex problems through step-wise reasoning, improving mathematical problem-solving capabilities.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS2.p5">
<p class="ltx_p" id="S3.SS5.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS2.p5.1.1">Search-based Question Answering.</span>
For search-based QA, WebGPT <cite class="ltx_cite ltx_citemacro_citep">(Nakano et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib157" title="">2021</a>)</cite> trains models using long-form question-answering datasets such as ELI5, TriviaQA, and ARC. By interacting with a web-browsing environment, the model generates answers and compares them with human evaluations. This feedback loop improves the model’s search capabilities and performance, particularly in tasks that require sourcing answers from the internet.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS2.p6">
<p class="ltx_p" id="S3.SS5.SSS2.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS2.p6.1.1">Code Generation and Logical Reasoning.</span>
WizardCoder <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib147" title="">2024</a>)</cite> and WizardMath <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib146" title="">2023</a>)</cite> have also advanced instruction generation in specific domains like coding and logic-based tasks. By extending the initial WizardLM framework, these models improve the diversity of instructions for code generation and math problem-solving, helping the model handle a wide range of domain-specific challenges.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.3. </span><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS3.1.1">Model Self-Improvement</span>
</h4>
<div class="ltx_para" id="S3.SS5.SSS3.p1">
<p class="ltx_p" id="S3.SS5.SSS3.p1.1">Model self-improvement focuses on enabling weaker LLMs to iteratively enhance their performance without requiring additional human-annotated data. This approach consists of two categories: self-feedback loops <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib9" title="">2022b</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib107" title="">2023</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib70" title="">2024</a>; Ye and Ng, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib236" title="">2024</a>; Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib242" title="">2024b</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib194" title="">2024b</a>; Hong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib80" title="">2023</a>)</cite>, where the model autonomously refines its outputs based on self-generated feedback, and external evaluation models <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib23" title="">2023b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib120" title="">2023e</a>; Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib48" title="">2023</a>; Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib242" title="">2024b</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib27" title="">2024a</a>)</cite>, which rely on external evaluators to assess the model’s responses. Both methods aim to create a scalable system of improvement by reducing dependency on human intervention, allowing the models to continuously optimize their performance through internal adjustments or external guidance.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS3.p2">
<p class="ltx_p" id="S3.SS5.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS3.p2.1.1">Self-Feedback Loops.</span>
One of the early methods that exemplify self-improvement through feedback loops is CAI <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib9" title="">2022b</a>)</cite>, which synthesizes alignment datasets by blending human and model-generated prompts using few-shot prompting. By focusing on tasks such as red teaming and helpfulness, CAI enables iterative improvement through AI self-critique and chain-of-thought reasoning, reducing reliance on human feedback. This laid the foundation for RLAIF <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib107" title="">2023</a>)</cite>, where AI selects preference data that aligns with constitutional requirements. However, early RLAIF faced challenges like distribution shifts due to offline data generation. To address this, methods like OAIF <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib70" title="">2024</a>)</cite>and SELF-JUDGE <cite class="ltx_cite ltx_citemacro_citep">(Ye and Ng, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib236" title="">2024</a>)</cite> introduced on-policy preference selection, where a pre-trained Judge model selects preferences in real-time, ensuring alignment with the LLM’s current state.
A key aspect of self-feedback loops is the role of reward models in refining responses. In earlier methods, reward models were often static, leading to problems such as reward hacking <cite class="ltx_cite ltx_citemacro_citep">(Skalse et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib189" title="">2022</a>)</cite>. Self-Rewarding <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib242" title="">2024b</a>)</cite> introduced a dynamic solution by allowing the LLM to act as its own reward model, iteratively selecting preference data and improving itself through Direct Preference Optimization (DPO)<cite class="ltx_cite ltx_citemacro_citep">(Rafailov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib166" title="">2024</a>)</cite> training. This approach ensures that both the model and the reward mechanism evolve together, maintaining alignment throughout the training process.
Another method for dynamic preference adjustment within self-feedback loops is SALMON <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib194" title="">2024b</a>)</cite>, which introduced an Instructable Reward Model that allows for flexible scoring of responses based on different principles. This adaptability enables more precise preference alignment during training. Additionally, CycleAlign <cite class="ltx_cite ltx_citemacro_citep">(Hong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib80" title="">2023</a>)</cite> uses the probability of LLM-generated outputs to rank similar responses, selecting the longest common subsequence of two ranking results to refine the final ordering.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS3.p3">
<p class="ltx_p" id="S3.SS5.SSS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS3.p3.1.1">External Evaluation Models.</span>
External evaluation models play an important role in several self-improvement frameworks. For example, ALPAGASUS <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib23" title="">2023b</a>)</cite> employs a strong LLM like ChatGPT to filter out low-quality data, significantly reducing the training set size while improving model performance. This method demonstrates how external evaluation can enhance model refinement by focusing only on high-quality inputs.
Another prominent technique is Instruction Backtranslation <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib120" title="">2023e</a>)</cite>, which generates instruction prompts for unlabeled web data and selects high-quality pairs for fine-tuning. This approach boosts the model’s ability to follow instructions without requiring large-scale human annotations. SteerLM <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib48" title="">2023</a>)</cite> takes this a step further by fine-tuning models based on specific attributes like humor, creativity, and toxicity. This is done through an attribute prediction model that evaluates responses and refines them using datasets such as Open Assistant <cite class="ltx_cite ltx_citemacro_citep">(Köpf et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib98" title="">2024</a>)</cite> and HH-RLHF <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib8" title="">2022a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS3.p4">
<p class="ltx_p" id="S3.SS5.SSS3.p4.1">Recent approaches like Self-Rewarding <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib242" title="">2024b</a>)</cite> and SPIN <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib27" title="">2024a</a>)</cite> further integrate preference optimization with iterative feedback systems. Self-Rewarding employs the DPO framework, where LLMs generate new prompts and candidate responses, which are then judged by the LLM itself, continuously improving alignment. SPIN, in contrast, eliminates the need for reward models by relying entirely on human-annotated data for iterative improvement. However, as SPIN notes, this approach can become bottlenecked once the model reaches human-level performance, as further improvement requires continuous human intervention.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.4. </span><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS4.1.1">Data Augmentation</span>
</h4>
<div class="ltx_para" id="S3.SS5.SSS4.p1">
<p class="ltx_p" id="S3.SS5.SSS4.p1.1">Data augmentation is essential for enhancing large model alignment by creating task-specific variations of existing data, which strengthens model generalization and robustness. This approach increases the diversity of training data without the need for additional data collection. Techniques like Data Labeling <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib274" title="">2023a</a>; Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib251" title="">2024</a>; Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib241" title="">2024a</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib127" title="">2024a</a>)</cite>, Data Reformation <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib211" title="">2023b</a>; Honovich et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib81" title="">2022</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib219" title="">2023b</a>; Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib251" title="">2024</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib120" title="">2023e</a>)</cite>, and Co-Annotation <cite class="ltx_cite ltx_citemacro_citep">(Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib86" title="">2024</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib212" title="">2024b</a>)</cite> are employed to ensure that the augmented data remains relevant and consistent, contributing to more precise model performance across various tasks.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS4.p2">
<p class="ltx_p" id="S3.SS5.SSS4.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS4.p2.1.1">Data Labeling.</span>
Data labeling plays a crucial role in aligning models with human preferences by providing structured, high-quality feedback to guide learning. Starling-7B <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib274" title="">2023a</a>)</cite> is an example of this, collecting ranked data from chat prompts and generating millions of pairwise comparisons to refine model alignment. A more dynamic approach, inspired by Instruction Evolution <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib251" title="">2024</a>)</cite>, is seen in UltraInteract <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib241" title="">2024a</a>)</cite>, which constructs a Preference Tree from LLM responses. This tree refines incorrect responses based on feedback from models like GPT-4, creating more diverse and robust preference data. These advancements reflect the need for dynamic evolution in instructions to enhance preference labeling and refinement, as CriticBench <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib127" title="">2024a</a>)</cite> suggests that feedback may be more effective than generation in certain knowledge domains.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS4.p3">
<p class="ltx_p" id="S3.SS5.SSS4.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS4.p3.1.1">Data Reformation.</span>
Data reformation is the process of restructuring existing data to better align with task-specific objectives, enabling models to improve their adaptability and performance. A prominent method in this area is in-context learning <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib47" title="">2022</a>)</cite>, where examples embedded in prompts guide LLMs to generate responses that reflect the provided patterns. This closely aligns with Instruction Evolution, which emphasizes increasing the complexity and diversity of instructions. Early works like Self-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib211" title="">2023b</a>)</cite> and Unnatural Instructions <cite class="ltx_cite ltx_citemacro_citep">(Honovich et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib81" title="">2022</a>)</cite>relied on task pools with hand-crafted seed examples, while LaMini-LM <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib219" title="">2023b</a>)</cite> expanded this approach by incorporating rich data from Wikipedia to generate more diverse instructions. Auto Evol-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib251" title="">2024</a>)</cite>, initially developed to evolve instructions, automates the optimization of evolution rules by allowing an Optimizer LLM to iteratively improve the rules based on evolving feedback data. Additionally, Instruction Backtranslation <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib120" title="">2023e</a>)</cite> enhances instruction-following abilities by generating instruction-response pairs from unannotated data, reducing the need for manual annotation. This ongoing refinement in data reformation is crucial for boosting performance across a variety of tasks.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS4.p4">
<p class="ltx_p" id="S3.SS5.SSS4.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS4.p4.1.1">Co-Annotation.</span>
Co-Annotation refers to the collaborative process of combining human and machine-generated annotations to improve the quality and safety of model outputs. For instance, BEAVERTAILS <cite class="ltx_cite ltx_citemacro_citep">(Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib86" title="">2024</a>)</cite> demonstrates how synthesizing data from over 44,000 adversarial prompts, with both human and machine input, generates QA pairs with safety meta-labels, helping models avoid harmful outputs. Similarly, HelpSteer2 <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib212" title="">2024b</a>)</cite> blends human expertise with LLM-generated responses to refine multi-turn conversations, ensuring that models adhere more closely to ethical guidelines.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6. </span>Applications</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">Most large language models (LLMs) are pretrained and finetuned on general-purpose corpora. To apply them effectively to downstream tasks, continuous task-specific pretraining or finetuning is often required, where data quality and relevance become critical for performance on specialized tasks. However, unlike the abundance of general-purpose data, domain-specific datasets are often scarce due to the intensive knowledge required for their creation. To address the problem, many studies have explored the synthesizing of specialized data with diverse characteristics tailored to each application.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.1. </span><span class="ltx_text ltx_font_bold" id="S3.SS6.SSS1.1.1">Math</span>
</h4>
<div class="ltx_para" id="S3.SS6.SSS1.p1">
<p class="ltx_p" id="S3.SS6.SSS1.p1.1">Applying LLMs in mathematical scenarios, which involves question understanding and answering, requires intensive logical reasoning. Many researchers have proposed that generating more rationale corpus <cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib246" title="">2024</a>; Taylor et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib198" title="">2022</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib239" title="">2024</a>; Zelikman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib249" title="">2022</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib146" title="">2023</a>; Xin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib221" title="">2024</a>)</cite> and diverse questions and answers <cite class="ltx_cite ltx_citemacro_citep">(Abdullin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib2" title="">2024</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib132" title="">2024f</a>)</cite> in the training corpus helps the model to better understand and reason.</p>
</div>
<div class="ltx_para" id="S3.SS6.SSS1.p2">
<p class="ltx_p" id="S3.SS6.SSS1.p2.1">Some studies focus on generating chain of thoughts (CoTs) that explicitly outlines the reasoning steps, either by data augmentation or through LLMs.
Galactia <cite class="ltx_cite ltx_citemacro_citep">(Taylor et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib198" title="">2022</a>)</cite> proposes a working memory token, wrapping step-by-step reasoning within “¡work¿ ¡/work¿” for 4 datasets to form a new mathematical reasoning dataset. Additionally, recent literature harnesses the reasoning capability of advanced closed-source LLMs. MammoTH <cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib246" title="">2024</a>)</cite> compiles 13 math datasets with intermediate rationales to form the MathInstruct dataset, and extends them with hybrid CoT and PoT (Programe-of-Thought) rationales with the help of GPT-4.</p>
</div>
<div class="ltx_para" id="S3.SS6.SSS1.p3">
<p class="ltx_p" id="S3.SS6.SSS1.p3.1">Diverse questions and solutions can also enhance the math understanding and reasoning capability of LLMs. General purpose LLMs like GPT-4 are utilized to model linear programming of a specific real-world problem <cite class="ltx_cite ltx_citemacro_citep">(Abdullin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib2" title="">2024</a>)</cite> through conversations between two LLM agents, and supervised by another GPT-4 agent for quality and correctness. MetaMath <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib239" title="">2024</a>)</cite> increases the diversity of questions by rewriting mathematical questions from two reasoning paths and rephrasing using LLMs. Some methods attempt to compose questions from seed questions. For example, Liu et al.<cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib132" title="">2024f</a>)</cite> propose an Iterative Question Composing (IQC) method, which uses an LLM to compose questions from seed questions and employs another LLM to conduct rejection sampling.</p>
</div>
<div class="ltx_para" id="S3.SS6.SSS1.p4">
<p class="ltx_p" id="S3.SS6.SSS1.p4.1">As mathematical questions and answers are verifiable, some approaches expand the training corpus through self-generated formulated problems and proofs that have been verified by themselves or external tools and models.
STaR <cite class="ltx_cite ltx_citemacro_citep">(Zelikman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib249" title="">2022</a>)</cite> generates rationales to answer many answers with few rationale examples as a prompt to bootstrap the reasoning capability of GPT-J <cite class="ltx_cite ltx_citemacro_citep">(Wang, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib204" title="">533Z</a>)</cite>. The answers are verified and, if the generated answers are wrong, the model will retry until the rationale gives the correct answer. All correct rationales are used for fine-tuning.
DeepSeekProver <cite class="ltx_cite ltx_citemacro_citep">(Xin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib221" title="">2024</a>)</cite> is initialized using DeepSeekMath-Base <cite class="ltx_cite ltx_citemacro_citep">(Shao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib182" title="">2024b</a>)</cite> finetuned on MMA dataset <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib87" title="">2023a</a>)</cite> to acquire basic auto-formalization capability. The model generates formulated math problems and proofs, and filters the generated data with miniF2F-valid as few-shot examples. The proofs are generated for both original theorems and their negations as an augmentation approach. All valid generated data will be used to train the model, and the process is repeated several times until a marginal performance gain is observed.
WizardMath <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib146" title="">2023</a>)</cite> generates evolved instructions using ChatGPT and also provides supervision during the generation process. It also evolves the Wizard-E model through Reinforcement Learning from Evol-Instruct Feedback (RLEIF), alongside Evol-Instruct and Reinforcement Learning, to improve LLM reasoning performance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.2. </span><span class="ltx_text ltx_font_bold" id="S3.SS6.SSS2.1.1">Science</span>
</h4>
<div class="ltx_para" id="S3.SS6.SSS2.p1">
<p class="ltx_p" id="S3.SS6.SSS2.p1.1">Scientific applications require a deep understanding of knowledge-intensive concepts and reasoning, which requires high-quality data sets for effective instruction fine-tuning. However, generating such data sets is challenging due to the varying formats across disciplines and the underlying logic can be difficult to articulate.</p>
</div>
<div class="ltx_para" id="S3.SS6.SSS2.p2">
<p class="ltx_p" id="S3.SS6.SSS2.p2.1">Unifying the format of different disciplines is the first step in processing science-related corpus, by transforming structured data into readable texts <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib119" title="">2024c</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib255" title="">2024f</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib254" title="">d</a>)</cite> or a specialized tokenizer <cite class="ltx_cite ltx_citemacro_citep">(Taylor et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib198" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS6.SSS2.p3">
<p class="ltx_p" id="S3.SS6.SSS2.p3.1">The instruction tuning data sets are then generated from the collected raw data. To ensure diversity, SciLitLLM <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib119" title="">2024c</a>)</cite> proposes a three-step pipeline to generate diverse and high-quality instructions for scientific contexts. It first collects keywords and task descriptions for different scientific domains from domain literature and datasets, and then prompts GPT-4o with keywords and descriptions sampled from previously collected data.
ChemLLM <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib255" title="">2024f</a>)</cite> employ a Play as Playwrights CoT style while prompting GPT-4 to promote context richness and logical coherence.
Chain-of-thought also aid the model to better understand scientific topics.
SciGLM <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib254" title="">2024d</a>)</cite> collects the raw questions on college-level physics, chemistry, math, and formal proofs, generating CoT for them using GPT-4 with a self-correction <cite class="ltx_cite ltx_citemacro_citep">(Shinn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib184" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib116" title="">2023c</a>)</cite> process. An instruction quality classifier is trained with positive data labeled by LLMs and humans, and negative data identified by LLMs.
Galactia <cite class="ltx_cite ltx_citemacro_citep">(Taylor et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib198" title="">2022</a>)</cite> focuses on knowledge-intensive scientific tasks: equations, chemical reactions, and citation prediction, and proposes a set of tokenizers for various input formats. It also creates a working memory technique for better in-context reasoning.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS6.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.3. </span><span class="ltx_text ltx_font_bold" id="S3.SS6.SSS3.1.1">Code</span>
</h4>
<div class="ltx_para" id="S3.SS6.SSS3.p1">
<p class="ltx_p" id="S3.SS6.SSS3.p1.1">Generating synthetic data that enhance coding performance has long been studied, which requires a clear understanding of questions and accurate reasoning to produce correct code. Since the accuracy of the code can be easily validated in a simulated coding environment, this enables the generation of large-scale instruction tuning datasets for coding tasks.
Haluptzok et. al. <cite class="ltx_cite ltx_citemacro_citep">(Haluptzok et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib73" title="">2023</a>)</cite> proposed a self-play method to generate programming puzzles <cite class="ltx_cite ltx_citemacro_citep">(Haluptzok et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib71" title="">2022a</a>)</cite> and its solution, where the correctness is verified by a Python interpreter. They further finetune the LLM on the generated data to improve the performance.
For more time-efficient code generation, Shypula et. al. <cite class="ltx_cite ltx_citemacro_citep">(Shypula et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib187" title="">2024</a>)</cite> present a data generation method for code optimization through self-play and few-shot Chain-of-Thoughts. They prompt GPT-3.5 to generate more optimization problems on the proposed PIE dataset, and then the answer is generated by a finetuned GPT-3.5 on the PIE dataset, resulting in more samples.
WizardCoder <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib147" title="">2024</a>)</cite> utilizes StarCoder 15B as the foundation and finetunes it using the code instruction-following evolved through Evol-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib225" title="">2024c</a>)</cite>.
Code Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Chaudhary, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib18" title="">2023</a>)</cite> is fine-tuned from a 7B and 13B LLaMA model on 20K instruction-following data generated by the techniques in the Self-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib209" title="">2023d</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS6.SSS3.p2">
<p class="ltx_p" id="S3.SS6.SSS3.p2.1">Diverse programming problems and the corresponding answers promote the capabilities of LLMs.
MagicCoder <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib216" title="">2024</a>)</cite> utilizes open-source code snippets to generate more diverse, realistic, and controllable coding instruction data.
Code LLama <cite class="ltx_cite ltx_citemacro_citep">(Rozière et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib175" title="">2024</a>)</cite> constructs a dataset of 14,000 question-test-solution triplets by generating 52,000 unique programming questions with Llama 2 70B and using Code Llama 7B to create unit tests and Python solutions.
Phi-1 <cite class="ltx_cite ltx_citemacro_citep">(Gunasekar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib68" title="">2023</a>)</cite> enhances diversity by constraining topics and target audiences when generating text-code interleaved textbooks using GPT-3.5 as a training corpus. Its successor, Phi-1.5 <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib121" title="">2023b</a>)</cite>, expands this approach by creating synthetic textbook-style data for common sense reasoning and world knowledge, while incorporating samples from web datasets to further increase diversity.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS6.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.4. </span><span class="ltx_text ltx_font_bold" id="S3.SS6.SSS4.1.1">Medical</span>
</h4>
<div class="ltx_para" id="S3.SS6.SSS4.p1">
<p class="ltx_p" id="S3.SS6.SSS4.p1.1">In medical applications, large models mainly serve as medical dialogue chatbots, and need to interact with patients with multi-round dialogues. To achieve interactive data synthesis, specialized documents are first collected as seed corpus, such as medical diagnostic records. Based on that, diverse question-and-answer pairs can be generated with the aid of general-purpose large language models, and used to improve understanding ability to produce helpful responses.</p>
</div>
<div class="ltx_para" id="S3.SS6.SSS4.p2">
<p class="ltx_p" id="S3.SS6.SSS4.p2.1">To simulate conversations between the patient and the doctor, DISC-MedLLM <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib11" title="">2023</a>)</cite> refines the MedDialog <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib250" title="">0 11</a>)</cite> and cMedQA2 <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib262" title="">2018</a>)</cite> datasets by leveraging GPT-3.5 to generate high-quality conversation samples. This process involves the removal of colloquial expressions, resolution of inconsistencies, and standardization of language for greater uniformity.
HuatuoGPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib257" title="">2023a</a>)</cite> utilizes data distilled from ChatGPT through self-instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib209" title="">2023d</a>)</cite> and conversations between two ChatGPTs.
HuatuoGPT-II <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib21" title="">1 16</a>)</cite> introduces a one-stage adaptation strategy that merges traditional continued pre-training and supervised fine-tuning into a single step of supervised fine-tuning. It employs GPT-4 for data unification, converting domain-specific pre-training text into a (instruction, output) format with a standardized language and style.
ChatCounselor <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib133" title="">9 27</a>)</cite> is a mental health support trained on GPT-4 refined query-answer pairs based on doctor-patient dialogues, where GPT-4 also generates summaries of key information from each interaction for broader context.</p>
</div>
<div class="ltx_para" id="S3.SS6.SSS4.p3">
<p class="ltx_p" id="S3.SS6.SSS4.p3.1">Privacy is also a primary issue when dealing with sensitive medical records, some literatures extract medical knowledge from knowledge graphs, and generate synthetic medical text without any personal information. ClinGen <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib227" title="">2024a</a>)</cite> alleviate this by generating synthetic clinical text from Knowledge Graphs <cite class="ltx_cite ltx_citemacro_citep">(Su et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib191" title="">2023</a>)</cite> and chatGPT. DISC-MedLLM <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib11" title="">2023</a>)</cite> synthesizes medical dialogues by incorporating domain-specific knowledge graphs question-answer pairs.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS6.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.5. </span><span class="ltx_text ltx_font_bold" id="S3.SS6.SSS5.1.1">Law</span>
</h4>
<div class="ltx_para" id="S3.SS6.SSS5.p1">
<p class="ltx_p" id="S3.SS6.SSS5.p1.1">LLM-based legal assistants have gained considerable attention for providing affordable and convenient legal services, particularly in the areas of legal question answering and consultation. Recent studies focus on the quantity and quality of finetuning datasets by using data synthesis to improve the clarity and formality of the responses.
DISC-LawLLM <cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib244" title="">2023a</a>)</cite> design prompts for GPT-3.5-turbo to refine the legal syllogism, knowledge expansion, and Chain-of-Thought reasoning for the SFT instruction dataset.
LawyerLLaMA <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib83" title="">2023</a>)</cite> uses ChatGPT to generate explanations for each question, or to generate explanations based on questions and answers. In this work, the data generated by human experts produces better results than only 6k and 42k automatically generated data. They identify that more automatically generated data may achieve better performance, and mark it as future work.
LawGPT <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib273" title="">2024b</a>)</cite> refine open-source datasets by prompting chatGPT for instruction-tuning data to generate more formal, polite, and clear answers.
WisdomInterrogatory <cite class="ltx_cite ltx_citemacro_citep">(zhihaiLLM, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib271" title="">847Z</a>)</cite> prompt GPT-3.5 models as agents to imitate conversations between a law agent and a user to generate multi-turn instruction text.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS6.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.6.6. </span><span class="ltx_text ltx_font_bold" id="S3.SS6.SSS6.1.1">Others</span>
</h4>
<div class="ltx_para" id="S3.SS6.SSS6.p1">
<p class="ltx_p" id="S3.SS6.SSS6.p1.1">In addition to these previous applications, the potential of synthetic datasets is also explored in financial <cite class="ltx_cite ltx_citemacro_citep">(Bhatia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib13" title="">2024</a>)</cite> and education <cite class="ltx_cite ltx_citemacro_citep">(Doughty et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib49" title="">2024</a>; Latif et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib103" title="">2024</a>)</cite>. These areas are more challenging for data synthesis due to the higher knowledge density and demands on quality. As research continues, these areas may become increasingly promising.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Functionality</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">From the functional perspective of LLMs, data synthesis and augmentation can be divided into four categories: understanding, logic, memory, and generation <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib200" title="">2023</a>; Chowdhery et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib32" title="">2023</a>)</cite>.
By exploring the four basic functions in LLMs, data synthesis and augmentation can fully capture the inherent patterns in large-scale data and effectively apply them to downstream applications and tasks.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Understanding</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Understanding functionality leverages the powerful language understanding of LLM to understand the inherent patterns in the data.
From the perspective of the content of understanding, it includes uni-modal and multi-modal understandings. Uni-modal understanding mainly comprehends the semantics of texts, including text understanding and semantic labeling. Multi-modal understanding combines multiple modalities.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Text Understanding.</span>
To improve the instruction-following capacities of LLMs, Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib197" title="">2023</a>)</cite> and Alpagasus <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib23" title="">2023b</a>)</cite> utilize human-written instruction-response pairs to generate instruction-following demonstrations via few-shot prompting, improving the capabilities of instruction-following models.
Along this line, WizardLM <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib224" title="">2023b</a>)</cite> generates more diversified and multitask-oriented instructions from both depth and breadth to improve the performance of LLMs.
To perform the model self-improvement, Self-Instruct seeks to improve the instruction-following capabilities of LLMs by bootstrapping off their own generations <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib211" title="">2023b</a>)</cite>. It classifies the generated instruction and generates possible class labels to promote diversity.
To further comprehend text content, SPIN <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib27" title="">2024a</a>)</cite> considers instruction-tuning as a two-player game by using the main player as the discriminator to distinguish the responses between LLM and humans. This strengthens LLM without the need for additional human-annotated data by understanding and discriminating textual responses.
Moreover, WRAP<cite class="ltx_cite ltx_citemacro_citep">(Maini et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib151" title="">2024</a>)</cite> capitalizes on the inherent diversity of web articles, enabling LLMs to produce high-quality paraphrases of noisy and unstructured content found online.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Semantic Labeling.</span>
For a deeper understanding of text properties, Instruction Backtranslation promotes instruction following the language model by automatically labelling human-written documents with corresponding instructions <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib120" title="">2023e</a>)</cite>.
Analogously, ChatGPT-4 is used to classify political messages through instructions by labeling them with political tendencies <cite class="ltx_cite ltx_citemacro_citep">(Törnberg, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib199" title="">2023</a>)</cite>, further understanding semantics from messages.
Introducing the data augmentation pattern of ChatGPT annotating, Co-Annotating <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib117" title="">2023d</a>)</cite> collaboratively views the relation between humans and LLMs and the selected Pareto efficiency concept enables participators to compare strategies and gain a deeper comprehension of tradeoffs between cost and performance.
To offer annotators with explanations, ChatGPT is introduced to improve human labeling of sponsored content on social media by designing prompts to generate model explanations <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib260" title="">2023e</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Multi-Modal Understanding.</span>
LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib131" title="">2024c</a>)</cite> is the first work to use LLMs to generate high-quality language-image instruction-following data for general-purpose visual and language understanding. It employs captions and bounding boxes of images as symbolic representations to encode the images as an LLM-recognizable sequence and produces three types of instruction-following data to fully capture semantic information from the images.
It has been demonstrated that synthetic data improves the performance of models for image recognition tasks in data-scarce settings<cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib77" title="">2022</a>)</cite>.
ChartLlama<cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib74" title="">2023</a>)</cite> is trained on the dataset generated through a multi-step process, encompassing a broader variety of chart and task types than those found in existing datasets.
Genixer<cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib267" title="">2023a</a>)</cite> flexibly produces a variety of visual instruction tuning data from unlabeled images, enhancing capabilities for visual question answering and referring expression comprehension tasks.
The quality of SciLitInsis<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib119" title="">2024c</a>)</cite> has been evaluated from five aspects including clarity, complexity, correctness, usefulness, and adaptability to improve LLMs’ ability in scientific literature understanding.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Logic</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Logic functionality fully taps the reasoning and logic functions in the process of synthesizing and augmenting data. According to the application of logic, there are the following three categories: code logic, math logic, and reasoning.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Code.</span>
To provide complex reasoning capabilities, ReST<sup class="ltx_sup" id="S4.SS2.p2.1.2"><span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.2.1">EM</span></sup> <cite class="ltx_cite ltx_citemacro_citep">(Singh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib188" title="">2023</a>)</cite> selects training samples with a binary reward function to output synthetic data. It leverages logical reasoning to improve the performance of LLMs on problem-solving tasks, such as code generation and mathematical problems.
To optimize code, ToolCoder <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib260" title="">2023e</a>)</cite> utilizes prompts to generate API-augmented codes for the API search tool with ChatGPT.
The code generation process is enhanced by integrating API search tools, enabling the model to automatically utilize the search tool for suggestions when selecting an API.
Moreover, OSS-Instruct<cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib216" title="">2024</a>)</cite> enlightens LLMs to generate more diverse, realistic, and controllable coding instruction data, which can substantially boost the reasoning performance of various LLMs.
High-quality Case2Code<cite class="ltx_cite ltx_citemacro_citep">(Shao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib181" title="">2024a</a>)</cite> dataset is automatically and efficiently harvested from pre-training code texts by leveraging small LLMs and a coder interpreter
The LLM’s performance is enhanced through fine-tuning on synthetic programming and solutions which are verified by a Python interpreter<cite class="ltx_cite ltx_citemacro_citep">(Haluptzok et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib72" title="">2022b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Math.</span>
MathInstruct<cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib245" title="">2023b</a>)</cite> has two main characteristics including broad coverage of different math fields and complexity levels, and hybrid CoT and PoT rationales, enabling MAmmoTH to outperform existing open-source models.
Benefiting from the diverse data sources of MMIQC<cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib132" title="">2024f</a>)</cite> and the effectiveness of iterative question composing, the models fine-tuned on MMIQC achieve new SOTAs on math benchmark.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">Reasoning.</span>
For reliable reasoning performance, Orca <cite class="ltx_cite ltx_citemacro_citep">(Mukherjee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib155" title="">2023</a>)</cite> proposes progressive learning from complex explanation Traces of GPT-4 and learns to imitate the reasoning process of LLM.
Further, Orca2 <cite class="ltx_cite ltx_citemacro_citep">(Mitra et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib153" title="">2023</a>)</cite> designs various reasoning strategies, e.g., step-by-step and recall-reason-generate, to enhance the reasoning abilities of smaller LLMs.
To generate synthetic data for instruction-tuning, CAI <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib9" title="">2022b</a>)</cite> sample a critique request from a short list of constitutions and prompt the model to generate a critique of the response, identifying harmful outputs to build a harmless but nonevasive AI assistant <cite class="ltx_cite ltx_citemacro_citep">(Rosati et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib173" title="">2024</a>)</cite>.
To enhance the science question answering task, T-SciQ <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib206" title="">2024d</a>)</cite> uses the CoT reasoning capabilities of LLMs to generate QA-CoT samples as teaching data samples. Since some questions are exceptionally intricate, T-SciQ further designs a 3-step zero-shot prompting way to produce planning-based CoT rationales, which breaks down complex problems into simpler subproblems that can be easily solved.
Utilizing CoT examples as a guide, the LLM generates high-confidence CoT reasoning paths which serve as the final training samples to be fed back to the model for fine-tuning<cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib82" title="">2022</a>)</cite>.
In addition, STaR<cite class="ltx_cite ltx_citemacro_citep">(Zelikman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib249" title="">2022</a>)</cite> improves the LLM itself by augmenting a fine-tuning dataset using rationalization and justifying ground-truth answers to problems the model failed to solve.
Symbol tuning<cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib214" title="">2023a</a>)</cite> leverages the intuition that when a model cannot use instructions or natural language labels to figure out a task, it must instead reason with input-label mappings in-context.</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">Moreover, reasoning can also be extended to multi-modal scenarios.
To optimize the visual question answering task, SelTDA <cite class="ltx_cite ltx_citemacro_citep">(Khan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib95" title="">2023</a>)</cite> prompts VLM to caption various images and then converts each caption into a boolean question. By this means, SelTDA builds a direct image-to-text task and pseudo-labels unlabeled images by sampling questions and answers.
Recognizing that LLMs struggle with abstract image perception and reasoning, multimodal self-instruct<cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib264" title="">2024b</a>)</cite> creates a benchmark of abstract images tailored for everyday contexts.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Memory</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Memory functionality remembers and utilizes previously learned information in LLM when synthesizing data <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib84" title="">2024</a>)</cite>. According to the properties of memory content, memory functionality can be divided into three categories: procedural memory, semantic memory, and episodic memory <cite class="ltx_cite ltx_citemacro_citep">(Tulving, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib202" title="">1985</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Procedural Memory.</span>
Procedural memory preserves a specific process of how tasks and actions are executed.
To perform the code sorting task for software engineering, a sorting algorithm is used to sort out the code LLMs by recognizing affiliation categories, including different communities and contributors <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib270" title="">[n. d.]</a>)</cite>.
To enable the LLM agent <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib266" title="">2024b</a>)</cite> to operate automatically, a code generation model employs three types of in-context supervision to specify library functions and construct the agent’s codes for solving user-instructed tasks <cite class="ltx_cite ltx_citemacro_citep">(Patel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib161" title="">2023</a>)</cite>.
Additionally, to explore the internal reasoning processes of LLM, Quiet-STaR <cite class="ltx_cite ltx_citemacro_citep">(Zelikman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib248" title="">2024</a>)</cite> generates thoughts and rationales to explain future text when following all tokens in the text based on Self-Taught Reasoner <cite class="ltx_cite ltx_citemacro_citep">(Zelikman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib249" title="">2022</a>)</cite>.
It resorts to procedural reasoning to produce the process of thinking like humans.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Semantic Memory.</span>
Semantic memory synthesizes symbolically representable data to retain acknowledged world knowledge like knowledge graphs, documents, and APIs.
To achieve autonomous knowledge graph construction and reasoning tasks, AutoKG designs a multi-agent-based approach that combines multiple intelligent agents to play different roles like assistant and domain expert to collaborate to complete tasks <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib275" title="">2024</a>)</cite>. Meanwhile, AutoKG incorporates the external knowledge base and Internet resources into the knowledge graph to make up for the limitations of LLMs.
To explore semantic content from general documents, KPC designs a knowledge-driven prompt chaining-based code generation model <cite class="ltx_cite ltx_citemacro_citep">(Ren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib168" title="">2023a</a>)</cite>. It disaggregates the process of code generation into iterative validation and revision phases and employs fine-grained exception-handling knowledge derived from documentation to facilitate LLMs in code generation.
To effectively utilize existing APIs and prevent the model from inventing non-existent APIs, De-Hallucinator autonomously recognizes API references pertinent to specific projects with the model’s preliminary predictions, subsequently incorporating these references into the prompt <cite class="ltx_cite ltx_citemacro_citep">(Eghbali and Pradel, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib53" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p4.1.1">Episodic Memory.</span>
Episodic memory remembers contextual content that is closely related to the current state and personal experiences.
To create diverse synthetic data to showcase diverse personas, Persona Hub proposes a persona-driven data synthesis methodology to capture knowledge and experiences from real-world users <cite class="ltx_cite ltx_citemacro_citep">(Chan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib17" title="">2024</a>)</cite>.
Persona Hub generates 1 billion diverse personas from massive web data by text-to-persona and persona-to-persona ways.
To obtain an accurate response, AceCoder proposes a prompt-enhanced technique to capture context information. It first tells LLM to analyze and clarify requirements and then selects similar programs as supplementary examples in prompts to provide relevant content for prompts <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib114" title="">2023g</a>)</cite>.
To effectively utilize user experience information, RepoCoder optimizes the code completion process at the repository level by integrating a similarity-based retrieval mechanism and a pre-trained code language model within an iterative retrieval-generation framework <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib256" title="">2023c</a>)</cite>.
It adopts available retrievers to locate pertinent knowledge within a repository, thereby improving the contextual foundation for LLM.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Generation</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Generation functionality aims at producing coherent and contextually relevant content for downstream tasks and applications.
Based on the form of generated content, there are the following two categories: content generation (e.g.,text and multi-modal generation) and retrieval-augmented generation.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">Text Generation.</span>
To accomplish sequence understanding and generation for machine translation, a LLM-based prompt strategy <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib253" title="">2023d</a>)</cite> leverages back-/forward-translation to augment monolingual data via zero-shot prompting.
For text augmentation, ChatAug rephrases a sentence into multiple semantically similar sentences <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib38" title="">2023a</a>)</cite>.
Genie <cite class="ltx_cite ltx_citemacro_citep">(Yehudai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib237" title="">2024</a>)</cite> generates high-quality data that is natural, faithful and diverse, and the models trained on the synthetic data are more faithful to the grounding content.
From the guidance of principles of diversity and quality, UltraMedical <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib259" title="">2024g</a>)</cite> dataset can be used to build specialized generalists in biomedicine.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">By utilizing paraphrase proximity and critic-guided distillation, Impossible Distillation <cite class="ltx_cite ltx_citemacro_citep">(Jung et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib92" title="">2024</a>)</cite> produces a high-quality dataset to enhance the model’s capability of paraphrase generation and sentence summarization.
HuaTuo<cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib205" title="">2023c</a>)</cite> enhances the factual accuracy of its responses by initially drawing knowledge instances from a knowledge graph, followed by the creation of instances tailored to that specific knowledge using ChatGPT.
TinyStories<cite class="ltx_cite ltx_citemacro_citep">(Eldan and Li, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib54" title="">2023</a>)</cite> is designed to capture the essence of natural language, thereby enabling the models trained upon it to produce fluent and consistent stories.
UltraChat is created using two distinct ChatGPT Turbo APIs to craft informative and realistic multi-turn dialogues. One simulates the user to produce queries while the other crafts the response<cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib45" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">Additionally, Baize<cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib223" title="">2023a</a>)</cite> employs self-distillation techniques that incorporate feedback, enabling the model to discern nuance within the feedback and carry out fine-grained optimization.
DIALOGIC<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib123" title="">2022a</a>)</cite> efficiently scales a limited dialogue dataset with minimal to zero human involvement and parameter adjustments, addressing the challenge of low-resource scenarios.
DISC-MedLLM<cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib11" title="">2023</a>)</cite> leverages ChatGPT to rephrase existing medical NLP datasets to provide accurate and truthful medical responses in end-to-end conversational healthcare services.
In ReST<cite class="ltx_cite ltx_citemacro_citep">(Gulcehre et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib67" title="">2023</a>)</cite>, multiple steps of negative log-likelihood training with progressively increasing filtering thresholds in the improve step and the human evaluation scores in the grow step lead to continuous improvements in the model’s performance.
Self-translate-train<cite class="ltx_cite ltx_citemacro_citep">(Ri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib171" title="">2024</a>)</cite> enhances cross-lingual performance by effectively harnessing the model’s inherent translation capabilities.
TinyDialogues<cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib57" title="">2024</a>)</cite> discovered that a diverse blend of data sources outperforms homogeneous conversation data, indicating that high-quality synthetic conversation data yields better performance than natural conversation data.</p>
</div>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p5.1.1">Multi-Modal Generation.</span>
To achieve automated language-guided image augmentation, ALIA <cite class="ltx_cite ltx_citemacro_citep">(Dunlap et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib52" title="">2023</a>)</cite> summarizes the image captions to generate natural language descriptions via LLM and shows
significantly visual diversity compared to the original data.</p>
</div>
<div class="ltx_para" id="S4.SS4.p6">
<p class="ltx_p" id="S4.SS4.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p6.1.1">Retrieval-Augmented Generation.</span>
Retrieval-Augmented Generation (RAG) integrates external knowledge to generate accurate and contextually appropriate content <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib19" title="">2024b</a>)</cite>.
To address holistic questions across an entire text corpus, GraphRAG <cite class="ltx_cite ltx_citemacro_citep">(Darren Edge, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib41" title="">2024</a>)</cite> utilizes LLMs to construct a graph-based text index and integrates all partial responses to generate a global response. The constructed knowledge graph contains a wealth of entities and relations in GraphRAG, which is conducive to generating more comprehensive and diversified answers.
Considering the limitations of knowledge embedding model and
independent processing between query and context, RankRAG <cite class="ltx_cite ltx_citemacro_citep">(Yue Yu, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib247" title="">2024</a>)</cite> instruction-tunes LLMs to concurrently assess the relevance between queries and contexts and utilizes the retrieved context to generate accurate answers.
To mitigate the imbalance between the burden of
the retriever, LongRAG  <cite class="ltx_cite ltx_citemacro_citep">(Jiang, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib89" title="">2024</a>)</cite> designs the long retriever and reader components to reduce the corpus size, thereby achieving effective retrieval recall with only a few top units.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Challenges and Limitations</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Data synthesis and augmentation play a pivotal role in enhancing the capabilities of LLMs. However, these techniques face significant challenges and limitations that can impede their effectiveness.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Synthesizing and Augmenting Method</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Despite the significance of synthesizing and augmenting data, there are critical challenges of using different synthesizing and augmenting method in practice.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Dependence on LLMs.</span>
The ability of LLMs to perform few-shot or zero-shot tasks is leveraeged to generate data, suggesting that these models need to be sufficiently large to possess a certain level of reasoning or data generation capabilities <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib233" title="">2024a</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib108" title="">2024</a>)</cite>. Otherwise, data synthesis or augmentation with smaller models may not be sufficient to enhance teh specific capabilities of LLMs. Hence, the methods of data synthesis and augmentation seriously rely on the abilities of LLMs themselves.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Complicating Evaluation and Depollution in Model Training</span>.
The use of synthetic data in model training can significantly complicate fair evaluation. Evaluation benchmarks are often created based on public text sources (such as course websites or forums), and the introduction of synthetic data can exacerbate this issue. While the community has proposed several techniques for detecting evaluation pollution, these token-based depollution methods may prove ineffective when synthetic data is involved in model training <cite class="ltx_cite ltx_citemacro_citep">(Golchin and Surdeanu, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib66" title="">2023</a>)</cite>. It is recommended that model developers invest in creating and maintaining internal and protected evaluation benchmarks to ensure the integrity of the evaluation process.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">Uncertainty and Search Complexity in RLAIF.</span> Preference alignment has progressed from human feedback to AI-generated feedback. Despite these advancements, current methodologies remain largely dependent on passive feedback mechanisms, where meaningful insights are only gathered through active evaluation and querying of the model’s performance <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib107" title="">2023</a>)</cite>. This poses a significant challenge, as the expansive search space demands a considerable number of test samples to acquire valuable feedback. Moreover, a critical concern with AI-generated feedback is the uncertainty regarding its true alignment with human preferences. To date, no definitive evidence has been presented to confirm that AI feedback can reliably function as an effective alignment signal, raising doubts about its credibility and validity.</p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p5.1.1">Unstable and Inconsistent Logical Paths.</span> In applications that depend significantly on logical reasoning, researchers strive to generate synthetic reasoning paths <cite class="ltx_cite ltx_citemacro_citep">(Zelikman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib249" title="">2022</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib146" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib255" title="">2024f</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib254" title="">d</a>; Shypula et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib187" title="">2024</a>)</cite> to enhance the logical capabilities of large language models (LLMs), often verifying only a subset of the steps or only the answers. However, verifying the correctness and consistency of every step in these reasoning paths remains challenging, particularly in knowledge-intensive disciplines. Integrating knowledge bases or formal logical reasoning tools into data generation systems may facilitate the creation of more stable and consistent logical paths.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Data Quality</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Data quality is paramount particularly when it comes to synthetic and augmented data for LLMs. Unlike the diverse, credible, high-quality real data that already exists, the nature of data synthesis and augmentation may influence the quality the generated data.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Data Diversity.</span>
Many current methods rely on predefined templates or algorithms that may inadvertently produce homogeneous outputs. This homogeneity restricts the model’s exposure to diverse linguistic patterns and contexts, ultimately hindering its ability to generalize to varied real-world inputs <cite class="ltx_cite ltx_citemacro_citep">(Chung et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib34" title="">2023</a>; Cegin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib16" title="">2024</a>)</cite>.
In addition, synthetic and augmented data often fails to capture the intricate nuances of real-world language use, such as idiomatic expressions, cultural references, and contextual variations. This shortcoming arises because synthetic and augmented data may not incorporate the richness and diversity found in authentic conversations and interactions. Consequently, models trained predominantly on these data may struggle with the subtleties of language that are crucial for effective communication and comprehension <cite class="ltx_cite ltx_citemacro_citep">(Nagireddy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib156" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">Long-tail Phenomenon.</span> The most significant improvements in data generated by LLMs are observed in languages that are widely used. Conversely, there may be only slight advancements in contexts where languages are less frequently used. As a consequence, the methods for data synthesis and augmentation might struggle to effectively handle rare or innovative instructions <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib218" title="">2024a</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib268" title="">2024a</a>)</cite>.
This causes the long-tail problem that the distribution of language usage is highly skewed. Common phrases and contexts may dominate the synthetic and augmented data, while rare or specialized cases receive little to no representation. This can lead to significant performance drops in scenarios involving infrequent language use, such as technical jargon or niche topics.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Reliability</span>. The reliability of synthetic data cannot be effectively guaranteed <cite class="ltx_cite ltx_citemacro_citep">(Long et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib141" title="">2024</a>; Giglou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib63" title="">2024</a>)</cite>. For example, despite conforming to linguistic rules, is fluent, and is aligned with human preferences, there are still many problems in the fact and timeliness of the synthetic content, and it is not possible to make a reliable evaluation of the synthesized content. More important, synthesized data can reflect biases or inaccuracies from the original datasets, leading to unreliable outputs <cite class="ltx_cite ltx_citemacro_citep">(Mondal and Lipizzi, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib154" title="">2024</a>)</cite>. If a dataset used for training contains biased samples, the synthesized data will likely perpetuate those biases.</p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p5.1.1">Distribution Inconsistency</span>.
The inconsistency between the distributions of synthetic and human-generated data is crucial for LLMs. When there is a significant mismatch, models may struggle to understand and generate responses that resonate with users <cite class="ltx_cite ltx_citemacro_citep">(Xiong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib222" title="">2023</a>)</cite>.
First, biases in the original data and LLMs may generate inappropriate data that do not reflect the natural distribution of language usage, leading to distribution discrepancies.
Second, the lack of context in synthetic and augmented data causes the generated phrases or sentences to be out of alignment with the pragmatics of human communication.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Impact of Data Synthesis and Augmentation</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">In addition to the quality and methodological factors of the data itself, data synthesis and augmentation can also have a certain impact on the external environment, involving individuals, groups, societies, and human values.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">Privacy</span>.
Synthetic and augmented data may contain traces or patterns derived from real individuals, thereby compromising user privacy. This challenge is exacerbated in applications where the generated data mimics sensitive attributes, making it difficult to ensure complete anonymity <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib229" title="">2024</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib210" title="">2024c</a>)</cite>.
Moreover, to address concerns over ownership and authenticity, data watermarking techniques are being explored. These techniques allow for the embedding of identifiable information within synthetic data, enabling traceability and accountability. However, the implementation of such systems must balance the need for privacy with the ability to verify data provenance, raising questions about the feasibility of effective watermarking without compromising data utility.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.1">Security.</span>
While synthetic and augmented data is often seen as a solution to privacy concerns, its security is not guaranteed. The generation process can be exploited by malicious actors to craft adversarial examples that deceive LLMs <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib258" title="">2024a</a>)</cite>. Moreover, these generated data can sometimes be reverse-engineered, revealing patterns that can lead to the reconstruction of sensitive real-world data.
Another potential risk is that synthetic and augmented data can also serve as a vector for introducing vulnerabilities into models. Attackers may embed backdoors into the data generation process, leading to models that behave unpredictably under specific conditions. Such vulnerabilities can be exploited for nefarious purposes, raising significant security concerns <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib217" title="">2024b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.1">Social Impacts.</span>
Data synthetic and augmentation introduce a myriad of legal challenges. For instance, the authenticity of generated data can complicate issues of intellectual property rights and data ownership <cite class="ltx_cite ltx_citemacro_citep">(Patel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib160" title="">2024</a>)</cite>.
Furthermore, the misuse of data poses a significant threat to the dissemination of misleading information. In particular, its application in mimicking real individuals, and manipulating public opinion, carries grave risks.
An important fact is that these data can inadvertently perpetuate or amplify societal biases. If LLMs are trained on biased data, the outputs derived from LLMs may reflect and reinforce harmful stereotypes.
Moreover, cultural norms vary significantly across societies; thus, synthetic and augmented data that is appropriate in one context may be offensive or inappropriate in another. The socio-cultural implications of data should be carefully considered to avoid entrenching existing inequalities <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib183" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Impact on Different Applications and Tasks</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Data synthesis and augmentation techniques play a crucial role in enhancing the performance of LLMs across diverse tasks. However, the effectiveness of these methods can vary significantly depending on their implementation and the specific characteristics of the synthetic and augmented data.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p2.1.1">Generalization</span>.
While synthetic and augmented data can be useful for training models in scenarios where labeled data is scarce, its ability to generalize to unseen examples often falls short. This limitation arises from the fact that the generated data may not fully capture the complexity and variability present in real-world data. As a result, models trained predominantly on these data may exhibit poor performance when applied to new, real-world situations, leading to a phenomenon known as overfitting to the specific patterns of data. For instance, in the medical domain, data synthesized from one healthcare system might not be applicable to another due to differing protocols or patient demographics <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib232" title="">2024c</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib231" title="">b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p3.1.1">Transferability and Domain Adaptation</span>.
Synthetic and augmented data can be designed to mimic various domains. However, the transfer performance to different domains remains a challenge. Data pattern from one domain may struggle to adapt to another due to discrepancies between the synthetic and real data distributions <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib128" title="">2024e</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib207" title="">2024e</a>)</cite>. Therefore, when data is generated, it often embodies specific characteristics of the source domain, which may not fully align with the target domain. This misalignment can hinder the model’s ability to effectively transfer its knowledge. For instance, a model trained on synthetic dialogues that simulate formal language may perform poorly when faced with informal conversational contexts in real-world applications.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5. </span>Future Directions</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS5.p1.1.1">Multi-Modal Synthesis</span>. Multi-modal data synthesis technology addresses the integration of diverse data types (e.g., text, images, audio, and sensor data) to create richer and more informative data <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib88" title="">2023b</a>)</cite>. This helps to create diverse training samples that enhance model robustness against over-fitting and variability. These diverse samples effectively alleviate the data scarcity problem in many specific scenarios like healthcare and autonomous systems.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS5.p2.1.1">Real-time Synthesis</span>. Advances in real-time data synthesis will allow for dynamic generations, enabling applications like virtual assistants and interactive gaming environments to adapt seamlessly to user inputs <cite class="ltx_cite ltx_citemacro_citep">(Karunya et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib94" title="">2023</a>)</cite>. This capability will enhance user engagement and create more personalized experiences across various platforms.</p>
</div>
<div class="ltx_para" id="S5.SS5.p3">
<p class="ltx_p" id="S5.SS5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS5.p3.1.1">Domain Model Distillation.</span> Existing investigations mainly focus on synthesizing data using a general model, such as GPT. However, domain-specific models have stronger capabilities within their respective domains compared to general models. Therefore, leveraging domain-specific models to synthesize domain data is expected to further enhance the capabilities of LLMs within that domain <cite class="ltx_cite ltx_citemacro_citep">(Lightman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib125" title="">2023</a>; Nakano et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib157" title="">2021</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib147" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS5.p4">
<p class="ltx_p" id="S5.SS5.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS5.p4.1.1">Large-Scale Synthesis.</span> Pre-training a large model requires a vast amount of high-quality data. Existing data synthesis or augmentation methods have limited scalability in generating data. The synthesis of large-scale pre-training datasets is expected to enhance the comprehensive capabilities of pre-trained models <cite class="ltx_cite ltx_citemacro_citep">(Karunya et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib94" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS5.p5">
<p class="ltx_p" id="S5.SS5.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS5.p5.1.1">Robust Quality Evaluation Metrics.</span>
Data synthesis and augmentation need to ensure the quality of generated data <cite class="ltx_cite ltx_citemacro_citep">(Chundawat et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib33" title="">2022</a>)</cite>. Current methods lack standardized metrics that can adequately evaluate the diversity and relevance of synthesized datasets. Future research should focus on developing robust metrics that assess not just the linguistic quality of generated text but also its contextual appropriateness, diversity, and potential biases.</p>
</div>
<div class="ltx_para" id="S5.SS5.p6">
<p class="ltx_p" id="S5.SS5.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS5.p6.1.1">Ethical Considerations and Responsible Data Synthesis and Augmentation.</span>
Ethical considerations and responsible practices in data synthesis and augmentation for LLM are critical to ensuring the integrity and fairness of AI systems. As these methods increasingly influence model training and outcomes, some key issues such as bias propagation, misinformation, and data misuse become obvious. It is essential to establish ethical guidelines that govern the generation and application of synthetic and augmented data, promoting transparency, accountability, and inclusivity <cite class="ltx_cite ltx_citemacro_citep">(Jiao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.12896v1#bib.bib90" title="">2024</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Data synthesis and augmentation are essential for advancing LLMs, particularly in meeting the need for large-scale and high-quality data for LLMs. This survey provides a comprehensive review of LLM-oriented data synthesis and augmentation techniques, systematically exploring their applications across the entire lifecycle and core functions of LLMs, while building a framework that connects existing research, highlights key methods, and clarifies strengths and limitations. We envision that advancements in LLM-oriented data synthesis and augmentation methods will unlock new possibilities to enhance data efficiency, improve generalization across tasks, and drive the evolution of data-centric AI. We hope this survey serves as a foundation for future research, inspiring innovation and progress in the field of LLM-oriented data synthesis and augmentation.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This research was supported by “Pioneer” and “Leading Goose” R&amp;D Program of Zhejiang (No. 2024C01020), National Key R&amp;D Program of China (No. 2023YFF0725600), National Science Foundation of China (No. 62406015).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdullin et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yelaman Abdullin, Diego Mollá Aliod, Bahadorreza Ofoghi, John Yearwood, and Qingyang Li. 2024.

</span>
<span class="ltx_bibblock">Synthetic Dialogue Dataset Generation using LLM Agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">arXiv</em> abs/2401.17461 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al<span class="ltx_text" id="bib.bib3.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.4.1">arXiv preprint arXiv:2303.08774</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alizadeh et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Meysam Alizadeh, Maël Kubli, Zeynab Samei, Shirin Dehghani, Juan Diego Bermeo, Maria Korobeynikova, and Fabrizio Gilardi. 2023.

</span>
<span class="ltx_bibblock">Open-source large language models outperform crowd workers and approach ChatGPT in text-annotation tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">arXiv preprint arXiv:2307.02179</em> 101 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">An et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. 2023.

</span>
<span class="ltx_bibblock">Learning from mistakes makes llm better reasoner.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">arXiv preprint arXiv:2310.20689</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Askell et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al<span class="ltx_text" id="bib.bib6.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">A general language assistant as a laboratory for alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.4.1">arXiv preprint arXiv:2112.00861</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azerbayev et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023.

</span>
<span class="ltx_bibblock">Llemma: An open language model for mathematics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">arXiv preprint arXiv:2310.10631</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al<span class="ltx_text" id="bib.bib8.3.1">.</span> 2022a.

</span>
<span class="ltx_bibblock">Training a helpful and harmless assistant with reinforcement learning from human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.4.1">arXiv preprint arXiv:2204.05862</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al<span class="ltx_text" id="bib.bib9.3.1">.</span> 2022b.

</span>
<span class="ltx_bibblock">Constitutional ai: Harmlessness from ai feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.4.1">arXiv preprint arXiv:2212.08073</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bang et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al<span class="ltx_text" id="bib.bib10.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.4.1">Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 675–718.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhijie Bao, Wei Chen, Shengze Xiao, Kuang Ren, Jiaao Wu, Cheng Zhong, Jiajie Peng, Xuanjing Huang, and Zhongyu Wei. 2023.

</span>
<span class="ltx_bibblock">DISC-MedLLM: Bridging General Large Language Models and Real-World Medical Consultation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">arXiv</em> abs/2308.14346 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bertaglia et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Thales Bertaglia, Stefan Huber, Catalina Goanta, Gerasimos Spanakis, and Adriana Iamnitchi. 2023.

</span>
<span class="ltx_bibblock">Closing the loop: Testing chatgpt to generate model explanations to improve human labelling of sponsored content on social media. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">World Conference on Explainable Artificial Intelligence</em>. Springer, 198–213.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhatia et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Gagan Bhatia, El Moatez Billah Nagoudi, Hasan Cavusoglu, and Muhammad Abdul-Mageed. 2024.

</span>
<span class="ltx_bibblock">FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2402.10986 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown (2020)</span>
<span class="ltx_bibblock">
Tom B Brown. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2005.14165</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calandriello et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang, Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko, Tianqi Liu, et al<span class="ltx_text" id="bib.bib15.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Human alignment of large language models through online preference optimisation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.4.1">arXiv preprint arXiv:2403.08635</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cegin et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jan Cegin, Branislav Pecher, Jakub Simko, Ivan Srba, Maria Bielikova, and Peter Brusilovsky. 2024.

</span>
<span class="ltx_bibblock">Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">arXiv preprint arXiv:2401.06643</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. 2024.

</span>
<span class="ltx_bibblock">Scaling synthetic data creation with 1,000,000,000 personas.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">arXiv preprint arXiv:2406.20094</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chaudhary (2023)</span>
<span class="ltx_bibblock">
Sahil Chaudhary. 2023.

</span>
<span class="ltx_bibblock">Code alpaca: An instruction-following llama model for code generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">GitHub repository</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024b.

</span>
<span class="ltx_bibblock">Benchmarking large language models in retrieval-augmented generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 38. 17754–17762.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Jiuhai Chen, Rifaa Qadri, Yuxin Wen, Neel Jain, John Kirchenbauer, Tianyi Zhou, and Tom Goldstein. 2024c.

</span>
<span class="ltx_bibblock">GenQA: Generating Millions of Instructions from a Handful of Prompts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">arXiv preprint arXiv:2406.10323</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (1 16)</span>
<span class="ltx_bibblock">
Junying Chen, Xidong Wang, Anningzhe Gao, Feng Jiang, Shunian Chen, Hongbo Zhang, Dingjie Song, Wenya Xie, Chuyi Kong, Jianquan Li, Xiang Wan, Haizhou Li, and Benyou Wang. 2023-11-16.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs</em>.

</span>
<span class="ltx_bibblock">arXiv:2311.09774 [cs]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2311.09774" title="">http://arxiv.org/abs/2311.09774</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2023a.

</span>
<span class="ltx_bibblock">Sharegpt4v: Improving large multi-modal models with better captions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">arXiv preprint arXiv:2311.12793</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al<span class="ltx_text" id="bib.bib23.3.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">Alpagasus: Training a better alpaca with fewer data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.4.1">arXiv preprint arXiv:2307.08701</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al<span class="ltx_text" id="bib.bib24.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.4.1">arXiv preprint arXiv:2107.03374</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jianfeng Gao, and Chunyuan Li. 2023c.

</span>
<span class="ltx_bibblock">Llava-interactive: An all-in-one demo for image chat, segmentation, generation and editing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">arXiv preprint arXiv:2311.00571</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Yirong Chen, Zhenyu Wang, Xiaofen Xing, Zhipei Xu, Kai Fang, Junhong Wang, Sihang Li, Jieling Wu, Qi Liu, Xiangmin Xu, et al<span class="ltx_text" id="bib.bib26.3.1">.</span> 2023d.

</span>
<span class="ltx_bibblock">Bianque: Balancing the questioning and suggestion ability of health llms with multi-turn health conversations polished by chatgpt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.4.1">arXiv preprint arXiv:2310.15896</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. 2024a.

</span>
<span class="ltx_bibblock">Self-play fine-tuning converts weak language models to strong language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">arXiv preprint arXiv:2401.01335</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, and Kyle Richardson. 2022.

</span>
<span class="ltx_bibblock">DISCO: Distilling counterfactuals with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">arXiv preprint arXiv:2212.10534</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chern et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ethan Chern, Haoyang Zou, Xuefeng Li, Jiewen Hu, Kuhua Feng, junlong Li, and Pengfei Liu. 2023.

</span>
<span class="ltx_bibblock">Generative AI for Math: Abel.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al<span class="ltx_text" id="bib.bib30.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.4.1">See https://vicuna. lmsys. org (accessed 14 April 2023)</em> 2, 3 (2023), 6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al<span class="ltx_text" id="bib.bib31.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Chatbot arena: An open platform for evaluating llms by human preference.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.4.1">arXiv preprint arXiv:2403.04132</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al<span class="ltx_text" id="bib.bib32.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.4.1">Journal of Machine Learning Research</em> 24, 240 (2023), 1–113.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chundawat et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Vikram S Chundawat, Ayush K Tarun, Murari Mandal, Mukund Lahoti, and Pratik Narang. 2022.

</span>
<span class="ltx_bibblock">A universal metric for robust evaluation of synthetic tabular data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">IEEE Transactions on Artificial Intelligence</em> 5, 1 (2022), 300–309.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
John Joon Young Chung, Ece Kamar, and Saleema Amershi. 2023.

</span>
<span class="ltx_bibblock">Increasing diversity while maintaining accuracy: Text data generation with large language models and human interventions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">arXiv preprint arXiv:2306.04140</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cubuk et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. 2019.

</span>
<span class="ltx_bibblock">Autoaugment: Learning augmentation strategies from data. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 113–123.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023b.

</span>
<span class="ltx_bibblock">Ultrafeedback: Boosting language models with high-quality feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">arXiv preprint arXiv:2310.01377</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and Li Yuan. 2023a.

</span>
<span class="ltx_bibblock">Chatlaw: Open-source legal large language model with integrated external knowledge bases.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">arXiv preprint arXiv:2306.16092</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, et al<span class="ltx_text" id="bib.bib38.3.1">.</span> 2023a.

</span>
<span class="ltx_bibblock">Chataug: Leveraging chatgpt for text data augmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.4.1">arXiv preprint arXiv:2302.13007</em> 1, 2 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. 2023b.

</span>
<span class="ltx_bibblock">Safe rlhf: Safe reinforcement learning from human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">arXiv preprint arXiv:2310.12773</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dalal et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Dhairya Dalal, Marco Valentino, André Freitas, and Paul Buitelaar. 2024.

</span>
<span class="ltx_bibblock">Inference to the Best Explanation in Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">arXiv preprint arXiv:2402.10767</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Darren Edge (2024)</span>
<span class="ltx_bibblock">
Newman Cheng Joshua Bradley Alex Chao Apurva Mody Steven Truitt Jonathan Larson Darren Edge, Ha Trinh. 2024.

</span>
<span class="ltx_bibblock">From Local to Global: A Graph RAG Approach to Query-Focused Summarization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2404.16130</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:1810.04805</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhariwal and Nichol (2021)</span>
<span class="ltx_bibblock">
Prafulla Dhariwal and Alexander Nichol. 2021.

</span>
<span class="ltx_bibblock">Diffusion models beat gans on image synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Advances in neural information processing systems</em> 34 (2021), 8780–8794.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li, Guizhen Chen, Wenhan Xia, Junjie Hu, Anh Tuan Luu, and Shafiq Joty. 2024.

</span>
<span class="ltx_bibblock">Data augmentation using llms: Data perspectives, learning paradigms and challenges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">arXiv preprint arXiv:2403.02990</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023.

</span>
<span class="ltx_bibblock">Enhancing chat language models by scaling high-quality instructional conversations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">arXiv preprint arXiv:2305.14233</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dixit et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022.

</span>
<span class="ltx_bibblock">CORE: A retrieve-then-edit framework for counterfactual data generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">arXiv preprint arXiv:2210.04873</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022.

</span>
<span class="ltx_bibblock">A survey on in-context learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">arXiv preprint arXiv:2301.00234</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yi Dong, Zhilin Wang, Makesh Narsimhan Sreedhar, Xianchao Wu, and Oleksii Kuchaiev. 2023.

</span>
<span class="ltx_bibblock">Steerlm: Attribute conditioned sft as an (user-steerable) alternative to rlhf.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">arXiv preprint arXiv:2310.05344</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doughty et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jacob Doughty, Zipiao Wan, Anishka Bompelli, Jubahed Qayum, Taozhi Wang, Juran Zhang, Yujia Zheng, Aidan Doyle, Pragnya Sridhar, Arav Agarwal, Christopher Bogart, Eric Keylor, Can Kultur, Jaromir Savelka, and Majd Sakr. 2024.

</span>
<span class="ltx_bibblock">A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">Proceedings of the 26th Australasian Computing Education Conference</em>. Association for Computing Machinery, 114–123.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yifan Du, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Jinpeng Wang, Chuyuan Wang, Mingchen Cai, Ruihua Song, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock">What makes for good visual instructions? synthesizing complex visual reasoning instructions for visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">arXiv preprint arXiv:2311.01487</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al<span class="ltx_text" id="bib.bib51.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.4.1">arXiv preprint arXiv:2407.21783</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dunlap et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E Gonzalez, and Trevor Darrell. 2023.

</span>
<span class="ltx_bibblock">Diversify your vision datasets with automatic diffusion-based augmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">Advances in neural information processing systems</em> 36 (2023), 79024–79034.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eghbali and Pradel (2024)</span>
<span class="ltx_bibblock">
Aryaz Eghbali and Michael Pradel. 2024.

</span>
<span class="ltx_bibblock">De-hallucinator: Iterative grounding for llm-based code completion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2401.01701</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eldan and Li (2023)</span>
<span class="ltx_bibblock">
Ronen Eldan and Yuanzhi Li. 2023.

</span>
<span class="ltx_bibblock">Tinystories: How small can language models be and still speak coherent english?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2305.07759</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al<span class="ltx_text" id="bib.bib55.3.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, Jang Hyun Cho, Marco Pavone, Song Han, and Hongxu Yin. 2024.

</span>
<span class="ltx_bibblock"><math alttext="VILA^{2}" class="ltx_Math" display="inline" id="bib.bib55.1.m1.1"><semantics id="bib.bib55.1.m1.1a"><mrow id="bib.bib55.1.m1.1.1" xref="bib.bib55.1.m1.1.1.cmml"><mi id="bib.bib55.1.m1.1.1.2" xref="bib.bib55.1.m1.1.1.2.cmml">V</mi><mo id="bib.bib55.1.m1.1.1.1" xref="bib.bib55.1.m1.1.1.1.cmml">⁢</mo><mi id="bib.bib55.1.m1.1.1.3" xref="bib.bib55.1.m1.1.1.3.cmml">I</mi><mo id="bib.bib55.1.m1.1.1.1a" xref="bib.bib55.1.m1.1.1.1.cmml">⁢</mo><mi id="bib.bib55.1.m1.1.1.4" xref="bib.bib55.1.m1.1.1.4.cmml">L</mi><mo id="bib.bib55.1.m1.1.1.1b" xref="bib.bib55.1.m1.1.1.1.cmml">⁢</mo><msup id="bib.bib55.1.m1.1.1.5" xref="bib.bib55.1.m1.1.1.5.cmml"><mi id="bib.bib55.1.m1.1.1.5.2" xref="bib.bib55.1.m1.1.1.5.2.cmml">A</mi><mn id="bib.bib55.1.m1.1.1.5.3" xref="bib.bib55.1.m1.1.1.5.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="bib.bib55.1.m1.1b"><apply id="bib.bib55.1.m1.1.1.cmml" xref="bib.bib55.1.m1.1.1"><times id="bib.bib55.1.m1.1.1.1.cmml" xref="bib.bib55.1.m1.1.1.1"></times><ci id="bib.bib55.1.m1.1.1.2.cmml" xref="bib.bib55.1.m1.1.1.2">𝑉</ci><ci id="bib.bib55.1.m1.1.1.3.cmml" xref="bib.bib55.1.m1.1.1.3">𝐼</ci><ci id="bib.bib55.1.m1.1.1.4.cmml" xref="bib.bib55.1.m1.1.1.4">𝐿</ci><apply id="bib.bib55.1.m1.1.1.5.cmml" xref="bib.bib55.1.m1.1.1.5"><csymbol cd="ambiguous" id="bib.bib55.1.m1.1.1.5.1.cmml" xref="bib.bib55.1.m1.1.1.5">superscript</csymbol><ci id="bib.bib55.1.m1.1.1.5.2.cmml" xref="bib.bib55.1.m1.1.1.5.2">𝐴</ci><cn id="bib.bib55.1.m1.1.1.5.3.cmml" type="integer" xref="bib.bib55.1.m1.1.1.5.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib55.1.m1.1c">VILA^{2}</annotation><annotation encoding="application/x-llamapun" id="bib.bib55.1.m1.1d">italic_V italic_I italic_L italic_A start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>: VILA Augmented VILA.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.4.1">arXiv preprint arXiv:2407.17453</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Steven Y Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021.

</span>
<span class="ltx_bibblock">A Survey of Data Augmentation Approaches for NLP. In <em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</em>. 968–988.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Steven Y Feng, Noah D Goodman, and Michael C Frank. 2024.

</span>
<span class="ltx_bibblock">Is Child-Directed Speech Effective Training Data for Language Models?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">arXiv preprint arXiv:2408.03617</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Floridi and Chiriatti (2020)</span>
<span class="ltx_bibblock">
Luciano Floridi and Massimo Chiriatti. 2020.

</span>
<span class="ltx_bibblock">GPT-3: Its nature, scope, limits, and consequences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Minds and Machines</em> 30 (2020), 681–694.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gandhi et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Saumya Gandhi, Ritu Gala, Vijay Viswanathan, Tongshuang Wu, and Graham Neubig. 2024.

</span>
<span class="ltx_bibblock">Better Synthetic Data by Retrieving and Transforming Existing Datasets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">arXiv preprint arXiv:2404.14361</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Bofei Gao, Feifan Song, Yibo Miao, Zefan Cai, Zhe Yang, Liang Chen, Helan Hu, Runxin Xu, Qingxiu Dong, Ce Zheng, et al<span class="ltx_text" id="bib.bib60.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Towards a Unified View of Preference Learning for Large Language Models: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.4.1">arXiv preprint arXiv:2409.02795</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gehman et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. 2020.

</span>
<span class="ltx_bibblock">Realtoxicityprompts: Evaluating neural toxic degeneration in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">arXiv preprint arXiv:2009.11462</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghiasi et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. 2021.

</span>
<span class="ltx_bibblock">Simple copy-paste is a strong data augmentation method for instance segmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 2918–2928.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Giglou et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Hamed Babaei Giglou, Jennifer D’Souza, and Sören Auer. 2024.

</span>
<span class="ltx_bibblock">LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">arXiv preprint arXiv:2409.18812</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilardi et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023.

</span>
<span class="ltx_bibblock">ChatGPT outperforms crowd workers for text-annotation tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">Proceedings of the National Academy of Sciences</em> 120, 30 (2023), e2305016120.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gillespie et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (1987)</span>
<span class="ltx_bibblock">
Alan R Gillespie, Anne B Kahle, and Richard E Walker. 1987.

</span>
<span class="ltx_bibblock">Color enhancement of highly correlated images. II. Channel ratio and “chromaticity” transformation techniques.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">Remote Sensing of Environment</em> 22, 3 (1987), 343–365.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Golchin and Surdeanu (2023)</span>
<span class="ltx_bibblock">
Shahriar Golchin and Mihai Surdeanu. 2023.

</span>
<span class="ltx_bibblock">Time travel in llms: Tracing data contamination in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv preprint arXiv:2308.08493</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gulcehre et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al<span class="ltx_text" id="bib.bib67.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Reinforced self-training (rest) for language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.4.1">arXiv preprint arXiv:2308.08998</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gunasekar et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al<span class="ltx_text" id="bib.bib68.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Textbooks are all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.4.1">arXiv preprint arXiv:2306.11644</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib69.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023.

</span>
<span class="ltx_bibblock">How close is chatgpt to human experts? comparison corpus, evaluation, and detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.3.1">arXiv preprint arXiv:2301.07597</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib70.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al<span class="ltx_text" id="bib.bib70.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Direct language model alignment from online ai feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.4.1">arXiv preprint arXiv:2402.04792</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haluptzok et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. 2022a.

</span>
<span class="ltx_bibblock">Generating Programming Puzzles to Train Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">Deep Learning for Code Workshop</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haluptzok et al<span class="ltx_text" id="bib.bib72.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. 2022b.

</span>
<span class="ltx_bibblock">Language models can teach themselves to program better.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.3.1">arXiv preprint arXiv:2207.14502</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haluptzok et al<span class="ltx_text" id="bib.bib73.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. 2023.

</span>
<span class="ltx_bibblock">Language Models Can Teach Themselves to Program Better. In <em class="ltx_emph ltx_font_italic" id="bib.bib73.3.1">International Conference on Learning Representations (ICLR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. 2023.

</span>
<span class="ltx_bibblock">Chartllama: A multimodal llm for chart understanding and generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.3.1">arXiv preprint arXiv:2311.16483</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al<span class="ltx_text" id="bib.bib75.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jing Hao, Yuxiang Zhao, Song Chen, Yanpeng Sun, Qiang Chen, Gang Zhang, Kun Yao, Errui Ding, and Jingdong Wang. 2024.

</span>
<span class="ltx_bibblock">FullAnno: A Data Engine for Enhancing Image Comprehension of MLLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.3.1">arXiv preprint arXiv:2409.13540</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiaoshuai Hao, Yi Zhu, Srikar Appalaraju, Aston Zhang, Wanqian Zhang, Bo Li, and Mu Li. 2023.

</span>
<span class="ltx_bibblock">Mixgen: A new multi-modal data augmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib76.3.1">Proceedings of the IEEE/CVF winter conference on applications of computer vision</em>. 379–389.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and Xiaojuan Qi. 2022.

</span>
<span class="ltx_bibblock">Is synthetic data from generative models ready for image recognition?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.3.1">arXiv preprint arXiv:2210.07574</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al<span class="ltx_text" id="bib.bib78.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021.

</span>
<span class="ltx_bibblock">Measuring mathematical problem solving with the math dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.3.1">arXiv preprint arXiv:2103.03874</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.3.1">Advances in neural information processing systems</em> 33 (2020), 6840–6851.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong et al<span class="ltx_text" id="bib.bib80.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jixiang Hong, Quan Tu, Changyu Chen, Xing Gao, Ji Zhang, and Rui Yan. 2023.

</span>
<span class="ltx_bibblock">Cyclealign: Iterative distillation from black-box llm to white-box models for better human alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.3.1">arXiv preprint arXiv:2310.16271</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et al<span class="ltx_text" id="bib.bib81.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022.

</span>
<span class="ltx_bibblock">Unnatural instructions: Tuning language models with (almost) no human labor.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.3.1">arXiv preprint arXiv:2212.09689</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib82.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.

</span>
<span class="ltx_bibblock">Large language models can self-improve.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.3.1">arXiv preprint arXiv:2210.11610</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib83.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Quzhe Huang, Mingxu Tao, Zhenwei An, Chen Zhang, Cong Jiang, Zhibin Chen, Zirui Wu, and Yansong Feng. 2023.

</span>
<span class="ltx_bibblock">Lawyer LLaMA Technical Report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.3.1">arXiv</em> abs/2305.15062 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib84.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yanxian Huang, Wanjun Zhong, Ensheng Shi, Min Yang, Jiachi Chen, Hui Li, Yuchi Ma, Qianxiang Wang, Zibin Zheng, and Yanlin Wang. 2024.

</span>
<span class="ltx_bibblock">Agents in Software Engineering: Survey, Landscape, and Vision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.3.1">arXiv preprint arXiv:2409.09030</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Inoue (2018)</span>
<span class="ltx_bibblock">
Hiroshi Inoue. 2018.

</span>
<span class="ltx_bibblock">Data augmentation by pairing samples for images classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">arXiv preprint arXiv:1801.02929</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al<span class="ltx_text" id="bib.bib86.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2024.

</span>
<span class="ltx_bibblock">Beavertails: Towards improved safety alignment of llm via a human-preference dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.3.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib87.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Wenda Li, and Mateja Jamnik. 2023a.

</span>
<span class="ltx_bibblock">Multilingual Mathematical Autoformalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.3.1">arXiv</em> abs/2311.03755 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib88.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Lan Jiang, Ye Mao, Xiangfeng Wang, Xi Chen, and Chao Li. 2023b.

</span>
<span class="ltx_bibblock">Cola-diff: Conditional latent diffusion model for multi-modal mri synthesis. In <em class="ltx_emph ltx_font_italic" id="bib.bib88.3.1">International Conference on Medical Image Computing and Computer-Assisted Intervention</em>. Springer, 398–408.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang (2024)</span>
<span class="ltx_bibblock">
Ziyan Jiang. 2024.

</span>
<span class="ltx_bibblock">LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">arXiv preprint arXiv:2406.15319</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao et al<span class="ltx_text" id="bib.bib90.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Junfeng Jiao, Saleh Afroogh, Yiming Xu, and Connor Phillips. 2024.

</span>
<span class="ltx_bibblock">Navigating llm ethics: Advancements, challenges, and future directions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.3.1">arXiv preprint arXiv:2406.18841</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jocher (2020)</span>
<span class="ltx_bibblock">
Glenn Jocher. 2020.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">YOLOv5 by Ultralytics</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.5281/zenodo.3908559" title="">https://doi.org/10.5281/zenodo.3908559</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jung et al<span class="ltx_text" id="bib.bib92.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, and Yejin Choi. 2024.

</span>
<span class="ltx_bibblock">Impossible Distillation for Paraphrasing and Summarization: How to Make High-quality Lemonade out of Small, Low-quality Model. In <em class="ltx_emph ltx_font_italic" id="bib.bib92.3.1">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>. 4439–4454.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalra et al<span class="ltx_text" id="bib.bib93.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Agastya Kalra, Guy Stoppi, Bradley Brown, Rishav Agarwal, and Achuta Kadambi. 2021.

</span>
<span class="ltx_bibblock">Towards rotation invariance in object detection. In <em class="ltx_emph ltx_font_italic" id="bib.bib93.3.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 3530–3540.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karunya et al<span class="ltx_text" id="bib.bib94.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
S Karunya, M Jalakandeshwaran, Thanuja Babu, and R Uma. 2023.

</span>
<span class="ltx_bibblock">AI-Powered Real-Time Speech-to-Speech Translation for Virtual Meetings Using Machine Learning Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib94.3.1">2023 Intelligent Computing and Control for Engineering and Business Systems (ICCEBS)</em>. IEEE, 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al<span class="ltx_text" id="bib.bib95.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zaid Khan, Vijay Kumar BG, Samuel Schulter, Xiang Yu, Yun Fu, and Manmohan Chandraker. 2023.

</span>
<span class="ltx_bibblock">Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!. In <em class="ltx_emph ltx_font_italic" id="bib.bib95.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 15005–15015.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kobayashi (2018)</span>
<span class="ltx_bibblock">
Sosuke Kobayashi. 2018.

</span>
<span class="ltx_bibblock">Contextual augmentation: Data augmentation by words with paradigmatic relations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">arXiv preprint arXiv:1805.06201</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köksal et al<span class="ltx_text" id="bib.bib97.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Abdullatif Köksal, Timo Schick, Anna Korhonen, and Hinrich Schütze. 2023.

</span>
<span class="ltx_bibblock">Longform: Optimizing instruction tuning for long text generation with corpus extraction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib97.3.1">arXiv preprint arXiv:2304.08460</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köpf et al<span class="ltx_text" id="bib.bib98.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, et al<span class="ltx_text" id="bib.bib98.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Openassistant conversations-democratizing large language model alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib98.4.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krell and Kim (2017)</span>
<span class="ltx_bibblock">
Mario Michael Krell and Su Kyoung Kim. 2017.

</span>
<span class="ltx_bibblock">Rotational data augmentation for electroencephalographic data. In <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</em>. IEEE, 471–474.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Labruna et al<span class="ltx_text" id="bib.bib100.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Tiziano Labruna, Sofia Brenna, Andrea Zaninello, and Bernardo Magnini. 2023.

</span>
<span class="ltx_bibblock">Unraveling chatgpt: A critical analysis of ai-generated goal-oriented dialogues and annotations. In <em class="ltx_emph ltx_font_italic" id="bib.bib100.3.1">International Conference of the Italian Association for Artificial Intelligence</em>. Springer, 151–171.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al<span class="ltx_text" id="bib.bib101.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. 2024.

</span>
<span class="ltx_bibblock">Lisa: Reasoning segmentation via large language model. In <em class="ltx_emph ltx_font_italic" id="bib.bib101.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 9579–9589.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lambert et al<span class="ltx_text" id="bib.bib102.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al<span class="ltx_text" id="bib.bib102.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Rewardbench: Evaluating reward models for language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib102.4.1">arXiv preprint arXiv:2403.13787</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Latif et al<span class="ltx_text" id="bib.bib103.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ehsan Latif, Luyang Fang, Ping Ma, and Xiaoming Zhai. 2024.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib103.3.1">Knowledge Distillation of LLMs for Automatic Scoring of Science Assessments</em>.

</span>
<span class="ltx_bibblock">Springer Nature Switzerland. 166–174 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Latif et al<span class="ltx_text" id="bib.bib104.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Siddique Latif, Muhammad Usama, Mohammad Ibrahim Malik, and Björn W Schuller. 2023.

</span>
<span class="ltx_bibblock">Can large language models aid in annotating speech emotional data? uncovering new frontiers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib104.3.1">arXiv preprint arXiv:2307.06090</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et al<span class="ltx_text" id="bib.bib105.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022.

</span>
<span class="ltx_bibblock">Coderl: Mastering code generation through pretrained models and deep reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib105.3.1">Advances in Neural Information Processing Systems</em> 35 (2022), 21314–21328.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le Scao et al<span class="ltx_text" id="bib.bib106.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al<span class="ltx_text" id="bib.bib106.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib107.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023.

</span>
<span class="ltx_bibblock">Rlaif: Scaling reinforcement learning from human feedback with ai feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib107.3.1">arXiv preprint arXiv:2309.00267</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib108.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Nicholas Lee, Thanakul Wattanawong, Sehoon Kim, Karttikeya Mangalam, Sheng Shen, Gopala Anumanchipali, Michael W Mahoney, Kurt Keutzer, and Amir Gholami. 2024.

</span>
<span class="ltx_bibblock">Llm2llm: Boosting llms with novel iterative data enhancement.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib108.3.1">arXiv preprint arXiv:2403.15042</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewkowycz et al<span class="ltx_text" id="bib.bib109.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al<span class="ltx_text" id="bib.bib109.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Solving quantitative reasoning problems with language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.4.1">Advances in Neural Information Processing Systems</em> 35 (2022), 3843–3857.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib110.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng. 2024d.

</span>
<span class="ltx_bibblock">Common 7b language models already possess strong math capabilities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib110.3.1">arXiv preprint arXiv:2403.04706</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib111.2.2.1">.</span> (2024e)</span>
<span class="ltx_bibblock">
Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. 2024e.

</span>
<span class="ltx_bibblock">Llava-med: Training a large language-and-vision assistant for biomedicine in one day.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib111.3.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib112.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang, Haoyang Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, et al<span class="ltx_text" id="bib.bib112.3.1">.</span> 2024b.

</span>
<span class="ltx_bibblock">Synthetic data (almost) from scratch: Generalized instruction tuning for language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib112.4.1">arXiv preprint arXiv:2402.13064</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib113.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022b.

</span>
<span class="ltx_bibblock">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib113.3.1">International conference on machine learning</em>. PMLR, 12888–12900.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib114.2.2.1">.</span> (2023g)</span>
<span class="ltx_bibblock">
Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and Zhi Jin. 2023g.

</span>
<span class="ltx_bibblock">AceCoder: Utilizing Existing Code to Enhance Code Generation. CoRR (2023).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib115.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, and Tianyi Zhou. 2024a.

</span>
<span class="ltx_bibblock">Selective reflection-tuning: Student-selected data recycling for llm instruction-tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib115.3.1">arXiv preprint arXiv:2402.10110</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib116.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Heng Huang, Jiuxiang Gu, and Tianyi Zhou. 2023c.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib116.3.1">Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.11716" title="">https://arxiv.org/abs/2310.11716</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib117.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Minzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan, Nancy F Chen, Zhengyuan Liu, and Diyi Yang. 2023d.

</span>
<span class="ltx_bibblock">Coannotating: Uncertainty-guided work allocation between human and large language models for data annotation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib117.3.1">arXiv preprint arXiv:2310.15638</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib118.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al<span class="ltx_text" id="bib.bib118.3.1">.</span> 2023a.

</span>
<span class="ltx_bibblock">Starcoder: may the source be with you!

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib118.4.1">arXiv preprint arXiv:2305.06161</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib119.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Sihang Li, Jian Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun Xu, Xiang Wang, Linfeng Zhang, Guolin Ke, and Hengxing Cai. 2024c.

</span>
<span class="ltx_bibblock">SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib119.3.1">arXiv preprint arXiv:2408.15545</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib120.2.2.1">.</span> (2023e)</span>
<span class="ltx_bibblock">
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 2023e.

</span>
<span class="ltx_bibblock">Self-alignment with instruction backtranslation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib120.3.1">arXiv preprint arXiv:2308.06259</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib121.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023b.

</span>
<span class="ltx_bibblock">Textbooks are all you need ii: phi-1.5 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib121.3.1">arXiv preprint arXiv:2309.05463</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib122.2.2.1">.</span> (2023f)</span>
<span class="ltx_bibblock">
Yanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin Fu, Guosheng Lin, Chunhua Shen, Ling Chen, and Yunchao Wei. 2023f.

</span>
<span class="ltx_bibblock">Stablellava: Enhanced visual instruction tuning with synthesized image-dialogue data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib122.3.1">arXiv preprint arXiv:2308.10253</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib123.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, and Xifeng Yan. 2022a.

</span>
<span class="ltx_bibblock">Controllable dialogue simulation with in-context learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib123.3.1">arXiv preprint arXiv:2210.04185</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al<span class="ltx_text" id="bib.bib124.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiao Liang, Xinyu Hu, Simiao Zuo, Yeyun Gong, Qiang Lou, Yi Liu, Shao-Lun Huang, and Jian Jiao. 2024.

</span>
<span class="ltx_bibblock">Task Oriented In-Domain Data Augmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib124.3.1">arXiv preprint arXiv:2406.16694</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lightman et al<span class="ltx_text" id="bib.bib125.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023.

</span>
<span class="ltx_bibblock">Let’s verify step by step.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib125.3.1">arXiv preprint arXiv:2305.20050</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib126.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Bingqian Lin, Yunshuang Nie, Ziming Wei, Jiaqi Chen, Shikui Ma, Jianhua Han, Hang Xu, Xiaojun Chang, and Xiaodan Liang. 2024b.

</span>
<span class="ltx_bibblock">NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib126.3.1">arXiv preprint arXiv:2403.07376</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib127.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. 2024a.

</span>
<span class="ltx_bibblock">CriticBench: Benchmarking LLMs for Critique-Correct Reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib127.3.1">arXiv preprint arXiv:2402.14809</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib128.2.2.1">.</span> (2024e)</span>
<span class="ltx_bibblock">
An Liu, Zonghan Yang, Zhenhe Zhang, Qingyuan Hu, Peng Li, Ming Yan, Ji Zhang, Fei Huang, and Yang Liu. 2024e.

</span>
<span class="ltx_bibblock">PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib128.3.1">arXiv preprint arXiv:2402.12835</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib129.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang. 2023a.

</span>
<span class="ltx_bibblock">Tinygsm: achieving¿ 80% on gsm8k with small language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib129.3.1">arXiv preprint arXiv:2312.09241</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib130.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024b.

</span>
<span class="ltx_bibblock">Improved baselines with visual instruction tuning. In <em class="ltx_emph ltx_font_italic" id="bib.bib130.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 26296–26306.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib131.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024c.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib131.3.1">Advances in neural information processing systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib132.2.2.1">.</span> (2024f)</span>
<span class="ltx_bibblock">
Haoxiong Liu, Yifan Zhang, Yifan Luo, and Andrew C Yao. 2024f.

</span>
<span class="ltx_bibblock">Augmenting Math Word Problems via Iterative Question Composing. In <em class="ltx_emph ltx_font_italic" id="bib.bib132.3.1">ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib133.2.2.1">.</span> (9 27)</span>
<span class="ltx_bibblock">
June M. Liu, Donghao Li, He Cao, Tianhe Ren, Zeyi Liao, and Jiamin Wu. 2023-09-27.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib133.3.1">ChatCounselor: A Large Language Models for Mental Health Support</em>.

</span>
<span class="ltx_bibblock">arXiv:2309.15461 [cs]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2309.15461" title="">http://arxiv.org/abs/2309.15461</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib134.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Linlin Liu, Xin Li, Ruidan He, Lidong Bing, Shafiq Joty, and Luo Si. 2021.

</span>
<span class="ltx_bibblock">Enhancing multilingual language model with massive multilingual knowledge triples.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib134.3.1">arXiv preprint arXiv:2111.10962</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib135.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Ning Liu, Siavash Jafarzadeh, Brian Y Lattimer, Shuna Ni, Jim Lua, and Yue Yu. 2024a.

</span>
<span class="ltx_bibblock">Large language models, physics-based modeling, experimental measurements: the trinity of data-scarce learning of polymer properties.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib135.3.1">arXiv preprint arXiv:2407.02770</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib136.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Pei Liu, Xuemin Wang, Chao Xiang, and Weiye Meng. 2020.

</span>
<span class="ltx_bibblock">A survey of text data augmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib136.3.1">2020 International Conference on Computer Communication and Network Security (CCNS)</em>. IEEE, 191–195.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib137.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, and Andrew M Dai. 2022.

</span>
<span class="ltx_bibblock">Mind’s eye: Grounded language model reasoning through simulation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib137.3.1">arXiv preprint arXiv:2210.05359</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib138.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, and Andrew M. Dai. 2024d.

</span>
<span class="ltx_bibblock">Best Practices and Lessons Learned on Synthetic Data for Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.07503 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib139.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al<span class="ltx_text" id="bib.bib139.3.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">Llava-plus: Learning to use tools for creating multimodal agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib139.4.1">arXiv preprint arXiv:2311.05437</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">arXiv preprint arXiv:1907.11692</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long et al<span class="ltx_text" id="bib.bib141.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. 2024.

</span>
<span class="ltx_bibblock">On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey. In <em class="ltx_emph ltx_font_italic" id="bib.bib141.3.1">Findings of the Association for Computational Linguistics ACL 2024</em>. 11065–11082.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Longpre et al<span class="ltx_text" id="bib.bib142.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al<span class="ltx_text" id="bib.bib142.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">The flan collection: Designing data and methods for effective instruction tuning. In <em class="ltx_emph ltx_font_italic" id="bib.bib142.4.1">International Conference on Machine Learning</em>. PMLR, 22631–22648.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span class="ltx_text" id="bib.bib143.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Bo-Ru Lu, Nikita Haduong, Chia-Hsuan Lee, Zeqiu Wu, Hao Cheng, Paul Koester, Jean Utke, Tao Yu, Noah A Smith, and Mari Ostendorf. 2023.

</span>
<span class="ltx_bibblock">DIALGEN: collaborative human-lm generated dialogues for improved understanding of human-human conversations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib143.3.1">arXiv preprint arXiv:2307.07047</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span class="ltx_text" id="bib.bib144.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Yinquan Lu, Wenhao Zhu, Lei Li, Yu Qiao, and Fei Yuan. 2024b.

</span>
<span class="ltx_bibblock">Llamax: Scaling linguistic horizons of llm by enhancing translation capabilities beyond 100 languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib144.3.1">arXiv preprint arXiv:2407.05975</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span class="ltx_text" id="bib.bib145.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Zimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. 2024a.

</span>
<span class="ltx_bibblock">Mathgenie: Generating synthetic data with question back-translation for enhancing mathematical reasoning of llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib145.3.1">arXiv preprint arXiv:2402.16352</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib146.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023.

</span>
<span class="ltx_bibblock">Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib146.3.1">arXiv preprint arXiv:2308.09583</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib147.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2024.

</span>
<span class="ltx_bibblock">WizardCoder: Empowering Code Large Language Models with Evol-Instruct. In <em class="ltx_emph ltx_font_italic" id="bib.bib147.3.1">International Conference on Learning Representations (ICLR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib148.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Runyuan Ma, Wei Li, and Fukai Shang. 2024.

</span>
<span class="ltx_bibblock">Investigating Public Fine-Tuning Datasets: A Complex Review of Current Practices from a Construction Perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib148.3.1">arXiv preprint arXiv:2407.08475</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et al<span class="ltx_text" id="bib.bib149.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al<span class="ltx_text" id="bib.bib149.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Self-refine: Iterative refinement with self-feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib149.4.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maharana et al<span class="ltx_text" id="bib.bib150.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kiran Maharana, Surajit Mondal, and Bhushankumar Nemade. 2022.

</span>
<span class="ltx_bibblock">A review: Data pre-processing and data augmentation techniques.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib150.3.1">Global Transitions Proceedings</em> 3, 1 (2022), 91–99.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maini et al<span class="ltx_text" id="bib.bib151.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. 2024.

</span>
<span class="ltx_bibblock">Rephrasing the web: A recipe for compute and data-efficient language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib151.3.1">arXiv preprint arXiv:2401.16380</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mañas et al<span class="ltx_text" id="bib.bib152.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Oscar Mañas, Benno Krojer, and Aishwarya Agrawal. 2024.

</span>
<span class="ltx_bibblock">Improving automatic vqa evaluation using large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib152.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 38. 4171–4179.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitra et al<span class="ltx_text" id="bib.bib153.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, et al<span class="ltx_text" id="bib.bib153.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Orca 2: Teaching small language models how to reason.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib153.4.1">arXiv preprint arXiv:2311.11045</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mondal and Lipizzi (2024)</span>
<span class="ltx_bibblock">
Devam Mondal and Carlo Lipizzi. 2024.

</span>
<span class="ltx_bibblock">Mitigating Large Language Model Bias: Automated Dataset Augmentation and Prejudice Quantification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">Computers</em> 13, 6 (2024), 141.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mukherjee et al<span class="ltx_text" id="bib.bib155.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023.

</span>
<span class="ltx_bibblock">Orca: Progressive learning from complex explanation traces of gpt-4.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib155.3.1">arXiv preprint arXiv:2306.02707</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nagireddy et al<span class="ltx_text" id="bib.bib156.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Manish Nagireddy, Bernat Guillén Pegueroles, and Ioana Baldini. 2024.

</span>
<span class="ltx_bibblock">DARE to Diversify: DAta Driven and Diverse LLM REd Teaming. In <em class="ltx_emph ltx_font_italic" id="bib.bib156.3.1">Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>. 6420–6421.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano et al<span class="ltx_text" id="bib.bib157.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al<span class="ltx_text" id="bib.bib157.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Webgpt: Browser-assisted question-answering with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib157.4.1">arXiv preprint arXiv:2112.09332</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nijkamp et al<span class="ltx_text" id="bib.bib158.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022.

</span>
<span class="ltx_bibblock">A conversational paradigm for program synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib158.3.1">arXiv preprint arXiv:2203.13474</em> 30 (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Özdemir and Akagündüz (2024)</span>
<span class="ltx_bibblock">
Övgü Özdemir and Erdem Akagündüz. 2024.

</span>
<span class="ltx_bibblock">Enhancing Visual Question Answering through Question-Driven Image Captions as Prompts. In <em class="ltx_emph ltx_font_italic" id="bib.bib159.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 1562–1571.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel et al<span class="ltx_text" id="bib.bib160.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ajay Patel, Colin Raffel, and Chris Callison-Burch. 2024.

</span>
<span class="ltx_bibblock">DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib160.3.1">arXiv preprint arXiv:2402.10379</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel et al<span class="ltx_text" id="bib.bib161.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Arkil Patel, Siva Reddy, Dzmitry Bahdanau, and Pradeep Dasigi. 2023.

</span>
<span class="ltx_bibblock">Evaluating In-Context Learning of Libraries for Code Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib161.3.1">arXiv preprint arXiv:2311.09635</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pinaya et al<span class="ltx_text" id="bib.bib162.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Walter HL Pinaya, Mark S Graham, Eric Kerfoot, Petru-Daniel Tudosiu, Jessica Dafflon, Virginia Fernandez, Pedro Sanchez, Julia Wolleb, Pedro F Da Costa, Ashay Patel, et al<span class="ltx_text" id="bib.bib162.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Generative ai for medical imaging: extending the monai framework.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib162.4.1">arXiv preprint arXiv:2307.15208</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu et al<span class="ltx_text" id="bib.bib163.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Han Qiu, Yi Zeng, Tianwei Zhang, Yong Jiang, and Meikang Qiu. 2020.

</span>
<span class="ltx_bibblock">Fencebox: A platform for defeating adversarial examples with data augmentation techniques.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib163.3.1">arXiv preprint arXiv:2012.01701</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu et al<span class="ltx_text" id="bib.bib164.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jianing Qiu, Lin Li, Jiankai Sun, Jiachuan Peng, Peilun Shi, Ruiyang Zhang, Yinzhao Dong, Kyle Lam, Frank P-W Lo, Bo Xiao, et al<span class="ltx_text" id="bib.bib164.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Large ai models in health informatics: Applications, challenges, and the future.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib164.4.1">IEEE Journal of Biomedical and Health Informatics</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span class="ltx_text" id="bib.bib165.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al<span class="ltx_text" id="bib.bib165.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision. In <em class="ltx_emph ltx_font_italic" id="bib.bib165.4.1">International conference on machine learning</em>. PMLR, 8748–8763.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et al<span class="ltx_text" id="bib.bib166.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib166.3.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rathje et al<span class="ltx_text" id="bib.bib167.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Steve Rathje, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire E Robertson, and Jay J Van Bavel. 2024.

</span>
<span class="ltx_bibblock">GPT is an effective tool for multilingual psychological text analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib167.3.1">Proceedings of the National Academy of Sciences</em> 121, 34 (2024), e2308950121.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span class="ltx_text" id="bib.bib168.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Xiaoxue Ren, Xinyuan Ye, Dehai Zhao, Zhenchang Xing, and Xiaohu Yang. 2023a.

</span>
<span class="ltx_bibblock">From Misuse to Mastery: Enhancing Code Generation with Knowledge-Driven AI Chaining. In <em class="ltx_emph ltx_font_italic" id="bib.bib168.3.1">2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)</em>. IEEE, 976–987.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span class="ltx_text" id="bib.bib169.5.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda Zhang, Alexander Podolskiy, Grigory Arshinov, et al<span class="ltx_text" id="bib.bib169.6.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">Pangu-<math alttext="\{" class="ltx_Math" display="inline" id="bib.bib169.1.m1.1"><semantics id="bib.bib169.1.m1.1a"><mo id="bib.bib169.1.m1.1.1" stretchy="false" xref="bib.bib169.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib169.1.m1.1b"><ci id="bib.bib169.1.m1.1.1.cmml" xref="bib.bib169.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib169.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib169.1.m1.1d">{</annotation></semantics></math><math alttext="\backslash" class="ltx_Math" display="inline" id="bib.bib169.2.m2.1"><semantics id="bib.bib169.2.m2.1a"><mo id="bib.bib169.2.m2.1.1" xref="bib.bib169.2.m2.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="bib.bib169.2.m2.1b"><ci id="bib.bib169.2.m2.1.1.cmml" xref="bib.bib169.2.m2.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib169.2.m2.1c">\backslash</annotation><annotation encoding="application/x-llamapun" id="bib.bib169.2.m2.1d">\</annotation></semantics></math>Sigma<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib169.3.m3.1"><semantics id="bib.bib169.3.m3.1a"><mo id="bib.bib169.3.m3.1.1" stretchy="false" xref="bib.bib169.3.m3.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib169.3.m3.1b"><ci id="bib.bib169.3.m3.1.1.cmml" xref="bib.bib169.3.m3.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib169.3.m3.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib169.3.m3.1d">}</annotation></semantics></math>: Towards trillion parameter language model with sparse heterogeneous computing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib169.7.1">arXiv preprint arXiv:2303.10845</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rezende and Mohamed (2015)</span>
<span class="ltx_bibblock">
Danilo Rezende and Shakir Mohamed. 2015.

</span>
<span class="ltx_bibblock">Variational inference with normalizing flows. In <em class="ltx_emph ltx_font_italic" id="bib.bib170.1.1">International conference on machine learning</em>. PMLR, 1530–1538.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ri et al<span class="ltx_text" id="bib.bib171.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ryokan Ri, Shun Kiyono, and Sho Takase. 2024.

</span>
<span class="ltx_bibblock">Self-Translate-Train: A Simple but Strong Baseline for Cross-lingual Transfer of Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib171.3.1">arXiv preprint arXiv:2407.00454</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al<span class="ltx_text" id="bib.bib172.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models. In <em class="ltx_emph ltx_font_italic" id="bib.bib172.3.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 10684–10695.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosati et al<span class="ltx_text" id="bib.bib173.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Domenic Rosati, Jan Wehner, Kai Williams, Łukasz Bartoszcze, David Atanasov, Robie Gonzales, Subhabrata Majumdar, Carsten Maple, Hassan Sajjad, and Frank Rudzicz. 2024.

</span>
<span class="ltx_bibblock">Representation noising effectively prevents harmful fine-tuning on LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib173.3.1">arXiv preprint arXiv:2405.14577</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rozière et al<span class="ltx_text" id="bib.bib174.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023.

</span>
<span class="ltx_bibblock">Code Llama: Open Foundation Models for Code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib174.3.1">arXiv</em> abs/2308.12950 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rozière et al<span class="ltx_text" id="bib.bib175.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2024.

</span>
<span class="ltx_bibblock">Code Llama: Open Foundation Models for Code.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2308.12950" title="">https://doi.org/10.48550/arXiv.2308.12950</a>
arXiv:2308.12950 [cs]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Samuel et al<span class="ltx_text" id="bib.bib176.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Vinay Samuel, Houda Aynaou, Arijit Ghosh Chowdhury, Karthik Venkat Ramanan, and Aman Chadha. 2023.

</span>
<span class="ltx_bibblock">Can llms augment low-resource reading comprehension datasets? opportunities and challenges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib176.3.1">arXiv preprint arXiv:2309.12426</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saxton et al<span class="ltx_text" id="bib.bib177.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019.

</span>
<span class="ltx_bibblock">Analysing mathematical reasoning abilities of neural models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib177.3.1">arXiv preprint arXiv:1904.01557</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al<span class="ltx_text" id="bib.bib178.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to use tools.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib178.3.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schumann et al<span class="ltx_text" id="bib.bib179.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu, Stefan Riezler, and William Yang Wang. 2024.

</span>
<span class="ltx_bibblock">Velma: Verbalization embodiment of llm agents for vision and language navigation in street view. In <em class="ltx_emph ltx_font_italic" id="bib.bib179.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 38. 18924–18933.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">See et al<span class="ltx_text" id="bib.bib180.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Abigail See, Peter J Liu, and Christopher D Manning. 2017.

</span>
<span class="ltx_bibblock">Get to the point: Summarization with pointer-generator networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib180.3.1">arXiv preprint arXiv:1704.04368</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al<span class="ltx_text" id="bib.bib181.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Yunfan Shao, Linyang Li, Yichuan Ma, Peiji Li, Demin Song, Qinyuan Cheng, Shimin Li, Xiaonan Li, Pengyu Wang, Qipeng Guo, et al<span class="ltx_text" id="bib.bib181.3.1">.</span> 2024a.

</span>
<span class="ltx_bibblock">Case2Code: Learning Inductive Reasoning with Synthetic Data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib181.4.1">arXiv preprint arXiv:2407.12504</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al<span class="ltx_text" id="bib.bib182.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024b.

</span>
<span class="ltx_bibblock">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib182.3.1">arXiv</em> abs/2402.03300 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib183.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hong Shen, Tianshi Li, Toby Jia-Jun Li, Joon Sung Park, and Diyi Yang. 2023.

</span>
<span class="ltx_bibblock">Shaping the emerging norms of using large language models in social computing research. In <em class="ltx_emph ltx_font_italic" id="bib.bib183.3.1">Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing</em>. 569–571.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et al<span class="ltx_text" id="bib.bib184.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023.

</span>
<span class="ltx_bibblock">Reflexion: language agents with verbal reinforcement learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib184.3.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://papers.nips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html" title="">http://papers.nips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shorten and Khoshgoftaar (2019)</span>
<span class="ltx_bibblock">
Connor Shorten and Taghi M Khoshgoftaar. 2019.

</span>
<span class="ltx_bibblock">A survey on image data augmentation for deep learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib185.1.1">Journal of big data</em> 6, 1 (2019), 1–48.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shorten et al<span class="ltx_text" id="bib.bib186.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Connor Shorten, Taghi M Khoshgoftaar, and Borko Furht. 2021.

</span>
<span class="ltx_bibblock">Text data augmentation for deep learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib186.3.1">Journal of big Data</em> 8, 1 (2021), 101.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shypula et al<span class="ltx_text" id="bib.bib187.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Alexander G Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob R. Gardner, Yiming Yang, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, and Amir Yazdanbakhsh. 2024.

</span>
<span class="ltx_bibblock">Learning Performance-Improving Code Edits. In <em class="ltx_emph ltx_font_italic" id="bib.bib187.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al<span class="ltx_text" id="bib.bib188.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, et al<span class="ltx_text" id="bib.bib188.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Beyond human data: Scaling self-training for problem-solving with language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib188.4.1">arXiv preprint arXiv:2312.06585</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Skalse et al<span class="ltx_text" id="bib.bib189.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. 2022.

</span>
<span class="ltx_bibblock">Defining and characterizing reward gaming.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib189.3.1">Advances in Neural Information Processing Systems</em> 35 (2022), 9460–9471.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stiennon et al<span class="ltx_text" id="bib.bib190.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020.

</span>
<span class="ltx_bibblock">Learning to summarize with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib190.3.1">Advances in Neural Information Processing Systems</em> 33 (2020), 3008–3021.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al<span class="ltx_text" id="bib.bib191.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Chang Su, Yu Hou, Manqi Zhou, Suraj Rajendran, Jacqueline R. M. A. Maasch, Zehra Abedi, Haotan Zhang, Zilong Bai, Anthony Cuturrufo, Winston Guo, Fayzan F. Chaudhry, Gregory Ghahramani, Jian Tang, Feixiong Cheng, Yue Li, Rui Zhang, Steven T. DeKosky, Jiang Bian, and Fei Wang. 2023.

</span>
<span class="ltx_bibblock">Biomedical discovery through the integrative biomedical knowledge hub (iBKH).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib191.3.1">iScience</em> 26, 4 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sudalairaj et al<span class="ltx_text" id="bib.bib192.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu, David D Cox, and Akash Srivastava. 2024.

</span>
<span class="ltx_bibblock">Lab: Large-scale alignment for chatbots.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib192.3.1">arXiv preprint arXiv:2403.01081</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib193.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Shichao Sun, Junlong Li, Weizhe Yuan, Ruifeng Yuan, Wenjie Li, and Pengfei Liu. 2024a.

</span>
<span class="ltx_bibblock">The Critique of Critique.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2401.04518 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.04518" title="">https://arxiv.org/abs/2401.04518</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib194.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Daniel Cox, Yiming Yang, and Chuang Gan. 2024b.

</span>
<span class="ltx_bibblock">SALMON: Self-Alignment with Instructable Reward Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib194.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Takahashi et al<span class="ltx_text" id="bib.bib195.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Ryo Takahashi, Takashi Matsubara, and Kuniaki Uehara. 2019.

</span>
<span class="ltx_bibblock">Data augmentation using random image cropping and patching for deep CNNs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib195.3.1">IEEE Transactions on Circuits and Systems for Video Technology</em> 30, 9 (2019), 2917–2931.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al<span class="ltx_text" id="bib.bib196.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. 2023.

</span>
<span class="ltx_bibblock">Does synthetic data generation of llms help clinical text mining?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib196.3.1">arXiv preprint arXiv:2303.04360</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et al<span class="ltx_text" id="bib.bib197.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" title="">https://github.com/tatsu-lab/stanford_alpaca</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taylor et al<span class="ltx_text" id="bib.bib198.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022.

</span>
<span class="ltx_bibblock">Galactica: A Large Language Model for Science.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib198.3.1">arXiv</em> abs/2211.09085 (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Törnberg (2023)</span>
<span class="ltx_bibblock">
Petter Törnberg. 2023.

</span>
<span class="ltx_bibblock">Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib199.1.1">arXiv preprint arXiv:2304.06588</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span class="ltx_text" id="bib.bib200.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al<span class="ltx_text" id="bib.bib200.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib200.4.1">arXiv preprint arXiv:2302.13971</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsai et al<span class="ltx_text" id="bib.bib201.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yun-Da Tsai, Mingjie Liu, and Haoxing Ren. 2024.

</span>
<span class="ltx_bibblock">Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data Pruning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib201.3.1">arXiv preprint arXiv:2407.05040</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tulving (1985)</span>
<span class="ltx_bibblock">
Endel Tulving. 1985.

</span>
<span class="ltx_bibblock">Memory and consciousness.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib202.1.1">Canadian Psychology/Psychologie canadienne</em> 26, 1 (1985), 1.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Völske et al<span class="ltx_text" id="bib.bib203.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Michael Völske, Martin Potthast, Shahbaz Syed, and Benno Stein. 2017.

</span>
<span class="ltx_bibblock">Tl; dr: Mining reddit to learn automatic summarization. In <em class="ltx_emph ltx_font_italic" id="bib.bib203.3.1">Proceedings of the Workshop on New Frontiers in Summarization</em>. 59–63.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang (533Z)</span>
<span class="ltx_bibblock">
Ben Wang. 2024-09-01T09:35:33Z.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib204.1.1">Kingoflolz/Mesh-Transformer-Jax</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/kingoflolz/mesh-transformer-jax" title="">https://github.com/kingoflolz/mesh-transformer-jax</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib205.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. 2023c.

</span>
<span class="ltx_bibblock">Huatuo: Tuning llama model with chinese medical knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib205.3.1">arXiv preprint arXiv:2304.06975</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib206.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Lei Wang, Yi Hu, Jiabang He, Xing Xu, Ning Liu, Hui Liu, and Heng Tao Shen. 2024d.

</span>
<span class="ltx_bibblock">T-sciq: Teaching multimodal chain-of-thought reasoning via large language model signals for science question answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib206.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 38. 19162–19170.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib207.2.2.1">.</span> (2024e)</span>
<span class="ltx_bibblock">
Rui Wang, Fei Mi, Yi Chen, Boyang Xue, Hongru Wang, Qi Zhu, Kam-Fai Wong, and Ruifeng Xu. 2024e.

</span>
<span class="ltx_bibblock">Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib207.3.1">arXiv preprint arXiv:2403.02756</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib208.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al<span class="ltx_text" id="bib.bib208.3.1">.</span> 2024a.

</span>
<span class="ltx_bibblock">Visionllm: Large language model is also an open-ended decoder for vision-centric tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib208.4.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib209.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023d.

</span>
<span class="ltx_bibblock">Self-Consistency Improves Chain of Thought Reasoning in Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib209.3.1">The Eleventh International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib210.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Yuxin Wang, Duanyu Feng, Yongfu Dai, Zhengyu Chen, Jimin Huang, Sophia Ananiadou, Qianqian Xie, and Hao Wang. 2024c.

</span>
<span class="ltx_bibblock">HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib210.3.1">arXiv preprint arXiv:2408.02927</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib211.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b.

</span>
<span class="ltx_bibblock">Self-Instruct: Aligning Language Models with Self-Generated Instructions. In <em class="ltx_emph ltx_font_italic" id="bib.bib211.3.1">The 61st Annual Meeting Of The Association For Computational Linguistics</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib212.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. 2024b.

</span>
<span class="ltx_bibblock">HelpSteer2: Open-source dataset for training top-performing reward models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib212.3.1">arXiv preprint arXiv:2406.08673</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib213.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, et al<span class="ltx_text" id="bib.bib213.3.1">.</span> 2023a.

</span>
<span class="ltx_bibblock">Helpsteer: Multi-attribute helpfulness dataset for steerlm.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib213.4.1">arXiv preprint arXiv:2311.09528</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib214.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, et al<span class="ltx_text" id="bib.bib214.3.1">.</span> 2023a.

</span>
<span class="ltx_bibblock">Symbol tuning improves in-context learning in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib214.4.1">arXiv preprint arXiv:2305.08298</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib215.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V Le. 2023b.

</span>
<span class="ltx_bibblock">Simple synthetic data reduces sycophancy in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib215.3.1">arXiv preprint arXiv:2308.03958</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib216.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and LINGMING ZHANG. 2024.

</span>
<span class="ltx_bibblock">Magicoder: Empowering Code Generation with OSS-Instruct. In <em class="ltx_emph ltx_font_italic" id="bib.bib216.3.1">Forty-first International Conference on Machine Learning</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib217.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Fangzhou Wu, Ning Zhang, Somesh Jha, Patrick McDaniel, and Chaowei Xiao. 2024b.

</span>
<span class="ltx_bibblock">A new era in llm security: Exploring security concerns in real-world llm-based systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib217.3.1">arXiv preprint arXiv:2402.18649</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib218.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Junda Wu, Cheng-Chun Chang, Tong Yu, Zhankui He, Jianing Wang, Yupeng Hou, and Julian McAuley. 2024a.

</span>
<span class="ltx_bibblock">CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib218.3.1">Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>. 3391–3401.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib219.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. 2023b.

</span>
<span class="ltx_bibblock">Lamini-lm: A diverse herd of distilled models from large-scale instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib219.3.1">arXiv preprint arXiv:2304.14402</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib220.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2023a.

</span>
<span class="ltx_bibblock">Next-gpt: Any-to-any multimodal llm.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib220.3.1">arXiv preprint arXiv:2309.05519</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xin et al<span class="ltx_text" id="bib.bib221.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. 2024.

</span>
<span class="ltx_bibblock">DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib221.3.1">arXiv</em> abs/2405.14333 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et al<span class="ltx_text" id="bib.bib222.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. 2023.

</span>
<span class="ltx_bibblock">Examining inter-consistency of large language models collaboration: An in-depth analysis via debate.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib222.3.1">arXiv preprint arXiv:2305.11595</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib223.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023a.

</span>
<span class="ltx_bibblock">Baize: An open-source chat model with parameter-efficient tuning on self-chat data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib223.3.1">arXiv preprint arXiv:2304.01196</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib224.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023b.

</span>
<span class="ltx_bibblock">Wizardlm: Empowering large language models to follow complex instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib224.3.1">arXiv preprint arXiv:2304.12244</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib225.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. 2024c.

</span>
<span class="ltx_bibblock">WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions. In <em class="ltx_emph ltx_font_italic" id="bib.bib225.3.1">International Conference on Learning Representations (ICLR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib226.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. 2021.

</span>
<span class="ltx_bibblock">Bot-adversarial dialogue for safe conversational agents. In <em class="ltx_emph ltx_font_italic" id="bib.bib226.3.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>. 2950–2968.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib227">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib227.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Ran Xu, Hejie Cui, Yue Yu, Xuan Kan, Wenqi Shi, Yuchen Zhuang, May Dongmei Wang, Wei Jin, Joyce Ho, and Carl Yang. 2024a.

</span>
<span class="ltx_bibblock">Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib227.3.1">Conference on Semantics in Text Processing (STEP)</em>. 15496–15523.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib228">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib228.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. 2024b.

</span>
<span class="ltx_bibblock">Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib228.3.1">arXiv preprint arXiv:2406.08464</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib229">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al<span class="ltx_text" id="bib.bib229.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Biwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue Zhang, Zhaochun Ren, and Xiuzheng Cheng. 2024.

</span>
<span class="ltx_bibblock">On protecting the data privacy of large language models (llms): A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib229.3.1">arXiv preprint arXiv:2403.05156</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib230">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib230.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. 2023b.

</span>
<span class="ltx_bibblock">Fingpt: Open-source financial large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib230.3.1">arXiv preprint arXiv:2306.06031</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib231">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib231.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Haoran Yang, Hongyuan Lu, Wai Lam, and Deng Cai. 2024b.

</span>
<span class="ltx_bibblock">Exploring Compositional Generalization of Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib231.3.1">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 4: Student Research Workshop)</em>. 16–24.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib232">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib232.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Haoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, Pheng Ann Heng, and Wai Lam. 2024c.

</span>
<span class="ltx_bibblock">Unveiling the generalization power of fine-tuned large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib232.3.1">arXiv preprint arXiv:2403.09162</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib233">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib233.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Jiaxi Yang, Binyuan Hui, Min Yang, Jian Yang, Junyang Lin, and Chang Zhou. 2024a.

</span>
<span class="ltx_bibblock">Synthesizing text-to-sql data from weak and strong llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib233.3.1">arXiv preprint arXiv:2408.03256</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib234">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib234.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Suorong Yang, Weikang Xiao, Mengchen Zhang, Suhan Guo, Jian Zhao, and Furao Shen. 2022.

</span>
<span class="ltx_bibblock">Image data augmentation for deep learning: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib234.3.1">arXiv preprint arXiv:2204.08610</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib235">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib235.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. 2023a.

</span>
<span class="ltx_bibblock">Bigtranslate: Augmenting large language models with multilingual translation capability over 100 languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib235.3.1">arXiv preprint arXiv:2305.18098</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib236">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye and Ng (2024)</span>
<span class="ltx_bibblock">
Hai Ye and Hwee Tou Ng. 2024.

</span>
<span class="ltx_bibblock">Self-Judge: Selective Instruction Following with Alignment Self-Evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib236.1.1">arXiv preprint arXiv:2409.00935</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib237">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yehudai et al<span class="ltx_text" id="bib.bib237.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv, Nathaniel Mills, Assaf Toledo, Eyal Shnarch, and Leshem Choshen. 2024.

</span>
<span class="ltx_bibblock">Genie: Achieving human parity in content-grounded datasets generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib237.3.1">arXiv preprint arXiv:2401.14367</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib238">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoo et al<span class="ltx_text" id="bib.bib238.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and Woomyeong Park. 2021.

</span>
<span class="ltx_bibblock">Gpt3mix: Leveraging large-scale language models for text augmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib238.3.1">arXiv preprint arXiv:2104.08826</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib239">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib239.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2024.

</span>
<span class="ltx_bibblock">MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib239.3.1">International Conference on Learning Representations (ICLR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib240">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib240.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al<span class="ltx_text" id="bib.bib240.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Florence: A new foundation model for computer vision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib240.4.1">arXiv preprint arXiv:2111.11432</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib241">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib241.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, et al<span class="ltx_text" id="bib.bib241.3.1">.</span> 2024a.

</span>
<span class="ltx_bibblock">Advancing llm reasoning generalists with preference trees.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib241.4.1">arXiv preprint arXiv:2404.02078</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib242">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib242.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024b.

</span>
<span class="ltx_bibblock">Self-rewarding language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib242.3.1">arXiv preprint arXiv:2401.10020</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib243">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib243.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 2023.

</span>
<span class="ltx_bibblock">Scaling relationship on learning mathematical reasoning with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib243.3.1">arXiv preprint arXiv:2308.01825</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib244">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al<span class="ltx_text" id="bib.bib244.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Xuanjing Huang, and Zhongyu Wei. 2023a.

</span>
<span class="ltx_bibblock">DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib244.3.1">arXiv</em> abs/2309.11325 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib245">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al<span class="ltx_text" id="bib.bib245.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023b.

</span>
<span class="ltx_bibblock">Mammoth: Building math generalist models through hybrid instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib245.3.1">arXiv preprint arXiv:2309.05653</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib246">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al<span class="ltx_text" id="bib.bib246.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2024.

</span>
<span class="ltx_bibblock">MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning. In <em class="ltx_emph ltx_font_italic" id="bib.bib246.3.1">International Conference on Learning Representations (ICLR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib247">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue Yu (2024)</span>
<span class="ltx_bibblock">
Zihan Liu Boxin Wang Jiaxuan You Chao Zhang MohammadShoeybi Bryan Catanzaro Yue Yu, Wei Ping. 2024.

</span>
<span class="ltx_bibblock">RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib247.1.1">arXiv preprint arXiv:2407.02485</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib248">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zelikman et al<span class="ltx_text" id="bib.bib248.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman. 2024.

</span>
<span class="ltx_bibblock">Quiet-star: Language models can teach themselves to think before speaking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib248.3.1">arXiv preprint arXiv:2403.09629</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib249">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zelikman et al<span class="ltx_text" id="bib.bib249.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022.

</span>
<span class="ltx_bibblock">STaR: Bootstrapping Reasoning With Reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib249.3.1">Advances in Neural Information Processing Systems</em> 35 (2022), 15476–15488.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib250">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al<span class="ltx_text" id="bib.bib250.2.2.1">.</span> (0 11)</span>
<span class="ltx_bibblock">
Guangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang, Sicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi Zeng, Xiangyu Dong, Ruoyu Zhang, Hongchao Fang, Penghui Zhu, Shu Chen, and Pengtao Xie. 2020-11.

</span>
<span class="ltx_bibblock">MedDialog: Large-scale Medical Dialogue Datasets. In <em class="ltx_emph ltx_font_italic" id="bib.bib250.3.1">EMNLP 2020</em> (Online), Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 9241–9250.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.emnlp-main.743" title="">https://aclanthology.org/2020.emnlp-main.743</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib251">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al<span class="ltx_text" id="bib.bib251.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Weihao Zeng, Can Xu, Yingxiu Zhao, Jian-Guang Lou, and Weizhu Chen. 2024.

</span>
<span class="ltx_bibblock">Automatic Instruction Evolving for Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib251.3.1">arXiv preprint arXiv:2406.00770</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib252">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhan et al<span class="ltx_text" id="bib.bib252.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et al<span class="ltx_text" id="bib.bib252.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Anygpt: Unified multimodal llm with discrete sequence modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib252.4.1">arXiv preprint arXiv:2402.12226</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib253">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib253.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Biao Zhang, Barry Haddow, and Alexandra Birch. 2023d.

</span>
<span class="ltx_bibblock">Prompting large language model for machine translation: A case study. In <em class="ltx_emph ltx_font_italic" id="bib.bib253.3.1">International Conference on Machine Learning</em>. PMLR, 41092–41110.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib254">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib254.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024d.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib254.3.1">SciGLM: Training Scientific Language Models with Self-Reflective Instruction Annotation and Tuning</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.07950" title="">https://arxiv.org/abs/2401.07950</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib255">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib255.2.2.1">.</span> (2024f)</span>
<span class="ltx_bibblock">
Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, Shufei Zhang, Mao Su, Hansen Zhong, Yuqiang Li, and Wanli Ouyang. 2024f.

</span>
<span class="ltx_bibblock">ChemLLM: A Chemical Large Language Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib255.3.1">arXiv</em> abs/2402.06852 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib256">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib256.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. 2023c.

</span>
<span class="ltx_bibblock">Repocoder: Repository-level code completion through iterative retrieval and generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib256.3.1">arXiv preprint arXiv:2303.12570</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib257">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib257.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Guiming Chen, Jianquan Li, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, and Haizhou Li. 2023a.

</span>
<span class="ltx_bibblock">HuatuoGPT, Towards Taming Language Model to Be a Doctor. In <em class="ltx_emph ltx_font_italic" id="bib.bib257.3.1">Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>. 10859–10885.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib258">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib258.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Jie Zhang, Haoyu Bu, Hui Wen, Yu Chen, Lun Li, and Hongsong Zhu. 2024a.

</span>
<span class="ltx_bibblock">When llms meet cybersecurity: A systematic literature review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib258.3.1">arXiv preprint arXiv:2405.03644</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib259">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib259.2.2.1">.</span> (2024g)</span>
<span class="ltx_bibblock">
Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, et al<span class="ltx_text" id="bib.bib259.3.1">.</span> 2024g.

</span>
<span class="ltx_bibblock">Ultramedical: Building specialized generalists in biomedicine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib259.4.1">arXiv preprint arXiv:2406.03949</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib260">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib260.2.2.1">.</span> (2023e)</span>
<span class="ltx_bibblock">
Kechi Zhang, Huangzhao Zhang, Ge Li, Jia Li, Zhuo Li, and Zhi Jin. 2023e.

</span>
<span class="ltx_bibblock">Toolcoder: Teach code generation models to use api search tools.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib260.3.1">arXiv preprint arXiv:2305.04032</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib261">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib261.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B Tenenbaum, and Chuang Gan. 2023b.

</span>
<span class="ltx_bibblock">Planning with large language models for code generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib261.3.1">arXiv preprint arXiv:2303.05510</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib262">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib262.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Sheng Zhang, Xin Zhang, Hui Wang, Lixiang Guo, and Shanshan Liu. 2018.

</span>
<span class="ltx_bibblock">Multi-Scale Attentive Interaction Networks for Chinese Medical Question Answer Selection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib262.3.1">IEEE Access</em> 6 (2018), 74061–74071.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ieeexplore.ieee.org/document/8548603/" title="">https://ieeexplore.ieee.org/document/8548603/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib263">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib263.2.2.1">.</span> (2024e)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto. 2024e.

</span>
<span class="ltx_bibblock">Benchmarking large language models for news summarization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib263.3.1">Transactions of the Association for Computational Linguistics</em> 12 (2024), 39–57.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib264">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib264.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Wenqi Zhang, Zhenglin Cheng, Yuanyu He, Mengna Wang, Yongliang Shen, Zeqi Tan, Guiyang Hou, Mingqian He, Yanna Ma, Weiming Lu, et al<span class="ltx_text" id="bib.bib264.3.1">.</span> 2024b.

</span>
<span class="ltx_bibblock">Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib264.4.1">arXiv preprint arXiv:2407.07053</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib265">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib265.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Pan, and Lidong Bing. 2024c.

</span>
<span class="ltx_bibblock">Sentiment Analysis in the Era of Large Language Models: A Reality Check. In <em class="ltx_emph ltx_font_italic" id="bib.bib265.3.1">Findings of the Association for Computational Linguistics: NAACL 2024</em>. 3881–3906.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib266">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib266.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. 2024b.

</span>
<span class="ltx_bibblock">Expel: Llm agents are experiential learners. In <em class="ltx_emph ltx_font_italic" id="bib.bib266.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 38. 19632–19642.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib267">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib267.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Henry Hengyuan Zhao, Pan Zhou, and Mike Zheng Shou. 2023a.

</span>
<span class="ltx_bibblock">Genixer: Empowering Multimodal Large Language Models as a Powerful Data Generator.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib267.3.1">arXiv preprint arXiv:2312.06731</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib268">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib268.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Qihao Zhao, Yalun Dai, Hao Li, Wei Hu, Fan Zhang, and Jun Liu. 2024a.

</span>
<span class="ltx_bibblock">LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content. In <em class="ltx_emph ltx_font_italic" id="bib.bib268.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 19510–19520.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib269">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib269.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al<span class="ltx_text" id="bib.bib269.3.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">A survey of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib269.4.1">arXiv preprint arXiv:2303.18223</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib270">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib270.2.2.1">.</span> ([n. d.])</span>
<span class="ltx_bibblock">
Z Zheng, K Ning, J Chen, Y Wang, W Chen, L Guo, and W Wang. [n. d.].

</span>
<span class="ltx_bibblock">Towards an understanding of large language models in software engineering tasks (2023).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib270.3.1">arXiv preprint arXiv:2308.11396</em> ([n. d.]).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib271">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">zhihaiLLM (847Z)</span>
<span class="ltx_bibblock">
zhihaiLLM. 2024-07-31T18:18:47Z.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib271.1.1">zhihaiLLM/wisdomInterrogatory</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/zhihaiLLM/wisdomInterrogatory" title="">https://github.com/zhihaiLLM/wisdomInterrogatory</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib272">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib272.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Yue Zhou, Chenlu Guo, Xu Wang, Yi Chang, and Yuan Wu. 2024a.

</span>
<span class="ltx_bibblock">A survey on data augmentation in large model era.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib272.3.1">arXiv preprint arXiv:2401.15422</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib273">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib273.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Zhi Zhou, Jiang-Xin Shi, Peng-Xiao Song, Xiaowen Yang, Yi-Xuan Jin, Lan-Zhe Guo, and Yu-Feng Li. 2024b.

</span>
<span class="ltx_bibblock">LawGPT: A Chinese Legal Knowledge-Enhanced Large Language Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib273.3.1">arXiv</em> abs/2406.04614 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib274">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib274.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. 2023a.

</span>
<span class="ltx_bibblock">Starling-7b: Improving llm helpfulness &amp; harmlessness with rlaif.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib275">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib275.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2024.

</span>
<span class="ltx_bibblock">Llms for knowledge graph construction and reasoning: Recent capabilities and future opportunities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib275.3.1">World Wide Web</em> 27, 5 (2024), 58.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib276">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib276.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yiming Zhu, Peixian Zhang, Ehsan-Ul Haq, Pan Hui, and Gareth Tyson. 2023b.

</span>
<span class="ltx_bibblock">Can chatgpt reproduce human-generated labels? a study of social computing tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib276.3.1">arXiv preprint arXiv:2304.10145</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib277">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu and Li (2023)</span>
<span class="ltx_bibblock">
Zeyuan Allen Zhu and Yuanzhi Li. 2023.

</span>
<span class="ltx_bibblock">Physics of language models: Part 3.1, knowledge storage and extraction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib277.1.1">arXiv preprint arXiv:2309.14316</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Oct 16 16:10:33 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
