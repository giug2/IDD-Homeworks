<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2110.01107] TinyFedTL: Federated Transfer Learning on Tiny Devices</title><meta property="og:description" content="TinyML has rose to popularity in an era where data is everywhere. However, the data that is in most demand is subject to strict privacy and security guarantees. In addition, the deployment of TinyML hardware in the reaâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TinyFedTL: Federated Transfer Learning on Tiny Devices">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="TinyFedTL: Federated Transfer Learning on Tiny Devices">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2110.01107">

<!--Generated on Tue Mar 19 14:44:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">thanks: </span>Authors contributed equally to this research</span></span></span>
<h1 class="ltx_title ltx_title_document">TinyFedTL: Federated Transfer Learning on Tiny Devices
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Kavya Kopparapu * 
<br class="ltx_break">Department of Computer Science
<br class="ltx_break">Harvard University
<br class="ltx_break">Cambridge, MA, USA 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">kavyakopparapu@college.harvard.edu</span> 
<br class="ltx_break">&amp;Eric Lin * 
<br class="ltx_break">Department of Computer Science
<br class="ltx_break">Harvard University
<br class="ltx_break">Cambridge, MA, USA 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">eric_lin@college.harvard.edu</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">TinyML has rose to popularity in an era where data is everywhere. However, the data that is in most demand is subject to strict privacy and security guarantees. In addition, the deployment of TinyML hardware in the real world has significant memory and communication constraints that traditional ML fails to address. In light of these challenges, we present TinyFedTL, the first implementation of federated transfer learning on a resource-constrained microcontroller.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.5" class="ltx_p"><em id="p1.5.1" class="ltx_emph ltx_font_bold ltx_font_italic">Keywords</em>â€‚TinyML Â <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">â‹…</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">â‹…</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
Federated Learning Â <math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">â‹…</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">â‹…</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
Microcontrollers Â <math id="p1.3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.3.m3.1a"><mo id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml">â‹…</mo><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><ci id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1">â‹…</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">\cdot</annotation></semantics></math>
Transfer Learning Â <math id="p1.4.m4.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.4.m4.1a"><mo id="p1.4.m4.1.1" xref="p1.4.m4.1.1.cmml">â‹…</mo><annotation-xml encoding="MathML-Content" id="p1.4.m4.1b"><ci id="p1.4.m4.1.1.cmml" xref="p1.4.m4.1.1">â‹…</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.4.m4.1c">\cdot</annotation></semantics></math>
Privacy Â <math id="p1.5.m5.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.5.m5.1a"><mo id="p1.5.m5.1.1" xref="p1.5.m5.1.1.cmml">â‹…</mo><annotation-xml encoding="MathML-Content" id="p1.5.m5.1b"><ci id="p1.5.m5.1.1.cmml" xref="p1.5.m5.1.1">â‹…</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.5.m5.1c">\cdot</annotation></semantics></math>
Machine Learning</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Our presentation is available at <a target="_blank" href="https://harvard.zoom.us/rec/share/xolCUYS-w-MmXAfRVWHl71b8HcpewsKCL7hqSAVlEy4G9kv-iC4Xk06acVW3oFQ5.XGkN1q3kZp51U_Uv?startTime=1608180080000" title="" class="ltx_ref ltx_href">this link</a>.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">In recent years, the emphasis on data privacy has grown in the wake of several privacy scandals and information leaks. More than ever, individuals are concerned with who has access to their personal data and where it is being shared. Most current successful machine learning methods, for both the Tiny domain and not, benefit from large and diverse datasets, so this rise in concern regarding digital privacy appears to come at the cost of progress.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In addition, in many applications there is a significant need for distributed learning agents that operate in an environment with significant communication costs and minimal on-device storage. These distributed computing setups have even taken hold in consumer-facing applications such as the Amazon Go store. Tiny Machine Learning is a rapidly growing field at the intersection of embedded systems and machine learning, allowing significant insights, data collection, and algorithmic development that was not previously possible.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Put together, the opportunity is twofold: data from several sources can no longer be consolidated for a singular learner to access due to privacy concerns and low-cost learning devices on the edge need a new method to aggregate shared insights that doesnâ€™t require large on-device memory and constant communication to a central server. Thus, we see the field of TinyML is ripe for applications of privacy-preserving machine learning. However, frameworks like Tensorflow Lite do not currently support model training on-device, but rather enable the deployment of static models for inference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">In this work, we contribute:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">The first implementation of federated learning on a microcontroller.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A method of deploying transfer learning on a resource-constrained microconroller without growing storage costs as the number of training examples increases.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">The method of federated transfer learning for the CIFAR-10 benchmark doesnâ€™t necessitate a dense layer with pre-trained weights, meaning devices can be re-trained to different types of classification problems.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Implemmentation of federated transfer learning on an Arduino microcontroller.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i5.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i5.p1.1" class="ltx_p">Identification of challenges and limitations encountered in the process of training on-device with a federated learning framework.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>TinyML</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">Previous work in TinyML is focused on optimizations to compress models and reduce the inference latency but does not aim to allow continuous learning, especially in a privacy-preserving manner. Prior research in deploying learning on-device in the TinyML domain has included simple NN classifiers like k-Nearest Neighbors to transfer learn on-device <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. However, such a classifier scales poorly with more training examples and does not follow federated learning privacy guarantees, as information about the input to a network can be generated from the embedding generated without the final dense layer. Therefore, such an implementation is essentially infeasible on a hardware platform such as the Arduino Nano.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Federated Learning</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">The domain of Federated Learning has been a hot research topic in recent years. The topic first emerged in late 2015, with the seminal paper by McMahan et al. of Google AI detailing their novel approach and proposing a concrete â€œFederated averagingâ€ algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The introduction of this algorithm presented a solution to several bottlenecks concerning mobile devices and privacy constraints. By distributing the data across edge devices, user privacy can be respected while learning models can be conducted on the aggregate updates on the collection of devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. There has been continued work in addressing problems in Federated Learning, including non-iid data distribution, attacks, and communication cost <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>FL for TinyML</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">Due to the fact that both fields are relatively new, there has been little work conducted in the intersection of tinyML and Federated Learning. There has been some work in the implementation of Federated Learning in mobile edge network and in selectively updating parts of large networks to make transfer learning more feasible but none have integrated into a significantly constrained device, but instead onto a Raspberry Pi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods and Experiments</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Transfer Learning Task</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">In order to turn a CNN into a feature extractor, we removed the final fully connected (dense) layer, so the output of the CNN was a feature vector <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">n</annotation></semantics></math> units long. We took the weights for this feature vector off-the-shelf, meaning we did not train the model on any specific task but rather kept the weights from a large dataset that were used to generate meaningful features (in this case, the dataset was either ImageNet or Visual Wake Words) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. We then used the features extracted by the CNN extractor to be the input into a fully connected layer, whose outputs were then put through a softmax to get class probabilities. Training the weights of this FC on-device later but not the feature extractor (which is set aside as constant) is the task.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.4" class="ltx_p">A typical benchmark for Federated Learning is the CIFAR-10 dataset, which consists of <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><cn type="integer" id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">32</annotation></semantics></math>x<math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mn id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><cn type="integer" id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">32</annotation></semantics></math> images in 10 different classes. These images were upscaled to <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="96" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mn id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">96</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><cn type="integer" id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">96</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">96</annotation></semantics></math>x<math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="96" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mn id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">96</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><cn type="integer" id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">96</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">96</annotation></semantics></math> and turned into two binary classification problems: dog versus no-dog and cat versus no-cat. The dog classification problem was treated as the original problem (for optionally pre-training the the dense layer of the off-the-shelf model trained on ImageNet or Visual Wake Words) and the cat classification problem was treated as the transfer problem.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Hardware</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">The microcontroller of choice for these experiments was the Arduino Nano 33 BLE Sense. The BLE Sense is the hardware of choice for many TinyML applications due to its variety of sensors (including temperature, pressure, humidity, light, color, and more) and interface with the Arduino IDE.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">The Arduino federated learning implementation also used a 5MP Arducam to collect real-world images as local training data. A macbook simulated a global server by communicating with the Arduino through a serial port. Finally, unlike other approaches for on-device learning, no SD card or external storage was used in simulations. This meant that all training and data storage had to take place in the 1MB of flash memory and 256KB of SRAM, further constraining the memory capacity.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Model Transfer Learning On-Device</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">Due to the incompatibility of the current version of Tensorflow Lite Micro with on-device training and update techniques, we implemented our own fully connected (FC) layer inference and backpropogation update in C++.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Arduino Federated Learning Implementation</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The Federated Averaging algorithm, the gold standard for enabling federated learning across distributed devices, is shown in Algorithm <a href="#alg1" title="Algorithm 1 â€£ 3.4 Arduino Federated Learning Implementation â€£ 3 Methods and Experiments â€£ TinyFedTL: Federated Transfer Learning on Tiny Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. This algorithm is the basis for our TinyFedTL system.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> FedAvg Algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite></figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">Â Â <span id="alg1.l1.1" class="ltx_text ltx_font_bold">Input:</span> Devices <math id="alg1.l1.m1.3" class="ltx_Math" alttext="i=1,...,N" display="inline"><semantics id="alg1.l1.m1.3a"><mrow id="alg1.l1.m1.3.4" xref="alg1.l1.m1.3.4.cmml"><mi id="alg1.l1.m1.3.4.2" xref="alg1.l1.m1.3.4.2.cmml">i</mi><mo id="alg1.l1.m1.3.4.1" xref="alg1.l1.m1.3.4.1.cmml">=</mo><mrow id="alg1.l1.m1.3.4.3.2" xref="alg1.l1.m1.3.4.3.1.cmml"><mn id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">1</mn><mo id="alg1.l1.m1.3.4.3.2.1" xref="alg1.l1.m1.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="alg1.l1.m1.2.2" xref="alg1.l1.m1.2.2.cmml">â€¦</mi><mo id="alg1.l1.m1.3.4.3.2.2" xref="alg1.l1.m1.3.4.3.1.cmml">,</mo><mi id="alg1.l1.m1.3.3" xref="alg1.l1.m1.3.3.cmml">N</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.3b"><apply id="alg1.l1.m1.3.4.cmml" xref="alg1.l1.m1.3.4"><eq id="alg1.l1.m1.3.4.1.cmml" xref="alg1.l1.m1.3.4.1"></eq><ci id="alg1.l1.m1.3.4.2.cmml" xref="alg1.l1.m1.3.4.2">ğ‘–</ci><list id="alg1.l1.m1.3.4.3.1.cmml" xref="alg1.l1.m1.3.4.3.2"><cn type="integer" id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">1</cn><ci id="alg1.l1.m1.2.2.cmml" xref="alg1.l1.m1.2.2">â€¦</ci><ci id="alg1.l1.m1.3.3.cmml" xref="alg1.l1.m1.3.3">ğ‘</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.3c">i=1,...,N</annotation></semantics></math>

</div>
<div id="alg1.l2" class="ltx_listingline">Â Â <span id="alg1.l2.1" class="ltx_text ltx_font_bold">for</span>Â epoch <math id="alg1.l2.m1.4" class="ltx_Math" alttext="t=1,2,\dots,T" display="inline"><semantics id="alg1.l2.m1.4a"><mrow id="alg1.l2.m1.4.5" xref="alg1.l2.m1.4.5.cmml"><mi id="alg1.l2.m1.4.5.2" xref="alg1.l2.m1.4.5.2.cmml">t</mi><mo id="alg1.l2.m1.4.5.1" xref="alg1.l2.m1.4.5.1.cmml">=</mo><mrow id="alg1.l2.m1.4.5.3.2" xref="alg1.l2.m1.4.5.3.1.cmml"><mn id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml">1</mn><mo id="alg1.l2.m1.4.5.3.2.1" xref="alg1.l2.m1.4.5.3.1.cmml">,</mo><mn id="alg1.l2.m1.2.2" xref="alg1.l2.m1.2.2.cmml">2</mn><mo id="alg1.l2.m1.4.5.3.2.2" xref="alg1.l2.m1.4.5.3.1.cmml">,</mo><mi mathvariant="normal" id="alg1.l2.m1.3.3" xref="alg1.l2.m1.3.3.cmml">â€¦</mi><mo id="alg1.l2.m1.4.5.3.2.3" xref="alg1.l2.m1.4.5.3.1.cmml">,</mo><mi id="alg1.l2.m1.4.4" xref="alg1.l2.m1.4.4.cmml">T</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.4b"><apply id="alg1.l2.m1.4.5.cmml" xref="alg1.l2.m1.4.5"><eq id="alg1.l2.m1.4.5.1.cmml" xref="alg1.l2.m1.4.5.1"></eq><ci id="alg1.l2.m1.4.5.2.cmml" xref="alg1.l2.m1.4.5.2">ğ‘¡</ci><list id="alg1.l2.m1.4.5.3.1.cmml" xref="alg1.l2.m1.4.5.3.2"><cn type="integer" id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1">1</cn><cn type="integer" id="alg1.l2.m1.2.2.cmml" xref="alg1.l2.m1.2.2">2</cn><ci id="alg1.l2.m1.3.3.cmml" xref="alg1.l2.m1.3.3">â€¦</ci><ci id="alg1.l2.m1.4.4.cmml" xref="alg1.l2.m1.4.4">ğ‘‡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.4c">t=1,2,\dots,T</annotation></semantics></math>Â <span id="alg1.l2.2" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="alg1.l3" class="ltx_listingline">Â Â Â Â Â <span id="alg1.l3.1" class="ltx_text ltx_font_bold">for</span>Â <math id="alg1.l3.m1.1" class="ltx_Math" alttext="i\in\textit{N}" display="inline"><semantics id="alg1.l3.m1.1a"><mrow id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mi id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml">i</mi><mo id="alg1.l3.m1.1.1.1" xref="alg1.l3.m1.1.1.1.cmml">âˆˆ</mo><mtext class="ltx_mathvariant_italic" id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3a.cmml">N</mtext></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><in id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1.1"></in><ci id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">ğ‘–</ci><ci id="alg1.l3.m1.1.1.3a.cmml" xref="alg1.l3.m1.1.1.3"><mtext class="ltx_mathvariant_italic" id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3">N</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">i\in\textit{N}</annotation></semantics></math>Â <span id="alg1.l3.2" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="alg1.l4" class="ltx_listingline">Â Â Â Â Â Â Â Â Device <math id="alg1.l4.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="alg1.l4.m1.1a"><mi id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><ci id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">i</annotation></semantics></math> trains all models <math id="alg1.l4.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="alg1.l4.m2.1a"><mi id="alg1.l4.m2.1.1" xref="alg1.l4.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="alg1.l4.m2.1b"><ci id="alg1.l4.m2.1.1.cmml" xref="alg1.l4.m2.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m2.1c">m</annotation></semantics></math> on its local data for <math id="alg1.l4.m3.1" class="ltx_Math" alttext="E" display="inline"><semantics id="alg1.l4.m3.1a"><mi id="alg1.l4.m3.1.1" xref="alg1.l4.m3.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="alg1.l4.m3.1b"><ci id="alg1.l4.m3.1.1.cmml" xref="alg1.l4.m3.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m3.1c">E</annotation></semantics></math> local episodes

</div>
<div id="alg1.l5" class="ltx_listingline">Â Â Â Â Â <span id="alg1.l5.1" class="ltx_text ltx_font_bold">end</span>Â <span id="alg1.l5.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l6" class="ltx_listingline">Â Â Â Â Â <span id="alg1.l6.1" class="ltx_text ltx_font_bold">for</span>Â <math id="alg1.l6.m1.4" class="ltx_Math" alttext="m=1,2,\dots,M" display="inline"><semantics id="alg1.l6.m1.4a"><mrow id="alg1.l6.m1.4.5" xref="alg1.l6.m1.4.5.cmml"><mi id="alg1.l6.m1.4.5.2" xref="alg1.l6.m1.4.5.2.cmml">m</mi><mo id="alg1.l6.m1.4.5.1" xref="alg1.l6.m1.4.5.1.cmml">=</mo><mrow id="alg1.l6.m1.4.5.3.2" xref="alg1.l6.m1.4.5.3.1.cmml"><mn id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml">1</mn><mo id="alg1.l6.m1.4.5.3.2.1" xref="alg1.l6.m1.4.5.3.1.cmml">,</mo><mn id="alg1.l6.m1.2.2" xref="alg1.l6.m1.2.2.cmml">2</mn><mo id="alg1.l6.m1.4.5.3.2.2" xref="alg1.l6.m1.4.5.3.1.cmml">,</mo><mi mathvariant="normal" id="alg1.l6.m1.3.3" xref="alg1.l6.m1.3.3.cmml">â€¦</mi><mo id="alg1.l6.m1.4.5.3.2.3" xref="alg1.l6.m1.4.5.3.1.cmml">,</mo><mi id="alg1.l6.m1.4.4" xref="alg1.l6.m1.4.4.cmml">M</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.4b"><apply id="alg1.l6.m1.4.5.cmml" xref="alg1.l6.m1.4.5"><eq id="alg1.l6.m1.4.5.1.cmml" xref="alg1.l6.m1.4.5.1"></eq><ci id="alg1.l6.m1.4.5.2.cmml" xref="alg1.l6.m1.4.5.2">ğ‘š</ci><list id="alg1.l6.m1.4.5.3.1.cmml" xref="alg1.l6.m1.4.5.3.2"><cn type="integer" id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1">1</cn><cn type="integer" id="alg1.l6.m1.2.2.cmml" xref="alg1.l6.m1.2.2">2</cn><ci id="alg1.l6.m1.3.3.cmml" xref="alg1.l6.m1.3.3">â€¦</ci><ci id="alg1.l6.m1.4.4.cmml" xref="alg1.l6.m1.4.4">ğ‘€</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.4c">m=1,2,\dots,M</annotation></semantics></math>Â <span id="alg1.l6.2" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="alg1.l7" class="ltx_listingline">Â Â Â Â Â Â Â Â <math id="alg1.l7.m1.2" class="ltx_Math" alttext="\textit{w\_avg}=\text{AverageWeights}(i\text{ s.t. }c_{m}^{(i)}\neq 0)" display="inline"><semantics id="alg1.l7.m1.2a"><mrow id="alg1.l7.m1.2.2" xref="alg1.l7.m1.2.2.cmml"><mtext class="ltx_mathvariant_italic" id="alg1.l7.m1.2.2.3" xref="alg1.l7.m1.2.2.3a.cmml">w_avg</mtext><mo id="alg1.l7.m1.2.2.2" xref="alg1.l7.m1.2.2.2.cmml">=</mo><mrow id="alg1.l7.m1.2.2.1" xref="alg1.l7.m1.2.2.1.cmml"><mtext id="alg1.l7.m1.2.2.1.3" xref="alg1.l7.m1.2.2.1.3a.cmml">AverageWeights</mtext><mo lspace="0em" rspace="0em" id="alg1.l7.m1.2.2.1.2" xref="alg1.l7.m1.2.2.1.2.cmml">â€‹</mo><mrow id="alg1.l7.m1.2.2.1.1.1" xref="alg1.l7.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="alg1.l7.m1.2.2.1.1.1.2" xref="alg1.l7.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="alg1.l7.m1.2.2.1.1.1.1" xref="alg1.l7.m1.2.2.1.1.1.1.cmml"><mrow id="alg1.l7.m1.2.2.1.1.1.1.2" xref="alg1.l7.m1.2.2.1.1.1.1.2.cmml"><mi id="alg1.l7.m1.2.2.1.1.1.1.2.2" xref="alg1.l7.m1.2.2.1.1.1.1.2.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l7.m1.2.2.1.1.1.1.2.1" xref="alg1.l7.m1.2.2.1.1.1.1.2.1.cmml">â€‹</mo><mtext id="alg1.l7.m1.2.2.1.1.1.1.2.3" xref="alg1.l7.m1.2.2.1.1.1.1.2.3a.cmml">Â s.t.Â </mtext><mo lspace="0em" rspace="0em" id="alg1.l7.m1.2.2.1.1.1.1.2.1a" xref="alg1.l7.m1.2.2.1.1.1.1.2.1.cmml">â€‹</mo><msubsup id="alg1.l7.m1.2.2.1.1.1.1.2.4" xref="alg1.l7.m1.2.2.1.1.1.1.2.4.cmml"><mi id="alg1.l7.m1.2.2.1.1.1.1.2.4.2.2" xref="alg1.l7.m1.2.2.1.1.1.1.2.4.2.2.cmml">c</mi><mi id="alg1.l7.m1.2.2.1.1.1.1.2.4.2.3" xref="alg1.l7.m1.2.2.1.1.1.1.2.4.2.3.cmml">m</mi><mrow id="alg1.l7.m1.1.1.1.3" xref="alg1.l7.m1.2.2.1.1.1.1.2.4.cmml"><mo stretchy="false" id="alg1.l7.m1.1.1.1.3.1" xref="alg1.l7.m1.2.2.1.1.1.1.2.4.cmml">(</mo><mi id="alg1.l7.m1.1.1.1.1" xref="alg1.l7.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="alg1.l7.m1.1.1.1.3.2" xref="alg1.l7.m1.2.2.1.1.1.1.2.4.cmml">)</mo></mrow></msubsup></mrow><mo id="alg1.l7.m1.2.2.1.1.1.1.1" xref="alg1.l7.m1.2.2.1.1.1.1.1.cmml">â‰ </mo><mn id="alg1.l7.m1.2.2.1.1.1.1.3" xref="alg1.l7.m1.2.2.1.1.1.1.3.cmml">0</mn></mrow><mo stretchy="false" id="alg1.l7.m1.2.2.1.1.1.3" xref="alg1.l7.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.2b"><apply id="alg1.l7.m1.2.2.cmml" xref="alg1.l7.m1.2.2"><eq id="alg1.l7.m1.2.2.2.cmml" xref="alg1.l7.m1.2.2.2"></eq><ci id="alg1.l7.m1.2.2.3a.cmml" xref="alg1.l7.m1.2.2.3"><mtext class="ltx_mathvariant_italic" id="alg1.l7.m1.2.2.3.cmml" xref="alg1.l7.m1.2.2.3">w_avg</mtext></ci><apply id="alg1.l7.m1.2.2.1.cmml" xref="alg1.l7.m1.2.2.1"><times id="alg1.l7.m1.2.2.1.2.cmml" xref="alg1.l7.m1.2.2.1.2"></times><ci id="alg1.l7.m1.2.2.1.3a.cmml" xref="alg1.l7.m1.2.2.1.3"><mtext id="alg1.l7.m1.2.2.1.3.cmml" xref="alg1.l7.m1.2.2.1.3">AverageWeights</mtext></ci><apply id="alg1.l7.m1.2.2.1.1.1.1.cmml" xref="alg1.l7.m1.2.2.1.1.1"><neq id="alg1.l7.m1.2.2.1.1.1.1.1.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.1"></neq><apply id="alg1.l7.m1.2.2.1.1.1.1.2.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.2"><times id="alg1.l7.m1.2.2.1.1.1.1.2.1.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.2.1"></times><ci id="alg1.l7.m1.2.2.1.1.1.1.2.2.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.2.2">ğ‘–</ci><ci id="alg1.l7.m1.2.2.1.1.1.1.2.3a.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.2.3"><mtext id="alg1.l7.m1.2.2.1.1.1.1.2.3.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.2.3">Â s.t.Â </mtext></ci><apply id="alg1.l7.m1.2.2.1.1.1.1.2.4.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.2.4"><csymbol cd="ambiguous" id="alg1.l7.m1.2.2.1.1.1.1.2.4.1.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.2.4">superscript</csymbol><apply id="alg1.l7.m1.2.2.1.1.1.1.2.4.2.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.2.4"><csymbol cd="ambiguous" id="alg1.l7.m1.2.2.1.1.1.1.2.4.2.1.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.2.4">subscript</csymbol><ci id="alg1.l7.m1.2.2.1.1.1.1.2.4.2.2.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.2.4.2.2">ğ‘</ci><ci id="alg1.l7.m1.2.2.1.1.1.1.2.4.2.3.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.2.4.2.3">ğ‘š</ci></apply><ci id="alg1.l7.m1.1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1.1">ğ‘–</ci></apply></apply><cn type="integer" id="alg1.l7.m1.2.2.1.1.1.1.3.cmml" xref="alg1.l7.m1.2.2.1.1.1.1.3">0</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.2c">\textit{w\_avg}=\text{AverageWeights}(i\text{ s.t. }c_{m}^{(i)}\neq 0)</annotation></semantics></math>
</div>
<div id="alg1.l8" class="ltx_listingline">Â Â Â Â Â Â Â Â Learner updates model <math id="alg1.l8.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="alg1.l8.m1.1a"><mi id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b"><ci id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.1c">m</annotation></semantics></math> with <span id="alg1.l8.1" class="ltx_text ltx_font_italic">w_avg</span>
</div>
<div id="alg1.l9" class="ltx_listingline">Â Â Â Â Â <span id="alg1.l9.1" class="ltx_text ltx_font_bold">end</span>Â <span id="alg1.l9.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l10" class="ltx_listingline">Â Â Â Â Â Evaluate models with global validation data

</div>
<div id="alg1.l11" class="ltx_listingline">Â Â <span id="alg1.l11.1" class="ltx_text ltx_font_bold">end</span>Â <span id="alg1.l11.2" class="ltx_text ltx_font_bold">for</span>
</div>
</div>
</figure>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.p2.1" class="ltx_p">Federated Learning in the real-world likely only sees a given example or data point once. This is notably different than normal federated (or not) training schemes, in which training examples are reused over epochs of training. Furthermore, theoretical federated learning schemes push and send weight updates on a carefully scheduled and consistent basis. This is simply infeasible in many applications of tinyML edge devices where unstable network connection may prevent comunication for long stretches of time. Moreover, data input is not uniform â€“ changing environments may result in some days where a certain edge device sees many pieces of new data, while other devices see none.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.p3.1" class="ltx_p">Our setup mirrors real-world scenarios as described above. Data available to the model is different for every epoch of training and collected through the Arducam. Furthermore, the arduino continuously sees and trains on new data until it is contacted by the global server to update its weights. When the server does initiate contact, model weight and bias updates have to be sent and read byte by byte via the serial port between the arduino and the server. The batch size on our real-world implementation is 1 (instantaneous inference and model update).</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Models</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">We implemented two versions of MobileNetV2 for our task: the Tensorflow built-in MobileNet compressed with a factor of <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="alpha=0.35" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mrow id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2.2" xref="S4.SS1.p1.1.m1.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.2.1" xref="S4.SS1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS1.p1.1.m1.1.1.2.3" xref="S4.SS1.p1.1.m1.1.1.2.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.2.1a" xref="S4.SS1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS1.p1.1.m1.1.1.2.4" xref="S4.SS1.p1.1.m1.1.1.2.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.2.1b" xref="S4.SS1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS1.p1.1.m1.1.1.2.5" xref="S4.SS1.p1.1.m1.1.1.2.5.cmml">h</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.2.1c" xref="S4.SS1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS1.p1.1.m1.1.1.2.6" xref="S4.SS1.p1.1.m1.1.1.2.6.cmml">a</mi></mrow><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">0.35</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><eq id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></eq><apply id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2"><times id="S4.SS1.p1.1.m1.1.1.2.1.cmml" xref="S4.SS1.p1.1.m1.1.1.2.1"></times><ci id="S4.SS1.p1.1.m1.1.1.2.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2.2">ğ‘</ci><ci id="S4.SS1.p1.1.m1.1.1.2.3.cmml" xref="S4.SS1.p1.1.m1.1.1.2.3">ğ‘™</ci><ci id="S4.SS1.p1.1.m1.1.1.2.4.cmml" xref="S4.SS1.p1.1.m1.1.1.2.4">ğ‘</ci><ci id="S4.SS1.p1.1.m1.1.1.2.5.cmml" xref="S4.SS1.p1.1.m1.1.1.2.5">â„</ci><ci id="S4.SS1.p1.1.m1.1.1.2.6.cmml" xref="S4.SS1.p1.1.m1.1.1.2.6">ğ‘</ci></apply><cn type="float" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">0.35</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">alpha=0.35</annotation></semantics></math> (hereby referenced as tf-mobilenet) pretrained on ImageNet and the MobileNet from the TinyML Perf Benchmark (hereby referenced as perf-mobilenet) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. The compressed tf-mobilenet has more parameters than perf-mobilenet has x parameters. Both models were frozen and the input and outputs were quantized using post-training 8-bit quantization to interface correctly with the microcontroller.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Deployed Model Descriptions</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="4"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Model Parameters</span></td>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S4.T1.1.2.2.1.1" class="ltx_text ltx_font_bold">Name</span></th>
<td id="S4.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.2.1" class="ltx_text ltx_font_bold ltx_font_italic">Total</span></td>
<td id="S4.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.3.1" class="ltx_text ltx_font_bold ltx_font_italic">Trainable</span></td>
<td id="S4.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.4.1" class="ltx_text ltx_font_bold ltx_font_italic">Trainable</span></td>
<td id="S4.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.5.1" class="ltx_text ltx_font_bold ltx_font_italic">Trained on</span></td>
</tr>
<tr id="S4.T1.1.3.3" class="ltx_tr">
<th id="S4.T1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">tf-mobilenet</th>
<td id="S4.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">412,770</td>
<td id="S4.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2,562</td>
<td id="S4.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">410,208</td>
<td id="S4.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ImageNet</td>
</tr>
<tr id="S4.T1.1.4.4" class="ltx_tr">
<th id="S4.T1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">perf-mobilenet</th>
<td id="S4.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">221,794</td>
<td id="S4.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">514</td>
<td id="S4.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">221,280</td>
<td id="S4.T1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Visual Wake Words</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">We ran into issues with transfer learning on the perf-mobilenet model. Specifically, the embeddings generated from the network were incredibly sparse, where only 16 of the 256 outputs were ever non-zero. This may be as a result of encouraging sparsity during training via methods such as L2 weight regularization so the embeddings generated are highly specific to the task it was initially trained on (person detection through the visual wake words dataset). If given more time, we would have re-trained the perf-mobilenet to get more meaningful embeddings.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p">In our simulations, tf-mobilenet performed much better and successfully learned the task of cat identification. However, it was too large to fit on-device for our microcontroller. Since we were using an Arduino Nano 33 BLE Sense with 1MB of flash memory and 256KB of SRAM, we believe that other microcontrollers could have fit the tf-mobilenet model. Therefore, we used the perf-mobilenet for memory and time benchmarking on our arduino and the tf-mobilenet model in our simulations to understand performance.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Transfer Learning from Scratch versus Pretrained</h3>

<figure id="S4.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F1.1" class="ltx_p ltx_align_center ltx_figure_panel"><span id="S4.F1.1.1" class="ltx_text"><img src="/html/2110.01107/assets/x1.png" id="S4.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="257" height="181" alt="Refer to caption"></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F1.2" class="ltx_p ltx_align_center ltx_figure_panel"><span id="S4.F1.2.1" class="ltx_text"><img src="/html/2110.01107/assets/x2.png" id="S4.F1.2.1.g1" class="ltx_graphics ltx_img_landscape" width="257" height="181" alt="Refer to caption"></span></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Train and validation accuracy across 10 experiments comparing the training performance of the tf-mobilenet and perf-mobilenet models. The results represent the mean <math id="S4.F1.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.F1.4.m1.1b"><mo id="S4.F1.4.m1.1.1" xref="S4.F1.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.F1.4.m1.1c"><csymbol cd="latexml" id="S4.F1.4.m1.1.1.cmml" xref="S4.F1.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.F1.4.m1.1d">\pm</annotation></semantics></math> standard deviation across 10 experiments with 5 local episodes and a batch size of 20. The FL implementation has 2 devices. </figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">There are two methods of transfer learning under our setup: using a FC layer trained on a different task and changing the weights, or starting with a randomly-initialized FC layer. Ideally, the differences would be negligible or the randomly-initialized layer would perform better, since then a CNN-based feature extractor could be developed and deployed once and then the size of the dense layers (embedding size x class number) can be varied for various types of applications with different class sizes. As we can see in Figure <a href="#S4.F1" title="Figure 1 â€£ 4.2 Transfer Learning from Scratch versus Pretrained â€£ 4 Results â€£ TinyFedTL: Federated Transfer Learning on Tiny Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the differences between the randomly initialized model and the model pre-trained on the Dog classification problem have equivalent performance in the Cat classification problem. This is a great result, because it means that whenever we want to switch our feature extractor to a different task to transfer learn, we can just initialize a random FC layer, and the features that were lost in the replacement are not useful for transfer learning.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Federated Learning</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">Just to note on the following graphs, the measure training examples seen is across all the devices. Therefore, both devices in a federated context and devices not in federated context are trained and benchmarked against each other. In addition, these results were collected through simulation of the same C++ code that was deployed on the Arudino platform: so the results are identical to performance on-device without having to wait for training and transfer of weighs to occur.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p">Itâ€™s interesting to see that accross all our graphs, models did not benefit from additional data after <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="\sim 3000" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml"></mi><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">âˆ¼</mo><mn id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">3000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">3000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\sim 3000</annotation></semantics></math> training examples likely because they were stuck in local optima. This is likely due to the simplicity in the optimizer we used in training our model and presents significant future opportunity.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Number of Devices</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">Figure <a href="#S4.F2" title="Figure 2 â€£ 4.3.1 Number of Devices â€£ 4.3 Federated Learning â€£ 4 Results â€£ TinyFedTL: Federated Transfer Learning on Tiny Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the effectiveness of federated learning. As we can see, the validation accuracy drops as the number of devices increases, as with more distributed data model updates may cancel each other out or progress may be lost during the averaging step. This is as compared to the baseline of regular (non-FL) transfer learning, shown on the graph as device number of 1.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<p id="S4.F2.1" class="ltx_p ltx_align_center"><span id="S4.F2.1.1" class="ltx_text"><img src="/html/2110.01107/assets/x3.png" id="S4.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="257" height="181" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The validation accuracy across epochs of training across different number of devices participating in federated learning. The results represent the mean <math id="S4.F2.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.F2.3.m1.1b"><mo id="S4.F2.3.m1.1.1" xref="S4.F2.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.F2.3.m1.1c"><csymbol cd="latexml" id="S4.F2.3.m1.1.1.cmml" xref="S4.F2.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.3.m1.1d">\pm</annotation></semantics></math> standard deviation across 10 experiments with 5 local episodes and a batch size of 20.</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Batch Size</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">As we can see in Figure <a href="#S4.F3" title="Figure 3 â€£ 4.3.2 Batch Size â€£ 4.3 Federated Learning â€£ 4 Results â€£ TinyFedTL: Federated Transfer Learning on Tiny Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> that batch size has a significant impact on the variability in epoch-to-epoch performance of FL. This makes sense even in a non-FL scenario, as more data during an update better approximates the true gradients in stochastic gradient descent and prevents over-indexing on a certain piece of data. Ideally in a TinyML application, we would use a batch size of 1 since inference would be instantaneous rather than having to allocate storage and produce results once sufficient data has been collected.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<p id="S4.F3.1" class="ltx_p ltx_align_center"><span id="S4.F3.1.1" class="ltx_text"><img src="/html/2110.01107/assets/x4.png" id="S4.F3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="257" height="179" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The validation accuracy across epochs of training across different batch sizes. The results represent the mean <math id="S4.F3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.F3.3.m1.1b"><mo id="S4.F3.3.m1.1.1" xref="S4.F3.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.F3.3.m1.1c"><csymbol cd="latexml" id="S4.F3.3.m1.1.1.cmml" xref="S4.F3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.3.m1.1d">\pm</annotation></semantics></math> standard deviation across 10 experiments with 5 local episodes and 2 devices.</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Local Episodes</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">The number of local episodes is defined as the number of times the model was trained on a given batch of data. In Figure <a href="#S4.F4" title="Figure 4 â€£ 4.3.3 Local Episodes â€£ 4.3 Federated Learning â€£ 4 Results â€£ TinyFedTL: Federated Transfer Learning on Tiny Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, itâ€™s clear to see that the number of local episodes leads to smaller variability in the epoch-to-epoch validation accuracy. On the other hand, extra local episodes may lead to overfitting to the specific epoch and has greater associated computational costs. For this task, the increase in the number of local episodes from 5 to 6 doesnâ€™t present a significant increase in validation accuracy or a decrease in variability.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<p id="S4.F4.1" class="ltx_p ltx_align_center"><span id="S4.F4.1.1" class="ltx_text"><img src="/html/2110.01107/assets/x5.png" id="S4.F4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="230" height="163" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The validation accuracy across epochs of training across different number of local episodes. The results represent the mean <math id="S4.F4.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.F4.3.m1.1b"><mo id="S4.F4.3.m1.1.1" xref="S4.F4.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.F4.3.m1.1c"><csymbol cd="latexml" id="S4.F4.3.m1.1.1.cmml" xref="S4.F4.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.3.m1.1d">\pm</annotation></semantics></math> standard deviation across 20 experiments with a batch 20 and 2 devices.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Performance on-Device</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p">As stated above, the perf-mobilenet model was used to measure on-device memory and communication cost time. Images were collected through the 5MP arducam, converted to 96x96x3 RGB data, and trained on for 20 local epochs.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Memory</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">Deploying the model utilized 210 KB (80%) of dynamic memory and 657 KB (66%) of program storage space. This excludes further memory costs associated with communicating with the global server and encoding captured images.</p>
</div>
<div id="S4.SS4.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.SSS1.p2.1" class="ltx_p">An important advantage of federated learning is the ability for the device to continuously take in new input data then discard it after itâ€™s done training. Since data is not transferred from the edge device to a central server, the device only has to retain its model weights in the federated learning scheme. Therefore, memory footprint does not increase with the number of training samples per device. This is particularly useful in scenarios where external data storage (even SD cards) are not available. Compared to implementations in previous work with KNN and other similar models, we clearly see a vast improvement in memory footprint.</p>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Time</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">By picking a sparse model and writing implementation from scratch without use of external libraries, we were able to optimize our deployment runtime. Image capture and inference took between 8-10 seconds as a buffer from Arducam data was processed as input to the perf-mobilenet model. The actual training process was quite fast as it took only 214ms to go through 20 episodes of local epochs on-device. Then, weights and bias data had to be sent between the arduino and the global server in both directions. These were encoded as bytes, then sent through the serial port in packets of 32 bits. The 514 floats (256x2 weights and 2 bias) often took up on average <math id="S4.SS4.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S4.SS4.SSS2.p1.1.m1.1a"><mo id="S4.SS4.SSS2.p1.1.m1.1.1" xref="S4.SS4.SSS2.p1.1.m1.1.1.cmml">â‰¥</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS2.p1.1.m1.1b"><geq id="S4.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS2.p1.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS2.p1.1.m1.1c">\geq</annotation></semantics></math> 6000 bytes, and thus took over 30 seconds (one way) to upload and download.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Responsible AI and Privacy</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">Our work has shown the possibility of utilizing a federated learning schema to learn tasks in a tinyML domain without the need to share data with a central server. Since data never leaves the device, this opens up avenues for a multitude of applications that have been thus far hesitant to adopt machine learning techniques due to privacy concerns. For instance, in the healthcare context, patient wearables and sensors can continuously assess situations and learn without the risk of privacy infringement for the user. This opens up data in many new fields to be utilized to improve quality of life and user experience without sacrificing privacy.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p">Moreover, we show that privacy-centric on-device transfer learning is not only possible but also effective. In our results above, we see that tinyFedTL performed on par with our simulations with regular learning techniques. The federated averaging method effectively captures learnings from edge devices through only sharing of the weights without the need for any attributes of the data.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Future opportunities for On-Device Learning</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">There are a myriad of future directions this work can go. The first is to implement support for better weight decay and a better optimizer to solve the local optima issue discussed earlier. The second is more algorithmic: in order to further compress the amount of memory needed to run the FC layer training on-device, we could implement feature reduction methods between the embedding from the CNN and the FC layer trained on-device. This would significantly reduce the number of weights stored and trained, therefore also increasing latency and allowing the storage of embeddings to form batches.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p">Communication cost is also an area of improvement. Although a 1-minute round-trip communication time per update is not infeasible, this presents problems for edge devices in remote areas with unstable wifi. There, satellites may only have clear connection at short intervals, and packets may be dropped without notice. Thus, possible future work will be to modify our communication protocol for greater resiliency. Other work may take advantage of the sparsity of the model to develop a more efficient message encoding rather than directly translating all weight parameters into bytes. However, there are some costs and decreases in accuracy as the number of devices grow in the federated learning scheme, as shown above. This can be mitigated with capturing more data (our data was limited by the number of images per class in CIFAR-10) and also employing other federated learning techniques like hierarchical training. In those schemas, instead of only having one global server, several intermediary nodes are used to break up edge devices into different groups, meaning there are fewer devices per server node. This would decrease the likelihood of conflicting updates and help sustain accuracy.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">We have shown a successful first-attempt at deploying on-device federated learning. We have demonstrated the efficacy of tinyFedTL with one-shot examples such that storage costs donâ€™t increase as the number of training examples increase. We have also shown that pre-trained weights arenâ€™t necessary for transfer learning, meaning that edge devices can be re-trained for multiple different classification problems. Finally, our work has also identified several challenges and future avenues of research. The lack of precedent in this task means there is ample room for exploration in terms of designing better optimizers and hypertuning parameters.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Robert David, Jared Duke, Advait Jain, VijayÂ Janapa Reddi, Nat Jeffries, Jian
Li, Nick Kreeger, Ian Nappier, Meghna Natraj, Shlomi Regev, etÂ al.

</span>
<span class="ltx_bibblock">Tensorflow lite micro: Embedded machine learning on tinyml systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.08678</span>, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Simone Disabato and Manuel Roveri.

</span>
<span class="ltx_bibblock">Incremental on-device tiny machine learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2nd International Workshop on Challenges
in Artificial Intelligence and Machine Learning for Internet of Things</span>,
pages 7â€“13, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and BlaiseÂ Aguera
yÂ Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Artificial Intelligence and Statistics</span>, pages 1273â€“1282.
PMLR, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Peter Kairouz, HÂ Brendan McMahan, Brendan Avent, AurÃ©lien Bellet, Mehdi
Bennis, ArjunÂ Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, etÂ al.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.04977</span>, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
MariaÂ Florina Balcan, Avrim Blum, Shai Fine, and Yishay Mansour.

</span>
<span class="ltx_bibblock">Distributed learning, communication complexity and privacy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Conference on Learning Theory</span>, pages 26â€“1, 2012.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Neta Shoham, Tomer Avidor, Aviv Keren, Nadav Israel, Daniel Benditkis, Liron
Mor-Yosef, and Itai Zeitak.

</span>
<span class="ltx_bibblock">Overcoming forgetting in federated learning on non-iid data.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1910.07796</span>, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Kavya Kopparapu, Eric Lin, and Jessica Zhao.

</span>
<span class="ltx_bibblock">Fedcd: Improving performance in non-iid federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.09637</span>, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Wei YangÂ Bryan Lim, NguyenÂ Cong Luong, DinhÂ Thai Hoang, Yutao Jiao, Ying-Chang
Liang, Qiang Yang, Dusit Niyato, and Chunyan Miao.

</span>
<span class="ltx_bibblock">Federated learning in mobile edge networks: A comprehensive survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</span>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Han Cai, Chuang Gan, Ligeng Zhu, and Song Han.

</span>
<span class="ltx_bibblock">Tinytl: Reduce memory, not parameters for efficient on-device
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 33, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and LiÂ Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">2009 IEEE conference on computer vision and pattern
recognition</span>, pages 248â€“255. Ieee, 2009.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Pete Warden, Jonathon Shlens, Andrew Howard, and Rocky
Rhodes.

</span>
<span class="ltx_bibblock">Visual wake words dataset.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1906.05721</span>, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
ColbyÂ R Banbury, VijayÂ Janapa Reddi, Max Lam, William Fu, Amin Fazel, Jeremy
Holleman, Xinyuan Huang, Robert Hurtado, David Kanter, Anton Lokhmotov,
etÂ al.

</span>
<span class="ltx_bibblock">Benchmarking tinyml systems: Challenges and direction.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.04821</span>, 2020.

</span>
</li>
</ul>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Appendix A: High Level Description of Code</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">All our code, including model modifications, C++ NN and FL implementations, Arduino modifications, and more is available at <a target="_blank" href="https://github.com/kavyakvk/TinyFederatedLearning" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/kavyakvk/TinyFederatedLearning</a>. Below is information on our file structure, graph and result generation code, and the important files for training (marked with a ***).</p>
<pre id="Sx1.p1.2" class="ltx_verbatim ltx_font_typewriter">
dl
source
----arduino_training_final_v3
-------&gt;***the .ino file has the implementation
        of our FL code for the Arduino IDE to
        compile
-------&gt;***python_final_script.py acts as the
        "central server" for the arduino
----simulation
-------&gt;***NeuralNetwork.cpp has our FC
        implementation and the FL implementation
-------&gt;***simulation.cc is the file with the
        code necessary to run our simulations.
-------&gt;simulation-xxx the executable that can
        be run with ./ for each of our
        experiments
-------&gt; the .txt files are the output from
        terminal when running the experiments
-------&gt;fl_simulation_analysis generates the
        .csv from the .txt
-------&gt;graphing.ipynb has the information to
        graph our figures from the paper
        from the .csv files
tensorflow (no changes)
third_party (no changes)
</pre>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2110.01106" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2110.01107" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2110.01107">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2110.01107" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2110.01108" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 14:44:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
