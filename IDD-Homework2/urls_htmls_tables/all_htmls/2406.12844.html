<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.12844] Synergizing Foundation Models and Federated Learning: A Survey</title><meta property="og:description" content="The recent development of Foundation Models (FMs), represented by large language models, vision transformers, and multimodal models, has been making a significant impact on both academia and industry. Compared with smaâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synergizing Foundation Models and Federated Learning: A Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synergizing Foundation Models and Federated Learning: A Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.12844">

<!--Generated on Sat Jul  6 00:28:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Synergizing Foundation Models and Federated Learning: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Shenghui Li<sup id="id14.14.id1" class="ltx_sup"><span id="id14.14.id1.1" class="ltx_text ltx_font_italic">1</span></sup>Â Â Â Â 
Fanghua Ye<sup id="id15.15.id2" class="ltx_sup"><span id="id15.15.id2.1" class="ltx_text ltx_font_italic">2</span></sup>Â Â Â Â <span id="id3.3.1" class="ltx_text ltx_font_bold">Meng Fang<sup id="id3.3.1.1" class="ltx_sup"><span id="id3.3.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup></span>Â Â Â Â 
Jiaxu Zhao<sup id="id16.16.id3" class="ltx_sup"><span id="id16.16.id3.1" class="ltx_text ltx_font_italic">4</span></sup> 
<br class="ltx_break"><span id="id5.5.2" class="ltx_text ltx_font_bold">Yun-Hin Chan<sup id="id5.5.2.1" class="ltx_sup"><span id="id5.5.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">5</span></sup></span>Â Â Â Â 
<span id="id6.6.3" class="ltx_text ltx_font_bold">Edith C.-H. Ngai<sup id="id6.6.3.1" class="ltx_sup"><span id="id6.6.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">5</span></sup></span>Â Â Â Â 
<span id="id7.7.4" class="ltx_text ltx_font_bold">Thiemo Voigt<sup id="id7.7.4.1" class="ltx_sup"><span id="id7.7.4.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1,6</span></sup></span> 
<br class="ltx_break"><sup id="id17.17.id4" class="ltx_sup">1</sup> Uppsala University, Sweden. <em id="id18.18.id5" class="ltx_emph ltx_font_italic">shenghui.li@it.uu.se</em> 
<br class="ltx_break"><sup id="id19.19.id6" class="ltx_sup">2</sup> University College London, United Kingdom. <em id="id20.20.id7" class="ltx_emph ltx_font_italic">fanghua.ye.19@ucl.ac.uk</em> 
<br class="ltx_break"><sup id="id21.21.id8" class="ltx_sup">3</sup> University of Liverpool, United Kingdom. <em id="id22.22.id9" class="ltx_emph ltx_font_italic">mfang@liverpool.ac.uk</em> 
<br class="ltx_break"><sup id="id23.23.id10" class="ltx_sup">4</sup> Eindhoven University of Technology, the Netherlands. <em id="id24.24.id11" class="ltx_emph ltx_font_italic">j.zhao@@tue.nl</em> 
<br class="ltx_break"><sup id="id25.25.id12" class="ltx_sup">5</sup> The University of Hong Kong, China. <em id="id26.26.id13" class="ltx_emph ltx_font_italic"><span id="id26.26.id13.1" class="ltx_text ltx_font_upright">{</span>chngai@eee, chanyunhin@connect<span id="id26.26.id13.2" class="ltx_text ltx_font_upright">}</span>.hku.hk</em> 
<br class="ltx_break"><sup id="id27.27.id14" class="ltx_sup">6</sup> Research Institutes of Sweden, Sweden. <em id="id28.28.id15" class="ltx_emph ltx_font_italic">thiemo.voigt@angstrom.uu.se</em> 
<br class="ltx_break">
</span><span class="ltx_author_notes">Â Â Â Corresponding Author.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id29.id1" class="ltx_p">The recent development of Foundation Models (FMs), represented by large language models, vision transformers, and multimodal models, has been making a significant impact on both academia and industry. Compared with small-scale models, FMs have a much stronger demand for high-volume data during the pre-training phase. Although general FMs can be pre-trained on data collected from open sources such as the Internet, domain-specific FMs need proprietary data, posing a practical challenge regarding the amount of data available due to privacy concerns. Federated Learning (FL) is a collaborative learning paradigm that breaks the barrier of data availability from different participants. Therefore, it provides a promising solution to customize and adapt FMs to a wide range of domain-specific tasks using distributed datasets whilst preserving privacy. This survey paper discusses the potentials and challenges of synergizing FL and FMs and summarizes core techniques, future directions, and applications.
A periodically updated paper collection on FM-FL is available atÂ <a target="_blank" href="https://github.com/lishenghui/awesome-fm-fl" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/lishenghui/awesome-fm-fl</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The landscape of Artificial Intelligence (AI) has been revolutionized by the emergence of <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">Foundation Models (FMs)</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Bommasani etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>, such as BERTÂ <cite class="ltx_cite ltx_citemacro_cite">Devlin etÂ al. (<a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite>, GPT seriesÂ <cite class="ltx_cite ltx_citemacro_cite">Brown etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>); OpenAI (<a href="#bib.bib122" title="" class="ltx_ref">2022</a>, <a href="#bib.bib123" title="" class="ltx_ref">2024</a>)</cite>, and LLaMA seriesÂ <cite class="ltx_cite ltx_citemacro_cite">Touvron etÂ al. (<a href="#bib.bib166" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib167" title="" class="ltx_ref">b</a>)</cite> in Natural Language Processing (NLP); ViTsÂ <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy etÂ al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite> and SAMÂ <cite class="ltx_cite ltx_citemacro_cite">Kirillov etÂ al. (<a href="#bib.bib82" title="" class="ltx_ref">2023</a>)</cite> in Computer Vision (CV); CLIPÂ <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a href="#bib.bib131" title="" class="ltx_ref">2021</a>)</cite>, DALL-EÂ <cite class="ltx_cite ltx_citemacro_cite">Ramesh etÂ al. (<a href="#bib.bib134" title="" class="ltx_ref">2021</a>)</cite>, GeminiÂ <cite class="ltx_cite ltx_citemacro_cite">Google (<a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite>, and GPT-4o in multimodal applications. These FMs have become pivotal in a myriad of AI applications across diverse domains. Their superb capability to generalize across tasks and domains stems from their pre-training on extensive datasetsÂ <cite class="ltx_cite ltx_citemacro_citep">(Gunasekar etÂ al., <a href="#bib.bib56" title="" class="ltx_ref">2023</a>)</cite>, which imbues them with a profound understanding of language, vision, and multimodal data.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While general-purpose FMs can leverage openly accessible data from the Internet, domain-specific FMs require proprietary data. It is, however, challenging to collect vast amounts of proprietary data and perform centralized pre-training or fine-tuning for domain-specific FMs, due to privacy restrictionsÂ <cite class="ltx_cite ltx_citemacro_cite">Jo and Gebru (<a href="#bib.bib76" title="" class="ltx_ref">2020</a>); GDPR (<a href="#bib.bib50" title="" class="ltx_ref">2016</a>); CCPA (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>. Particularly in domains such as law, healthcare, and finance, where data is inherently privacy-sensitive, there is a pressing need for stringent privacy safeguards. Furthermore, given that data often constitutes a pivotal asset for enterprises, its widespread distribution is prohibitive.
Consequently, there is an urgent need for novel strategies to handle data availability and facilitate model training, thereby unlocking the potential of domain-specific FMs whilst respecting data privacy.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address the challenges associated with data privacy in model training, Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_citep">(McMahan etÂ al., <a href="#bib.bib119" title="" class="ltx_ref">2017</a>)</cite> has emerged as a promising paradigm. FL facilitates collaborative model training across decentralized clients without the need to share raw data, thus ensuring privacy preservation. Concretely, FL encompasses periodic interactions between the server and decentralized clients for the exchange of trainable model parameters without the requirement for private client data.
Recognizing such a benefit, <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">integrating FMs with FL presents a compelling solution for domain-specific FMs</span>Â <cite class="ltx_cite ltx_citemacro_cite">Zhuang etÂ al. (<a href="#bib.bib216" title="" class="ltx_ref">2023</a>); Yu etÂ al. (<a href="#bib.bib199" title="" class="ltx_ref">2023d</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Despite the potential synergies between FL and FMs, the field is still nascent, lacking a comprehensive understanding of challenges, methodologies, and directions. This survey aims to bridge this gap by providing a thorough exploration of the integration of FMs and FL. We delve into the motivations and challenges of combining these two paradigms, highlight representative techniques, and discuss applications and future directions. By elucidating the intersection of FL and FMs, we aim to catalyze further research and innovation in this burgeoning area, ultimately advancing the development of privacy-aware, domain-specific FMs.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The paper continues as follows:
The next section introduces background on FMs and FL. SectionÂ <a href="#S3" title="3 FM-FL: Motivation &amp; Challenges â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the motivation and challenges for synergizing FMs and FL. SectionÂ <a href="#S4" title="4 Techniques â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> highlights representative techniques. SectionÂ <a href="#S5" title="5 Applications of FM-FL â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> explores the applications across various domains. Before concluding, we discuss representative future directions in SectionÂ <a href="#S6" title="6 Future Directions â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Foundation Models</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">An FM is a model that can be adapted to a wide array of tasks through fine-tuning after initial pre-trainingÂ <cite class="ltx_cite ltx_citemacro_cite">Bommasani etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>. The lifecycle of FMs typically involves pre-training on extensive generic data to establish the basis of their abilitiesÂ <cite class="ltx_cite ltx_citemacro_cite">Bubeck etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>, followed by adaptation to downstream tasks such as domain-specific question answeringÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib208" title="" class="ltx_ref">2023e</a>)</cite>, and ultimately application in various domains.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">FMs have sparked a significant paradigm shift in various fields of AI such as NLP, CV, speech and acoustics, and beyond. In the realm of NLP, the most prominent example is Large Language Models (LLMs) with substantial parameter sizesÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a href="#bib.bib213" title="" class="ltx_ref">2023</a>)</cite>. These models, such as ChatGPT and GPT-4Â <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib122" title="" class="ltx_ref">2022</a>, <a href="#bib.bib123" title="" class="ltx_ref">2024</a>)</cite>, demonstrate exceptional abilities in natural language understanding and generation, enabling them to comprehend and respond to user inputs with remarkable contextual relevance. This capability proves invaluable in applications like customer service, virtual assistants, and chatbots, where effective communication is paramount. Moreover, LLMs eliminate the need for training models from scratch for specific tasks, be it machine translation, document summarization, text generation, or other language-related tasks.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">In the realm of CV and other modalities, FMs have also made remarkable progress. Vision Transformers (ViTs)Â <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy etÂ al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite> segment images into distinct patches, which serve as inputs for transformer architectures. SAMÂ <cite class="ltx_cite ltx_citemacro_cite">Kirillov etÂ al. (<a href="#bib.bib82" title="" class="ltx_ref">2023</a>)</cite> can segment anything in images according to the input prompts. CLIPÂ <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a href="#bib.bib131" title="" class="ltx_ref">2021</a>)</cite> bridges the gap between text and images through contrastive learning. DALL<math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mo id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">â‹…</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">â‹…</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">\cdot</annotation></semantics></math>E, proposed byÂ <cite class="ltx_cite ltx_citemacro_citet">Ramesh etÂ al. (<a href="#bib.bib134" title="" class="ltx_ref">2021</a>)</cite>, generates images from textual descriptions, expanding the possibilities of creative image generation. Additionally, models like GAtoÂ <cite class="ltx_cite ltx_citemacro_citep">(Reed etÂ al., <a href="#bib.bib137" title="" class="ltx_ref">2022</a>)</cite>, exhibit versatility by being applicable across various tasks such as conversational agents, robotic control, and gaming.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Federated Learning</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">FLÂ <cite class="ltx_cite ltx_citemacro_cite">McMahan etÂ al. (<a href="#bib.bib119" title="" class="ltx_ref">2017</a>)</cite> is a learning paradigm that enables a collection of clients to collaboratively learn a shared global model by leveraging their private datasets in a distributed manner, assisted by the coordination of a central server.
The general goal of FL is to find a parameter set <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="\bm{\theta}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">ğœ½</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">ğœ½</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\bm{\theta}</annotation></semantics></math> that minimizes the following distributed optimization objective:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.4" class="ltx_Math" alttext="\min\limits_{\bm{\theta}}F(\bm{\theta}):=\frac{1}{K}\sum_{k\in[K]}F_{k}(\bm{\theta})," display="block"><semantics id="S2.E1.m1.4a"><mrow id="S2.E1.m1.4.4.1" xref="S2.E1.m1.4.4.1.1.cmml"><mrow id="S2.E1.m1.4.4.1.1" xref="S2.E1.m1.4.4.1.1.cmml"><mrow id="S2.E1.m1.4.4.1.1.2" xref="S2.E1.m1.4.4.1.1.2.cmml"><mrow id="S2.E1.m1.4.4.1.1.2.2" xref="S2.E1.m1.4.4.1.1.2.2.cmml"><munder id="S2.E1.m1.4.4.1.1.2.2.1" xref="S2.E1.m1.4.4.1.1.2.2.1.cmml"><mi id="S2.E1.m1.4.4.1.1.2.2.1.2" xref="S2.E1.m1.4.4.1.1.2.2.1.2.cmml">min</mi><mi id="S2.E1.m1.4.4.1.1.2.2.1.3" xref="S2.E1.m1.4.4.1.1.2.2.1.3.cmml">ğœ½</mi></munder><mo lspace="0.167em" id="S2.E1.m1.4.4.1.1.2.2a" xref="S2.E1.m1.4.4.1.1.2.2.cmml">â¡</mo><mi id="S2.E1.m1.4.4.1.1.2.2.2" xref="S2.E1.m1.4.4.1.1.2.2.2.cmml">F</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.1.1.2.1" xref="S2.E1.m1.4.4.1.1.2.1.cmml">â€‹</mo><mrow id="S2.E1.m1.4.4.1.1.2.3.2" xref="S2.E1.m1.4.4.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.4.4.1.1.2.3.2.1" xref="S2.E1.m1.4.4.1.1.2.cmml">(</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">ğœ½</mi><mo rspace="0.278em" stretchy="false" id="S2.E1.m1.4.4.1.1.2.3.2.2" xref="S2.E1.m1.4.4.1.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.278em" id="S2.E1.m1.4.4.1.1.1" xref="S2.E1.m1.4.4.1.1.1.cmml">:=</mo><mrow id="S2.E1.m1.4.4.1.1.3" xref="S2.E1.m1.4.4.1.1.3.cmml"><mfrac id="S2.E1.m1.4.4.1.1.3.2" xref="S2.E1.m1.4.4.1.1.3.2.cmml"><mn id="S2.E1.m1.4.4.1.1.3.2.2" xref="S2.E1.m1.4.4.1.1.3.2.2.cmml">1</mn><mi id="S2.E1.m1.4.4.1.1.3.2.3" xref="S2.E1.m1.4.4.1.1.3.2.3.cmml">K</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.1.1.3.1" xref="S2.E1.m1.4.4.1.1.3.1.cmml">â€‹</mo><mrow id="S2.E1.m1.4.4.1.1.3.3" xref="S2.E1.m1.4.4.1.1.3.3.cmml"><munder id="S2.E1.m1.4.4.1.1.3.3.1" xref="S2.E1.m1.4.4.1.1.3.3.1.cmml"><mo movablelimits="false" id="S2.E1.m1.4.4.1.1.3.3.1.2" xref="S2.E1.m1.4.4.1.1.3.3.1.2.cmml">âˆ‘</mo><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.3" xref="S2.E1.m1.1.1.1.3.cmml">k</mi><mo id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.2.cmml">âˆˆ</mo><mrow id="S2.E1.m1.1.1.1.4.2" xref="S2.E1.m1.1.1.1.4.1.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.4.2.1" xref="S2.E1.m1.1.1.1.4.1.1.cmml">[</mo><mi id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">K</mi><mo stretchy="false" id="S2.E1.m1.1.1.1.4.2.2" xref="S2.E1.m1.1.1.1.4.1.1.cmml">]</mo></mrow></mrow></munder><mrow id="S2.E1.m1.4.4.1.1.3.3.2" xref="S2.E1.m1.4.4.1.1.3.3.2.cmml"><msub id="S2.E1.m1.4.4.1.1.3.3.2.2" xref="S2.E1.m1.4.4.1.1.3.3.2.2.cmml"><mi id="S2.E1.m1.4.4.1.1.3.3.2.2.2" xref="S2.E1.m1.4.4.1.1.3.3.2.2.2.cmml">F</mi><mi id="S2.E1.m1.4.4.1.1.3.3.2.2.3" xref="S2.E1.m1.4.4.1.1.3.3.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.1.1.3.3.2.1" xref="S2.E1.m1.4.4.1.1.3.3.2.1.cmml">â€‹</mo><mrow id="S2.E1.m1.4.4.1.1.3.3.2.3.2" xref="S2.E1.m1.4.4.1.1.3.3.2.cmml"><mo stretchy="false" id="S2.E1.m1.4.4.1.1.3.3.2.3.2.1" xref="S2.E1.m1.4.4.1.1.3.3.2.cmml">(</mo><mi id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">ğœ½</mi><mo stretchy="false" id="S2.E1.m1.4.4.1.1.3.3.2.3.2.2" xref="S2.E1.m1.4.4.1.1.3.3.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S2.E1.m1.4.4.1.2" xref="S2.E1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.4b"><apply id="S2.E1.m1.4.4.1.1.cmml" xref="S2.E1.m1.4.4.1"><csymbol cd="latexml" id="S2.E1.m1.4.4.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1">assign</csymbol><apply id="S2.E1.m1.4.4.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.2"><times id="S2.E1.m1.4.4.1.1.2.1.cmml" xref="S2.E1.m1.4.4.1.1.2.1"></times><apply id="S2.E1.m1.4.4.1.1.2.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2"><apply id="S2.E1.m1.4.4.1.1.2.2.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.2.2.1.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1">subscript</csymbol><min id="S2.E1.m1.4.4.1.1.2.2.1.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.2"></min><ci id="S2.E1.m1.4.4.1.1.2.2.1.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.3">ğœ½</ci></apply><ci id="S2.E1.m1.4.4.1.1.2.2.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2">ğ¹</ci></apply><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">ğœ½</ci></apply><apply id="S2.E1.m1.4.4.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.3"><times id="S2.E1.m1.4.4.1.1.3.1.cmml" xref="S2.E1.m1.4.4.1.1.3.1"></times><apply id="S2.E1.m1.4.4.1.1.3.2.cmml" xref="S2.E1.m1.4.4.1.1.3.2"><divide id="S2.E1.m1.4.4.1.1.3.2.1.cmml" xref="S2.E1.m1.4.4.1.1.3.2"></divide><cn type="integer" id="S2.E1.m1.4.4.1.1.3.2.2.cmml" xref="S2.E1.m1.4.4.1.1.3.2.2">1</cn><ci id="S2.E1.m1.4.4.1.1.3.2.3.cmml" xref="S2.E1.m1.4.4.1.1.3.2.3">ğ¾</ci></apply><apply id="S2.E1.m1.4.4.1.1.3.3.cmml" xref="S2.E1.m1.4.4.1.1.3.3"><apply id="S2.E1.m1.4.4.1.1.3.3.1.cmml" xref="S2.E1.m1.4.4.1.1.3.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.3.3.1.1.cmml" xref="S2.E1.m1.4.4.1.1.3.3.1">subscript</csymbol><sum id="S2.E1.m1.4.4.1.1.3.3.1.2.cmml" xref="S2.E1.m1.4.4.1.1.3.3.1.2"></sum><apply id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><in id="S2.E1.m1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.2"></in><ci id="S2.E1.m1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.3">ğ‘˜</ci><apply id="S2.E1.m1.1.1.1.4.1.cmml" xref="S2.E1.m1.1.1.1.4.2"><csymbol cd="latexml" id="S2.E1.m1.1.1.1.4.1.1.cmml" xref="S2.E1.m1.1.1.1.4.2.1">delimited-[]</csymbol><ci id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1">ğ¾</ci></apply></apply></apply><apply id="S2.E1.m1.4.4.1.1.3.3.2.cmml" xref="S2.E1.m1.4.4.1.1.3.3.2"><times id="S2.E1.m1.4.4.1.1.3.3.2.1.cmml" xref="S2.E1.m1.4.4.1.1.3.3.2.1"></times><apply id="S2.E1.m1.4.4.1.1.3.3.2.2.cmml" xref="S2.E1.m1.4.4.1.1.3.3.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.3.3.2.2.1.cmml" xref="S2.E1.m1.4.4.1.1.3.3.2.2">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.3.3.2.2.2.cmml" xref="S2.E1.m1.4.4.1.1.3.3.2.2.2">ğ¹</ci><ci id="S2.E1.m1.4.4.1.1.3.3.2.2.3.cmml" xref="S2.E1.m1.4.4.1.1.3.3.2.2.3">ğ‘˜</ci></apply><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">ğœ½</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.4c">\min\limits_{\bm{\theta}}F(\bm{\theta}):=\frac{1}{K}\sum_{k\in[K]}F_{k}(\bm{\theta}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p1.7" class="ltx_p">where <math id="S2.SS2.p1.2.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS2.p1.2.m1.1a"><mi id="S2.SS2.p1.2.m1.1.1" xref="S2.SS2.p1.2.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m1.1b"><ci id="S2.SS2.p1.2.m1.1.1.cmml" xref="S2.SS2.p1.2.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m1.1c">K</annotation></semantics></math> represents the total number of clients and <math id="S2.SS2.p1.3.m2.4" class="ltx_Math" alttext="F_{k}(\bm{\theta})=\mathbb{E}_{\bm{z}\sim\mathcal{D}_{k}}[\ell(\bm{\theta};\bm{z})]" display="inline"><semantics id="S2.SS2.p1.3.m2.4a"><mrow id="S2.SS2.p1.3.m2.4.4" xref="S2.SS2.p1.3.m2.4.4.cmml"><mrow id="S2.SS2.p1.3.m2.4.4.3" xref="S2.SS2.p1.3.m2.4.4.3.cmml"><msub id="S2.SS2.p1.3.m2.4.4.3.2" xref="S2.SS2.p1.3.m2.4.4.3.2.cmml"><mi id="S2.SS2.p1.3.m2.4.4.3.2.2" xref="S2.SS2.p1.3.m2.4.4.3.2.2.cmml">F</mi><mi id="S2.SS2.p1.3.m2.4.4.3.2.3" xref="S2.SS2.p1.3.m2.4.4.3.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.p1.3.m2.4.4.3.1" xref="S2.SS2.p1.3.m2.4.4.3.1.cmml">â€‹</mo><mrow id="S2.SS2.p1.3.m2.4.4.3.3.2" xref="S2.SS2.p1.3.m2.4.4.3.cmml"><mo stretchy="false" id="S2.SS2.p1.3.m2.4.4.3.3.2.1" xref="S2.SS2.p1.3.m2.4.4.3.cmml">(</mo><mi id="S2.SS2.p1.3.m2.1.1" xref="S2.SS2.p1.3.m2.1.1.cmml">ğœ½</mi><mo stretchy="false" id="S2.SS2.p1.3.m2.4.4.3.3.2.2" xref="S2.SS2.p1.3.m2.4.4.3.cmml">)</mo></mrow></mrow><mo id="S2.SS2.p1.3.m2.4.4.2" xref="S2.SS2.p1.3.m2.4.4.2.cmml">=</mo><mrow id="S2.SS2.p1.3.m2.4.4.1" xref="S2.SS2.p1.3.m2.4.4.1.cmml"><msub id="S2.SS2.p1.3.m2.4.4.1.3" xref="S2.SS2.p1.3.m2.4.4.1.3.cmml"><mi id="S2.SS2.p1.3.m2.4.4.1.3.2" xref="S2.SS2.p1.3.m2.4.4.1.3.2.cmml">ğ”¼</mi><mrow id="S2.SS2.p1.3.m2.4.4.1.3.3" xref="S2.SS2.p1.3.m2.4.4.1.3.3.cmml"><mi id="S2.SS2.p1.3.m2.4.4.1.3.3.2" xref="S2.SS2.p1.3.m2.4.4.1.3.3.2.cmml">ğ’›</mi><mo id="S2.SS2.p1.3.m2.4.4.1.3.3.1" xref="S2.SS2.p1.3.m2.4.4.1.3.3.1.cmml">âˆ¼</mo><msub id="S2.SS2.p1.3.m2.4.4.1.3.3.3" xref="S2.SS2.p1.3.m2.4.4.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.3.m2.4.4.1.3.3.3.2" xref="S2.SS2.p1.3.m2.4.4.1.3.3.3.2.cmml">ğ’Ÿ</mi><mi id="S2.SS2.p1.3.m2.4.4.1.3.3.3.3" xref="S2.SS2.p1.3.m2.4.4.1.3.3.3.3.cmml">k</mi></msub></mrow></msub><mo lspace="0em" rspace="0em" id="S2.SS2.p1.3.m2.4.4.1.2" xref="S2.SS2.p1.3.m2.4.4.1.2.cmml">â€‹</mo><mrow id="S2.SS2.p1.3.m2.4.4.1.1.1" xref="S2.SS2.p1.3.m2.4.4.1.1.2.cmml"><mo stretchy="false" id="S2.SS2.p1.3.m2.4.4.1.1.1.2" xref="S2.SS2.p1.3.m2.4.4.1.1.2.1.cmml">[</mo><mrow id="S2.SS2.p1.3.m2.4.4.1.1.1.1" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.cmml"><mi mathvariant="normal" id="S2.SS2.p1.3.m2.4.4.1.1.1.1.2" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.2.cmml">â„“</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.3.m2.4.4.1.1.1.1.1" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.1.cmml">â€‹</mo><mrow id="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.2" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.2.1" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.1.cmml">(</mo><mi id="S2.SS2.p1.3.m2.2.2" xref="S2.SS2.p1.3.m2.2.2.cmml">ğœ½</mi><mo id="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.2.2" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.1.cmml">;</mo><mi id="S2.SS2.p1.3.m2.3.3" xref="S2.SS2.p1.3.m2.3.3.cmml">ğ’›</mi><mo stretchy="false" id="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.2.3" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.SS2.p1.3.m2.4.4.1.1.1.3" xref="S2.SS2.p1.3.m2.4.4.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m2.4b"><apply id="S2.SS2.p1.3.m2.4.4.cmml" xref="S2.SS2.p1.3.m2.4.4"><eq id="S2.SS2.p1.3.m2.4.4.2.cmml" xref="S2.SS2.p1.3.m2.4.4.2"></eq><apply id="S2.SS2.p1.3.m2.4.4.3.cmml" xref="S2.SS2.p1.3.m2.4.4.3"><times id="S2.SS2.p1.3.m2.4.4.3.1.cmml" xref="S2.SS2.p1.3.m2.4.4.3.1"></times><apply id="S2.SS2.p1.3.m2.4.4.3.2.cmml" xref="S2.SS2.p1.3.m2.4.4.3.2"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m2.4.4.3.2.1.cmml" xref="S2.SS2.p1.3.m2.4.4.3.2">subscript</csymbol><ci id="S2.SS2.p1.3.m2.4.4.3.2.2.cmml" xref="S2.SS2.p1.3.m2.4.4.3.2.2">ğ¹</ci><ci id="S2.SS2.p1.3.m2.4.4.3.2.3.cmml" xref="S2.SS2.p1.3.m2.4.4.3.2.3">ğ‘˜</ci></apply><ci id="S2.SS2.p1.3.m2.1.1.cmml" xref="S2.SS2.p1.3.m2.1.1">ğœ½</ci></apply><apply id="S2.SS2.p1.3.m2.4.4.1.cmml" xref="S2.SS2.p1.3.m2.4.4.1"><times id="S2.SS2.p1.3.m2.4.4.1.2.cmml" xref="S2.SS2.p1.3.m2.4.4.1.2"></times><apply id="S2.SS2.p1.3.m2.4.4.1.3.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m2.4.4.1.3.1.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3">subscript</csymbol><ci id="S2.SS2.p1.3.m2.4.4.1.3.2.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3.2">ğ”¼</ci><apply id="S2.SS2.p1.3.m2.4.4.1.3.3.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3.3"><csymbol cd="latexml" id="S2.SS2.p1.3.m2.4.4.1.3.3.1.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3.3.1">similar-to</csymbol><ci id="S2.SS2.p1.3.m2.4.4.1.3.3.2.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3.3.2">ğ’›</ci><apply id="S2.SS2.p1.3.m2.4.4.1.3.3.3.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m2.4.4.1.3.3.3.1.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3.3.3">subscript</csymbol><ci id="S2.SS2.p1.3.m2.4.4.1.3.3.3.2.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3.3.3.2">ğ’Ÿ</ci><ci id="S2.SS2.p1.3.m2.4.4.1.3.3.3.3.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3.3.3.3">ğ‘˜</ci></apply></apply></apply><apply id="S2.SS2.p1.3.m2.4.4.1.1.2.cmml" xref="S2.SS2.p1.3.m2.4.4.1.1.1"><csymbol cd="latexml" id="S2.SS2.p1.3.m2.4.4.1.1.2.1.cmml" xref="S2.SS2.p1.3.m2.4.4.1.1.1.2">delimited-[]</csymbol><apply id="S2.SS2.p1.3.m2.4.4.1.1.1.1.cmml" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1"><times id="S2.SS2.p1.3.m2.4.4.1.1.1.1.1.cmml" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.1"></times><ci id="S2.SS2.p1.3.m2.4.4.1.1.1.1.2.cmml" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.2">â„“</ci><list id="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.1.cmml" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.2"><ci id="S2.SS2.p1.3.m2.2.2.cmml" xref="S2.SS2.p1.3.m2.2.2">ğœ½</ci><ci id="S2.SS2.p1.3.m2.3.3.cmml" xref="S2.SS2.p1.3.m2.3.3">ğ’›</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m2.4c">F_{k}(\bm{\theta})=\mathbb{E}_{\bm{z}\sim\mathcal{D}_{k}}[\ell(\bm{\theta};\bm{z})]</annotation></semantics></math> denotes the expected risk of the <math id="S2.SS2.p1.4.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS2.p1.4.m3.1a"><mi id="S2.SS2.p1.4.m3.1.1" xref="S2.SS2.p1.4.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m3.1b"><ci id="S2.SS2.p1.4.m3.1.1.cmml" xref="S2.SS2.p1.4.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m3.1c">k</annotation></semantics></math>-th client. Here, <math id="S2.SS2.p1.5.m4.1" class="ltx_Math" alttext="\mathcal{D}_{k}" display="inline"><semantics id="S2.SS2.p1.5.m4.1a"><msub id="S2.SS2.p1.5.m4.1.1" xref="S2.SS2.p1.5.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.5.m4.1.1.2" xref="S2.SS2.p1.5.m4.1.1.2.cmml">ğ’Ÿ</mi><mi id="S2.SS2.p1.5.m4.1.1.3" xref="S2.SS2.p1.5.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m4.1b"><apply id="S2.SS2.p1.5.m4.1.1.cmml" xref="S2.SS2.p1.5.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m4.1.1.1.cmml" xref="S2.SS2.p1.5.m4.1.1">subscript</csymbol><ci id="S2.SS2.p1.5.m4.1.1.2.cmml" xref="S2.SS2.p1.5.m4.1.1.2">ğ’Ÿ</ci><ci id="S2.SS2.p1.5.m4.1.1.3.cmml" xref="S2.SS2.p1.5.m4.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m4.1c">\mathcal{D}_{k}</annotation></semantics></math> is the data distribution for the <math id="S2.SS2.p1.6.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS2.p1.6.m5.1a"><mi id="S2.SS2.p1.6.m5.1.1" xref="S2.SS2.p1.6.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m5.1b"><ci id="S2.SS2.p1.6.m5.1.1.cmml" xref="S2.SS2.p1.6.m5.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m5.1c">k</annotation></semantics></math>-th client, and <math id="S2.SS2.p1.7.m6.2" class="ltx_Math" alttext="\ell(\cdot;\cdot)" display="inline"><semantics id="S2.SS2.p1.7.m6.2a"><mrow id="S2.SS2.p1.7.m6.2.3" xref="S2.SS2.p1.7.m6.2.3.cmml"><mi mathvariant="normal" id="S2.SS2.p1.7.m6.2.3.2" xref="S2.SS2.p1.7.m6.2.3.2.cmml">â„“</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.7.m6.2.3.1" xref="S2.SS2.p1.7.m6.2.3.1.cmml">â€‹</mo><mrow id="S2.SS2.p1.7.m6.2.3.3.2" xref="S2.SS2.p1.7.m6.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS2.p1.7.m6.2.3.3.2.1" xref="S2.SS2.p1.7.m6.2.3.3.1.cmml">(</mo><mo lspace="0em" rspace="0em" id="S2.SS2.p1.7.m6.1.1" xref="S2.SS2.p1.7.m6.1.1.cmml">â‹…</mo><mo rspace="0em" id="S2.SS2.p1.7.m6.2.3.3.2.2" xref="S2.SS2.p1.7.m6.2.3.3.1.cmml">;</mo><mo lspace="0em" rspace="0em" id="S2.SS2.p1.7.m6.2.2" xref="S2.SS2.p1.7.m6.2.2.cmml">â‹…</mo><mo stretchy="false" id="S2.SS2.p1.7.m6.2.3.3.2.3" xref="S2.SS2.p1.7.m6.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m6.2b"><apply id="S2.SS2.p1.7.m6.2.3.cmml" xref="S2.SS2.p1.7.m6.2.3"><times id="S2.SS2.p1.7.m6.2.3.1.cmml" xref="S2.SS2.p1.7.m6.2.3.1"></times><ci id="S2.SS2.p1.7.m6.2.3.2.cmml" xref="S2.SS2.p1.7.m6.2.3.2">â„“</ci><list id="S2.SS2.p1.7.m6.2.3.3.1.cmml" xref="S2.SS2.p1.7.m6.2.3.3.2"><ci id="S2.SS2.p1.7.m6.1.1.cmml" xref="S2.SS2.p1.7.m6.1.1">â‹…</ci><ci id="S2.SS2.p1.7.m6.2.2.cmml" xref="S2.SS2.p1.7.m6.2.2">â‹…</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m6.2c">\ell(\cdot;\cdot)</annotation></semantics></math> is a user-specified loss function.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The most representative algorithms in the FL literature are the <span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">FedAvg</span>-family algorithmsÂ <cite class="ltx_cite ltx_citemacro_cite">McMahan etÂ al. (<a href="#bib.bib119" title="" class="ltx_ref">2017</a>); Reddi etÂ al. (<a href="#bib.bib136" title="" class="ltx_ref">2021</a>)</cite>. The standard <span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_typewriter">FedAvg</span> involves periodic interactions between the server and decentralized clients to exchange trainable model parameters.
In this process, each client independently trains the model on its local data and sends the model updates to a central server. The server aggregates these updates by computing their average to update the global model, which is subsequently redistributed to the clients for further iterations.
Many variants have been proposed to tackle issues such as convergence and local data heterogeneityÂ <cite class="ltx_cite ltx_citemacro_cite">Diao etÂ al. (<a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>. For example, <span id="S2.SS2.p2.1.3" class="ltx_text ltx_font_typewriter">FedProx</span>Â <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a href="#bib.bib92" title="" class="ltx_ref">2020</a>)</cite> and <span id="S2.SS2.p2.1.4" class="ltx_text ltx_font_typewriter">FedDyn</span>Â <cite class="ltx_cite ltx_citemacro_cite">Acar etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite> introduce regularizer terms to penalize client updates that are far away from the server model. A general framework <span id="S2.SS2.p2.1.5" class="ltx_text ltx_font_typewriter">FedOpt</span>Â <cite class="ltx_cite ltx_citemacro_cite">Reddi etÂ al. (<a href="#bib.bib136" title="" class="ltx_ref">2021</a>)</cite> unifies adaptive optimizers (<span id="S2.SS2.p2.1.6" class="ltx_text ltx_font_italic">Adam</span>, <span id="S2.SS2.p2.1.7" class="ltx_text ltx_font_italic">Yogi</span>, <em id="S2.SS2.p2.1.8" class="ltx_emph ltx_font_italic">etc</em>.) and demonstrates superior convergence speed when compared to the naive <span id="S2.SS2.p2.1.9" class="ltx_text ltx_font_typewriter">FedAvg</span>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">FL offers an efficient privacy-preserving way to train models on large-scale and diverse data <cite class="ltx_cite ltx_citemacro_cite">Kairouz etÂ al. (<a href="#bib.bib79" title="" class="ltx_ref">2021</a>)</cite>, leading to its application across various domains such as healthcareÂ <cite class="ltx_cite ltx_citemacro_cite">Lincy and Kowshalya (<a href="#bib.bib101" title="" class="ltx_ref">2020</a>); Rieke etÂ al. (<a href="#bib.bib140" title="" class="ltx_ref">2020</a>); Joshi etÂ al. (<a href="#bib.bib78" title="" class="ltx_ref">2022</a>)</cite>, financeÂ <cite class="ltx_cite ltx_citemacro_cite">Chatterjee etÂ al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>); Liu etÂ al. (<a href="#bib.bib106" title="" class="ltx_ref">2023b</a>)</cite>, and smart citiesÂ <cite class="ltx_cite ltx_citemacro_cite">Ramu etÂ al. (<a href="#bib.bib135" title="" class="ltx_ref">2022</a>); Pandya etÂ al. (<a href="#bib.bib125" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>FM-FL: Motivation &amp; Challenges</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we first motivate the synergy of FMs and FL (SectionÂ <a href="#S3.SS1" title="3.1 Motivation â€£ 3 FM-FL: Motivation &amp; Challenges â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), then summarize the key challenges (SectionÂ <a href="#S3.SS2" title="3.2 Core Challenges â€£ 3 FM-FL: Motivation &amp; Challenges â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Motivation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The integration of FMs and FL represents a compelling collaboration that leverages each otherâ€™s strengths to address their respective limitations, embodying a complementary relationship <cite class="ltx_cite ltx_citemacro_cite">Zhuang etÂ al. (<a href="#bib.bib216" title="" class="ltx_ref">2023</a>); Li and Wang (<a href="#bib.bib93" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">FL expands data availability for FMs</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">By leveraging data from a wide range of sources in a privacy-preserving manner, FL makes it possible to build models on sensitive data in specific domains, such as healthcare <cite class="ltx_cite ltx_citemacro_cite">Lincy and Kowshalya (<a href="#bib.bib101" title="" class="ltx_ref">2020</a>); Joshi etÂ al. (<a href="#bib.bib78" title="" class="ltx_ref">2022</a>); Rieke etÂ al. (<a href="#bib.bib140" title="" class="ltx_ref">2020</a>)</cite> and finance <cite class="ltx_cite ltx_citemacro_cite">Chatterjee etÂ al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>); Liu etÂ al. (<a href="#bib.bib106" title="" class="ltx_ref">2023b</a>)</cite>. This enhances the diversity and volume of training data, improving model robustness and adaptability. Moreover, FL enables the integration of personal and task-specific data, allowing FMs to be customized for personal applications. For instance, Google has trained next-word-prediction language models on mobile keyboard input data with FL to improve user experienceÂ <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a href="#bib.bib188" title="" class="ltx_ref">2023</a>); Bonawitz etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">FMs boost FL with feature representation and few-shot learning capabilities</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">By pre-training on large-scale generic data, FMs acquire essential knowledge and understanding capabilitiesÂ <cite class="ltx_cite ltx_citemacro_cite">Brown etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite>, providing multiple benefits to FL. Firstly, they benefit FL systems by offering advanced feature representations and learning capabilities from the outset. Secondly, leveraging the pre-learned knowledge of FMs can accelerate the FL process, enabling efficient and effective adaptation to specific tasks with minimal additional training. Thirdly, FMsâ€™ powerful generative capabilities could help FL overcome the data heterogeneity challenge by synthesizing extra data, thus accelerating model convergenceÂ <cite class="ltx_cite ltx_citemacro_cite">Huang etÂ al. (<a href="#bib.bib68" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Core Challenges</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In this part, we discuss challenges emerging from the FM-FL marriage in three aspects: <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">efficiency</span>, <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">adaptability</span>, as well as <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_italic">trustworthiness</span>.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Efficiency Challenges</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">Efficiency challenges stem from the mismatch between the significant resource demands of FM training and the limited, heterogeneous system resources (<em id="S3.SS2.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> mobile devices) within FL systems, such as communication bandwidth, computational power, and memoryÂ <cite class="ltx_cite ltx_citemacro_cite">Su etÂ al. (<a href="#bib.bib152" title="" class="ltx_ref">2023</a>)</cite>. The communication bottleneck of FL is induced by frequently exchanging training information between the server and clients over limited bandwidth channelsÂ <cite class="ltx_cite ltx_citemacro_cite">Kairouz etÂ al. (<a href="#bib.bib79" title="" class="ltx_ref">2021</a>)</cite>. The substantial number of parameters in FMs further exacerbates this burden, thus hindering the training process.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Adaptability Challenges</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">Adaptability challenges arise from the adaptation of an FM to a specific downstream task (<em id="S3.SS2.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> by fine-tuning)
in FL settings. Key challenges include <span id="S3.SS2.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">data heterogeneity</span> and <span id="S3.SS2.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_italic">resource heterogeneity</span>. Performance degradation in FL, attributed to heterogeneous data distributions among clients, is a well-recognized issue <cite class="ltx_cite ltx_citemacro_cite">Kairouz etÂ al. (<a href="#bib.bib79" title="" class="ltx_ref">2021</a>); Li etÂ al. (<a href="#bib.bib88" title="" class="ltx_ref">2022</a>)</cite>. A recent studyÂ <cite class="ltx_cite ltx_citemacro_cite">Babakniya etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2023a</a>)</cite> has shown that such performance penalty is even more substantial when fine-tuning FMs. For NLP tasks, data heterogeneity can manifest as variations in language, style, topic, or sentiment across datasets held by different clients. In multi-modal scenarios, the challenge is even more pronounced due to the inherent diversity in data types (<em id="S3.SS2.SSS0.Px2.p1.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> text, images, and audio) <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib196" title="" class="ltx_ref">2023a</a>)</cite>. Addressing data heterogeneity involves not just identifying and measuring it but also developing algorithms that are robust to such diversity, ensuring that the model can learn effectively from varied data contributions without compromising on performance. In terms of resource heterogeneity, the memory and computational resources of the devices for different participants may be diverseÂ <cite class="ltx_cite ltx_citemacro_cite">Diao etÂ al. (<a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>, which could cause delays for model synchronization and inactivation of some participants, <em id="S3.SS2.SSS0.Px2.p1.1.5" class="ltx_emph ltx_font_italic">i.e.,</em> stragglers, making it challenging to leverage the full potential of FMs in FL settings.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Trustworthiness Challenges</h5>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">Trustworthiness challenges emphasize the concerns regarding privacy, security, and ethical considerations in the lifecycle of FM-FL, from the pre-training and model adaptation to the application stages. We present two representative challenges from this perspective: (1) <span id="S3.SS2.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">intellectual property</span>: Intellectual Property (IP) protection in FM-FL primarily involves attributing ownership rights for both models and data.
From the serverâ€™s perspective, broadcasting a pre-trained model to multiple nodes for fine-tuning poses IP protection and security risks (<em id="S3.SS2.SSS0.Px3.p1.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> model theft), necessitating measures to safeguard IP rights and ensure model integrityÂ <cite class="ltx_cite ltx_citemacro_cite">Kang etÂ al. (<a href="#bib.bib80" title="" class="ltx_ref">2024</a>)</cite>; (2)
<span id="S3.SS2.SSS0.Px3.p1.1.3" class="ltx_text ltx_font_italic">privacy leakage</span>: Although FL does not immediately share data, studies have shown that it may not always guarantee sufficient privacy preservationÂ <cite class="ltx_cite ltx_citemacro_cite">Geiping etÂ al. (<a href="#bib.bib51" title="" class="ltx_ref">2020</a>)</cite>, as model parameters (<em id="S3.SS2.SSS0.Px3.p1.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> weights or gradients) may leak sensitive information to malicious adversariesÂ <cite class="ltx_cite ltx_citemacro_cite">Zhu etÂ al. (<a href="#bib.bib215" title="" class="ltx_ref">2019</a>)</cite>.
(3)<span id="S3.SS2.SSS0.Px3.p1.1.5" class="ltx_text ltx_font_italic">Poisoning Attacks</span>: FL systems are inherently vulnerable to attacks due to their wide attack surface and reliance on network communicationÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a href="#bib.bib91" title="" class="ltx_ref">2023b</a>)</cite>. Poisoning attacks are carried out by malicious participants, aiming to bias the global model to the desire of attackers.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<div id="S3.F1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:292.9pt;vertical-align:-288.2pt;"><span class="ltx_transformed_inner" style="transform:translate(-127.4pt,1.4pt) scale(0.629913545298441,0.629913545298441) ;"><span id="S3.F1.1.1" class="ltx_ERROR undefined">{forest}</span>
<p id="S3.F1.1.2" class="ltx_p">for tree=
edge path=[<span id="S3.F1.1.2.1" class="ltx_ERROR undefined">\forestoption</span>edge,-&gt;, &gt;=Latex[length=1.mm,width=1.mm]]
(!u.parent anchor) â€“ +(4pt,0pt) |- (.child anchor)
<span id="S3.F1.1.2.2" class="ltx_ERROR undefined">\forestoption</span>edge label;,
grow=east,
reversed=true,
anchor=base west,
parent anchor=east,
child anchor=west,
base=center,
font=,
rectangle,
draw=hidden-draw,
rounded corners,
align=center,
minimum width=4em,
edge+=semithick, draw=hidden-draw,line width=0.5pt,
s sep=2pt,
l sep=12pt,
inner xsep=4pt,
inner ysep=3pt,
ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center,
,
where level=0l sep = 1pt, s sep = 1pt,
where level=3l sep = 6pt
[FM-FL, ver,
[Efficiency 
<br class="ltx_break">(Â§<a href="#S4.SS1" title="4.1 Efficiency â€£ 4 Techniques â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>), l1node,
[Parameter-Efficient 
<br class="ltx_break">Fine-Tuning, l2node,
[Selective, l3node,
[
Â <span id="S3.F1.1.2.3" class="ltx_text ltx_font_italic">RaFFM</span>Â <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib198" title="" class="ltx_ref">2023c</a>)</cite>,
<span id="S3.F1.1.2.4" class="ltx_text ltx_font_italic">FedBF</span>Â <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib210" title="" class="ltx_ref">2023f</a>)</cite>
, leaf]
]
[Additive, l3node,
[
Â <span id="S3.F1.1.2.5" class="ltx_text ltx_font_italic">FedCLIP</span>Â <cite class="ltx_cite ltx_citemacro_cite">Lu etÂ al. (<a href="#bib.bib110" title="" class="ltx_ref">2023a</a>)</cite>, <span id="S3.F1.1.2.6" class="ltx_text ltx_font_italic">FedDAT</span>Â <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2024</a>)</cite>
, leaf]
]
[Reparameterization-based, l3node,
[
Â <span id="S3.F1.1.2.7" class="ltx_text ltx_font_smallcaps">HetLoRA</span>Â <cite class="ltx_cite ltx_citemacro_cite">Cho etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2024</a>)</cite>, <span id="S3.F1.1.2.8" class="ltx_text ltx_font_italic">FedDPA</span>Â <cite class="ltx_cite ltx_citemacro_cite">Yang etÂ al. (<a href="#bib.bib192" title="" class="ltx_ref">2024b</a>)</cite>, leaf
]
]
]
[Model Compression, l2node,
[Sparsification, l3node,
[Â <span id="S3.F1.1.2.9" class="ltx_text ltx_font_italic">PruneFL</span>Â <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a href="#bib.bib75" title="" class="ltx_ref">2023c</a>)</cite>,
<span id="S3.F1.1.2.10" class="ltx_text ltx_font_italic">FLASH</span>Â <cite class="ltx_cite ltx_citemacro_cite">Babakniya etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2023b</a>)</cite>,leaf]
]
[Quantization, l3node,
[Â <span id="S3.F1.1.2.11" class="ltx_text ltx_font_italic">FedSplitBERT</span>Â <cite class="ltx_cite ltx_citemacro_cite">Lit etÂ al. (<a href="#bib.bib103" title="" class="ltx_ref">2022</a>)</cite>,leaf]
]
]
[Zeroth-Order 
<br class="ltx_break">Optimization, l2node,
[
Â <span id="S3.F1.1.2.12" class="ltx_text ltx_font_italic">BAFFLE</span>Â <cite class="ltx_cite ltx_citemacro_cite">Feng etÂ al. (<a href="#bib.bib46" title="" class="ltx_ref">2023b</a>)</cite>, <span id="S3.F1.1.2.13" class="ltx_text ltx_font_italic">FedZeN</span>Â <cite class="ltx_cite ltx_citemacro_cite">Maritan etÂ al. (<a href="#bib.bib118" title="" class="ltx_ref">2023</a>)</cite>, <span id="S3.F1.1.2.14" class="ltx_text ltx_font_italic">FedKSeed</span>Â <cite class="ltx_cite ltx_citemacro_cite">Qin etÂ al. (<a href="#bib.bib129" title="" class="ltx_ref">2024</a>)</cite>, 
<br class="ltx_break"><span id="S3.F1.1.2.15" class="ltx_text ltx_font_italic">FwdLLM</span>Â <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a href="#bib.bib185" title="" class="ltx_ref">2024a</a>)</cite>, <span id="S3.F1.1.2.16" class="ltx_text ltx_font_italic">ZooPFL</span>Â <cite class="ltx_cite ltx_citemacro_cite">Lu etÂ al. (<a href="#bib.bib111" title="" class="ltx_ref">2023b</a>)</cite>,
<span id="S3.F1.1.2.17" class="ltx_text ltx_font_italic">FedMeZO</span>Â <cite class="ltx_cite ltx_citemacro_cite">Ling etÂ al. (<a href="#bib.bib102" title="" class="ltx_ref">2024</a>)</cite>,wide leaf]
]
]
[Adaptability
<br class="ltx_break">(Â§<a href="#S4.SS2" title="4.2 Adaptability â€£ 4 Techniques â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>), l1node,
[Domain-Centric, l2node,
[Domain-Adaptive Pre-Training, l3node,
[

Â <span id="S3.F1.1.2.18" class="ltx_text ltx_font_italic">FMTDA</span>Â <cite class="ltx_cite ltx_citemacro_cite">Yao etÂ al. (<a href="#bib.bib193" title="" class="ltx_ref">2022</a>)</cite>,
<span id="S3.F1.1.2.19" class="ltx_text ltx_font_italic">FEDBFPT</span>Â <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a href="#bib.bib172" title="" class="ltx_ref">2023</a>)</cite>
, leaf
]
]
[Multi-Domain Adaptation, l3node,
[
Â FedAPTÂ <cite class="ltx_cite ltx_citemacro_cite">Su etÂ al. (<a href="#bib.bib153" title="" class="ltx_ref">2024</a>)</cite>, DiPrompTÂ <cite class="ltx_cite ltx_citemacro_cite">Bai etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2024b</a>)</cite>, leaf
]
]
]
[Client-Centric, l2node,
[Personalization, l3node,
[
Â <span id="S3.F1.1.2.20" class="ltx_text ltx_font_italic">FedDAT</span>Â <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2024</a>)</cite>,
<span id="S3.F1.1.2.21" class="ltx_text ltx_font_italic">Fed-MNMT</span>Â <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib108" title="" class="ltx_ref">2023d</a>)</cite>,leaf
]
]
[Client Clustering, l3node,
[
Â <span id="S3.F1.1.2.22" class="ltx_text ltx_font_italic">FedLFC</span>Â <cite class="ltx_cite ltx_citemacro_cite">Guo etÂ al. (<a href="#bib.bib60" title="" class="ltx_ref">2024b</a>)</cite>,
<span id="S3.F1.1.2.23" class="ltx_text ltx_font_italic">FL-TAC</span>Â <cite class="ltx_cite ltx_citemacro_cite">Ping etÂ al. (<a href="#bib.bib127" title="" class="ltx_ref">2024</a>)</cite>,leaf
]
]
]
[System-Centric, l2node,
[Resource-Heterogeneous, l3node,
[
Â <span id="S3.F1.1.2.24" class="ltx_text ltx_font_italic">FedRA</span>Â <cite class="ltx_cite ltx_citemacro_cite">Su etÂ al. (<a href="#bib.bib152" title="" class="ltx_ref">2023</a>)</cite>, <span id="S3.F1.1.2.25" class="ltx_text ltx_font_smallcaps">HetLoRA</span>Â <cite class="ltx_cite ltx_citemacro_cite">Cho etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2024</a>)</cite>,leaf
]
]
[
Split Learning, l3node,
[Â <span id="S3.F1.1.2.26" class="ltx_text ltx_font_italic">FedBERT</span>Â <cite class="ltx_cite ltx_citemacro_cite">Tian etÂ al. (<a href="#bib.bib165" title="" class="ltx_ref">2022</a>)</cite>, <span id="S3.F1.1.2.27" class="ltx_text ltx_font_italic">FedSplitX</span>Â <cite class="ltx_cite ltx_citemacro_cite">Shin etÂ al. (<a href="#bib.bib149" title="" class="ltx_ref">2023b</a>)</cite>,leaf]
]
]
]
[Trustworthiness 
<br class="ltx_break">(Â§<a href="#S4.SS3" title="4.3 Trustworthiness â€£ 4 Techniques â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>), l1node,
[IP Protection, l2node,
[Watermarking, l3node,
[
Â <span id="S3.F1.1.2.28" class="ltx_text ltx_font_italic">WAFFLE</span>Â <cite class="ltx_cite ltx_citemacro_cite">Tekgul etÂ al. (<a href="#bib.bib163" title="" class="ltx_ref">2021</a>)</cite>, <span id="S3.F1.1.2.29" class="ltx_text ltx_font_italic">DUW</span>Â <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib197" title="" class="ltx_ref">2023b</a>)</cite>,leaf
]
]
[Black-Box Tuning, l3node,
[
Â <span id="S3.F1.1.2.30" class="ltx_text ltx_font_italic">Fed-BBPT</span>Â <cite class="ltx_cite ltx_citemacro_cite">Lin etÂ al. (<a href="#bib.bib100" title="" class="ltx_ref">2023</a>)</cite>,
<span id="S3.F1.1.2.31" class="ltx_text ltx_font_italic">pFedGPT</span>Â <cite class="ltx_cite ltx_citemacro_cite">Rui etÂ al. (<a href="#bib.bib143" title="" class="ltx_ref">2024</a>)</cite>,leaf
]
]
]
[Privacy Protection, l2node,
[Privacy-Preserving Techniques, l3node,
[

Â <span id="S3.F1.1.2.32" class="ltx_text ltx_font_italic">DP-FTRL</span>Â <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a href="#bib.bib188" title="" class="ltx_ref">2023</a>)</cite>,
<span id="S3.F1.1.2.33" class="ltx_text ltx_font_italic">DP-LoRA</span>Â <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib107" title="" class="ltx_ref">2023c</a>)</cite>
,
leaf
]
]
[Privacy Attack, l3node,
[
Â FILMÂ <cite class="ltx_cite ltx_citemacro_cite">Gupta etÂ al. (<a href="#bib.bib61" title="" class="ltx_ref">2022</a>)</cite>,
DRAÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib209" title="" class="ltx_ref">2024c</a>)</cite>
, leaf
]
]
]
[Attack Robustness, l2node,
[Poisoning Attacks, l3node,
[Â Fed-EBDÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a href="#bib.bib96" title="" class="ltx_ref">2024c</a>)</cite>, leaf
]
]
[Defense Techniques, l3node,
[Â <span id="S3.F1.1.2.34" class="ltx_text ltx_font_italic">ClippedClustering</span>Â <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a href="#bib.bib91" title="" class="ltx_ref">2023b</a>)</cite>, <span id="S3.F1.1.2.35" class="ltx_text ltx_font_italic">Fed-FA</span>Â <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib207" title="" class="ltx_ref">2023d</a>)</cite>, leaf
]
]
]
]
]</p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Taxonomy of research in foundation models with federated learning.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Techniques</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Recent work has begun to address challenges associated with adapting pre-trained FMs to specific downstream tasks in FL settings. In this section, we survey FM-FL techniques on three aspects, namely <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">efficiency</span>Â (SectionÂ <a href="#S4.SS1" title="4.1 Efficiency â€£ 4 Techniques â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>), <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">adaptability</span>Â (SectionÂ <a href="#S4.SS2" title="4.2 Adaptability â€£ 4 Techniques â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>), and <span id="S4.p1.1.3" class="ltx_text ltx_font_italic">trustworthiness</span>Â (SectionÂ <a href="#S4.SS3" title="4.3 Trustworthiness â€£ 4 Techniques â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>). As illustrated in FigureÂ <a href="#S3.F1" title="Figure 1 â€£ Trustworthiness Challenges â€£ 3.2 Core Challenges â€£ 3 FM-FL: Motivation &amp; Challenges â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we further refine them according to the key features of different methods.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Efficiency</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">There has been a considerable focus on developing resource-efficient approaches. This part describes techniques that improve resource efficiency.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Parameter-Efficient Fine-Tuning</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">Federated Parameter-Efficient Fine-Tuning (FedPEFT), originating from the fine-tuning practices of FMsÂ <cite class="ltx_cite ltx_citemacro_cite">Lester etÂ al. (<a href="#bib.bib84" title="" class="ltx_ref">2021</a>); Hu etÂ al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>); Li and Liang (<a href="#bib.bib97" title="" class="ltx_ref">2021</a>)</cite>, is a suite of techniques designed to reduce both the computational load and the associated communication overheadsÂ <cite class="ltx_cite ltx_citemacro_cite">Malaviya etÂ al. (<a href="#bib.bib113" title="" class="ltx_ref">2023</a>); WoisetschlÃ¤ger etÂ al. (<a href="#bib.bib175" title="" class="ltx_ref">2024</a>)</cite>.
In alignment with existing FM fine-tuning taxonomiesÂ <cite class="ltx_cite ltx_citemacro_cite">Lialin etÂ al. (<a href="#bib.bib99" title="" class="ltx_ref">2023</a>); Ding etÂ al. (<a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>, we present FedPEFT methods in three categories: <span id="S4.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">selective methods</span>, <span id="S4.SS1.SSS1.p1.1.2" class="ltx_text ltx_font_italic">additive methods</span>, and <span id="S4.SS1.SSS1.p1.1.3" class="ltx_text ltx_font_italic">reparameterization-based methods</span>.</p>
</div>
<section id="S4.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Selective Methods</h5>

<div id="S4.SS1.SSS1.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.Px1.p1.2" class="ltx_p">Selective methods fine-tune a small subset of the parameters, leaving the majority unchanged. In the field of LLMs, a prominent example of such methods is BitFitÂ <cite class="ltx_cite ltx_citemacro_cite">BenÂ Zaken etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>, which only fine-tunes the bias terms. BitFit has inspired a series of studies in FedPEFTÂ <cite class="ltx_cite ltx_citemacro_cite">Bu etÂ al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>); Sun etÂ al. (<a href="#bib.bib155" title="" class="ltx_ref">2022a</a>); Zhang etÂ al. (<a href="#bib.bib210" title="" class="ltx_ref">2023f</a>)</cite>,
demonstrating the superior communication efficiency of only updating the bias terms while still achieving competitive performance. More sophisticated methods strive to find sparse subnetworks for partial fine-tuning. Among them, various methods <cite class="ltx_cite ltx_citemacro_cite">Seo etÂ al. (<a href="#bib.bib145" title="" class="ltx_ref">2021</a>); Li etÂ al. (<a href="#bib.bib85" title="" class="ltx_ref">2021a</a>); Tamirisa etÂ al. (<a href="#bib.bib160" title="" class="ltx_ref">2024</a>)</cite> advocate for the Lottery Ticket Hypothesis (LTH) <cite class="ltx_cite ltx_citemacro_cite">Frankle and Carbin (<a href="#bib.bib48" title="" class="ltx_ref">2019</a>)</cite>, positing that a dense network contains many subnetworks whose inference capabilities are as accurate as that of the original network. FedSelect <cite class="ltx_cite ltx_citemacro_cite">Tamirisa etÂ al. (<a href="#bib.bib160" title="" class="ltx_ref">2024</a>)</cite> is a representative method that encourages clients to find optimal subnetworks based on LTH and continually fine-tunes these derived subnetworks to encapsulate local knowledge. As another important aspect, RaFFM <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib198" title="" class="ltx_ref">2023c</a>)</cite> proposes to prioritize specialized salient parameters by ranking them using salience evaluation metrics such as the <math id="S4.SS1.SSS1.Px1.p1.1.m1.1" class="ltx_Math" alttext="\ell_{1}" display="inline"><semantics id="S4.SS1.SSS1.Px1.p1.1.m1.1a"><msub id="S4.SS1.SSS1.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.2" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1.2.cmml">â„“</mi><mn id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.3" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px1.p1.1.m1.1b"><apply id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1.2">â„“</ci><cn type="integer" id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px1.p1.1.m1.1c">\ell_{1}</annotation></semantics></math> and <math id="S4.SS1.SSS1.Px1.p1.2.m2.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S4.SS1.SSS1.Px1.p1.2.m2.1a"><msub id="S4.SS1.SSS1.Px1.p1.2.m2.1.1" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1.cmml"><mi mathvariant="normal" id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.2" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1.2.cmml">â„“</mi><mn id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.3" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px1.p1.2.m2.1b"><apply id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1.2">â„“</ci><cn type="integer" id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px1.p1.2.m2.1c">\ell_{2}</annotation></semantics></math> norms.</p>
</div>
</section>
<section id="S4.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Additive Methods</h5>

<div id="S4.SS1.SSS1.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS1.Px2.p1.1" class="ltx_p">Instead of fine-tuning a subset of model parameters, additive methods incorporate lightweight trainable blocks into frozen FMs and tune the additional parameters for model adaptation. These methods not only enhance computational and communicational efficiency but also introduce an extra benefit: personalizationÂ <cite class="ltx_cite ltx_citemacro_cite">Lu etÂ al. (<a href="#bib.bib110" title="" class="ltx_ref">2023a</a>)</cite>, <em id="S4.SS1.SSS1.Px2.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> the integration of these supplementary parameters allows for the customization of heterogeneous models tailored to specific local data characteristics or user preferences. Key branches within additive methods include <span id="S4.SS1.SSS1.Px2.p1.1.2" class="ltx_text ltx_font_italic">adapter tuning</span> and <span id="S4.SS1.SSS1.Px2.p1.1.3" class="ltx_text ltx_font_italic">prompt tuning</span>. Adapter tuning integrates small-scale neural networks (known as â€œadaptersâ€) into the pre-trained modelsÂ <cite class="ltx_cite ltx_citemacro_cite">Houlsby etÂ al. (<a href="#bib.bib66" title="" class="ltx_ref">2019</a>); Hu etÂ al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite>. On the other hand, prompt tuning incorporates trainable task-specific continuous prompt vectors at the input layerÂ <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib104" title="" class="ltx_ref">2023a</a>); Dong etÂ al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>. More details on these methods are provided in AppendixÂ <a href="#A1" title="Appendix A Additional Details of Adapter Tuning â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section id="S4.SS1.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Reparameterization-based Methods</h5>

<div id="S4.SS1.SSS1.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS1.Px3.p1.13" class="ltx_p">The hypothesis behind reparameterization-based methods is that fine-tuning adaptations can be re-parameterized into optimization within low-rank subspacesÂ <cite class="ltx_cite ltx_citemacro_cite">Aghajanyan etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite>. Low-Rank Adaptation (LoRA)Â <cite class="ltx_cite ltx_citemacro_cite">Hu etÂ al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite>, as a popular PEFT method from the area of LLMs, reduces the number of trainable parameters for downstream tasks by representing the weight updates with two smaller matrices (called update matrices) through low-rank decompositionÂ <cite class="ltx_cite ltx_citemacro_cite">Ding etÂ al. (<a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>. When optimizing a parameter matrix <math id="S4.SS1.SSS1.Px3.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{W}\in\mathbb{R}^{m\times n}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.1.m1.1a"><mrow id="S4.SS1.SSS1.Px3.p1.1.m1.1.1" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.2" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.2.cmml">ğ–</mi><mo id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.1" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.2" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.2" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.1" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.3" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.3.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.1.m1.1b"><apply id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1"><in id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.1"></in><ci id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.2">ğ–</ci><apply id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.2">â„</ci><apply id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3"><times id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.1"></times><ci id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.2">ğ‘š</ci><ci id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.3">ğ‘›</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.1.m1.1c">\mathbf{W}\in\mathbb{R}^{m\times n}</annotation></semantics></math>, the update equation can be written as: <math id="S4.SS1.SSS1.Px3.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{W}\leftarrow\mathbf{W}+\Delta\mathbf{W}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.2.m2.1a"><mrow id="S4.SS1.SSS1.Px3.p1.2.m2.1.1" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.2" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.2.cmml">ğ–</mi><mo stretchy="false" id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.1" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.1.cmml">â†</mo><mrow id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.2" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.2.cmml">ğ–</mi><mo id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.1" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.1.cmml">+</mo><mrow id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.cmml"><mi mathvariant="normal" id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.2" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.1" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.3" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.3.cmml">ğ–</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.2.m2.1b"><apply id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1"><ci id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.1">â†</ci><ci id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.2">ğ–</ci><apply id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3"><plus id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.1"></plus><ci id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.2">ğ–</ci><apply id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3"><times id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.1"></times><ci id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.2">Î”</ci><ci id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.3">ğ–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.2.m2.1c">\mathbf{W}\leftarrow\mathbf{W}+\Delta\mathbf{W}</annotation></semantics></math>.
The core idea of LoRA is to freeze the original matrix <math id="S4.SS1.SSS1.Px3.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{W}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.3.m3.1a"><mi id="S4.SS1.SSS1.Px3.p1.3.m3.1.1" xref="S4.SS1.SSS1.Px3.p1.3.m3.1.1.cmml">ğ–</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.3.m3.1b"><ci id="S4.SS1.SSS1.Px3.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.3.m3.1.1">ğ–</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.3.m3.1c">\mathbf{W}</annotation></semantics></math> while approximating the parameter update <math id="S4.SS1.SSS1.Px3.p1.4.m4.1" class="ltx_Math" alttext="\Delta\mathbf{W}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.4.m4.1a"><mrow id="S4.SS1.SSS1.Px3.p1.4.m4.1.1" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.cmml"><mi mathvariant="normal" id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.2" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.1" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.3" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.3.cmml">ğ–</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.4.m4.1b"><apply id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1"><times id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.1"></times><ci id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.2">Î”</ci><ci id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.3">ğ–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.4.m4.1c">\Delta\mathbf{W}</annotation></semantics></math> by low-rank decomposition matrices, <em id="S4.SS1.SSS1.Px3.p1.13.1" class="ltx_emph ltx_font_italic">i.e.,</em> <math id="S4.SS1.SSS1.Px3.p1.5.m5.1" class="ltx_Math" alttext="\Delta\mathbf{W}=\mathbf{A}\cdot\mathbf{B}^{\top}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.5.m5.1a"><mrow id="S4.SS1.SSS1.Px3.p1.5.m5.1.1" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.cmml"><mrow id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.cmml"><mi mathvariant="normal" id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.2" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.1" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.3" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.3.cmml">ğ–</mi></mrow><mo id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.1" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.1.cmml">=</mo><mrow id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.2" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.2.cmml">ğ€</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.1" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.1.cmml">â‹…</mo><msup id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.2" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.2.cmml">ğ</mi><mo id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.3" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.3.cmml">âŠ¤</mo></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.5.m5.1b"><apply id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1"><eq id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.1"></eq><apply id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2"><times id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.1.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.1"></times><ci id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.2.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.2">Î”</ci><ci id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.3.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.3">ğ–</ci></apply><apply id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3"><ci id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.1">â‹…</ci><ci id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.2">ğ€</ci><apply id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3">superscript</csymbol><ci id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.2">ğ</ci><csymbol cd="latexml" id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.3">top</csymbol></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.5.m5.1c">\Delta\mathbf{W}=\mathbf{A}\cdot\mathbf{B}^{\top}</annotation></semantics></math>, where <math id="S4.SS1.SSS1.Px3.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{A}\in\mathbb{R}^{m\times k}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.6.m6.1a"><mrow id="S4.SS1.SSS1.Px3.p1.6.m6.1.1" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.2" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.2.cmml">ğ€</mi><mo id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.1" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.1.cmml">âˆˆ</mo><msup id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.2" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.2.cmml">â„</mi><mrow id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.2" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.1" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.1.cmml">Ã—</mo><mi id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.3" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.6.m6.1b"><apply id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1"><in id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.1"></in><ci id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.2">ğ€</ci><apply id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.2">â„</ci><apply id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3"><times id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.1"></times><ci id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.2">ğ‘š</ci><ci id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.3">ğ‘˜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.6.m6.1c">\mathbf{A}\in\mathbb{R}^{m\times k}</annotation></semantics></math> and <math id="S4.SS1.SSS1.Px3.p1.7.m7.1" class="ltx_Math" alttext="\mathbf{B}\in\mathbb{R}^{n\times k}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.7.m7.1a"><mrow id="S4.SS1.SSS1.Px3.p1.7.m7.1.1" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2.cmml">ğ</mi><mo id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.1" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.1.cmml">âˆˆ</mo><msup id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.2" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.2.cmml">â„</mi><mrow id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.2" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.1" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.1.cmml">Ã—</mo><mi id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.3" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.7.m7.1b"><apply id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1"><in id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.1"></in><ci id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2">ğ</ci><apply id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.2">â„</ci><apply id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3"><times id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.1"></times><ci id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.2">ğ‘›</ci><ci id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.3">ğ‘˜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.7.m7.1c">\mathbf{B}\in\mathbb{R}^{n\times k}</annotation></semantics></math> are the trainable parameters for task adaptation and <math id="S4.SS1.SSS1.Px3.p1.8.m8.3" class="ltx_Math" alttext="k\ll\min(m,n)" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.8.m8.3a"><mrow id="S4.SS1.SSS1.Px3.p1.8.m8.3.4" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.cmml"><mi id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.2" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.2.cmml">k</mi><mo id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.1" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.1.cmml">â‰ª</mo><mrow id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.2" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.8.m8.1.1" xref="S4.SS1.SSS1.Px3.p1.8.m8.1.1.cmml">min</mi><mo id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.2a" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.1.cmml">â¡</mo><mrow id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.2.1" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.1.cmml"><mo stretchy="false" id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.2.1.1" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.1.cmml">(</mo><mi id="S4.SS1.SSS1.Px3.p1.8.m8.2.2" xref="S4.SS1.SSS1.Px3.p1.8.m8.2.2.cmml">m</mi><mo id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.2.1.2" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.1.cmml">,</mo><mi id="S4.SS1.SSS1.Px3.p1.8.m8.3.3" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.3.cmml">n</mi><mo stretchy="false" id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.2.1.3" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.8.m8.3b"><apply id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4"><csymbol cd="latexml" id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.1.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.1">much-less-than</csymbol><ci id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.2.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.2">ğ‘˜</ci><apply id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.2"><min id="S4.SS1.SSS1.Px3.p1.8.m8.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.1.1"></min><ci id="S4.SS1.SSS1.Px3.p1.8.m8.2.2.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.2.2">ğ‘š</ci><ci id="S4.SS1.SSS1.Px3.p1.8.m8.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.3">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.8.m8.3c">k\ll\min(m,n)</annotation></semantics></math> is the reduced rank. The trainable parameter size is then reduced from <math id="S4.SS1.SSS1.Px3.p1.9.m9.1" class="ltx_Math" alttext="mn" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.9.m9.1a"><mrow id="S4.SS1.SSS1.Px3.p1.9.m9.1.1" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.1" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.3" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.9.m9.1b"><apply id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1"><times id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.1"></times><ci id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2">ğ‘š</ci><ci id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.9.m9.1c">mn</annotation></semantics></math> to <math id="S4.SS1.SSS1.Px3.p1.10.m10.1" class="ltx_Math" alttext="k(m+n)" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.10.m10.1a"><mrow id="S4.SS1.SSS1.Px3.p1.10.m10.1.1" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.3" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.3.cmml">k</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.2" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.2.cmml">â€‹</mo><mrow id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.2" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.2" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.2.cmml">m</mi><mo id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.1" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.1.cmml">+</mo><mi id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.3" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.3.cmml">n</mi></mrow><mo stretchy="false" id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.3" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.10.m10.1b"><apply id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1"><times id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.2"></times><ci id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.3">ğ‘˜</ci><apply id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1"><plus id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.1"></plus><ci id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.2">ğ‘š</ci><ci id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.3">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.10.m10.1c">k(m+n)</annotation></semantics></math>. The major benefit of LoRA is that it can largely save memory and storage usage. A straightforward way to perform federated finetuning with LoRA is to train the LoRA modules <math id="S4.SS1.SSS1.Px3.p1.11.m11.1" class="ltx_Math" alttext="\mathbf{A}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.11.m11.1a"><mi id="S4.SS1.SSS1.Px3.p1.11.m11.1.1" xref="S4.SS1.SSS1.Px3.p1.11.m11.1.1.cmml">ğ€</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.11.m11.1b"><ci id="S4.SS1.SSS1.Px3.p1.11.m11.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.11.m11.1.1">ğ€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.11.m11.1c">\mathbf{A}</annotation></semantics></math> and <math id="S4.SS1.SSS1.Px3.p1.12.m12.1" class="ltx_Math" alttext="\mathbf{B}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.12.m12.1a"><mi id="S4.SS1.SSS1.Px3.p1.12.m12.1.1" xref="S4.SS1.SSS1.Px3.p1.12.m12.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.12.m12.1b"><ci id="S4.SS1.SSS1.Px3.p1.12.m12.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.12.m12.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.12.m12.1c">\mathbf{B}</annotation></semantics></math> with homogeneous rank <math id="S4.SS1.SSS1.Px3.p1.13.m13.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.13.m13.1a"><mi id="S4.SS1.SSS1.Px3.p1.13.m13.1.1" xref="S4.SS1.SSS1.Px3.p1.13.m13.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.13.m13.1b"><ci id="S4.SS1.SSS1.Px3.p1.13.m13.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.13.m13.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.13.m13.1c">k</annotation></semantics></math> across all clients with standard FL such as <span id="S4.SS1.SSS1.Px3.p1.13.2" class="ltx_text ltx_font_typewriter">FedAvg</span>Â <cite class="ltx_cite ltx_citemacro_cite">McMahan etÂ al. (<a href="#bib.bib119" title="" class="ltx_ref">2017</a>)</cite>. Serval studies have shown that this method can achieve an outstanding level of trade-off between performance and communication overhead for a wide range of FMs, including language modelsÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib204" title="" class="ltx_ref">2024b</a>, <a href="#bib.bib210" title="" class="ltx_ref">2023f</a>)</cite>, vision-language modelsÂ <cite class="ltx_cite ltx_citemacro_cite">Nguyen etÂ al. (<a href="#bib.bib121" title="" class="ltx_ref">2024</a>)</cite>, and speech-to-text modelsÂ <cite class="ltx_cite ltx_citemacro_cite">Du etÂ al. (<a href="#bib.bib39" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2406.12844/assets/figs/fedpeft.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="475" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Taxonomy of Federated Parameter-Efficient Fine-Tuning (FedPEFT). Apart from efficiency, some methods also account for other considerations, such as data and resource heterogeneity challenges that are identified in SectionÂ <a href="#S3.SS2.SSS0.Px2" title="Adaptability Challenges â€£ 3.2 Core Challenges â€£ 3 FM-FL: Motivation &amp; Challenges â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> and black-box tuning (see SectionÂ <a href="#S4.SS3" title="4.3 Trustworthiness â€£ 4 Techniques â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>).</figcaption>
</figure>
</section>
<section id="S4.SS1.SSS1.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Comparison of FedPEFT methods</h5>

<div id="S4.SS1.SSS1.Px4.p1" class="ltx_para">
<p id="S4.SS1.SSS1.Px4.p1.1" class="ltx_p">FigureÂ <a href="#S4.F2" title="Figure 2 â€£ Reparameterization-based Methods â€£ 4.1.1 Parameter-Efficient Fine-Tuning â€£ 4.1 Efficiency â€£ 4 Techniques â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> depicts the taxonomy of FedPEFT with representative methods. Note that some methods may belong to multiple overlapping categories. To compare the communication efficiency of different FedPEFT methods, TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.1.2 Model Compression â€£ 4.1 Efficiency â€£ 4 Techniques â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> gives a brief overview of experimental evaluations from representative studies. Compared to full-model fine-tuning, FedPEFT methods only require 0.1%-30% communication overhead. We note that the differences can be attributed to several factors, including model complexity and implementation details.</p>
</div>
</section>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Model Compression</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">Model compression refers to the techniques used to reduce the size of models, thereby improving resource efficiencyÂ <cite class="ltx_cite ltx_citemacro_cite">Shah and Lau (<a href="#bib.bib146" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of Federated Parameter-Efficient Fine-Tuning (FedPEFT) Methods.</figcaption>
<p id="S4.T1.6" class="ltx_p ltx_align_center"><span id="S4.T1.6.6" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S4.T1.6.6.6.6" class="ltx_inline-block ltx_transformed_outer" style="width:1057.9pt;height:348.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S4.T1.6.6.6.6.6" class="ltx_p"><span id="S4.T1.6.6.6.6.6.6" class="ltx_text">
<span id="S4.T1.6.6.6.6.6.6.6" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.6.6.6.6.6.6.6.7" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.1.1" class="ltx_text"></span> <span id="S4.T1.6.6.6.6.6.6.6.7.1.2" class="ltx_text">
<span id="S4.T1.6.6.6.6.6.6.6.7.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.6.6.6.6.6.6.6.7.1.2.1.1" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Category</span></span></span>
</span></span><span id="S4.T1.6.6.6.6.6.6.6.7.1.3" class="ltx_text"></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.2.1" class="ltx_text ltx_font_bold">Representative Work</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.3.1" class="ltx_text ltx_font_bold">Modality</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.4.1" class="ltx_text ltx_font_bold">Model</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.5.1" class="ltx_text"></span> <span id="S4.T1.6.6.6.6.6.6.6.7.5.2" class="ltx_text">
<span id="S4.T1.6.6.6.6.6.6.6.7.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.6.6.6.6.6.6.6.7.5.2.1.1" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.5.2.1.1.1.1" class="ltx_text ltx_font_bold"># Full</span></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.5.2.1.2" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.5.2.1.2.1.1" class="ltx_text ltx_font_bold">Params.</span></span></span>
</span></span><span id="S4.T1.6.6.6.6.6.6.6.7.5.3" class="ltx_text"></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.6.1" class="ltx_text"></span> <span id="S4.T1.6.6.6.6.6.6.6.7.6.2" class="ltx_text">
<span id="S4.T1.6.6.6.6.6.6.6.7.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.6.6.6.6.6.6.6.7.6.2.1.1" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.6.2.1.1.1.1" class="ltx_text ltx_font_bold"># Train.</span></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.6.2.1.2" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.6.2.1.2.1.1" class="ltx_text ltx_font_bold">Params.</span></span></span>
</span></span><span id="S4.T1.6.6.6.6.6.6.6.7.6.3" class="ltx_text"></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.7.1" class="ltx_text"></span> <span id="S4.T1.6.6.6.6.6.6.6.7.7.2" class="ltx_text">
<span id="S4.T1.6.6.6.6.6.6.6.7.7.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.6.6.6.6.6.6.6.7.7.2.1.1" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.7.2.1.1.1.1" class="ltx_text ltx_font_bold">Training</span></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.7.2.1.2" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.7.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.7.2.1.2.1.1" class="ltx_text ltx_font_bold">Accel.</span></span></span>
</span></span><span id="S4.T1.6.6.6.6.6.6.6.7.7.3" class="ltx_text"></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.8.1" class="ltx_text"></span> <span id="S4.T1.6.6.6.6.6.6.6.7.8.2" class="ltx_text">
<span id="S4.T1.6.6.6.6.6.6.6.7.8.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.6.6.6.6.6.6.6.7.8.2.1.1" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.8.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.8.2.1.1.1.1" class="ltx_text ltx_font_bold">Comm.</span></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.8.2.1.2" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.8.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.8.2.1.2.1.1" class="ltx_text ltx_font_bold">Cost</span></span></span>
</span></span><span id="S4.T1.6.6.6.6.6.6.6.7.8.3" class="ltx_text"></span></span></span>
<span id="S4.T1.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2 ltx_colspan ltx_colspan_2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.1.1.1.1.1.1.1.1.2.1" class="ltx_text"><span id="S4.T1.1.1.1.1.1.1.1.1.2.1.1" class="ltx_text"></span> <span id="S4.T1.1.1.1.1.1.1.1.1.2.1.2" class="ltx_text">
<span id="S4.T1.1.1.1.1.1.1.1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.1.1.1.1.1.1.1.1.2.1.2.1.1" class="ltx_tr">
<span id="S4.T1.1.1.1.1.1.1.1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.1.1.1.1.1.1.1.1.2.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Selective</span></span></span>
</span></span> <span id="S4.T1.1.1.1.1.1.1.1.1.2.1.3" class="ltx_text"></span></span></span>
<span id="S4.T1.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">RaFFMÂ <cite class="ltx_cite ltx_citemacro_cite">Yu etÂ al. (<a href="#bib.bib198" title="" class="ltx_ref">2023c</a>)</cite></span>
<span id="S4.T1.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BERT-LargeÂ <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite></span>
<span id="S4.T1.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">336M</span>
<span id="S4.T1.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">100M</span>
<span id="S4.T1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_math_unparsed" alttext="6.13\times" display="inline"><semantics id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1a"><mrow id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1b"><mn id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1.1">6.13</mn><mo lspace="0.222em" id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1.2">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1c">6.13\times</annotation></semantics></math></span>
<span id="S4.T1.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">29.8%</span></span>
<span id="S4.T1.2.2.2.2.2.2.2.2" class="ltx_tr">
<span id="S4.T1.2.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedBFÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib210" title="" class="ltx_ref">2023f</a>)</cite></span>
<span id="S4.T1.2.2.2.2.2.2.2.2.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.2.2.2.2.2.2.2.2.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Roberta-BaseÂ <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib109" title="" class="ltx_ref">2019</a>)</cite></span>
<span id="S4.T1.2.2.2.2.2.2.2.2.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">125M</span>
<span id="S4.T1.2.2.2.2.2.2.2.2.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.66M</span>
<span id="S4.T1.2.2.2.2.2.2.2.2.7" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.2.2.2.2.2.2.2.2.1" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="1.6\%" display="inline"><semantics id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1a"><mrow id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1" xref="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.cmml"><mn id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.2" xref="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.2.cmml">1.6</mn><mo id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.1" xref="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1b"><apply id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.1.cmml" xref="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.2.cmml" xref="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.2">1.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1c">1.6\%</annotation></semantics></math></span></span>
<span id="S4.T1.3.3.3.3.3.3.3.3" class="ltx_tr">
<span id="S4.T1.3.3.3.3.3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_10" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.3.3.3.3.3.3.3.3.2.1" class="ltx_text"><span id="S4.T1.3.3.3.3.3.3.3.3.2.1.1" class="ltx_text"></span> <span id="S4.T1.3.3.3.3.3.3.3.3.2.1.2" class="ltx_text">
<span id="S4.T1.3.3.3.3.3.3.3.3.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.3.3.3.3.3.3.3.3.2.1.2.1.1" class="ltx_tr">
<span id="S4.T1.3.3.3.3.3.3.3.3.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.3.3.3.3.3.3.3.3.2.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Additive</span></span></span>
</span></span> <span id="S4.T1.3.3.3.3.3.3.3.3.2.1.3" class="ltx_text"></span></span></span>
<span id="S4.T1.3.3.3.3.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_6" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.3.3.3.3.3.3.3.3.3.1" class="ltx_text"><span id="S4.T1.3.3.3.3.3.3.3.3.3.1.1" class="ltx_text"></span> <span id="S4.T1.3.3.3.3.3.3.3.3.3.1.2" class="ltx_text">
<span id="S4.T1.3.3.3.3.3.3.3.3.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.3.3.3.3.3.3.3.3.3.1.2.1.1" class="ltx_tr">
<span id="S4.T1.3.3.3.3.3.3.3.3.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.3.3.3.3.3.3.3.3.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Adapter</span></span></span>
</span></span> <span id="S4.T1.3.3.3.3.3.3.3.3.3.1.3" class="ltx_text"></span></span></span>
<span id="S4.T1.3.3.3.3.3.3.3.3.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedAPÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib210" title="" class="ltx_ref">2023f</a>)</cite></span>
<span id="S4.T1.3.3.3.3.3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.3.3.3.3.3.3.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Roberta-BaseÂ <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib109" title="" class="ltx_ref">2019</a>)</cite></span>
<span id="S4.T1.3.3.3.3.3.3.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">125M</span>
<span id="S4.T1.3.3.3.3.3.3.3.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2M</span>
<span id="S4.T1.3.3.3.3.3.3.3.3.9" class="ltx_td ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.3.3.3.3.3.3.3.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1" class="ltx_Math" alttext="1.6\%" display="inline"><semantics id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1a"><mrow id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1" xref="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.cmml"><mn id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.2" xref="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.2.cmml">1.6</mn><mo id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.1" xref="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1b"><apply id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.1.cmml" xref="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.2.cmml" xref="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.2">1.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1c">1.6\%</annotation></semantics></math></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.8" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.8.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedCLIPÂ <cite class="ltx_cite ltx_citemacro_cite">Lu etÂ al. (<a href="#bib.bib110" title="" class="ltx_ref">2023a</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.8.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Vis.-Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.8.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">ViT-B/32Â <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib176" title="" class="ltx_ref">2020a</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.8.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">150M</span>
<span id="S4.T1.6.6.6.6.6.6.6.8.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.53M</span>
<span id="S4.T1.6.6.6.6.6.6.6.8.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.8.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.5%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.9" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.9.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedDATÂ <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2024</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.9.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Vis.-Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.9.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">ALBEFÂ <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib87" title="" class="ltx_ref">2021b</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.9.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">290M</span>
<span id="S4.T1.6.6.6.6.6.6.6.9.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.86M</span>
<span id="S4.T1.6.6.6.6.6.6.6.9.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.9.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">9.9%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.10" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.10.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">C2AÂ <cite class="ltx_cite ltx_citemacro_cite">Kim etÂ al. (<a href="#bib.bib81" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.10.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.10.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">DistilBERTÂ <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib144" title="" class="ltx_ref">2020</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.10.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">66M</span>
<span id="S4.T1.6.6.6.6.6.6.6.10.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.06M</span>
<span id="S4.T1.6.6.6.6.6.6.6.10.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.10.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.1%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.11" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.11.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.11.1.1" class="ltx_text ltx_font_italic">Fed-MNMT</span>Â <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib108" title="" class="ltx_ref">2023d</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.11.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.11.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">mBART-50Â <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib162" title="" class="ltx_ref">2020</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.11.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">611M</span>
<span id="S4.T1.6.6.6.6.6.6.6.11.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">8M</span>
<span id="S4.T1.6.6.6.6.6.6.6.11.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.11.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.3%</span></span>
<span id="S4.T1.4.4.4.4.4.4.4.4" class="ltx_tr">
<span id="S4.T1.4.4.4.4.4.4.4.4.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">AdaFLÂ <cite class="ltx_cite ltx_citemacro_cite">Cai etÂ al. (<a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S4.T1.4.4.4.4.4.4.4.4.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.4.4.4.4.4.4.4.4.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">BERTÂ <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite></span>
<span id="S4.T1.4.4.4.4.4.4.4.4.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">110M</span>
<span id="S4.T1.4.4.4.4.4.4.4.4.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.61M</span>
<span id="S4.T1.4.4.4.4.4.4.4.4.1" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math id="S4.T1.4.4.4.4.4.4.4.4.1.m1.1" class="ltx_math_unparsed" alttext="1.63\times" display="inline"><semantics id="S4.T1.4.4.4.4.4.4.4.4.1.m1.1a"><mrow id="S4.T1.4.4.4.4.4.4.4.4.1.m1.1b"><mn id="S4.T1.4.4.4.4.4.4.4.4.1.m1.1.1">1.63</mn><mo lspace="0.222em" id="S4.T1.4.4.4.4.4.4.4.4.1.m1.1.2">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S4.T1.4.4.4.4.4.4.4.4.1.m1.1c">1.63\times</annotation></semantics></math></span>
<span id="S4.T1.4.4.4.4.4.4.4.4.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.6%</span></span>
<span id="S4.T1.5.5.5.5.5.5.5.5" class="ltx_tr">
<span id="S4.T1.5.5.5.5.5.5.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.5.5.5.5.5.5.5.5.2.1" class="ltx_text"><span id="S4.T1.5.5.5.5.5.5.5.5.2.1.1" class="ltx_text"></span> <span id="S4.T1.5.5.5.5.5.5.5.5.2.1.2" class="ltx_text">
<span id="S4.T1.5.5.5.5.5.5.5.5.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.5.5.5.5.5.5.5.5.2.1.2.1.1" class="ltx_tr">
<span id="S4.T1.5.5.5.5.5.5.5.5.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.5.5.5.5.5.5.5.5.2.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Prompt</span></span></span>
</span></span> <span id="S4.T1.5.5.5.5.5.5.5.5.2.1.3" class="ltx_text"></span></span></span>
<span id="S4.T1.5.5.5.5.5.5.5.5.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">PromptFLÂ <cite class="ltx_cite ltx_citemacro_cite">Guo etÂ al. (<a href="#bib.bib58" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S4.T1.5.5.5.5.5.5.5.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Vis.-Txt.</span>
<span id="S4.T1.5.5.5.5.5.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">ViT-B/16Â <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite></span>
<span id="S4.T1.5.5.5.5.5.5.5.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">87M</span>
<span id="S4.T1.5.5.5.5.5.5.5.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.87M</span>
<span id="S4.T1.5.5.5.5.5.5.5.5.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1" class="ltx_math_unparsed" alttext="2.38\times" display="inline"><semantics id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1a"><mrow id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1b"><mn id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1.1">2.38</mn><mo lspace="0.222em" id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1.2">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1c">2.38\times</annotation></semantics></math></span>
<span id="S4.T1.5.5.5.5.5.5.5.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.9%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.12" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.12.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">MFPTÂ <cite class="ltx_cite ltx_citemacro_cite">Zhao etÂ al. (<a href="#bib.bib212" title="" class="ltx_ref">2024b</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.12.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.12.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">XLM-RoBERTaÂ <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.12.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">270M</span>
<span id="S4.T1.6.6.6.6.6.6.6.12.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.2M</span>
<span id="S4.T1.6.6.6.6.6.6.6.12.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.12.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.4%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.13" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.13.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedAPTÂ <cite class="ltx_cite ltx_citemacro_cite">Su etÂ al. (<a href="#bib.bib153" title="" class="ltx_ref">2024</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.13.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Vis.-Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.13.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">ViT-B/32Â <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib176" title="" class="ltx_ref">2020a</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.13.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">88M</span>
<span id="S4.T1.6.6.6.6.6.6.6.13.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.8M</span>
<span id="S4.T1.6.6.6.6.6.6.6.13.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.13.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.2%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.14" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.14.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.14.1.1" class="ltx_text ltx_font_italic">FedSP</span>Â <cite class="ltx_cite ltx_citemacro_cite">Dong etÂ al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.14.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.14.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">GPT2-XLÂ <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib133" title="" class="ltx_ref">2019</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.14.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.6B</span>
<span id="S4.T1.6.6.6.6.6.6.6.14.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">111M</span>
<span id="S4.T1.6.6.6.6.6.6.6.14.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.14.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.5%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.6" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_5 ltx_colspan ltx_colspan_2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.6.2.1" class="ltx_text"><span id="S4.T1.6.6.6.6.6.6.6.6.2.1.1" class="ltx_text"></span> <span id="S4.T1.6.6.6.6.6.6.6.6.2.1.2" class="ltx_text">
<span id="S4.T1.6.6.6.6.6.6.6.6.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.6.6.6.6.6.6.6.6.2.1.2.1.1" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.6.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.6.2.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Reparameterization-based</span></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.6.2.1.2.1.2" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.6.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.6.2.1.2.1.2.1.1" class="ltx_text ltx_font_bold">Methods</span></span></span>
</span></span> <span id="S4.T1.6.6.6.6.6.6.6.6.2.1.3" class="ltx_text"></span></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.6.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">SLoRAÂ <cite class="ltx_cite ltx_citemacro_cite">Babakniya etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2023a</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">DistilBERTÂ <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib144" title="" class="ltx_ref">2020</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">67M</span>
<span id="S4.T1.6.6.6.6.6.6.6.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.7M</span>
<span id="S4.T1.6.6.6.6.6.6.6.6.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math id="S4.T1.6.6.6.6.6.6.6.6.1.m1.1" class="ltx_math_unparsed" alttext="13.47\times" display="inline"><semantics id="S4.T1.6.6.6.6.6.6.6.6.1.m1.1a"><mrow id="S4.T1.6.6.6.6.6.6.6.6.1.m1.1b"><mn id="S4.T1.6.6.6.6.6.6.6.6.1.m1.1.1">13.47</mn><mo lspace="0.222em" id="S4.T1.6.6.6.6.6.6.6.6.1.m1.1.2">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S4.T1.6.6.6.6.6.6.6.6.1.m1.1c">13.47\times</annotation></semantics></math></span>
<span id="S4.T1.6.6.6.6.6.6.6.6.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.8%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.15" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.15.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">LP-FLÂ <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a href="#bib.bib73" title="" class="ltx_ref">2023a</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.15.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.15.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">BERT-LargeÂ <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.15.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">336M</span>
<span id="S4.T1.6.6.6.6.6.6.6.15.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">100M</span>
<span id="S4.T1.6.6.6.6.6.6.6.15.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.15.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">30%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.16" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.16.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedMSÂ <cite class="ltx_cite ltx_citemacro_cite">Wu etÂ al. (<a href="#bib.bib179" title="" class="ltx_ref">2023c</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.16.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Vis.-Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.16.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">ViT-B/16Â <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.16.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">87M</span>
<span id="S4.T1.6.6.6.6.6.6.6.16.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.6M</span>
<span id="S4.T1.6.6.6.6.6.6.6.16.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.16.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">10%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.17" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.17.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">pFedS2TÂ <cite class="ltx_cite ltx_citemacro_cite">Du etÂ al. (<a href="#bib.bib39" title="" class="ltx_ref">2024</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.17.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Aud.</span>
<span id="S4.T1.6.6.6.6.6.6.6.17.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">WhisperÂ <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib132" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.17.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">254M</span>
<span id="S4.T1.6.6.6.6.6.6.6.17.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">10.1M</span>
<span id="S4.T1.6.6.6.6.6.6.6.17.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.17.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">4%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.18" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.18.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">FFA-LoRAÂ <cite class="ltx_cite ltx_citemacro_cite">Sun etÂ al. (<a href="#bib.bib159" title="" class="ltx_ref">2024b</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.18.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.18.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">RoBERTa-LargeÂ <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib109" title="" class="ltx_ref">2019</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.18.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">355M</span>
<span id="S4.T1.6.6.6.6.6.6.6.18.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.39M</span>
<span id="S4.T1.6.6.6.6.6.6.6.18.6" class="ltx_td ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.18.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.1%</span></span>
</span></span></span>
</span></span></span></p>
</figure>
<section id="S4.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Sparsification</h5>

<div id="S4.SS1.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS2.Px1.p1.1" class="ltx_p">Model sparsification methods reduce communication burden by only transmitting a subset of FM parameters across the networkÂ <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a href="#bib.bib75" title="" class="ltx_ref">2023c</a>)</cite>. Typical methods focus on identifying and cultivating high-potential subnetworksÂ <cite class="ltx_cite ltx_citemacro_cite">Frankle and Carbin (<a href="#bib.bib48" title="" class="ltx_ref">2019</a>); Tsouvalas etÂ al. (<a href="#bib.bib168" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS1.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Quantization</h5>

<div id="S4.SS1.SSS2.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.Px2.p1.1" class="ltx_p">Quantization is well-established in both the FM and FL domainsÂ <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a href="#bib.bib186" title="" class="ltx_ref">2024b</a>); Reisizadeh etÂ al. (<a href="#bib.bib138" title="" class="ltx_ref">2020</a>)</cite>, which involves decreasing the precision of floating-point parameters for mitigating the storage, computational, and communication demands. Quantization is orthogonal to other resource-efficient techniques, making it feasible to combine them for greater efficiency and flexibilityÂ <cite class="ltx_cite ltx_citemacro_cite">Lit etÂ al. (<a href="#bib.bib103" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Zeroth-Order Optimization</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">In contrast to the use of gradient descent in most FL optimization algorithms, a particular line of research advocates for the removal of BackPropagationÂ (BP) <cite class="ltx_cite ltx_citemacro_cite">Malladi etÂ al. (<a href="#bib.bib114" title="" class="ltx_ref">2023a</a>)</cite> in favor of Zeroth-Order Optimization (ZOO) <cite class="ltx_cite ltx_citemacro_cite">Fang etÂ al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>); Li and Chen (<a href="#bib.bib98" title="" class="ltx_ref">2021</a>)</cite>. BP-free methods conserve memory needed for computing gradients and minimize communication overhead for model aggregation <cite class="ltx_cite ltx_citemacro_cite">Qin etÂ al. (<a href="#bib.bib129" title="" class="ltx_ref">2024</a>)</cite>, making FMs more accessible for lower-end devices, thereby enhancing their applicability in diverse hardware environments.</p>
</div>
<div id="S4.SS1.SSS3.p2" class="ltx_para">
<p id="S4.SS1.SSS3.p2.3" class="ltx_p">ZOO methods primarily rely on perturbation methods to estimate gradients with forward propagation. Given a model with parameters <math id="S4.SS1.SSS3.p2.1.m1.1" class="ltx_Math" alttext="\bm{\theta}\in\mathbb{R}^{d}" display="inline"><semantics id="S4.SS1.SSS3.p2.1.m1.1a"><mrow id="S4.SS1.SSS3.p2.1.m1.1.1" xref="S4.SS1.SSS3.p2.1.m1.1.1.cmml"><mi id="S4.SS1.SSS3.p2.1.m1.1.1.2" xref="S4.SS1.SSS3.p2.1.m1.1.1.2.cmml">ğœ½</mi><mo id="S4.SS1.SSS3.p2.1.m1.1.1.1" xref="S4.SS1.SSS3.p2.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S4.SS1.SSS3.p2.1.m1.1.1.3" xref="S4.SS1.SSS3.p2.1.m1.1.1.3.cmml"><mi id="S4.SS1.SSS3.p2.1.m1.1.1.3.2" xref="S4.SS1.SSS3.p2.1.m1.1.1.3.2.cmml">â„</mi><mi id="S4.SS1.SSS3.p2.1.m1.1.1.3.3" xref="S4.SS1.SSS3.p2.1.m1.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.1.m1.1b"><apply id="S4.SS1.SSS3.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p2.1.m1.1.1"><in id="S4.SS1.SSS3.p2.1.m1.1.1.1.cmml" xref="S4.SS1.SSS3.p2.1.m1.1.1.1"></in><ci id="S4.SS1.SSS3.p2.1.m1.1.1.2.cmml" xref="S4.SS1.SSS3.p2.1.m1.1.1.2">ğœ½</ci><apply id="S4.SS1.SSS3.p2.1.m1.1.1.3.cmml" xref="S4.SS1.SSS3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS3.p2.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS3.p2.1.m1.1.1.3.2.cmml" xref="S4.SS1.SSS3.p2.1.m1.1.1.3.2">â„</ci><ci id="S4.SS1.SSS3.p2.1.m1.1.1.3.3.cmml" xref="S4.SS1.SSS3.p2.1.m1.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.1.m1.1c">\bm{\theta}\in\mathbb{R}^{d}</annotation></semantics></math> and a loss function <math id="S4.SS1.SSS3.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S4.SS1.SSS3.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS3.p2.2.m2.1.1" xref="S4.SS1.SSS3.p2.2.m2.1.1.cmml">â„’</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.2.m2.1b"><ci id="S4.SS1.SSS3.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS3.p2.2.m2.1.1">â„’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.2.m2.1c">\mathcal{L}</annotation></semantics></math>, a typical gradient estimator estimates the gradient on a minibatch <math id="S4.SS1.SSS3.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{B}" display="inline"><semantics id="S4.SS1.SSS3.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS3.p2.3.m3.1.1" xref="S4.SS1.SSS3.p2.3.m3.1.1.cmml">â„¬</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.3.m3.1b"><ci id="S4.SS1.SSS3.p2.3.m3.1.1.cmml" xref="S4.SS1.SSS3.p2.3.m3.1.1">â„¬</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.3.m3.1c">\mathcal{B}</annotation></semantics></math> as</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.7" class="ltx_Math" alttext="\hat{\nabla}\mathcal{L}(\bm{\theta};\mathcal{B})=\frac{\mathcal{L}(\bm{\theta}+\epsilon\bm{z};\mathcal{B})-\mathcal{L}(\bm{\theta};\mathcal{B})}{2\epsilon}\bm{z}," display="block"><semantics id="S4.E2.m1.7a"><mrow id="S4.E2.m1.7.7.1" xref="S4.E2.m1.7.7.1.1.cmml"><mrow id="S4.E2.m1.7.7.1.1" xref="S4.E2.m1.7.7.1.1.cmml"><mrow id="S4.E2.m1.7.7.1.1.2" xref="S4.E2.m1.7.7.1.1.2.cmml"><mover accent="true" id="S4.E2.m1.7.7.1.1.2.2" xref="S4.E2.m1.7.7.1.1.2.2.cmml"><mo id="S4.E2.m1.7.7.1.1.2.2.2" xref="S4.E2.m1.7.7.1.1.2.2.2.cmml">âˆ‡</mo><mo id="S4.E2.m1.7.7.1.1.2.2.1" xref="S4.E2.m1.7.7.1.1.2.2.1.cmml">^</mo></mover><mo lspace="0.167em" rspace="0em" id="S4.E2.m1.7.7.1.1.2.1" xref="S4.E2.m1.7.7.1.1.2.1.cmml">â€‹</mo><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.7.7.1.1.2.3" xref="S4.E2.m1.7.7.1.1.2.3.cmml">â„’</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.7.7.1.1.2.1a" xref="S4.E2.m1.7.7.1.1.2.1.cmml">â€‹</mo><mrow id="S4.E2.m1.7.7.1.1.2.4.2" xref="S4.E2.m1.7.7.1.1.2.4.1.cmml"><mo stretchy="false" id="S4.E2.m1.7.7.1.1.2.4.2.1" xref="S4.E2.m1.7.7.1.1.2.4.1.cmml">(</mo><mi id="S4.E2.m1.5.5" xref="S4.E2.m1.5.5.cmml">ğœ½</mi><mo id="S4.E2.m1.7.7.1.1.2.4.2.2" xref="S4.E2.m1.7.7.1.1.2.4.1.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.6.6" xref="S4.E2.m1.6.6.cmml">â„¬</mi><mo stretchy="false" id="S4.E2.m1.7.7.1.1.2.4.2.3" xref="S4.E2.m1.7.7.1.1.2.4.1.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.7.7.1.1.1" xref="S4.E2.m1.7.7.1.1.1.cmml">=</mo><mrow id="S4.E2.m1.7.7.1.1.3" xref="S4.E2.m1.7.7.1.1.3.cmml"><mfrac id="S4.E2.m1.4.4" xref="S4.E2.m1.4.4.cmml"><mrow id="S4.E2.m1.4.4.4" xref="S4.E2.m1.4.4.4.cmml"><mrow id="S4.E2.m1.4.4.4.4" xref="S4.E2.m1.4.4.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.4.4.4.4.3" xref="S4.E2.m1.4.4.4.4.3.cmml">â„’</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.4.4.4.4.2" xref="S4.E2.m1.4.4.4.4.2.cmml">â€‹</mo><mrow id="S4.E2.m1.4.4.4.4.1.1" xref="S4.E2.m1.4.4.4.4.1.2.cmml"><mo stretchy="false" id="S4.E2.m1.4.4.4.4.1.1.2" xref="S4.E2.m1.4.4.4.4.1.2.cmml">(</mo><mrow id="S4.E2.m1.4.4.4.4.1.1.1" xref="S4.E2.m1.4.4.4.4.1.1.1.cmml"><mi id="S4.E2.m1.4.4.4.4.1.1.1.2" xref="S4.E2.m1.4.4.4.4.1.1.1.2.cmml">ğœ½</mi><mo id="S4.E2.m1.4.4.4.4.1.1.1.1" xref="S4.E2.m1.4.4.4.4.1.1.1.1.cmml">+</mo><mrow id="S4.E2.m1.4.4.4.4.1.1.1.3" xref="S4.E2.m1.4.4.4.4.1.1.1.3.cmml"><mi id="S4.E2.m1.4.4.4.4.1.1.1.3.2" xref="S4.E2.m1.4.4.4.4.1.1.1.3.2.cmml">Ïµ</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.4.4.4.4.1.1.1.3.1" xref="S4.E2.m1.4.4.4.4.1.1.1.3.1.cmml">â€‹</mo><mi id="S4.E2.m1.4.4.4.4.1.1.1.3.3" xref="S4.E2.m1.4.4.4.4.1.1.1.3.3.cmml">ğ’›</mi></mrow></mrow><mo id="S4.E2.m1.4.4.4.4.1.1.3" xref="S4.E2.m1.4.4.4.4.1.2.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml">â„¬</mi><mo stretchy="false" id="S4.E2.m1.4.4.4.4.1.1.4" xref="S4.E2.m1.4.4.4.4.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.4.4.4.5" xref="S4.E2.m1.4.4.4.5.cmml">âˆ’</mo><mrow id="S4.E2.m1.4.4.4.6" xref="S4.E2.m1.4.4.4.6.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.4.4.4.6.2" xref="S4.E2.m1.4.4.4.6.2.cmml">â„’</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.4.4.4.6.1" xref="S4.E2.m1.4.4.4.6.1.cmml">â€‹</mo><mrow id="S4.E2.m1.4.4.4.6.3.2" xref="S4.E2.m1.4.4.4.6.3.1.cmml"><mo stretchy="false" id="S4.E2.m1.4.4.4.6.3.2.1" xref="S4.E2.m1.4.4.4.6.3.1.cmml">(</mo><mi id="S4.E2.m1.2.2.2.2" xref="S4.E2.m1.2.2.2.2.cmml">ğœ½</mi><mo id="S4.E2.m1.4.4.4.6.3.2.2" xref="S4.E2.m1.4.4.4.6.3.1.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.3.3.3.3" xref="S4.E2.m1.3.3.3.3.cmml">â„¬</mi><mo stretchy="false" id="S4.E2.m1.4.4.4.6.3.2.3" xref="S4.E2.m1.4.4.4.6.3.1.cmml">)</mo></mrow></mrow></mrow><mrow id="S4.E2.m1.4.4.6" xref="S4.E2.m1.4.4.6.cmml"><mn id="S4.E2.m1.4.4.6.2" xref="S4.E2.m1.4.4.6.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.E2.m1.4.4.6.1" xref="S4.E2.m1.4.4.6.1.cmml">â€‹</mo><mi id="S4.E2.m1.4.4.6.3" xref="S4.E2.m1.4.4.6.3.cmml">Ïµ</mi></mrow></mfrac><mo lspace="0em" rspace="0em" id="S4.E2.m1.7.7.1.1.3.1" xref="S4.E2.m1.7.7.1.1.3.1.cmml">â€‹</mo><mi id="S4.E2.m1.7.7.1.1.3.2" xref="S4.E2.m1.7.7.1.1.3.2.cmml">ğ’›</mi></mrow></mrow><mo id="S4.E2.m1.7.7.1.2" xref="S4.E2.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.7b"><apply id="S4.E2.m1.7.7.1.1.cmml" xref="S4.E2.m1.7.7.1"><eq id="S4.E2.m1.7.7.1.1.1.cmml" xref="S4.E2.m1.7.7.1.1.1"></eq><apply id="S4.E2.m1.7.7.1.1.2.cmml" xref="S4.E2.m1.7.7.1.1.2"><times id="S4.E2.m1.7.7.1.1.2.1.cmml" xref="S4.E2.m1.7.7.1.1.2.1"></times><apply id="S4.E2.m1.7.7.1.1.2.2.cmml" xref="S4.E2.m1.7.7.1.1.2.2"><ci id="S4.E2.m1.7.7.1.1.2.2.1.cmml" xref="S4.E2.m1.7.7.1.1.2.2.1">^</ci><ci id="S4.E2.m1.7.7.1.1.2.2.2.cmml" xref="S4.E2.m1.7.7.1.1.2.2.2">âˆ‡</ci></apply><ci id="S4.E2.m1.7.7.1.1.2.3.cmml" xref="S4.E2.m1.7.7.1.1.2.3">â„’</ci><list id="S4.E2.m1.7.7.1.1.2.4.1.cmml" xref="S4.E2.m1.7.7.1.1.2.4.2"><ci id="S4.E2.m1.5.5.cmml" xref="S4.E2.m1.5.5">ğœ½</ci><ci id="S4.E2.m1.6.6.cmml" xref="S4.E2.m1.6.6">â„¬</ci></list></apply><apply id="S4.E2.m1.7.7.1.1.3.cmml" xref="S4.E2.m1.7.7.1.1.3"><times id="S4.E2.m1.7.7.1.1.3.1.cmml" xref="S4.E2.m1.7.7.1.1.3.1"></times><apply id="S4.E2.m1.4.4.cmml" xref="S4.E2.m1.4.4"><divide id="S4.E2.m1.4.4.5.cmml" xref="S4.E2.m1.4.4"></divide><apply id="S4.E2.m1.4.4.4.cmml" xref="S4.E2.m1.4.4.4"><minus id="S4.E2.m1.4.4.4.5.cmml" xref="S4.E2.m1.4.4.4.5"></minus><apply id="S4.E2.m1.4.4.4.4.cmml" xref="S4.E2.m1.4.4.4.4"><times id="S4.E2.m1.4.4.4.4.2.cmml" xref="S4.E2.m1.4.4.4.4.2"></times><ci id="S4.E2.m1.4.4.4.4.3.cmml" xref="S4.E2.m1.4.4.4.4.3">â„’</ci><list id="S4.E2.m1.4.4.4.4.1.2.cmml" xref="S4.E2.m1.4.4.4.4.1.1"><apply id="S4.E2.m1.4.4.4.4.1.1.1.cmml" xref="S4.E2.m1.4.4.4.4.1.1.1"><plus id="S4.E2.m1.4.4.4.4.1.1.1.1.cmml" xref="S4.E2.m1.4.4.4.4.1.1.1.1"></plus><ci id="S4.E2.m1.4.4.4.4.1.1.1.2.cmml" xref="S4.E2.m1.4.4.4.4.1.1.1.2">ğœ½</ci><apply id="S4.E2.m1.4.4.4.4.1.1.1.3.cmml" xref="S4.E2.m1.4.4.4.4.1.1.1.3"><times id="S4.E2.m1.4.4.4.4.1.1.1.3.1.cmml" xref="S4.E2.m1.4.4.4.4.1.1.1.3.1"></times><ci id="S4.E2.m1.4.4.4.4.1.1.1.3.2.cmml" xref="S4.E2.m1.4.4.4.4.1.1.1.3.2">italic-Ïµ</ci><ci id="S4.E2.m1.4.4.4.4.1.1.1.3.3.cmml" xref="S4.E2.m1.4.4.4.4.1.1.1.3.3">ğ’›</ci></apply></apply><ci id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1">â„¬</ci></list></apply><apply id="S4.E2.m1.4.4.4.6.cmml" xref="S4.E2.m1.4.4.4.6"><times id="S4.E2.m1.4.4.4.6.1.cmml" xref="S4.E2.m1.4.4.4.6.1"></times><ci id="S4.E2.m1.4.4.4.6.2.cmml" xref="S4.E2.m1.4.4.4.6.2">â„’</ci><list id="S4.E2.m1.4.4.4.6.3.1.cmml" xref="S4.E2.m1.4.4.4.6.3.2"><ci id="S4.E2.m1.2.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2">ğœ½</ci><ci id="S4.E2.m1.3.3.3.3.cmml" xref="S4.E2.m1.3.3.3.3">â„¬</ci></list></apply></apply><apply id="S4.E2.m1.4.4.6.cmml" xref="S4.E2.m1.4.4.6"><times id="S4.E2.m1.4.4.6.1.cmml" xref="S4.E2.m1.4.4.6.1"></times><cn type="integer" id="S4.E2.m1.4.4.6.2.cmml" xref="S4.E2.m1.4.4.6.2">2</cn><ci id="S4.E2.m1.4.4.6.3.cmml" xref="S4.E2.m1.4.4.6.3">italic-Ïµ</ci></apply></apply><ci id="S4.E2.m1.7.7.1.1.3.2.cmml" xref="S4.E2.m1.7.7.1.1.3.2">ğ’›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.7c">\hat{\nabla}\mathcal{L}(\bm{\theta};\mathcal{B})=\frac{\mathcal{L}(\bm{\theta}+\epsilon\bm{z};\mathcal{B})-\mathcal{L}(\bm{\theta};\mathcal{B})}{2\epsilon}\bm{z},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.SSS3.p2.7" class="ltx_p">where <math id="S4.SS1.SSS3.p2.4.m1.1" class="ltx_Math" alttext="\bm{z}\in\mathbb{R}^{d}" display="inline"><semantics id="S4.SS1.SSS3.p2.4.m1.1a"><mrow id="S4.SS1.SSS3.p2.4.m1.1.1" xref="S4.SS1.SSS3.p2.4.m1.1.1.cmml"><mi id="S4.SS1.SSS3.p2.4.m1.1.1.2" xref="S4.SS1.SSS3.p2.4.m1.1.1.2.cmml">ğ’›</mi><mo id="S4.SS1.SSS3.p2.4.m1.1.1.1" xref="S4.SS1.SSS3.p2.4.m1.1.1.1.cmml">âˆˆ</mo><msup id="S4.SS1.SSS3.p2.4.m1.1.1.3" xref="S4.SS1.SSS3.p2.4.m1.1.1.3.cmml"><mi id="S4.SS1.SSS3.p2.4.m1.1.1.3.2" xref="S4.SS1.SSS3.p2.4.m1.1.1.3.2.cmml">â„</mi><mi id="S4.SS1.SSS3.p2.4.m1.1.1.3.3" xref="S4.SS1.SSS3.p2.4.m1.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.4.m1.1b"><apply id="S4.SS1.SSS3.p2.4.m1.1.1.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1"><in id="S4.SS1.SSS3.p2.4.m1.1.1.1.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1.1"></in><ci id="S4.SS1.SSS3.p2.4.m1.1.1.2.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1.2">ğ’›</ci><apply id="S4.SS1.SSS3.p2.4.m1.1.1.3.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.4.m1.1.1.3.1.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS3.p2.4.m1.1.1.3.2.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1.3.2">â„</ci><ci id="S4.SS1.SSS3.p2.4.m1.1.1.3.3.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.4.m1.1c">\bm{z}\in\mathbb{R}^{d}</annotation></semantics></math> with <math id="S4.SS1.SSS3.p2.5.m2.2" class="ltx_Math" alttext="\bm{z}\sim\mathcal{N}(0,\bm{I}_{d})" display="inline"><semantics id="S4.SS1.SSS3.p2.5.m2.2a"><mrow id="S4.SS1.SSS3.p2.5.m2.2.2" xref="S4.SS1.SSS3.p2.5.m2.2.2.cmml"><mi id="S4.SS1.SSS3.p2.5.m2.2.2.3" xref="S4.SS1.SSS3.p2.5.m2.2.2.3.cmml">ğ’›</mi><mo id="S4.SS1.SSS3.p2.5.m2.2.2.2" xref="S4.SS1.SSS3.p2.5.m2.2.2.2.cmml">âˆ¼</mo><mrow id="S4.SS1.SSS3.p2.5.m2.2.2.1" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS3.p2.5.m2.2.2.1.3" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.3.cmml">ğ’©</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS3.p2.5.m2.2.2.1.2" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.2.cmml">â€‹</mo><mrow id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.2.cmml"><mo stretchy="false" id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.2" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.2.cmml">(</mo><mn id="S4.SS1.SSS3.p2.5.m2.1.1" xref="S4.SS1.SSS3.p2.5.m2.1.1.cmml">0</mn><mo id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.3" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.2.cmml">,</mo><msub id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.cmml"><mi id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.2" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.2.cmml">ğ‘°</mi><mi id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.3" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.3.cmml">d</mi></msub><mo stretchy="false" id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.4" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.5.m2.2b"><apply id="S4.SS1.SSS3.p2.5.m2.2.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2"><csymbol cd="latexml" id="S4.SS1.SSS3.p2.5.m2.2.2.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.2">similar-to</csymbol><ci id="S4.SS1.SSS3.p2.5.m2.2.2.3.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.3">ğ’›</ci><apply id="S4.SS1.SSS3.p2.5.m2.2.2.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1"><times id="S4.SS1.SSS3.p2.5.m2.2.2.1.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.2"></times><ci id="S4.SS1.SSS3.p2.5.m2.2.2.1.3.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.3">ğ’©</ci><interval closure="open" id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1"><cn type="integer" id="S4.SS1.SSS3.p2.5.m2.1.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.1.1">0</cn><apply id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1">subscript</csymbol><ci id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.2">ğ‘°</ci><ci id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.3.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.3">ğ‘‘</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.5.m2.2c">\bm{z}\sim\mathcal{N}(0,\bm{I}_{d})</annotation></semantics></math> and <math id="S4.SS1.SSS3.p2.6.m3.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S4.SS1.SSS3.p2.6.m3.1a"><mi id="S4.SS1.SSS3.p2.6.m3.1.1" xref="S4.SS1.SSS3.p2.6.m3.1.1.cmml">Ïµ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.6.m3.1b"><ci id="S4.SS1.SSS3.p2.6.m3.1.1.cmml" xref="S4.SS1.SSS3.p2.6.m3.1.1">italic-Ïµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.6.m3.1c">\epsilon</annotation></semantics></math> is the <em id="S4.SS1.SSS3.p2.7.1" class="ltx_emph ltx_font_italic">perturbation scale</em>Â  <cite class="ltx_cite ltx_citemacro_cite">Duchi etÂ al. (<a href="#bib.bib40" title="" class="ltx_ref">2015</a>)</cite>. It requires only two forward passes through the model to compute the estimation of gradient, serving as a memory-efficient alternative to BP. However, Eq. (<a href="#S4.E2" title="In 4.1.3 Zeroth-Order Optimization â€£ 4.1 Efficiency â€£ 4 Techniques â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) provides a biased gradient estimation, leading to a certain degree of information loss <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib105" title="" class="ltx_ref">2020</a>)</cite>. Alternatively, many studies opt for two-point gradient estimators that can yield a more stable and reliable approximationÂ <cite class="ltx_cite ltx_citemacro_cite">Spall (<a href="#bib.bib151" title="" class="ltx_ref">1992</a>); Malladi etÂ al. (<a href="#bib.bib114" title="" class="ltx_ref">2023a</a>); Lin etÂ al. (<a href="#bib.bib100" title="" class="ltx_ref">2023</a>); Ling etÂ al. (<a href="#bib.bib102" title="" class="ltx_ref">2024</a>)</cite>. The standard two-point gradient estimator estimates the gradient on a minibatch <math id="S4.SS1.SSS3.p2.7.m4.1" class="ltx_Math" alttext="\mathcal{B}" display="inline"><semantics id="S4.SS1.SSS3.p2.7.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS3.p2.7.m4.1.1" xref="S4.SS1.SSS3.p2.7.m4.1.1.cmml">â„¬</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.7.m4.1b"><ci id="S4.SS1.SSS3.p2.7.m4.1.1.cmml" xref="S4.SS1.SSS3.p2.7.m4.1.1">â„¬</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.7.m4.1c">\mathcal{B}</annotation></semantics></math> as</p>
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.7" class="ltx_Math" alttext="\hat{\nabla}\mathcal{L}(\bm{\theta};\mathcal{B})=\frac{\mathcal{L}(\bm{\theta}+\epsilon\bm{z};\mathcal{B})-\mathcal{L}(\bm{\theta}-\epsilon\bm{z};\mathcal{B})}{2\epsilon}\bm{z}." display="block"><semantics id="S4.E3.m1.7a"><mrow id="S4.E3.m1.7.7.1" xref="S4.E3.m1.7.7.1.1.cmml"><mrow id="S4.E3.m1.7.7.1.1" xref="S4.E3.m1.7.7.1.1.cmml"><mrow id="S4.E3.m1.7.7.1.1.2" xref="S4.E3.m1.7.7.1.1.2.cmml"><mover accent="true" id="S4.E3.m1.7.7.1.1.2.2" xref="S4.E3.m1.7.7.1.1.2.2.cmml"><mo id="S4.E3.m1.7.7.1.1.2.2.2" xref="S4.E3.m1.7.7.1.1.2.2.2.cmml">âˆ‡</mo><mo id="S4.E3.m1.7.7.1.1.2.2.1" xref="S4.E3.m1.7.7.1.1.2.2.1.cmml">^</mo></mover><mo lspace="0.167em" rspace="0em" id="S4.E3.m1.7.7.1.1.2.1" xref="S4.E3.m1.7.7.1.1.2.1.cmml">â€‹</mo><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.7.7.1.1.2.3" xref="S4.E3.m1.7.7.1.1.2.3.cmml">â„’</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.7.7.1.1.2.1a" xref="S4.E3.m1.7.7.1.1.2.1.cmml">â€‹</mo><mrow id="S4.E3.m1.7.7.1.1.2.4.2" xref="S4.E3.m1.7.7.1.1.2.4.1.cmml"><mo stretchy="false" id="S4.E3.m1.7.7.1.1.2.4.2.1" xref="S4.E3.m1.7.7.1.1.2.4.1.cmml">(</mo><mi id="S4.E3.m1.5.5" xref="S4.E3.m1.5.5.cmml">ğœ½</mi><mo id="S4.E3.m1.7.7.1.1.2.4.2.2" xref="S4.E3.m1.7.7.1.1.2.4.1.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.6.6" xref="S4.E3.m1.6.6.cmml">â„¬</mi><mo stretchy="false" id="S4.E3.m1.7.7.1.1.2.4.2.3" xref="S4.E3.m1.7.7.1.1.2.4.1.cmml">)</mo></mrow></mrow><mo id="S4.E3.m1.7.7.1.1.1" xref="S4.E3.m1.7.7.1.1.1.cmml">=</mo><mrow id="S4.E3.m1.7.7.1.1.3" xref="S4.E3.m1.7.7.1.1.3.cmml"><mfrac id="S4.E3.m1.4.4" xref="S4.E3.m1.4.4.cmml"><mrow id="S4.E3.m1.4.4.4" xref="S4.E3.m1.4.4.4.cmml"><mrow id="S4.E3.m1.3.3.3.3" xref="S4.E3.m1.3.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.3.3.3.3.3" xref="S4.E3.m1.3.3.3.3.3.cmml">â„’</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.3.3.3.3.2" xref="S4.E3.m1.3.3.3.3.2.cmml">â€‹</mo><mrow id="S4.E3.m1.3.3.3.3.1.1" xref="S4.E3.m1.3.3.3.3.1.2.cmml"><mo stretchy="false" id="S4.E3.m1.3.3.3.3.1.1.2" xref="S4.E3.m1.3.3.3.3.1.2.cmml">(</mo><mrow id="S4.E3.m1.3.3.3.3.1.1.1" xref="S4.E3.m1.3.3.3.3.1.1.1.cmml"><mi id="S4.E3.m1.3.3.3.3.1.1.1.2" xref="S4.E3.m1.3.3.3.3.1.1.1.2.cmml">ğœ½</mi><mo id="S4.E3.m1.3.3.3.3.1.1.1.1" xref="S4.E3.m1.3.3.3.3.1.1.1.1.cmml">+</mo><mrow id="S4.E3.m1.3.3.3.3.1.1.1.3" xref="S4.E3.m1.3.3.3.3.1.1.1.3.cmml"><mi id="S4.E3.m1.3.3.3.3.1.1.1.3.2" xref="S4.E3.m1.3.3.3.3.1.1.1.3.2.cmml">Ïµ</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.3.3.3.3.1.1.1.3.1" xref="S4.E3.m1.3.3.3.3.1.1.1.3.1.cmml">â€‹</mo><mi id="S4.E3.m1.3.3.3.3.1.1.1.3.3" xref="S4.E3.m1.3.3.3.3.1.1.1.3.3.cmml">ğ’›</mi></mrow></mrow><mo id="S4.E3.m1.3.3.3.3.1.1.3" xref="S4.E3.m1.3.3.3.3.1.2.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.cmml">â„¬</mi><mo stretchy="false" id="S4.E3.m1.3.3.3.3.1.1.4" xref="S4.E3.m1.3.3.3.3.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E3.m1.4.4.4.5" xref="S4.E3.m1.4.4.4.5.cmml">âˆ’</mo><mrow id="S4.E3.m1.4.4.4.4" xref="S4.E3.m1.4.4.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.4.4.4.4.3" xref="S4.E3.m1.4.4.4.4.3.cmml">â„’</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.4.4.4.4.2" xref="S4.E3.m1.4.4.4.4.2.cmml">â€‹</mo><mrow id="S4.E3.m1.4.4.4.4.1.1" xref="S4.E3.m1.4.4.4.4.1.2.cmml"><mo stretchy="false" id="S4.E3.m1.4.4.4.4.1.1.2" xref="S4.E3.m1.4.4.4.4.1.2.cmml">(</mo><mrow id="S4.E3.m1.4.4.4.4.1.1.1" xref="S4.E3.m1.4.4.4.4.1.1.1.cmml"><mi id="S4.E3.m1.4.4.4.4.1.1.1.2" xref="S4.E3.m1.4.4.4.4.1.1.1.2.cmml">ğœ½</mi><mo id="S4.E3.m1.4.4.4.4.1.1.1.1" xref="S4.E3.m1.4.4.4.4.1.1.1.1.cmml">âˆ’</mo><mrow id="S4.E3.m1.4.4.4.4.1.1.1.3" xref="S4.E3.m1.4.4.4.4.1.1.1.3.cmml"><mi id="S4.E3.m1.4.4.4.4.1.1.1.3.2" xref="S4.E3.m1.4.4.4.4.1.1.1.3.2.cmml">Ïµ</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.4.4.4.4.1.1.1.3.1" xref="S4.E3.m1.4.4.4.4.1.1.1.3.1.cmml">â€‹</mo><mi id="S4.E3.m1.4.4.4.4.1.1.1.3.3" xref="S4.E3.m1.4.4.4.4.1.1.1.3.3.cmml">ğ’›</mi></mrow></mrow><mo id="S4.E3.m1.4.4.4.4.1.1.3" xref="S4.E3.m1.4.4.4.4.1.2.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.2.2.2.2" xref="S4.E3.m1.2.2.2.2.cmml">â„¬</mi><mo stretchy="false" id="S4.E3.m1.4.4.4.4.1.1.4" xref="S4.E3.m1.4.4.4.4.1.2.cmml">)</mo></mrow></mrow></mrow><mrow id="S4.E3.m1.4.4.6" xref="S4.E3.m1.4.4.6.cmml"><mn id="S4.E3.m1.4.4.6.2" xref="S4.E3.m1.4.4.6.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.E3.m1.4.4.6.1" xref="S4.E3.m1.4.4.6.1.cmml">â€‹</mo><mi id="S4.E3.m1.4.4.6.3" xref="S4.E3.m1.4.4.6.3.cmml">Ïµ</mi></mrow></mfrac><mo lspace="0em" rspace="0em" id="S4.E3.m1.7.7.1.1.3.1" xref="S4.E3.m1.7.7.1.1.3.1.cmml">â€‹</mo><mi id="S4.E3.m1.7.7.1.1.3.2" xref="S4.E3.m1.7.7.1.1.3.2.cmml">ğ’›</mi></mrow></mrow><mo lspace="0em" id="S4.E3.m1.7.7.1.2" xref="S4.E3.m1.7.7.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.7b"><apply id="S4.E3.m1.7.7.1.1.cmml" xref="S4.E3.m1.7.7.1"><eq id="S4.E3.m1.7.7.1.1.1.cmml" xref="S4.E3.m1.7.7.1.1.1"></eq><apply id="S4.E3.m1.7.7.1.1.2.cmml" xref="S4.E3.m1.7.7.1.1.2"><times id="S4.E3.m1.7.7.1.1.2.1.cmml" xref="S4.E3.m1.7.7.1.1.2.1"></times><apply id="S4.E3.m1.7.7.1.1.2.2.cmml" xref="S4.E3.m1.7.7.1.1.2.2"><ci id="S4.E3.m1.7.7.1.1.2.2.1.cmml" xref="S4.E3.m1.7.7.1.1.2.2.1">^</ci><ci id="S4.E3.m1.7.7.1.1.2.2.2.cmml" xref="S4.E3.m1.7.7.1.1.2.2.2">âˆ‡</ci></apply><ci id="S4.E3.m1.7.7.1.1.2.3.cmml" xref="S4.E3.m1.7.7.1.1.2.3">â„’</ci><list id="S4.E3.m1.7.7.1.1.2.4.1.cmml" xref="S4.E3.m1.7.7.1.1.2.4.2"><ci id="S4.E3.m1.5.5.cmml" xref="S4.E3.m1.5.5">ğœ½</ci><ci id="S4.E3.m1.6.6.cmml" xref="S4.E3.m1.6.6">â„¬</ci></list></apply><apply id="S4.E3.m1.7.7.1.1.3.cmml" xref="S4.E3.m1.7.7.1.1.3"><times id="S4.E3.m1.7.7.1.1.3.1.cmml" xref="S4.E3.m1.7.7.1.1.3.1"></times><apply id="S4.E3.m1.4.4.cmml" xref="S4.E3.m1.4.4"><divide id="S4.E3.m1.4.4.5.cmml" xref="S4.E3.m1.4.4"></divide><apply id="S4.E3.m1.4.4.4.cmml" xref="S4.E3.m1.4.4.4"><minus id="S4.E3.m1.4.4.4.5.cmml" xref="S4.E3.m1.4.4.4.5"></minus><apply id="S4.E3.m1.3.3.3.3.cmml" xref="S4.E3.m1.3.3.3.3"><times id="S4.E3.m1.3.3.3.3.2.cmml" xref="S4.E3.m1.3.3.3.3.2"></times><ci id="S4.E3.m1.3.3.3.3.3.cmml" xref="S4.E3.m1.3.3.3.3.3">â„’</ci><list id="S4.E3.m1.3.3.3.3.1.2.cmml" xref="S4.E3.m1.3.3.3.3.1.1"><apply id="S4.E3.m1.3.3.3.3.1.1.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1.1"><plus id="S4.E3.m1.3.3.3.3.1.1.1.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1.1.1"></plus><ci id="S4.E3.m1.3.3.3.3.1.1.1.2.cmml" xref="S4.E3.m1.3.3.3.3.1.1.1.2">ğœ½</ci><apply id="S4.E3.m1.3.3.3.3.1.1.1.3.cmml" xref="S4.E3.m1.3.3.3.3.1.1.1.3"><times id="S4.E3.m1.3.3.3.3.1.1.1.3.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1.1.3.1"></times><ci id="S4.E3.m1.3.3.3.3.1.1.1.3.2.cmml" xref="S4.E3.m1.3.3.3.3.1.1.1.3.2">italic-Ïµ</ci><ci id="S4.E3.m1.3.3.3.3.1.1.1.3.3.cmml" xref="S4.E3.m1.3.3.3.3.1.1.1.3.3">ğ’›</ci></apply></apply><ci id="S4.E3.m1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1">â„¬</ci></list></apply><apply id="S4.E3.m1.4.4.4.4.cmml" xref="S4.E3.m1.4.4.4.4"><times id="S4.E3.m1.4.4.4.4.2.cmml" xref="S4.E3.m1.4.4.4.4.2"></times><ci id="S4.E3.m1.4.4.4.4.3.cmml" xref="S4.E3.m1.4.4.4.4.3">â„’</ci><list id="S4.E3.m1.4.4.4.4.1.2.cmml" xref="S4.E3.m1.4.4.4.4.1.1"><apply id="S4.E3.m1.4.4.4.4.1.1.1.cmml" xref="S4.E3.m1.4.4.4.4.1.1.1"><minus id="S4.E3.m1.4.4.4.4.1.1.1.1.cmml" xref="S4.E3.m1.4.4.4.4.1.1.1.1"></minus><ci id="S4.E3.m1.4.4.4.4.1.1.1.2.cmml" xref="S4.E3.m1.4.4.4.4.1.1.1.2">ğœ½</ci><apply id="S4.E3.m1.4.4.4.4.1.1.1.3.cmml" xref="S4.E3.m1.4.4.4.4.1.1.1.3"><times id="S4.E3.m1.4.4.4.4.1.1.1.3.1.cmml" xref="S4.E3.m1.4.4.4.4.1.1.1.3.1"></times><ci id="S4.E3.m1.4.4.4.4.1.1.1.3.2.cmml" xref="S4.E3.m1.4.4.4.4.1.1.1.3.2">italic-Ïµ</ci><ci id="S4.E3.m1.4.4.4.4.1.1.1.3.3.cmml" xref="S4.E3.m1.4.4.4.4.1.1.1.3.3">ğ’›</ci></apply></apply><ci id="S4.E3.m1.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2">â„¬</ci></list></apply></apply><apply id="S4.E3.m1.4.4.6.cmml" xref="S4.E3.m1.4.4.6"><times id="S4.E3.m1.4.4.6.1.cmml" xref="S4.E3.m1.4.4.6.1"></times><cn type="integer" id="S4.E3.m1.4.4.6.2.cmml" xref="S4.E3.m1.4.4.6.2">2</cn><ci id="S4.E3.m1.4.4.6.3.cmml" xref="S4.E3.m1.4.4.6.3">italic-Ïµ</ci></apply></apply><ci id="S4.E3.m1.7.7.1.1.3.2.cmml" xref="S4.E3.m1.7.7.1.1.3.2">ğ’›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.7c">\hat{\nabla}\mathcal{L}(\bm{\theta};\mathcal{B})=\frac{\mathcal{L}(\bm{\theta}+\epsilon\bm{z};\mathcal{B})-\mathcal{L}(\bm{\theta}-\epsilon\bm{z};\mathcal{B})}{2\epsilon}\bm{z}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.SSS3.p3" class="ltx_para">
<p id="S4.SS1.SSS3.p3.1" class="ltx_p">Based on the above gradient estimation frameworks,
recent work, such as that byÂ <cite class="ltx_cite ltx_citemacro_citet">Xu etÂ al. (<a href="#bib.bib185" title="" class="ltx_ref">2024a</a>); Lu etÂ al. (<a href="#bib.bib111" title="" class="ltx_ref">2023b</a>)</cite>, has initiated preliminary explorations into the deployment of both FedPEFT and full-model fine-tuning of billion-sized FMs, like LLaMA, on mobile devices. The naive ZOO methods remain impractical for training large FMs in standard FL frameworks such as <span id="S4.SS1.SSS3.p3.1.1" class="ltx_text ltx_font_typewriter">FedAvg</span>, as they still result in a significant communication burden for model aggregation. In light of this, FedKSeedÂ <cite class="ltx_cite ltx_citemacro_cite">Qin etÂ al. (<a href="#bib.bib129" title="" class="ltx_ref">2024</a>)</cite> was proposed to further reduce communication overheads between the server and clients by using just a few random seeds and
scalar gradients, requiring only a few thousand bytes for communication.</p>
</div>
<div id="S4.SS1.SSS3.p4" class="ltx_para">
<p id="S4.SS1.SSS3.p4.1" class="ltx_p">Although ZOO methods have shown promise in resource-efficient FLÂ <cite class="ltx_cite ltx_citemacro_cite">Ling etÂ al. (<a href="#bib.bib102" title="" class="ltx_ref">2024</a>)</cite>, they generally require many iterations to achieve strong performanceÂ <cite class="ltx_cite ltx_citemacro_cite">Malladi etÂ al. (<a href="#bib.bib115" title="" class="ltx_ref">2023b</a>)</cite>. Compared to the well-established BP-based optimization, ZOO is still in the early stages of development, particularly for FM-FL settings, necessitating further research and optimization.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Adaptability</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Adaptation refers to the process of tailoring a pre-trained FM to perform effectively across varying FL settings and scenarios. This mainly includes the capability to learn from different domains, cater to individual user needs, and work across diverse devices while retaining overall performance and efficiency. We focus on three key aspects of adaptation, namely <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">domain-centric adaptation</span>, <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">client-centric adaptation</span>, and <span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_italic">system-centric adaptation</span>.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Domain-Centric Adaptation</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">Domain-centric adaptation focuses on adapting FMs within specific domains by addressing the domain diversity across client datasets.</p>
</div>
<section id="S4.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Domain-Adaptive Pre-Training</h5>

<div id="S4.SS2.SSS1.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px1.p1.1" class="ltx_p">Despite being heavily reliant on large-scale and public datasets for their initial training, FMs often require further Domain-Adaptive Pre-Training (DAPT) with domain-specific data for tasks that necessitate specialized knowledgeÂ <cite class="ltx_cite ltx_citemacro_cite">Gururangan etÂ al. (<a href="#bib.bib62" title="" class="ltx_ref">2020</a>); Guo and Yu (<a href="#bib.bib59" title="" class="ltx_ref">2022</a>)</cite>.
In domains like healthcare, FL allows for the continued pre-training of these models using sensitive, domain-specific data without compromising privacy. Based on this idea,Â <cite class="ltx_cite ltx_citemacro_citet">Jiang etÂ al. (<a href="#bib.bib74" title="" class="ltx_ref">2023b</a>)</cite> proposed FFDAPT, a computational-efficient further pre-training algorithm that freezes a portion of consecutive layers while optimizing the rest of the layers. Similarly,Â <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a href="#bib.bib172" title="" class="ltx_ref">2023</a>)</cite> proposed FEDBFPT that builds a local model for each client, progressively training the shallower layers of local models while sampling deeper layers, and aggregating trained parameters on a server to create the final global model.</p>
</div>
</section>
<section id="S4.SS2.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Multi-Domain Adaptation</h5>

<div id="S4.SS2.SSS1.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px2.p1.1" class="ltx_p">Given that client data may belong to various domains in real-world FL scenarios, some effortsÂ <cite class="ltx_cite ltx_citemacro_cite">Feng etÂ al. (<a href="#bib.bib47" title="" class="ltx_ref">2023c</a>); Su etÂ al. (<a href="#bib.bib153" title="" class="ltx_ref">2024</a>)</cite> have been devoted to facilitating multi-domain collaborative adaptation.Â <cite class="ltx_cite ltx_citemacro_citet">Feng etÂ al. (<a href="#bib.bib47" title="" class="ltx_ref">2023c</a>)</cite> applied a pre-trained CLIP to the multi-domain scenario and proposed an adaptive prompt tuning method that uses domain-specific keys to generate prompts
for each test sample. Furthermore,Â <cite class="ltx_cite ltx_citemacro_citet">Su etÂ al. (<a href="#bib.bib153" title="" class="ltx_ref">2024</a>)</cite> employed knowledge distillation to selectively distill global knowledge based on an entropy measure, improving the generalization across different domains.</p>
</div>
</section>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Client-Centric Adaptation</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">Client-centric adaptation refers to the process of tailoring an FM to meet the specific needs or preferences of individual clients while leveraging the decentralized and privacy-preserving nature of FL. Particularly, we discuss two types of popular personalized methods as follows:</p>
</div>
<section id="S4.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Personalization</h5>

<div id="S4.SS2.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px1.p1.1" class="ltx_p">Adapter-based methods introduce small, trainable adapters into the frozen pre-trained FMs, allowing for client-specific model adaptation without altering the original FL. FedDATÂ <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2024</a>)</cite> leverages a dual-adapter structure, with personalized adapters focusing on client-specific knowledge and a global adapter maintaining client-agnostic knowledge. FedDAT executes bi-directional knowledge distillation between personalized adapters and the global adapter to regularize the clientâ€™s updates and prevent overfitting. Prompt-based methods involve using client-specific soft prompts to guide the modelâ€™s response. pFedPGÂ <cite class="ltx_cite ltx_citemacro_cite">Yang etÂ al. (<a href="#bib.bib189" title="" class="ltx_ref">2023a</a>)</cite> trains a prompt generator to exploit underlying client-specific characteristics and produce personalized prompts for each client, thereby enabling efficient and personalized adaptation.</p>
</div>
</section>
<section id="S4.SS2.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Client Clustering</h5>

<div id="S4.SS2.SSS2.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px2.p1.1" class="ltx_p">This branch of study aims to cluster clients based on the underlying relationships and tailor FMs for the client group with similar data distributions, thus reducing the negative impact of data heterogeneity and improving accuracy.Â <cite class="ltx_cite ltx_citemacro_citet">Guo etÂ al. (<a href="#bib.bib60" title="" class="ltx_ref">2024b</a>)</cite> proposed a FedPEFT-based framework for multilingual modeling, which employs language family clustering to alleviate parameter conflicts of LoRA tuning.</p>
</div>
</section>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>System-Centric Adaptation</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">System-centric aims to improve adaptability at the system level. This involves handling resource heterogeneity in the FL systems while ensuring training efficiency and model utility.</p>
</div>
<section id="S4.SS2.SSS3.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Resource-Heterogeneous Methods</h5>

<div id="S4.SS2.SSS3.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS3.Px1.p1.1" class="ltx_p">Cross-device FL systems may be composed of devices equipped with heterogeneous resources, leading to disparities where certain devices exhibit more efficient model training than others <cite class="ltx_cite ltx_citemacro_cite">Chen etÂ al. (<a href="#bib.bib27" title="" class="ltx_ref">2024</a>)</cite>. To address this issue, several methods have been developed to customize model architectures for resource-heterogeneous FL systems. In FL environments possessing heterogeneous resources, LoRA-based FedPEFT exhibits distinctive flexibility and adaptation in fine-tuning frozen FMs without overburdening client devices. <cite class="ltx_cite ltx_citemacro_citet">Su etÂ al. (<a href="#bib.bib152" title="" class="ltx_ref">2023</a>)</cite> suggested assigning LoRA adapters to varying numbers of layers for heterogeneous clients according to a randomly generated mask matrix. An alternative and more targeted idea is to choose diverse LoRA ranks across clients based on their system capabilities. <cite class="ltx_cite ltx_citemacro_citet">Bai etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2024a</a>)</cite> proposed FlexLoRA to adjust local LoRA ranks dynamically. FlexLoRA reconstructs the uniform full-sized LoRA module <math id="S4.SS2.SSS3.Px1.p1.1.m1.1" class="ltx_Math" alttext="\Delta\mathbf{W}" display="inline"><semantics id="S4.SS2.SSS3.Px1.p1.1.m1.1a"><mrow id="S4.SS2.SSS3.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.2" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.1" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3.cmml">ğ–</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.Px1.p1.1.m1.1b"><apply id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1"><times id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.1"></times><ci id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.2">Î”</ci><ci id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3">ğ–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.Px1.p1.1.m1.1c">\Delta\mathbf{W}</annotation></semantics></math> for server-side model aggregation followed by an SVD-based parameter redistribution. However, concurrent research by <cite class="ltx_cite ltx_citemacro_citet">Cho etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2024</a>)</cite> has empirically demonstrated that the reconstruct-redistribute method suffers from performance loss compared to homogeneous LoRA. Instead, they proposed <span id="S4.SS2.SSS3.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">HetLoRA</span> <cite class="ltx_cite ltx_citemacro_cite">Cho etÂ al. (<a href="#bib.bib30" title="" class="ltx_ref">2024</a>)</cite> that utilizes zero-padding to align module size before aggregation. It then truncates the global LoRA modules for the specific rank of the next selected clients.</p>
</div>
</section>
<section id="S4.SS2.SSS3.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Split Learning</h5>

<div id="S4.SS2.SSS3.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS3.Px2.p1.1" class="ltx_p">Split learning addresses the resource heterogeneity between servers and clients by splitting a large model at a cut layer into client and server modelsÂ <cite class="ltx_cite ltx_citemacro_cite">Thapa etÂ al. (<a href="#bib.bib164" title="" class="ltx_ref">2022</a>)</cite>. For each training step, the output tensor, so-called smashed data, from the client model and the corresponding labels are transmitted over to the server. The server continues the forward propagation by processing the smashed data through its remaining layers; it then computes the loss using the transmitted label and performs backpropagation. The gradient generated
at the first layer of the server model is then transmitted back to the client for further backpropagation.
Along this line, FedBERTÂ <cite class="ltx_cite ltx_citemacro_cite">Tian etÂ al. (<a href="#bib.bib165" title="" class="ltx_ref">2022</a>)</cite> proposes to leverage split learning for training the BERT model, showing the feasibility of training large FMs in FL settings.
FedSplitXÂ <cite class="ltx_cite ltx_citemacro_cite">Shin etÂ al. (<a href="#bib.bib149" title="" class="ltx_ref">2023b</a>)</cite> is a more fine-grained method that allows multiple partition points for model splitting, accommodating more diverse client capabilities. Compared to conventional FL, split learning scales better with the size of FMs as it communicates only small-sized smashed data instead of model parametersÂ <cite class="ltx_cite ltx_citemacro_cite">Singh etÂ al. (<a href="#bib.bib150" title="" class="ltx_ref">2019</a>)</cite>. Despite its merits, split learning is highly dependent on the network connection quality. Given that server-client interactions occur at every step of the optimization process <cite class="ltx_cite ltx_citemacro_cite">Zheng etÂ al. (<a href="#bib.bib214" title="" class="ltx_ref">2023</a>)</cite>, communication delays cause a more significant impact on efficiency.</p>
</div>
</section>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Trustworthiness</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">This line of work aims to enhance trustworthiness throughout the FM-FL lifecycle, covering a variety of key aspects including, but not limited to, <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">IP protection</span>, <span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_italic">privacy protection</span>, and <span id="S4.SS3.p1.1.3" class="ltx_text ltx_font_italic">attack robustness</span>.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>IP Protection</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">Existing IP protection involves safeguarding ownership of FMs from unauthorized use (<em id="S4.SS3.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> model theft)Â <cite class="ltx_cite ltx_citemacro_cite">Tekgul etÂ al. (<a href="#bib.bib163" title="" class="ltx_ref">2021</a>)</cite>. We discuss the following two mainstream IP protection strategies: <span id="S4.SS3.SSS1.p1.1.2" class="ltx_text ltx_font_italic">watermarking</span> and <span id="S4.SS3.SSS1.p1.1.3" class="ltx_text ltx_font_italic">black-box tuning</span>.</p>
</div>
<section id="S4.SS3.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Watermarking</h5>

<div id="S4.SS3.SSS1.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.Px1.p1.1" class="ltx_p">Watermarking is a well-known deterrence technology for model IP protection by providing the identities of model owners to demonstrate ownership of their modelsÂ <cite class="ltx_cite ltx_citemacro_cite">Adi etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Tekgul etÂ al. (<a href="#bib.bib163" title="" class="ltx_ref">2021</a>)</cite> proposed WAFFLE, the first solution that addresses the ownership problem by injecting a watermark into the global model in FL environments. Recently, <cite class="ltx_cite ltx_citemacro_citet">Yu etÂ al. (<a href="#bib.bib197" title="" class="ltx_ref">2023b</a>)</cite> proposed DUW that embeds a client-unique key into each clientâ€™s local model, aiming to identify the infringer of a leaked model while verifying the FL modelâ€™s ownership.</p>
</div>
</section>
<section id="S4.SS3.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Black-Box Tuning</h5>

<div id="S4.SS3.SSS1.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS1.Px2.p1.1" class="ltx_p">Black-Box Tuning (BBT) is a set of ZOO-based methods that fine-tune FMs without direct access to model parameters <cite class="ltx_cite ltx_citemacro_cite">Sun etÂ al. (<a href="#bib.bib158" title="" class="ltx_ref">2022c</a>, <a href="#bib.bib157" title="" class="ltx_ref">b</a>)</cite>. BBT methods are often additive, introducing additional parameters while keeping the original model frozen (see Section <a href="#S4.SS1.SSS1.Px2" title="Additive Methods â€£ 4.1.1 Parameter-Efficient Fine-Tuning â€£ 4.1 Efficiency â€£ 4 Techniques â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1.1</span></a>). Fed-BBPT <cite class="ltx_cite ltx_citemacro_cite">Lin etÂ al. (<a href="#bib.bib100" title="" class="ltx_ref">2023</a>)</cite> is a general prompt tuning framework
that facilitates the joint training of a global lightweight prompt generator across multiple clients.
FedBPT <cite class="ltx_cite ltx_citemacro_cite">Sun etÂ al. (<a href="#bib.bib156" title="" class="ltx_ref">2024a</a>)</cite> adopts a classic evolutionary-based ZOO method, CMA-ES <cite class="ltx_cite ltx_citemacro_cite">Hansen and Ostermeier (<a href="#bib.bib64" title="" class="ltx_ref">2001</a>)</cite>, for training an optimal prompt that improves the performance of frozen FMs. ZooPFL <cite class="ltx_cite ltx_citemacro_cite">Lu etÂ al. (<a href="#bib.bib111" title="" class="ltx_ref">2023b</a>)</cite>, on the other hand, applies coordinate-wise gradient estimate to learn input surgery that incorporates client-specific embeddings. BBT allows for local fine-tuning of FMs while not infringing IP constraints. However, current research in this line is limited to few-shot learning with small datasets for LLM fine-tuning <cite class="ltx_cite ltx_citemacro_cite">Sun etÂ al. (<a href="#bib.bib157" title="" class="ltx_ref">2022b</a>)</cite>, while larger datasets and other modalities remain unexplored.</p>
</div>
</section>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Privacy Protection</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">Protecting privacy in FM-FL requires both designing protective measures and studying privacy attack strategies.</p>
</div>
<section id="S4.SS3.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Privacy-Preserving Techniques</h5>

<div id="S4.SS3.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS2.Px1.p1.1" class="ltx_p">Differential Privacy (DP) is a theoretical framework that governs privacy boundaries and manages the tradeoff between privacy and model convergenceÂ <cite class="ltx_cite ltx_citemacro_cite">Wei etÂ al. (<a href="#bib.bib173" title="" class="ltx_ref">2020</a>); Xu etÂ al. (<a href="#bib.bib188" title="" class="ltx_ref">2023</a>)</cite>. DP-based FL approaches often add artificial noise (<em id="S4.SS3.SSS2.Px1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> Gaussian noise) to parameters at the clientsâ€™ side before aggregating to prevent information leakageÂ <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a href="#bib.bib188" title="" class="ltx_ref">2023</a>)</cite>. Besides, DP is compatible with most FedPEFT methods. For instance, <cite class="ltx_cite ltx_citemacro_citet">Sun etÂ al. (<a href="#bib.bib159" title="" class="ltx_ref">2024b</a>)</cite> showed that DP noise can even be amplified by the locally â€œsemi-quadraticâ€ nature of LoRA-based methods, motivating the integration of LoRA with DP to improve resource efficiency while maintaining data privacyÂ <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib107" title="" class="ltx_ref">2023c</a>)</cite>. In addition to DP, Secure Multi-Party Computation (SMPC)Â <cite class="ltx_cite ltx_citemacro_cite">Mugunthan etÂ al. (<a href="#bib.bib120" title="" class="ltx_ref">2019</a>)</cite> and Homomorphic Encryption (HE)Â <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib201" title="" class="ltx_ref">2020</a>)</cite> are also effective privacy-preserving mechanisms. However, they do not scale well enough for large-scale deployments in FM-FL.</p>
</div>
</section>
<section id="S4.SS3.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Privacy Attack</h5>

<div id="S4.SS3.SSS2.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.Px2.p1.1" class="ltx_p">Privacy attacks in FM-FL involve extracting sensitive information from the data used in training, even though the data itself is not directly shared. Major attacks include <span id="S4.SS3.SSS2.Px2.p1.1.1" class="ltx_text ltx_font_italic">membership inference attack</span> and <span id="S4.SS3.SSS2.Px2.p1.1.2" class="ltx_text ltx_font_italic">data reconstruction attack</span>, where the former aims to determine whether a specific data sample is in a victim clientâ€™s training set, and the latter strives to reconstruct original input data from the model parameters or gradientsÂ <cite class="ltx_cite ltx_citemacro_cite">Ren etÂ al. (<a href="#bib.bib139" title="" class="ltx_ref">2024</a>)</cite>. Regarding membership inference attacks,Â <cite class="ltx_cite ltx_citemacro_citet">Vu etÂ al. (<a href="#bib.bib169" title="" class="ltx_ref">2024</a>)</cite> revealed the vulnerabilities of popular LLMs, including BERT, DistilBERT, and OpenAIâ€™s GPTs. In terms of data reconstruction attacks, <cite class="ltx_cite ltx_citemacro_citet">Gupta etÂ al. (<a href="#bib.bib61" title="" class="ltx_ref">2022</a>)</cite> presented an attack <span id="S4.SS3.SSS2.Px2.p1.1.3" class="ltx_text ltx_font_italic">FILM</span>, which recovers private text data by extracting information from gradients transmitted during training despite employing a DP mechanism.</p>
</div>
</section>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Attack Robustness</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">Due to the distributed characteristic of optimization, FL
is vulnerable to poisoning attacksÂ <cite class="ltx_cite ltx_citemacro_cite">Lyu etÂ al. (<a href="#bib.bib112" title="" class="ltx_ref">2022</a>); RodrÃ­guez-Barroso etÂ al. (<a href="#bib.bib141" title="" class="ltx_ref">2023</a>)</cite>, wherein certain
participants may deviate from the prescribed update protocol
and upload arbitrary parameters to the central server.</p>
</div>
<section id="S4.SS3.SSS3.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Poisoning Attacks</h5>

<div id="S4.SS3.SSS3.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS3.Px1.p1.1" class="ltx_p">Depending on the adversarial goals, poisoning attacks in FL can be classified as <span id="S4.SS3.SSS3.Px1.p1.1.1" class="ltx_text ltx_font_italic">targeted</span> and <span id="S4.SS3.SSS3.Px1.p1.1.2" class="ltx_text ltx_font_italic">untargeted</span>Â <cite class="ltx_cite ltx_citemacro_cite">Jere etÂ al. (<a href="#bib.bib70" title="" class="ltx_ref">2020</a>)</cite>. Targeted attacks, like backdoor attacks, aim to manipulate the global model to generate attacker-desired misclassifications for some particular samplesÂ <cite class="ltx_cite ltx_citemacro_cite">Xie etÂ al. (<a href="#bib.bib182" title="" class="ltx_ref">2020</a>); Bagdasaryan etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>. In contrast, untargeted attacks seek to degrade the modelâ€™s overall performance indiscriminatelyÂ <cite class="ltx_cite ltx_citemacro_cite">Fang etÂ al. (<a href="#bib.bib42" title="" class="ltx_ref">2020</a>)</cite>. In addition to the well-recognized attacks on conventional FL studiesÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a href="#bib.bib91" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib90" title="" class="ltx_ref">2024b</a>)</cite>, FM-FL also faces potential threats from compromised pre-trained FMsÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a href="#bib.bib94" title="" class="ltx_ref">2023c</a>)</cite>. Thus, The attacker can introduce backdoors to downstream tasks without prior knowledgeÂ <cite class="ltx_cite ltx_citemacro_cite">Shen etÂ al. (<a href="#bib.bib147" title="" class="ltx_ref">2021</a>)</cite>. Specifically,Â <cite class="ltx_cite ltx_citemacro_citet">Li etÂ al. (<a href="#bib.bib95" title="" class="ltx_ref">2023d</a>)</cite> proposed Fed-EBD that introduces a backdoor-compromised FM to generate a public, synthetic dataset for FL training. The clientsâ€™ models, pre-trained on this dataset, inherit the backdoor throughout the training.</p>
</div>
</section>
<section id="S4.SS3.SSS3.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Defense Techniques</h5>

<div id="S4.SS3.SSS3.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS3.Px2.p1.1" class="ltx_p">As for defenses, robust aggregation rules are widely applied to make an attack-resilient estimation of the true updates and exclude the influence of malicious updatesÂ <cite class="ltx_cite ltx_citemacro_cite">Blanchard etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2017</a>); Yin etÂ al. (<a href="#bib.bib195" title="" class="ltx_ref">2018</a>); Chen etÂ al. (<a href="#bib.bib28" title="" class="ltx_ref">2017</a>); Li etÂ al. (<a href="#bib.bib89" title="" class="ltx_ref">2023a</a>)</cite>. Other research directions include trust-based strategiesÂ <cite class="ltx_cite ltx_citemacro_cite">Cao etÂ al. (<a href="#bib.bib24" title="" class="ltx_ref">2021</a>); Xu etÂ al. (<a href="#bib.bib184" title="" class="ltx_ref">2022</a>); Park etÂ al. (<a href="#bib.bib126" title="" class="ltx_ref">2021</a>)</cite> and variance-reduced algorithmsÂ <cite class="ltx_cite ltx_citemacro_cite">Gorbunov etÂ al. (<a href="#bib.bib55" title="" class="ltx_ref">2023</a>); Wu etÂ al. (<a href="#bib.bib181" title="" class="ltx_ref">2020b</a>)</cite>. Although these techniques have been widely examined in various FL settings, their effectiveness has yet to be explored in the FM-FL paradigm.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>A list of representative studies on the applications of FM-FL. Abbreviations: LoRA Tuning (LT), Adapter Tuning (AT), Full-Parameter Tuning (FT), Selective Tuning (ST), Prompt Tuning (PT). </figcaption>
<p id="S4.T2.1" class="ltx_p ltx_align_center"><span id="S4.T2.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S4.T2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:806.9pt;height:324.9pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S4.T2.1.1.1.1" class="ltx_p"><span id="S4.T2.1.1.1.1.1" class="ltx_text">
<span id="S4.T2.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Domain/Application</span></span>
<span id="S4.T2.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Task</span></span>
<span id="S4.T2.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Representative Work</span></span>
<span id="S4.T2.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span id="S4.T2.1.1.1.1.1.1.1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:45.7pt;vertical-align:-19.4pt;"><span class="ltx_transformed_inner" style="width:45.7pt;transform:translate(-19.43pt,0pt) rotate(-90deg) ;">
<span id="S4.T2.1.1.1.1.1.1.1.4.1.1" class="ltx_p"><span id="S4.T2.1.1.1.1.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold" style="color:#006BA4;">On-Device</span></span>
</span></span></span>
<span id="S4.T2.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span id="S4.T2.1.1.1.1.1.1.1.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:89.9pt;vertical-align:-41.5pt;"><span class="ltx_transformed_inner" style="width:89.9pt;transform:translate(-41.5pt,0pt) rotate(-90deg) ;">
<span id="S4.T2.1.1.1.1.1.1.1.5.1.1" class="ltx_p"><span id="S4.T2.1.1.1.1.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold" style="color:#1B9E77;">Personalization</span></span>
</span></span></span>
<span id="S4.T2.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Modality</span></span>
<span id="S4.T2.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.1.7.1" class="ltx_text ltx_font_bold">Backbone</span></span>
<span id="S4.T2.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span id="S4.T2.1.1.1.1.1.1.1.8.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:53.5pt;vertical-align:-24.3pt;"><span class="ltx_transformed_inner" style="width:53.5pt;transform:translate(-22.35pt,2.92pt) rotate(-90deg) ;">
<span id="S4.T2.1.1.1.1.1.1.1.8.1.1" class="ltx_p"><span id="S4.T2.1.1.1.1.1.1.1.8.1.1.1" class="ltx_text ltx_font_bold">Fine-Tuning</span></span>
</span></span></span></span>
<span id="S4.T2.1.1.1.1.1.1.2" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt ltx_rowspan ltx_rowspan_4" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold">Multilingual NLP</span></span>
<span id="S4.T2.1.1.1.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;">Language Understanding</span>
<span id="S4.T2.1.1.1.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;">FedKCÂ <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a href="#bib.bib171" title="" class="ltx_ref">2022</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.2.4.1" class="ltx_text" style="color:#006BA4;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.2.5.1" class="ltx_text" style="color:#1B9E77;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.2.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.2.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;">mBERT</span>
<span id="S4.T2.1.1.1.1.1.1.2.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;">FT</span></span>
<span id="S4.T2.1.1.1.1.1.1.3" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">Multi-Tasks</span>
<span id="S4.T2.1.1.1.1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">PMMFLÂ <cite class="ltx_cite ltx_citemacro_cite">Weller etÂ al. (<a href="#bib.bib174" title="" class="ltx_ref">2022</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.3.3" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.3.3.1" class="ltx_text" style="color:#006BA4;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.3.4.1" class="ltx_text" style="color:#1B9E77;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.3.5" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.3.6" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">mBERT</span>
<span id="S4.T2.1.1.1.1.1.1.3.7" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">FT</span></span>
<span id="S4.T2.1.1.1.1.1.1.4" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">Machine Translation</span>
<span id="S4.T2.1.1.1.1.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">Fed-MNMTÂ <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib108" title="" class="ltx_ref">2023d</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.4.3" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.4.3.1" class="ltx_text" style="color:#006BA4;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.4.4.1" class="ltx_text" style="color:#1B9E77;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.4.5" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.4.6" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">mBART-50</span>
<span id="S4.T2.1.1.1.1.1.1.4.7" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">AT</span></span>
<span id="S4.T2.1.1.1.1.1.1.5" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">Machine Translation</span>
<span id="S4.T2.1.1.1.1.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">FL-MetaSendÂ <cite class="ltx_cite ltx_citemacro_cite">Chu etÂ al. (<a href="#bib.bib31" title="" class="ltx_ref">2024</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.5.3" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.5.3.1" class="ltx_text" style="color:#006BA4;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.5.4.1" class="ltx_text" style="color:#1B9E77;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.5.5" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.5.6" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">M2M-100</span>
<span id="S4.T2.1.1.1.1.1.1.5.7" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">ST</span></span>
<span id="S4.T2.1.1.1.1.1.1.6" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.6.1" class="ltx_td ltx_border_rr" style="padding-top:0.9pt;padding-bottom:0.9pt;"></span>
<span id="S4.T2.1.1.1.1.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">Multi-Tasks</span>
<span id="S4.T2.1.1.1.1.1.1.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">MFPTÂ <cite class="ltx_cite ltx_citemacro_cite">Zhao etÂ al. (<a href="#bib.bib212" title="" class="ltx_ref">2024b</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.6.4" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.6.4.1" class="ltx_text" style="color:#006BA4;">âœ“</span></span>
<span id="S4.T2.1.1.1.1.1.1.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.6.5.1" class="ltx_text" style="color:#1B9E77;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.6.6" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.6.7" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">XLM-RoBERTa</span>
<span id="S4.T2.1.1.1.1.1.1.6.8" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">PT</span></span>
<span id="S4.T2.1.1.1.1.1.1.7" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.7.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t ltx_rowspan ltx_rowspan_2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.7.1.1" class="ltx_text ltx_font_bold">Speech</span></span>
<span id="S4.T2.1.1.1.1.1.1.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">Speech-to-Text</span>
<span id="S4.T2.1.1.1.1.1.1.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">pFedS2TÂ <cite class="ltx_cite ltx_citemacro_cite">Du etÂ al. (<a href="#bib.bib39" title="" class="ltx_ref">2024</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.7.4.1" class="ltx_text" style="color:#006BA4;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.7.5.1" class="ltx_text" style="color:#1B9E77;">âœ“</span></span>
<span id="S4.T2.1.1.1.1.1.1.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">Aud.</span>
<span id="S4.T2.1.1.1.1.1.1.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">Conformer/Whisper</span>
<span id="S4.T2.1.1.1.1.1.1.7.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">LT</span></span>
<span id="S4.T2.1.1.1.1.1.1.8" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.8.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">Speech Recognition</span>
<span id="S4.T2.1.1.1.1.1.1.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">FedASRÂ <cite class="ltx_cite ltx_citemacro_cite">Jia etÂ al. (<a href="#bib.bib71" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.8.3" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.8.3.1" class="ltx_text" style="color:#006BA4;">âœ“</span></span>
<span id="S4.T2.1.1.1.1.1.1.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.8.4.1" class="ltx_text" style="color:#1B9E77;">âœ“</span></span>
<span id="S4.T2.1.1.1.1.1.1.8.5" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">Aud.</span>
<span id="S4.T2.1.1.1.1.1.1.8.6" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">RNN-T</span>
<span id="S4.T2.1.1.1.1.1.1.8.7" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">AT</span></span>
<span id="S4.T2.1.1.1.1.1.1.9" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.9.1" class="ltx_td ltx_border_rr" style="padding-top:0.9pt;padding-bottom:0.9pt;"></span>
<span id="S4.T2.1.1.1.1.1.1.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">Speech Recognition</span>
<span id="S4.T2.1.1.1.1.1.1.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">FedE2EASR<cite class="ltx_cite ltx_citemacro_cite">Azam etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2023a</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.9.4" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.9.4.1" class="ltx_text" style="color:#006BA4;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.9.5.1" class="ltx_text" style="color:#1B9E77;">âœ“</span></span>
<span id="S4.T2.1.1.1.1.1.1.9.6" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">Aud.</span>
<span id="S4.T2.1.1.1.1.1.1.9.7" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">CTC-AED</span>
<span id="S4.T2.1.1.1.1.1.1.9.8" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">FT</span></span>
<span id="S4.T2.1.1.1.1.1.1.10" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.10.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t ltx_rowspan ltx_rowspan_3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.10.1.1" class="ltx_text ltx_font_bold">Recommendation</span></span>
<span id="S4.T2.1.1.1.1.1.1.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.10.2.1" class="ltx_text ltx_font_italic">General</span></span>
<span id="S4.T2.1.1.1.1.1.1.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">PPLRÂ <cite class="ltx_cite ltx_citemacro_cite">Zhao etÂ al. (<a href="#bib.bib211" title="" class="ltx_ref">2024a</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.10.4.1" class="ltx_text" style="color:#006BA4;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.10.5.1" class="ltx_text" style="color:#1B9E77;">âœ“</span></span>
<span id="S4.T2.1.1.1.1.1.1.10.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.10.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">LLaMA-7B/LongFormer</span>
<span id="S4.T2.1.1.1.1.1.1.10.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">FT</span></span>
<span id="S4.T2.1.1.1.1.1.1.11" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.11.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.11.1.1" class="ltx_text ltx_font_italic">General</span></span>
<span id="S4.T2.1.1.1.1.1.1.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">TransFRÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib202" title="" class="ltx_ref">2024a</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.11.3" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.11.3.1" class="ltx_text" style="color:#006BA4;">âœ“</span></span>
<span id="S4.T2.1.1.1.1.1.1.11.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.11.4.1" class="ltx_text" style="color:#1B9E77;">âœ“</span></span>
<span id="S4.T2.1.1.1.1.1.1.11.5" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.11.6" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">DistBERT</span>
<span id="S4.T2.1.1.1.1.1.1.11.7" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">AT</span></span>
<span id="S4.T2.1.1.1.1.1.1.12" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.12.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.12.1.1" class="ltx_text ltx_font_italic">General</span></span>
<span id="S4.T2.1.1.1.1.1.1.12.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">GPT-FedRecÂ <cite class="ltx_cite ltx_citemacro_cite">Zeng etÂ al. (<a href="#bib.bib200" title="" class="ltx_ref">2024</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.12.3" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.12.3.1" class="ltx_text" style="color:#006BA4;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.12.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.12.4.1" class="ltx_text" style="color:#1B9E77;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.12.5" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.12.6" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">ChatGPT</span>
<span id="S4.T2.1.1.1.1.1.1.12.7" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.12.7.1" class="ltx_text ltx_font_italic">NA</span></span></span>
<span id="S4.T2.1.1.1.1.1.1.13" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t ltx_rowspan ltx_rowspan_2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.13.1.1" class="ltx_text ltx_font_bold">Healthcare</span></span>
<span id="S4.T2.1.1.1.1.1.1.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">Mental Health Prediction</span>
<span id="S4.T2.1.1.1.1.1.1.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">FedTherapistÂ <cite class="ltx_cite ltx_citemacro_cite">Shin etÂ al. (<a href="#bib.bib148" title="" class="ltx_ref">2023a</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.13.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.13.4.1" class="ltx_text" style="color:#006BA4;">âœ“</span></span>
<span id="S4.T2.1.1.1.1.1.1.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.13.5.1" class="ltx_text" style="color:#1B9E77;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.13.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.13.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">BERT &amp; LLaMa-7B</span>
<span id="S4.T2.1.1.1.1.1.1.13.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">LT</span></span>
<span id="S4.T2.1.1.1.1.1.1.14" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.14.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">MRI Reconstruction</span>
<span id="S4.T2.1.1.1.1.1.1.14.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">FedPRÂ <cite class="ltx_cite ltx_citemacro_cite">Feng etÂ al. (<a href="#bib.bib45" title="" class="ltx_ref">2023a</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.14.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.14.3.1" class="ltx_text" style="color:#006BA4;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.14.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.14.4.1" class="ltx_text" style="color:#1B9E77;">âœ—</span></span>
<span id="S4.T2.1.1.1.1.1.1.14.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.9pt;padding-bottom:0.9pt;">Vis.</span>
<span id="S4.T2.1.1.1.1.1.1.14.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.9pt;padding-bottom:0.9pt;">Swin
Transformers</span>
<span id="S4.T2.1.1.1.1.1.1.14.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.9pt;padding-bottom:0.9pt;">PT</span></span>
</span></span></span>
</span></span></span></p>
</figure>
</section>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Applications of FM-FL</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this part, we briefly review the recent progress on FM-FL applications. TableÂ <a href="#S4.T2" title="Table 2 â€£ Defense Techniques â€£ 4.3.3 Attack Robustness â€£ 4.3 Trustworthiness â€£ 4 Techniques â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> lists representative work on specific applications and domains.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>FM-FL for Multilingual NLP</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Multilingual NLP refers to the techniques that handle multiple natural languagesÂ <cite class="ltx_cite ltx_citemacro_cite">Pires etÂ al. (<a href="#bib.bib128" title="" class="ltx_ref">2019</a>)</cite>, often to perform equally well across themÂ <cite class="ltx_cite ltx_citemacro_cite">Wu and Dredze (<a href="#bib.bib180" title="" class="ltx_ref">2020</a>)</cite>. Earlier researchÂ <cite class="ltx_cite ltx_citemacro_cite">Johnson etÂ al. (<a href="#bib.bib77" title="" class="ltx_ref">2017</a>)</cite> has shown that parameter sharing among different languages boosts the modelâ€™s performance in multilingual NLP, especially for low-resource languages for which significantly less content is available. However, real-world multilingual text data is often distributed across devices or regions, with each client (user) accessing only a limited subset of languages, where transferring the data to a central server is often problematic or prohibited due to privacy issuesÂ <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a href="#bib.bib171" title="" class="ltx_ref">2022</a>)</cite>. Thanks to its inherent privacy-preserving characteristic, FL holds promise in breaking the barriers of cross-lingual modeling and data isolation by allowing models to learn from decentralized datasets.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The pioneer work byÂ <cite class="ltx_cite ltx_citemacro_citet">Weller etÂ al. (<a href="#bib.bib174" title="" class="ltx_ref">2022</a>)</cite> has firstly demonstrated that fine-tuning pre-trained language models with FL can perform similarly to pre-trained models fine-tuned with the standard centralized method under multilingual NLP settings. Various subsequent studies have focused on adapting pre-trained FMs through FedPEFT techniques such as adapter tuningÂ <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib108" title="" class="ltx_ref">2023d</a>)</cite>, prompt tuningÂ <cite class="ltx_cite ltx_citemacro_cite">Zhao etÂ al. (<a href="#bib.bib212" title="" class="ltx_ref">2024b</a>)</cite>, and LoRAÂ <cite class="ltx_cite ltx_citemacro_cite">Guo etÂ al. (<a href="#bib.bib60" title="" class="ltx_ref">2024b</a>)</cite>, aiming to enhance training efficiency.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Considering the adverse effect of conflicting parameters from diverse languages during federated fine-tuning, recent studies have exploited clustering strategies to alleviate this issue. For instance, <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a href="#bib.bib171" title="" class="ltx_ref">2022</a>)</cite> applied <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mi id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><ci id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">k</annotation></semantics></math>-means clustering on each clientâ€™s data to obtain representative knowledge, specifically the clustered data centroids.
These centroids were then shared across clients for local training, enriching training data and addressing the challenges associated with data heterogeneity. Another compelling strategy along this line is language family-based clustering. <cite class="ltx_cite ltx_citemacro_citet">Liu etÂ al. (<a href="#bib.bib108" title="" class="ltx_ref">2023d</a>)</cite> explored various clustering strategies to group adapter parameters to mitigate the negative effects of multilingual data heterogeneity, showing that language family-based clustering significantly outperforms the other clustering strategies. Similarly,Â <cite class="ltx_cite ltx_citemacro_citet">Guo etÂ al. (<a href="#bib.bib60" title="" class="ltx_ref">2024b</a>)</cite> proposed fine-tuning FMs with LoRA and language family-based clustering to address the heterogeneity issue of multilingual modeling.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">General downstream tasks include language
modelingÂ <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a href="#bib.bib171" title="" class="ltx_ref">2022</a>)</cite>, machine translationÂ <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib108" title="" class="ltx_ref">2023d</a>); Chu etÂ al. (<a href="#bib.bib31" title="" class="ltx_ref">2024</a>)</cite>, and text classificationÂ <cite class="ltx_cite ltx_citemacro_cite">Weller etÂ al. (<a href="#bib.bib174" title="" class="ltx_ref">2022</a>)</cite>. In addition, some studies also focus on more specific applications such as medical transcript analysisÂ <cite class="ltx_cite ltx_citemacro_cite">Manoel etÂ al. (<a href="#bib.bib117" title="" class="ltx_ref">2023</a>)</cite> and hate speech detectionÂ <cite class="ltx_cite ltx_citemacro_cite">Akshay and Rahul (<a href="#bib.bib4" title="" class="ltx_ref">2024</a>)</cite>. These advancements illustrate the applicability of FM-FL across a wide range of scenarios in multilingual NLP.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>FM-FL for Speech</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">With the development of AI, researchers have also carried out many studies on speech-related FMs, <em id="S5.SS2.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> wav2vec 2.0Â <cite class="ltx_cite ltx_citemacro_cite">Baevski etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite> and WhisperÂ <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a href="#bib.bib132" title="" class="ltx_ref">2023</a>)</cite>. In this field, the adaptation of FMs often relies on FL to facilitate scenarios where the audio data is privacy-sensitive. Compared to other data modalities, speech-related FM-FL applications especially attract excessive attention to the aspects of <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_bold">on-device training</span> and <span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_bold">personalization</span>, motivated by the following considerations: (1) Audio data is continually generated on end-devices such as mobile phones, and owned
by individual usersâ€”thus it should be processed locally, rather than being transferred elsewhere; (2) Although FL takes advantage of all user data to collectively train one model that maximizes speaker-independent accuracy, such a one-model-fits-all solution can be sub-optimal for individual usersÂ <cite class="ltx_cite ltx_citemacro_cite">Jia etÂ al. (<a href="#bib.bib71" title="" class="ltx_ref">2023</a>)</cite>. Specific tasks in this field include Automatic Speech Recognition (ASR)Â <cite class="ltx_cite ltx_citemacro_cite">Azam etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2023b</a>)</cite> and Speech-to-Text (S2T)Â <cite class="ltx_cite ltx_citemacro_cite">Du etÂ al. (<a href="#bib.bib39" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>FM-FL for Recommendation</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Federated Recommendation (FR) strives to capture underlying user preferences and recommend appropriate information to users while safeguarding data privacyÂ <cite class="ltx_cite ltx_citemacro_cite">Bobadilla etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2013</a>); Zhang etÂ al. (<a href="#bib.bib203" title="" class="ltx_ref">2023a</a>)</cite>. Typical FR systems consist of a server and multiple clients, where clients represent individual users or local data servers possessing smaller datasets and retaining private user informationÂ <cite class="ltx_cite ltx_citemacro_cite">Ammad-Ud-Din etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>. These clients collaborate to train a global model while ensuring their data privacy protection by abstaining from direct data sharingÂ <cite class="ltx_cite ltx_citemacro_cite">Zeng etÂ al. (<a href="#bib.bib200" title="" class="ltx_ref">2024</a>); Zhang etÂ al. (<a href="#bib.bib203" title="" class="ltx_ref">2023a</a>)</cite>. Recently, LLM-based recommendations have been gaining increasing attentionÂ <cite class="ltx_cite ltx_citemacro_cite">Wu etÂ al. (<a href="#bib.bib178" title="" class="ltx_ref">2023b</a>)</cite> due to their strong capacities in language understanding and domain generalization. The benefits are mainly twofold: (1) LLMs mitigate the cold-start issue by utilizing textual descriptions to make recommendations without the need for extensive historical dataÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib206" title="" class="ltx_ref">2023c</a>)</cite>; (2) The inherent transferability of LLMs allows them to apply cross-domain knowledge and side information to improve accuracy and relevance across diverse items and user interestsÂ <cite class="ltx_cite ltx_citemacro_cite">Gao etÂ al. (<a href="#bib.bib49" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">One straightforward way to adapt FMs for FR is by fine-tuning them with historical user-item data. More specifically, FedPEFT techniques such as adapter tuningÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib202" title="" class="ltx_ref">2024a</a>)</cite> and split learningÂ <cite class="ltx_cite ltx_citemacro_cite">Zhao etÂ al. (<a href="#bib.bib211" title="" class="ltx_ref">2024a</a>)</cite> can be employed to improve resource efficiency.
Apart from parameter fine-tuning, LLMs can also be adapted to assist the recommendation in a zero-shot paradigm through prompt engineeringÂ (<em id="S5.SS3.p2.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> without parameter tuning)Â <cite class="ltx_cite ltx_citemacro_cite">Gao etÂ al. (<a href="#bib.bib49" title="" class="ltx_ref">2023</a>)</cite>. For example,Â <cite class="ltx_cite ltx_citemacro_citet">Zeng etÂ al. (<a href="#bib.bib200" title="" class="ltx_ref">2024</a>)</cite> proposed GPT-FedRec, a two-stage FR framework that leverages ChatGPT for its powerful zero-shot generalization ability. Firstly, GPT-FedRec facilitates hybrid retrieval by collaboratively training ID and text retrievers, after which the retrieved results are transformed into text prompts and submitted to GPT for re-ranking in the second stage. Additionally,Â <cite class="ltx_cite ltx_citemacro_citet">Guo etÂ al. (<a href="#bib.bib57" title="" class="ltx_ref">2024a</a>)</cite> employed a pre-trained BERT to obtain the representation vectors of item descriptions, which are then fed into a recommender system as augmented input.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>FM-FL for Healthcare</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">FMs, especially LLMs, have been found to excel in healthcare applications, showcasing impressive capabilities in tasks like mental health analysisÂ <cite class="ltx_cite ltx_citemacro_cite">Yang etÂ al. (<a href="#bib.bib190" title="" class="ltx_ref">2023b</a>)</cite>, disease diagnosisÂ <cite class="ltx_cite ltx_citemacro_cite">Panagoulias etÂ al. (<a href="#bib.bib124" title="" class="ltx_ref">2024</a>)</cite>, and drug discoveryÂ <cite class="ltx_cite ltx_citemacro_cite">Chenthamarakshan etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite>. However, it raises privacy concerns to upload the health information of patientsÂ <cite class="ltx_cite ltx_citemacro_cite">Tang etÂ al. (<a href="#bib.bib161" title="" class="ltx_ref">2023</a>)</cite> into a commercial server that supports the FMs. Meanwhile, FL has consistently received widespread attention in the healthcare domainÂ <cite class="ltx_cite ltx_citemacro_cite">Lincy and Kowshalya (<a href="#bib.bib101" title="" class="ltx_ref">2020</a>); Rieke etÂ al. (<a href="#bib.bib140" title="" class="ltx_ref">2020</a>); Joshi etÂ al. (<a href="#bib.bib78" title="" class="ltx_ref">2022</a>)</cite>, driven by the need for collaborative model training across different medical institutions without compromising patient data privacy. By breaking the barriers of private data availability, the FM-FL paradigm shows the potential to further harness the power of FMs in the healthcare domain.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">A recent studyÂ <cite class="ltx_cite ltx_citemacro_cite">Shin etÂ al. (<a href="#bib.bib148" title="" class="ltx_ref">2023a</a>)</cite> presents a mobile mental health monitoring system, FedTherapist, which leverages user speech
and keyboard input to fine-tune FMs with FL, demonstrating superior accuracy in mental health
prediction tasks such as depression,
stress, and mood prediction. Another representative studyÂ <cite class="ltx_cite ltx_citemacro_cite">Feng etÂ al. (<a href="#bib.bib45" title="" class="ltx_ref">2023a</a>)</cite> focuses on Magnetic Resonance Imaging (MRI) reconstruction, which involves retrieving a complex-valued image from its under-sampled signal. The authors adopted an FM pre-trained on public datasets and trained visual prompts from decentralized clinical datasets via a personalized FL mechanism, thereby reducing communication costs
and achieving competitive performance on limited local data.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">Despite the efforts, it has been shown that FMs in healthcare risk generating misleading information due to their imperfect understanding of complex medical dataÂ <cite class="ltx_cite ltx_citemacro_cite">Jeblick etÂ al. (<a href="#bib.bib69" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Future Directions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Although recent work has already begun to address the challenges
discussed in SectionÂ <a href="#S3.SS2" title="3.2 Core Challenges â€£ 3 FM-FL: Motivation &amp; Challenges â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, many critical open directions are yet to be explored. Here, we outline several representative ones.</p>
</div>
<section id="S6.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Multimodal FM-FL</h5>

<div id="S6.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p1.1" class="ltx_p">With the development of mobile technology and IoT infrastructuresÂ <cite class="ltx_cite ltx_citemacro_cite">Brunete etÂ al. (<a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>, numerous edge devices produce data from a range of modalities, such as sensory, visual, and audio. In the era of FMs, the success of LLMs and their multimodal
derivativesÂ <cite class="ltx_cite ltx_citemacro_cite">Ramesh etÂ al. (<a href="#bib.bib134" title="" class="ltx_ref">2021</a>); Google (<a href="#bib.bib54" title="" class="ltx_ref">2023</a>); OpenAI (<a href="#bib.bib123" title="" class="ltx_ref">2024</a>)</cite> have demonstrated the potential of multimodal FMs. The potential opportunities and challenges for multimodal FM-FL have yet to be explored.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Continual Learning</h5>

<div id="S6.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p1.1" class="ltx_p">Continual learning enables models to adapt to new data over time, improving their performance and accuracy. By incorporating new data into the model training process, FL and FMs can continuously improve and adapt to changing environments and user needsÂ <cite class="ltx_cite ltx_citemacro_cite">Yang etÂ al. (<a href="#bib.bib191" title="" class="ltx_ref">2024a</a>)</cite>. Future directions may involve leveraging transfer learning techniques in continual learning for FL and FMs. Models can transfer knowledge from previous tasks or domains to new ones, enabling more efficient adaptationÂ <cite class="ltx_cite ltx_citemacro_cite">Good etÂ al. (<a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Efficient Federated Black-Box Tuning</h5>

<div id="S6.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px3.p1.1" class="ltx_p">In scenarios where gradient access is unavailable, preliminary efforts have focused on federated fine-tuning black-box FMsÂ <cite class="ltx_cite ltx_citemacro_cite">Lin etÂ al. (<a href="#bib.bib100" title="" class="ltx_ref">2023</a>); Sun etÂ al. (<a href="#bib.bib156" title="" class="ltx_ref">2024a</a>); Lu etÂ al. (<a href="#bib.bib111" title="" class="ltx_ref">2023b</a>); Rui etÂ al. (<a href="#bib.bib143" title="" class="ltx_ref">2024</a>)</cite> utilizing ZOO. However, ZOOâ€™s noticeably slower convergence rates, especially in high-dimensional contexts compared to gradient-based methodsÂ <cite class="ltx_cite ltx_citemacro_cite">Golovin etÂ al. (<a href="#bib.bib52" title="" class="ltx_ref">2020</a>)</cite>, indicate an important direction for further research. The impact of these slower convergence rates on overall efficiency and computational load within FL, particularly concerning large-scale FMs, has not been adequately investigated and understood.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">FL with AI-Generated Content</h5>

<div id="S6.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px4.p1.1" class="ltx_p">AI-Generated Content (AIGC) denotes content produced via advanced generative FMsÂ <cite class="ltx_cite ltx_citemacro_cite">Wu etÂ al. (<a href="#bib.bib177" title="" class="ltx_ref">2023a</a>)</cite>. The strong generative capability of FMs offers the advantage of rapidly automating the creation of inexhaustible synthetic data.
This capability positions AIGC as a valuable supplementary data source for model training and evaluation in many tasksÂ <cite class="ltx_cite ltx_citemacro_cite">Xu etÂ al. (<a href="#bib.bib187" title="" class="ltx_ref">2024c</a>)</cite>. Despite some effortsÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib205" title="" class="ltx_ref">2023b</a>)</cite>, more potential opportunities and challenges for AIGC-aided FL have yet to be explored.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusions</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this survey, we have meticulously surveyed the intersection of FM and FL. We identified core challenges in efficiency, adaptability, and trustworthiness and proposed a comprehensive taxonomy of techniques in response to these challenges. In addition, we discussed future directions and applications in this research field, hoping to attract more breakthroughs in future research.</p>
</div>
<section id="S7.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Limitations</h3>

<div id="S7.SSx1.p1" class="ltx_para">
<p id="S7.SSx1.p1.1" class="ltx_p">FM and FL are very fast-moving fields. We have put a lot of effort into including the latest research efforts in the community in this survey. Therefore, we believe that our survey will help to inspire and push further research and innovation in these important areas. Our survey does not focus on experimental evaluation of the available ideas and systems. We believe that would be an important next step that we are leaving for future work.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acar etÂ al. (2021)</span>
<span class="ltx_bibblock">
Durmus AlpÂ Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=B7v4QMR6Z9w" title="" class="ltx_ref ltx_href">Federated learning based on dynamic regularization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adi etÂ al. (2018)</span>
<span class="ltx_bibblock">
Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.usenix.org/conference/usenixsecurity18/presentation/adi" title="" class="ltx_ref ltx_href">Turning your weakness into a strength: Watermarking deep neural networks by backdooring</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">27th USENIX Security Symposium (USENIX Security 18)</em>, pages 1615â€“1631, Baltimore, MD. USENIX Association.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aghajanyan etÂ al. (2021)</span>
<span class="ltx_bibblock">
Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.568" title="" class="ltx_ref ltx_href">Intrinsic dimensionality explains the effectiveness of language model fine-tuning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 7319â€“7328, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akshay and Rahul (2024)</span>
<span class="ltx_bibblock">
Singh Akshay and Thakur Rahul. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://2024.naacl.org/program/accepted_papers/" title="" class="ltx_ref ltx_href">Generalizable multilingual hate speech detection on low resource indian languages using fair selection in federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ammad-Ud-Din etÂ al. (2019)</span>
<span class="ltx_bibblock">
Muhammad Ammad-Ud-Din, Elena Ivannikova, SuleimanÂ A. Khan, Were Oyomno, Qiang Fu, KuanÂ Eeik Tan, and Adrian Flanagan. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1901.09888" title="" class="ltx_ref ltx_href">Federated collaborative filtering for privacy-preserving personalized recommendation system</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.09888</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azam etÂ al. (2023a)</span>
<span class="ltx_bibblock">
SheikhÂ Shams Azam, Tatiana Likhomanenko, Martin Pelikan, and JanÂ â€œHonzaâ€ Silovsky. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ASRU57964.2023.10389620" title="" class="ltx_ref ltx_href">Importance of smoothness induced by optimizers in fl4asr: Towards understanding federated learning for end-to-end asr</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, pages 1â€“8.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azam etÂ al. (2023b)</span>
<span class="ltx_bibblock">
SheikhÂ Shams Azam, Martin Pelikan, Vitaly Feldman, Kunal Talwar, Jan Silovsky, and Tatiana Likhomanenko. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=ozN92d7CHX" title="" class="ltx_ref ltx_href">Federated learning for speech recognition: Revisiting current trends towards large-scale ASR</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International Workshop on Federated Learning in the Age of Foundation Models in Conjunction with NeurIPS 2023</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Babakniya etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Sara Babakniya, Ahmed Elkordy, Yahya Ezzeldin, Qingfeng Liu, Kee-Bong Song, MOSTAFA EL-Khamy, and Salman Avestimehr. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=06quMTmtRV" title="" class="ltx_ref ltx_href">SLoRA: Federated parameter efficient fine-tuning of language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">International Workshop on Federated Learning in the Age of Foundation Models in Conjunction with NeurIPS 2023</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Babakniya etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Sara Babakniya, Souvik Kundu, Saurav Prakash, Yue Niu, and Salman Avestimehr. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=iHyhdpsnyi" title="" class="ltx_ref ltx_href">Revisiting sparsity hunting in federated learning: Why does sparsity consensus matter?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski etÂ al. (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf" title="" class="ltx_ref ltx_href">wav2vec 2.0: A framework for self-supervised learning of speech representations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volumeÂ 33, pages 12449â€“12460. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bagdasaryan etÂ al. (2020)</span>
<span class="ltx_bibblock">
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v108/bagdasaryan20a.html" title="" class="ltx_ref ltx_href">How to backdoor federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence and Statistics</em>, pages 2938â€“2948. PMLR.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Jiamu Bai, Daoyuan Chen, Bingchen Qian, Liuyi Yao, and Yaliang Li. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.11505" title="" class="ltx_ref ltx_href">Federated fine-tuning of large language models under heterogeneous language tasks and client resources</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.11505</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Sikai Bai, Jie Zhang, Shuaicheng Li, Song Guo, Jingcai Guo, Jun Hou, Tao Han, and Xiaocheng Lu. 2024b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2403.08506" title="" class="ltx_ref ltx_href">Diprompt: Disentangled prompt tuning for multiple latent domain generalization in federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.08506</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BenÂ Zaken etÂ al. (2022)</span>
<span class="ltx_bibblock">
Elad BenÂ Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-short.1" title="" class="ltx_ref ltx_href">BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, pages 1â€“9, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blanchard etÂ al. (2017)</span>
<span class="ltx_bibblock">
Peva Blanchard, ElÂ Mahdi ElÂ Mhamdi, Rachid Guerraoui, and Julien Stainer. 2017.

</span>
<span class="ltx_bibblock">Machine learning with adversaries: Byzantine tolerant gradient descent.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st International Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bobadilla etÂ al. (2013)</span>
<span class="ltx_bibblock">
J.Â Bobadilla, F.Â Ortega, A.Â Hernando, and A.Â GutiÃ©rrez. 2013.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1016/j.knosys.2013.03.012" title="" class="ltx_ref ltx_href">Recommender systems survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Knowledge-Based Systems</em>, 46:109â€“132.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bommasani etÂ al. (2021)</span>
<span class="ltx_bibblock">
Rishi Bommasani, DrewÂ A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, MichaelÂ S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, etÂ al. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2108.07258" title="" class="ltx_ref ltx_href">On the opportunities and risks of foundation models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.07258</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz etÂ al. (2021)</span>
<span class="ltx_bibblock">
Kallista Bonawitz, Peter Kairouz, Brendan McMahan, and Daniel Ramage. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3494834.3500240" title="" class="ltx_ref ltx_href">Federated learning and privacy: Building privacy-preserving systems for machine learning and data science on decentralized data</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Queue</em>, 19(5):87â€“114.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown etÂ al. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" title="" class="ltx_ref ltx_href">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volumeÂ 33, pages 1877â€“1901. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brunete etÂ al. (2021)</span>
<span class="ltx_bibblock">
Alberto Brunete, Ernesto Gambao, Miguel Hernando, and Raquel Cedazo. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.mdpi.com/1424-8220/21/6/2212" title="" class="ltx_ref ltx_href">Smart assistive architecture for the integration of iot devices, robotic systems, and multimodal interfaces in healthcare environments</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, 21(6):2212.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, and George Karypis. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=6Bo1vhoHolh" title="" class="ltx_ref ltx_href">Differentially private bias-term only fine-tuning of foundation models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS 2022</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck etÂ al. (2023)</span>
<span class="ltx_bibblock">
SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, YinÂ Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, MarcoÂ Tulio Ribeiro, and YiÂ Zhang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.12712" title="" class="ltx_ref ltx_href">Sparks of artificial general intelligence: Early experiments with gpt-4</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.12712</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai etÂ al. (2023)</span>
<span class="ltx_bibblock">
Dongqi Cai, Yaozong Wu, Shangguang Wang, FelixÂ Xiaozhu Lin, and Mengwei Xu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3570361.3592505" title="" class="ltx_ref ltx_href">Efficient federated learning for modern nlp</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th Annual International Conference on Mobile Computing and Networking</em>, ACM MobiCom â€™23, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao etÂ al. (2021)</span>
<span class="ltx_bibblock">
Xiaoyu Cao, Minghong Fang, Jia Liu, and NeilÂ Zhenqiang Gong. 2021.

</span>
<span class="ltx_bibblock">Fltrust: Byzantine-robust federated learning via trust bootstrapping.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ISOC Network and Distributed System Security Symposium (NDSS)</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CCPA (2023)</span>
<span class="ltx_bibblock">
CCPA. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://oag.ca.gov/privacy/ccpa" title="" class="ltx_ref ltx_href">California consumer privacy act (ccpa)</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chatterjee etÂ al. (2023)</span>
<span class="ltx_bibblock">
Pushpita Chatterjee, Debashis Das, and DandaÂ B Rawat. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.12944" title="" class="ltx_ref ltx_href">Use of federated learning and blockchain towards securing financial services</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.12944</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2024)</span>
<span class="ltx_bibblock">
Haokun Chen, Yao Zhang, Denis Krompass, Jindong Gu, and Volker Tresp. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1609/aaai.v38i10.29007" title="" class="ltx_ref ltx_href">Feddat: An approach for foundation model finetuning in multi-modal heterogeneous federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, 38(10):11285â€“11293.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2017)</span>
<span class="ltx_bibblock">
Yudong Chen, Lili Su, and Jiaming Xu. 2017.

</span>
<span class="ltx_bibblock">Distributed statistical machine learning in adversarial settings: Byzantine gradient descent.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Measurement and Analysis of Computing Systems</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chenthamarakshan etÂ al. (2023)</span>
<span class="ltx_bibblock">
Vijil Chenthamarakshan, SamuelÂ C. Hoffman, C.Â David Owen, Petra Lukacik, Claire Strain-Damerell, Daren Fearon, TikaÂ R. Malla, Anthony Tumber, ChristopherÂ J. Schofield, HelenÂ M.E. Duyvesteyn, Wanwisa Dejnirattisai, Loic Carrique, ThomasÂ S. Walter, GavinÂ R. Screaton, Tetiana Matviiuk, Aleksandra Mojsilovic, Jason Crain, MartinÂ A. Walsh, DavidÂ I. Stuart, and Payel Das. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1126/sciadv.adg7865" title="" class="ltx_ref ltx_href">Accelerating drug target inhibitor discovery with a deep generative foundation model</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Science Advances</em>, 9(25):eadg7865.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho etÂ al. (2024)</span>
<span class="ltx_bibblock">
YaeÂ Jee Cho, Luyang Liu, Zheng Xu, Aldi Fahrezi, and Gauri Joshi. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2401.06432" title="" class="ltx_ref ltx_href">Heterogeneous lora for federated fine-tuning of on-device foundation models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.06432</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yun-Wei Chu, Dong-Jun Han, and ChristopherÂ G. Brinton. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3589335.3651931" title="" class="ltx_ref ltx_href">Only send what you need: Learning to communicate efficiently in federated multilingual machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Companion Proceedings of the ACM on Web Conference 2024</em>, page 1548â€“1557, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau etÂ al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.747" title="" class="ltx_ref ltx_href">Unsupervised cross-lingual representation learning at scale</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 8440â€“8451, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng etÂ al. (2024)</span>
<span class="ltx_bibblock">
Wenlong Deng, Christos Thrampoulidis, and Xiaoxiao Li. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2310.18285" title="" class="ltx_ref ltx_href">Unlocking the potential of prompt-tuning in bridging generalized and personalized federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1423" title="" class="ltx_ref ltx_href">BERT: Pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171â€“4186, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diao etÂ al. (2021)</span>
<span class="ltx_bibblock">
Enmao Diao, Jie Ding, and Vahid Tarokh. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=TNkPBBYFkXg" title="" class="ltx_ref ltx_href">Hetero{fl}: Computation and communication efficient federated learning for heterogeneous clients</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, etÂ al. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1038/s42256-023-00626-4" title="" class="ltx_ref ltx_href">Parameter-efficient fine-tuning of large-scale pre-trained language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, 5(3):220â€“235.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong etÂ al. (2023)</span>
<span class="ltx_bibblock">
Chenhe Dong, Yuexiang Xie, Bolin Ding, Ying Shen, and Yaliang Li. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-emnlp.976" title="" class="ltx_ref ltx_href">Tunable soft prompts are messengers in federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 14665â€“14675, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy etÂ al. (2021)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=YicbFdNTTy" title="" class="ltx_ref ltx_href">An image is worth 16x16 words: Transformers for image recognition at scale</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yichao Du, Zhirui Zhang, Linan Yue, XuÂ Huang, Yuqing Zhang, Tong Xu, Linli Xu, and Enhong Chen. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP48485.2024.10447662" title="" class="ltx_ref ltx_href">Communication-efficient personalized federated learning for speech-to-text tasks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 10001â€“10005.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duchi etÂ al. (2015)</span>
<span class="ltx_bibblock">
JohnÂ C Duchi, MichaelÂ I Jordan, MartinÂ J Wainwright, and Andre Wibisono. 2015.

</span>
<span class="ltx_bibblock">Optimal rates for zero-order convex optimization: The power of two function evaluations.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Theory</em>, 61(5):2788â€“2806.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan etÂ al. (2023)</span>
<span class="ltx_bibblock">
Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, and Qiang Yang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.10049" title="" class="ltx_ref ltx_href">Fate-llm: A industrial grade federated learning framework for large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.10049</em>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang etÂ al. (2020)</span>
<span class="ltx_bibblock">
Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.usenix.org/system/files/sec20summer_fang_prepub.pdf" title="" class="ltx_ref ltx_href">Local model poisoning attacks to byzantine-robust federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">29th USENIX Security Symposium (USENIX Security 20)</em>, pages 1605â€“1622.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Wenzhi Fang, Ziyi Yu, Yuning Jiang, Yuanming Shi, ColinÂ N. Jones, and Yong Zhou. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TSP.2022.3214122" title="" class="ltx_ref ltx_href">Communication-efficient stochastic zeroth-order optimization for federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Signal Processing</em>, 70:5058â€“5073.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">FedML (2023)</span>
<span class="ltx_bibblock">
FedML. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://blog.fedml.ai/releasing-fedllm-build-your-own-large-language-models-on-proprietary-data-using-the-fedml-platform/" title="" class="ltx_ref ltx_href">Fedllm</a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Chun-Mei Feng, Bangjun Li, Xinxing Xu, Yong Liu, Huazhu Fu, and Wangmeng Zuo. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR52729.2023.00779" title="" class="ltx_ref ltx_href">Learning federated visual prompt in null space for mri reconstruction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 8064â€“8073.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Haozhe Feng, Tianyu Pang, Chao Du, Wei Chen, Shuicheng Yan, and Min Lin. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2301.12195" title="" class="ltx_ref ltx_href">Does federated learning really need backpropagation?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.12195</em>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng etÂ al. (2023c)</span>
<span class="ltx_bibblock">
Xiachong Feng, Xiaocheng Feng, Xiyuan Du, Min-Yen Kan, and Bing Qin. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.03275" title="" class="ltx_ref ltx_href">Adapter-based selective knowledge distillation for federated multi-domain meeting summarization</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.03275</em>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frankle and Carbin (2019)</span>
<span class="ltx_bibblock">
Jonathan Frankle and Michael Carbin. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=rJl-b3RcF7" title="" class="ltx_ref ltx_href">The lottery ticket hypothesis: Finding sparse, trainable neural networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.14524" title="" class="ltx_ref ltx_href">Chat-rec: Towards interactive and explainable llms-augmented recommender system</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.14524</em>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GDPR (2016)</span>
<span class="ltx_bibblock">
GDPR. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://data.europa.eu/eli/reg/2016/679/2016-05-04" title="" class="ltx_ref ltx_href">Regulation (eu) 2016/679 of the european parliament and of the council</a>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geiping etÂ al. (2020)</span>
<span class="ltx_bibblock">
Jonas Geiping, Hartmut Bauermeister, Hannah DrÃ¶ge, and Michael Moeller. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/c4ede56bbd98819ae6112b20ac6bf145-Paper.pdf" title="" class="ltx_ref ltx_href">Inverting gradients - how easy is it to break privacy in federated learning?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volumeÂ 33, pages 16937â€“16947. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Golovin etÂ al. (2020)</span>
<span class="ltx_bibblock">
Daniel Golovin, John Karro, Greg Kochanski, Chansoo Lee, Xingyou Song, and Qiuyi Zhang. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=Skep6TVYDB" title="" class="ltx_ref ltx_href">Gradientless descent: High-dimensional zeroth-order optimization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Good etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jack Good, Jimit Majmudar, Christophe Dupuy, Jixuan Wang, Charith Peris, Clement Chung, Richard Zemel, and Rahul Gupta. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.emnlp-industry.32" title="" class="ltx_ref ltx_href">Coordinated replay sample selection for continual federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track</em>, pages 331â€“342, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2023)</span>
<span class="ltx_bibblock">
GeminiÂ Team Google. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2312.11805" title="" class="ltx_ref ltx_href">Gemini: a family of highly capable multimodal models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.11805</em>.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gorbunov etÂ al. (2023)</span>
<span class="ltx_bibblock">
Eduard Gorbunov, Samuel HorvÃ¡th, Peter RichtÃ¡rik, and Gauthier Gidel. 2023.

</span>
<span class="ltx_bibblock">Variance reduction is an antidote to byzantines: Better rates, weaker assumptions and communication compression as a cherry on the top.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gunasekar etÂ al. (2023)</span>
<span class="ltx_bibblock">
Suriya Gunasekar, YiÂ Zhang, Jyoti Aneja, Caio CÃ©sarÂ Teodoro Mendes, AllieÂ Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo deÂ Rosa, Olli Saarikivi, Adil Salim, Shital Shah, HarkiratÂ Singh Behl, Xin Wang, SÃ©bastien Bubeck, Ronen Eldan, AdamÂ Tauman Kalai, YinÂ Tat Lee, and Yuanzhi Li. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2306.11644" title="" class="ltx_ref ltx_href">Textbooks are all you need</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.11644</em>.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Lei Guo, Ziang Lu, Junliang Yu, Quoc VietÂ Hung Nguyen, and Hongzhi Yin. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3589334.3645337" title="" class="ltx_ref ltx_href">Prompt-enhanced federated content representation learning for cross-domain recommendation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Web Conference 2024</em>, WWW â€™24, page 3139â€“3149, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo etÂ al. (2023)</span>
<span class="ltx_bibblock">
Tao Guo, Song Guo, Junxiao Wang, Xueyang Tang, and Wenchao Xu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TMC.2023.3302410" title="" class="ltx_ref ltx_href">Promptfl: Let federated participants cooperatively learn prompts instead of models - federated learning in age of foundation model</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Mobile Computing</em>, pages 1â€“15.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo and Yu (2022)</span>
<span class="ltx_bibblock">
XuÂ Guo and Han Yu. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2211.03154" title="" class="ltx_ref ltx_href">On the domain adaptation and generalization of pretrained language models: A survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.03154</em>.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Zhihan Guo, Yifei Zhang, Zhuo Zhang, Zenglin Xu, and Irwin King. 2024b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2024.findings-naacl.98" title="" class="ltx_ref ltx_href">FedLFC: Towards efficient federated multilingual modeling with LoRA-based language family clustering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: NAACL 2024</em>, pages 1519â€“1528, Mexico City, Mexico. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta etÂ al. (2022)</span>
<span class="ltx_bibblock">
Samyak Gupta, Yangsibo Huang, Zexuan Zhong, Tianyu Gao, Kai Li, and Danqi Chen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/35b5c175e139bff5f22a5361270fce87-Paper-Conference.pdf" title="" class="ltx_ref ltx_href">Recovering private text in federated learning of language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volumeÂ 35, pages 8130â€“8143. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan etÂ al. (2020)</span>
<span class="ltx_bibblock">
Suchin Gururangan, Ana MarasoviÄ‡, Swabha Swayamdipta, Kyle Lo, IzÂ Beltagy, Doug Downey, and NoahÂ A. Smith. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.740" title="" class="ltx_ref ltx_href">Donâ€™t stop pretraining: Adapt language models to domains and tasks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 8342â€“8360, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ha etÂ al. (2017)</span>
<span class="ltx_bibblock">
David Ha, AndrewÂ M. Dai, and QuocÂ V. Le. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=rkpACe1lx" title="" class="ltx_ref ltx_href">Hypernetworks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hansen and Ostermeier (2001)</span>
<span class="ltx_bibblock">
Nikolaus Hansen and Andreas Ostermeier. 2001.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/106365601750190398" title="" class="ltx_ref ltx_href">Completely derandomized self-adaptation in evolution strategies</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Evolutionary Computation</em>, 9(2):159â€“195.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2020)</span>
<span class="ltx_bibblock">
Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, MiÂ Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Xinghua Zhu, Jianzong Wang, LiÂ Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram, and Salman Avestimehr. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2007.13518" title="" class="ltx_ref ltx_href">Fedml: A research library and benchmark for federated machine learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.13518</em>.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby etÂ al. (2019)</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin DeÂ Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v97/houlsby19a.html" title="" class="ltx_ref ltx_href">Parameter-efficient transfer learning for NLP</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 36th International Conference on Machine Learning</em>, volumeÂ 97 of <em id="bib.bib66.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 2790â€“2799. PMLR.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2022)</span>
<span class="ltx_bibblock">
EdwardÂ J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, LuÂ Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="" class="ltx_ref ltx_href">LoRA: Low-rank adaptation of large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Xumin Huang, Peichun Li, Hongyang Du, Jiawen Kang, Dusit Niyato, DongÂ In Kim, and Yuan Wu. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/MNET.2024.3353377" title="" class="ltx_ref ltx_href">Federated learning-empowered ai-generated content in wireless networks</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">IEEE Network</em>, pages 1â€“1.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jeblick etÂ al. (2024)</span>
<span class="ltx_bibblock">
Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, AnnaÂ Theresa StÃ¼ber, Johanna Topalis, Tobias Weber, Philipp Wesp, BastianÂ Oliver Sabel, Jens Ricke, etÂ al. 2024.

</span>
<span class="ltx_bibblock">Chatgpt makes medicine easy to swallow: an exploratory case study on simplified radiology reports.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">European radiology</em>, 34(5):2817â€“2825.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jere etÂ al. (2020)</span>
<span class="ltx_bibblock">
MalharÂ S Jere, Tyler Farnan, and Farinaz Koushanfar. 2020.

</span>
<span class="ltx_bibblock">A taxonomy of attacks on federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">IEEE Security &amp; Privacy</em>, 19(2).

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia etÂ al. (2023)</span>
<span class="ltx_bibblock">
Junteng Jia, KeÂ Li, Mani Malek, Kshitiz Malik, Jay Mahadeokar, Ozlem Kalinli, and Frank Seide. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ASRU57964.2023.10389738" title="" class="ltx_ref ltx_href">Joint federated learning and personalization for on-device asr</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, pages 1â€“8.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia etÂ al. (2022)</span>
<span class="ltx_bibblock">
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-031-19827-4_41" title="" class="ltx_ref ltx_href">Visual prompt tuning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Computer Vision â€“ ECCV 2022</em>, pages 709â€“727, Cham. Springer Nature Switzerland.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Jingang Jiang, Xiangyang Liu, and Chenyou Fan. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2307.13896" title="" class="ltx_ref ltx_href">Low-parameter federated learning with large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.13896</em>.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Lekang Jiang, Filip Svoboda, and NicholasÂ Donald Lane. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=ESCL5T3EgV" title="" class="ltx_ref ltx_href">FDAPT: Federated domain-adaptive pre-training for language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">International Workshop on Federated Learning in the Age of Foundation Models in Conjunction with NeurIPS 2023</em>.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2023c)</span>
<span class="ltx_bibblock">
Yuang Jiang, Shiqiang Wang, VÃ­ctor Valls, BongÂ Jun Ko, Wei-Han Lee, KinÂ K. Leung, and Leandros Tassiulas. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TNNLS.2022.3166101" title="" class="ltx_ref ltx_href">Model pruning enables efficient federated learning on edge devices</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, 34(12):10374â€“10386.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jo and Gebru (2020)</span>
<span class="ltx_bibblock">
EunÂ Seo Jo and Timnit Gebru. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3351095.3372829" title="" class="ltx_ref ltx_href">Lessons from archives: Strategies for collecting sociocultural data in machine learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>, FAcctT â€™20, pages 306â€“316, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson etÂ al. (2017)</span>
<span class="ltx_bibblock">
Melvin Johnson, Mike Schuster, QuocÂ V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda ViÃ©gas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00065" title="" class="ltx_ref ltx_href">Googleâ€™s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 5:339â€“351.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi etÂ al. (2022)</span>
<span class="ltx_bibblock">
Madhura Joshi, Ankit Pal, and Malaikannan Sankarasubbu. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3533708" title="" class="ltx_ref ltx_href">Federated learning for healthcare domain - pipeline, applications and challenges</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Comput. Healthcare</em>, 3(4).

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz etÂ al. (2021)</span>
<span class="ltx_bibblock">
Peter Kairouz etÂ al. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1561/2200000083" title="" class="ltx_ref ltx_href">Advances and open problems in federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Foundations and Trends in Machine Learning</em>, 14(1â€“2):1â€“210.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yan Kang, Tao Fan, Hanlin Gu, Xiaojin Zhang, Lixin Fan, and Qiang Yang. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2311.17431" title="" class="ltx_ref ltx_href">Grounding foundation models through federated transfer learning: A general framework</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.17431</em>.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yeachan Kim, Junho Kim, Wing-Lam Mok, Jun-Hyung Park, and SangKeun Lee. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-acl.75" title="" class="ltx_ref ltx_href">Client-customized adaptation for parameter-efficient federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 1159â€“1172, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirillov etÂ al. (2023)</span>
<span class="ltx_bibblock">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, AlexanderÂ C Berg, Wan-Yen Lo, etÂ al. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.48550/arXiv.2304.02643" title="" class="ltx_ref ltx_href">Segment anything</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.02643</em>.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, and Jingren Zhou. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2309.00363" title="" class="ltx_ref ltx_href">Federatedscope-llm: A comprehensive package for fine-tuning large language models in federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.00363</em>.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester etÂ al. (2021)</span>
<span class="ltx_bibblock">
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.243" title="" class="ltx_ref ltx_href">The power of scale for parameter-efficient prompt tuning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 3045â€“3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2021a)</span>
<span class="ltx_bibblock">
Ang Li, Jingwei Sun, Binghui Wang, Lin Duan, Sicheng Li, Yiran Chen, and Hai Li. 2021a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3453142.3492909" title="" class="ltx_ref ltx_href">Lotteryfl: Empower edge intelligence with personalized and communication-efficient federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">2021 IEEE/ACM Symposium on Edge Computing (SEC)</em>, pages 68â€“79.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Guanghao Li, Wansen Wu, Yan Sun, LiÂ Shen, Baoyuan Wu, and Dacheng Tao. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=dUVejidXO7" title="" class="ltx_ref ltx_href">Visual prompt based personalized federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2021b)</span>
<span class="ltx_bibblock">
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven ChuÂ Hong Hoi. 2021b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf" title="" class="ltx_ref ltx_href">Align before fuse: Vision and language representation learning with momentum distillation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volumeÂ 34, pages 9694â€“9705. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2022)</span>
<span class="ltx_bibblock">
Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICDE53745.2022.00077" title="" class="ltx_ref ltx_href">Federated learning on non-iid data silos: An experimental study</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">2022 IEEE 38th International Conference on Data Engineering (ICDE)</em>, pages 965â€“978.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Shenghui Li, Edith Ngai, and Thiemo Voigt. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TII.2021.3128164" title="" class="ltx_ref ltx_href">Byzantine-robust aggregation in federated learning empowered industrial iot</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Industrial Informatics</em>, 19(2):1165â€“1175.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Shenghui Li, Edith Ngai, Fanghua Ye, LiÂ Ju, Tianru Zhang, and Thiemo Voigt. 2024b.

</span>
<span class="ltx_bibblock">Blades: A unified benchmark suite for byzantine attacks and defenses in federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">2024 IEEE/ACM Ninth International Conference on Internet-of-Things Design and Implementation (IoTDI)</em>.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Shenghui Li, Edith C.-H. Ngai, and Thiemo Voigt. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TBDATA.2023.3237397" title="" class="ltx_ref ltx_href">An experimental study of byzantine-robust aggregation schemes in federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Big Data</em>, pages 1â€“13.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2020)</span>
<span class="ltx_bibblock">
Tian Li, AnitÂ Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlsys.org/paper_files/paper/2020/file/1f5fe83998a09396ebe6477d9475ba0c-Paper.pdf" title="" class="ltx_ref ltx_href">Federated optimization in heterogeneous networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems</em>, volumeÂ 2, pages 429â€“450.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Wang (2024)</span>
<span class="ltx_bibblock">
XiÂ Li and Jiaqi Wang. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.01857" title="" class="ltx_ref ltx_href">Position paper: Assessing robustness, privacy, and fairness in federated learning integrated with foundation models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.01857</em>.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023c)</span>
<span class="ltx_bibblock">
XiÂ Li, Songhe Wang, Chen Wu, Hao Zhou, and Jiaqi Wang. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=BrcHuO2BVc" title="" class="ltx_ref ltx_href">Backdoor threats from compromised foundation models to federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">International Workshop on Federated Learning in the Age of Foundation Models in Conjunction with NeurIPS 2023</em>.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023d)</span>
<span class="ltx_bibblock">
XiÂ Li, Chen Wu, and Jiaqi Wang. 2023d.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2311.18350" title="" class="ltx_ref ltx_href">Unveiling backdoor risks brought by foundation models in heterogeneous federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.18350</em>.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2024c)</span>
<span class="ltx_bibblock">
XiÂ Li, Chen Wu, and Jiaqi Wang. 2024c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://link.springer.com/chapter/10.1007/978-981-97-2259-4_13" title="" class="ltx_ref ltx_href">Unveiling backdoor risks brought byÂ foundation models inÂ heterogeneous federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">Advances in Knowledge Discovery and Data Mining</em>, pages 168â€“181, Singapore. Springer Nature Singapore.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Liang (2021)</span>
<span class="ltx_bibblock">
XiangÂ Lisa Li and Percy Liang. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.353" title="" class="ltx_ref ltx_href">Prefix-tuning: Optimizing continuous prompts for generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 4582â€“4597. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Chen (2021)</span>
<span class="ltx_bibblock">
Zan Li and LiÂ Chen. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/WCSP52459.2021.9613620" title="" class="ltx_ref ltx_href">Communication-efficient decentralized zeroth-order method on heterogeneous data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">2021 13th International Conference on Wireless Communications and Signal Processing (WCSP)</em>, pages 1â€“6.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lialin etÂ al. (2023)</span>
<span class="ltx_bibblock">
Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.48550/arXiv.2303.15647" title="" class="ltx_ref ltx_href">Scaling down to scale up: A guide to parameter-efficient fine-tuning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.15647</em>.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zihao Lin, Yan Sun, Yifan Shi, Xueqian Wang, Lifu Huang, LiÂ Shen, and Dacheng Tao. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2310.03123" title="" class="ltx_ref ltx_href">Efficient federated prompt tuning for black-box large pre-trained models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2310.03123.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lincy and Kowshalya (2020)</span>
<span class="ltx_bibblock">
MÂ Lincy and AÂ Meena Kowshalya. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:234514776" title="" class="ltx_ref ltx_href">Early detection of type-2 diabetes using federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">International Journal of Scientific Research in Science, Engineering and Technology</em>, 12:257â€“267.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ling etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, and Ying Shen. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.05926" title="" class="ltx_ref ltx_href">On the convergence of zeroth-order federated tuning in large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.05926</em>.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lit etÂ al. (2022)</span>
<span class="ltx_bibblock">
Zhengyang Lit, Shijing Sit, Jianzong Wang, and Jing Xiao. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/IJCNN55064.2022.9892845" title="" class="ltx_ref ltx_href">Federated split bert for heterogeneous text classification</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">2022 International Joint Conference on Neural Networks (IJCNN)</em>, pages 1â€“8.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3560815" title="" class="ltx_ref ltx_href">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">ACM Comput. Surv.</em>, 55(9).

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, AlfredÂ O. HeroÂ III, and PramodÂ K. Varshney. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/MSP.2020.3003837" title="" class="ltx_ref ltx_href">A primer on zeroth-order optimization in signal processing and machine learning: Principals, recent advances, and applications</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>, 37(5):43â€“54.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Tao Liu, Zhi Wang, Hui He, Wei Shi, Liangliang Lin, Ran An, and Chenhao Li. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3390/app13105877" title="" class="ltx_ref ltx_href">Efficient and secure federated learning for financial applications</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, 13(10).

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023c)</span>
<span class="ltx_bibblock">
Xiao-Yang Liu, Rongyi Zhu, Daochen Zha, Jiechao Gao, Shan Zhong, and Meikang Qiu. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2312.17493" title="" class="ltx_ref ltx_href">Differentially private low-rank adaptation of large language model using federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.17493</em>.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023d)</span>
<span class="ltx_bibblock">
YiÂ Liu, Xiaohan Bi, Lei Li, Sishuo Chen, Wenkai Yang, and XuÂ Sun. 2023d.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-acl.327" title="" class="ltx_ref ltx_href">Communication efficient federated learning for multilingual neural machine translation with adapter</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 5315â€“5328, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1907.11692" title="" class="ltx_ref ltx_href">Roberta: A robustly optimized bert pretraining approach</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Wang Lu, Xixu Hu, Jindong Wang, and Xing Xie. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://sites.computer.org/debull/A23mar/p52.pdf" title="" class="ltx_ref ltx_href">Fedclip: Fast generalization and personalization for CLIP in federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">IEEE Data Eng. Bull.</em>, 46(1):52â€“66.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Wang Lu, Hao Yu, Jindong Wang, Damien Teney, Haohan Wang, Yiqiang Chen, Qiang Yang, Xing Xie, and Xiangyang Ji. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2310.05143" title="" class="ltx_ref ltx_href">Zoopfl: Exploring black-box foundation models for personalized federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.05143</em>.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Han Yu, Xingjun Ma, Chen Chen, Lichao Sun, Jun Zhao, Qiang Yang, and PhilipÂ S. Yu. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TNNLS.2022.3216981" title="" class="ltx_ref ltx_href">Privacy and robustness in federated learning: Attacks and defenses</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, pages 1â€“21.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malaviya etÂ al. (2023)</span>
<span class="ltx_bibblock">
Shubham Malaviya, Manish Shukla, and Sachin Lodha. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v232/malaviya23a.html" title="" class="ltx_ref ltx_href">Reducing communication overhead in federated learning for pre-trained language models using parameter-efficient finetuning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 2nd Conference on Lifelong Learning Agents</em>, volume 232 of <em id="bib.bib113.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 456â€“469. PMLR.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malladi etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, JasonÂ D. Lee, Danqi Chen, and Sanjeev Arora. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=CcsdvOOzMp" title="" class="ltx_ref ltx_href">Fine-tuning language models with just forward passes</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">Workshop on Efficient Systems for Foundation Models @ ICML2023</em>.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malladi etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, JasonÂ D Lee, Danqi Chen, and Sanjeev Arora. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/a627810151be4d13f907ac898ff7e948-Paper-Conference.pdf" title="" class="ltx_ref ltx_href">Fine-tuning language models with just forward passes</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volumeÂ 36, pages 53038â€“53075. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mangrulkar etÂ al. (2022)</span>
<span class="ltx_bibblock">
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. 2022.

</span>
<span class="ltx_bibblock">Peft: State-of-the-art parameter-efficient fine-tuning methods.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/huggingface/peft" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/huggingface/peft</a>.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manoel etÂ al. (2023)</span>
<span class="ltx_bibblock">
Andrea Manoel, Mirian del CarmenÂ Hipolito Garcia, Tal Baumel, Shize Su, Jialei Chen, Robert Sim, Dan Miller, Danny Karmon, and Dimitrios Dimitriadis. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v209/manoel23a.html" title="" class="ltx_ref ltx_href">Federated multilingual models for medical transcript analysis</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Conference on Health, Inference, and Learning</em>, volume 209 of <em id="bib.bib117.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 147â€“162. PMLR.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maritan etÂ al. (2023)</span>
<span class="ltx_bibblock">
Alessio Maritan, Subhrakanti Dey, and Luca Schenato. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2309.17174" title="" class="ltx_ref ltx_href">Fedzen: Towards superlinear zeroth-order federated learning via incremental hessian estimation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.17174</em>.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan etÂ al. (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and BlaiseÂ Aguera yÂ Arcas. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v54/mcmahan17a.html" title="" class="ltx_ref ltx_href">Communication-efficient learning of deep networks from decentralized data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>, pages 1273â€“1282. PMLR.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mugunthan etÂ al. (2019)</span>
<span class="ltx_bibblock">
Vaikkunth Mugunthan, Antigoni Polychroniadou, David Byrd, and TuckerÂ Hybinette Balch. 2019.

</span>
<span class="ltx_bibblock">Smpai: Secure multi-party computation for federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">Proceedings of the NeurIPS 2019 Workshop on Robust AI in Financial Services</em>, volumeÂ 21. MIT Press Cambridge, MA, USA.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen etÂ al. (2024)</span>
<span class="ltx_bibblock">
DuyÂ Phuong Nguyen, J.Â Pablo Munoz, and Ali Jannesari. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2404.15182" title="" class="ltx_ref ltx_href">Flora: Enhancing vision-language models with parameter-efficient federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.15182</em>.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/blog/chatgpt/" title="" class="ltx_ref ltx_href">Chatgpt</a>.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.08774" title="" class="ltx_ref ltx_href">Gpt-4 technical report</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panagoulias etÂ al. (2024)</span>
<span class="ltx_bibblock">
DimitriosÂ P. Panagoulias, Maria Virvou, and GeorgeÂ A. Tsihrintzis. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.01730" title="" class="ltx_ref ltx_href">Evaluating llm â€“ generated multimodal diagnosis from medical images and symptom analysis</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.01730</em>.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pandya etÂ al. (2023)</span>
<span class="ltx_bibblock">
Sharnil Pandya, Gautam Srivastava, Rutvij Jhaveri, M.Â Rajasekhara Babu, Sweta Bhattacharya, Praveen KumarÂ Reddy Maddikunta, Spyridon Mastorakis, Md.Â Jalil Piran, and ThippaÂ Reddy Gadekallu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1016/j.seta.2022.102987" title="" class="ltx_ref ltx_href">Federated learning for smart cities: A comprehensive survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">Sustainable Energy Technologies and Assessments</em>, 55:102987.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park etÂ al. (2021)</span>
<span class="ltx_bibblock">
Jungwuk Park, Dong-Jun Han, Minseok Choi, and Jaekyun Moon. 2021.

</span>
<span class="ltx_bibblock">Sageflow: Robust federated learning against both stragglers and adversaries.

</span>
<span class="ltx_bibblock">In <em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volumeÂ 34, pages 840â€“851. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ping etÂ al. (2024)</span>
<span class="ltx_bibblock">
Siqi Ping, Yuzhu Mao, Yang Liu, Xiao-Ping Zhang, and Wenbo Ding. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=JDmAymuFFQ" title="" class="ltx_ref ltx_href">FL-TAC: Enhanced fine-tuning in federated learning via low-rank, task-specific adapter clustering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">ICLR 2024 Workshop on Large Language Model (LLM) Agents</em>.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pires etÂ al. (2019)</span>
<span class="ltx_bibblock">
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1906.01502" title="" class="ltx_ref ltx_href">How multilingual is multilingual bert?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.01502</em>.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zhen Qin, Daoyuan Chen, Bingchen Qian, Bolin Ding, Yaliang Li, and Shuiguang Deng. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2312.06353" title="" class="ltx_ref ltx_href">Federated full-parameter tuning of billion-sized language models with communication cost under 18 kilobytes</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 41th International Conference on Machine Learning</em>.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Chen Qiu, Xingyu Li, ChaithanyaÂ Kumar Mummadi, MadanÂ Ravi Ganesh, Zhenzhen Li, LuÂ Peng, and Wan-Yi Lin. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=NW31gAylIm" title="" class="ltx_ref ltx_href">Federated text-driven prompt generation for vision-language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, etÂ al. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v139/radford21a.html" title="" class="ltx_ref ltx_href">Learning transferable visual models from natural language supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 8748â€“8763. PMLR.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2023)</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v202/radford23a.html" title="" class="ltx_ref ltx_href">Robust speech recognition via large-scale weak supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine Learning</em>, volume 202 of <em id="bib.bib132.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 28492â€“28518. PMLR.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford and Wu (2019)</span>
<span class="ltx_bibblock">
Alec Radford and Jeffrey Wu. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" title="" class="ltx_ref ltx_href">Rewon child, david luan, dario amodei, and ilya sutskever. 2019</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">Language models are unsupervised multitask learners. OpenAI blog</em>, 1(8):9.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh etÂ al. (2021)</span>
<span class="ltx_bibblock">
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v139/ramesh21a.html" title="" class="ltx_ref ltx_href">Zero-shot text-to-image generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib134.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 8821â€“8831. PMLR.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramu etÂ al. (2022)</span>
<span class="ltx_bibblock">
SwarnaÂ Priya Ramu, Parimala Boopalan, Quoc-Viet Pham, Praveen KumarÂ Reddy Maddikunta, Thien Huynh-The, Mamoun Alazab, ThanhÂ Thi Nguyen, and ThippaÂ Reddy Gadekallu. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1016/j.scs.2021.103663" title="" class="ltx_ref ltx_href">Federated learning enabled digital twins for smart cities: Concepts, recent advances, and future directions</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">Sustainable Cities and Society</em>, 79:103663.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddi etÂ al. (2021)</span>
<span class="ltx_bibblock">
SashankÂ J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub KoneÄnÃ½, Sanjiv Kumar, and HughÂ Brendan McMahan. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=LkFG3lB13U5" title="" class="ltx_ref ltx_href">Adaptive federated optimization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reed etÂ al. (2022)</span>
<span class="ltx_bibblock">
Scott Reed etÂ al. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=1ikK0kHjvj" title="" class="ltx_ref ltx_href">A generalist agent</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>.

</span>
<span class="ltx_bibblock">Featured Certification, Outstanding Certification.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reisizadeh etÂ al. (2020)</span>
<span class="ltx_bibblock">
Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v108/reisizadeh20a.html" title="" class="ltx_ref ltx_href">Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib138.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics</em>, volume 108 of <em id="bib.bib138.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 2021â€“2031. PMLR.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren etÂ al. (2024)</span>
<span class="ltx_bibblock">
Chao Ren, Han Yu, Hongyi Peng, Xiaoli Tang, Anran Li, Yulan Gao, AlysaÂ Ziying Tan, BoÂ Zhao, Xiaoxiao Li, Zengxiang Li, and Qiang Yang. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2404.15381" title="" class="ltx_ref ltx_href">Advances and open challenges in federated learning with foundation models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.15381</em>.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rieke etÂ al. (2020)</span>
<span class="ltx_bibblock">
Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, HolgerÂ R Roth, Shadi Albarqouni, Spyridon Bakas, MathieuÂ N Galtier, BennettÂ A Landman, Klaus Maier-Hein, etÂ al. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1038/s41746-020-00323-1" title="" class="ltx_ref ltx_href">The future of digital health with federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib140.1.1" class="ltx_emph ltx_font_italic">NPJ digital medicine</em>, 3(1):119.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">RodrÃ­guez-Barroso etÂ al. (2023)</span>
<span class="ltx_bibblock">
Nuria RodrÃ­guez-Barroso, Daniel JimÃ©nez-LÃ³pez, MÂ Victoria LuzÃ³n, Francisco Herrera, and Eugenio MartÃ­nez-CÃ¡mara. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.sciencedirect.com/science/article/abs/pii/S1566253522001439" title="" class="ltx_ref ltx_href">Survey on federated learning threats: Concepts, taxonomy on attacks and defences, experimental study and challenges</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">Information Fusion</em>, 90:148â€“173.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roth etÂ al. (2024)</span>
<span class="ltx_bibblock">
HolgerÂ R. Roth, Ziyue Xu, Yuan-Ting Hsieh, Adithya Renduchintala, Isaac Yang, Zhihong Zhang, Yuhong Wen, Sean Yang, Kevin Lu, Kristopher Kersten, Camir Ricketts, Daguang Xu, Chester Chen, Yan Cheng, and Andrew Feng. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.07792" title="" class="ltx_ref ltx_href">Empowering federated learning for massive models with nvidia flare</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.07792</em>.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rui etÂ al. (2024)</span>
<span class="ltx_bibblock">
Wang Rui, YuÂ Tong, Zhang Ruiyi, Kim Sungchul, Rossi RyanÂ A., Zhao Handong, WuÂ Junda, Mitra Subrata, Yao Lina, and Henao Ricardo. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/pdf?id=bpUgtLeSAp" title="" class="ltx_ref ltx_href">Personalized federated learning for text classification with gradient-free prompt tuning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib143.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh etÂ al. (2020)</span>
<span class="ltx_bibblock">
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1910.01108" title="" class="ltx_ref ltx_href">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib144.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.01108</em>.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seo etÂ al. (2021)</span>
<span class="ltx_bibblock">
Sejin Seo, Seung-Woo Ko, Jihong Park, Seong-Lyun Kim, and Mehdi Bennis. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/SPAWC51858.2021.9593126" title="" class="ltx_ref ltx_href">Communication-efficient and personalized federated lottery ticket learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib145.1.1" class="ltx_emph ltx_font_italic">2021 IEEE 22nd International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)</em>, pages 581â€“585.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah and Lau (2023)</span>
<span class="ltx_bibblock">
SuhailÂ Mohmad Shah and Vincent K.Â N. Lau. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TNNLS.2021.3131614" title="" class="ltx_ref ltx_href">Model compression for communication efficient federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib146.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, 34(9):5937â€“5951.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen etÂ al. (2021)</span>
<span class="ltx_bibblock">
Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang Fang, Jianwei Yin, and Ting Wang. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3460120.3485370" title="" class="ltx_ref ltx_href">Backdoor pre-trained models can transfer to all</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib147.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security</em>, CCS â€™21. ACM.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Jaemin Shin, Hyungjun Yoon, Seungjoo Lee, Sungjoon Park, Yunxin Liu, Jinho Choi, and Sung-Ju Lee. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.emnlp-main.734" title="" class="ltx_ref ltx_href">FedTherapist: Mental health monitoring with user-generated linguistic expressions on smartphones via federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib148.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 11971â€“11988, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Jiyun Shin, Jinhyun Ahn, Honggu Kang, and Joonhyuk Kang. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.14579" title="" class="ltx_ref ltx_href">Fedsplitx: Federated split learning for computationally-constrained heterogeneous clients</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib149.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.14579</em>.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh etÂ al. (2019)</span>
<span class="ltx_bibblock">
Abhishek Singh, Praneeth Vepakomma, Otkrist Gupta, and Ramesh Raskar. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1909.09145" title="" class="ltx_ref ltx_href">Detailed comparison of communication efficiency of split learning and federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib150.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.09145</em>.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spall (1992)</span>
<span class="ltx_bibblock">
J.C. Spall. 1992.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://ieeexplore.ieee.org/document/119632" title="" class="ltx_ref ltx_href">Multivariate stochastic approximation using a simultaneous perturbation gradient approximation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Automatic Control</em>, 37(3):332â€“341.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su etÂ al. (2023)</span>
<span class="ltx_bibblock">
Shangchao Su, Bin Li, and Xiangyang Xue. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2311.11227" title="" class="ltx_ref ltx_href">Fedra: A random allocation strategy for federated tuning to unleash the power of heterogeneous clients</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib152.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.11227</em>.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su etÂ al. (2024)</span>
<span class="ltx_bibblock">
Shangchao Su, Mingzhao Yang, Bin Li, and Xiangyang Xue. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1609/aaai.v38i13.29434" title="" class="ltx_ref ltx_href">Federated adaptive prompt tuning for multi-domain collaborative learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib153.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, 38(13):15117â€“15125.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2023)</span>
<span class="ltx_bibblock">
Guangyu Sun, Matias Mendieta, Jun Luo, Shandong Wu, and Chen Chen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content/ICCV2023/html/Sun_FedPerfix_Towards_Partial_Model_Personalization_of_Vision_Transformers_in_Federated_ICCV_2023_paper.html" title="" class="ltx_ref ltx_href">Fedperfix: Towards partial model personalization of vision transformers in federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib154.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 4988â€“4998.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Guangyu Sun, Matias Mendieta, Taojiannan Yang, and Chen Chen. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.48550/arXiv.2210.01708" title="" class="ltx_ref ltx_href">Conquering the communication constraints to enable large pre-trained models in federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib155.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.01708</em>.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Jingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang, Daguang Xu, Yiran Chen, and HolgerÂ R Roth. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.48550/arXiv.2310.01467" title="" class="ltx_ref ltx_href">Fedbpt: Efficient federated black-box prompt tuning for large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib156.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 41th International Conference on Machine Learning</em>.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Tianxiang Sun, Zhengfu He, Hong Qian, Yunhua Zhou, Xuanjing Huang, and Xipeng Qiu. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.emnlp-main.259" title="" class="ltx_ref ltx_href">BBTv2: Towards a gradient-free future with large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib157.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 3916â€“3930, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2022c)</span>
<span class="ltx_bibblock">
Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. 2022c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v162/sun22e.html" title="" class="ltx_ref ltx_href">Black-box tuning for language-model-as-a-service</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib158.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 39th International Conference on Machine Learning</em>, volume 162 of <em id="bib.bib158.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 20841â€“20855. PMLR.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Youbang Sun, Zitao Li, Yaliang Li, and Bolin Ding. 2024b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=NLPzL6HWNl" title="" class="ltx_ref ltx_href">Improving loRA in privacy-preserving federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib159.1.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tamirisa etÂ al. (2024)</span>
<span class="ltx_bibblock">
Rishub Tamirisa, John Won, Chengjun Lu, Ron Arel, and Andy Zhou. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=TXtRWPZIZ0" title="" class="ltx_ref ltx_href">Fedselect: Customized selection of parameters for fine-tuning during personalized federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib160.1.1" class="ltx_emph ltx_font_italic">Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities</em>.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.04360" title="" class="ltx_ref ltx_href">Does synthetic data generation of llms help clinical text mining?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib161.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.04360</em>.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2020)</span>
<span class="ltx_bibblock">
Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2008.00401" title="" class="ltx_ref ltx_href">Multilingual translation with extensible multilingual pretraining and finetuning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib162.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2008.00401</em>.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tekgul etÂ al. (2021)</span>
<span class="ltx_bibblock">
Buse G.Â A. Tekgul, Yuxi Xia, Samuel Marchal, and N.Â Asokan. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/SRDS53918.2021.00038" title="" class="ltx_ref ltx_href">Waffle: Watermarking in federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib163.1.1" class="ltx_emph ltx_font_italic">2021 40th International Symposium on Reliable Distributed Systems (SRDS)</em>, pages 310â€“320.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thapa etÂ al. (2022)</span>
<span class="ltx_bibblock">
Chandra Thapa, PathumÂ Chamikara MahawagaÂ Arachchige, Seyit Camtepe, and Lichao Sun. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1609/aaai.v36i8.20825" title="" class="ltx_ref ltx_href">Splitfed: When federated learning meets split learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib164.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, 36(8):8485â€“8493.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yuanyishu Tian, Yao Wan, Lingjuan Lyu, Dezhong Yao, Hai Jin, and Lichao Sun. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3510033" title="" class="ltx_ref ltx_href">Fedbert: When federated learning meets pre-training</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib165.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Intell. Syst. Technol.</em>, 13(4).

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, etÂ al. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2302.13971" title="" class="ltx_ref ltx_href">Llama: Open and efficient foundation language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib166.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, etÂ al. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2307.09288" title="" class="ltx_ref ltx_href">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib167.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsouvalas etÂ al. (2023)</span>
<span class="ltx_bibblock">
Vasileios Tsouvalas, Yuki Asano, and Aaqib Saeed. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2311.17299" title="" class="ltx_ref ltx_href">Federated fine-tuning of foundation models via probabilistic masking</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib168.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.17299</em>.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Minh Vu, Truc Nguyen, Treâ€™ Jeter, and MyÂ T.Â Thai. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v238/vu24a.html" title="" class="ltx_ref ltx_href">Analysis of privacy leakage in federated large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib169.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 27th International Conference on Artificial Intelligence and Statistics</em>, volume 238 of <em id="bib.bib169.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 1423â€“1431. PMLR.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang (2023)</span>
<span class="ltx_bibblock">
Eric Wang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/tloen/alpaca-lora" title="" class="ltx_ref ltx_href">Alpaca-lora</a>.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Haoyu Wang, Handong Zhao, Yaqing Wang, Tong Yu, Jiuxiang Gu, and Jing Gao. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3485447.3511988" title="" class="ltx_ref ltx_href">Fedkc: Federated knowledge composition for multilingual natural language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib171.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM Web Conference 2022</em>, WWW â€™22, page 1839â€“1850, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Xinâ€™ao Wang, Huan Li, KeÂ Chen, and Lidan Shou. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.24963/ijcai.2023/483" title="" class="ltx_ref ltx_href">Fedbfpt: An efficient federated learning framework for bert further pre-training</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib172.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23</em>, pages 4344â€“4352. International Joint Conferences on Artificial Intelligence Organization.

</span>
<span class="ltx_bibblock">Main Track.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2020)</span>
<span class="ltx_bibblock">
Kang Wei, Jun Li, Ming Ding, Chuan Ma, HowardÂ H. Yang, Farhad Farokhi, Shi Jin, Tony Q.Â S. Quek, and H.Â VincentÂ Poor. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TIFS.2020.2988575" title="" class="ltx_ref ltx_href">Federated learning with differential privacy: Algorithms and performance analysis</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib173.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics and Security</em>, 15:3454â€“3469.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weller etÂ al. (2022)</span>
<span class="ltx_bibblock">
Orion Weller, Marc Marone, Vladimir Braverman, Dawn Lawrie, and Benjamin VanÂ Durme. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.naacl-main.101" title="" class="ltx_ref ltx_href">Pretrained models for multilingual federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib174.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 1413â€“1421, Seattle, United States. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WoisetschlÃ¤ger etÂ al. (2024)</span>
<span class="ltx_bibblock">
Herbert WoisetschlÃ¤ger, Alexander Isenko, Shiqiang Wang, Ruben Mayer, and Hans-Arno Jacobsen. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2401.04472" title="" class="ltx_ref ltx_href">A survey on efficient federated learning methods for foundation model training</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib175.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.04472</em>.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2020a)</span>
<span class="ltx_bibblock">
Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. 2020a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2006.03677" title="" class="ltx_ref ltx_href">Visual transformers: Token-based image representation and processing for computer vision</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib176.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.03677</em>.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and Hong Lin. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2304.06632" title="" class="ltx_ref ltx_href">Ai-generated content (aigc): A survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib177.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.06632</em>.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, QiÂ Liu, Hui Xiong, and Enhong Chen. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.19860" title="" class="ltx_ref ltx_href">A survey on large language models for recommendation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib178.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.19860</em>.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2023c)</span>
<span class="ltx_bibblock">
Panlong Wu, Kangshuo Li, Ting Wang, and Fangxin Wang. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2312.15926" title="" class="ltx_ref ltx_href">Fedms: Federated learning with mixture of sparsely activated foundations models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib179.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.15926</em>.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Dredze (2020)</span>
<span class="ltx_bibblock">
Shijie Wu and Mark Dredze. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.repl4nlp-1.16" title="" class="ltx_ref ltx_href">Are all languages created equal in multilingual BERT?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib180.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 5th Workshop on Representation Learning for NLP</em>, pages 120â€“130, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2020b)</span>
<span class="ltx_bibblock">
Zhaoxian Wu, Qing Ling, Tianyi Chen, and GeorgiosÂ B. Giannakis. 2020b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TSP.2020.3012952" title="" class="ltx_ref ltx_href">Federated variance-reduced stochastic gradient descent with robustness to byzantine attacks</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib181.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Signal Processing</em>, 68:4583â€“4596.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie etÂ al. (2020)</span>
<span class="ltx_bibblock">
Chulin Xie, Keli Huang, Pin-Yu Chen, and BoÂ Li. 2020.

</span>
<span class="ltx_bibblock">Dba: Distributed backdoor attacks against federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib182.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yuexiang Xie, Zhen Wang, Dawei Gao, Daoyuan Chen, Liuyi Yao, Weirui Kuang, Yaliang Li, Bolin Ding, and Jingren Zhou. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.14778/3579075.3579081" title="" class="ltx_ref ltx_href">Federatedscope: A flexible federated learning platform for heterogeneity</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib183.1.1" class="ltx_emph ltx_font_italic">Proc. VLDB Endow.</em>, 16(5):1059â€“1072.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Chang Xu, YuÂ Jia, Liehuang Zhu, Chuan Zhang, Guoxie Jin, and Kashif Sharif. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TPDS.2022.3205714" title="" class="ltx_ref ltx_href">Tdfl: Truth discovery based byzantine robust federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib184.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems</em>, 33(12).

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Mengwei Xu, Dongqi Cai, Yaozong Wu, Xiang Li, and Shangguang Wang. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.13894" title="" class="ltx_ref ltx_href">Fwdllm: Efficient fedllm using forward gradient</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib185.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.13894</em>.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, Qiyang Zhang, Zhenyan Lu, LiÂ Zhang, Shangguang Wang, Yuanchun Li, Yunxin Liu, Xin Jin, and Xuanzhe Liu. 2024b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2401.08092" title="" class="ltx_ref ltx_href">A survey of resource-efficient llm and multimodal foundation models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib186.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.08092</em>.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2024c)</span>
<span class="ltx_bibblock">
Minrui Xu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, Abbas Jamalipour, DongÂ In Kim, Xuemin Shen, Victor C.Â M. Leung, and H.Â Vincent Poor. 2024c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/COMST.2024.3353265" title="" class="ltx_ref ltx_href">Unleashing the power of edge-cloud generative ai in mobile networks: A survey of aigc services</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib187.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>, pages 1â€“1.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zheng Xu, Yanxiang Zhang, Galen Andrew, Christopher Choquette, Peter Kairouz, Brendan Mcmahan, Jesse Rosenstock, and Yuanbo Zhang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-industry.60" title="" class="ltx_ref ltx_href">Federated learning of gboard language models with differential privacy</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib188.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)</em>, pages 629â€“639, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Fu-En Yang, Chien-Yi Wang, and Yu-ChiangÂ Frank Wang. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV51070.2023.01755" title="" class="ltx_ref ltx_href">Efficient model personalization in federated learning via client-specific prompt generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib189.1.1" class="ltx_emph ltx_font_italic">2023 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pages 19102â€“19111.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Kailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie, Ziyan Kuang, and Sophia Ananiadou. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2304.03347" title="" class="ltx_ref ltx_href">Towards interpretable mental health analysis with large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib190.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.03347</em>.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Xin Yang, Hao Yu, Xin Gao, Hao Wang, Junbo Zhang, and Tianrui Li. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/tkde.2024.3363240" title="" class="ltx_ref ltx_href">Federated continual learning via knowledge fusion: A survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib191.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</em>, page 1â€“20.

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Yiyuan Yang, Guodong Long, Tao Shen, Jing Jiang, and Michael Blumenstein. 2024b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2403.19211" title="" class="ltx_ref ltx_href">Dual-personalizing adapter for federated foundation models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib192.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.19211</em>.

</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao etÂ al. (2022)</span>
<span class="ltx_bibblock">
Chun-Han Yao, Boqing Gong, Hang Qi, Yin Cui, Yukun Zhu, and Ming-Hsuan Yang. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/WACV51458.2022.00115" title="" class="ltx_ref ltx_href">Federated multi-target domain adaptation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib193.1.1" class="ltx_emph ltx_font_italic">2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, pages 1081â€“1090.

</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye etÂ al. (2024)</span>
<span class="ltx_bibblock">
Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du, Yanfeng Wang, and Siheng Chen. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.06954" title="" class="ltx_ref ltx_href">Openfedllm: Training large language models on decentralized private data via federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib194.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.06954</em>.

</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin etÂ al. (2018)</span>
<span class="ltx_bibblock">
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. 2018.

</span>
<span class="ltx_bibblock">Byzantine-robust distributed learning: Towards optimal statistical rates.

</span>
<span class="ltx_bibblock">In <em id="bib.bib195.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>. PMLR.

</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Qiying Yu, Yang Liu, Yimu Wang, KeÂ Xu, and Jingjing Liu. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=Hnk1WRMAYqg" title="" class="ltx_ref ltx_href">Multimodal federated learning via contrastive representation ensemble</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib196.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Shuyang Yu, Junyuan Hong, YiÂ Zeng, Fei Wang, Ruoxi Jia, and Jiayu Zhou. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=qhP1aHHyeA" title="" class="ltx_ref ltx_href">Who leaked the model? tracking IP infringers in accountable federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib197.1.1" class="ltx_emph ltx_font_italic">NeurIPS 2023 Workshop on Regulatable ML</em>.

</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2023c)</span>
<span class="ltx_bibblock">
Sixing Yu, JÂ Pablo MuÃ±oz, and Ali Jannesari. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2310.00247" title="" class="ltx_ref ltx_href">Bridging the gap between foundation models and heterogeneous federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib198.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.00247</em>.

</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2023d)</span>
<span class="ltx_bibblock">
Sixing Yu, JÂ Pablo MuÃ±oz, and Ali Jannesari. 2023d.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.11414" title="" class="ltx_ref ltx_href">Federated foundation models: Privacy-preserving and collaborative learning for large models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib199.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.11414</em>.

</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng etÂ al. (2024)</span>
<span class="ltx_bibblock">
Huimin Zeng, Zhenrui Yue, Qian Jiang, and Dong Wang. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2403.04256" title="" class="ltx_ref ltx_href">Federated recommendation via hybrid retrieval augmented generation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib200.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.04256</em>.

</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2020)</span>
<span class="ltx_bibblock">
Chengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, Feng Yan, and Yang Liu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.usenix.org/conference/atc20/presentation/zhang-chengliang" title="" class="ltx_ref ltx_href">BatchCrypt: Efficient homomorphic encryption for Cross-Silo federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib201.1.1" class="ltx_emph ltx_font_italic">2020 USENIX Annual Technical Conference (USENIX ATC 20)</em>, pages 493â€“506. USENIX Association.

</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Honglei Zhang, HeÂ Liu, Haoxuan Li, and Yidong Li. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.01124" title="" class="ltx_ref ltx_href">Transfr: Transferable federated recommendation with pre-trained language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib202.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.01124</em>.

</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Honglei Zhang, Fangyuan Luo, Jun Wu, Xiangnan He, and Yidong Li. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3578361" title="" class="ltx_ref ltx_href">Lightfr: Lightweight federated recommendation with privacy-preserving matrix factorization</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib203.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Inf. Syst.</em>, 41(4).

</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Tong Yu, Guoyin Wang, and Yiran Chen. 2024b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP48485.2024.10447454" title="" class="ltx_ref ltx_href">Towards building the federatedgpt: Federated instruction tuning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib204.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 6915â€“6919.

</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Jie Zhang, Xiaohua Qi, and BoÂ Zhao. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2306.16064" title="" class="ltx_ref ltx_href">Federated generative learning with foundation models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib205.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.16064</em>.

</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023c)</span>
<span class="ltx_bibblock">
Junjie Zhang, Ruobing Xie, Yupeng Hou, WayneÂ Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.07001" title="" class="ltx_ref ltx_href">Recommendation as instruction following: A large language model empowered recommendation approach</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib206.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.07001</em>.

</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023d)</span>
<span class="ltx_bibblock">
Zhiyuan Zhang, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and XuÂ Sun. 2023d.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/c39578c86423df5f9e8834ce1cd456e4-Paper-Conference.pdf" title="" class="ltx_ref ltx_href">Fed-fa: Theoretically modeling client data divergence for federated language backdoor defense</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib207.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volumeÂ 36, pages 62006â€“62031. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023e)</span>
<span class="ltx_bibblock">
Zhuo Zhang, Xiangjing Hu, Jingyuan Zhang, Yating Zhang, Hui Wang, Lizhen Qu, and Zenglin Xu. 2023e.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.193" title="" class="ltx_ref ltx_href">FEDLEGAL: The first real-world federated learning benchmark for legal NLP</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib208.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3492â€“3507, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib209" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2024c)</span>
<span class="ltx_bibblock">
Zhuo Zhang, Jintao Huang, Xiangjing Hu, Jingyuan Zhang, Yating Zhang, Hui Wang, Yue Yu, Qifan Wang, Lizhen Qu, and Zenglin Xu. 2024c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2024.lrec-main.1227" title="" class="ltx_ref ltx_href">Revisiting data reconstruction attacks on real-world dataset for federated natural language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib209.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</em>, pages 14080â€“14091, Torino, Italia. ELRA and ICCL.

</span>
</li>
<li id="bib.bib210" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023f)</span>
<span class="ltx_bibblock">
Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, and Zenglin Xu. 2023f.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-acl.632" title="" class="ltx_ref ltx_href">FedPETuning: When federated learning meets the parameter-efficient tuning methods of pre-trained language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib210.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 9963â€“9977, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib211" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Jujia Zhao, Wenjie Wang, Chen Xu, Zhaochun Ren, See-Kiong Ng, and Tat-Seng Chua. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.09959" title="" class="ltx_ref ltx_href">Llm-based federated recommendation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib211.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.09959</em>.

</span>
</li>
<li id="bib.bib212" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Wanru Zhao, Yihong Chen, Royson Lee, Xinchi Qiu, Yan Gao, Hongxiang Fan, and NicholasÂ Donald Lane. 2024b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=zzqn5G9fjn" title="" class="ltx_ref ltx_href">Breaking physical and linguistic borders: Multilingual federated prompt tuning for low-resource languages</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib212.1.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib213" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2023)</span>
<span class="ltx_bibblock">
WayneÂ Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, etÂ al. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2303.18223" title="" class="ltx_ref ltx_href">A survey of large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib213.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.18223</em>.

</span>
</li>
<li id="bib.bib214" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng etÂ al. (2023)</span>
<span class="ltx_bibblock">
Fei Zheng, Chaochao Chen, Lingjuan Lyu, and Binhui Yao. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.24963/ijcai.2023/519" title="" class="ltx_ref ltx_href">Reducing communication for split learning by randomized top-k sparsification</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib214.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence</em>, IJCAI â€™23.

</span>
</li>
<li id="bib.bib215" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al. (2019)</span>
<span class="ltx_bibblock">
Ligeng Zhu, Zhijian Liu, and Song Han. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2019/file/60a6c4002cc7b29142def8871531281a-Paper.pdf" title="" class="ltx_ref ltx_href">Deep leakage from gradients</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib215.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volumeÂ 32. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib216" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Weiming Zhuang, Chen Chen, and Lingjuan Lyu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2306.15546" title="" class="ltx_ref ltx_href">When foundation model meets federated learning: Motivations, challenges, and future directions</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib216.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.15546</em>.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Details of Adapter Tuning</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Adapter Tuning</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">Adapter tuning integrates small-scale neural networks (known as â€œadaptersâ€) into the pre-trained modelsÂ <cite class="ltx_cite ltx_citemacro_cite">Houlsby etÂ al. (<a href="#bib.bib66" title="" class="ltx_ref">2019</a>); Hu etÂ al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite>. A straightforward implementation of adapter tuning is to collaboratively train a shared adapter among all clients in the <span id="A1.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">FedAvg</span> manner, as highlighted byÂ <cite class="ltx_cite ltx_citemacro_citet">Sun etÂ al. (<a href="#bib.bib155" title="" class="ltx_ref">2022a</a>)</cite>. Based on <span id="A1.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">FedAvg</span>, FedCLIPÂ <cite class="ltx_cite ltx_citemacro_cite">Lu etÂ al. (<a href="#bib.bib110" title="" class="ltx_ref">2023a</a>)</cite> incorporates an attention-based adapter for the image encoder in CLIP modelsÂ <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a href="#bib.bib131" title="" class="ltx_ref">2021</a>)</cite>.
In the domain of multilingual machine translation, where different language pairs exhibit substantial discrepancies in data distributions, Fed-MNMTÂ <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib108" title="" class="ltx_ref">2023d</a>)</cite> explores clustering strategies that group adapter parameters and makes inner-cluster parameters aggregation for alleviating the undesirable effect of data discrepancy. Another representative approach named C2AÂ <cite class="ltx_cite ltx_citemacro_cite">Kim etÂ al. (<a href="#bib.bib81" title="" class="ltx_ref">2023</a>)</cite> employs hypernetworksÂ <cite class="ltx_cite ltx_citemacro_cite">Ha etÂ al. (<a href="#bib.bib63" title="" class="ltx_ref">2017</a>)</cite> to generate client-specific adapters by conditioning on the clientâ€™s information, maximizing the utility of shared model parameters while minimizing the divergence caused by
data heterogeneity.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Prompt Tuning</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">Prompt tuning incorporates trainable task-specific continuous prompt vectors at the input layerÂ <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a href="#bib.bib104" title="" class="ltx_ref">2023a</a>); Dong etÂ al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>. Compared to full fine-tuning, it achieves comparable performance but with <math id="A1.SS2.p1.1.m1.1" class="ltx_math_unparsed" alttext="1000\times" display="inline"><semantics id="A1.SS2.p1.1.m1.1a"><mrow id="A1.SS2.p1.1.m1.1b"><mn id="A1.SS2.p1.1.m1.1.1">1000</mn><mo lspace="0.222em" id="A1.SS2.p1.1.m1.1.2">Ã—</mo></mrow><annotation encoding="application/x-tex" id="A1.SS2.p1.1.m1.1c">1000\times</annotation></semantics></math> less parameter storage and communicationÂ <cite class="ltx_cite ltx_citemacro_cite">Jia etÂ al. (<a href="#bib.bib72" title="" class="ltx_ref">2022</a>)</cite>. A variation of prompt tuning, FedPerfixÂ <cite class="ltx_cite ltx_citemacro_cite">Sun etÂ al. (<a href="#bib.bib154" title="" class="ltx_ref">2023</a>)</cite> uses a local adapter to generate the prefixes and aggregate the original self-attention layers.</p>
</div>
<div id="A1.SS2.p2" class="ltx_para">
<p id="A1.SS2.p2.1" class="ltx_p">Depending on target modalities, prompt tuning in current literature can be further classified into three categories:</p>
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p"><span id="A1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Textual Prompt Tuning.</span> Task-specific prompt embeddings are combined with the input text embeddings, which are subsequently fed into language models. These soft prompts serve as instructive contexts to influence the generation process of LLMs by steering the probability distribution of the next tokenÂ <cite class="ltx_cite ltx_citemacro_cite">Dong etÂ al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p"><span id="A1.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Visual Prompt Tuning.</span> Taking inspiration from advances in efficiently tuning LLMs, prompts are also introduced in the input space of vision modelsÂ <cite class="ltx_cite ltx_citemacro_cite">Jia etÂ al. (<a href="#bib.bib72" title="" class="ltx_ref">2022</a>)</cite>. Naive implementations introduce prompts at the pixel level, acting as a form of data augmentationÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a href="#bib.bib86" title="" class="ltx_ref">2024a</a>)</cite>. Alternatively, one could also insert the prompts as latent vectors for the first Transformer layerÂ <cite class="ltx_cite ltx_citemacro_cite">Deng etÂ al. (<a href="#bib.bib33" title="" class="ltx_ref">2024</a>); Yang etÂ al. (<a href="#bib.bib189" title="" class="ltx_ref">2023a</a>)</cite>. Nevertheless, an empirical studyÂ <cite class="ltx_cite ltx_citemacro_cite">Jia etÂ al. (<a href="#bib.bib72" title="" class="ltx_ref">2022</a>)</cite> has suggested that it is easier for visual prompts to learn condensed task-dependent signals in the latent input space of Transformers.</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p"><span id="A1.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Textual-Visual Prompt Tuning.</span> Unlike single-modal FMs, vision-language FMs can process and interpret both visual data and textual information, endowing them with powerful representation ability and transferabilityÂ <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a href="#bib.bib131" title="" class="ltx_ref">2021</a>)</cite>. Based on vision-language FMs like CLIP, textual-visual prompt tuning shows promising capabilities in FLÂ <cite class="ltx_cite ltx_citemacro_cite">Guo etÂ al. (<a href="#bib.bib58" title="" class="ltx_ref">2023</a>)</cite>, especially in cross-domain scenarios, where the model needs to generalize across varied domains and unseen classesÂ <cite class="ltx_cite ltx_citemacro_cite">Qiu etÂ al. (<a href="#bib.bib130" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<figure id="A1.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>A list of existing FM-FL libraries and benchmarks. <span id="A1.T3.2.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">Missing or inapplicable details denoted by N/A</span>. âœ“Â denotes a strong focus or presence; âœ—Â indicates no focus or absence; â—Â signifies a moderate focus or partial inclusion.</figcaption>
<p id="A1.T3.3" class="ltx_p ltx_align_center"><span id="A1.T3.3.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="A1.T3.3.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:898.7pt;height:277.8pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A1.T3.3.1.1.1" class="ltx_p"><span id="A1.T3.3.1.1.1.1" class="ltx_text">
<span id="A1.T3.3.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A1.T3.3.1.1.1.1.1.1" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Library/Benchmark</span></span>
<span id="A1.T3.3.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">FL Backend</span></span>
<span id="A1.T3.3.1.1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="A1.T3.3.1.1.1.1.1.1.3.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:60.3pt;vertical-align:-27.7pt;"><span class="ltx_transformed_inner" style="width:60.3pt;transform:translate(-25.76pt,2.92pt) rotate(-90deg) ;">
<span id="A1.T3.3.1.1.1.1.1.1.3.1.1" class="ltx_p"><span id="A1.T3.3.1.1.1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold" style="color:#1B9E77;">LLM Support</span></span>
</span></span></span>
<span id="A1.T3.3.1.1.1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="A1.T3.3.1.1.1.1.1.1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:109.6pt;vertical-align:-52.3pt;"><span class="ltx_transformed_inner" style="width:109.6pt;transform:translate(-50.36pt,2.92pt) rotate(-90deg) ;">
<span id="A1.T3.3.1.1.1.1.1.1.4.1.1" class="ltx_p"><span id="A1.T3.3.1.1.1.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold" style="color:#D73027;">MultiModal FM Support</span></span>
</span></span></span>
<span id="A1.T3.3.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="A1.T3.3.1.1.1.1.1.1.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:43.1pt;vertical-align:-18.1pt;"><span class="ltx_transformed_inner" style="width:43.1pt;transform:translate(-18.06pt,0pt) rotate(-90deg) ;">
<span id="A1.T3.3.1.1.1.1.1.1.5.1.1" class="ltx_p"><span id="A1.T3.3.1.1.1.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold" style="color:#006BA4;">FedPEFT</span></span>
</span></span></span>
<span id="A1.T3.3.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="A1.T3.3.1.1.1.1.1.1.6.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:106pt;vertical-align:-50.6pt;"><span class="ltx_transformed_inner" style="width:106.0pt;transform:translate(-48.61pt,2.92pt) rotate(-90deg) ;">
<span id="A1.T3.3.1.1.1.1.1.1.6.1.1" class="ltx_p"><span id="A1.T3.3.1.1.1.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold" style="color:#FF800E;">On-Device Training</span></span>
</span></span></span>
<span id="A1.T3.3.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="A1.T3.3.1.1.1.1.1.1.7.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:132.8pt;vertical-align:-62.9pt;"><span class="ltx_transformed_inner" style="width:132.8pt;transform:translate(-62.93pt,0pt) rotate(-90deg) ;">
<span id="A1.T3.3.1.1.1.1.1.1.7.1.1" class="ltx_p"><span id="A1.T3.3.1.1.1.1.1.1.7.1.1.1" class="ltx_text ltx_font_bold" style="color:#ABAB2A;">Distributed &amp; Clustered</span></span>
</span></span></span>
<span id="A1.T3.3.1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="A1.T3.3.1.1.1.1.1.1.8.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:85.4pt;vertical-align:-40.2pt;"><span class="ltx_transformed_inner" style="width:85.3pt;transform:translate(-38.22pt,2.92pt) rotate(-90deg) ;">
<span id="A1.T3.3.1.1.1.1.1.1.8.1.1" class="ltx_p"><span id="A1.T3.3.1.1.1.1.1.1.8.1.1.1" class="ltx_text ltx_font_bold" style="color:#893D56;">Differential Privacy</span></span>
</span></span></span>
<span id="A1.T3.3.1.1.1.1.1.1.9" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.1.9.1" class="ltx_text ltx_font_bold">Description</span></span></span>
<span id="A1.T3.3.1.1.1.1.1.2" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">FederatedScope-LLMÂ <cite class="ltx_cite ltx_citemacro_cite">Kuang etÂ al. (<a href="#bib.bib83" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="A1.T3.3.1.1.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">FederatedScope</span>
<span id="A1.T3.3.1.1.1.1.1.2.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.2.3.1" class="ltx_text" style="color:#1B9E77;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.2.4" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.2.4.1" class="ltx_text" style="color:#D73027;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.2.5.1" class="ltx_text" style="color:#006BA4;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.2.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.2.6.1" class="ltx_text" style="color:#FF800E;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.2.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.2.7.1" class="ltx_text" style="color:#ABAB2A;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.2.8.1" class="ltx_text" style="color:#893D56;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.2.9" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">An end-to-end benchmark for efficient fine-tuning LLMs with FL</span></span>
<span id="A1.T3.3.1.1.1.1.1.3" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.5pt;padding-bottom:0.5pt;">NVIDIA FLAREÂ <cite class="ltx_cite ltx_citemacro_cite">Roth etÂ al. (<a href="#bib.bib142" title="" class="ltx_ref">2024</a>)</cite></span>
<span id="A1.T3.3.1.1.1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">NVFlare</span>
<span id="A1.T3.3.1.1.1.1.1.3.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.3.3.1" class="ltx_text" style="color:#1B9E77;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.3.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.3.4.1" class="ltx_text" style="color:#D73027;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.3.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.3.5.1" class="ltx_text" style="color:#006BA4;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.3.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.3.6.1" class="ltx_text" style="color:#FF800E;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.3.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.3.7.1" class="ltx_text" style="color:#ABAB2A;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.3.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.3.8.1" class="ltx_text" style="color:#893D56;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.3.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">Scalable and efficient fine-tuning LLMs with FL</span></span>
<span id="A1.T3.3.1.1.1.1.1.4" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.5pt;padding-bottom:0.5pt;">FATE-LLMÂ <cite class="ltx_cite ltx_citemacro_cite">Fan etÂ al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="A1.T3.3.1.1.1.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">FATE</span>
<span id="A1.T3.3.1.1.1.1.1.4.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.4.3.1" class="ltx_text" style="color:#1B9E77;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.4.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.4.4.1" class="ltx_text" style="color:#D73027;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.4.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.4.5.1" class="ltx_text" style="color:#006BA4;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.4.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.4.6.1" class="ltx_text" style="color:#FF800E;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.4.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.4.7.1" class="ltx_text" style="color:#ABAB2A;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.4.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.4.8.1" class="ltx_text" style="color:#893D56;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.4.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">Focuses on IP and privacy protection in federated LLM</span></span>
<span id="A1.T3.3.1.1.1.1.1.5" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.5.1" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedLLMÂ <cite class="ltx_cite ltx_citemacro_cite">FedML (<a href="#bib.bib44" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="A1.T3.3.1.1.1.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedML</span>
<span id="A1.T3.3.1.1.1.1.1.5.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.5.3.1" class="ltx_text" style="color:#1B9E77;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.5.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.5.4.1" class="ltx_text" style="color:#D73027;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.5.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.5.5.1" class="ltx_text" style="color:#006BA4;">â—</span></span>
<span id="A1.T3.3.1.1.1.1.1.5.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.5.6.1" class="ltx_text" style="color:#FF800E;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.5.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.5.7.1" class="ltx_text" style="color:#ABAB2A;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.5.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.5.8.1" class="ltx_text" style="color:#893D56;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.5.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">An MLOps-supported training pipeline based on FedML</span></span>
<span id="A1.T3.3.1.1.1.1.1.6" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.6.1" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.5pt;padding-bottom:0.5pt;">OpenFedLLMÂ <cite class="ltx_cite ltx_citemacro_cite">Ye etÂ al. (<a href="#bib.bib194" title="" class="ltx_ref">2024</a>)</cite></span>
<span id="A1.T3.3.1.1.1.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.6.2.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">N/A</span></span>
<span id="A1.T3.3.1.1.1.1.1.6.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.6.3.1" class="ltx_text" style="color:#1B9E77;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.6.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.6.4.1" class="ltx_text" style="color:#D73027;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.6.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.6.5.1" class="ltx_text" style="color:#006BA4;">â—</span></span>
<span id="A1.T3.3.1.1.1.1.1.6.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.6.6.1" class="ltx_text" style="color:#FF800E;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.6.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.6.7.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">N/A</span></span>
<span id="A1.T3.3.1.1.1.1.1.6.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.6.8.1" class="ltx_text" style="color:#893D56;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.6.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">An LLM framework focusing on FL instruction tuning/alignment</span></span>
<span id="A1.T3.3.1.1.1.1.1.7" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.7.1" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.5pt;padding-bottom:0.5pt;">ShepherdÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib204" title="" class="ltx_ref">2024b</a>)</cite></span>
<span id="A1.T3.3.1.1.1.1.1.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.7.2.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">N/A</span></span>
<span id="A1.T3.3.1.1.1.1.1.7.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.7.3.1" class="ltx_text" style="color:#1B9E77;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.7.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.7.4.1" class="ltx_text" style="color:#D73027;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.7.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.7.5.1" class="ltx_text" style="color:#006BA4;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.7.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.7.6.1" class="ltx_text" style="color:#FF800E;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.7.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.7.7.1" class="ltx_text" style="color:#ABAB2A;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.7.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.7.8.1" class="ltx_text" style="color:#893D56;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.7.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">Federated instruction tuning based on Hugging Face</span></span>
<span id="A1.T3.3.1.1.1.1.1.8" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.8.1" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedPETuningÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib210" title="" class="ltx_ref">2023f</a>)</cite></span>
<span id="A1.T3.3.1.1.1.1.1.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedLab</span>
<span id="A1.T3.3.1.1.1.1.1.8.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.8.3.1" class="ltx_text" style="color:#1B9E77;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.8.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.8.4.1" class="ltx_text" style="color:#D73027;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.8.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.8.5.1" class="ltx_text" style="color:#006BA4;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.8.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.8.6.1" class="ltx_text" style="color:#FF800E;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.8.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.8.7.1" class="ltx_text" style="color:#ABAB2A;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.8.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.8.8.1" class="ltx_text" style="color:#893D56;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.8.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">A benchmark comprising four FedPEFT methods</span></span>
<span id="A1.T3.3.1.1.1.1.1.9" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedLegalÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib208" title="" class="ltx_ref">2023e</a>)</cite></span>
<span id="A1.T3.3.1.1.1.1.1.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedLab</span>
<span id="A1.T3.3.1.1.1.1.1.9.3" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.9.3.1" class="ltx_text" style="color:#1B9E77;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.9.4" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.9.4.1" class="ltx_text" style="color:#D73027;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.9.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.9.5.1" class="ltx_text" style="color:#006BA4;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.9.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.9.6.1" class="ltx_text" style="color:#FF800E;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.9.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.9.7.1" class="ltx_text" style="color:#ABAB2A;">âœ“</span></span>
<span id="A1.T3.3.1.1.1.1.1.9.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.9.8.1" class="ltx_text" style="color:#893D56;">âœ—</span></span>
<span id="A1.T3.3.1.1.1.1.1.9.9" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:0.5pt;padding-bottom:0.5pt;">A benchmark comprising six legal NLP tasks under FL settings</span></span>
</span></span></span>
</span></span></span></p>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Libraries and Benchmarks</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">This part briefly introduces a series of available libraries and benchmarks for developing and examining FM-FL techniques. An overview is provided in TableÂ <a href="#A1.T3" title="Table 3 â€£ A.2 Prompt Tuning â€£ Appendix A Additional Details of Adapter Tuning â€£ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="A2.p2" class="ltx_para">
<ul id="A2.I1" class="ltx_itemize">
<li id="A2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i1.p1.1" class="ltx_p"><span id="A2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">FederatedScope-LLM</span>Â <cite class="ltx_cite ltx_citemacro_cite">Kuang etÂ al. (<a href="#bib.bib83" title="" class="ltx_ref">2023</a>)</cite> is an open-source package for fine-tuning LLMs via FL. Built on top of a popular FL backend FederatedScopeÂ <cite class="ltx_cite ltx_citemacro_cite">Xie etÂ al. (<a href="#bib.bib183" title="" class="ltx_ref">2023</a>)</cite>, it supports federated fine-tuning of LLMs under various FL scenarios, including FedPEFT and model personalization.</p>
</div>
</li>
<li id="A2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i2.p1.1" class="ltx_p"><span id="A2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">NVIDIA FLARE</span>Â <cite class="ltx_cite ltx_citemacro_cite">Roth etÂ al. (<a href="#bib.bib142" title="" class="ltx_ref">2024</a>)</cite> is an FL framework that allows researchers and data scientists to seamlessly move their machine learning and deep learning workflows into a federated paradigm.</p>
</div>
</li>
<li id="A2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i3.p1" class="ltx_para">
<p id="A2.I1.i3.p1.1" class="ltx_p"><span id="A2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">FATE-LLM</span>Â <cite class="ltx_cite ltx_citemacro_cite">Fan etÂ al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite> is an industrial-grade FL framework for LLM. Apart from FedPEFT, it provides a privacy hub integrating several IP protection and privacy-preserving mechanisms to protect model security and data privacy.</p>
</div>
</li>
<li id="A2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i4.p1" class="ltx_para">
<p id="A2.I1.i4.p1.1" class="ltx_p"><span id="A2.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">FedLLM</span>Â <cite class="ltx_cite ltx_citemacro_cite">FedML (<a href="#bib.bib44" title="" class="ltx_ref">2023</a>)</cite> is an MLOps-supported training pipeline built upon the FedML AI platformÂ <cite class="ltx_cite ltx_citemacro_cite">He etÂ al. (<a href="#bib.bib65" title="" class="ltx_ref">2020</a>)</cite>. FedLLM is compatible with popular LLM libraries such as HuggingFace and DeepSpeed to support a large range of FMs and datasets.</p>
</div>
</li>
<li id="A2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i5.p1" class="ltx_para">
<p id="A2.I1.i5.p1.1" class="ltx_p"><span id="A2.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">OpenFedLLM</span>Â <cite class="ltx_cite ltx_citemacro_cite">Ye etÂ al. (<a href="#bib.bib194" title="" class="ltx_ref">2024</a>)</cite> is a federated tuning framework for LLMs, which covers applications of instruction tuning and value alignment, diverse FL baselines, training datasets, and evaluation datasets.</p>
</div>
</li>
<li id="A2.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i6.p1" class="ltx_para">
<p id="A2.I1.i6.p1.1" class="ltx_p"><span id="A2.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Shepherd</span>Â <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib204" title="" class="ltx_ref">2024b</a>)</cite> is a lightweight federated tuning framework. The local training process of Shepherd is built upon the implementations of Alpaca-LoRAÂ <cite class="ltx_cite ltx_citemacro_cite">Wang (<a href="#bib.bib170" title="" class="ltx_ref">2023</a>)</cite>, and Hugging Faceâ€™s PEFTÂ <cite class="ltx_cite ltx_citemacro_cite">Mangrulkar etÂ al. (<a href="#bib.bib116" title="" class="ltx_ref">2022</a>)</cite>, enabling efficient fine-tuning.</p>
</div>
</li>
<li id="A2.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i7.p1" class="ltx_para">
<p id="A2.I1.i7.p1.1" class="ltx_p"><span id="A2.I1.i7.p1.1.1" class="ltx_text ltx_font_bold">FedPETuning</span>Â <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib210" title="" class="ltx_ref">2023f</a>)</cite> is a pioneering federated benchmark for four representative FedPEFT methods, covering adapter tuning, prefix tuning, LoRA, and BitFit.</p>
</div>
</li>
<li id="A2.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i8.p1" class="ltx_para">
<p id="A2.I1.i8.p1.1" class="ltx_p"><span id="A2.I1.i8.p1.1.1" class="ltx_text ltx_font_bold">FedLegal</span>Â <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a href="#bib.bib208" title="" class="ltx_ref">2023e</a>)</cite> is the very first real-world FL benchmark for legal NLP, which comprises five legal NLP tasks and one privacy task based on the data from Chinese courts.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.12843" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.12844" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.12844">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.12844" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.12845" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Jul  6 00:28:13 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
