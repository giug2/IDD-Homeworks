<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.12844] Synergizing Foundation Models and Federated Learning: A Survey</title><meta property="og:description" content="The recent development of Foundation Models (FMs), represented by large language models, vision transformers, and multimodal models, has been making a significant impact on both academia and industry. Compared with sma…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synergizing Foundation Models and Federated Learning: A Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synergizing Foundation Models and Federated Learning: A Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.12844">

<!--Generated on Sat Jul  6 00:28:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Synergizing Foundation Models and Federated Learning: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Shenghui Li<sup id="id14.14.id1" class="ltx_sup"><span id="id14.14.id1.1" class="ltx_text ltx_font_italic">1</span></sup>    
Fanghua Ye<sup id="id15.15.id2" class="ltx_sup"><span id="id15.15.id2.1" class="ltx_text ltx_font_italic">2</span></sup>    <span id="id3.3.1" class="ltx_text ltx_font_bold">Meng Fang<sup id="id3.3.1.1" class="ltx_sup"><span id="id3.3.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup></span>    
Jiaxu Zhao<sup id="id16.16.id3" class="ltx_sup"><span id="id16.16.id3.1" class="ltx_text ltx_font_italic">4</span></sup> 
<br class="ltx_break"><span id="id5.5.2" class="ltx_text ltx_font_bold">Yun-Hin Chan<sup id="id5.5.2.1" class="ltx_sup"><span id="id5.5.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">5</span></sup></span>    
<span id="id6.6.3" class="ltx_text ltx_font_bold">Edith C.-H. Ngai<sup id="id6.6.3.1" class="ltx_sup"><span id="id6.6.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">5</span></sup></span>    
<span id="id7.7.4" class="ltx_text ltx_font_bold">Thiemo Voigt<sup id="id7.7.4.1" class="ltx_sup"><span id="id7.7.4.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1,6</span></sup></span> 
<br class="ltx_break"><sup id="id17.17.id4" class="ltx_sup">1</sup> Uppsala University, Sweden. <em id="id18.18.id5" class="ltx_emph ltx_font_italic">shenghui.li@it.uu.se</em> 
<br class="ltx_break"><sup id="id19.19.id6" class="ltx_sup">2</sup> University College London, United Kingdom. <em id="id20.20.id7" class="ltx_emph ltx_font_italic">fanghua.ye.19@ucl.ac.uk</em> 
<br class="ltx_break"><sup id="id21.21.id8" class="ltx_sup">3</sup> University of Liverpool, United Kingdom. <em id="id22.22.id9" class="ltx_emph ltx_font_italic">mfang@liverpool.ac.uk</em> 
<br class="ltx_break"><sup id="id23.23.id10" class="ltx_sup">4</sup> Eindhoven University of Technology, the Netherlands. <em id="id24.24.id11" class="ltx_emph ltx_font_italic">j.zhao@@tue.nl</em> 
<br class="ltx_break"><sup id="id25.25.id12" class="ltx_sup">5</sup> The University of Hong Kong, China. <em id="id26.26.id13" class="ltx_emph ltx_font_italic"><span id="id26.26.id13.1" class="ltx_text ltx_font_upright">{</span>chngai@eee, chanyunhin@connect<span id="id26.26.id13.2" class="ltx_text ltx_font_upright">}</span>.hku.hk</em> 
<br class="ltx_break"><sup id="id27.27.id14" class="ltx_sup">6</sup> Research Institutes of Sweden, Sweden. <em id="id28.28.id15" class="ltx_emph ltx_font_italic">thiemo.voigt@angstrom.uu.se</em> 
<br class="ltx_break">
</span><span class="ltx_author_notes">   Corresponding Author.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id29.id1" class="ltx_p">The recent development of Foundation Models (FMs), represented by large language models, vision transformers, and multimodal models, has been making a significant impact on both academia and industry. Compared with small-scale models, FMs have a much stronger demand for high-volume data during the pre-training phase. Although general FMs can be pre-trained on data collected from open sources such as the Internet, domain-specific FMs need proprietary data, posing a practical challenge regarding the amount of data available due to privacy concerns. Federated Learning (FL) is a collaborative learning paradigm that breaks the barrier of data availability from different participants. Therefore, it provides a promising solution to customize and adapt FMs to a wide range of domain-specific tasks using distributed datasets whilst preserving privacy. This survey paper discusses the potentials and challenges of synergizing FL and FMs and summarizes core techniques, future directions, and applications.
A periodically updated paper collection on FM-FL is available at <a target="_blank" href="https://github.com/lishenghui/awesome-fm-fl" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/lishenghui/awesome-fm-fl</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The landscape of Artificial Intelligence (AI) has been revolutionized by the emergence of <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">Foundation Models (FMs)</span> <cite class="ltx_cite ltx_citemacro_citep">(Bommasani et al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>, such as BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite>, GPT series <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>); OpenAI (<a href="#bib.bib122" title="" class="ltx_ref">2022</a>, <a href="#bib.bib123" title="" class="ltx_ref">2024</a>)</cite>, and LLaMA series <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib166" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib167" title="" class="ltx_ref">b</a>)</cite> in Natural Language Processing (NLP); ViTs <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite> and SAM <cite class="ltx_cite ltx_citemacro_cite">Kirillov et al. (<a href="#bib.bib82" title="" class="ltx_ref">2023</a>)</cite> in Computer Vision (CV); CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib131" title="" class="ltx_ref">2021</a>)</cite>, DALL-E <cite class="ltx_cite ltx_citemacro_cite">Ramesh et al. (<a href="#bib.bib134" title="" class="ltx_ref">2021</a>)</cite>, Gemini <cite class="ltx_cite ltx_citemacro_cite">Google (<a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite>, and GPT-4o in multimodal applications. These FMs have become pivotal in a myriad of AI applications across diverse domains. Their superb capability to generalize across tasks and domains stems from their pre-training on extensive datasets <cite class="ltx_cite ltx_citemacro_citep">(Gunasekar et al., <a href="#bib.bib56" title="" class="ltx_ref">2023</a>)</cite>, which imbues them with a profound understanding of language, vision, and multimodal data.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While general-purpose FMs can leverage openly accessible data from the Internet, domain-specific FMs require proprietary data. It is, however, challenging to collect vast amounts of proprietary data and perform centralized pre-training or fine-tuning for domain-specific FMs, due to privacy restrictions <cite class="ltx_cite ltx_citemacro_cite">Jo and Gebru (<a href="#bib.bib76" title="" class="ltx_ref">2020</a>); GDPR (<a href="#bib.bib50" title="" class="ltx_ref">2016</a>); CCPA (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>. Particularly in domains such as law, healthcare, and finance, where data is inherently privacy-sensitive, there is a pressing need for stringent privacy safeguards. Furthermore, given that data often constitutes a pivotal asset for enterprises, its widespread distribution is prohibitive.
Consequently, there is an urgent need for novel strategies to handle data availability and facilitate model training, thereby unlocking the potential of domain-specific FMs whilst respecting data privacy.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address the challenges associated with data privacy in model training, Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib119" title="" class="ltx_ref">2017</a>)</cite> has emerged as a promising paradigm. FL facilitates collaborative model training across decentralized clients without the need to share raw data, thus ensuring privacy preservation. Concretely, FL encompasses periodic interactions between the server and decentralized clients for the exchange of trainable model parameters without the requirement for private client data.
Recognizing such a benefit, <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">integrating FMs with FL presents a compelling solution for domain-specific FMs</span> <cite class="ltx_cite ltx_citemacro_cite">Zhuang et al. (<a href="#bib.bib216" title="" class="ltx_ref">2023</a>); Yu et al. (<a href="#bib.bib199" title="" class="ltx_ref">2023d</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Despite the potential synergies between FL and FMs, the field is still nascent, lacking a comprehensive understanding of challenges, methodologies, and directions. This survey aims to bridge this gap by providing a thorough exploration of the integration of FMs and FL. We delve into the motivations and challenges of combining these two paradigms, highlight representative techniques, and discuss applications and future directions. By elucidating the intersection of FL and FMs, we aim to catalyze further research and innovation in this burgeoning area, ultimately advancing the development of privacy-aware, domain-specific FMs.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The paper continues as follows:
The next section introduces background on FMs and FL. Section <a href="#S3" title="3 FM-FL: Motivation &amp; Challenges ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the motivation and challenges for synergizing FMs and FL. Section <a href="#S4" title="4 Techniques ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> highlights representative techniques. Section <a href="#S5" title="5 Applications of FM-FL ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> explores the applications across various domains. Before concluding, we discuss representative future directions in Section <a href="#S6" title="6 Future Directions ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Foundation Models</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">An FM is a model that can be adapted to a wide array of tasks through fine-tuning after initial pre-training <cite class="ltx_cite ltx_citemacro_cite">Bommasani et al. (<a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>. The lifecycle of FMs typically involves pre-training on extensive generic data to establish the basis of their abilities <cite class="ltx_cite ltx_citemacro_cite">Bubeck et al. (<a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>, followed by adaptation to downstream tasks such as domain-specific question answering <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib208" title="" class="ltx_ref">2023e</a>)</cite>, and ultimately application in various domains.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">FMs have sparked a significant paradigm shift in various fields of AI such as NLP, CV, speech and acoustics, and beyond. In the realm of NLP, the most prominent example is Large Language Models (LLMs) with substantial parameter sizes <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a href="#bib.bib213" title="" class="ltx_ref">2023</a>)</cite>. These models, such as ChatGPT and GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib122" title="" class="ltx_ref">2022</a>, <a href="#bib.bib123" title="" class="ltx_ref">2024</a>)</cite>, demonstrate exceptional abilities in natural language understanding and generation, enabling them to comprehend and respond to user inputs with remarkable contextual relevance. This capability proves invaluable in applications like customer service, virtual assistants, and chatbots, where effective communication is paramount. Moreover, LLMs eliminate the need for training models from scratch for specific tasks, be it machine translation, document summarization, text generation, or other language-related tasks.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">In the realm of CV and other modalities, FMs have also made remarkable progress. Vision Transformers (ViTs) <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite> segment images into distinct patches, which serve as inputs for transformer architectures. SAM <cite class="ltx_cite ltx_citemacro_cite">Kirillov et al. (<a href="#bib.bib82" title="" class="ltx_ref">2023</a>)</cite> can segment anything in images according to the input prompts. CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib131" title="" class="ltx_ref">2021</a>)</cite> bridges the gap between text and images through contrastive learning. DALL<math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mo id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">\cdot</annotation></semantics></math>E, proposed by <cite class="ltx_cite ltx_citemacro_citet">Ramesh et al. (<a href="#bib.bib134" title="" class="ltx_ref">2021</a>)</cite>, generates images from textual descriptions, expanding the possibilities of creative image generation. Additionally, models like GAto <cite class="ltx_cite ltx_citemacro_citep">(Reed et al., <a href="#bib.bib137" title="" class="ltx_ref">2022</a>)</cite>, exhibit versatility by being applicable across various tasks such as conversational agents, robotic control, and gaming.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Federated Learning</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">FL <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib119" title="" class="ltx_ref">2017</a>)</cite> is a learning paradigm that enables a collection of clients to collaboratively learn a shared global model by leveraging their private datasets in a distributed manner, assisted by the coordination of a central server.
The general goal of FL is to find a parameter set <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="\bm{\theta}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">𝜽</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝜽</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\bm{\theta}</annotation></semantics></math> that minimizes the following distributed optimization objective:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.4" class="ltx_Math" alttext="\min\limits_{\bm{\theta}}F(\bm{\theta}):=\frac{1}{K}\sum_{k\in[K]}F_{k}(\bm{\theta})," display="block"><semantics id="S2.E1.m1.4a"><mrow id="S2.E1.m1.4.4.1" xref="S2.E1.m1.4.4.1.1.cmml"><mrow id="S2.E1.m1.4.4.1.1" xref="S2.E1.m1.4.4.1.1.cmml"><mrow id="S2.E1.m1.4.4.1.1.2" xref="S2.E1.m1.4.4.1.1.2.cmml"><mrow id="S2.E1.m1.4.4.1.1.2.2" xref="S2.E1.m1.4.4.1.1.2.2.cmml"><munder id="S2.E1.m1.4.4.1.1.2.2.1" xref="S2.E1.m1.4.4.1.1.2.2.1.cmml"><mi id="S2.E1.m1.4.4.1.1.2.2.1.2" xref="S2.E1.m1.4.4.1.1.2.2.1.2.cmml">min</mi><mi id="S2.E1.m1.4.4.1.1.2.2.1.3" xref="S2.E1.m1.4.4.1.1.2.2.1.3.cmml">𝜽</mi></munder><mo lspace="0.167em" id="S2.E1.m1.4.4.1.1.2.2a" xref="S2.E1.m1.4.4.1.1.2.2.cmml">⁡</mo><mi id="S2.E1.m1.4.4.1.1.2.2.2" xref="S2.E1.m1.4.4.1.1.2.2.2.cmml">F</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.1.1.2.1" xref="S2.E1.m1.4.4.1.1.2.1.cmml">​</mo><mrow id="S2.E1.m1.4.4.1.1.2.3.2" xref="S2.E1.m1.4.4.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.4.4.1.1.2.3.2.1" xref="S2.E1.m1.4.4.1.1.2.cmml">(</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">𝜽</mi><mo rspace="0.278em" stretchy="false" id="S2.E1.m1.4.4.1.1.2.3.2.2" xref="S2.E1.m1.4.4.1.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.278em" id="S2.E1.m1.4.4.1.1.1" xref="S2.E1.m1.4.4.1.1.1.cmml">:=</mo><mrow id="S2.E1.m1.4.4.1.1.3" xref="S2.E1.m1.4.4.1.1.3.cmml"><mfrac id="S2.E1.m1.4.4.1.1.3.2" xref="S2.E1.m1.4.4.1.1.3.2.cmml"><mn id="S2.E1.m1.4.4.1.1.3.2.2" xref="S2.E1.m1.4.4.1.1.3.2.2.cmml">1</mn><mi id="S2.E1.m1.4.4.1.1.3.2.3" xref="S2.E1.m1.4.4.1.1.3.2.3.cmml">K</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.1.1.3.1" xref="S2.E1.m1.4.4.1.1.3.1.cmml">​</mo><mrow id="S2.E1.m1.4.4.1.1.3.3" xref="S2.E1.m1.4.4.1.1.3.3.cmml"><munder id="S2.E1.m1.4.4.1.1.3.3.1" xref="S2.E1.m1.4.4.1.1.3.3.1.cmml"><mo movablelimits="false" id="S2.E1.m1.4.4.1.1.3.3.1.2" xref="S2.E1.m1.4.4.1.1.3.3.1.2.cmml">∑</mo><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.3" xref="S2.E1.m1.1.1.1.3.cmml">k</mi><mo id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.2.cmml">∈</mo><mrow id="S2.E1.m1.1.1.1.4.2" xref="S2.E1.m1.1.1.1.4.1.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.4.2.1" xref="S2.E1.m1.1.1.1.4.1.1.cmml">[</mo><mi id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">K</mi><mo stretchy="false" id="S2.E1.m1.1.1.1.4.2.2" xref="S2.E1.m1.1.1.1.4.1.1.cmml">]</mo></mrow></mrow></munder><mrow id="S2.E1.m1.4.4.1.1.3.3.2" xref="S2.E1.m1.4.4.1.1.3.3.2.cmml"><msub id="S2.E1.m1.4.4.1.1.3.3.2.2" xref="S2.E1.m1.4.4.1.1.3.3.2.2.cmml"><mi id="S2.E1.m1.4.4.1.1.3.3.2.2.2" xref="S2.E1.m1.4.4.1.1.3.3.2.2.2.cmml">F</mi><mi id="S2.E1.m1.4.4.1.1.3.3.2.2.3" xref="S2.E1.m1.4.4.1.1.3.3.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.1.1.3.3.2.1" xref="S2.E1.m1.4.4.1.1.3.3.2.1.cmml">​</mo><mrow id="S2.E1.m1.4.4.1.1.3.3.2.3.2" xref="S2.E1.m1.4.4.1.1.3.3.2.cmml"><mo stretchy="false" id="S2.E1.m1.4.4.1.1.3.3.2.3.2.1" xref="S2.E1.m1.4.4.1.1.3.3.2.cmml">(</mo><mi id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">𝜽</mi><mo stretchy="false" id="S2.E1.m1.4.4.1.1.3.3.2.3.2.2" xref="S2.E1.m1.4.4.1.1.3.3.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S2.E1.m1.4.4.1.2" xref="S2.E1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.4b"><apply id="S2.E1.m1.4.4.1.1.cmml" xref="S2.E1.m1.4.4.1"><csymbol cd="latexml" id="S2.E1.m1.4.4.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1">assign</csymbol><apply id="S2.E1.m1.4.4.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.2"><times id="S2.E1.m1.4.4.1.1.2.1.cmml" xref="S2.E1.m1.4.4.1.1.2.1"></times><apply id="S2.E1.m1.4.4.1.1.2.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2"><apply id="S2.E1.m1.4.4.1.1.2.2.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.2.2.1.1.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1">subscript</csymbol><min id="S2.E1.m1.4.4.1.1.2.2.1.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.2"></min><ci id="S2.E1.m1.4.4.1.1.2.2.1.3.cmml" xref="S2.E1.m1.4.4.1.1.2.2.1.3">𝜽</ci></apply><ci id="S2.E1.m1.4.4.1.1.2.2.2.cmml" xref="S2.E1.m1.4.4.1.1.2.2.2">𝐹</ci></apply><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">𝜽</ci></apply><apply id="S2.E1.m1.4.4.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.3"><times id="S2.E1.m1.4.4.1.1.3.1.cmml" xref="S2.E1.m1.4.4.1.1.3.1"></times><apply id="S2.E1.m1.4.4.1.1.3.2.cmml" xref="S2.E1.m1.4.4.1.1.3.2"><divide id="S2.E1.m1.4.4.1.1.3.2.1.cmml" xref="S2.E1.m1.4.4.1.1.3.2"></divide><cn type="integer" id="S2.E1.m1.4.4.1.1.3.2.2.cmml" xref="S2.E1.m1.4.4.1.1.3.2.2">1</cn><ci id="S2.E1.m1.4.4.1.1.3.2.3.cmml" xref="S2.E1.m1.4.4.1.1.3.2.3">𝐾</ci></apply><apply id="S2.E1.m1.4.4.1.1.3.3.cmml" xref="S2.E1.m1.4.4.1.1.3.3"><apply id="S2.E1.m1.4.4.1.1.3.3.1.cmml" xref="S2.E1.m1.4.4.1.1.3.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.3.3.1.1.cmml" xref="S2.E1.m1.4.4.1.1.3.3.1">subscript</csymbol><sum id="S2.E1.m1.4.4.1.1.3.3.1.2.cmml" xref="S2.E1.m1.4.4.1.1.3.3.1.2"></sum><apply id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><in id="S2.E1.m1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.2"></in><ci id="S2.E1.m1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.3">𝑘</ci><apply id="S2.E1.m1.1.1.1.4.1.cmml" xref="S2.E1.m1.1.1.1.4.2"><csymbol cd="latexml" id="S2.E1.m1.1.1.1.4.1.1.cmml" xref="S2.E1.m1.1.1.1.4.2.1">delimited-[]</csymbol><ci id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1">𝐾</ci></apply></apply></apply><apply id="S2.E1.m1.4.4.1.1.3.3.2.cmml" xref="S2.E1.m1.4.4.1.1.3.3.2"><times id="S2.E1.m1.4.4.1.1.3.3.2.1.cmml" xref="S2.E1.m1.4.4.1.1.3.3.2.1"></times><apply id="S2.E1.m1.4.4.1.1.3.3.2.2.cmml" xref="S2.E1.m1.4.4.1.1.3.3.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.3.3.2.2.1.cmml" xref="S2.E1.m1.4.4.1.1.3.3.2.2">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.3.3.2.2.2.cmml" xref="S2.E1.m1.4.4.1.1.3.3.2.2.2">𝐹</ci><ci id="S2.E1.m1.4.4.1.1.3.3.2.2.3.cmml" xref="S2.E1.m1.4.4.1.1.3.3.2.2.3">𝑘</ci></apply><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">𝜽</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.4c">\min\limits_{\bm{\theta}}F(\bm{\theta}):=\frac{1}{K}\sum_{k\in[K]}F_{k}(\bm{\theta}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p1.7" class="ltx_p">where <math id="S2.SS2.p1.2.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS2.p1.2.m1.1a"><mi id="S2.SS2.p1.2.m1.1.1" xref="S2.SS2.p1.2.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m1.1b"><ci id="S2.SS2.p1.2.m1.1.1.cmml" xref="S2.SS2.p1.2.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m1.1c">K</annotation></semantics></math> represents the total number of clients and <math id="S2.SS2.p1.3.m2.4" class="ltx_Math" alttext="F_{k}(\bm{\theta})=\mathbb{E}_{\bm{z}\sim\mathcal{D}_{k}}[\ell(\bm{\theta};\bm{z})]" display="inline"><semantics id="S2.SS2.p1.3.m2.4a"><mrow id="S2.SS2.p1.3.m2.4.4" xref="S2.SS2.p1.3.m2.4.4.cmml"><mrow id="S2.SS2.p1.3.m2.4.4.3" xref="S2.SS2.p1.3.m2.4.4.3.cmml"><msub id="S2.SS2.p1.3.m2.4.4.3.2" xref="S2.SS2.p1.3.m2.4.4.3.2.cmml"><mi id="S2.SS2.p1.3.m2.4.4.3.2.2" xref="S2.SS2.p1.3.m2.4.4.3.2.2.cmml">F</mi><mi id="S2.SS2.p1.3.m2.4.4.3.2.3" xref="S2.SS2.p1.3.m2.4.4.3.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.p1.3.m2.4.4.3.1" xref="S2.SS2.p1.3.m2.4.4.3.1.cmml">​</mo><mrow id="S2.SS2.p1.3.m2.4.4.3.3.2" xref="S2.SS2.p1.3.m2.4.4.3.cmml"><mo stretchy="false" id="S2.SS2.p1.3.m2.4.4.3.3.2.1" xref="S2.SS2.p1.3.m2.4.4.3.cmml">(</mo><mi id="S2.SS2.p1.3.m2.1.1" xref="S2.SS2.p1.3.m2.1.1.cmml">𝜽</mi><mo stretchy="false" id="S2.SS2.p1.3.m2.4.4.3.3.2.2" xref="S2.SS2.p1.3.m2.4.4.3.cmml">)</mo></mrow></mrow><mo id="S2.SS2.p1.3.m2.4.4.2" xref="S2.SS2.p1.3.m2.4.4.2.cmml">=</mo><mrow id="S2.SS2.p1.3.m2.4.4.1" xref="S2.SS2.p1.3.m2.4.4.1.cmml"><msub id="S2.SS2.p1.3.m2.4.4.1.3" xref="S2.SS2.p1.3.m2.4.4.1.3.cmml"><mi id="S2.SS2.p1.3.m2.4.4.1.3.2" xref="S2.SS2.p1.3.m2.4.4.1.3.2.cmml">𝔼</mi><mrow id="S2.SS2.p1.3.m2.4.4.1.3.3" xref="S2.SS2.p1.3.m2.4.4.1.3.3.cmml"><mi id="S2.SS2.p1.3.m2.4.4.1.3.3.2" xref="S2.SS2.p1.3.m2.4.4.1.3.3.2.cmml">𝒛</mi><mo id="S2.SS2.p1.3.m2.4.4.1.3.3.1" xref="S2.SS2.p1.3.m2.4.4.1.3.3.1.cmml">∼</mo><msub id="S2.SS2.p1.3.m2.4.4.1.3.3.3" xref="S2.SS2.p1.3.m2.4.4.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.3.m2.4.4.1.3.3.3.2" xref="S2.SS2.p1.3.m2.4.4.1.3.3.3.2.cmml">𝒟</mi><mi id="S2.SS2.p1.3.m2.4.4.1.3.3.3.3" xref="S2.SS2.p1.3.m2.4.4.1.3.3.3.3.cmml">k</mi></msub></mrow></msub><mo lspace="0em" rspace="0em" id="S2.SS2.p1.3.m2.4.4.1.2" xref="S2.SS2.p1.3.m2.4.4.1.2.cmml">​</mo><mrow id="S2.SS2.p1.3.m2.4.4.1.1.1" xref="S2.SS2.p1.3.m2.4.4.1.1.2.cmml"><mo stretchy="false" id="S2.SS2.p1.3.m2.4.4.1.1.1.2" xref="S2.SS2.p1.3.m2.4.4.1.1.2.1.cmml">[</mo><mrow id="S2.SS2.p1.3.m2.4.4.1.1.1.1" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.cmml"><mi mathvariant="normal" id="S2.SS2.p1.3.m2.4.4.1.1.1.1.2" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.2.cmml">ℓ</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.3.m2.4.4.1.1.1.1.1" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.1.cmml">​</mo><mrow id="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.2" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.2.1" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.1.cmml">(</mo><mi id="S2.SS2.p1.3.m2.2.2" xref="S2.SS2.p1.3.m2.2.2.cmml">𝜽</mi><mo id="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.2.2" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.1.cmml">;</mo><mi id="S2.SS2.p1.3.m2.3.3" xref="S2.SS2.p1.3.m2.3.3.cmml">𝒛</mi><mo stretchy="false" id="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.2.3" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.SS2.p1.3.m2.4.4.1.1.1.3" xref="S2.SS2.p1.3.m2.4.4.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m2.4b"><apply id="S2.SS2.p1.3.m2.4.4.cmml" xref="S2.SS2.p1.3.m2.4.4"><eq id="S2.SS2.p1.3.m2.4.4.2.cmml" xref="S2.SS2.p1.3.m2.4.4.2"></eq><apply id="S2.SS2.p1.3.m2.4.4.3.cmml" xref="S2.SS2.p1.3.m2.4.4.3"><times id="S2.SS2.p1.3.m2.4.4.3.1.cmml" xref="S2.SS2.p1.3.m2.4.4.3.1"></times><apply id="S2.SS2.p1.3.m2.4.4.3.2.cmml" xref="S2.SS2.p1.3.m2.4.4.3.2"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m2.4.4.3.2.1.cmml" xref="S2.SS2.p1.3.m2.4.4.3.2">subscript</csymbol><ci id="S2.SS2.p1.3.m2.4.4.3.2.2.cmml" xref="S2.SS2.p1.3.m2.4.4.3.2.2">𝐹</ci><ci id="S2.SS2.p1.3.m2.4.4.3.2.3.cmml" xref="S2.SS2.p1.3.m2.4.4.3.2.3">𝑘</ci></apply><ci id="S2.SS2.p1.3.m2.1.1.cmml" xref="S2.SS2.p1.3.m2.1.1">𝜽</ci></apply><apply id="S2.SS2.p1.3.m2.4.4.1.cmml" xref="S2.SS2.p1.3.m2.4.4.1"><times id="S2.SS2.p1.3.m2.4.4.1.2.cmml" xref="S2.SS2.p1.3.m2.4.4.1.2"></times><apply id="S2.SS2.p1.3.m2.4.4.1.3.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m2.4.4.1.3.1.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3">subscript</csymbol><ci id="S2.SS2.p1.3.m2.4.4.1.3.2.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3.2">𝔼</ci><apply id="S2.SS2.p1.3.m2.4.4.1.3.3.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3.3"><csymbol cd="latexml" id="S2.SS2.p1.3.m2.4.4.1.3.3.1.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3.3.1">similar-to</csymbol><ci id="S2.SS2.p1.3.m2.4.4.1.3.3.2.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3.3.2">𝒛</ci><apply id="S2.SS2.p1.3.m2.4.4.1.3.3.3.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m2.4.4.1.3.3.3.1.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3.3.3">subscript</csymbol><ci id="S2.SS2.p1.3.m2.4.4.1.3.3.3.2.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3.3.3.2">𝒟</ci><ci id="S2.SS2.p1.3.m2.4.4.1.3.3.3.3.cmml" xref="S2.SS2.p1.3.m2.4.4.1.3.3.3.3">𝑘</ci></apply></apply></apply><apply id="S2.SS2.p1.3.m2.4.4.1.1.2.cmml" xref="S2.SS2.p1.3.m2.4.4.1.1.1"><csymbol cd="latexml" id="S2.SS2.p1.3.m2.4.4.1.1.2.1.cmml" xref="S2.SS2.p1.3.m2.4.4.1.1.1.2">delimited-[]</csymbol><apply id="S2.SS2.p1.3.m2.4.4.1.1.1.1.cmml" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1"><times id="S2.SS2.p1.3.m2.4.4.1.1.1.1.1.cmml" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.1"></times><ci id="S2.SS2.p1.3.m2.4.4.1.1.1.1.2.cmml" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.2">ℓ</ci><list id="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.1.cmml" xref="S2.SS2.p1.3.m2.4.4.1.1.1.1.3.2"><ci id="S2.SS2.p1.3.m2.2.2.cmml" xref="S2.SS2.p1.3.m2.2.2">𝜽</ci><ci id="S2.SS2.p1.3.m2.3.3.cmml" xref="S2.SS2.p1.3.m2.3.3">𝒛</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m2.4c">F_{k}(\bm{\theta})=\mathbb{E}_{\bm{z}\sim\mathcal{D}_{k}}[\ell(\bm{\theta};\bm{z})]</annotation></semantics></math> denotes the expected risk of the <math id="S2.SS2.p1.4.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS2.p1.4.m3.1a"><mi id="S2.SS2.p1.4.m3.1.1" xref="S2.SS2.p1.4.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m3.1b"><ci id="S2.SS2.p1.4.m3.1.1.cmml" xref="S2.SS2.p1.4.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m3.1c">k</annotation></semantics></math>-th client. Here, <math id="S2.SS2.p1.5.m4.1" class="ltx_Math" alttext="\mathcal{D}_{k}" display="inline"><semantics id="S2.SS2.p1.5.m4.1a"><msub id="S2.SS2.p1.5.m4.1.1" xref="S2.SS2.p1.5.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.5.m4.1.1.2" xref="S2.SS2.p1.5.m4.1.1.2.cmml">𝒟</mi><mi id="S2.SS2.p1.5.m4.1.1.3" xref="S2.SS2.p1.5.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m4.1b"><apply id="S2.SS2.p1.5.m4.1.1.cmml" xref="S2.SS2.p1.5.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m4.1.1.1.cmml" xref="S2.SS2.p1.5.m4.1.1">subscript</csymbol><ci id="S2.SS2.p1.5.m4.1.1.2.cmml" xref="S2.SS2.p1.5.m4.1.1.2">𝒟</ci><ci id="S2.SS2.p1.5.m4.1.1.3.cmml" xref="S2.SS2.p1.5.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m4.1c">\mathcal{D}_{k}</annotation></semantics></math> is the data distribution for the <math id="S2.SS2.p1.6.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS2.p1.6.m5.1a"><mi id="S2.SS2.p1.6.m5.1.1" xref="S2.SS2.p1.6.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m5.1b"><ci id="S2.SS2.p1.6.m5.1.1.cmml" xref="S2.SS2.p1.6.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m5.1c">k</annotation></semantics></math>-th client, and <math id="S2.SS2.p1.7.m6.2" class="ltx_Math" alttext="\ell(\cdot;\cdot)" display="inline"><semantics id="S2.SS2.p1.7.m6.2a"><mrow id="S2.SS2.p1.7.m6.2.3" xref="S2.SS2.p1.7.m6.2.3.cmml"><mi mathvariant="normal" id="S2.SS2.p1.7.m6.2.3.2" xref="S2.SS2.p1.7.m6.2.3.2.cmml">ℓ</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.7.m6.2.3.1" xref="S2.SS2.p1.7.m6.2.3.1.cmml">​</mo><mrow id="S2.SS2.p1.7.m6.2.3.3.2" xref="S2.SS2.p1.7.m6.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS2.p1.7.m6.2.3.3.2.1" xref="S2.SS2.p1.7.m6.2.3.3.1.cmml">(</mo><mo lspace="0em" rspace="0em" id="S2.SS2.p1.7.m6.1.1" xref="S2.SS2.p1.7.m6.1.1.cmml">⋅</mo><mo rspace="0em" id="S2.SS2.p1.7.m6.2.3.3.2.2" xref="S2.SS2.p1.7.m6.2.3.3.1.cmml">;</mo><mo lspace="0em" rspace="0em" id="S2.SS2.p1.7.m6.2.2" xref="S2.SS2.p1.7.m6.2.2.cmml">⋅</mo><mo stretchy="false" id="S2.SS2.p1.7.m6.2.3.3.2.3" xref="S2.SS2.p1.7.m6.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m6.2b"><apply id="S2.SS2.p1.7.m6.2.3.cmml" xref="S2.SS2.p1.7.m6.2.3"><times id="S2.SS2.p1.7.m6.2.3.1.cmml" xref="S2.SS2.p1.7.m6.2.3.1"></times><ci id="S2.SS2.p1.7.m6.2.3.2.cmml" xref="S2.SS2.p1.7.m6.2.3.2">ℓ</ci><list id="S2.SS2.p1.7.m6.2.3.3.1.cmml" xref="S2.SS2.p1.7.m6.2.3.3.2"><ci id="S2.SS2.p1.7.m6.1.1.cmml" xref="S2.SS2.p1.7.m6.1.1">⋅</ci><ci id="S2.SS2.p1.7.m6.2.2.cmml" xref="S2.SS2.p1.7.m6.2.2">⋅</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m6.2c">\ell(\cdot;\cdot)</annotation></semantics></math> is a user-specified loss function.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The most representative algorithms in the FL literature are the <span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">FedAvg</span>-family algorithms <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib119" title="" class="ltx_ref">2017</a>); Reddi et al. (<a href="#bib.bib136" title="" class="ltx_ref">2021</a>)</cite>. The standard <span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_typewriter">FedAvg</span> involves periodic interactions between the server and decentralized clients to exchange trainable model parameters.
In this process, each client independently trains the model on its local data and sends the model updates to a central server. The server aggregates these updates by computing their average to update the global model, which is subsequently redistributed to the clients for further iterations.
Many variants have been proposed to tackle issues such as convergence and local data heterogeneity <cite class="ltx_cite ltx_citemacro_cite">Diao et al. (<a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>. For example, <span id="S2.SS2.p2.1.3" class="ltx_text ltx_font_typewriter">FedProx</span> <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib92" title="" class="ltx_ref">2020</a>)</cite> and <span id="S2.SS2.p2.1.4" class="ltx_text ltx_font_typewriter">FedDyn</span> <cite class="ltx_cite ltx_citemacro_cite">Acar et al. (<a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite> introduce regularizer terms to penalize client updates that are far away from the server model. A general framework <span id="S2.SS2.p2.1.5" class="ltx_text ltx_font_typewriter">FedOpt</span> <cite class="ltx_cite ltx_citemacro_cite">Reddi et al. (<a href="#bib.bib136" title="" class="ltx_ref">2021</a>)</cite> unifies adaptive optimizers (<span id="S2.SS2.p2.1.6" class="ltx_text ltx_font_italic">Adam</span>, <span id="S2.SS2.p2.1.7" class="ltx_text ltx_font_italic">Yogi</span>, <em id="S2.SS2.p2.1.8" class="ltx_emph ltx_font_italic">etc</em>.) and demonstrates superior convergence speed when compared to the naive <span id="S2.SS2.p2.1.9" class="ltx_text ltx_font_typewriter">FedAvg</span>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">FL offers an efficient privacy-preserving way to train models on large-scale and diverse data <cite class="ltx_cite ltx_citemacro_cite">Kairouz et al. (<a href="#bib.bib79" title="" class="ltx_ref">2021</a>)</cite>, leading to its application across various domains such as healthcare <cite class="ltx_cite ltx_citemacro_cite">Lincy and Kowshalya (<a href="#bib.bib101" title="" class="ltx_ref">2020</a>); Rieke et al. (<a href="#bib.bib140" title="" class="ltx_ref">2020</a>); Joshi et al. (<a href="#bib.bib78" title="" class="ltx_ref">2022</a>)</cite>, finance <cite class="ltx_cite ltx_citemacro_cite">Chatterjee et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>); Liu et al. (<a href="#bib.bib106" title="" class="ltx_ref">2023b</a>)</cite>, and smart cities <cite class="ltx_cite ltx_citemacro_cite">Ramu et al. (<a href="#bib.bib135" title="" class="ltx_ref">2022</a>); Pandya et al. (<a href="#bib.bib125" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>FM-FL: Motivation &amp; Challenges</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we first motivate the synergy of FMs and FL (Section <a href="#S3.SS1" title="3.1 Motivation ‣ 3 FM-FL: Motivation &amp; Challenges ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), then summarize the key challenges (Section <a href="#S3.SS2" title="3.2 Core Challenges ‣ 3 FM-FL: Motivation &amp; Challenges ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Motivation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The integration of FMs and FL represents a compelling collaboration that leverages each other’s strengths to address their respective limitations, embodying a complementary relationship <cite class="ltx_cite ltx_citemacro_cite">Zhuang et al. (<a href="#bib.bib216" title="" class="ltx_ref">2023</a>); Li and Wang (<a href="#bib.bib93" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">FL expands data availability for FMs</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">By leveraging data from a wide range of sources in a privacy-preserving manner, FL makes it possible to build models on sensitive data in specific domains, such as healthcare <cite class="ltx_cite ltx_citemacro_cite">Lincy and Kowshalya (<a href="#bib.bib101" title="" class="ltx_ref">2020</a>); Joshi et al. (<a href="#bib.bib78" title="" class="ltx_ref">2022</a>); Rieke et al. (<a href="#bib.bib140" title="" class="ltx_ref">2020</a>)</cite> and finance <cite class="ltx_cite ltx_citemacro_cite">Chatterjee et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>); Liu et al. (<a href="#bib.bib106" title="" class="ltx_ref">2023b</a>)</cite>. This enhances the diversity and volume of training data, improving model robustness and adaptability. Moreover, FL enables the integration of personal and task-specific data, allowing FMs to be customized for personal applications. For instance, Google has trained next-word-prediction language models on mobile keyboard input data with FL to improve user experience <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib188" title="" class="ltx_ref">2023</a>); Bonawitz et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">FMs boost FL with feature representation and few-shot learning capabilities</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">By pre-training on large-scale generic data, FMs acquire essential knowledge and understanding capabilities <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite>, providing multiple benefits to FL. Firstly, they benefit FL systems by offering advanced feature representations and learning capabilities from the outset. Secondly, leveraging the pre-learned knowledge of FMs can accelerate the FL process, enabling efficient and effective adaptation to specific tasks with minimal additional training. Thirdly, FMs’ powerful generative capabilities could help FL overcome the data heterogeneity challenge by synthesizing extra data, thus accelerating model convergence <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib68" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Core Challenges</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In this part, we discuss challenges emerging from the FM-FL marriage in three aspects: <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">efficiency</span>, <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">adaptability</span>, as well as <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_italic">trustworthiness</span>.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Efficiency Challenges</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">Efficiency challenges stem from the mismatch between the significant resource demands of FM training and the limited, heterogeneous system resources (<em id="S3.SS2.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> mobile devices) within FL systems, such as communication bandwidth, computational power, and memory <cite class="ltx_cite ltx_citemacro_cite">Su et al. (<a href="#bib.bib152" title="" class="ltx_ref">2023</a>)</cite>. The communication bottleneck of FL is induced by frequently exchanging training information between the server and clients over limited bandwidth channels <cite class="ltx_cite ltx_citemacro_cite">Kairouz et al. (<a href="#bib.bib79" title="" class="ltx_ref">2021</a>)</cite>. The substantial number of parameters in FMs further exacerbates this burden, thus hindering the training process.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Adaptability Challenges</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">Adaptability challenges arise from the adaptation of an FM to a specific downstream task (<em id="S3.SS2.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> by fine-tuning)
in FL settings. Key challenges include <span id="S3.SS2.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">data heterogeneity</span> and <span id="S3.SS2.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_italic">resource heterogeneity</span>. Performance degradation in FL, attributed to heterogeneous data distributions among clients, is a well-recognized issue <cite class="ltx_cite ltx_citemacro_cite">Kairouz et al. (<a href="#bib.bib79" title="" class="ltx_ref">2021</a>); Li et al. (<a href="#bib.bib88" title="" class="ltx_ref">2022</a>)</cite>. A recent study <cite class="ltx_cite ltx_citemacro_cite">Babakniya et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023a</a>)</cite> has shown that such performance penalty is even more substantial when fine-tuning FMs. For NLP tasks, data heterogeneity can manifest as variations in language, style, topic, or sentiment across datasets held by different clients. In multi-modal scenarios, the challenge is even more pronounced due to the inherent diversity in data types (<em id="S3.SS2.SSS0.Px2.p1.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> text, images, and audio) <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib196" title="" class="ltx_ref">2023a</a>)</cite>. Addressing data heterogeneity involves not just identifying and measuring it but also developing algorithms that are robust to such diversity, ensuring that the model can learn effectively from varied data contributions without compromising on performance. In terms of resource heterogeneity, the memory and computational resources of the devices for different participants may be diverse <cite class="ltx_cite ltx_citemacro_cite">Diao et al. (<a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>, which could cause delays for model synchronization and inactivation of some participants, <em id="S3.SS2.SSS0.Px2.p1.1.5" class="ltx_emph ltx_font_italic">i.e.,</em> stragglers, making it challenging to leverage the full potential of FMs in FL settings.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Trustworthiness Challenges</h5>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">Trustworthiness challenges emphasize the concerns regarding privacy, security, and ethical considerations in the lifecycle of FM-FL, from the pre-training and model adaptation to the application stages. We present two representative challenges from this perspective: (1) <span id="S3.SS2.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">intellectual property</span>: Intellectual Property (IP) protection in FM-FL primarily involves attributing ownership rights for both models and data.
From the server’s perspective, broadcasting a pre-trained model to multiple nodes for fine-tuning poses IP protection and security risks (<em id="S3.SS2.SSS0.Px3.p1.1.2" class="ltx_emph ltx_font_italic">e.g.,</em> model theft), necessitating measures to safeguard IP rights and ensure model integrity <cite class="ltx_cite ltx_citemacro_cite">Kang et al. (<a href="#bib.bib80" title="" class="ltx_ref">2024</a>)</cite>; (2)
<span id="S3.SS2.SSS0.Px3.p1.1.3" class="ltx_text ltx_font_italic">privacy leakage</span>: Although FL does not immediately share data, studies have shown that it may not always guarantee sufficient privacy preservation <cite class="ltx_cite ltx_citemacro_cite">Geiping et al. (<a href="#bib.bib51" title="" class="ltx_ref">2020</a>)</cite>, as model parameters (<em id="S3.SS2.SSS0.Px3.p1.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> weights or gradients) may leak sensitive information to malicious adversaries <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib215" title="" class="ltx_ref">2019</a>)</cite>.
(3)<span id="S3.SS2.SSS0.Px3.p1.1.5" class="ltx_text ltx_font_italic">Poisoning Attacks</span>: FL systems are inherently vulnerable to attacks due to their wide attack surface and reliance on network communication <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib91" title="" class="ltx_ref">2023b</a>)</cite>. Poisoning attacks are carried out by malicious participants, aiming to bias the global model to the desire of attackers.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<div id="S3.F1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:292.9pt;vertical-align:-288.2pt;"><span class="ltx_transformed_inner" style="transform:translate(-127.4pt,1.4pt) scale(0.629913545298441,0.629913545298441) ;"><span id="S3.F1.1.1" class="ltx_ERROR undefined">{forest}</span>
<p id="S3.F1.1.2" class="ltx_p">for tree=
edge path=[<span id="S3.F1.1.2.1" class="ltx_ERROR undefined">\forestoption</span>edge,-&gt;, &gt;=Latex[length=1.mm,width=1.mm]]
(!u.parent anchor) – +(4pt,0pt) |- (.child anchor)
<span id="S3.F1.1.2.2" class="ltx_ERROR undefined">\forestoption</span>edge label;,
grow=east,
reversed=true,
anchor=base west,
parent anchor=east,
child anchor=west,
base=center,
font=,
rectangle,
draw=hidden-draw,
rounded corners,
align=center,
minimum width=4em,
edge+=semithick, draw=hidden-draw,line width=0.5pt,
s sep=2pt,
l sep=12pt,
inner xsep=4pt,
inner ysep=3pt,
ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center,
,
where level=0l sep = 1pt, s sep = 1pt,
where level=3l sep = 6pt
[FM-FL, ver,
[Efficiency 
<br class="ltx_break">(§<a href="#S4.SS1" title="4.1 Efficiency ‣ 4 Techniques ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>), l1node,
[Parameter-Efficient 
<br class="ltx_break">Fine-Tuning, l2node,
[Selective, l3node,
[
 <span id="S3.F1.1.2.3" class="ltx_text ltx_font_italic">RaFFM</span> <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib198" title="" class="ltx_ref">2023c</a>)</cite>,
<span id="S3.F1.1.2.4" class="ltx_text ltx_font_italic">FedBF</span> <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib210" title="" class="ltx_ref">2023f</a>)</cite>
, leaf]
]
[Additive, l3node,
[
 <span id="S3.F1.1.2.5" class="ltx_text ltx_font_italic">FedCLIP</span> <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib110" title="" class="ltx_ref">2023a</a>)</cite>, <span id="S3.F1.1.2.6" class="ltx_text ltx_font_italic">FedDAT</span> <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib27" title="" class="ltx_ref">2024</a>)</cite>
, leaf]
]
[Reparameterization-based, l3node,
[
 <span id="S3.F1.1.2.7" class="ltx_text ltx_font_smallcaps">HetLoRA</span> <cite class="ltx_cite ltx_citemacro_cite">Cho et al. (<a href="#bib.bib30" title="" class="ltx_ref">2024</a>)</cite>, <span id="S3.F1.1.2.8" class="ltx_text ltx_font_italic">FedDPA</span> <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib192" title="" class="ltx_ref">2024b</a>)</cite>, leaf
]
]
]
[Model Compression, l2node,
[Sparsification, l3node,
[ <span id="S3.F1.1.2.9" class="ltx_text ltx_font_italic">PruneFL</span> <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a href="#bib.bib75" title="" class="ltx_ref">2023c</a>)</cite>,
<span id="S3.F1.1.2.10" class="ltx_text ltx_font_italic">FLASH</span> <cite class="ltx_cite ltx_citemacro_cite">Babakniya et al. (<a href="#bib.bib9" title="" class="ltx_ref">2023b</a>)</cite>,leaf]
]
[Quantization, l3node,
[ <span id="S3.F1.1.2.11" class="ltx_text ltx_font_italic">FedSplitBERT</span> <cite class="ltx_cite ltx_citemacro_cite">Lit et al. (<a href="#bib.bib103" title="" class="ltx_ref">2022</a>)</cite>,leaf]
]
]
[Zeroth-Order 
<br class="ltx_break">Optimization, l2node,
[
 <span id="S3.F1.1.2.12" class="ltx_text ltx_font_italic">BAFFLE</span> <cite class="ltx_cite ltx_citemacro_cite">Feng et al. (<a href="#bib.bib46" title="" class="ltx_ref">2023b</a>)</cite>, <span id="S3.F1.1.2.13" class="ltx_text ltx_font_italic">FedZeN</span> <cite class="ltx_cite ltx_citemacro_cite">Maritan et al. (<a href="#bib.bib118" title="" class="ltx_ref">2023</a>)</cite>, <span id="S3.F1.1.2.14" class="ltx_text ltx_font_italic">FedKSeed</span> <cite class="ltx_cite ltx_citemacro_cite">Qin et al. (<a href="#bib.bib129" title="" class="ltx_ref">2024</a>)</cite>, 
<br class="ltx_break"><span id="S3.F1.1.2.15" class="ltx_text ltx_font_italic">FwdLLM</span> <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib185" title="" class="ltx_ref">2024a</a>)</cite>, <span id="S3.F1.1.2.16" class="ltx_text ltx_font_italic">ZooPFL</span> <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib111" title="" class="ltx_ref">2023b</a>)</cite>,
<span id="S3.F1.1.2.17" class="ltx_text ltx_font_italic">FedMeZO</span> <cite class="ltx_cite ltx_citemacro_cite">Ling et al. (<a href="#bib.bib102" title="" class="ltx_ref">2024</a>)</cite>,wide leaf]
]
]
[Adaptability
<br class="ltx_break">(§<a href="#S4.SS2" title="4.2 Adaptability ‣ 4 Techniques ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>), l1node,
[Domain-Centric, l2node,
[Domain-Adaptive Pre-Training, l3node,
[

 <span id="S3.F1.1.2.18" class="ltx_text ltx_font_italic">FMTDA</span> <cite class="ltx_cite ltx_citemacro_cite">Yao et al. (<a href="#bib.bib193" title="" class="ltx_ref">2022</a>)</cite>,
<span id="S3.F1.1.2.19" class="ltx_text ltx_font_italic">FEDBFPT</span> <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib172" title="" class="ltx_ref">2023</a>)</cite>
, leaf
]
]
[Multi-Domain Adaptation, l3node,
[
 FedAPT <cite class="ltx_cite ltx_citemacro_cite">Su et al. (<a href="#bib.bib153" title="" class="ltx_ref">2024</a>)</cite>, DiPrompT <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a href="#bib.bib13" title="" class="ltx_ref">2024b</a>)</cite>, leaf
]
]
]
[Client-Centric, l2node,
[Personalization, l3node,
[
 <span id="S3.F1.1.2.20" class="ltx_text ltx_font_italic">FedDAT</span> <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib27" title="" class="ltx_ref">2024</a>)</cite>,
<span id="S3.F1.1.2.21" class="ltx_text ltx_font_italic">Fed-MNMT</span> <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib108" title="" class="ltx_ref">2023d</a>)</cite>,leaf
]
]
[Client Clustering, l3node,
[
 <span id="S3.F1.1.2.22" class="ltx_text ltx_font_italic">FedLFC</span> <cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a href="#bib.bib60" title="" class="ltx_ref">2024b</a>)</cite>,
<span id="S3.F1.1.2.23" class="ltx_text ltx_font_italic">FL-TAC</span> <cite class="ltx_cite ltx_citemacro_cite">Ping et al. (<a href="#bib.bib127" title="" class="ltx_ref">2024</a>)</cite>,leaf
]
]
]
[System-Centric, l2node,
[Resource-Heterogeneous, l3node,
[
 <span id="S3.F1.1.2.24" class="ltx_text ltx_font_italic">FedRA</span> <cite class="ltx_cite ltx_citemacro_cite">Su et al. (<a href="#bib.bib152" title="" class="ltx_ref">2023</a>)</cite>, <span id="S3.F1.1.2.25" class="ltx_text ltx_font_smallcaps">HetLoRA</span> <cite class="ltx_cite ltx_citemacro_cite">Cho et al. (<a href="#bib.bib30" title="" class="ltx_ref">2024</a>)</cite>,leaf
]
]
[
Split Learning, l3node,
[ <span id="S3.F1.1.2.26" class="ltx_text ltx_font_italic">FedBERT</span> <cite class="ltx_cite ltx_citemacro_cite">Tian et al. (<a href="#bib.bib165" title="" class="ltx_ref">2022</a>)</cite>, <span id="S3.F1.1.2.27" class="ltx_text ltx_font_italic">FedSplitX</span> <cite class="ltx_cite ltx_citemacro_cite">Shin et al. (<a href="#bib.bib149" title="" class="ltx_ref">2023b</a>)</cite>,leaf]
]
]
]
[Trustworthiness 
<br class="ltx_break">(§<a href="#S4.SS3" title="4.3 Trustworthiness ‣ 4 Techniques ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>), l1node,
[IP Protection, l2node,
[Watermarking, l3node,
[
 <span id="S3.F1.1.2.28" class="ltx_text ltx_font_italic">WAFFLE</span> <cite class="ltx_cite ltx_citemacro_cite">Tekgul et al. (<a href="#bib.bib163" title="" class="ltx_ref">2021</a>)</cite>, <span id="S3.F1.1.2.29" class="ltx_text ltx_font_italic">DUW</span> <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib197" title="" class="ltx_ref">2023b</a>)</cite>,leaf
]
]
[Black-Box Tuning, l3node,
[
 <span id="S3.F1.1.2.30" class="ltx_text ltx_font_italic">Fed-BBPT</span> <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib100" title="" class="ltx_ref">2023</a>)</cite>,
<span id="S3.F1.1.2.31" class="ltx_text ltx_font_italic">pFedGPT</span> <cite class="ltx_cite ltx_citemacro_cite">Rui et al. (<a href="#bib.bib143" title="" class="ltx_ref">2024</a>)</cite>,leaf
]
]
]
[Privacy Protection, l2node,
[Privacy-Preserving Techniques, l3node,
[

 <span id="S3.F1.1.2.32" class="ltx_text ltx_font_italic">DP-FTRL</span> <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib188" title="" class="ltx_ref">2023</a>)</cite>,
<span id="S3.F1.1.2.33" class="ltx_text ltx_font_italic">DP-LoRA</span> <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib107" title="" class="ltx_ref">2023c</a>)</cite>
,
leaf
]
]
[Privacy Attack, l3node,
[
 FILM <cite class="ltx_cite ltx_citemacro_cite">Gupta et al. (<a href="#bib.bib61" title="" class="ltx_ref">2022</a>)</cite>,
DRA <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib209" title="" class="ltx_ref">2024c</a>)</cite>
, leaf
]
]
]
[Attack Robustness, l2node,
[Poisoning Attacks, l3node,
[ Fed-EBD <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib96" title="" class="ltx_ref">2024c</a>)</cite>, leaf
]
]
[Defense Techniques, l3node,
[ <span id="S3.F1.1.2.34" class="ltx_text ltx_font_italic">ClippedClustering</span> <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib91" title="" class="ltx_ref">2023b</a>)</cite>, <span id="S3.F1.1.2.35" class="ltx_text ltx_font_italic">Fed-FA</span> <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib207" title="" class="ltx_ref">2023d</a>)</cite>, leaf
]
]
]
]
]</p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Taxonomy of research in foundation models with federated learning.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Techniques</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Recent work has begun to address challenges associated with adapting pre-trained FMs to specific downstream tasks in FL settings. In this section, we survey FM-FL techniques on three aspects, namely <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">efficiency</span> (Section <a href="#S4.SS1" title="4.1 Efficiency ‣ 4 Techniques ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>), <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">adaptability</span> (Section <a href="#S4.SS2" title="4.2 Adaptability ‣ 4 Techniques ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>), and <span id="S4.p1.1.3" class="ltx_text ltx_font_italic">trustworthiness</span> (Section <a href="#S4.SS3" title="4.3 Trustworthiness ‣ 4 Techniques ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>). As illustrated in Figure <a href="#S3.F1" title="Figure 1 ‣ Trustworthiness Challenges ‣ 3.2 Core Challenges ‣ 3 FM-FL: Motivation &amp; Challenges ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we further refine them according to the key features of different methods.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Efficiency</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">There has been a considerable focus on developing resource-efficient approaches. This part describes techniques that improve resource efficiency.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Parameter-Efficient Fine-Tuning</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">Federated Parameter-Efficient Fine-Tuning (FedPEFT), originating from the fine-tuning practices of FMs <cite class="ltx_cite ltx_citemacro_cite">Lester et al. (<a href="#bib.bib84" title="" class="ltx_ref">2021</a>); Hu et al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>); Li and Liang (<a href="#bib.bib97" title="" class="ltx_ref">2021</a>)</cite>, is a suite of techniques designed to reduce both the computational load and the associated communication overheads <cite class="ltx_cite ltx_citemacro_cite">Malaviya et al. (<a href="#bib.bib113" title="" class="ltx_ref">2023</a>); Woisetschläger et al. (<a href="#bib.bib175" title="" class="ltx_ref">2024</a>)</cite>.
In alignment with existing FM fine-tuning taxonomies <cite class="ltx_cite ltx_citemacro_cite">Lialin et al. (<a href="#bib.bib99" title="" class="ltx_ref">2023</a>); Ding et al. (<a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>, we present FedPEFT methods in three categories: <span id="S4.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">selective methods</span>, <span id="S4.SS1.SSS1.p1.1.2" class="ltx_text ltx_font_italic">additive methods</span>, and <span id="S4.SS1.SSS1.p1.1.3" class="ltx_text ltx_font_italic">reparameterization-based methods</span>.</p>
</div>
<section id="S4.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Selective Methods</h5>

<div id="S4.SS1.SSS1.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.Px1.p1.2" class="ltx_p">Selective methods fine-tune a small subset of the parameters, leaving the majority unchanged. In the field of LLMs, a prominent example of such methods is BitFit <cite class="ltx_cite ltx_citemacro_cite">Ben Zaken et al. (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>, which only fine-tunes the bias terms. BitFit has inspired a series of studies in FedPEFT <cite class="ltx_cite ltx_citemacro_cite">Bu et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>); Sun et al. (<a href="#bib.bib155" title="" class="ltx_ref">2022a</a>); Zhang et al. (<a href="#bib.bib210" title="" class="ltx_ref">2023f</a>)</cite>,
demonstrating the superior communication efficiency of only updating the bias terms while still achieving competitive performance. More sophisticated methods strive to find sparse subnetworks for partial fine-tuning. Among them, various methods <cite class="ltx_cite ltx_citemacro_cite">Seo et al. (<a href="#bib.bib145" title="" class="ltx_ref">2021</a>); Li et al. (<a href="#bib.bib85" title="" class="ltx_ref">2021a</a>); Tamirisa et al. (<a href="#bib.bib160" title="" class="ltx_ref">2024</a>)</cite> advocate for the Lottery Ticket Hypothesis (LTH) <cite class="ltx_cite ltx_citemacro_cite">Frankle and Carbin (<a href="#bib.bib48" title="" class="ltx_ref">2019</a>)</cite>, positing that a dense network contains many subnetworks whose inference capabilities are as accurate as that of the original network. FedSelect <cite class="ltx_cite ltx_citemacro_cite">Tamirisa et al. (<a href="#bib.bib160" title="" class="ltx_ref">2024</a>)</cite> is a representative method that encourages clients to find optimal subnetworks based on LTH and continually fine-tunes these derived subnetworks to encapsulate local knowledge. As another important aspect, RaFFM <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib198" title="" class="ltx_ref">2023c</a>)</cite> proposes to prioritize specialized salient parameters by ranking them using salience evaluation metrics such as the <math id="S4.SS1.SSS1.Px1.p1.1.m1.1" class="ltx_Math" alttext="\ell_{1}" display="inline"><semantics id="S4.SS1.SSS1.Px1.p1.1.m1.1a"><msub id="S4.SS1.SSS1.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.2" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1.2.cmml">ℓ</mi><mn id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.3" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px1.p1.1.m1.1b"><apply id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1.2">ℓ</ci><cn type="integer" id="S4.SS1.SSS1.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.Px1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px1.p1.1.m1.1c">\ell_{1}</annotation></semantics></math> and <math id="S4.SS1.SSS1.Px1.p1.2.m2.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S4.SS1.SSS1.Px1.p1.2.m2.1a"><msub id="S4.SS1.SSS1.Px1.p1.2.m2.1.1" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1.cmml"><mi mathvariant="normal" id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.2" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1.2.cmml">ℓ</mi><mn id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.3" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px1.p1.2.m2.1b"><apply id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1.2">ℓ</ci><cn type="integer" id="S4.SS1.SSS1.Px1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS1.Px1.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px1.p1.2.m2.1c">\ell_{2}</annotation></semantics></math> norms.</p>
</div>
</section>
<section id="S4.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Additive Methods</h5>

<div id="S4.SS1.SSS1.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS1.Px2.p1.1" class="ltx_p">Instead of fine-tuning a subset of model parameters, additive methods incorporate lightweight trainable blocks into frozen FMs and tune the additional parameters for model adaptation. These methods not only enhance computational and communicational efficiency but also introduce an extra benefit: personalization <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib110" title="" class="ltx_ref">2023a</a>)</cite>, <em id="S4.SS1.SSS1.Px2.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> the integration of these supplementary parameters allows for the customization of heterogeneous models tailored to specific local data characteristics or user preferences. Key branches within additive methods include <span id="S4.SS1.SSS1.Px2.p1.1.2" class="ltx_text ltx_font_italic">adapter tuning</span> and <span id="S4.SS1.SSS1.Px2.p1.1.3" class="ltx_text ltx_font_italic">prompt tuning</span>. Adapter tuning integrates small-scale neural networks (known as “adapters”) into the pre-trained models <cite class="ltx_cite ltx_citemacro_cite">Houlsby et al. (<a href="#bib.bib66" title="" class="ltx_ref">2019</a>); Hu et al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite>. On the other hand, prompt tuning incorporates trainable task-specific continuous prompt vectors at the input layer <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib104" title="" class="ltx_ref">2023a</a>); Dong et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>. More details on these methods are provided in Appendix <a href="#A1" title="Appendix A Additional Details of Adapter Tuning ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section id="S4.SS1.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Reparameterization-based Methods</h5>

<div id="S4.SS1.SSS1.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS1.Px3.p1.13" class="ltx_p">The hypothesis behind reparameterization-based methods is that fine-tuning adaptations can be re-parameterized into optimization within low-rank subspaces <cite class="ltx_cite ltx_citemacro_cite">Aghajanyan et al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite>. Low-Rank Adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite>, as a popular PEFT method from the area of LLMs, reduces the number of trainable parameters for downstream tasks by representing the weight updates with two smaller matrices (called update matrices) through low-rank decomposition <cite class="ltx_cite ltx_citemacro_cite">Ding et al. (<a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>. When optimizing a parameter matrix <math id="S4.SS1.SSS1.Px3.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{W}\in\mathbb{R}^{m\times n}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.1.m1.1a"><mrow id="S4.SS1.SSS1.Px3.p1.1.m1.1.1" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.2" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.2.cmml">𝐖</mi><mo id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.1" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.2" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.2" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.1" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.3" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.3.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.1.m1.1b"><apply id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1"><in id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.1"></in><ci id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.2">𝐖</ci><apply id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3"><times id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.1"></times><ci id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.2">𝑚</ci><ci id="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.1.m1.1.1.3.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.1.m1.1c">\mathbf{W}\in\mathbb{R}^{m\times n}</annotation></semantics></math>, the update equation can be written as: <math id="S4.SS1.SSS1.Px3.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{W}\leftarrow\mathbf{W}+\Delta\mathbf{W}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.2.m2.1a"><mrow id="S4.SS1.SSS1.Px3.p1.2.m2.1.1" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.2" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.2.cmml">𝐖</mi><mo stretchy="false" id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.1" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.1.cmml">←</mo><mrow id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.2" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.2.cmml">𝐖</mi><mo id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.1" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.1.cmml">+</mo><mrow id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.cmml"><mi mathvariant="normal" id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.2" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.1" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.3" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.3.cmml">𝐖</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.2.m2.1b"><apply id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1"><ci id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.1">←</ci><ci id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.2">𝐖</ci><apply id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3"><plus id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.1"></plus><ci id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.2">𝐖</ci><apply id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3"><times id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.1"></times><ci id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.2">Δ</ci><ci id="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.2.m2.1.1.3.3.3">𝐖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.2.m2.1c">\mathbf{W}\leftarrow\mathbf{W}+\Delta\mathbf{W}</annotation></semantics></math>.
The core idea of LoRA is to freeze the original matrix <math id="S4.SS1.SSS1.Px3.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{W}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.3.m3.1a"><mi id="S4.SS1.SSS1.Px3.p1.3.m3.1.1" xref="S4.SS1.SSS1.Px3.p1.3.m3.1.1.cmml">𝐖</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.3.m3.1b"><ci id="S4.SS1.SSS1.Px3.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.3.m3.1.1">𝐖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.3.m3.1c">\mathbf{W}</annotation></semantics></math> while approximating the parameter update <math id="S4.SS1.SSS1.Px3.p1.4.m4.1" class="ltx_Math" alttext="\Delta\mathbf{W}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.4.m4.1a"><mrow id="S4.SS1.SSS1.Px3.p1.4.m4.1.1" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.cmml"><mi mathvariant="normal" id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.2" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.1" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.3" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.3.cmml">𝐖</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.4.m4.1b"><apply id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1"><times id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.1"></times><ci id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.2">Δ</ci><ci id="S4.SS1.SSS1.Px3.p1.4.m4.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.4.m4.1.1.3">𝐖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.4.m4.1c">\Delta\mathbf{W}</annotation></semantics></math> by low-rank decomposition matrices, <em id="S4.SS1.SSS1.Px3.p1.13.1" class="ltx_emph ltx_font_italic">i.e.,</em> <math id="S4.SS1.SSS1.Px3.p1.5.m5.1" class="ltx_Math" alttext="\Delta\mathbf{W}=\mathbf{A}\cdot\mathbf{B}^{\top}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.5.m5.1a"><mrow id="S4.SS1.SSS1.Px3.p1.5.m5.1.1" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.cmml"><mrow id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.cmml"><mi mathvariant="normal" id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.2" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.1" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.1.cmml">​</mo><mi id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.3" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.3.cmml">𝐖</mi></mrow><mo id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.1" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.1.cmml">=</mo><mrow id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.2" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.2.cmml">𝐀</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.1" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.1.cmml">⋅</mo><msup id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.2" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.2.cmml">𝐁</mi><mo id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.3" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.3.cmml">⊤</mo></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.5.m5.1b"><apply id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1"><eq id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.1"></eq><apply id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2"><times id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.1.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.1"></times><ci id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.2.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.2">Δ</ci><ci id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.3.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.2.3">𝐖</ci></apply><apply id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3"><ci id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.1">⋅</ci><ci id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.2">𝐀</ci><apply id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3">superscript</csymbol><ci id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.2">𝐁</ci><csymbol cd="latexml" id="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.5.m5.1.1.3.3.3">top</csymbol></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.5.m5.1c">\Delta\mathbf{W}=\mathbf{A}\cdot\mathbf{B}^{\top}</annotation></semantics></math>, where <math id="S4.SS1.SSS1.Px3.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{A}\in\mathbb{R}^{m\times k}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.6.m6.1a"><mrow id="S4.SS1.SSS1.Px3.p1.6.m6.1.1" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.2" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.2.cmml">𝐀</mi><mo id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.1" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.1.cmml">∈</mo><msup id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.2" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.2" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.1" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.1.cmml">×</mo><mi id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.3" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.6.m6.1b"><apply id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1"><in id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.1"></in><ci id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.2">𝐀</ci><apply id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.2">ℝ</ci><apply id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3"><times id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.1"></times><ci id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.2">𝑚</ci><ci id="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.6.m6.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.6.m6.1c">\mathbf{A}\in\mathbb{R}^{m\times k}</annotation></semantics></math> and <math id="S4.SS1.SSS1.Px3.p1.7.m7.1" class="ltx_Math" alttext="\mathbf{B}\in\mathbb{R}^{n\times k}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.7.m7.1a"><mrow id="S4.SS1.SSS1.Px3.p1.7.m7.1.1" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2.cmml">𝐁</mi><mo id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.1" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.1.cmml">∈</mo><msup id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.2" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.cmml"><mi id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.2" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.1" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.1.cmml">×</mo><mi id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.3" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.7.m7.1b"><apply id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1"><in id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.1"></in><ci id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.2">𝐁</ci><apply id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.2">ℝ</ci><apply id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3"><times id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.1"></times><ci id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.2.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.2">𝑛</ci><ci id="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.7.m7.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.7.m7.1c">\mathbf{B}\in\mathbb{R}^{n\times k}</annotation></semantics></math> are the trainable parameters for task adaptation and <math id="S4.SS1.SSS1.Px3.p1.8.m8.3" class="ltx_Math" alttext="k\ll\min(m,n)" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.8.m8.3a"><mrow id="S4.SS1.SSS1.Px3.p1.8.m8.3.4" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.cmml"><mi id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.2" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.2.cmml">k</mi><mo id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.1" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.1.cmml">≪</mo><mrow id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.2" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.8.m8.1.1" xref="S4.SS1.SSS1.Px3.p1.8.m8.1.1.cmml">min</mi><mo id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.2a" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.1.cmml">⁡</mo><mrow id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.2.1" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.1.cmml"><mo stretchy="false" id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.2.1.1" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.1.cmml">(</mo><mi id="S4.SS1.SSS1.Px3.p1.8.m8.2.2" xref="S4.SS1.SSS1.Px3.p1.8.m8.2.2.cmml">m</mi><mo id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.2.1.2" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.1.cmml">,</mo><mi id="S4.SS1.SSS1.Px3.p1.8.m8.3.3" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.3.cmml">n</mi><mo stretchy="false" id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.2.1.3" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.8.m8.3b"><apply id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4"><csymbol cd="latexml" id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.1.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.1">much-less-than</csymbol><ci id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.2.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.2">𝑘</ci><apply id="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.1.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.4.3.2"><min id="S4.SS1.SSS1.Px3.p1.8.m8.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.1.1"></min><ci id="S4.SS1.SSS1.Px3.p1.8.m8.2.2.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.2.2">𝑚</ci><ci id="S4.SS1.SSS1.Px3.p1.8.m8.3.3.cmml" xref="S4.SS1.SSS1.Px3.p1.8.m8.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.8.m8.3c">k\ll\min(m,n)</annotation></semantics></math> is the reduced rank. The trainable parameter size is then reduced from <math id="S4.SS1.SSS1.Px3.p1.9.m9.1" class="ltx_Math" alttext="mn" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.9.m9.1a"><mrow id="S4.SS1.SSS1.Px3.p1.9.m9.1.1" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.1" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.3" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.9.m9.1b"><apply id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1"><times id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.1"></times><ci id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.2">𝑚</ci><ci id="S4.SS1.SSS1.Px3.p1.9.m9.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.9.m9.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.9.m9.1c">mn</annotation></semantics></math> to <math id="S4.SS1.SSS1.Px3.p1.10.m10.1" class="ltx_Math" alttext="k(m+n)" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.10.m10.1a"><mrow id="S4.SS1.SSS1.Px3.p1.10.m10.1.1" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.3" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.3.cmml">k</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.2" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.2.cmml">​</mo><mrow id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.2" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.cmml"><mi id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.2" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.2.cmml">m</mi><mo id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.1" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.1.cmml">+</mo><mi id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.3" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.3.cmml">n</mi></mrow><mo stretchy="false" id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.3" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.10.m10.1b"><apply id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1"><times id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.2"></times><ci id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.3">𝑘</ci><apply id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1"><plus id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.1"></plus><ci id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.2.cmml" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.2">𝑚</ci><ci id="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.3.cmml" xref="S4.SS1.SSS1.Px3.p1.10.m10.1.1.1.1.1.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.10.m10.1c">k(m+n)</annotation></semantics></math>. The major benefit of LoRA is that it can largely save memory and storage usage. A straightforward way to perform federated finetuning with LoRA is to train the LoRA modules <math id="S4.SS1.SSS1.Px3.p1.11.m11.1" class="ltx_Math" alttext="\mathbf{A}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.11.m11.1a"><mi id="S4.SS1.SSS1.Px3.p1.11.m11.1.1" xref="S4.SS1.SSS1.Px3.p1.11.m11.1.1.cmml">𝐀</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.11.m11.1b"><ci id="S4.SS1.SSS1.Px3.p1.11.m11.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.11.m11.1.1">𝐀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.11.m11.1c">\mathbf{A}</annotation></semantics></math> and <math id="S4.SS1.SSS1.Px3.p1.12.m12.1" class="ltx_Math" alttext="\mathbf{B}" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.12.m12.1a"><mi id="S4.SS1.SSS1.Px3.p1.12.m12.1.1" xref="S4.SS1.SSS1.Px3.p1.12.m12.1.1.cmml">𝐁</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.12.m12.1b"><ci id="S4.SS1.SSS1.Px3.p1.12.m12.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.12.m12.1.1">𝐁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.12.m12.1c">\mathbf{B}</annotation></semantics></math> with homogeneous rank <math id="S4.SS1.SSS1.Px3.p1.13.m13.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS1.SSS1.Px3.p1.13.m13.1a"><mi id="S4.SS1.SSS1.Px3.p1.13.m13.1.1" xref="S4.SS1.SSS1.Px3.p1.13.m13.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.Px3.p1.13.m13.1b"><ci id="S4.SS1.SSS1.Px3.p1.13.m13.1.1.cmml" xref="S4.SS1.SSS1.Px3.p1.13.m13.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.Px3.p1.13.m13.1c">k</annotation></semantics></math> across all clients with standard FL such as <span id="S4.SS1.SSS1.Px3.p1.13.2" class="ltx_text ltx_font_typewriter">FedAvg</span> <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib119" title="" class="ltx_ref">2017</a>)</cite>. Serval studies have shown that this method can achieve an outstanding level of trade-off between performance and communication overhead for a wide range of FMs, including language models <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib204" title="" class="ltx_ref">2024b</a>, <a href="#bib.bib210" title="" class="ltx_ref">2023f</a>)</cite>, vision-language models <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib121" title="" class="ltx_ref">2024</a>)</cite>, and speech-to-text models <cite class="ltx_cite ltx_citemacro_cite">Du et al. (<a href="#bib.bib39" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2406.12844/assets/figs/fedpeft.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="475" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Taxonomy of Federated Parameter-Efficient Fine-Tuning (FedPEFT). Apart from efficiency, some methods also account for other considerations, such as data and resource heterogeneity challenges that are identified in Section <a href="#S3.SS2.SSS0.Px2" title="Adaptability Challenges ‣ 3.2 Core Challenges ‣ 3 FM-FL: Motivation &amp; Challenges ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> and black-box tuning (see Section <a href="#S4.SS3" title="4.3 Trustworthiness ‣ 4 Techniques ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>).</figcaption>
</figure>
</section>
<section id="S4.SS1.SSS1.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Comparison of FedPEFT methods</h5>

<div id="S4.SS1.SSS1.Px4.p1" class="ltx_para">
<p id="S4.SS1.SSS1.Px4.p1.1" class="ltx_p">Figure <a href="#S4.F2" title="Figure 2 ‣ Reparameterization-based Methods ‣ 4.1.1 Parameter-Efficient Fine-Tuning ‣ 4.1 Efficiency ‣ 4 Techniques ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> depicts the taxonomy of FedPEFT with representative methods. Note that some methods may belong to multiple overlapping categories. To compare the communication efficiency of different FedPEFT methods, Table <a href="#S4.T1" title="Table 1 ‣ 4.1.2 Model Compression ‣ 4.1 Efficiency ‣ 4 Techniques ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> gives a brief overview of experimental evaluations from representative studies. Compared to full-model fine-tuning, FedPEFT methods only require 0.1%-30% communication overhead. We note that the differences can be attributed to several factors, including model complexity and implementation details.</p>
</div>
</section>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Model Compression</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">Model compression refers to the techniques used to reduce the size of models, thereby improving resource efficiency <cite class="ltx_cite ltx_citemacro_cite">Shah and Lau (<a href="#bib.bib146" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of Federated Parameter-Efficient Fine-Tuning (FedPEFT) Methods.</figcaption>
<p id="S4.T1.6" class="ltx_p ltx_align_center"><span id="S4.T1.6.6" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S4.T1.6.6.6.6" class="ltx_inline-block ltx_transformed_outer" style="width:1057.9pt;height:348.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S4.T1.6.6.6.6.6" class="ltx_p"><span id="S4.T1.6.6.6.6.6.6" class="ltx_text">
<span id="S4.T1.6.6.6.6.6.6.6" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.6.6.6.6.6.6.6.7" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.1.1" class="ltx_text"></span> <span id="S4.T1.6.6.6.6.6.6.6.7.1.2" class="ltx_text">
<span id="S4.T1.6.6.6.6.6.6.6.7.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.6.6.6.6.6.6.6.7.1.2.1.1" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Category</span></span></span>
</span></span><span id="S4.T1.6.6.6.6.6.6.6.7.1.3" class="ltx_text"></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.2.1" class="ltx_text ltx_font_bold">Representative Work</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.3.1" class="ltx_text ltx_font_bold">Modality</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.4.1" class="ltx_text ltx_font_bold">Model</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.5.1" class="ltx_text"></span> <span id="S4.T1.6.6.6.6.6.6.6.7.5.2" class="ltx_text">
<span id="S4.T1.6.6.6.6.6.6.6.7.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.6.6.6.6.6.6.6.7.5.2.1.1" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.5.2.1.1.1.1" class="ltx_text ltx_font_bold"># Full</span></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.5.2.1.2" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.5.2.1.2.1.1" class="ltx_text ltx_font_bold">Params.</span></span></span>
</span></span><span id="S4.T1.6.6.6.6.6.6.6.7.5.3" class="ltx_text"></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.6.1" class="ltx_text"></span> <span id="S4.T1.6.6.6.6.6.6.6.7.6.2" class="ltx_text">
<span id="S4.T1.6.6.6.6.6.6.6.7.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.6.6.6.6.6.6.6.7.6.2.1.1" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.6.2.1.1.1.1" class="ltx_text ltx_font_bold"># Train.</span></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.6.2.1.2" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.6.2.1.2.1.1" class="ltx_text ltx_font_bold">Params.</span></span></span>
</span></span><span id="S4.T1.6.6.6.6.6.6.6.7.6.3" class="ltx_text"></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.7.1" class="ltx_text"></span> <span id="S4.T1.6.6.6.6.6.6.6.7.7.2" class="ltx_text">
<span id="S4.T1.6.6.6.6.6.6.6.7.7.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.6.6.6.6.6.6.6.7.7.2.1.1" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.7.2.1.1.1.1" class="ltx_text ltx_font_bold">Training</span></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.7.2.1.2" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.7.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.7.2.1.2.1.1" class="ltx_text ltx_font_bold">Accel.</span></span></span>
</span></span><span id="S4.T1.6.6.6.6.6.6.6.7.7.3" class="ltx_text"></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.8.1" class="ltx_text"></span> <span id="S4.T1.6.6.6.6.6.6.6.7.8.2" class="ltx_text">
<span id="S4.T1.6.6.6.6.6.6.6.7.8.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.6.6.6.6.6.6.6.7.8.2.1.1" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.8.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.8.2.1.1.1.1" class="ltx_text ltx_font_bold">Comm.</span></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.7.8.2.1.2" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.7.8.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.7.8.2.1.2.1.1" class="ltx_text ltx_font_bold">Cost</span></span></span>
</span></span><span id="S4.T1.6.6.6.6.6.6.6.7.8.3" class="ltx_text"></span></span></span>
<span id="S4.T1.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2 ltx_colspan ltx_colspan_2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.1.1.1.1.1.1.1.1.2.1" class="ltx_text"><span id="S4.T1.1.1.1.1.1.1.1.1.2.1.1" class="ltx_text"></span> <span id="S4.T1.1.1.1.1.1.1.1.1.2.1.2" class="ltx_text">
<span id="S4.T1.1.1.1.1.1.1.1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.1.1.1.1.1.1.1.1.2.1.2.1.1" class="ltx_tr">
<span id="S4.T1.1.1.1.1.1.1.1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.1.1.1.1.1.1.1.1.2.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Selective</span></span></span>
</span></span> <span id="S4.T1.1.1.1.1.1.1.1.1.2.1.3" class="ltx_text"></span></span></span>
<span id="S4.T1.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">RaFFM <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib198" title="" class="ltx_ref">2023c</a>)</cite></span>
<span id="S4.T1.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BERT-Large <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite></span>
<span id="S4.T1.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">336M</span>
<span id="S4.T1.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">100M</span>
<span id="S4.T1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_math_unparsed" alttext="6.13\times" display="inline"><semantics id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1a"><mrow id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1b"><mn id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1.1">6.13</mn><mo lspace="0.222em" id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1c">6.13\times</annotation></semantics></math></span>
<span id="S4.T1.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">29.8%</span></span>
<span id="S4.T1.2.2.2.2.2.2.2.2" class="ltx_tr">
<span id="S4.T1.2.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedBF <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib210" title="" class="ltx_ref">2023f</a>)</cite></span>
<span id="S4.T1.2.2.2.2.2.2.2.2.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.2.2.2.2.2.2.2.2.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Roberta-Base <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib109" title="" class="ltx_ref">2019</a>)</cite></span>
<span id="S4.T1.2.2.2.2.2.2.2.2.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">125M</span>
<span id="S4.T1.2.2.2.2.2.2.2.2.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.66M</span>
<span id="S4.T1.2.2.2.2.2.2.2.2.7" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.2.2.2.2.2.2.2.2.1" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="1.6\%" display="inline"><semantics id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1a"><mrow id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1" xref="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.cmml"><mn id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.2" xref="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.2.cmml">1.6</mn><mo id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.1" xref="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1b"><apply id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.1.cmml" xref="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.2.cmml" xref="S4.T1.2.2.2.2.2.2.2.2.1.m1.1.1.2">1.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.2.2.2.2.1.m1.1c">1.6\%</annotation></semantics></math></span></span>
<span id="S4.T1.3.3.3.3.3.3.3.3" class="ltx_tr">
<span id="S4.T1.3.3.3.3.3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_10" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.3.3.3.3.3.3.3.3.2.1" class="ltx_text"><span id="S4.T1.3.3.3.3.3.3.3.3.2.1.1" class="ltx_text"></span> <span id="S4.T1.3.3.3.3.3.3.3.3.2.1.2" class="ltx_text">
<span id="S4.T1.3.3.3.3.3.3.3.3.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.3.3.3.3.3.3.3.3.2.1.2.1.1" class="ltx_tr">
<span id="S4.T1.3.3.3.3.3.3.3.3.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.3.3.3.3.3.3.3.3.2.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Additive</span></span></span>
</span></span> <span id="S4.T1.3.3.3.3.3.3.3.3.2.1.3" class="ltx_text"></span></span></span>
<span id="S4.T1.3.3.3.3.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_6" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.3.3.3.3.3.3.3.3.3.1" class="ltx_text"><span id="S4.T1.3.3.3.3.3.3.3.3.3.1.1" class="ltx_text"></span> <span id="S4.T1.3.3.3.3.3.3.3.3.3.1.2" class="ltx_text">
<span id="S4.T1.3.3.3.3.3.3.3.3.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.3.3.3.3.3.3.3.3.3.1.2.1.1" class="ltx_tr">
<span id="S4.T1.3.3.3.3.3.3.3.3.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.3.3.3.3.3.3.3.3.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Adapter</span></span></span>
</span></span> <span id="S4.T1.3.3.3.3.3.3.3.3.3.1.3" class="ltx_text"></span></span></span>
<span id="S4.T1.3.3.3.3.3.3.3.3.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedAP <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib210" title="" class="ltx_ref">2023f</a>)</cite></span>
<span id="S4.T1.3.3.3.3.3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.3.3.3.3.3.3.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Roberta-Base <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib109" title="" class="ltx_ref">2019</a>)</cite></span>
<span id="S4.T1.3.3.3.3.3.3.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">125M</span>
<span id="S4.T1.3.3.3.3.3.3.3.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2M</span>
<span id="S4.T1.3.3.3.3.3.3.3.3.9" class="ltx_td ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.3.3.3.3.3.3.3.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1" class="ltx_Math" alttext="1.6\%" display="inline"><semantics id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1a"><mrow id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1" xref="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.cmml"><mn id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.2" xref="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.2.cmml">1.6</mn><mo id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.1" xref="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1b"><apply id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.1.cmml" xref="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.2.cmml" xref="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.2">1.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1c">1.6\%</annotation></semantics></math></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.8" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.8.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedCLIP <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib110" title="" class="ltx_ref">2023a</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.8.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Vis.-Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.8.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">ViT-B/32 <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib176" title="" class="ltx_ref">2020a</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.8.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">150M</span>
<span id="S4.T1.6.6.6.6.6.6.6.8.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.53M</span>
<span id="S4.T1.6.6.6.6.6.6.6.8.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.8.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.5%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.9" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.9.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedDAT <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib27" title="" class="ltx_ref">2024</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.9.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Vis.-Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.9.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">ALBEF <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib87" title="" class="ltx_ref">2021b</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.9.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">290M</span>
<span id="S4.T1.6.6.6.6.6.6.6.9.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.86M</span>
<span id="S4.T1.6.6.6.6.6.6.6.9.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.9.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">9.9%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.10" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.10.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">C2A <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib81" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.10.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.10.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">DistilBERT <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib144" title="" class="ltx_ref">2020</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.10.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">66M</span>
<span id="S4.T1.6.6.6.6.6.6.6.10.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.06M</span>
<span id="S4.T1.6.6.6.6.6.6.6.10.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.10.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.1%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.11" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.11.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.11.1.1" class="ltx_text ltx_font_italic">Fed-MNMT</span> <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib108" title="" class="ltx_ref">2023d</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.11.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.11.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">mBART-50 <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib162" title="" class="ltx_ref">2020</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.11.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">611M</span>
<span id="S4.T1.6.6.6.6.6.6.6.11.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">8M</span>
<span id="S4.T1.6.6.6.6.6.6.6.11.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.11.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.3%</span></span>
<span id="S4.T1.4.4.4.4.4.4.4.4" class="ltx_tr">
<span id="S4.T1.4.4.4.4.4.4.4.4.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">AdaFL <cite class="ltx_cite ltx_citemacro_cite">Cai et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S4.T1.4.4.4.4.4.4.4.4.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.4.4.4.4.4.4.4.4.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">BERT <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite></span>
<span id="S4.T1.4.4.4.4.4.4.4.4.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">110M</span>
<span id="S4.T1.4.4.4.4.4.4.4.4.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.61M</span>
<span id="S4.T1.4.4.4.4.4.4.4.4.1" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math id="S4.T1.4.4.4.4.4.4.4.4.1.m1.1" class="ltx_math_unparsed" alttext="1.63\times" display="inline"><semantics id="S4.T1.4.4.4.4.4.4.4.4.1.m1.1a"><mrow id="S4.T1.4.4.4.4.4.4.4.4.1.m1.1b"><mn id="S4.T1.4.4.4.4.4.4.4.4.1.m1.1.1">1.63</mn><mo lspace="0.222em" id="S4.T1.4.4.4.4.4.4.4.4.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S4.T1.4.4.4.4.4.4.4.4.1.m1.1c">1.63\times</annotation></semantics></math></span>
<span id="S4.T1.4.4.4.4.4.4.4.4.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.6%</span></span>
<span id="S4.T1.5.5.5.5.5.5.5.5" class="ltx_tr">
<span id="S4.T1.5.5.5.5.5.5.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.5.5.5.5.5.5.5.5.2.1" class="ltx_text"><span id="S4.T1.5.5.5.5.5.5.5.5.2.1.1" class="ltx_text"></span> <span id="S4.T1.5.5.5.5.5.5.5.5.2.1.2" class="ltx_text">
<span id="S4.T1.5.5.5.5.5.5.5.5.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.5.5.5.5.5.5.5.5.2.1.2.1.1" class="ltx_tr">
<span id="S4.T1.5.5.5.5.5.5.5.5.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.5.5.5.5.5.5.5.5.2.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Prompt</span></span></span>
</span></span> <span id="S4.T1.5.5.5.5.5.5.5.5.2.1.3" class="ltx_text"></span></span></span>
<span id="S4.T1.5.5.5.5.5.5.5.5.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">PromptFL <cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a href="#bib.bib58" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S4.T1.5.5.5.5.5.5.5.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Vis.-Txt.</span>
<span id="S4.T1.5.5.5.5.5.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">ViT-B/16 <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite></span>
<span id="S4.T1.5.5.5.5.5.5.5.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">87M</span>
<span id="S4.T1.5.5.5.5.5.5.5.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.87M</span>
<span id="S4.T1.5.5.5.5.5.5.5.5.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1" class="ltx_math_unparsed" alttext="2.38\times" display="inline"><semantics id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1a"><mrow id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1b"><mn id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1.1">2.38</mn><mo lspace="0.222em" id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1c">2.38\times</annotation></semantics></math></span>
<span id="S4.T1.5.5.5.5.5.5.5.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.9%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.12" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.12.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">MFPT <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib212" title="" class="ltx_ref">2024b</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.12.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.12.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">XLM-RoBERTa <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.12.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">270M</span>
<span id="S4.T1.6.6.6.6.6.6.6.12.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.2M</span>
<span id="S4.T1.6.6.6.6.6.6.6.12.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.12.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.4%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.13" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.13.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedAPT <cite class="ltx_cite ltx_citemacro_cite">Su et al. (<a href="#bib.bib153" title="" class="ltx_ref">2024</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.13.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Vis.-Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.13.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">ViT-B/32 <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib176" title="" class="ltx_ref">2020a</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.13.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">88M</span>
<span id="S4.T1.6.6.6.6.6.6.6.13.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.8M</span>
<span id="S4.T1.6.6.6.6.6.6.6.13.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.13.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.2%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.14" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.14.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.14.1.1" class="ltx_text ltx_font_italic">FedSP</span> <cite class="ltx_cite ltx_citemacro_cite">Dong et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.14.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.14.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">GPT2-XL <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib133" title="" class="ltx_ref">2019</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.14.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.6B</span>
<span id="S4.T1.6.6.6.6.6.6.6.14.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">111M</span>
<span id="S4.T1.6.6.6.6.6.6.6.14.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.14.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.5%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.6" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_5 ltx_colspan ltx_colspan_2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.6.2.1" class="ltx_text"><span id="S4.T1.6.6.6.6.6.6.6.6.2.1.1" class="ltx_text"></span> <span id="S4.T1.6.6.6.6.6.6.6.6.2.1.2" class="ltx_text">
<span id="S4.T1.6.6.6.6.6.6.6.6.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.6.6.6.6.6.6.6.6.2.1.2.1.1" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.6.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.6.2.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Reparameterization-based</span></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.6.2.1.2.1.2" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.6.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S4.T1.6.6.6.6.6.6.6.6.2.1.2.1.2.1.1" class="ltx_text ltx_font_bold">Methods</span></span></span>
</span></span> <span id="S4.T1.6.6.6.6.6.6.6.6.2.1.3" class="ltx_text"></span></span></span>
<span id="S4.T1.6.6.6.6.6.6.6.6.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">SLoRA <cite class="ltx_cite ltx_citemacro_cite">Babakniya et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023a</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">DistilBERT <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib144" title="" class="ltx_ref">2020</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">67M</span>
<span id="S4.T1.6.6.6.6.6.6.6.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.7M</span>
<span id="S4.T1.6.6.6.6.6.6.6.6.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math id="S4.T1.6.6.6.6.6.6.6.6.1.m1.1" class="ltx_math_unparsed" alttext="13.47\times" display="inline"><semantics id="S4.T1.6.6.6.6.6.6.6.6.1.m1.1a"><mrow id="S4.T1.6.6.6.6.6.6.6.6.1.m1.1b"><mn id="S4.T1.6.6.6.6.6.6.6.6.1.m1.1.1">13.47</mn><mo lspace="0.222em" id="S4.T1.6.6.6.6.6.6.6.6.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S4.T1.6.6.6.6.6.6.6.6.1.m1.1c">13.47\times</annotation></semantics></math></span>
<span id="S4.T1.6.6.6.6.6.6.6.6.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.8%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.15" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.15.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">LP-FL <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a href="#bib.bib73" title="" class="ltx_ref">2023a</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.15.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.15.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">BERT-Large <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.15.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">336M</span>
<span id="S4.T1.6.6.6.6.6.6.6.15.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">100M</span>
<span id="S4.T1.6.6.6.6.6.6.6.15.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.15.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">30%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.16" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.16.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedMS <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib179" title="" class="ltx_ref">2023c</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.16.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Vis.-Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.16.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">ViT-B/16 <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.16.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">87M</span>
<span id="S4.T1.6.6.6.6.6.6.6.16.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.6M</span>
<span id="S4.T1.6.6.6.6.6.6.6.16.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.16.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">10%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.17" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.17.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">pFedS2T <cite class="ltx_cite ltx_citemacro_cite">Du et al. (<a href="#bib.bib39" title="" class="ltx_ref">2024</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.17.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Aud.</span>
<span id="S4.T1.6.6.6.6.6.6.6.17.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">Whisper <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib132" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.17.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">254M</span>
<span id="S4.T1.6.6.6.6.6.6.6.17.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">10.1M</span>
<span id="S4.T1.6.6.6.6.6.6.6.17.6" class="ltx_td" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.17.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">4%</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.18" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.18.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">FFA-LoRA <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib159" title="" class="ltx_ref">2024b</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.18.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">Txt.</span>
<span id="S4.T1.6.6.6.6.6.6.6.18.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">RoBERTa-Large <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib109" title="" class="ltx_ref">2019</a>)</cite></span>
<span id="S4.T1.6.6.6.6.6.6.6.18.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">355M</span>
<span id="S4.T1.6.6.6.6.6.6.6.18.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.39M</span>
<span id="S4.T1.6.6.6.6.6.6.6.18.6" class="ltx_td ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;"></span>
<span id="S4.T1.6.6.6.6.6.6.6.18.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.1%</span></span>
</span></span></span>
</span></span></span></p>
</figure>
<section id="S4.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Sparsification</h5>

<div id="S4.SS1.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS2.Px1.p1.1" class="ltx_p">Model sparsification methods reduce communication burden by only transmitting a subset of FM parameters across the network <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a href="#bib.bib75" title="" class="ltx_ref">2023c</a>)</cite>. Typical methods focus on identifying and cultivating high-potential subnetworks <cite class="ltx_cite ltx_citemacro_cite">Frankle and Carbin (<a href="#bib.bib48" title="" class="ltx_ref">2019</a>); Tsouvalas et al. (<a href="#bib.bib168" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS1.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Quantization</h5>

<div id="S4.SS1.SSS2.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.Px2.p1.1" class="ltx_p">Quantization is well-established in both the FM and FL domains <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib186" title="" class="ltx_ref">2024b</a>); Reisizadeh et al. (<a href="#bib.bib138" title="" class="ltx_ref">2020</a>)</cite>, which involves decreasing the precision of floating-point parameters for mitigating the storage, computational, and communication demands. Quantization is orthogonal to other resource-efficient techniques, making it feasible to combine them for greater efficiency and flexibility <cite class="ltx_cite ltx_citemacro_cite">Lit et al. (<a href="#bib.bib103" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Zeroth-Order Optimization</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">In contrast to the use of gradient descent in most FL optimization algorithms, a particular line of research advocates for the removal of BackPropagation (BP) <cite class="ltx_cite ltx_citemacro_cite">Malladi et al. (<a href="#bib.bib114" title="" class="ltx_ref">2023a</a>)</cite> in favor of Zeroth-Order Optimization (ZOO) <cite class="ltx_cite ltx_citemacro_cite">Fang et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>); Li and Chen (<a href="#bib.bib98" title="" class="ltx_ref">2021</a>)</cite>. BP-free methods conserve memory needed for computing gradients and minimize communication overhead for model aggregation <cite class="ltx_cite ltx_citemacro_cite">Qin et al. (<a href="#bib.bib129" title="" class="ltx_ref">2024</a>)</cite>, making FMs more accessible for lower-end devices, thereby enhancing their applicability in diverse hardware environments.</p>
</div>
<div id="S4.SS1.SSS3.p2" class="ltx_para">
<p id="S4.SS1.SSS3.p2.3" class="ltx_p">ZOO methods primarily rely on perturbation methods to estimate gradients with forward propagation. Given a model with parameters <math id="S4.SS1.SSS3.p2.1.m1.1" class="ltx_Math" alttext="\bm{\theta}\in\mathbb{R}^{d}" display="inline"><semantics id="S4.SS1.SSS3.p2.1.m1.1a"><mrow id="S4.SS1.SSS3.p2.1.m1.1.1" xref="S4.SS1.SSS3.p2.1.m1.1.1.cmml"><mi id="S4.SS1.SSS3.p2.1.m1.1.1.2" xref="S4.SS1.SSS3.p2.1.m1.1.1.2.cmml">𝜽</mi><mo id="S4.SS1.SSS3.p2.1.m1.1.1.1" xref="S4.SS1.SSS3.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S4.SS1.SSS3.p2.1.m1.1.1.3" xref="S4.SS1.SSS3.p2.1.m1.1.1.3.cmml"><mi id="S4.SS1.SSS3.p2.1.m1.1.1.3.2" xref="S4.SS1.SSS3.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mi id="S4.SS1.SSS3.p2.1.m1.1.1.3.3" xref="S4.SS1.SSS3.p2.1.m1.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.1.m1.1b"><apply id="S4.SS1.SSS3.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p2.1.m1.1.1"><in id="S4.SS1.SSS3.p2.1.m1.1.1.1.cmml" xref="S4.SS1.SSS3.p2.1.m1.1.1.1"></in><ci id="S4.SS1.SSS3.p2.1.m1.1.1.2.cmml" xref="S4.SS1.SSS3.p2.1.m1.1.1.2">𝜽</ci><apply id="S4.SS1.SSS3.p2.1.m1.1.1.3.cmml" xref="S4.SS1.SSS3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS3.p2.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS3.p2.1.m1.1.1.3.2.cmml" xref="S4.SS1.SSS3.p2.1.m1.1.1.3.2">ℝ</ci><ci id="S4.SS1.SSS3.p2.1.m1.1.1.3.3.cmml" xref="S4.SS1.SSS3.p2.1.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.1.m1.1c">\bm{\theta}\in\mathbb{R}^{d}</annotation></semantics></math> and a loss function <math id="S4.SS1.SSS3.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S4.SS1.SSS3.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS3.p2.2.m2.1.1" xref="S4.SS1.SSS3.p2.2.m2.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.2.m2.1b"><ci id="S4.SS1.SSS3.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS3.p2.2.m2.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.2.m2.1c">\mathcal{L}</annotation></semantics></math>, a typical gradient estimator estimates the gradient on a minibatch <math id="S4.SS1.SSS3.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{B}" display="inline"><semantics id="S4.SS1.SSS3.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS3.p2.3.m3.1.1" xref="S4.SS1.SSS3.p2.3.m3.1.1.cmml">ℬ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.3.m3.1b"><ci id="S4.SS1.SSS3.p2.3.m3.1.1.cmml" xref="S4.SS1.SSS3.p2.3.m3.1.1">ℬ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.3.m3.1c">\mathcal{B}</annotation></semantics></math> as</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.7" class="ltx_Math" alttext="\hat{\nabla}\mathcal{L}(\bm{\theta};\mathcal{B})=\frac{\mathcal{L}(\bm{\theta}+\epsilon\bm{z};\mathcal{B})-\mathcal{L}(\bm{\theta};\mathcal{B})}{2\epsilon}\bm{z}," display="block"><semantics id="S4.E2.m1.7a"><mrow id="S4.E2.m1.7.7.1" xref="S4.E2.m1.7.7.1.1.cmml"><mrow id="S4.E2.m1.7.7.1.1" xref="S4.E2.m1.7.7.1.1.cmml"><mrow id="S4.E2.m1.7.7.1.1.2" xref="S4.E2.m1.7.7.1.1.2.cmml"><mover accent="true" id="S4.E2.m1.7.7.1.1.2.2" xref="S4.E2.m1.7.7.1.1.2.2.cmml"><mo id="S4.E2.m1.7.7.1.1.2.2.2" xref="S4.E2.m1.7.7.1.1.2.2.2.cmml">∇</mo><mo id="S4.E2.m1.7.7.1.1.2.2.1" xref="S4.E2.m1.7.7.1.1.2.2.1.cmml">^</mo></mover><mo lspace="0.167em" rspace="0em" id="S4.E2.m1.7.7.1.1.2.1" xref="S4.E2.m1.7.7.1.1.2.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.7.7.1.1.2.3" xref="S4.E2.m1.7.7.1.1.2.3.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.7.7.1.1.2.1a" xref="S4.E2.m1.7.7.1.1.2.1.cmml">​</mo><mrow id="S4.E2.m1.7.7.1.1.2.4.2" xref="S4.E2.m1.7.7.1.1.2.4.1.cmml"><mo stretchy="false" id="S4.E2.m1.7.7.1.1.2.4.2.1" xref="S4.E2.m1.7.7.1.1.2.4.1.cmml">(</mo><mi id="S4.E2.m1.5.5" xref="S4.E2.m1.5.5.cmml">𝜽</mi><mo id="S4.E2.m1.7.7.1.1.2.4.2.2" xref="S4.E2.m1.7.7.1.1.2.4.1.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.6.6" xref="S4.E2.m1.6.6.cmml">ℬ</mi><mo stretchy="false" id="S4.E2.m1.7.7.1.1.2.4.2.3" xref="S4.E2.m1.7.7.1.1.2.4.1.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.7.7.1.1.1" xref="S4.E2.m1.7.7.1.1.1.cmml">=</mo><mrow id="S4.E2.m1.7.7.1.1.3" xref="S4.E2.m1.7.7.1.1.3.cmml"><mfrac id="S4.E2.m1.4.4" xref="S4.E2.m1.4.4.cmml"><mrow id="S4.E2.m1.4.4.4" xref="S4.E2.m1.4.4.4.cmml"><mrow id="S4.E2.m1.4.4.4.4" xref="S4.E2.m1.4.4.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.4.4.4.4.3" xref="S4.E2.m1.4.4.4.4.3.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.4.4.4.4.2" xref="S4.E2.m1.4.4.4.4.2.cmml">​</mo><mrow id="S4.E2.m1.4.4.4.4.1.1" xref="S4.E2.m1.4.4.4.4.1.2.cmml"><mo stretchy="false" id="S4.E2.m1.4.4.4.4.1.1.2" xref="S4.E2.m1.4.4.4.4.1.2.cmml">(</mo><mrow id="S4.E2.m1.4.4.4.4.1.1.1" xref="S4.E2.m1.4.4.4.4.1.1.1.cmml"><mi id="S4.E2.m1.4.4.4.4.1.1.1.2" xref="S4.E2.m1.4.4.4.4.1.1.1.2.cmml">𝜽</mi><mo id="S4.E2.m1.4.4.4.4.1.1.1.1" xref="S4.E2.m1.4.4.4.4.1.1.1.1.cmml">+</mo><mrow id="S4.E2.m1.4.4.4.4.1.1.1.3" xref="S4.E2.m1.4.4.4.4.1.1.1.3.cmml"><mi id="S4.E2.m1.4.4.4.4.1.1.1.3.2" xref="S4.E2.m1.4.4.4.4.1.1.1.3.2.cmml">ϵ</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.4.4.4.4.1.1.1.3.1" xref="S4.E2.m1.4.4.4.4.1.1.1.3.1.cmml">​</mo><mi id="S4.E2.m1.4.4.4.4.1.1.1.3.3" xref="S4.E2.m1.4.4.4.4.1.1.1.3.3.cmml">𝒛</mi></mrow></mrow><mo id="S4.E2.m1.4.4.4.4.1.1.3" xref="S4.E2.m1.4.4.4.4.1.2.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml">ℬ</mi><mo stretchy="false" id="S4.E2.m1.4.4.4.4.1.1.4" xref="S4.E2.m1.4.4.4.4.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.4.4.4.5" xref="S4.E2.m1.4.4.4.5.cmml">−</mo><mrow id="S4.E2.m1.4.4.4.6" xref="S4.E2.m1.4.4.4.6.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.4.4.4.6.2" xref="S4.E2.m1.4.4.4.6.2.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.4.4.4.6.1" xref="S4.E2.m1.4.4.4.6.1.cmml">​</mo><mrow id="S4.E2.m1.4.4.4.6.3.2" xref="S4.E2.m1.4.4.4.6.3.1.cmml"><mo stretchy="false" id="S4.E2.m1.4.4.4.6.3.2.1" xref="S4.E2.m1.4.4.4.6.3.1.cmml">(</mo><mi id="S4.E2.m1.2.2.2.2" xref="S4.E2.m1.2.2.2.2.cmml">𝜽</mi><mo id="S4.E2.m1.4.4.4.6.3.2.2" xref="S4.E2.m1.4.4.4.6.3.1.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.3.3.3.3" xref="S4.E2.m1.3.3.3.3.cmml">ℬ</mi><mo stretchy="false" id="S4.E2.m1.4.4.4.6.3.2.3" xref="S4.E2.m1.4.4.4.6.3.1.cmml">)</mo></mrow></mrow></mrow><mrow id="S4.E2.m1.4.4.6" xref="S4.E2.m1.4.4.6.cmml"><mn id="S4.E2.m1.4.4.6.2" xref="S4.E2.m1.4.4.6.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.E2.m1.4.4.6.1" xref="S4.E2.m1.4.4.6.1.cmml">​</mo><mi id="S4.E2.m1.4.4.6.3" xref="S4.E2.m1.4.4.6.3.cmml">ϵ</mi></mrow></mfrac><mo lspace="0em" rspace="0em" id="S4.E2.m1.7.7.1.1.3.1" xref="S4.E2.m1.7.7.1.1.3.1.cmml">​</mo><mi id="S4.E2.m1.7.7.1.1.3.2" xref="S4.E2.m1.7.7.1.1.3.2.cmml">𝒛</mi></mrow></mrow><mo id="S4.E2.m1.7.7.1.2" xref="S4.E2.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.7b"><apply id="S4.E2.m1.7.7.1.1.cmml" xref="S4.E2.m1.7.7.1"><eq id="S4.E2.m1.7.7.1.1.1.cmml" xref="S4.E2.m1.7.7.1.1.1"></eq><apply id="S4.E2.m1.7.7.1.1.2.cmml" xref="S4.E2.m1.7.7.1.1.2"><times id="S4.E2.m1.7.7.1.1.2.1.cmml" xref="S4.E2.m1.7.7.1.1.2.1"></times><apply id="S4.E2.m1.7.7.1.1.2.2.cmml" xref="S4.E2.m1.7.7.1.1.2.2"><ci id="S4.E2.m1.7.7.1.1.2.2.1.cmml" xref="S4.E2.m1.7.7.1.1.2.2.1">^</ci><ci id="S4.E2.m1.7.7.1.1.2.2.2.cmml" xref="S4.E2.m1.7.7.1.1.2.2.2">∇</ci></apply><ci id="S4.E2.m1.7.7.1.1.2.3.cmml" xref="S4.E2.m1.7.7.1.1.2.3">ℒ</ci><list id="S4.E2.m1.7.7.1.1.2.4.1.cmml" xref="S4.E2.m1.7.7.1.1.2.4.2"><ci id="S4.E2.m1.5.5.cmml" xref="S4.E2.m1.5.5">𝜽</ci><ci id="S4.E2.m1.6.6.cmml" xref="S4.E2.m1.6.6">ℬ</ci></list></apply><apply id="S4.E2.m1.7.7.1.1.3.cmml" xref="S4.E2.m1.7.7.1.1.3"><times id="S4.E2.m1.7.7.1.1.3.1.cmml" xref="S4.E2.m1.7.7.1.1.3.1"></times><apply id="S4.E2.m1.4.4.cmml" xref="S4.E2.m1.4.4"><divide id="S4.E2.m1.4.4.5.cmml" xref="S4.E2.m1.4.4"></divide><apply id="S4.E2.m1.4.4.4.cmml" xref="S4.E2.m1.4.4.4"><minus id="S4.E2.m1.4.4.4.5.cmml" xref="S4.E2.m1.4.4.4.5"></minus><apply id="S4.E2.m1.4.4.4.4.cmml" xref="S4.E2.m1.4.4.4.4"><times id="S4.E2.m1.4.4.4.4.2.cmml" xref="S4.E2.m1.4.4.4.4.2"></times><ci id="S4.E2.m1.4.4.4.4.3.cmml" xref="S4.E2.m1.4.4.4.4.3">ℒ</ci><list id="S4.E2.m1.4.4.4.4.1.2.cmml" xref="S4.E2.m1.4.4.4.4.1.1"><apply id="S4.E2.m1.4.4.4.4.1.1.1.cmml" xref="S4.E2.m1.4.4.4.4.1.1.1"><plus id="S4.E2.m1.4.4.4.4.1.1.1.1.cmml" xref="S4.E2.m1.4.4.4.4.1.1.1.1"></plus><ci id="S4.E2.m1.4.4.4.4.1.1.1.2.cmml" xref="S4.E2.m1.4.4.4.4.1.1.1.2">𝜽</ci><apply id="S4.E2.m1.4.4.4.4.1.1.1.3.cmml" xref="S4.E2.m1.4.4.4.4.1.1.1.3"><times id="S4.E2.m1.4.4.4.4.1.1.1.3.1.cmml" xref="S4.E2.m1.4.4.4.4.1.1.1.3.1"></times><ci id="S4.E2.m1.4.4.4.4.1.1.1.3.2.cmml" xref="S4.E2.m1.4.4.4.4.1.1.1.3.2">italic-ϵ</ci><ci id="S4.E2.m1.4.4.4.4.1.1.1.3.3.cmml" xref="S4.E2.m1.4.4.4.4.1.1.1.3.3">𝒛</ci></apply></apply><ci id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1">ℬ</ci></list></apply><apply id="S4.E2.m1.4.4.4.6.cmml" xref="S4.E2.m1.4.4.4.6"><times id="S4.E2.m1.4.4.4.6.1.cmml" xref="S4.E2.m1.4.4.4.6.1"></times><ci id="S4.E2.m1.4.4.4.6.2.cmml" xref="S4.E2.m1.4.4.4.6.2">ℒ</ci><list id="S4.E2.m1.4.4.4.6.3.1.cmml" xref="S4.E2.m1.4.4.4.6.3.2"><ci id="S4.E2.m1.2.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2">𝜽</ci><ci id="S4.E2.m1.3.3.3.3.cmml" xref="S4.E2.m1.3.3.3.3">ℬ</ci></list></apply></apply><apply id="S4.E2.m1.4.4.6.cmml" xref="S4.E2.m1.4.4.6"><times id="S4.E2.m1.4.4.6.1.cmml" xref="S4.E2.m1.4.4.6.1"></times><cn type="integer" id="S4.E2.m1.4.4.6.2.cmml" xref="S4.E2.m1.4.4.6.2">2</cn><ci id="S4.E2.m1.4.4.6.3.cmml" xref="S4.E2.m1.4.4.6.3">italic-ϵ</ci></apply></apply><ci id="S4.E2.m1.7.7.1.1.3.2.cmml" xref="S4.E2.m1.7.7.1.1.3.2">𝒛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.7c">\hat{\nabla}\mathcal{L}(\bm{\theta};\mathcal{B})=\frac{\mathcal{L}(\bm{\theta}+\epsilon\bm{z};\mathcal{B})-\mathcal{L}(\bm{\theta};\mathcal{B})}{2\epsilon}\bm{z},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.SSS3.p2.7" class="ltx_p">where <math id="S4.SS1.SSS3.p2.4.m1.1" class="ltx_Math" alttext="\bm{z}\in\mathbb{R}^{d}" display="inline"><semantics id="S4.SS1.SSS3.p2.4.m1.1a"><mrow id="S4.SS1.SSS3.p2.4.m1.1.1" xref="S4.SS1.SSS3.p2.4.m1.1.1.cmml"><mi id="S4.SS1.SSS3.p2.4.m1.1.1.2" xref="S4.SS1.SSS3.p2.4.m1.1.1.2.cmml">𝒛</mi><mo id="S4.SS1.SSS3.p2.4.m1.1.1.1" xref="S4.SS1.SSS3.p2.4.m1.1.1.1.cmml">∈</mo><msup id="S4.SS1.SSS3.p2.4.m1.1.1.3" xref="S4.SS1.SSS3.p2.4.m1.1.1.3.cmml"><mi id="S4.SS1.SSS3.p2.4.m1.1.1.3.2" xref="S4.SS1.SSS3.p2.4.m1.1.1.3.2.cmml">ℝ</mi><mi id="S4.SS1.SSS3.p2.4.m1.1.1.3.3" xref="S4.SS1.SSS3.p2.4.m1.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.4.m1.1b"><apply id="S4.SS1.SSS3.p2.4.m1.1.1.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1"><in id="S4.SS1.SSS3.p2.4.m1.1.1.1.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1.1"></in><ci id="S4.SS1.SSS3.p2.4.m1.1.1.2.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1.2">𝒛</ci><apply id="S4.SS1.SSS3.p2.4.m1.1.1.3.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.4.m1.1.1.3.1.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS3.p2.4.m1.1.1.3.2.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1.3.2">ℝ</ci><ci id="S4.SS1.SSS3.p2.4.m1.1.1.3.3.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.4.m1.1c">\bm{z}\in\mathbb{R}^{d}</annotation></semantics></math> with <math id="S4.SS1.SSS3.p2.5.m2.2" class="ltx_Math" alttext="\bm{z}\sim\mathcal{N}(0,\bm{I}_{d})" display="inline"><semantics id="S4.SS1.SSS3.p2.5.m2.2a"><mrow id="S4.SS1.SSS3.p2.5.m2.2.2" xref="S4.SS1.SSS3.p2.5.m2.2.2.cmml"><mi id="S4.SS1.SSS3.p2.5.m2.2.2.3" xref="S4.SS1.SSS3.p2.5.m2.2.2.3.cmml">𝒛</mi><mo id="S4.SS1.SSS3.p2.5.m2.2.2.2" xref="S4.SS1.SSS3.p2.5.m2.2.2.2.cmml">∼</mo><mrow id="S4.SS1.SSS3.p2.5.m2.2.2.1" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS3.p2.5.m2.2.2.1.3" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.3.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS3.p2.5.m2.2.2.1.2" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.2.cmml">​</mo><mrow id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.2.cmml"><mo stretchy="false" id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.2" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.2.cmml">(</mo><mn id="S4.SS1.SSS3.p2.5.m2.1.1" xref="S4.SS1.SSS3.p2.5.m2.1.1.cmml">0</mn><mo id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.3" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.2.cmml">,</mo><msub id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.cmml"><mi id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.2" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.2.cmml">𝑰</mi><mi id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.3" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.3.cmml">d</mi></msub><mo stretchy="false" id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.4" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.5.m2.2b"><apply id="S4.SS1.SSS3.p2.5.m2.2.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2"><csymbol cd="latexml" id="S4.SS1.SSS3.p2.5.m2.2.2.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.2">similar-to</csymbol><ci id="S4.SS1.SSS3.p2.5.m2.2.2.3.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.3">𝒛</ci><apply id="S4.SS1.SSS3.p2.5.m2.2.2.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1"><times id="S4.SS1.SSS3.p2.5.m2.2.2.1.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.2"></times><ci id="S4.SS1.SSS3.p2.5.m2.2.2.1.3.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.3">𝒩</ci><interval closure="open" id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1"><cn type="integer" id="S4.SS1.SSS3.p2.5.m2.1.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.1.1">0</cn><apply id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1">subscript</csymbol><ci id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.2">𝑰</ci><ci id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.3.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.3">𝑑</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.5.m2.2c">\bm{z}\sim\mathcal{N}(0,\bm{I}_{d})</annotation></semantics></math> and <math id="S4.SS1.SSS3.p2.6.m3.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S4.SS1.SSS3.p2.6.m3.1a"><mi id="S4.SS1.SSS3.p2.6.m3.1.1" xref="S4.SS1.SSS3.p2.6.m3.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.6.m3.1b"><ci id="S4.SS1.SSS3.p2.6.m3.1.1.cmml" xref="S4.SS1.SSS3.p2.6.m3.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.6.m3.1c">\epsilon</annotation></semantics></math> is the <em id="S4.SS1.SSS3.p2.7.1" class="ltx_emph ltx_font_italic">perturbation scale</em>  <cite class="ltx_cite ltx_citemacro_cite">Duchi et al. (<a href="#bib.bib40" title="" class="ltx_ref">2015</a>)</cite>. It requires only two forward passes through the model to compute the estimation of gradient, serving as a memory-efficient alternative to BP. However, Eq. (<a href="#S4.E2" title="In 4.1.3 Zeroth-Order Optimization ‣ 4.1 Efficiency ‣ 4 Techniques ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) provides a biased gradient estimation, leading to a certain degree of information loss <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib105" title="" class="ltx_ref">2020</a>)</cite>. Alternatively, many studies opt for two-point gradient estimators that can yield a more stable and reliable approximation <cite class="ltx_cite ltx_citemacro_cite">Spall (<a href="#bib.bib151" title="" class="ltx_ref">1992</a>); Malladi et al. (<a href="#bib.bib114" title="" class="ltx_ref">2023a</a>); Lin et al. (<a href="#bib.bib100" title="" class="ltx_ref">2023</a>); Ling et al. (<a href="#bib.bib102" title="" class="ltx_ref">2024</a>)</cite>. The standard two-point gradient estimator estimates the gradient on a minibatch <math id="S4.SS1.SSS3.p2.7.m4.1" class="ltx_Math" alttext="\mathcal{B}" display="inline"><semantics id="S4.SS1.SSS3.p2.7.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS3.p2.7.m4.1.1" xref="S4.SS1.SSS3.p2.7.m4.1.1.cmml">ℬ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.7.m4.1b"><ci id="S4.SS1.SSS3.p2.7.m4.1.1.cmml" xref="S4.SS1.SSS3.p2.7.m4.1.1">ℬ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.7.m4.1c">\mathcal{B}</annotation></semantics></math> as</p>
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.7" class="ltx_Math" alttext="\hat{\nabla}\mathcal{L}(\bm{\theta};\mathcal{B})=\frac{\mathcal{L}(\bm{\theta}+\epsilon\bm{z};\mathcal{B})-\mathcal{L}(\bm{\theta}-\epsilon\bm{z};\mathcal{B})}{2\epsilon}\bm{z}." display="block"><semantics id="S4.E3.m1.7a"><mrow id="S4.E3.m1.7.7.1" xref="S4.E3.m1.7.7.1.1.cmml"><mrow id="S4.E3.m1.7.7.1.1" xref="S4.E3.m1.7.7.1.1.cmml"><mrow id="S4.E3.m1.7.7.1.1.2" xref="S4.E3.m1.7.7.1.1.2.cmml"><mover accent="true" id="S4.E3.m1.7.7.1.1.2.2" xref="S4.E3.m1.7.7.1.1.2.2.cmml"><mo id="S4.E3.m1.7.7.1.1.2.2.2" xref="S4.E3.m1.7.7.1.1.2.2.2.cmml">∇</mo><mo id="S4.E3.m1.7.7.1.1.2.2.1" xref="S4.E3.m1.7.7.1.1.2.2.1.cmml">^</mo></mover><mo lspace="0.167em" rspace="0em" id="S4.E3.m1.7.7.1.1.2.1" xref="S4.E3.m1.7.7.1.1.2.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.7.7.1.1.2.3" xref="S4.E3.m1.7.7.1.1.2.3.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.7.7.1.1.2.1a" xref="S4.E3.m1.7.7.1.1.2.1.cmml">​</mo><mrow id="S4.E3.m1.7.7.1.1.2.4.2" xref="S4.E3.m1.7.7.1.1.2.4.1.cmml"><mo stretchy="false" id="S4.E3.m1.7.7.1.1.2.4.2.1" xref="S4.E3.m1.7.7.1.1.2.4.1.cmml">(</mo><mi id="S4.E3.m1.5.5" xref="S4.E3.m1.5.5.cmml">𝜽</mi><mo id="S4.E3.m1.7.7.1.1.2.4.2.2" xref="S4.E3.m1.7.7.1.1.2.4.1.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.6.6" xref="S4.E3.m1.6.6.cmml">ℬ</mi><mo stretchy="false" id="S4.E3.m1.7.7.1.1.2.4.2.3" xref="S4.E3.m1.7.7.1.1.2.4.1.cmml">)</mo></mrow></mrow><mo id="S4.E3.m1.7.7.1.1.1" xref="S4.E3.m1.7.7.1.1.1.cmml">=</mo><mrow id="S4.E3.m1.7.7.1.1.3" xref="S4.E3.m1.7.7.1.1.3.cmml"><mfrac id="S4.E3.m1.4.4" xref="S4.E3.m1.4.4.cmml"><mrow id="S4.E3.m1.4.4.4" xref="S4.E3.m1.4.4.4.cmml"><mrow id="S4.E3.m1.3.3.3.3" xref="S4.E3.m1.3.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.3.3.3.3.3" xref="S4.E3.m1.3.3.3.3.3.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.3.3.3.3.2" xref="S4.E3.m1.3.3.3.3.2.cmml">​</mo><mrow id="S4.E3.m1.3.3.3.3.1.1" xref="S4.E3.m1.3.3.3.3.1.2.cmml"><mo stretchy="false" id="S4.E3.m1.3.3.3.3.1.1.2" xref="S4.E3.m1.3.3.3.3.1.2.cmml">(</mo><mrow id="S4.E3.m1.3.3.3.3.1.1.1" xref="S4.E3.m1.3.3.3.3.1.1.1.cmml"><mi id="S4.E3.m1.3.3.3.3.1.1.1.2" xref="S4.E3.m1.3.3.3.3.1.1.1.2.cmml">𝜽</mi><mo id="S4.E3.m1.3.3.3.3.1.1.1.1" xref="S4.E3.m1.3.3.3.3.1.1.1.1.cmml">+</mo><mrow id="S4.E3.m1.3.3.3.3.1.1.1.3" xref="S4.E3.m1.3.3.3.3.1.1.1.3.cmml"><mi id="S4.E3.m1.3.3.3.3.1.1.1.3.2" xref="S4.E3.m1.3.3.3.3.1.1.1.3.2.cmml">ϵ</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.3.3.3.3.1.1.1.3.1" xref="S4.E3.m1.3.3.3.3.1.1.1.3.1.cmml">​</mo><mi id="S4.E3.m1.3.3.3.3.1.1.1.3.3" xref="S4.E3.m1.3.3.3.3.1.1.1.3.3.cmml">𝒛</mi></mrow></mrow><mo id="S4.E3.m1.3.3.3.3.1.1.3" xref="S4.E3.m1.3.3.3.3.1.2.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.cmml">ℬ</mi><mo stretchy="false" id="S4.E3.m1.3.3.3.3.1.1.4" xref="S4.E3.m1.3.3.3.3.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E3.m1.4.4.4.5" xref="S4.E3.m1.4.4.4.5.cmml">−</mo><mrow id="S4.E3.m1.4.4.4.4" xref="S4.E3.m1.4.4.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.4.4.4.4.3" xref="S4.E3.m1.4.4.4.4.3.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.4.4.4.4.2" xref="S4.E3.m1.4.4.4.4.2.cmml">​</mo><mrow id="S4.E3.m1.4.4.4.4.1.1" xref="S4.E3.m1.4.4.4.4.1.2.cmml"><mo stretchy="false" id="S4.E3.m1.4.4.4.4.1.1.2" xref="S4.E3.m1.4.4.4.4.1.2.cmml">(</mo><mrow id="S4.E3.m1.4.4.4.4.1.1.1" xref="S4.E3.m1.4.4.4.4.1.1.1.cmml"><mi id="S4.E3.m1.4.4.4.4.1.1.1.2" xref="S4.E3.m1.4.4.4.4.1.1.1.2.cmml">𝜽</mi><mo id="S4.E3.m1.4.4.4.4.1.1.1.1" xref="S4.E3.m1.4.4.4.4.1.1.1.1.cmml">−</mo><mrow id="S4.E3.m1.4.4.4.4.1.1.1.3" xref="S4.E3.m1.4.4.4.4.1.1.1.3.cmml"><mi id="S4.E3.m1.4.4.4.4.1.1.1.3.2" xref="S4.E3.m1.4.4.4.4.1.1.1.3.2.cmml">ϵ</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.4.4.4.4.1.1.1.3.1" xref="S4.E3.m1.4.4.4.4.1.1.1.3.1.cmml">​</mo><mi id="S4.E3.m1.4.4.4.4.1.1.1.3.3" xref="S4.E3.m1.4.4.4.4.1.1.1.3.3.cmml">𝒛</mi></mrow></mrow><mo id="S4.E3.m1.4.4.4.4.1.1.3" xref="S4.E3.m1.4.4.4.4.1.2.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.2.2.2.2" xref="S4.E3.m1.2.2.2.2.cmml">ℬ</mi><mo stretchy="false" id="S4.E3.m1.4.4.4.4.1.1.4" xref="S4.E3.m1.4.4.4.4.1.2.cmml">)</mo></mrow></mrow></mrow><mrow id="S4.E3.m1.4.4.6" xref="S4.E3.m1.4.4.6.cmml"><mn id="S4.E3.m1.4.4.6.2" xref="S4.E3.m1.4.4.6.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.E3.m1.4.4.6.1" xref="S4.E3.m1.4.4.6.1.cmml">​</mo><mi id="S4.E3.m1.4.4.6.3" xref="S4.E3.m1.4.4.6.3.cmml">ϵ</mi></mrow></mfrac><mo lspace="0em" rspace="0em" id="S4.E3.m1.7.7.1.1.3.1" xref="S4.E3.m1.7.7.1.1.3.1.cmml">​</mo><mi id="S4.E3.m1.7.7.1.1.3.2" xref="S4.E3.m1.7.7.1.1.3.2.cmml">𝒛</mi></mrow></mrow><mo lspace="0em" id="S4.E3.m1.7.7.1.2" xref="S4.E3.m1.7.7.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.7b"><apply id="S4.E3.m1.7.7.1.1.cmml" xref="S4.E3.m1.7.7.1"><eq id="S4.E3.m1.7.7.1.1.1.cmml" xref="S4.E3.m1.7.7.1.1.1"></eq><apply id="S4.E3.m1.7.7.1.1.2.cmml" xref="S4.E3.m1.7.7.1.1.2"><times id="S4.E3.m1.7.7.1.1.2.1.cmml" xref="S4.E3.m1.7.7.1.1.2.1"></times><apply id="S4.E3.m1.7.7.1.1.2.2.cmml" xref="S4.E3.m1.7.7.1.1.2.2"><ci id="S4.E3.m1.7.7.1.1.2.2.1.cmml" xref="S4.E3.m1.7.7.1.1.2.2.1">^</ci><ci id="S4.E3.m1.7.7.1.1.2.2.2.cmml" xref="S4.E3.m1.7.7.1.1.2.2.2">∇</ci></apply><ci id="S4.E3.m1.7.7.1.1.2.3.cmml" xref="S4.E3.m1.7.7.1.1.2.3">ℒ</ci><list id="S4.E3.m1.7.7.1.1.2.4.1.cmml" xref="S4.E3.m1.7.7.1.1.2.4.2"><ci id="S4.E3.m1.5.5.cmml" xref="S4.E3.m1.5.5">𝜽</ci><ci id="S4.E3.m1.6.6.cmml" xref="S4.E3.m1.6.6">ℬ</ci></list></apply><apply id="S4.E3.m1.7.7.1.1.3.cmml" xref="S4.E3.m1.7.7.1.1.3"><times id="S4.E3.m1.7.7.1.1.3.1.cmml" xref="S4.E3.m1.7.7.1.1.3.1"></times><apply id="S4.E3.m1.4.4.cmml" xref="S4.E3.m1.4.4"><divide id="S4.E3.m1.4.4.5.cmml" xref="S4.E3.m1.4.4"></divide><apply id="S4.E3.m1.4.4.4.cmml" xref="S4.E3.m1.4.4.4"><minus id="S4.E3.m1.4.4.4.5.cmml" xref="S4.E3.m1.4.4.4.5"></minus><apply id="S4.E3.m1.3.3.3.3.cmml" xref="S4.E3.m1.3.3.3.3"><times id="S4.E3.m1.3.3.3.3.2.cmml" xref="S4.E3.m1.3.3.3.3.2"></times><ci id="S4.E3.m1.3.3.3.3.3.cmml" xref="S4.E3.m1.3.3.3.3.3">ℒ</ci><list id="S4.E3.m1.3.3.3.3.1.2.cmml" xref="S4.E3.m1.3.3.3.3.1.1"><apply id="S4.E3.m1.3.3.3.3.1.1.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1.1"><plus id="S4.E3.m1.3.3.3.3.1.1.1.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1.1.1"></plus><ci id="S4.E3.m1.3.3.3.3.1.1.1.2.cmml" xref="S4.E3.m1.3.3.3.3.1.1.1.2">𝜽</ci><apply id="S4.E3.m1.3.3.3.3.1.1.1.3.cmml" xref="S4.E3.m1.3.3.3.3.1.1.1.3"><times id="S4.E3.m1.3.3.3.3.1.1.1.3.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1.1.3.1"></times><ci id="S4.E3.m1.3.3.3.3.1.1.1.3.2.cmml" xref="S4.E3.m1.3.3.3.3.1.1.1.3.2">italic-ϵ</ci><ci id="S4.E3.m1.3.3.3.3.1.1.1.3.3.cmml" xref="S4.E3.m1.3.3.3.3.1.1.1.3.3">𝒛</ci></apply></apply><ci id="S4.E3.m1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1">ℬ</ci></list></apply><apply id="S4.E3.m1.4.4.4.4.cmml" xref="S4.E3.m1.4.4.4.4"><times id="S4.E3.m1.4.4.4.4.2.cmml" xref="S4.E3.m1.4.4.4.4.2"></times><ci id="S4.E3.m1.4.4.4.4.3.cmml" xref="S4.E3.m1.4.4.4.4.3">ℒ</ci><list id="S4.E3.m1.4.4.4.4.1.2.cmml" xref="S4.E3.m1.4.4.4.4.1.1"><apply id="S4.E3.m1.4.4.4.4.1.1.1.cmml" xref="S4.E3.m1.4.4.4.4.1.1.1"><minus id="S4.E3.m1.4.4.4.4.1.1.1.1.cmml" xref="S4.E3.m1.4.4.4.4.1.1.1.1"></minus><ci id="S4.E3.m1.4.4.4.4.1.1.1.2.cmml" xref="S4.E3.m1.4.4.4.4.1.1.1.2">𝜽</ci><apply id="S4.E3.m1.4.4.4.4.1.1.1.3.cmml" xref="S4.E3.m1.4.4.4.4.1.1.1.3"><times id="S4.E3.m1.4.4.4.4.1.1.1.3.1.cmml" xref="S4.E3.m1.4.4.4.4.1.1.1.3.1"></times><ci id="S4.E3.m1.4.4.4.4.1.1.1.3.2.cmml" xref="S4.E3.m1.4.4.4.4.1.1.1.3.2">italic-ϵ</ci><ci id="S4.E3.m1.4.4.4.4.1.1.1.3.3.cmml" xref="S4.E3.m1.4.4.4.4.1.1.1.3.3">𝒛</ci></apply></apply><ci id="S4.E3.m1.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2">ℬ</ci></list></apply></apply><apply id="S4.E3.m1.4.4.6.cmml" xref="S4.E3.m1.4.4.6"><times id="S4.E3.m1.4.4.6.1.cmml" xref="S4.E3.m1.4.4.6.1"></times><cn type="integer" id="S4.E3.m1.4.4.6.2.cmml" xref="S4.E3.m1.4.4.6.2">2</cn><ci id="S4.E3.m1.4.4.6.3.cmml" xref="S4.E3.m1.4.4.6.3">italic-ϵ</ci></apply></apply><ci id="S4.E3.m1.7.7.1.1.3.2.cmml" xref="S4.E3.m1.7.7.1.1.3.2">𝒛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.7c">\hat{\nabla}\mathcal{L}(\bm{\theta};\mathcal{B})=\frac{\mathcal{L}(\bm{\theta}+\epsilon\bm{z};\mathcal{B})-\mathcal{L}(\bm{\theta}-\epsilon\bm{z};\mathcal{B})}{2\epsilon}\bm{z}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.SSS3.p3" class="ltx_para">
<p id="S4.SS1.SSS3.p3.1" class="ltx_p">Based on the above gradient estimation frameworks,
recent work, such as that by <cite class="ltx_cite ltx_citemacro_citet">Xu et al. (<a href="#bib.bib185" title="" class="ltx_ref">2024a</a>); Lu et al. (<a href="#bib.bib111" title="" class="ltx_ref">2023b</a>)</cite>, has initiated preliminary explorations into the deployment of both FedPEFT and full-model fine-tuning of billion-sized FMs, like LLaMA, on mobile devices. The naive ZOO methods remain impractical for training large FMs in standard FL frameworks such as <span id="S4.SS1.SSS3.p3.1.1" class="ltx_text ltx_font_typewriter">FedAvg</span>, as they still result in a significant communication burden for model aggregation. In light of this, FedKSeed <cite class="ltx_cite ltx_citemacro_cite">Qin et al. (<a href="#bib.bib129" title="" class="ltx_ref">2024</a>)</cite> was proposed to further reduce communication overheads between the server and clients by using just a few random seeds and
scalar gradients, requiring only a few thousand bytes for communication.</p>
</div>
<div id="S4.SS1.SSS3.p4" class="ltx_para">
<p id="S4.SS1.SSS3.p4.1" class="ltx_p">Although ZOO methods have shown promise in resource-efficient FL <cite class="ltx_cite ltx_citemacro_cite">Ling et al. (<a href="#bib.bib102" title="" class="ltx_ref">2024</a>)</cite>, they generally require many iterations to achieve strong performance <cite class="ltx_cite ltx_citemacro_cite">Malladi et al. (<a href="#bib.bib115" title="" class="ltx_ref">2023b</a>)</cite>. Compared to the well-established BP-based optimization, ZOO is still in the early stages of development, particularly for FM-FL settings, necessitating further research and optimization.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Adaptability</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Adaptation refers to the process of tailoring a pre-trained FM to perform effectively across varying FL settings and scenarios. This mainly includes the capability to learn from different domains, cater to individual user needs, and work across diverse devices while retaining overall performance and efficiency. We focus on three key aspects of adaptation, namely <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">domain-centric adaptation</span>, <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">client-centric adaptation</span>, and <span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_italic">system-centric adaptation</span>.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Domain-Centric Adaptation</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">Domain-centric adaptation focuses on adapting FMs within specific domains by addressing the domain diversity across client datasets.</p>
</div>
<section id="S4.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Domain-Adaptive Pre-Training</h5>

<div id="S4.SS2.SSS1.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px1.p1.1" class="ltx_p">Despite being heavily reliant on large-scale and public datasets for their initial training, FMs often require further Domain-Adaptive Pre-Training (DAPT) with domain-specific data for tasks that necessitate specialized knowledge <cite class="ltx_cite ltx_citemacro_cite">Gururangan et al. (<a href="#bib.bib62" title="" class="ltx_ref">2020</a>); Guo and Yu (<a href="#bib.bib59" title="" class="ltx_ref">2022</a>)</cite>.
In domains like healthcare, FL allows for the continued pre-training of these models using sensitive, domain-specific data without compromising privacy. Based on this idea, <cite class="ltx_cite ltx_citemacro_citet">Jiang et al. (<a href="#bib.bib74" title="" class="ltx_ref">2023b</a>)</cite> proposed FFDAPT, a computational-efficient further pre-training algorithm that freezes a portion of consecutive layers while optimizing the rest of the layers. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a href="#bib.bib172" title="" class="ltx_ref">2023</a>)</cite> proposed FEDBFPT that builds a local model for each client, progressively training the shallower layers of local models while sampling deeper layers, and aggregating trained parameters on a server to create the final global model.</p>
</div>
</section>
<section id="S4.SS2.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Multi-Domain Adaptation</h5>

<div id="S4.SS2.SSS1.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px2.p1.1" class="ltx_p">Given that client data may belong to various domains in real-world FL scenarios, some efforts <cite class="ltx_cite ltx_citemacro_cite">Feng et al. (<a href="#bib.bib47" title="" class="ltx_ref">2023c</a>); Su et al. (<a href="#bib.bib153" title="" class="ltx_ref">2024</a>)</cite> have been devoted to facilitating multi-domain collaborative adaptation. <cite class="ltx_cite ltx_citemacro_citet">Feng et al. (<a href="#bib.bib47" title="" class="ltx_ref">2023c</a>)</cite> applied a pre-trained CLIP to the multi-domain scenario and proposed an adaptive prompt tuning method that uses domain-specific keys to generate prompts
for each test sample. Furthermore, <cite class="ltx_cite ltx_citemacro_citet">Su et al. (<a href="#bib.bib153" title="" class="ltx_ref">2024</a>)</cite> employed knowledge distillation to selectively distill global knowledge based on an entropy measure, improving the generalization across different domains.</p>
</div>
</section>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Client-Centric Adaptation</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">Client-centric adaptation refers to the process of tailoring an FM to meet the specific needs or preferences of individual clients while leveraging the decentralized and privacy-preserving nature of FL. Particularly, we discuss two types of popular personalized methods as follows:</p>
</div>
<section id="S4.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Personalization</h5>

<div id="S4.SS2.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px1.p1.1" class="ltx_p">Adapter-based methods introduce small, trainable adapters into the frozen pre-trained FMs, allowing for client-specific model adaptation without altering the original FL. FedDAT <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib27" title="" class="ltx_ref">2024</a>)</cite> leverages a dual-adapter structure, with personalized adapters focusing on client-specific knowledge and a global adapter maintaining client-agnostic knowledge. FedDAT executes bi-directional knowledge distillation between personalized adapters and the global adapter to regularize the client’s updates and prevent overfitting. Prompt-based methods involve using client-specific soft prompts to guide the model’s response. pFedPG <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib189" title="" class="ltx_ref">2023a</a>)</cite> trains a prompt generator to exploit underlying client-specific characteristics and produce personalized prompts for each client, thereby enabling efficient and personalized adaptation.</p>
</div>
</section>
<section id="S4.SS2.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Client Clustering</h5>

<div id="S4.SS2.SSS2.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px2.p1.1" class="ltx_p">This branch of study aims to cluster clients based on the underlying relationships and tailor FMs for the client group with similar data distributions, thus reducing the negative impact of data heterogeneity and improving accuracy. <cite class="ltx_cite ltx_citemacro_citet">Guo et al. (<a href="#bib.bib60" title="" class="ltx_ref">2024b</a>)</cite> proposed a FedPEFT-based framework for multilingual modeling, which employs language family clustering to alleviate parameter conflicts of LoRA tuning.</p>
</div>
</section>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>System-Centric Adaptation</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">System-centric aims to improve adaptability at the system level. This involves handling resource heterogeneity in the FL systems while ensuring training efficiency and model utility.</p>
</div>
<section id="S4.SS2.SSS3.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Resource-Heterogeneous Methods</h5>

<div id="S4.SS2.SSS3.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS3.Px1.p1.1" class="ltx_p">Cross-device FL systems may be composed of devices equipped with heterogeneous resources, leading to disparities where certain devices exhibit more efficient model training than others <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib27" title="" class="ltx_ref">2024</a>)</cite>. To address this issue, several methods have been developed to customize model architectures for resource-heterogeneous FL systems. In FL environments possessing heterogeneous resources, LoRA-based FedPEFT exhibits distinctive flexibility and adaptation in fine-tuning frozen FMs without overburdening client devices. <cite class="ltx_cite ltx_citemacro_citet">Su et al. (<a href="#bib.bib152" title="" class="ltx_ref">2023</a>)</cite> suggested assigning LoRA adapters to varying numbers of layers for heterogeneous clients according to a randomly generated mask matrix. An alternative and more targeted idea is to choose diverse LoRA ranks across clients based on their system capabilities. <cite class="ltx_cite ltx_citemacro_citet">Bai et al. (<a href="#bib.bib12" title="" class="ltx_ref">2024a</a>)</cite> proposed FlexLoRA to adjust local LoRA ranks dynamically. FlexLoRA reconstructs the uniform full-sized LoRA module <math id="S4.SS2.SSS3.Px1.p1.1.m1.1" class="ltx_Math" alttext="\Delta\mathbf{W}" display="inline"><semantics id="S4.SS2.SSS3.Px1.p1.1.m1.1a"><mrow id="S4.SS2.SSS3.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.2" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.1" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3.cmml">𝐖</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.Px1.p1.1.m1.1b"><apply id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1"><times id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.1"></times><ci id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.2">Δ</ci><ci id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3">𝐖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.Px1.p1.1.m1.1c">\Delta\mathbf{W}</annotation></semantics></math> for server-side model aggregation followed by an SVD-based parameter redistribution. However, concurrent research by <cite class="ltx_cite ltx_citemacro_citet">Cho et al. (<a href="#bib.bib30" title="" class="ltx_ref">2024</a>)</cite> has empirically demonstrated that the reconstruct-redistribute method suffers from performance loss compared to homogeneous LoRA. Instead, they proposed <span id="S4.SS2.SSS3.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">HetLoRA</span> <cite class="ltx_cite ltx_citemacro_cite">Cho et al. (<a href="#bib.bib30" title="" class="ltx_ref">2024</a>)</cite> that utilizes zero-padding to align module size before aggregation. It then truncates the global LoRA modules for the specific rank of the next selected clients.</p>
</div>
</section>
<section id="S4.SS2.SSS3.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Split Learning</h5>

<div id="S4.SS2.SSS3.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS3.Px2.p1.1" class="ltx_p">Split learning addresses the resource heterogeneity between servers and clients by splitting a large model at a cut layer into client and server models <cite class="ltx_cite ltx_citemacro_cite">Thapa et al. (<a href="#bib.bib164" title="" class="ltx_ref">2022</a>)</cite>. For each training step, the output tensor, so-called smashed data, from the client model and the corresponding labels are transmitted over to the server. The server continues the forward propagation by processing the smashed data through its remaining layers; it then computes the loss using the transmitted label and performs backpropagation. The gradient generated
at the first layer of the server model is then transmitted back to the client for further backpropagation.
Along this line, FedBERT <cite class="ltx_cite ltx_citemacro_cite">Tian et al. (<a href="#bib.bib165" title="" class="ltx_ref">2022</a>)</cite> proposes to leverage split learning for training the BERT model, showing the feasibility of training large FMs in FL settings.
FedSplitX <cite class="ltx_cite ltx_citemacro_cite">Shin et al. (<a href="#bib.bib149" title="" class="ltx_ref">2023b</a>)</cite> is a more fine-grained method that allows multiple partition points for model splitting, accommodating more diverse client capabilities. Compared to conventional FL, split learning scales better with the size of FMs as it communicates only small-sized smashed data instead of model parameters <cite class="ltx_cite ltx_citemacro_cite">Singh et al. (<a href="#bib.bib150" title="" class="ltx_ref">2019</a>)</cite>. Despite its merits, split learning is highly dependent on the network connection quality. Given that server-client interactions occur at every step of the optimization process <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a href="#bib.bib214" title="" class="ltx_ref">2023</a>)</cite>, communication delays cause a more significant impact on efficiency.</p>
</div>
</section>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Trustworthiness</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">This line of work aims to enhance trustworthiness throughout the FM-FL lifecycle, covering a variety of key aspects including, but not limited to, <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">IP protection</span>, <span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_italic">privacy protection</span>, and <span id="S4.SS3.p1.1.3" class="ltx_text ltx_font_italic">attack robustness</span>.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>IP Protection</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">Existing IP protection involves safeguarding ownership of FMs from unauthorized use (<em id="S4.SS3.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> model theft) <cite class="ltx_cite ltx_citemacro_cite">Tekgul et al. (<a href="#bib.bib163" title="" class="ltx_ref">2021</a>)</cite>. We discuss the following two mainstream IP protection strategies: <span id="S4.SS3.SSS1.p1.1.2" class="ltx_text ltx_font_italic">watermarking</span> and <span id="S4.SS3.SSS1.p1.1.3" class="ltx_text ltx_font_italic">black-box tuning</span>.</p>
</div>
<section id="S4.SS3.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Watermarking</h5>

<div id="S4.SS3.SSS1.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.Px1.p1.1" class="ltx_p">Watermarking is a well-known deterrence technology for model IP protection by providing the identities of model owners to demonstrate ownership of their models <cite class="ltx_cite ltx_citemacro_cite">Adi et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Tekgul et al. (<a href="#bib.bib163" title="" class="ltx_ref">2021</a>)</cite> proposed WAFFLE, the first solution that addresses the ownership problem by injecting a watermark into the global model in FL environments. Recently, <cite class="ltx_cite ltx_citemacro_citet">Yu et al. (<a href="#bib.bib197" title="" class="ltx_ref">2023b</a>)</cite> proposed DUW that embeds a client-unique key into each client’s local model, aiming to identify the infringer of a leaked model while verifying the FL model’s ownership.</p>
</div>
</section>
<section id="S4.SS3.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Black-Box Tuning</h5>

<div id="S4.SS3.SSS1.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS1.Px2.p1.1" class="ltx_p">Black-Box Tuning (BBT) is a set of ZOO-based methods that fine-tune FMs without direct access to model parameters <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib158" title="" class="ltx_ref">2022c</a>, <a href="#bib.bib157" title="" class="ltx_ref">b</a>)</cite>. BBT methods are often additive, introducing additional parameters while keeping the original model frozen (see Section <a href="#S4.SS1.SSS1.Px2" title="Additive Methods ‣ 4.1.1 Parameter-Efficient Fine-Tuning ‣ 4.1 Efficiency ‣ 4 Techniques ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1.1</span></a>). Fed-BBPT <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib100" title="" class="ltx_ref">2023</a>)</cite> is a general prompt tuning framework
that facilitates the joint training of a global lightweight prompt generator across multiple clients.
FedBPT <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib156" title="" class="ltx_ref">2024a</a>)</cite> adopts a classic evolutionary-based ZOO method, CMA-ES <cite class="ltx_cite ltx_citemacro_cite">Hansen and Ostermeier (<a href="#bib.bib64" title="" class="ltx_ref">2001</a>)</cite>, for training an optimal prompt that improves the performance of frozen FMs. ZooPFL <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib111" title="" class="ltx_ref">2023b</a>)</cite>, on the other hand, applies coordinate-wise gradient estimate to learn input surgery that incorporates client-specific embeddings. BBT allows for local fine-tuning of FMs while not infringing IP constraints. However, current research in this line is limited to few-shot learning with small datasets for LLM fine-tuning <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib157" title="" class="ltx_ref">2022b</a>)</cite>, while larger datasets and other modalities remain unexplored.</p>
</div>
</section>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Privacy Protection</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">Protecting privacy in FM-FL requires both designing protective measures and studying privacy attack strategies.</p>
</div>
<section id="S4.SS3.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Privacy-Preserving Techniques</h5>

<div id="S4.SS3.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS2.Px1.p1.1" class="ltx_p">Differential Privacy (DP) is a theoretical framework that governs privacy boundaries and manages the tradeoff between privacy and model convergence <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib173" title="" class="ltx_ref">2020</a>); Xu et al. (<a href="#bib.bib188" title="" class="ltx_ref">2023</a>)</cite>. DP-based FL approaches often add artificial noise (<em id="S4.SS3.SSS2.Px1.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> Gaussian noise) to parameters at the clients’ side before aggregating to prevent information leakage <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib188" title="" class="ltx_ref">2023</a>)</cite>. Besides, DP is compatible with most FedPEFT methods. For instance, <cite class="ltx_cite ltx_citemacro_citet">Sun et al. (<a href="#bib.bib159" title="" class="ltx_ref">2024b</a>)</cite> showed that DP noise can even be amplified by the locally “semi-quadratic” nature of LoRA-based methods, motivating the integration of LoRA with DP to improve resource efficiency while maintaining data privacy <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib107" title="" class="ltx_ref">2023c</a>)</cite>. In addition to DP, Secure Multi-Party Computation (SMPC) <cite class="ltx_cite ltx_citemacro_cite">Mugunthan et al. (<a href="#bib.bib120" title="" class="ltx_ref">2019</a>)</cite> and Homomorphic Encryption (HE) <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib201" title="" class="ltx_ref">2020</a>)</cite> are also effective privacy-preserving mechanisms. However, they do not scale well enough for large-scale deployments in FM-FL.</p>
</div>
</section>
<section id="S4.SS3.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Privacy Attack</h5>

<div id="S4.SS3.SSS2.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.Px2.p1.1" class="ltx_p">Privacy attacks in FM-FL involve extracting sensitive information from the data used in training, even though the data itself is not directly shared. Major attacks include <span id="S4.SS3.SSS2.Px2.p1.1.1" class="ltx_text ltx_font_italic">membership inference attack</span> and <span id="S4.SS3.SSS2.Px2.p1.1.2" class="ltx_text ltx_font_italic">data reconstruction attack</span>, where the former aims to determine whether a specific data sample is in a victim client’s training set, and the latter strives to reconstruct original input data from the model parameters or gradients <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib139" title="" class="ltx_ref">2024</a>)</cite>. Regarding membership inference attacks, <cite class="ltx_cite ltx_citemacro_citet">Vu et al. (<a href="#bib.bib169" title="" class="ltx_ref">2024</a>)</cite> revealed the vulnerabilities of popular LLMs, including BERT, DistilBERT, and OpenAI’s GPTs. In terms of data reconstruction attacks, <cite class="ltx_cite ltx_citemacro_citet">Gupta et al. (<a href="#bib.bib61" title="" class="ltx_ref">2022</a>)</cite> presented an attack <span id="S4.SS3.SSS2.Px2.p1.1.3" class="ltx_text ltx_font_italic">FILM</span>, which recovers private text data by extracting information from gradients transmitted during training despite employing a DP mechanism.</p>
</div>
</section>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Attack Robustness</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">Due to the distributed characteristic of optimization, FL
is vulnerable to poisoning attacks <cite class="ltx_cite ltx_citemacro_cite">Lyu et al. (<a href="#bib.bib112" title="" class="ltx_ref">2022</a>); Rodríguez-Barroso et al. (<a href="#bib.bib141" title="" class="ltx_ref">2023</a>)</cite>, wherein certain
participants may deviate from the prescribed update protocol
and upload arbitrary parameters to the central server.</p>
</div>
<section id="S4.SS3.SSS3.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Poisoning Attacks</h5>

<div id="S4.SS3.SSS3.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS3.Px1.p1.1" class="ltx_p">Depending on the adversarial goals, poisoning attacks in FL can be classified as <span id="S4.SS3.SSS3.Px1.p1.1.1" class="ltx_text ltx_font_italic">targeted</span> and <span id="S4.SS3.SSS3.Px1.p1.1.2" class="ltx_text ltx_font_italic">untargeted</span> <cite class="ltx_cite ltx_citemacro_cite">Jere et al. (<a href="#bib.bib70" title="" class="ltx_ref">2020</a>)</cite>. Targeted attacks, like backdoor attacks, aim to manipulate the global model to generate attacker-desired misclassifications for some particular samples <cite class="ltx_cite ltx_citemacro_cite">Xie et al. (<a href="#bib.bib182" title="" class="ltx_ref">2020</a>); Bagdasaryan et al. (<a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>. In contrast, untargeted attacks seek to degrade the model’s overall performance indiscriminately <cite class="ltx_cite ltx_citemacro_cite">Fang et al. (<a href="#bib.bib42" title="" class="ltx_ref">2020</a>)</cite>. In addition to the well-recognized attacks on conventional FL studies <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib91" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib90" title="" class="ltx_ref">2024b</a>)</cite>, FM-FL also faces potential threats from compromised pre-trained FMs <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib94" title="" class="ltx_ref">2023c</a>)</cite>. Thus, The attacker can introduce backdoors to downstream tasks without prior knowledge <cite class="ltx_cite ltx_citemacro_cite">Shen et al. (<a href="#bib.bib147" title="" class="ltx_ref">2021</a>)</cite>. Specifically, <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib95" title="" class="ltx_ref">2023d</a>)</cite> proposed Fed-EBD that introduces a backdoor-compromised FM to generate a public, synthetic dataset for FL training. The clients’ models, pre-trained on this dataset, inherit the backdoor throughout the training.</p>
</div>
</section>
<section id="S4.SS3.SSS3.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Defense Techniques</h5>

<div id="S4.SS3.SSS3.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS3.Px2.p1.1" class="ltx_p">As for defenses, robust aggregation rules are widely applied to make an attack-resilient estimation of the true updates and exclude the influence of malicious updates <cite class="ltx_cite ltx_citemacro_cite">Blanchard et al. (<a href="#bib.bib15" title="" class="ltx_ref">2017</a>); Yin et al. (<a href="#bib.bib195" title="" class="ltx_ref">2018</a>); Chen et al. (<a href="#bib.bib28" title="" class="ltx_ref">2017</a>); Li et al. (<a href="#bib.bib89" title="" class="ltx_ref">2023a</a>)</cite>. Other research directions include trust-based strategies <cite class="ltx_cite ltx_citemacro_cite">Cao et al. (<a href="#bib.bib24" title="" class="ltx_ref">2021</a>); Xu et al. (<a href="#bib.bib184" title="" class="ltx_ref">2022</a>); Park et al. (<a href="#bib.bib126" title="" class="ltx_ref">2021</a>)</cite> and variance-reduced algorithms <cite class="ltx_cite ltx_citemacro_cite">Gorbunov et al. (<a href="#bib.bib55" title="" class="ltx_ref">2023</a>); Wu et al. (<a href="#bib.bib181" title="" class="ltx_ref">2020b</a>)</cite>. Although these techniques have been widely examined in various FL settings, their effectiveness has yet to be explored in the FM-FL paradigm.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>A list of representative studies on the applications of FM-FL. Abbreviations: LoRA Tuning (LT), Adapter Tuning (AT), Full-Parameter Tuning (FT), Selective Tuning (ST), Prompt Tuning (PT). </figcaption>
<p id="S4.T2.1" class="ltx_p ltx_align_center"><span id="S4.T2.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S4.T2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:806.9pt;height:324.9pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S4.T2.1.1.1.1" class="ltx_p"><span id="S4.T2.1.1.1.1.1" class="ltx_text">
<span id="S4.T2.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Domain/Application</span></span>
<span id="S4.T2.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Task</span></span>
<span id="S4.T2.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Representative Work</span></span>
<span id="S4.T2.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span id="S4.T2.1.1.1.1.1.1.1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:45.7pt;vertical-align:-19.4pt;"><span class="ltx_transformed_inner" style="width:45.7pt;transform:translate(-19.43pt,0pt) rotate(-90deg) ;">
<span id="S4.T2.1.1.1.1.1.1.1.4.1.1" class="ltx_p"><span id="S4.T2.1.1.1.1.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold" style="color:#006BA4;">On-Device</span></span>
</span></span></span>
<span id="S4.T2.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span id="S4.T2.1.1.1.1.1.1.1.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:89.9pt;vertical-align:-41.5pt;"><span class="ltx_transformed_inner" style="width:89.9pt;transform:translate(-41.5pt,0pt) rotate(-90deg) ;">
<span id="S4.T2.1.1.1.1.1.1.1.5.1.1" class="ltx_p"><span id="S4.T2.1.1.1.1.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold" style="color:#1B9E77;">Personalization</span></span>
</span></span></span>
<span id="S4.T2.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Modality</span></span>
<span id="S4.T2.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.1.7.1" class="ltx_text ltx_font_bold">Backbone</span></span>
<span id="S4.T2.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">
<span id="S4.T2.1.1.1.1.1.1.1.8.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:53.5pt;vertical-align:-24.3pt;"><span class="ltx_transformed_inner" style="width:53.5pt;transform:translate(-22.35pt,2.92pt) rotate(-90deg) ;">
<span id="S4.T2.1.1.1.1.1.1.1.8.1.1" class="ltx_p"><span id="S4.T2.1.1.1.1.1.1.1.8.1.1.1" class="ltx_text ltx_font_bold">Fine-Tuning</span></span>
</span></span></span></span>
<span id="S4.T2.1.1.1.1.1.1.2" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt ltx_rowspan ltx_rowspan_4" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold">Multilingual NLP</span></span>
<span id="S4.T2.1.1.1.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;">Language Understanding</span>
<span id="S4.T2.1.1.1.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;">FedKC <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib171" title="" class="ltx_ref">2022</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.2.4.1" class="ltx_text" style="color:#006BA4;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.2.5.1" class="ltx_text" style="color:#1B9E77;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.2.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.2.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;">mBERT</span>
<span id="S4.T2.1.1.1.1.1.1.2.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.9pt;padding-bottom:0.9pt;">FT</span></span>
<span id="S4.T2.1.1.1.1.1.1.3" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">Multi-Tasks</span>
<span id="S4.T2.1.1.1.1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">PMMFL <cite class="ltx_cite ltx_citemacro_cite">Weller et al. (<a href="#bib.bib174" title="" class="ltx_ref">2022</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.3.3" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.3.3.1" class="ltx_text" style="color:#006BA4;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.3.4.1" class="ltx_text" style="color:#1B9E77;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.3.5" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.3.6" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">mBERT</span>
<span id="S4.T2.1.1.1.1.1.1.3.7" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">FT</span></span>
<span id="S4.T2.1.1.1.1.1.1.4" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">Machine Translation</span>
<span id="S4.T2.1.1.1.1.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">Fed-MNMT <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib108" title="" class="ltx_ref">2023d</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.4.3" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.4.3.1" class="ltx_text" style="color:#006BA4;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.4.4.1" class="ltx_text" style="color:#1B9E77;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.4.5" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.4.6" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">mBART-50</span>
<span id="S4.T2.1.1.1.1.1.1.4.7" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">AT</span></span>
<span id="S4.T2.1.1.1.1.1.1.5" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">Machine Translation</span>
<span id="S4.T2.1.1.1.1.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">FL-MetaSend <cite class="ltx_cite ltx_citemacro_cite">Chu et al. (<a href="#bib.bib31" title="" class="ltx_ref">2024</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.5.3" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.5.3.1" class="ltx_text" style="color:#006BA4;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.5.4.1" class="ltx_text" style="color:#1B9E77;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.5.5" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.5.6" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">M2M-100</span>
<span id="S4.T2.1.1.1.1.1.1.5.7" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">ST</span></span>
<span id="S4.T2.1.1.1.1.1.1.6" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.6.1" class="ltx_td ltx_border_rr" style="padding-top:0.9pt;padding-bottom:0.9pt;"></span>
<span id="S4.T2.1.1.1.1.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">Multi-Tasks</span>
<span id="S4.T2.1.1.1.1.1.1.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">MFPT <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib212" title="" class="ltx_ref">2024b</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.6.4" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.6.4.1" class="ltx_text" style="color:#006BA4;">✓</span></span>
<span id="S4.T2.1.1.1.1.1.1.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.6.5.1" class="ltx_text" style="color:#1B9E77;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.6.6" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.6.7" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">XLM-RoBERTa</span>
<span id="S4.T2.1.1.1.1.1.1.6.8" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">PT</span></span>
<span id="S4.T2.1.1.1.1.1.1.7" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.7.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t ltx_rowspan ltx_rowspan_2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.7.1.1" class="ltx_text ltx_font_bold">Speech</span></span>
<span id="S4.T2.1.1.1.1.1.1.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">Speech-to-Text</span>
<span id="S4.T2.1.1.1.1.1.1.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">pFedS2T <cite class="ltx_cite ltx_citemacro_cite">Du et al. (<a href="#bib.bib39" title="" class="ltx_ref">2024</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.7.4.1" class="ltx_text" style="color:#006BA4;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.7.5.1" class="ltx_text" style="color:#1B9E77;">✓</span></span>
<span id="S4.T2.1.1.1.1.1.1.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">Aud.</span>
<span id="S4.T2.1.1.1.1.1.1.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">Conformer/Whisper</span>
<span id="S4.T2.1.1.1.1.1.1.7.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">LT</span></span>
<span id="S4.T2.1.1.1.1.1.1.8" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.8.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">Speech Recognition</span>
<span id="S4.T2.1.1.1.1.1.1.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">FedASR <cite class="ltx_cite ltx_citemacro_cite">Jia et al. (<a href="#bib.bib71" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.8.3" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.8.3.1" class="ltx_text" style="color:#006BA4;">✓</span></span>
<span id="S4.T2.1.1.1.1.1.1.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.8.4.1" class="ltx_text" style="color:#1B9E77;">✓</span></span>
<span id="S4.T2.1.1.1.1.1.1.8.5" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">Aud.</span>
<span id="S4.T2.1.1.1.1.1.1.8.6" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">RNN-T</span>
<span id="S4.T2.1.1.1.1.1.1.8.7" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">AT</span></span>
<span id="S4.T2.1.1.1.1.1.1.9" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.9.1" class="ltx_td ltx_border_rr" style="padding-top:0.9pt;padding-bottom:0.9pt;"></span>
<span id="S4.T2.1.1.1.1.1.1.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">Speech Recognition</span>
<span id="S4.T2.1.1.1.1.1.1.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">FedE2EASR<cite class="ltx_cite ltx_citemacro_cite">Azam et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023a</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.9.4" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.9.4.1" class="ltx_text" style="color:#006BA4;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.9.5.1" class="ltx_text" style="color:#1B9E77;">✓</span></span>
<span id="S4.T2.1.1.1.1.1.1.9.6" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">Aud.</span>
<span id="S4.T2.1.1.1.1.1.1.9.7" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">CTC-AED</span>
<span id="S4.T2.1.1.1.1.1.1.9.8" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">FT</span></span>
<span id="S4.T2.1.1.1.1.1.1.10" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.10.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t ltx_rowspan ltx_rowspan_3" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.10.1.1" class="ltx_text ltx_font_bold">Recommendation</span></span>
<span id="S4.T2.1.1.1.1.1.1.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.10.2.1" class="ltx_text ltx_font_italic">General</span></span>
<span id="S4.T2.1.1.1.1.1.1.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">PPLR <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib211" title="" class="ltx_ref">2024a</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.10.4.1" class="ltx_text" style="color:#006BA4;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.10.5.1" class="ltx_text" style="color:#1B9E77;">✓</span></span>
<span id="S4.T2.1.1.1.1.1.1.10.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.10.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">LLaMA-7B/LongFormer</span>
<span id="S4.T2.1.1.1.1.1.1.10.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">FT</span></span>
<span id="S4.T2.1.1.1.1.1.1.11" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.11.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.11.1.1" class="ltx_text ltx_font_italic">General</span></span>
<span id="S4.T2.1.1.1.1.1.1.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">TransFR <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib202" title="" class="ltx_ref">2024a</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.11.3" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.11.3.1" class="ltx_text" style="color:#006BA4;">✓</span></span>
<span id="S4.T2.1.1.1.1.1.1.11.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.11.4.1" class="ltx_text" style="color:#1B9E77;">✓</span></span>
<span id="S4.T2.1.1.1.1.1.1.11.5" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.11.6" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">DistBERT</span>
<span id="S4.T2.1.1.1.1.1.1.11.7" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">AT</span></span>
<span id="S4.T2.1.1.1.1.1.1.12" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.12.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.12.1.1" class="ltx_text ltx_font_italic">General</span></span>
<span id="S4.T2.1.1.1.1.1.1.12.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">GPT-FedRec <cite class="ltx_cite ltx_citemacro_cite">Zeng et al. (<a href="#bib.bib200" title="" class="ltx_ref">2024</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.12.3" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.12.3.1" class="ltx_text" style="color:#006BA4;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.12.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.12.4.1" class="ltx_text" style="color:#1B9E77;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.12.5" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.12.6" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;">ChatGPT</span>
<span id="S4.T2.1.1.1.1.1.1.12.7" class="ltx_td ltx_align_center" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.12.7.1" class="ltx_text ltx_font_italic">NA</span></span></span>
<span id="S4.T2.1.1.1.1.1.1.13" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t ltx_rowspan ltx_rowspan_2" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.13.1.1" class="ltx_text ltx_font_bold">Healthcare</span></span>
<span id="S4.T2.1.1.1.1.1.1.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">Mental Health Prediction</span>
<span id="S4.T2.1.1.1.1.1.1.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">FedTherapist <cite class="ltx_cite ltx_citemacro_cite">Shin et al. (<a href="#bib.bib148" title="" class="ltx_ref">2023a</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.13.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.13.4.1" class="ltx_text" style="color:#006BA4;">✓</span></span>
<span id="S4.T2.1.1.1.1.1.1.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.13.5.1" class="ltx_text" style="color:#1B9E77;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.13.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">Txt.</span>
<span id="S4.T2.1.1.1.1.1.1.13.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">BERT &amp; LLaMa-7B</span>
<span id="S4.T2.1.1.1.1.1.1.13.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.9pt;padding-bottom:0.9pt;">LT</span></span>
<span id="S4.T2.1.1.1.1.1.1.14" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.14.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">MRI Reconstruction</span>
<span id="S4.T2.1.1.1.1.1.1.14.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;">FedPR <cite class="ltx_cite ltx_citemacro_cite">Feng et al. (<a href="#bib.bib45" title="" class="ltx_ref">2023a</a>)</cite></span>
<span id="S4.T2.1.1.1.1.1.1.14.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.14.3.1" class="ltx_text" style="color:#006BA4;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.14.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:0.9pt;padding-bottom:0.9pt;"><span id="S4.T2.1.1.1.1.1.1.14.4.1" class="ltx_text" style="color:#1B9E77;">✗</span></span>
<span id="S4.T2.1.1.1.1.1.1.14.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.9pt;padding-bottom:0.9pt;">Vis.</span>
<span id="S4.T2.1.1.1.1.1.1.14.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.9pt;padding-bottom:0.9pt;">Swin
Transformers</span>
<span id="S4.T2.1.1.1.1.1.1.14.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.9pt;padding-bottom:0.9pt;">PT</span></span>
</span></span></span>
</span></span></span></p>
</figure>
</section>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Applications of FM-FL</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this part, we briefly review the recent progress on FM-FL applications. Table <a href="#S4.T2" title="Table 2 ‣ Defense Techniques ‣ 4.3.3 Attack Robustness ‣ 4.3 Trustworthiness ‣ 4 Techniques ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> lists representative work on specific applications and domains.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>FM-FL for Multilingual NLP</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Multilingual NLP refers to the techniques that handle multiple natural languages <cite class="ltx_cite ltx_citemacro_cite">Pires et al. (<a href="#bib.bib128" title="" class="ltx_ref">2019</a>)</cite>, often to perform equally well across them <cite class="ltx_cite ltx_citemacro_cite">Wu and Dredze (<a href="#bib.bib180" title="" class="ltx_ref">2020</a>)</cite>. Earlier research <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib77" title="" class="ltx_ref">2017</a>)</cite> has shown that parameter sharing among different languages boosts the model’s performance in multilingual NLP, especially for low-resource languages for which significantly less content is available. However, real-world multilingual text data is often distributed across devices or regions, with each client (user) accessing only a limited subset of languages, where transferring the data to a central server is often problematic or prohibited due to privacy issues <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib171" title="" class="ltx_ref">2022</a>)</cite>. Thanks to its inherent privacy-preserving characteristic, FL holds promise in breaking the barriers of cross-lingual modeling and data isolation by allowing models to learn from decentralized datasets.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The pioneer work by <cite class="ltx_cite ltx_citemacro_citet">Weller et al. (<a href="#bib.bib174" title="" class="ltx_ref">2022</a>)</cite> has firstly demonstrated that fine-tuning pre-trained language models with FL can perform similarly to pre-trained models fine-tuned with the standard centralized method under multilingual NLP settings. Various subsequent studies have focused on adapting pre-trained FMs through FedPEFT techniques such as adapter tuning <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib108" title="" class="ltx_ref">2023d</a>)</cite>, prompt tuning <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib212" title="" class="ltx_ref">2024b</a>)</cite>, and LoRA <cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a href="#bib.bib60" title="" class="ltx_ref">2024b</a>)</cite>, aiming to enhance training efficiency.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Considering the adverse effect of conflicting parameters from diverse languages during federated fine-tuning, recent studies have exploited clustering strategies to alleviate this issue. For instance, <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a href="#bib.bib171" title="" class="ltx_ref">2022</a>)</cite> applied <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mi id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><ci id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">k</annotation></semantics></math>-means clustering on each client’s data to obtain representative knowledge, specifically the clustered data centroids.
These centroids were then shared across clients for local training, enriching training data and addressing the challenges associated with data heterogeneity. Another compelling strategy along this line is language family-based clustering. <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a href="#bib.bib108" title="" class="ltx_ref">2023d</a>)</cite> explored various clustering strategies to group adapter parameters to mitigate the negative effects of multilingual data heterogeneity, showing that language family-based clustering significantly outperforms the other clustering strategies. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Guo et al. (<a href="#bib.bib60" title="" class="ltx_ref">2024b</a>)</cite> proposed fine-tuning FMs with LoRA and language family-based clustering to address the heterogeneity issue of multilingual modeling.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">General downstream tasks include language
modeling <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib171" title="" class="ltx_ref">2022</a>)</cite>, machine translation <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib108" title="" class="ltx_ref">2023d</a>); Chu et al. (<a href="#bib.bib31" title="" class="ltx_ref">2024</a>)</cite>, and text classification <cite class="ltx_cite ltx_citemacro_cite">Weller et al. (<a href="#bib.bib174" title="" class="ltx_ref">2022</a>)</cite>. In addition, some studies also focus on more specific applications such as medical transcript analysis <cite class="ltx_cite ltx_citemacro_cite">Manoel et al. (<a href="#bib.bib117" title="" class="ltx_ref">2023</a>)</cite> and hate speech detection <cite class="ltx_cite ltx_citemacro_cite">Akshay and Rahul (<a href="#bib.bib4" title="" class="ltx_ref">2024</a>)</cite>. These advancements illustrate the applicability of FM-FL across a wide range of scenarios in multilingual NLP.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>FM-FL for Speech</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">With the development of AI, researchers have also carried out many studies on speech-related FMs, <em id="S5.SS2.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> wav2vec 2.0 <cite class="ltx_cite ltx_citemacro_cite">Baevski et al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite> and Whisper <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib132" title="" class="ltx_ref">2023</a>)</cite>. In this field, the adaptation of FMs often relies on FL to facilitate scenarios where the audio data is privacy-sensitive. Compared to other data modalities, speech-related FM-FL applications especially attract excessive attention to the aspects of <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_bold">on-device training</span> and <span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_bold">personalization</span>, motivated by the following considerations: (1) Audio data is continually generated on end-devices such as mobile phones, and owned
by individual users—thus it should be processed locally, rather than being transferred elsewhere; (2) Although FL takes advantage of all user data to collectively train one model that maximizes speaker-independent accuracy, such a one-model-fits-all solution can be sub-optimal for individual users <cite class="ltx_cite ltx_citemacro_cite">Jia et al. (<a href="#bib.bib71" title="" class="ltx_ref">2023</a>)</cite>. Specific tasks in this field include Automatic Speech Recognition (ASR) <cite class="ltx_cite ltx_citemacro_cite">Azam et al. (<a href="#bib.bib7" title="" class="ltx_ref">2023b</a>)</cite> and Speech-to-Text (S2T) <cite class="ltx_cite ltx_citemacro_cite">Du et al. (<a href="#bib.bib39" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>FM-FL for Recommendation</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Federated Recommendation (FR) strives to capture underlying user preferences and recommend appropriate information to users while safeguarding data privacy <cite class="ltx_cite ltx_citemacro_cite">Bobadilla et al. (<a href="#bib.bib16" title="" class="ltx_ref">2013</a>); Zhang et al. (<a href="#bib.bib203" title="" class="ltx_ref">2023a</a>)</cite>. Typical FR systems consist of a server and multiple clients, where clients represent individual users or local data servers possessing smaller datasets and retaining private user information <cite class="ltx_cite ltx_citemacro_cite">Ammad-Ud-Din et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>. These clients collaborate to train a global model while ensuring their data privacy protection by abstaining from direct data sharing <cite class="ltx_cite ltx_citemacro_cite">Zeng et al. (<a href="#bib.bib200" title="" class="ltx_ref">2024</a>); Zhang et al. (<a href="#bib.bib203" title="" class="ltx_ref">2023a</a>)</cite>. Recently, LLM-based recommendations have been gaining increasing attention <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib178" title="" class="ltx_ref">2023b</a>)</cite> due to their strong capacities in language understanding and domain generalization. The benefits are mainly twofold: (1) LLMs mitigate the cold-start issue by utilizing textual descriptions to make recommendations without the need for extensive historical data <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib206" title="" class="ltx_ref">2023c</a>)</cite>; (2) The inherent transferability of LLMs allows them to apply cross-domain knowledge and side information to improve accuracy and relevance across diverse items and user interests <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a href="#bib.bib49" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">One straightforward way to adapt FMs for FR is by fine-tuning them with historical user-item data. More specifically, FedPEFT techniques such as adapter tuning <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib202" title="" class="ltx_ref">2024a</a>)</cite> and split learning <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib211" title="" class="ltx_ref">2024a</a>)</cite> can be employed to improve resource efficiency.
Apart from parameter fine-tuning, LLMs can also be adapted to assist the recommendation in a zero-shot paradigm through prompt engineering (<em id="S5.SS3.p2.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> without parameter tuning) <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a href="#bib.bib49" title="" class="ltx_ref">2023</a>)</cite>. For example, <cite class="ltx_cite ltx_citemacro_citet">Zeng et al. (<a href="#bib.bib200" title="" class="ltx_ref">2024</a>)</cite> proposed GPT-FedRec, a two-stage FR framework that leverages ChatGPT for its powerful zero-shot generalization ability. Firstly, GPT-FedRec facilitates hybrid retrieval by collaboratively training ID and text retrievers, after which the retrieved results are transformed into text prompts and submitted to GPT for re-ranking in the second stage. Additionally, <cite class="ltx_cite ltx_citemacro_citet">Guo et al. (<a href="#bib.bib57" title="" class="ltx_ref">2024a</a>)</cite> employed a pre-trained BERT to obtain the representation vectors of item descriptions, which are then fed into a recommender system as augmented input.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>FM-FL for Healthcare</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">FMs, especially LLMs, have been found to excel in healthcare applications, showcasing impressive capabilities in tasks like mental health analysis <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib190" title="" class="ltx_ref">2023b</a>)</cite>, disease diagnosis <cite class="ltx_cite ltx_citemacro_cite">Panagoulias et al. (<a href="#bib.bib124" title="" class="ltx_ref">2024</a>)</cite>, and drug discovery <cite class="ltx_cite ltx_citemacro_cite">Chenthamarakshan et al. (<a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite>. However, it raises privacy concerns to upload the health information of patients <cite class="ltx_cite ltx_citemacro_cite">Tang et al. (<a href="#bib.bib161" title="" class="ltx_ref">2023</a>)</cite> into a commercial server that supports the FMs. Meanwhile, FL has consistently received widespread attention in the healthcare domain <cite class="ltx_cite ltx_citemacro_cite">Lincy and Kowshalya (<a href="#bib.bib101" title="" class="ltx_ref">2020</a>); Rieke et al. (<a href="#bib.bib140" title="" class="ltx_ref">2020</a>); Joshi et al. (<a href="#bib.bib78" title="" class="ltx_ref">2022</a>)</cite>, driven by the need for collaborative model training across different medical institutions without compromising patient data privacy. By breaking the barriers of private data availability, the FM-FL paradigm shows the potential to further harness the power of FMs in the healthcare domain.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">A recent study <cite class="ltx_cite ltx_citemacro_cite">Shin et al. (<a href="#bib.bib148" title="" class="ltx_ref">2023a</a>)</cite> presents a mobile mental health monitoring system, FedTherapist, which leverages user speech
and keyboard input to fine-tune FMs with FL, demonstrating superior accuracy in mental health
prediction tasks such as depression,
stress, and mood prediction. Another representative study <cite class="ltx_cite ltx_citemacro_cite">Feng et al. (<a href="#bib.bib45" title="" class="ltx_ref">2023a</a>)</cite> focuses on Magnetic Resonance Imaging (MRI) reconstruction, which involves retrieving a complex-valued image from its under-sampled signal. The authors adopted an FM pre-trained on public datasets and trained visual prompts from decentralized clinical datasets via a personalized FL mechanism, thereby reducing communication costs
and achieving competitive performance on limited local data.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">Despite the efforts, it has been shown that FMs in healthcare risk generating misleading information due to their imperfect understanding of complex medical data <cite class="ltx_cite ltx_citemacro_cite">Jeblick et al. (<a href="#bib.bib69" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Future Directions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Although recent work has already begun to address the challenges
discussed in Section <a href="#S3.SS2" title="3.2 Core Challenges ‣ 3 FM-FL: Motivation &amp; Challenges ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, many critical open directions are yet to be explored. Here, we outline several representative ones.</p>
</div>
<section id="S6.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Multimodal FM-FL</h5>

<div id="S6.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p1.1" class="ltx_p">With the development of mobile technology and IoT infrastructures <cite class="ltx_cite ltx_citemacro_cite">Brunete et al. (<a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>, numerous edge devices produce data from a range of modalities, such as sensory, visual, and audio. In the era of FMs, the success of LLMs and their multimodal
derivatives <cite class="ltx_cite ltx_citemacro_cite">Ramesh et al. (<a href="#bib.bib134" title="" class="ltx_ref">2021</a>); Google (<a href="#bib.bib54" title="" class="ltx_ref">2023</a>); OpenAI (<a href="#bib.bib123" title="" class="ltx_ref">2024</a>)</cite> have demonstrated the potential of multimodal FMs. The potential opportunities and challenges for multimodal FM-FL have yet to be explored.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Continual Learning</h5>

<div id="S6.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p1.1" class="ltx_p">Continual learning enables models to adapt to new data over time, improving their performance and accuracy. By incorporating new data into the model training process, FL and FMs can continuously improve and adapt to changing environments and user needs <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib191" title="" class="ltx_ref">2024a</a>)</cite>. Future directions may involve leveraging transfer learning techniques in continual learning for FL and FMs. Models can transfer knowledge from previous tasks or domains to new ones, enabling more efficient adaptation <cite class="ltx_cite ltx_citemacro_cite">Good et al. (<a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Efficient Federated Black-Box Tuning</h5>

<div id="S6.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px3.p1.1" class="ltx_p">In scenarios where gradient access is unavailable, preliminary efforts have focused on federated fine-tuning black-box FMs <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib100" title="" class="ltx_ref">2023</a>); Sun et al. (<a href="#bib.bib156" title="" class="ltx_ref">2024a</a>); Lu et al. (<a href="#bib.bib111" title="" class="ltx_ref">2023b</a>); Rui et al. (<a href="#bib.bib143" title="" class="ltx_ref">2024</a>)</cite> utilizing ZOO. However, ZOO’s noticeably slower convergence rates, especially in high-dimensional contexts compared to gradient-based methods <cite class="ltx_cite ltx_citemacro_cite">Golovin et al. (<a href="#bib.bib52" title="" class="ltx_ref">2020</a>)</cite>, indicate an important direction for further research. The impact of these slower convergence rates on overall efficiency and computational load within FL, particularly concerning large-scale FMs, has not been adequately investigated and understood.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">FL with AI-Generated Content</h5>

<div id="S6.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px4.p1.1" class="ltx_p">AI-Generated Content (AIGC) denotes content produced via advanced generative FMs <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib177" title="" class="ltx_ref">2023a</a>)</cite>. The strong generative capability of FMs offers the advantage of rapidly automating the creation of inexhaustible synthetic data.
This capability positions AIGC as a valuable supplementary data source for model training and evaluation in many tasks <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a href="#bib.bib187" title="" class="ltx_ref">2024c</a>)</cite>. Despite some efforts <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib205" title="" class="ltx_ref">2023b</a>)</cite>, more potential opportunities and challenges for AIGC-aided FL have yet to be explored.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusions</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this survey, we have meticulously surveyed the intersection of FM and FL. We identified core challenges in efficiency, adaptability, and trustworthiness and proposed a comprehensive taxonomy of techniques in response to these challenges. In addition, we discussed future directions and applications in this research field, hoping to attract more breakthroughs in future research.</p>
</div>
<section id="S7.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Limitations</h3>

<div id="S7.SSx1.p1" class="ltx_para">
<p id="S7.SSx1.p1.1" class="ltx_p">FM and FL are very fast-moving fields. We have put a lot of effort into including the latest research efforts in the community in this survey. Therefore, we believe that our survey will help to inspire and push further research and innovation in these important areas. Our survey does not focus on experimental evaluation of the available ideas and systems. We believe that would be an important next step that we are leaving for future work.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acar et al. (2021)</span>
<span class="ltx_bibblock">
Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=B7v4QMR6Z9w" title="" class="ltx_ref ltx_href">Federated learning based on dynamic regularization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adi et al. (2018)</span>
<span class="ltx_bibblock">
Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.usenix.org/conference/usenixsecurity18/presentation/adi" title="" class="ltx_ref ltx_href">Turning your weakness into a strength: Watermarking deep neural networks by backdooring</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">27th USENIX Security Symposium (USENIX Security 18)</em>, pages 1615–1631, Baltimore, MD. USENIX Association.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aghajanyan et al. (2021)</span>
<span class="ltx_bibblock">
Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.568" title="" class="ltx_ref ltx_href">Intrinsic dimensionality explains the effectiveness of language model fine-tuning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 7319–7328, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akshay and Rahul (2024)</span>
<span class="ltx_bibblock">
Singh Akshay and Thakur Rahul. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://2024.naacl.org/program/accepted_papers/" title="" class="ltx_ref ltx_href">Generalizable multilingual hate speech detection on low resource indian languages using fair selection in federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ammad-Ud-Din et al. (2019)</span>
<span class="ltx_bibblock">
Muhammad Ammad-Ud-Din, Elena Ivannikova, Suleiman A. Khan, Were Oyomno, Qiang Fu, Kuan Eeik Tan, and Adrian Flanagan. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1901.09888" title="" class="ltx_ref ltx_href">Federated collaborative filtering for privacy-preserving personalized recommendation system</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.09888</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azam et al. (2023a)</span>
<span class="ltx_bibblock">
Sheikh Shams Azam, Tatiana Likhomanenko, Martin Pelikan, and Jan “Honza” Silovsky. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ASRU57964.2023.10389620" title="" class="ltx_ref ltx_href">Importance of smoothness induced by optimizers in fl4asr: Towards understanding federated learning for end-to-end asr</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, pages 1–8.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azam et al. (2023b)</span>
<span class="ltx_bibblock">
Sheikh Shams Azam, Martin Pelikan, Vitaly Feldman, Kunal Talwar, Jan Silovsky, and Tatiana Likhomanenko. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=ozN92d7CHX" title="" class="ltx_ref ltx_href">Federated learning for speech recognition: Revisiting current trends towards large-scale ASR</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International Workshop on Federated Learning in the Age of Foundation Models in Conjunction with NeurIPS 2023</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Babakniya et al. (2023a)</span>
<span class="ltx_bibblock">
Sara Babakniya, Ahmed Elkordy, Yahya Ezzeldin, Qingfeng Liu, Kee-Bong Song, MOSTAFA EL-Khamy, and Salman Avestimehr. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=06quMTmtRV" title="" class="ltx_ref ltx_href">SLoRA: Federated parameter efficient fine-tuning of language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">International Workshop on Federated Learning in the Age of Foundation Models in Conjunction with NeurIPS 2023</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Babakniya et al. (2023b)</span>
<span class="ltx_bibblock">
Sara Babakniya, Souvik Kundu, Saurav Prakash, Yue Niu, and Salman Avestimehr. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=iHyhdpsnyi" title="" class="ltx_ref ltx_href">Revisiting sparsity hunting in federated learning: Why does sparsity consensus matter?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et al. (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf" title="" class="ltx_ref ltx_href">wav2vec 2.0: A framework for self-supervised learning of speech representations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 33, pages 12449–12460. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bagdasaryan et al. (2020)</span>
<span class="ltx_bibblock">
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v108/bagdasaryan20a.html" title="" class="ltx_ref ltx_href">How to backdoor federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence and Statistics</em>, pages 2938–2948. PMLR.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2024a)</span>
<span class="ltx_bibblock">
Jiamu Bai, Daoyuan Chen, Bingchen Qian, Liuyi Yao, and Yaliang Li. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.11505" title="" class="ltx_ref ltx_href">Federated fine-tuning of large language models under heterogeneous language tasks and client resources</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.11505</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2024b)</span>
<span class="ltx_bibblock">
Sikai Bai, Jie Zhang, Shuaicheng Li, Song Guo, Jingcai Guo, Jun Hou, Tao Han, and Xiaocheng Lu. 2024b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2403.08506" title="" class="ltx_ref ltx_href">Diprompt: Disentangled prompt tuning for multiple latent domain generalization in federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.08506</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ben Zaken et al. (2022)</span>
<span class="ltx_bibblock">
Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-short.1" title="" class="ltx_ref ltx_href">BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, pages 1–9, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blanchard et al. (2017)</span>
<span class="ltx_bibblock">
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. 2017.

</span>
<span class="ltx_bibblock">Machine learning with adversaries: Byzantine tolerant gradient descent.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st International Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bobadilla et al. (2013)</span>
<span class="ltx_bibblock">
J. Bobadilla, F. Ortega, A. Hernando, and A. Gutiérrez. 2013.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1016/j.knosys.2013.03.012" title="" class="ltx_ref ltx_href">Recommender systems survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Knowledge-Based Systems</em>, 46:109–132.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bommasani et al. (2021)</span>
<span class="ltx_bibblock">
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2108.07258" title="" class="ltx_ref ltx_href">On the opportunities and risks of foundation models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.07258</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. (2021)</span>
<span class="ltx_bibblock">
Kallista Bonawitz, Peter Kairouz, Brendan McMahan, and Daniel Ramage. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3494834.3500240" title="" class="ltx_ref ltx_href">Federated learning and privacy: Building privacy-preserving systems for machine learning and data science on decentralized data</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Queue</em>, 19(5):87–114.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown et al. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" title="" class="ltx_ref ltx_href">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 33, pages 1877–1901. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brunete et al. (2021)</span>
<span class="ltx_bibblock">
Alberto Brunete, Ernesto Gambao, Miguel Hernando, and Raquel Cedazo. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.mdpi.com/1424-8220/21/6/2212" title="" class="ltx_ref ltx_href">Smart assistive architecture for the integration of iot devices, robotic systems, and multimodal interfaces in healthcare environments</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, 21(6):2212.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bu et al. (2022)</span>
<span class="ltx_bibblock">
Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, and George Karypis. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=6Bo1vhoHolh" title="" class="ltx_ref ltx_href">Differentially private bias-term only fine-tuning of foundation models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS 2022</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck et al. (2023)</span>
<span class="ltx_bibblock">
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.12712" title="" class="ltx_ref ltx_href">Sparks of artificial general intelligence: Early experiments with gpt-4</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.12712</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. (2023)</span>
<span class="ltx_bibblock">
Dongqi Cai, Yaozong Wu, Shangguang Wang, Felix Xiaozhu Lin, and Mengwei Xu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3570361.3592505" title="" class="ltx_ref ltx_href">Efficient federated learning for modern nlp</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th Annual International Conference on Mobile Computing and Networking</em>, ACM MobiCom ’23, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. (2021)</span>
<span class="ltx_bibblock">
Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. 2021.

</span>
<span class="ltx_bibblock">Fltrust: Byzantine-robust federated learning via trust bootstrapping.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ISOC Network and Distributed System Security Symposium (NDSS)</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CCPA (2023)</span>
<span class="ltx_bibblock">
CCPA. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://oag.ca.gov/privacy/ccpa" title="" class="ltx_ref ltx_href">California consumer privacy act (ccpa)</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chatterjee et al. (2023)</span>
<span class="ltx_bibblock">
Pushpita Chatterjee, Debashis Das, and Danda B Rawat. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.12944" title="" class="ltx_ref ltx_href">Use of federated learning and blockchain towards securing financial services</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.12944</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Haokun Chen, Yao Zhang, Denis Krompass, Jindong Gu, and Volker Tresp. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1609/aaai.v38i10.29007" title="" class="ltx_ref ltx_href">Feddat: An approach for foundation model finetuning in multi-modal heterogeneous federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, 38(10):11285–11293.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2017)</span>
<span class="ltx_bibblock">
Yudong Chen, Lili Su, and Jiaming Xu. 2017.

</span>
<span class="ltx_bibblock">Distributed statistical machine learning in adversarial settings: Byzantine gradient descent.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Measurement and Analysis of Computing Systems</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chenthamarakshan et al. (2023)</span>
<span class="ltx_bibblock">
Vijil Chenthamarakshan, Samuel C. Hoffman, C. David Owen, Petra Lukacik, Claire Strain-Damerell, Daren Fearon, Tika R. Malla, Anthony Tumber, Christopher J. Schofield, Helen M.E. Duyvesteyn, Wanwisa Dejnirattisai, Loic Carrique, Thomas S. Walter, Gavin R. Screaton, Tetiana Matviiuk, Aleksandra Mojsilovic, Jason Crain, Martin A. Walsh, David I. Stuart, and Payel Das. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1126/sciadv.adg7865" title="" class="ltx_ref ltx_href">Accelerating drug target inhibitor discovery with a deep generative foundation model</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Science Advances</em>, 9(25):eadg7865.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al. (2024)</span>
<span class="ltx_bibblock">
Yae Jee Cho, Luyang Liu, Zheng Xu, Aldi Fahrezi, and Gauri Joshi. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2401.06432" title="" class="ltx_ref ltx_href">Heterogeneous lora for federated fine-tuning of on-device foundation models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.06432</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al. (2024)</span>
<span class="ltx_bibblock">
Yun-Wei Chu, Dong-Jun Han, and Christopher G. Brinton. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3589335.3651931" title="" class="ltx_ref ltx_href">Only send what you need: Learning to communicate efficiently in federated multilingual machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Companion Proceedings of the ACM on Web Conference 2024</em>, page 1548–1557, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.747" title="" class="ltx_ref ltx_href">Unsupervised cross-lingual representation learning at scale</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 8440–8451, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2024)</span>
<span class="ltx_bibblock">
Wenlong Deng, Christos Thrampoulidis, and Xiaoxiao Li. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2310.18285" title="" class="ltx_ref ltx_href">Unlocking the potential of prompt-tuning in bridging generalized and personalized federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1423" title="" class="ltx_ref ltx_href">BERT: Pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diao et al. (2021)</span>
<span class="ltx_bibblock">
Enmao Diao, Jie Ding, and Vahid Tarokh. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=TNkPBBYFkXg" title="" class="ltx_ref ltx_href">Hetero{fl}: Computation and communication efficient federated learning for heterogeneous clients</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. (2023)</span>
<span class="ltx_bibblock">
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1038/s42256-023-00626-4" title="" class="ltx_ref ltx_href">Parameter-efficient fine-tuning of large-scale pre-trained language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, 5(3):220–235.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. (2023)</span>
<span class="ltx_bibblock">
Chenhe Dong, Yuexiang Xie, Bolin Ding, Ying Shen, and Yaliang Li. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-emnlp.976" title="" class="ltx_ref ltx_href">Tunable soft prompts are messengers in federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 14665–14675, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2021)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=YicbFdNTTy" title="" class="ltx_ref ltx_href">An image is worth 16x16 words: Transformers for image recognition at scale</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2024)</span>
<span class="ltx_bibblock">
Yichao Du, Zhirui Zhang, Linan Yue, Xu Huang, Yuqing Zhang, Tong Xu, Linli Xu, and Enhong Chen. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP48485.2024.10447662" title="" class="ltx_ref ltx_href">Communication-efficient personalized federated learning for speech-to-text tasks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 10001–10005.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duchi et al. (2015)</span>
<span class="ltx_bibblock">
John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. 2015.

</span>
<span class="ltx_bibblock">Optimal rates for zero-order convex optimization: The power of two function evaluations.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Theory</em>, 61(5):2788–2806.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2023)</span>
<span class="ltx_bibblock">
Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, and Qiang Yang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.10049" title="" class="ltx_ref ltx_href">Fate-llm: A industrial grade federated learning framework for large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.10049</em>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2020)</span>
<span class="ltx_bibblock">
Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.usenix.org/system/files/sec20summer_fang_prepub.pdf" title="" class="ltx_ref ltx_href">Local model poisoning attacks to byzantine-robust federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">29th USENIX Security Symposium (USENIX Security 20)</em>, pages 1605–1622.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2022)</span>
<span class="ltx_bibblock">
Wenzhi Fang, Ziyi Yu, Yuning Jiang, Yuanming Shi, Colin N. Jones, and Yong Zhou. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TSP.2022.3214122" title="" class="ltx_ref ltx_href">Communication-efficient stochastic zeroth-order optimization for federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Signal Processing</em>, 70:5058–5073.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">FedML (2023)</span>
<span class="ltx_bibblock">
FedML. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://blog.fedml.ai/releasing-fedllm-build-your-own-large-language-models-on-proprietary-data-using-the-fedml-platform/" title="" class="ltx_ref ltx_href">Fedllm</a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2023a)</span>
<span class="ltx_bibblock">
Chun-Mei Feng, Bangjun Li, Xinxing Xu, Yong Liu, Huazhu Fu, and Wangmeng Zuo. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR52729.2023.00779" title="" class="ltx_ref ltx_href">Learning federated visual prompt in null space for mri reconstruction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 8064–8073.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2023b)</span>
<span class="ltx_bibblock">
Haozhe Feng, Tianyu Pang, Chao Du, Wei Chen, Shuicheng Yan, and Min Lin. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2301.12195" title="" class="ltx_ref ltx_href">Does federated learning really need backpropagation?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.12195</em>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2023c)</span>
<span class="ltx_bibblock">
Xiachong Feng, Xiaocheng Feng, Xiyuan Du, Min-Yen Kan, and Bing Qin. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.03275" title="" class="ltx_ref ltx_href">Adapter-based selective knowledge distillation for federated multi-domain meeting summarization</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.03275</em>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frankle and Carbin (2019)</span>
<span class="ltx_bibblock">
Jonathan Frankle and Michael Carbin. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=rJl-b3RcF7" title="" class="ltx_ref ltx_href">The lottery ticket hypothesis: Finding sparse, trainable neural networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.14524" title="" class="ltx_ref ltx_href">Chat-rec: Towards interactive and explainable llms-augmented recommender system</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.14524</em>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GDPR (2016)</span>
<span class="ltx_bibblock">
GDPR. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://data.europa.eu/eli/reg/2016/679/2016-05-04" title="" class="ltx_ref ltx_href">Regulation (eu) 2016/679 of the european parliament and of the council</a>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geiping et al. (2020)</span>
<span class="ltx_bibblock">
Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/c4ede56bbd98819ae6112b20ac6bf145-Paper.pdf" title="" class="ltx_ref ltx_href">Inverting gradients - how easy is it to break privacy in federated learning?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 33, pages 16937–16947. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Golovin et al. (2020)</span>
<span class="ltx_bibblock">
Daniel Golovin, John Karro, Greg Kochanski, Chansoo Lee, Xingyou Song, and Qiuyi Zhang. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=Skep6TVYDB" title="" class="ltx_ref ltx_href">Gradientless descent: High-dimensional zeroth-order optimization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Good et al. (2023)</span>
<span class="ltx_bibblock">
Jack Good, Jimit Majmudar, Christophe Dupuy, Jixuan Wang, Charith Peris, Clement Chung, Richard Zemel, and Rahul Gupta. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.emnlp-industry.32" title="" class="ltx_ref ltx_href">Coordinated replay sample selection for continual federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track</em>, pages 331–342, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2023)</span>
<span class="ltx_bibblock">
Gemini Team Google. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2312.11805" title="" class="ltx_ref ltx_href">Gemini: a family of highly capable multimodal models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.11805</em>.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gorbunov et al. (2023)</span>
<span class="ltx_bibblock">
Eduard Gorbunov, Samuel Horváth, Peter Richtárik, and Gauthier Gidel. 2023.

</span>
<span class="ltx_bibblock">Variance reduction is an antidote to byzantines: Better rates, weaker assumptions and communication compression as a cherry on the top.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gunasekar et al. (2023)</span>
<span class="ltx_bibblock">
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2306.11644" title="" class="ltx_ref ltx_href">Textbooks are all you need</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.11644</em>.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2024a)</span>
<span class="ltx_bibblock">
Lei Guo, Ziang Lu, Junliang Yu, Quoc Viet Hung Nguyen, and Hongzhi Yin. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3589334.3645337" title="" class="ltx_ref ltx_href">Prompt-enhanced federated content representation learning for cross-domain recommendation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Web Conference 2024</em>, WWW ’24, page 3139–3149, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2023)</span>
<span class="ltx_bibblock">
Tao Guo, Song Guo, Junxiao Wang, Xueyang Tang, and Wenchao Xu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TMC.2023.3302410" title="" class="ltx_ref ltx_href">Promptfl: Let federated participants cooperatively learn prompts instead of models - federated learning in age of foundation model</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Mobile Computing</em>, pages 1–15.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo and Yu (2022)</span>
<span class="ltx_bibblock">
Xu Guo and Han Yu. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2211.03154" title="" class="ltx_ref ltx_href">On the domain adaptation and generalization of pretrained language models: A survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.03154</em>.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2024b)</span>
<span class="ltx_bibblock">
Zhihan Guo, Yifei Zhang, Zhuo Zhang, Zenglin Xu, and Irwin King. 2024b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2024.findings-naacl.98" title="" class="ltx_ref ltx_href">FedLFC: Towards efficient federated multilingual modeling with LoRA-based language family clustering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: NAACL 2024</em>, pages 1519–1528, Mexico City, Mexico. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. (2022)</span>
<span class="ltx_bibblock">
Samyak Gupta, Yangsibo Huang, Zexuan Zhong, Tianyu Gao, Kai Li, and Danqi Chen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/35b5c175e139bff5f22a5361270fce87-Paper-Conference.pdf" title="" class="ltx_ref ltx_href">Recovering private text in federated learning of language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 35, pages 8130–8143. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan et al. (2020)</span>
<span class="ltx_bibblock">
Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.740" title="" class="ltx_ref ltx_href">Don’t stop pretraining: Adapt language models to domains and tasks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 8342–8360, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ha et al. (2017)</span>
<span class="ltx_bibblock">
David Ha, Andrew M. Dai, and Quoc V. Le. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=rkpACe1lx" title="" class="ltx_ref ltx_href">Hypernetworks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hansen and Ostermeier (2001)</span>
<span class="ltx_bibblock">
Nikolaus Hansen and Andreas Ostermeier. 2001.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/106365601750190398" title="" class="ltx_ref ltx_href">Completely derandomized self-adaptation in evolution strategies</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Evolutionary Computation</em>, 9(2):159–195.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2020)</span>
<span class="ltx_bibblock">
Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Xinghua Zhu, Jianzong Wang, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram, and Salman Avestimehr. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2007.13518" title="" class="ltx_ref ltx_href">Fedml: A research library and benchmark for federated machine learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.13518</em>.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et al. (2019)</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v97/houlsby19a.html" title="" class="ltx_ref ltx_href">Parameter-efficient transfer learning for NLP</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 36th International Conference on Machine Learning</em>, volume 97 of <em id="bib.bib66.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 2790–2799. PMLR.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2022)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="" class="ltx_ref ltx_href">LoRA: Low-rank adaptation of large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2024)</span>
<span class="ltx_bibblock">
Xumin Huang, Peichun Li, Hongyang Du, Jiawen Kang, Dusit Niyato, Dong In Kim, and Yuan Wu. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/MNET.2024.3353377" title="" class="ltx_ref ltx_href">Federated learning-empowered ai-generated content in wireless networks</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">IEEE Network</em>, pages 1–1.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jeblick et al. (2024)</span>
<span class="ltx_bibblock">
Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa Stüber, Johanna Topalis, Tobias Weber, Philipp Wesp, Bastian Oliver Sabel, Jens Ricke, et al. 2024.

</span>
<span class="ltx_bibblock">Chatgpt makes medicine easy to swallow: an exploratory case study on simplified radiology reports.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">European radiology</em>, 34(5):2817–2825.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jere et al. (2020)</span>
<span class="ltx_bibblock">
Malhar S Jere, Tyler Farnan, and Farinaz Koushanfar. 2020.

</span>
<span class="ltx_bibblock">A taxonomy of attacks on federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">IEEE Security &amp; Privacy</em>, 19(2).

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. (2023)</span>
<span class="ltx_bibblock">
Junteng Jia, Ke Li, Mani Malek, Kshitiz Malik, Jay Mahadeokar, Ozlem Kalinli, and Frank Seide. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ASRU57964.2023.10389738" title="" class="ltx_ref ltx_href">Joint federated learning and personalization for on-device asr</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, pages 1–8.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. (2022)</span>
<span class="ltx_bibblock">
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-031-19827-4_41" title="" class="ltx_ref ltx_href">Visual prompt tuning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Computer Vision – ECCV 2022</em>, pages 709–727, Cham. Springer Nature Switzerland.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023a)</span>
<span class="ltx_bibblock">
Jingang Jiang, Xiangyang Liu, and Chenyou Fan. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2307.13896" title="" class="ltx_ref ltx_href">Low-parameter federated learning with large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.13896</em>.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023b)</span>
<span class="ltx_bibblock">
Lekang Jiang, Filip Svoboda, and Nicholas Donald Lane. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=ESCL5T3EgV" title="" class="ltx_ref ltx_href">FDAPT: Federated domain-adaptive pre-training for language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">International Workshop on Federated Learning in the Age of Foundation Models in Conjunction with NeurIPS 2023</em>.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023c)</span>
<span class="ltx_bibblock">
Yuang Jiang, Shiqiang Wang, Víctor Valls, Bong Jun Ko, Wei-Han Lee, Kin K. Leung, and Leandros Tassiulas. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TNNLS.2022.3166101" title="" class="ltx_ref ltx_href">Model pruning enables efficient federated learning on edge devices</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, 34(12):10374–10386.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jo and Gebru (2020)</span>
<span class="ltx_bibblock">
Eun Seo Jo and Timnit Gebru. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3351095.3372829" title="" class="ltx_ref ltx_href">Lessons from archives: Strategies for collecting sociocultural data in machine learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>, FAcctT ’20, pages 306–316, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017)</span>
<span class="ltx_bibblock">
Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00065" title="" class="ltx_ref ltx_href">Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 5:339–351.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. (2022)</span>
<span class="ltx_bibblock">
Madhura Joshi, Ankit Pal, and Malaikannan Sankarasubbu. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3533708" title="" class="ltx_ref ltx_href">Federated learning for healthcare domain - pipeline, applications and challenges</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Comput. Healthcare</em>, 3(4).

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al. (2021)</span>
<span class="ltx_bibblock">
Peter Kairouz et al. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1561/2200000083" title="" class="ltx_ref ltx_href">Advances and open problems in federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Foundations and Trends in Machine Learning</em>, 14(1–2):1–210.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al. (2024)</span>
<span class="ltx_bibblock">
Yan Kang, Tao Fan, Hanlin Gu, Xiaojin Zhang, Lixin Fan, and Qiang Yang. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2311.17431" title="" class="ltx_ref ltx_href">Grounding foundation models through federated transfer learning: A general framework</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.17431</em>.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2023)</span>
<span class="ltx_bibblock">
Yeachan Kim, Junho Kim, Wing-Lam Mok, Jun-Hyung Park, and SangKeun Lee. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-acl.75" title="" class="ltx_ref ltx_href">Client-customized adaptation for parameter-efficient federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 1159–1172, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirillov et al. (2023)</span>
<span class="ltx_bibblock">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.48550/arXiv.2304.02643" title="" class="ltx_ref ltx_href">Segment anything</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.02643</em>.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuang et al. (2023)</span>
<span class="ltx_bibblock">
Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, and Jingren Zhou. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2309.00363" title="" class="ltx_ref ltx_href">Federatedscope-llm: A comprehensive package for fine-tuning large language models in federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.00363</em>.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester et al. (2021)</span>
<span class="ltx_bibblock">
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.243" title="" class="ltx_ref ltx_href">The power of scale for parameter-efficient prompt tuning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 3045–3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021a)</span>
<span class="ltx_bibblock">
Ang Li, Jingwei Sun, Binghui Wang, Lin Duan, Sicheng Li, Yiran Chen, and Hai Li. 2021a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3453142.3492909" title="" class="ltx_ref ltx_href">Lotteryfl: Empower edge intelligence with personalized and communication-efficient federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">2021 IEEE/ACM Symposium on Edge Computing (SEC)</em>, pages 68–79.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024a)</span>
<span class="ltx_bibblock">
Guanghao Li, Wansen Wu, Yan Sun, Li Shen, Baoyuan Wu, and Dacheng Tao. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=dUVejidXO7" title="" class="ltx_ref ltx_href">Visual prompt based personalized federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021b)</span>
<span class="ltx_bibblock">
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf" title="" class="ltx_ref ltx_href">Align before fuse: Vision and language representation learning with momentum distillation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 34, pages 9694–9705. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022)</span>
<span class="ltx_bibblock">
Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICDE53745.2022.00077" title="" class="ltx_ref ltx_href">Federated learning on non-iid data silos: An experimental study</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">2022 IEEE 38th International Conference on Data Engineering (ICDE)</em>, pages 965–978.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Shenghui Li, Edith Ngai, and Thiemo Voigt. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TII.2021.3128164" title="" class="ltx_ref ltx_href">Byzantine-robust aggregation in federated learning empowered industrial iot</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Industrial Informatics</em>, 19(2):1165–1175.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024b)</span>
<span class="ltx_bibblock">
Shenghui Li, Edith Ngai, Fanghua Ye, Li Ju, Tianru Zhang, and Thiemo Voigt. 2024b.

</span>
<span class="ltx_bibblock">Blades: A unified benchmark suite for byzantine attacks and defenses in federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">2024 IEEE/ACM Ninth International Conference on Internet-of-Things Design and Implementation (IoTDI)</em>.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Shenghui Li, Edith C.-H. Ngai, and Thiemo Voigt. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TBDATA.2023.3237397" title="" class="ltx_ref ltx_href">An experimental study of byzantine-robust aggregation schemes in federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Big Data</em>, pages 1–13.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlsys.org/paper_files/paper/2020/file/1f5fe83998a09396ebe6477d9475ba0c-Paper.pdf" title="" class="ltx_ref ltx_href">Federated optimization in heterogeneous networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems</em>, volume 2, pages 429–450.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Wang (2024)</span>
<span class="ltx_bibblock">
Xi Li and Jiaqi Wang. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.01857" title="" class="ltx_ref ltx_href">Position paper: Assessing robustness, privacy, and fairness in federated learning integrated with foundation models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.01857</em>.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023c)</span>
<span class="ltx_bibblock">
Xi Li, Songhe Wang, Chen Wu, Hao Zhou, and Jiaqi Wang. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=BrcHuO2BVc" title="" class="ltx_ref ltx_href">Backdoor threats from compromised foundation models to federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">International Workshop on Federated Learning in the Age of Foundation Models in Conjunction with NeurIPS 2023</em>.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023d)</span>
<span class="ltx_bibblock">
Xi Li, Chen Wu, and Jiaqi Wang. 2023d.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2311.18350" title="" class="ltx_ref ltx_href">Unveiling backdoor risks brought by foundation models in heterogeneous federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.18350</em>.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024c)</span>
<span class="ltx_bibblock">
Xi Li, Chen Wu, and Jiaqi Wang. 2024c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://link.springer.com/chapter/10.1007/978-981-97-2259-4_13" title="" class="ltx_ref ltx_href">Unveiling backdoor risks brought by foundation models in heterogeneous federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">Advances in Knowledge Discovery and Data Mining</em>, pages 168–181, Singapore. Springer Nature Singapore.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Liang (2021)</span>
<span class="ltx_bibblock">
Xiang Lisa Li and Percy Liang. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.353" title="" class="ltx_ref ltx_href">Prefix-tuning: Optimizing continuous prompts for generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 4582–4597. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Chen (2021)</span>
<span class="ltx_bibblock">
Zan Li and Li Chen. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/WCSP52459.2021.9613620" title="" class="ltx_ref ltx_href">Communication-efficient decentralized zeroth-order method on heterogeneous data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">2021 13th International Conference on Wireless Communications and Signal Processing (WCSP)</em>, pages 1–6.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lialin et al. (2023)</span>
<span class="ltx_bibblock">
Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.48550/arXiv.2303.15647" title="" class="ltx_ref ltx_href">Scaling down to scale up: A guide to parameter-efficient fine-tuning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.15647</em>.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023)</span>
<span class="ltx_bibblock">
Zihao Lin, Yan Sun, Yifan Shi, Xueqian Wang, Lifu Huang, Li Shen, and Dacheng Tao. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2310.03123" title="" class="ltx_ref ltx_href">Efficient federated prompt tuning for black-box large pre-trained models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2310.03123.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lincy and Kowshalya (2020)</span>
<span class="ltx_bibblock">
M Lincy and A Meena Kowshalya. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:234514776" title="" class="ltx_ref ltx_href">Early detection of type-2 diabetes using federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">International Journal of Scientific Research in Science, Engineering and Technology</em>, 12:257–267.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ling et al. (2024)</span>
<span class="ltx_bibblock">
Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, and Ying Shen. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.05926" title="" class="ltx_ref ltx_href">On the convergence of zeroth-order federated tuning in large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.05926</em>.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lit et al. (2022)</span>
<span class="ltx_bibblock">
Zhengyang Lit, Shijing Sit, Jianzong Wang, and Jing Xiao. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/IJCNN55064.2022.9892845" title="" class="ltx_ref ltx_href">Federated split bert for heterogeneous text classification</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">2022 International Joint Conference on Neural Networks (IJCNN)</em>, pages 1–8.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3560815" title="" class="ltx_ref ltx_href">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">ACM Comput. Surv.</em>, 55(9).

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2020)</span>
<span class="ltx_bibblock">
Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O. Hero III, and Pramod K. Varshney. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/MSP.2020.3003837" title="" class="ltx_ref ltx_href">A primer on zeroth-order optimization in signal processing and machine learning: Principals, recent advances, and applications</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>, 37(5):43–54.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Tao Liu, Zhi Wang, Hui He, Wei Shi, Liangliang Lin, Ran An, and Chenhao Li. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3390/app13105877" title="" class="ltx_ref ltx_href">Efficient and secure federated learning for financial applications</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, 13(10).

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023c)</span>
<span class="ltx_bibblock">
Xiao-Yang Liu, Rongyi Zhu, Daochen Zha, Jiechao Gao, Shan Zhong, and Meikang Qiu. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2312.17493" title="" class="ltx_ref ltx_href">Differentially private low-rank adaptation of large language model using federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.17493</em>.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023d)</span>
<span class="ltx_bibblock">
Yi Liu, Xiaohan Bi, Lei Li, Sishuo Chen, Wenkai Yang, and Xu Sun. 2023d.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-acl.327" title="" class="ltx_ref ltx_href">Communication efficient federated learning for multilingual neural machine translation with adapter</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 5315–5328, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1907.11692" title="" class="ltx_ref ltx_href">Roberta: A robustly optimized bert pretraining approach</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2023a)</span>
<span class="ltx_bibblock">
Wang Lu, Xixu Hu, Jindong Wang, and Xing Xie. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://sites.computer.org/debull/A23mar/p52.pdf" title="" class="ltx_ref ltx_href">Fedclip: Fast generalization and personalization for CLIP in federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">IEEE Data Eng. Bull.</em>, 46(1):52–66.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2023b)</span>
<span class="ltx_bibblock">
Wang Lu, Hao Yu, Jindong Wang, Damien Teney, Haohan Wang, Yiqiang Chen, Qiang Yang, Xing Xie, and Xiangyang Ji. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2310.05143" title="" class="ltx_ref ltx_href">Zoopfl: Exploring black-box foundation models for personalized federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.05143</em>.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2022)</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Han Yu, Xingjun Ma, Chen Chen, Lichao Sun, Jun Zhao, Qiang Yang, and Philip S. Yu. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TNNLS.2022.3216981" title="" class="ltx_ref ltx_href">Privacy and robustness in federated learning: Attacks and defenses</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, pages 1–21.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malaviya et al. (2023)</span>
<span class="ltx_bibblock">
Shubham Malaviya, Manish Shukla, and Sachin Lodha. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v232/malaviya23a.html" title="" class="ltx_ref ltx_href">Reducing communication overhead in federated learning for pre-trained language models using parameter-efficient finetuning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 2nd Conference on Lifelong Learning Agents</em>, volume 232 of <em id="bib.bib113.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 456–469. PMLR.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malladi et al. (2023a)</span>
<span class="ltx_bibblock">
Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev Arora. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=CcsdvOOzMp" title="" class="ltx_ref ltx_href">Fine-tuning language models with just forward passes</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">Workshop on Efficient Systems for Foundation Models @ ICML2023</em>.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malladi et al. (2023b)</span>
<span class="ltx_bibblock">
Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, Danqi Chen, and Sanjeev Arora. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/a627810151be4d13f907ac898ff7e948-Paper-Conference.pdf" title="" class="ltx_ref ltx_href">Fine-tuning language models with just forward passes</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 36, pages 53038–53075. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mangrulkar et al. (2022)</span>
<span class="ltx_bibblock">
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. 2022.

</span>
<span class="ltx_bibblock">Peft: State-of-the-art parameter-efficient fine-tuning methods.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/huggingface/peft" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/huggingface/peft</a>.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manoel et al. (2023)</span>
<span class="ltx_bibblock">
Andrea Manoel, Mirian del Carmen Hipolito Garcia, Tal Baumel, Shize Su, Jialei Chen, Robert Sim, Dan Miller, Danny Karmon, and Dimitrios Dimitriadis. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v209/manoel23a.html" title="" class="ltx_ref ltx_href">Federated multilingual models for medical transcript analysis</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Conference on Health, Inference, and Learning</em>, volume 209 of <em id="bib.bib117.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 147–162. PMLR.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maritan et al. (2023)</span>
<span class="ltx_bibblock">
Alessio Maritan, Subhrakanti Dey, and Luca Schenato. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2309.17174" title="" class="ltx_ref ltx_href">Fedzen: Towards superlinear zeroth-order federated learning via incremental hessian estimation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.17174</em>.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v54/mcmahan17a.html" title="" class="ltx_ref ltx_href">Communication-efficient learning of deep networks from decentralized data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>, pages 1273–1282. PMLR.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mugunthan et al. (2019)</span>
<span class="ltx_bibblock">
Vaikkunth Mugunthan, Antigoni Polychroniadou, David Byrd, and Tucker Hybinette Balch. 2019.

</span>
<span class="ltx_bibblock">Smpai: Secure multi-party computation for federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">Proceedings of the NeurIPS 2019 Workshop on Robust AI in Financial Services</em>, volume 21. MIT Press Cambridge, MA, USA.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2024)</span>
<span class="ltx_bibblock">
Duy Phuong Nguyen, J. Pablo Munoz, and Ali Jannesari. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2404.15182" title="" class="ltx_ref ltx_href">Flora: Enhancing vision-language models with parameter-efficient federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.15182</em>.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/blog/chatgpt/" title="" class="ltx_ref ltx_href">Chatgpt</a>.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.08774" title="" class="ltx_ref ltx_href">Gpt-4 technical report</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panagoulias et al. (2024)</span>
<span class="ltx_bibblock">
Dimitrios P. Panagoulias, Maria Virvou, and George A. Tsihrintzis. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.01730" title="" class="ltx_ref ltx_href">Evaluating llm – generated multimodal diagnosis from medical images and symptom analysis</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.01730</em>.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pandya et al. (2023)</span>
<span class="ltx_bibblock">
Sharnil Pandya, Gautam Srivastava, Rutvij Jhaveri, M. Rajasekhara Babu, Sweta Bhattacharya, Praveen Kumar Reddy Maddikunta, Spyridon Mastorakis, Md. Jalil Piran, and Thippa Reddy Gadekallu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1016/j.seta.2022.102987" title="" class="ltx_ref ltx_href">Federated learning for smart cities: A comprehensive survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">Sustainable Energy Technologies and Assessments</em>, 55:102987.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2021)</span>
<span class="ltx_bibblock">
Jungwuk Park, Dong-Jun Han, Minseok Choi, and Jaekyun Moon. 2021.

</span>
<span class="ltx_bibblock">Sageflow: Robust federated learning against both stragglers and adversaries.

</span>
<span class="ltx_bibblock">In <em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 34, pages 840–851. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ping et al. (2024)</span>
<span class="ltx_bibblock">
Siqi Ping, Yuzhu Mao, Yang Liu, Xiao-Ping Zhang, and Wenbo Ding. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=JDmAymuFFQ" title="" class="ltx_ref ltx_href">FL-TAC: Enhanced fine-tuning in federated learning via low-rank, task-specific adapter clustering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">ICLR 2024 Workshop on Large Language Model (LLM) Agents</em>.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pires et al. (2019)</span>
<span class="ltx_bibblock">
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1906.01502" title="" class="ltx_ref ltx_href">How multilingual is multilingual bert?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.01502</em>.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. (2024)</span>
<span class="ltx_bibblock">
Zhen Qin, Daoyuan Chen, Bingchen Qian, Bolin Ding, Yaliang Li, and Shuiguang Deng. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2312.06353" title="" class="ltx_ref ltx_href">Federated full-parameter tuning of billion-sized language models with communication cost under 18 kilobytes</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 41th International Conference on Machine Learning</em>.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu et al. (2024)</span>
<span class="ltx_bibblock">
Chen Qiu, Xingyu Li, Chaithanya Kumar Mummadi, Madan Ravi Ganesh, Zhenzhen Li, Lu Peng, and Wan-Yi Lin. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=NW31gAylIm" title="" class="ltx_ref ltx_href">Federated text-driven prompt generation for vision-language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v139/radford21a.html" title="" class="ltx_ref ltx_href">Learning transferable visual models from natural language supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 8748–8763. PMLR.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2023)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v202/radford23a.html" title="" class="ltx_ref ltx_href">Robust speech recognition via large-scale weak supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine Learning</em>, volume 202 of <em id="bib.bib132.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 28492–28518. PMLR.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford and Wu (2019)</span>
<span class="ltx_bibblock">
Alec Radford and Jeffrey Wu. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" title="" class="ltx_ref ltx_href">Rewon child, david luan, dario amodei, and ilya sutskever. 2019</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">Language models are unsupervised multitask learners. OpenAI blog</em>, 1(8):9.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh et al. (2021)</span>
<span class="ltx_bibblock">
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v139/ramesh21a.html" title="" class="ltx_ref ltx_href">Zero-shot text-to-image generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib134.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 8821–8831. PMLR.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramu et al. (2022)</span>
<span class="ltx_bibblock">
Swarna Priya Ramu, Parimala Boopalan, Quoc-Viet Pham, Praveen Kumar Reddy Maddikunta, Thien Huynh-The, Mamoun Alazab, Thanh Thi Nguyen, and Thippa Reddy Gadekallu. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1016/j.scs.2021.103663" title="" class="ltx_ref ltx_href">Federated learning enabled digital twins for smart cities: Concepts, recent advances, and future directions</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">Sustainable Cities and Society</em>, 79:103663.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddi et al. (2021)</span>
<span class="ltx_bibblock">
Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konečný, Sanjiv Kumar, and Hugh Brendan McMahan. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=LkFG3lB13U5" title="" class="ltx_ref ltx_href">Adaptive federated optimization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reed et al. (2022)</span>
<span class="ltx_bibblock">
Scott Reed et al. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=1ikK0kHjvj" title="" class="ltx_ref ltx_href">A generalist agent</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>.

</span>
<span class="ltx_bibblock">Featured Certification, Outstanding Certification.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reisizadeh et al. (2020)</span>
<span class="ltx_bibblock">
Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v108/reisizadeh20a.html" title="" class="ltx_ref ltx_href">Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib138.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics</em>, volume 108 of <em id="bib.bib138.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 2021–2031. PMLR.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2024)</span>
<span class="ltx_bibblock">
Chao Ren, Han Yu, Hongyi Peng, Xiaoli Tang, Anran Li, Yulan Gao, Alysa Ziying Tan, Bo Zhao, Xiaoxiao Li, Zengxiang Li, and Qiang Yang. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2404.15381" title="" class="ltx_ref ltx_href">Advances and open challenges in federated learning with foundation models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.15381</em>.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rieke et al. (2020)</span>
<span class="ltx_bibblock">
Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, Shadi Albarqouni, Spyridon Bakas, Mathieu N Galtier, Bennett A Landman, Klaus Maier-Hein, et al. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1038/s41746-020-00323-1" title="" class="ltx_ref ltx_href">The future of digital health with federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib140.1.1" class="ltx_emph ltx_font_italic">NPJ digital medicine</em>, 3(1):119.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rodríguez-Barroso et al. (2023)</span>
<span class="ltx_bibblock">
Nuria Rodríguez-Barroso, Daniel Jiménez-López, M Victoria Luzón, Francisco Herrera, and Eugenio Martínez-Cámara. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.sciencedirect.com/science/article/abs/pii/S1566253522001439" title="" class="ltx_ref ltx_href">Survey on federated learning threats: Concepts, taxonomy on attacks and defences, experimental study and challenges</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">Information Fusion</em>, 90:148–173.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roth et al. (2024)</span>
<span class="ltx_bibblock">
Holger R. Roth, Ziyue Xu, Yuan-Ting Hsieh, Adithya Renduchintala, Isaac Yang, Zhihong Zhang, Yuhong Wen, Sean Yang, Kevin Lu, Kristopher Kersten, Camir Ricketts, Daguang Xu, Chester Chen, Yan Cheng, and Andrew Feng. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.07792" title="" class="ltx_ref ltx_href">Empowering federated learning for massive models with nvidia flare</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.07792</em>.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rui et al. (2024)</span>
<span class="ltx_bibblock">
Wang Rui, Yu Tong, Zhang Ruiyi, Kim Sungchul, Rossi Ryan A., Zhao Handong, Wu Junda, Mitra Subrata, Yao Lina, and Henao Ricardo. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/pdf?id=bpUgtLeSAp" title="" class="ltx_ref ltx_href">Personalized federated learning for text classification with gradient-free prompt tuning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib143.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et al. (2020)</span>
<span class="ltx_bibblock">
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1910.01108" title="" class="ltx_ref ltx_href">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib144.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.01108</em>.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seo et al. (2021)</span>
<span class="ltx_bibblock">
Sejin Seo, Seung-Woo Ko, Jihong Park, Seong-Lyun Kim, and Mehdi Bennis. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/SPAWC51858.2021.9593126" title="" class="ltx_ref ltx_href">Communication-efficient and personalized federated lottery ticket learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib145.1.1" class="ltx_emph ltx_font_italic">2021 IEEE 22nd International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)</em>, pages 581–585.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah and Lau (2023)</span>
<span class="ltx_bibblock">
Suhail Mohmad Shah and Vincent K. N. Lau. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TNNLS.2021.3131614" title="" class="ltx_ref ltx_href">Model compression for communication efficient federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib146.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, 34(9):5937–5951.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2021)</span>
<span class="ltx_bibblock">
Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang Fang, Jianwei Yin, and Ting Wang. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3460120.3485370" title="" class="ltx_ref ltx_href">Backdoor pre-trained models can transfer to all</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib147.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security</em>, CCS ’21. ACM.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin et al. (2023a)</span>
<span class="ltx_bibblock">
Jaemin Shin, Hyungjun Yoon, Seungjoo Lee, Sungjoon Park, Yunxin Liu, Jinho Choi, and Sung-Ju Lee. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.emnlp-main.734" title="" class="ltx_ref ltx_href">FedTherapist: Mental health monitoring with user-generated linguistic expressions on smartphones via federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib148.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 11971–11988, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin et al. (2023b)</span>
<span class="ltx_bibblock">
Jiyun Shin, Jinhyun Ahn, Honggu Kang, and Joonhyuk Kang. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.14579" title="" class="ltx_ref ltx_href">Fedsplitx: Federated split learning for computationally-constrained heterogeneous clients</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib149.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.14579</em>.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2019)</span>
<span class="ltx_bibblock">
Abhishek Singh, Praneeth Vepakomma, Otkrist Gupta, and Ramesh Raskar. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1909.09145" title="" class="ltx_ref ltx_href">Detailed comparison of communication efficiency of split learning and federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib150.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.09145</em>.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spall (1992)</span>
<span class="ltx_bibblock">
J.C. Spall. 1992.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://ieeexplore.ieee.org/document/119632" title="" class="ltx_ref ltx_href">Multivariate stochastic approximation using a simultaneous perturbation gradient approximation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Automatic Control</em>, 37(3):332–341.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2023)</span>
<span class="ltx_bibblock">
Shangchao Su, Bin Li, and Xiangyang Xue. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2311.11227" title="" class="ltx_ref ltx_href">Fedra: A random allocation strategy for federated tuning to unleash the power of heterogeneous clients</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib152.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.11227</em>.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2024)</span>
<span class="ltx_bibblock">
Shangchao Su, Mingzhao Yang, Bin Li, and Xiangyang Xue. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1609/aaai.v38i13.29434" title="" class="ltx_ref ltx_href">Federated adaptive prompt tuning for multi-domain collaborative learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib153.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, 38(13):15117–15125.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2023)</span>
<span class="ltx_bibblock">
Guangyu Sun, Matias Mendieta, Jun Luo, Shandong Wu, and Chen Chen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content/ICCV2023/html/Sun_FedPerfix_Towards_Partial_Model_Personalization_of_Vision_Transformers_in_Federated_ICCV_2023_paper.html" title="" class="ltx_ref ltx_href">Fedperfix: Towards partial model personalization of vision transformers in federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib154.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 4988–4998.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2022a)</span>
<span class="ltx_bibblock">
Guangyu Sun, Matias Mendieta, Taojiannan Yang, and Chen Chen. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.48550/arXiv.2210.01708" title="" class="ltx_ref ltx_href">Conquering the communication constraints to enable large pre-trained models in federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib155.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.01708</em>.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2024a)</span>
<span class="ltx_bibblock">
Jingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang, Daguang Xu, Yiran Chen, and Holger R Roth. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.48550/arXiv.2310.01467" title="" class="ltx_ref ltx_href">Fedbpt: Efficient federated black-box prompt tuning for large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib156.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 41th International Conference on Machine Learning</em>.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2022b)</span>
<span class="ltx_bibblock">
Tianxiang Sun, Zhengfu He, Hong Qian, Yunhua Zhou, Xuanjing Huang, and Xipeng Qiu. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.emnlp-main.259" title="" class="ltx_ref ltx_href">BBTv2: Towards a gradient-free future with large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib157.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 3916–3930, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2022c)</span>
<span class="ltx_bibblock">
Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. 2022c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v162/sun22e.html" title="" class="ltx_ref ltx_href">Black-box tuning for language-model-as-a-service</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib158.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 39th International Conference on Machine Learning</em>, volume 162 of <em id="bib.bib158.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 20841–20855. PMLR.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2024b)</span>
<span class="ltx_bibblock">
Youbang Sun, Zitao Li, Yaliang Li, and Bolin Ding. 2024b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=NLPzL6HWNl" title="" class="ltx_ref ltx_href">Improving loRA in privacy-preserving federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib159.1.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tamirisa et al. (2024)</span>
<span class="ltx_bibblock">
Rishub Tamirisa, John Won, Chengjun Lu, Ron Arel, and Andy Zhou. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=TXtRWPZIZ0" title="" class="ltx_ref ltx_href">Fedselect: Customized selection of parameters for fine-tuning during personalized federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib160.1.1" class="ltx_emph ltx_font_italic">Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities</em>.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2023)</span>
<span class="ltx_bibblock">
Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, and Xia Hu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.04360" title="" class="ltx_ref ltx_href">Does synthetic data generation of llms help clinical text mining?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib161.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.04360</em>.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2020)</span>
<span class="ltx_bibblock">
Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2008.00401" title="" class="ltx_ref ltx_href">Multilingual translation with extensible multilingual pretraining and finetuning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib162.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2008.00401</em>.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tekgul et al. (2021)</span>
<span class="ltx_bibblock">
Buse G. A. Tekgul, Yuxi Xia, Samuel Marchal, and N. Asokan. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/SRDS53918.2021.00038" title="" class="ltx_ref ltx_href">Waffle: Watermarking in federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib163.1.1" class="ltx_emph ltx_font_italic">2021 40th International Symposium on Reliable Distributed Systems (SRDS)</em>, pages 310–320.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thapa et al. (2022)</span>
<span class="ltx_bibblock">
Chandra Thapa, Pathum Chamikara Mahawaga Arachchige, Seyit Camtepe, and Lichao Sun. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1609/aaai.v36i8.20825" title="" class="ltx_ref ltx_href">Splitfed: When federated learning meets split learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib164.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, 36(8):8485–8493.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al. (2022)</span>
<span class="ltx_bibblock">
Yuanyishu Tian, Yao Wan, Lingjuan Lyu, Dezhong Yao, Hai Jin, and Lichao Sun. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3510033" title="" class="ltx_ref ltx_href">Fedbert: When federated learning meets pre-training</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib165.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Intell. Syst. Technol.</em>, 13(4).

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2302.13971" title="" class="ltx_ref ltx_href">Llama: Open and efficient foundation language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib166.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2307.09288" title="" class="ltx_ref ltx_href">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib167.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsouvalas et al. (2023)</span>
<span class="ltx_bibblock">
Vasileios Tsouvalas, Yuki Asano, and Aaqib Saeed. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2311.17299" title="" class="ltx_ref ltx_href">Federated fine-tuning of foundation models via probabilistic masking</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib168.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.17299</em>.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu et al. (2024)</span>
<span class="ltx_bibblock">
Minh Vu, Truc Nguyen, Tre’ Jeter, and My T. Thai. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v238/vu24a.html" title="" class="ltx_ref ltx_href">Analysis of privacy leakage in federated large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib169.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 27th International Conference on Artificial Intelligence and Statistics</em>, volume 238 of <em id="bib.bib169.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 1423–1431. PMLR.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang (2023)</span>
<span class="ltx_bibblock">
Eric Wang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/tloen/alpaca-lora" title="" class="ltx_ref ltx_href">Alpaca-lora</a>.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Haoyu Wang, Handong Zhao, Yaqing Wang, Tong Yu, Jiuxiang Gu, and Jing Gao. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3485447.3511988" title="" class="ltx_ref ltx_href">Fedkc: Federated knowledge composition for multilingual natural language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib171.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM Web Conference 2022</em>, WWW ’22, page 1839–1850, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Xin’ao Wang, Huan Li, Ke Chen, and Lidan Shou. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.24963/ijcai.2023/483" title="" class="ltx_ref ltx_href">Fedbfpt: An efficient federated learning framework for bert further pre-training</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib172.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23</em>, pages 4344–4352. International Joint Conferences on Artificial Intelligence Organization.

</span>
<span class="ltx_bibblock">Main Track.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2020)</span>
<span class="ltx_bibblock">
Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H. Yang, Farhad Farokhi, Shi Jin, Tony Q. S. Quek, and H. Vincent Poor. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TIFS.2020.2988575" title="" class="ltx_ref ltx_href">Federated learning with differential privacy: Algorithms and performance analysis</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib173.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics and Security</em>, 15:3454–3469.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weller et al. (2022)</span>
<span class="ltx_bibblock">
Orion Weller, Marc Marone, Vladimir Braverman, Dawn Lawrie, and Benjamin Van Durme. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.naacl-main.101" title="" class="ltx_ref ltx_href">Pretrained models for multilingual federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib174.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 1413–1421, Seattle, United States. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Woisetschläger et al. (2024)</span>
<span class="ltx_bibblock">
Herbert Woisetschläger, Alexander Isenko, Shiqiang Wang, Ruben Mayer, and Hans-Arno Jacobsen. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2401.04472" title="" class="ltx_ref ltx_href">A survey on efficient federated learning methods for foundation model training</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib175.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.04472</em>.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2020a)</span>
<span class="ltx_bibblock">
Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. 2020a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2006.03677" title="" class="ltx_ref ltx_href">Visual transformers: Token-based image representation and processing for computer vision</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib176.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.03677</em>.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023a)</span>
<span class="ltx_bibblock">
Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and Hong Lin. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2304.06632" title="" class="ltx_ref ltx_href">Ai-generated content (aigc): A survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib177.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.06632</em>.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023b)</span>
<span class="ltx_bibblock">
Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.19860" title="" class="ltx_ref ltx_href">A survey on large language models for recommendation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib178.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.19860</em>.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023c)</span>
<span class="ltx_bibblock">
Panlong Wu, Kangshuo Li, Ting Wang, and Fangxin Wang. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2312.15926" title="" class="ltx_ref ltx_href">Fedms: Federated learning with mixture of sparsely activated foundations models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib179.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.15926</em>.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Dredze (2020)</span>
<span class="ltx_bibblock">
Shijie Wu and Mark Dredze. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.repl4nlp-1.16" title="" class="ltx_ref ltx_href">Are all languages created equal in multilingual BERT?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib180.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 5th Workshop on Representation Learning for NLP</em>, pages 120–130, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2020b)</span>
<span class="ltx_bibblock">
Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B. Giannakis. 2020b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TSP.2020.3012952" title="" class="ltx_ref ltx_href">Federated variance-reduced stochastic gradient descent with robustness to byzantine attacks</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib181.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Signal Processing</em>, 68:4583–4596.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2020)</span>
<span class="ltx_bibblock">
Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. 2020.

</span>
<span class="ltx_bibblock">Dba: Distributed backdoor attacks against federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib182.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2023)</span>
<span class="ltx_bibblock">
Yuexiang Xie, Zhen Wang, Dawei Gao, Daoyuan Chen, Liuyi Yao, Weirui Kuang, Yaliang Li, Bolin Ding, and Jingren Zhou. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.14778/3579075.3579081" title="" class="ltx_ref ltx_href">Federatedscope: A flexible federated learning platform for heterogeneity</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib183.1.1" class="ltx_emph ltx_font_italic">Proc. VLDB Endow.</em>, 16(5):1059–1072.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2022)</span>
<span class="ltx_bibblock">
Chang Xu, Yu Jia, Liehuang Zhu, Chuan Zhang, Guoxie Jin, and Kashif Sharif. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TPDS.2022.3205714" title="" class="ltx_ref ltx_href">Tdfl: Truth discovery based byzantine robust federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib184.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems</em>, 33(12).

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024a)</span>
<span class="ltx_bibblock">
Mengwei Xu, Dongqi Cai, Yaozong Wu, Xiang Li, and Shangguang Wang. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.13894" title="" class="ltx_ref ltx_href">Fwdllm: Efficient fedllm using forward gradient</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib185.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.13894</em>.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024b)</span>
<span class="ltx_bibblock">
Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, Qiyang Zhang, Zhenyan Lu, Li Zhang, Shangguang Wang, Yuanchun Li, Yunxin Liu, Xin Jin, and Xuanzhe Liu. 2024b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2401.08092" title="" class="ltx_ref ltx_href">A survey of resource-efficient llm and multimodal foundation models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib186.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.08092</em>.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024c)</span>
<span class="ltx_bibblock">
Minrui Xu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, Abbas Jamalipour, Dong In Kim, Xuemin Shen, Victor C. M. Leung, and H. Vincent Poor. 2024c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/COMST.2024.3353265" title="" class="ltx_ref ltx_href">Unleashing the power of edge-cloud generative ai in mobile networks: A survey of aigc services</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib187.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>, pages 1–1.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023)</span>
<span class="ltx_bibblock">
Zheng Xu, Yanxiang Zhang, Galen Andrew, Christopher Choquette, Peter Kairouz, Brendan Mcmahan, Jesse Rosenstock, and Yuanbo Zhang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-industry.60" title="" class="ltx_ref ltx_href">Federated learning of gboard language models with differential privacy</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib188.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)</em>, pages 629–639, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023a)</span>
<span class="ltx_bibblock">
Fu-En Yang, Chien-Yi Wang, and Yu-Chiang Frank Wang. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV51070.2023.01755" title="" class="ltx_ref ltx_href">Efficient model personalization in federated learning via client-specific prompt generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib189.1.1" class="ltx_emph ltx_font_italic">2023 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pages 19102–19111.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023b)</span>
<span class="ltx_bibblock">
Kailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie, Ziyan Kuang, and Sophia Ananiadou. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2304.03347" title="" class="ltx_ref ltx_href">Towards interpretable mental health analysis with large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib190.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.03347</em>.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024a)</span>
<span class="ltx_bibblock">
Xin Yang, Hao Yu, Xin Gao, Hao Wang, Junbo Zhang, and Tianrui Li. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/tkde.2024.3363240" title="" class="ltx_ref ltx_href">Federated continual learning via knowledge fusion: A survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib191.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</em>, page 1–20.

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024b)</span>
<span class="ltx_bibblock">
Yiyuan Yang, Guodong Long, Tao Shen, Jing Jiang, and Michael Blumenstein. 2024b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2403.19211" title="" class="ltx_ref ltx_href">Dual-personalizing adapter for federated foundation models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib192.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.19211</em>.

</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2022)</span>
<span class="ltx_bibblock">
Chun-Han Yao, Boqing Gong, Hang Qi, Yin Cui, Yukun Zhu, and Ming-Hsuan Yang. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/WACV51458.2022.00115" title="" class="ltx_ref ltx_href">Federated multi-target domain adaptation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib193.1.1" class="ltx_emph ltx_font_italic">2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, pages 1081–1090.

</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2024)</span>
<span class="ltx_bibblock">
Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du, Yanfeng Wang, and Siheng Chen. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.06954" title="" class="ltx_ref ltx_href">Openfedllm: Training large language models on decentralized private data via federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib194.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.06954</em>.

</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. (2018)</span>
<span class="ltx_bibblock">
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. 2018.

</span>
<span class="ltx_bibblock">Byzantine-robust distributed learning: Towards optimal statistical rates.

</span>
<span class="ltx_bibblock">In <em id="bib.bib195.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>. PMLR.

</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023a)</span>
<span class="ltx_bibblock">
Qiying Yu, Yang Liu, Yimu Wang, Ke Xu, and Jingjing Liu. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=Hnk1WRMAYqg" title="" class="ltx_ref ltx_href">Multimodal federated learning via contrastive representation ensemble</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib196.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023b)</span>
<span class="ltx_bibblock">
Shuyang Yu, Junyuan Hong, Yi Zeng, Fei Wang, Ruoxi Jia, and Jiayu Zhou. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=qhP1aHHyeA" title="" class="ltx_ref ltx_href">Who leaked the model? tracking IP infringers in accountable federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib197.1.1" class="ltx_emph ltx_font_italic">NeurIPS 2023 Workshop on Regulatable ML</em>.

</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023c)</span>
<span class="ltx_bibblock">
Sixing Yu, J Pablo Muñoz, and Ali Jannesari. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2310.00247" title="" class="ltx_ref ltx_href">Bridging the gap between foundation models and heterogeneous federated learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib198.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.00247</em>.

</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023d)</span>
<span class="ltx_bibblock">
Sixing Yu, J Pablo Muñoz, and Ali Jannesari. 2023d.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2305.11414" title="" class="ltx_ref ltx_href">Federated foundation models: Privacy-preserving and collaborative learning for large models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib199.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.11414</em>.

</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2024)</span>
<span class="ltx_bibblock">
Huimin Zeng, Zhenrui Yue, Qian Jiang, and Dong Wang. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2403.04256" title="" class="ltx_ref ltx_href">Federated recommendation via hybrid retrieval augmented generation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib200.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.04256</em>.

</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Chengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, Feng Yan, and Yang Liu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.usenix.org/conference/atc20/presentation/zhang-chengliang" title="" class="ltx_ref ltx_href">BatchCrypt: Efficient homomorphic encryption for Cross-Silo federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib201.1.1" class="ltx_emph ltx_font_italic">2020 USENIX Annual Technical Conference (USENIX ATC 20)</em>, pages 493–506. USENIX Association.

</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024a)</span>
<span class="ltx_bibblock">
Honglei Zhang, He Liu, Haoxuan Li, and Yidong Li. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.01124" title="" class="ltx_ref ltx_href">Transfr: Transferable federated recommendation with pre-trained language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib202.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.01124</em>.

</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023a)</span>
<span class="ltx_bibblock">
Honglei Zhang, Fangyuan Luo, Jun Wu, Xiangnan He, and Yidong Li. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3578361" title="" class="ltx_ref ltx_href">Lightfr: Lightweight federated recommendation with privacy-preserving matrix factorization</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib203.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Inf. Syst.</em>, 41(4).

</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024b)</span>
<span class="ltx_bibblock">
Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Tong Yu, Guoyin Wang, and Yiran Chen. 2024b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP48485.2024.10447454" title="" class="ltx_ref ltx_href">Towards building the federatedgpt: Federated instruction tuning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib204.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 6915–6919.

</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023b)</span>
<span class="ltx_bibblock">
Jie Zhang, Xiaohua Qi, and Bo Zhao. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2306.16064" title="" class="ltx_ref ltx_href">Federated generative learning with foundation models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib205.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.16064</em>.

</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023c)</span>
<span class="ltx_bibblock">
Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.07001" title="" class="ltx_ref ltx_href">Recommendation as instruction following: A large language model empowered recommendation approach</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib206.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.07001</em>.

</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023d)</span>
<span class="ltx_bibblock">
Zhiyuan Zhang, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023d.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/c39578c86423df5f9e8834ce1cd456e4-Paper-Conference.pdf" title="" class="ltx_ref ltx_href">Fed-fa: Theoretically modeling client data divergence for federated language backdoor defense</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib207.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 36, pages 62006–62031. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023e)</span>
<span class="ltx_bibblock">
Zhuo Zhang, Xiangjing Hu, Jingyuan Zhang, Yating Zhang, Hui Wang, Lizhen Qu, and Zenglin Xu. 2023e.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.193" title="" class="ltx_ref ltx_href">FEDLEGAL: The first real-world federated learning benchmark for legal NLP</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib208.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3492–3507, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib209" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024c)</span>
<span class="ltx_bibblock">
Zhuo Zhang, Jintao Huang, Xiangjing Hu, Jingyuan Zhang, Yating Zhang, Hui Wang, Yue Yu, Qifan Wang, Lizhen Qu, and Zenglin Xu. 2024c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2024.lrec-main.1227" title="" class="ltx_ref ltx_href">Revisiting data reconstruction attacks on real-world dataset for federated natural language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib209.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</em>, pages 14080–14091, Torino, Italia. ELRA and ICCL.

</span>
</li>
<li id="bib.bib210" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023f)</span>
<span class="ltx_bibblock">
Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, and Zenglin Xu. 2023f.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-acl.632" title="" class="ltx_ref ltx_href">FedPETuning: When federated learning meets the parameter-efficient tuning methods of pre-trained language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib210.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 9963–9977, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib211" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2024a)</span>
<span class="ltx_bibblock">
Jujia Zhao, Wenjie Wang, Chen Xu, Zhaochun Ren, See-Kiong Ng, and Tat-Seng Chua. 2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.09959" title="" class="ltx_ref ltx_href">Llm-based federated recommendation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib211.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.09959</em>.

</span>
</li>
<li id="bib.bib212" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2024b)</span>
<span class="ltx_bibblock">
Wanru Zhao, Yihong Chen, Royson Lee, Xinchi Qiu, Yan Gao, Hongxiang Fan, and Nicholas Donald Lane. 2024b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=zzqn5G9fjn" title="" class="ltx_ref ltx_href">Breaking physical and linguistic borders: Multilingual federated prompt tuning for low-resource languages</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib212.1.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib213" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023)</span>
<span class="ltx_bibblock">
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2303.18223" title="" class="ltx_ref ltx_href">A survey of large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib213.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.18223</em>.

</span>
</li>
<li id="bib.bib214" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023)</span>
<span class="ltx_bibblock">
Fei Zheng, Chaochao Chen, Lingjuan Lyu, and Binhui Yao. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.24963/ijcai.2023/519" title="" class="ltx_ref ltx_href">Reducing communication for split learning by randomized top-k sparsification</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib214.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence</em>, IJCAI ’23.

</span>
</li>
<li id="bib.bib215" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2019)</span>
<span class="ltx_bibblock">
Ligeng Zhu, Zhijian Liu, and Song Han. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2019/file/60a6c4002cc7b29142def8871531281a-Paper.pdf" title="" class="ltx_ref ltx_href">Deep leakage from gradients</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib215.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 32. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib216" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang et al. (2023)</span>
<span class="ltx_bibblock">
Weiming Zhuang, Chen Chen, and Lingjuan Lyu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2306.15546" title="" class="ltx_ref ltx_href">When foundation model meets federated learning: Motivations, challenges, and future directions</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib216.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.15546</em>.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Details of Adapter Tuning</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Adapter Tuning</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">Adapter tuning integrates small-scale neural networks (known as “adapters”) into the pre-trained models <cite class="ltx_cite ltx_citemacro_cite">Houlsby et al. (<a href="#bib.bib66" title="" class="ltx_ref">2019</a>); Hu et al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite>. A straightforward implementation of adapter tuning is to collaboratively train a shared adapter among all clients in the <span id="A1.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">FedAvg</span> manner, as highlighted by <cite class="ltx_cite ltx_citemacro_citet">Sun et al. (<a href="#bib.bib155" title="" class="ltx_ref">2022a</a>)</cite>. Based on <span id="A1.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">FedAvg</span>, FedCLIP <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib110" title="" class="ltx_ref">2023a</a>)</cite> incorporates an attention-based adapter for the image encoder in CLIP models <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib131" title="" class="ltx_ref">2021</a>)</cite>.
In the domain of multilingual machine translation, where different language pairs exhibit substantial discrepancies in data distributions, Fed-MNMT <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib108" title="" class="ltx_ref">2023d</a>)</cite> explores clustering strategies that group adapter parameters and makes inner-cluster parameters aggregation for alleviating the undesirable effect of data discrepancy. Another representative approach named C2A <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib81" title="" class="ltx_ref">2023</a>)</cite> employs hypernetworks <cite class="ltx_cite ltx_citemacro_cite">Ha et al. (<a href="#bib.bib63" title="" class="ltx_ref">2017</a>)</cite> to generate client-specific adapters by conditioning on the client’s information, maximizing the utility of shared model parameters while minimizing the divergence caused by
data heterogeneity.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Prompt Tuning</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">Prompt tuning incorporates trainable task-specific continuous prompt vectors at the input layer <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib104" title="" class="ltx_ref">2023a</a>); Dong et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>. Compared to full fine-tuning, it achieves comparable performance but with <math id="A1.SS2.p1.1.m1.1" class="ltx_math_unparsed" alttext="1000\times" display="inline"><semantics id="A1.SS2.p1.1.m1.1a"><mrow id="A1.SS2.p1.1.m1.1b"><mn id="A1.SS2.p1.1.m1.1.1">1000</mn><mo lspace="0.222em" id="A1.SS2.p1.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="A1.SS2.p1.1.m1.1c">1000\times</annotation></semantics></math> less parameter storage and communication <cite class="ltx_cite ltx_citemacro_cite">Jia et al. (<a href="#bib.bib72" title="" class="ltx_ref">2022</a>)</cite>. A variation of prompt tuning, FedPerfix <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib154" title="" class="ltx_ref">2023</a>)</cite> uses a local adapter to generate the prefixes and aggregate the original self-attention layers.</p>
</div>
<div id="A1.SS2.p2" class="ltx_para">
<p id="A1.SS2.p2.1" class="ltx_p">Depending on target modalities, prompt tuning in current literature can be further classified into three categories:</p>
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p"><span id="A1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Textual Prompt Tuning.</span> Task-specific prompt embeddings are combined with the input text embeddings, which are subsequently fed into language models. These soft prompts serve as instructive contexts to influence the generation process of LLMs by steering the probability distribution of the next token <cite class="ltx_cite ltx_citemacro_cite">Dong et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p"><span id="A1.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Visual Prompt Tuning.</span> Taking inspiration from advances in efficiently tuning LLMs, prompts are also introduced in the input space of vision models <cite class="ltx_cite ltx_citemacro_cite">Jia et al. (<a href="#bib.bib72" title="" class="ltx_ref">2022</a>)</cite>. Naive implementations introduce prompts at the pixel level, acting as a form of data augmentation <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib86" title="" class="ltx_ref">2024a</a>)</cite>. Alternatively, one could also insert the prompts as latent vectors for the first Transformer layer <cite class="ltx_cite ltx_citemacro_cite">Deng et al. (<a href="#bib.bib33" title="" class="ltx_ref">2024</a>); Yang et al. (<a href="#bib.bib189" title="" class="ltx_ref">2023a</a>)</cite>. Nevertheless, an empirical study <cite class="ltx_cite ltx_citemacro_cite">Jia et al. (<a href="#bib.bib72" title="" class="ltx_ref">2022</a>)</cite> has suggested that it is easier for visual prompts to learn condensed task-dependent signals in the latent input space of Transformers.</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p"><span id="A1.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Textual-Visual Prompt Tuning.</span> Unlike single-modal FMs, vision-language FMs can process and interpret both visual data and textual information, endowing them with powerful representation ability and transferability <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib131" title="" class="ltx_ref">2021</a>)</cite>. Based on vision-language FMs like CLIP, textual-visual prompt tuning shows promising capabilities in FL <cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a href="#bib.bib58" title="" class="ltx_ref">2023</a>)</cite>, especially in cross-domain scenarios, where the model needs to generalize across varied domains and unseen classes <cite class="ltx_cite ltx_citemacro_cite">Qiu et al. (<a href="#bib.bib130" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<figure id="A1.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>A list of existing FM-FL libraries and benchmarks. <span id="A1.T3.2.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">Missing or inapplicable details denoted by N/A</span>. ✓ denotes a strong focus or presence; ✗ indicates no focus or absence; ◐ signifies a moderate focus or partial inclusion.</figcaption>
<p id="A1.T3.3" class="ltx_p ltx_align_center"><span id="A1.T3.3.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="A1.T3.3.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:898.7pt;height:277.8pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A1.T3.3.1.1.1" class="ltx_p"><span id="A1.T3.3.1.1.1.1" class="ltx_text">
<span id="A1.T3.3.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A1.T3.3.1.1.1.1.1.1" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Library/Benchmark</span></span>
<span id="A1.T3.3.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">FL Backend</span></span>
<span id="A1.T3.3.1.1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="A1.T3.3.1.1.1.1.1.1.3.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:60.3pt;vertical-align:-27.7pt;"><span class="ltx_transformed_inner" style="width:60.3pt;transform:translate(-25.76pt,2.92pt) rotate(-90deg) ;">
<span id="A1.T3.3.1.1.1.1.1.1.3.1.1" class="ltx_p"><span id="A1.T3.3.1.1.1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold" style="color:#1B9E77;">LLM Support</span></span>
</span></span></span>
<span id="A1.T3.3.1.1.1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="A1.T3.3.1.1.1.1.1.1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:109.6pt;vertical-align:-52.3pt;"><span class="ltx_transformed_inner" style="width:109.6pt;transform:translate(-50.36pt,2.92pt) rotate(-90deg) ;">
<span id="A1.T3.3.1.1.1.1.1.1.4.1.1" class="ltx_p"><span id="A1.T3.3.1.1.1.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold" style="color:#D73027;">MultiModal FM Support</span></span>
</span></span></span>
<span id="A1.T3.3.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="A1.T3.3.1.1.1.1.1.1.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:43.1pt;vertical-align:-18.1pt;"><span class="ltx_transformed_inner" style="width:43.1pt;transform:translate(-18.06pt,0pt) rotate(-90deg) ;">
<span id="A1.T3.3.1.1.1.1.1.1.5.1.1" class="ltx_p"><span id="A1.T3.3.1.1.1.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold" style="color:#006BA4;">FedPEFT</span></span>
</span></span></span>
<span id="A1.T3.3.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="A1.T3.3.1.1.1.1.1.1.6.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:106pt;vertical-align:-50.6pt;"><span class="ltx_transformed_inner" style="width:106.0pt;transform:translate(-48.61pt,2.92pt) rotate(-90deg) ;">
<span id="A1.T3.3.1.1.1.1.1.1.6.1.1" class="ltx_p"><span id="A1.T3.3.1.1.1.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold" style="color:#FF800E;">On-Device Training</span></span>
</span></span></span>
<span id="A1.T3.3.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="A1.T3.3.1.1.1.1.1.1.7.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:132.8pt;vertical-align:-62.9pt;"><span class="ltx_transformed_inner" style="width:132.8pt;transform:translate(-62.93pt,0pt) rotate(-90deg) ;">
<span id="A1.T3.3.1.1.1.1.1.1.7.1.1" class="ltx_p"><span id="A1.T3.3.1.1.1.1.1.1.7.1.1.1" class="ltx_text ltx_font_bold" style="color:#ABAB2A;">Distributed &amp; Clustered</span></span>
</span></span></span>
<span id="A1.T3.3.1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="A1.T3.3.1.1.1.1.1.1.8.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:85.4pt;vertical-align:-40.2pt;"><span class="ltx_transformed_inner" style="width:85.3pt;transform:translate(-38.22pt,2.92pt) rotate(-90deg) ;">
<span id="A1.T3.3.1.1.1.1.1.1.8.1.1" class="ltx_p"><span id="A1.T3.3.1.1.1.1.1.1.8.1.1.1" class="ltx_text ltx_font_bold" style="color:#893D56;">Differential Privacy</span></span>
</span></span></span>
<span id="A1.T3.3.1.1.1.1.1.1.9" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.1.9.1" class="ltx_text ltx_font_bold">Description</span></span></span>
<span id="A1.T3.3.1.1.1.1.1.2" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">FederatedScope-LLM <cite class="ltx_cite ltx_citemacro_cite">Kuang et al. (<a href="#bib.bib83" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="A1.T3.3.1.1.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">FederatedScope</span>
<span id="A1.T3.3.1.1.1.1.1.2.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.2.3.1" class="ltx_text" style="color:#1B9E77;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.2.4" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.2.4.1" class="ltx_text" style="color:#D73027;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.2.5.1" class="ltx_text" style="color:#006BA4;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.2.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.2.6.1" class="ltx_text" style="color:#FF800E;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.2.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.2.7.1" class="ltx_text" style="color:#ABAB2A;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.2.8.1" class="ltx_text" style="color:#893D56;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.2.9" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">An end-to-end benchmark for efficient fine-tuning LLMs with FL</span></span>
<span id="A1.T3.3.1.1.1.1.1.3" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.5pt;padding-bottom:0.5pt;">NVIDIA FLARE <cite class="ltx_cite ltx_citemacro_cite">Roth et al. (<a href="#bib.bib142" title="" class="ltx_ref">2024</a>)</cite></span>
<span id="A1.T3.3.1.1.1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">NVFlare</span>
<span id="A1.T3.3.1.1.1.1.1.3.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.3.3.1" class="ltx_text" style="color:#1B9E77;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.3.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.3.4.1" class="ltx_text" style="color:#D73027;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.3.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.3.5.1" class="ltx_text" style="color:#006BA4;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.3.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.3.6.1" class="ltx_text" style="color:#FF800E;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.3.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.3.7.1" class="ltx_text" style="color:#ABAB2A;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.3.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.3.8.1" class="ltx_text" style="color:#893D56;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.3.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">Scalable and efficient fine-tuning LLMs with FL</span></span>
<span id="A1.T3.3.1.1.1.1.1.4" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.5pt;padding-bottom:0.5pt;">FATE-LLM <cite class="ltx_cite ltx_citemacro_cite">Fan et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="A1.T3.3.1.1.1.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">FATE</span>
<span id="A1.T3.3.1.1.1.1.1.4.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.4.3.1" class="ltx_text" style="color:#1B9E77;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.4.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.4.4.1" class="ltx_text" style="color:#D73027;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.4.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.4.5.1" class="ltx_text" style="color:#006BA4;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.4.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.4.6.1" class="ltx_text" style="color:#FF800E;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.4.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.4.7.1" class="ltx_text" style="color:#ABAB2A;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.4.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.4.8.1" class="ltx_text" style="color:#893D56;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.4.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">Focuses on IP and privacy protection in federated LLM</span></span>
<span id="A1.T3.3.1.1.1.1.1.5" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.5.1" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedLLM <cite class="ltx_cite ltx_citemacro_cite">FedML (<a href="#bib.bib44" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="A1.T3.3.1.1.1.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedML</span>
<span id="A1.T3.3.1.1.1.1.1.5.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.5.3.1" class="ltx_text" style="color:#1B9E77;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.5.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.5.4.1" class="ltx_text" style="color:#D73027;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.5.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.5.5.1" class="ltx_text" style="color:#006BA4;">◐</span></span>
<span id="A1.T3.3.1.1.1.1.1.5.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.5.6.1" class="ltx_text" style="color:#FF800E;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.5.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.5.7.1" class="ltx_text" style="color:#ABAB2A;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.5.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.5.8.1" class="ltx_text" style="color:#893D56;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.5.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">An MLOps-supported training pipeline based on FedML</span></span>
<span id="A1.T3.3.1.1.1.1.1.6" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.6.1" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.5pt;padding-bottom:0.5pt;">OpenFedLLM <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a href="#bib.bib194" title="" class="ltx_ref">2024</a>)</cite></span>
<span id="A1.T3.3.1.1.1.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.6.2.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">N/A</span></span>
<span id="A1.T3.3.1.1.1.1.1.6.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.6.3.1" class="ltx_text" style="color:#1B9E77;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.6.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.6.4.1" class="ltx_text" style="color:#D73027;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.6.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.6.5.1" class="ltx_text" style="color:#006BA4;">◐</span></span>
<span id="A1.T3.3.1.1.1.1.1.6.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.6.6.1" class="ltx_text" style="color:#FF800E;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.6.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.6.7.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">N/A</span></span>
<span id="A1.T3.3.1.1.1.1.1.6.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.6.8.1" class="ltx_text" style="color:#893D56;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.6.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">An LLM framework focusing on FL instruction tuning/alignment</span></span>
<span id="A1.T3.3.1.1.1.1.1.7" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.7.1" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.5pt;padding-bottom:0.5pt;">Shepherd <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib204" title="" class="ltx_ref">2024b</a>)</cite></span>
<span id="A1.T3.3.1.1.1.1.1.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.7.2.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">N/A</span></span>
<span id="A1.T3.3.1.1.1.1.1.7.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.7.3.1" class="ltx_text" style="color:#1B9E77;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.7.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.7.4.1" class="ltx_text" style="color:#D73027;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.7.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.7.5.1" class="ltx_text" style="color:#006BA4;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.7.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.7.6.1" class="ltx_text" style="color:#FF800E;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.7.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.7.7.1" class="ltx_text" style="color:#ABAB2A;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.7.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.7.8.1" class="ltx_text" style="color:#893D56;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.7.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">Federated instruction tuning based on Hugging Face</span></span>
<span id="A1.T3.3.1.1.1.1.1.8" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.8.1" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedPETuning <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib210" title="" class="ltx_ref">2023f</a>)</cite></span>
<span id="A1.T3.3.1.1.1.1.1.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedLab</span>
<span id="A1.T3.3.1.1.1.1.1.8.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.8.3.1" class="ltx_text" style="color:#1B9E77;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.8.4" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.8.4.1" class="ltx_text" style="color:#D73027;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.8.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.8.5.1" class="ltx_text" style="color:#006BA4;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.8.6" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.8.6.1" class="ltx_text" style="color:#FF800E;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.8.7" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.8.7.1" class="ltx_text" style="color:#ABAB2A;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.8.8" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.8.8.1" class="ltx_text" style="color:#893D56;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.8.9" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">A benchmark comprising four FedPEFT methods</span></span>
<span id="A1.T3.3.1.1.1.1.1.9" class="ltx_tr">
<span id="A1.T3.3.1.1.1.1.1.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedLegal <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib208" title="" class="ltx_ref">2023e</a>)</cite></span>
<span id="A1.T3.3.1.1.1.1.1.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedLab</span>
<span id="A1.T3.3.1.1.1.1.1.9.3" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.9.3.1" class="ltx_text" style="color:#1B9E77;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.9.4" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.9.4.1" class="ltx_text" style="color:#D73027;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.9.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.9.5.1" class="ltx_text" style="color:#006BA4;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.9.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.9.6.1" class="ltx_text" style="color:#FF800E;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.9.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.9.7.1" class="ltx_text" style="color:#ABAB2A;">✓</span></span>
<span id="A1.T3.3.1.1.1.1.1.9.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="A1.T3.3.1.1.1.1.1.9.8.1" class="ltx_text" style="color:#893D56;">✗</span></span>
<span id="A1.T3.3.1.1.1.1.1.9.9" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:0.5pt;padding-bottom:0.5pt;">A benchmark comprising six legal NLP tasks under FL settings</span></span>
</span></span></span>
</span></span></span></p>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Libraries and Benchmarks</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">This part briefly introduces a series of available libraries and benchmarks for developing and examining FM-FL techniques. An overview is provided in Table <a href="#A1.T3" title="Table 3 ‣ A.2 Prompt Tuning ‣ Appendix A Additional Details of Adapter Tuning ‣ Synergizing Foundation Models and Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="A2.p2" class="ltx_para">
<ul id="A2.I1" class="ltx_itemize">
<li id="A2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i1.p1.1" class="ltx_p"><span id="A2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">FederatedScope-LLM</span> <cite class="ltx_cite ltx_citemacro_cite">Kuang et al. (<a href="#bib.bib83" title="" class="ltx_ref">2023</a>)</cite> is an open-source package for fine-tuning LLMs via FL. Built on top of a popular FL backend FederatedScope <cite class="ltx_cite ltx_citemacro_cite">Xie et al. (<a href="#bib.bib183" title="" class="ltx_ref">2023</a>)</cite>, it supports federated fine-tuning of LLMs under various FL scenarios, including FedPEFT and model personalization.</p>
</div>
</li>
<li id="A2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i2.p1.1" class="ltx_p"><span id="A2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">NVIDIA FLARE</span> <cite class="ltx_cite ltx_citemacro_cite">Roth et al. (<a href="#bib.bib142" title="" class="ltx_ref">2024</a>)</cite> is an FL framework that allows researchers and data scientists to seamlessly move their machine learning and deep learning workflows into a federated paradigm.</p>
</div>
</li>
<li id="A2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i3.p1" class="ltx_para">
<p id="A2.I1.i3.p1.1" class="ltx_p"><span id="A2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">FATE-LLM</span> <cite class="ltx_cite ltx_citemacro_cite">Fan et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite> is an industrial-grade FL framework for LLM. Apart from FedPEFT, it provides a privacy hub integrating several IP protection and privacy-preserving mechanisms to protect model security and data privacy.</p>
</div>
</li>
<li id="A2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i4.p1" class="ltx_para">
<p id="A2.I1.i4.p1.1" class="ltx_p"><span id="A2.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">FedLLM</span> <cite class="ltx_cite ltx_citemacro_cite">FedML (<a href="#bib.bib44" title="" class="ltx_ref">2023</a>)</cite> is an MLOps-supported training pipeline built upon the FedML AI platform <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib65" title="" class="ltx_ref">2020</a>)</cite>. FedLLM is compatible with popular LLM libraries such as HuggingFace and DeepSpeed to support a large range of FMs and datasets.</p>
</div>
</li>
<li id="A2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i5.p1" class="ltx_para">
<p id="A2.I1.i5.p1.1" class="ltx_p"><span id="A2.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">OpenFedLLM</span> <cite class="ltx_cite ltx_citemacro_cite">Ye et al. (<a href="#bib.bib194" title="" class="ltx_ref">2024</a>)</cite> is a federated tuning framework for LLMs, which covers applications of instruction tuning and value alignment, diverse FL baselines, training datasets, and evaluation datasets.</p>
</div>
</li>
<li id="A2.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i6.p1" class="ltx_para">
<p id="A2.I1.i6.p1.1" class="ltx_p"><span id="A2.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Shepherd</span> <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib204" title="" class="ltx_ref">2024b</a>)</cite> is a lightweight federated tuning framework. The local training process of Shepherd is built upon the implementations of Alpaca-LoRA <cite class="ltx_cite ltx_citemacro_cite">Wang (<a href="#bib.bib170" title="" class="ltx_ref">2023</a>)</cite>, and Hugging Face’s PEFT <cite class="ltx_cite ltx_citemacro_cite">Mangrulkar et al. (<a href="#bib.bib116" title="" class="ltx_ref">2022</a>)</cite>, enabling efficient fine-tuning.</p>
</div>
</li>
<li id="A2.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i7.p1" class="ltx_para">
<p id="A2.I1.i7.p1.1" class="ltx_p"><span id="A2.I1.i7.p1.1.1" class="ltx_text ltx_font_bold">FedPETuning</span> <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib210" title="" class="ltx_ref">2023f</a>)</cite> is a pioneering federated benchmark for four representative FedPEFT methods, covering adapter tuning, prefix tuning, LoRA, and BitFit.</p>
</div>
</li>
<li id="A2.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i8.p1" class="ltx_para">
<p id="A2.I1.i8.p1.1" class="ltx_p"><span id="A2.I1.i8.p1.1.1" class="ltx_text ltx_font_bold">FedLegal</span> <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib208" title="" class="ltx_ref">2023e</a>)</cite> is the very first real-world FL benchmark for legal NLP, which comprises five legal NLP tasks and one privacy task based on the data from Chinese courts.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.12843" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.12844" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.12844">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.12844" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.12845" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Jul  6 00:28:13 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
