<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation</title>
<!--Generated on Tue Jun 11 21:22:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.08255v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#S1" title="In M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#S2" title="In M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Dataset Description</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#S2.SS0.SSS0.Px1" title="In 2 Dataset Description ‣ M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_title">Data Sourcing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#S2.SS0.SSS0.Px2" title="In 2 Dataset Description ‣ M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_title">Document Annotation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#S2.SS0.SSS0.Px3" title="In 2 Dataset Description ‣ M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_title">Supplementary Data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#S3" title="In M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Multi-Modal Document Translation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#S3.SS0.SSS0.Px1" title="In 3 Multi-Modal Document Translation ‣ M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_title">Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#S4" title="In M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#S5" title="In M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#A1" title="In M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Annotation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#A2" title="In M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Detailed Results</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_typewriter" id="id13.id1">M3T</span>: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Benjamin Hsu<sup class="ltx_sup" id="id14.13.id1">◇</sup>, Xiaoyu Liu<span class="ltx_note ltx_role_footnote" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Equal contribution. Work conducted during internships at Amazon.</span></span></span><sup class="ltx_sup" id="id15.14.id2">†</sup>,
Huayang Li<span class="ltx_note ltx_role_footnote" id="footnotex2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Equal contribution. Work conducted during internships at Amazon.</span></span></span><sup class="ltx_sup" id="id16.15.id3">‡</sup>,
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id11.11.8">Yoshinari Fujinuma<sup class="ltx_sup" id="id11.11.8.1"><span class="ltx_text ltx_font_medium" id="id11.11.8.1.1">◇</span></sup>,
Maria Nadejde<sup class="ltx_sup" id="id11.11.8.2"><span class="ltx_text ltx_font_medium" id="id11.11.8.2.1">◇</span></sup>,
Xing Niu<sup class="ltx_sup" id="id11.11.8.3"><span class="ltx_text ltx_font_medium" id="id11.11.8.3.1">◇</span></sup>, 
<br class="ltx_break"/>Yair Kittenplon<sup class="ltx_sup" id="id11.11.8.4"><span class="ltx_text ltx_font_medium" id="id11.11.8.4.1">◇</span></sup>,
Ron Litman<sup class="ltx_sup" id="id11.11.8.5"><span class="ltx_text ltx_font_medium" id="id11.11.8.5.1">◇</span></sup>,
Raghavendra Pappagari<sup class="ltx_sup" id="id11.11.8.6"><span class="ltx_text ltx_font_medium" id="id11.11.8.6.1">◇</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id11.11.8.7"><span class="ltx_text ltx_font_medium" id="id11.11.8.7.1">†</span></sup> University of Maryland, College Park               
<sup class="ltx_sup" id="id11.11.8.8"><span class="ltx_text ltx_font_medium" id="id11.11.8.8.1">‡</span></sup>Nara Institute of Science and Technology 
<br class="ltx_break"/>      </span><span class="ltx_text ltx_font_typewriter" id="id17.16.id4">xliu1231@umd.edu</span><span class="ltx_text ltx_font_bold" id="id18.17.id5">                              
    </span><span class="ltx_text ltx_font_typewriter" id="id12.12.9">li.huayang.lh6@is.naist.jp
<br class="ltx_break"/><sup class="ltx_sup" id="id12.12.9.1"><span class="ltx_text ltx_font_serif" id="id12.12.9.1.1">◇</span></sup></span><span class="ltx_text ltx_font_bold" id="id19.18.id6">AWS AI Labs
<br class="ltx_break"/></span><span class="ltx_text ltx_font_typewriter" id="id20.19.id7">{benhsu,fujinuy,mnnadejd,xingniu,yairk,litmanr,pappaga}@amazon.com</span><span class="ltx_text ltx_font_bold" id="id21.20.id8">
</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id22.id1">Document translation poses a challenge for Neural Machine Translation (NMT) systems. Most document-level NMT systems rely on meticulously curated sentence-level parallel data, assuming flawless extraction of text from documents along with their precise reading order. These systems also tend to disregard additional visual cues such as the document layout, deeming it irrelevant. However, real-world documents often possess intricate text layouts that defy these assumptions. Extracting information from Optical Character Recognition (OCR) or heuristic rules can result in errors, and the layout (e.g., paragraphs, headers) may convey relationships between distant sections of text. This complexity is particularly evident in widely used PDF documents, which represent information visually. This paper addresses this gap by introducing <span class="ltx_text ltx_font_typewriter" id="id22.id1.1">M3T</span> , a novel benchmark dataset tailored to evaluate NMT systems on the comprehensive task of translating semi-structured documents. This dataset aims to bridge the evaluation gap in document-level NMT systems, acknowledging the challenges posed by rich text layouts in real-world applications.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.12">
<p class="ltx_p" id="p1.12.13"><span class="ltx_text ltx_font_typewriter" id="p1.12.13.1">M3T</span><span class="ltx_text ltx_font_bold" id="p1.12.13.2">: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.12.12" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.12.12.12" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.12.12.12.12">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.3.3.3.3.3">
<span class="ltx_td ltx_align_center" id="p1.3.3.3.3.3.3"><span class="ltx_text ltx_font_bold" id="p1.3.3.3.3.3.3.3">Benjamin Hsu<sup class="ltx_sup" id="p1.3.3.3.3.3.3.3.1"><span class="ltx_text ltx_font_medium" id="p1.3.3.3.3.3.3.3.1.1">◇</span></sup>, Xiaoyu Liu<span class="ltx_note ltx_role_footnote" id="footnotex3"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex3.1.1.1">1</span></span>Equal contribution. Work conducted during internships at Amazon.</span></span></span><sup class="ltx_sup" id="p1.3.3.3.3.3.3.3.2"><span class="ltx_text ltx_font_medium" id="p1.3.3.3.3.3.3.3.2.1">†</span></sup>,
Huayang Li<span class="ltx_note ltx_role_footnote" id="footnotex4"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex4.1.1.1">1</span></span>Equal contribution. Work conducted during internships at Amazon.</span></span></span><sup class="ltx_sup" id="p1.3.3.3.3.3.3.3.3"><span class="ltx_text ltx_font_medium" id="p1.3.3.3.3.3.3.3.3.1">‡</span></sup>,</span></span></span>
<span class="ltx_tr" id="p1.6.6.6.6.6">
<span class="ltx_td ltx_align_center" id="p1.6.6.6.6.6.3"><span class="ltx_text ltx_font_bold" id="p1.6.6.6.6.6.3.3">Yoshinari Fujinuma<sup class="ltx_sup" id="p1.6.6.6.6.6.3.3.1"><span class="ltx_text ltx_font_medium" id="p1.6.6.6.6.6.3.3.1.1">◇</span></sup>,
Maria Nadejde<sup class="ltx_sup" id="p1.6.6.6.6.6.3.3.2"><span class="ltx_text ltx_font_medium" id="p1.6.6.6.6.6.3.3.2.1">◇</span></sup>,
Xing Niu<sup class="ltx_sup" id="p1.6.6.6.6.6.3.3.3"><span class="ltx_text ltx_font_medium" id="p1.6.6.6.6.6.3.3.3.1">◇</span></sup>,</span></span></span>
<span class="ltx_tr" id="p1.9.9.9.9.9">
<span class="ltx_td ltx_align_center" id="p1.9.9.9.9.9.3"><span class="ltx_text ltx_font_bold" id="p1.9.9.9.9.9.3.3">Yair Kittenplon<sup class="ltx_sup" id="p1.9.9.9.9.9.3.3.1"><span class="ltx_text ltx_font_medium" id="p1.9.9.9.9.9.3.3.1.1">◇</span></sup>,
Ron Litman<sup class="ltx_sup" id="p1.9.9.9.9.9.3.3.2"><span class="ltx_text ltx_font_medium" id="p1.9.9.9.9.9.3.3.2.1">◇</span></sup>,
Raghavendra Pappagari<sup class="ltx_sup" id="p1.9.9.9.9.9.3.3.3"><span class="ltx_text ltx_font_medium" id="p1.9.9.9.9.9.3.3.3.1">◇</span></sup></span></span></span>
<span class="ltx_tr" id="p1.11.11.11.11.11">
<span class="ltx_td ltx_align_center" id="p1.11.11.11.11.11.2"><sup class="ltx_sup" id="p1.11.11.11.11.11.2.1">†</sup> University of Maryland, College Park               
<sup class="ltx_sup" id="p1.11.11.11.11.11.2.2">‡</sup>Nara Institute of Science and Technology</span></span>
<span class="ltx_tr" id="p1.12.12.12.12.13.1">
<span class="ltx_td ltx_align_center" id="p1.12.12.12.12.13.1.1">      <span class="ltx_text ltx_font_typewriter" id="p1.12.12.12.12.13.1.1.1">xliu1231@umd.edu</span>                              
    <span class="ltx_text ltx_font_typewriter" id="p1.12.12.12.12.13.1.1.2">li.huayang.lh6@is.naist.jp</span></span></span>
<span class="ltx_tr" id="p1.12.12.12.12.12">
<span class="ltx_td ltx_align_center" id="p1.12.12.12.12.12.1"><sup class="ltx_sup" id="p1.12.12.12.12.12.1.1">◇</sup>AWS AI Labs</span></span>
<span class="ltx_tr" id="p1.12.12.12.12.14.2">
<span class="ltx_td ltx_align_center" id="p1.12.12.12.12.14.2.1"><span class="ltx_text ltx_font_typewriter" id="p1.12.12.12.12.14.2.1.1">{benhsu,fujinuy,mnnadejd,xingniu,yairk,litmanr,pappaga}@amazon.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Traditional machine translation (MT) systems primarily focus on textual content at the sentence level, ignoring both global context and visual layout structure of a document.
Given that long-range contextual dependencies are crucial for generating high quality translations <cite class="ltx_cite ltx_citemacro_cite">Läubli et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib11" title="">2018</a>); Toral et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib25" title="">2018</a>); Hassan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib8" title="">2018</a>)</cite>, these systems fail short of achieving human translation quality when considering entire documents <cite class="ltx_cite ltx_citemacro_cite">Junczys-Dowmunt (<a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib10" title="">2019</a>)</cite>. Several auhors have sought to address these gaps by including additional context, but these focused solely on text content (i.e. preceeding sentences) <cite class="ltx_cite ltx_citemacro_citep">(Fernandes et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib3" title="">2021</a>; Lopes et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib16" title="">2020</a>; Tiedemann and Scherrer, <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib24" title="">2017</a>)</cite>.
By treating documents solely as textual content, existing methods ignore a significant portion of the information encapsulated within the visual aspects of documents.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="293" id="S1.T1.g1" src="extracted/5660360/figures/example_ocr_errors.png" width="598"/></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S1.F1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example OCR errors that occur in layout-agnostic systems. Reading order is lost when extracting text and column breaks are not correctly recognized.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S1.T1.1" style="width:433.6pt;height:266.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(70.6pt,-43.5pt) scale(1.48295229916752,1.48295229916752) ;">
<table class="ltx_tabular ltx_align_middle" id="S1.T1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.1.1.1.1">
<td class="ltx_td ltx_align_center" colspan="2" id="S1.T1.1.1.1.1.1">OCR w/o layout</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.2.2.1">Src 1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.2.2.2">general principles for <span class="ltx_text ltx_font_bold" id="S1.T1.1.1.2.2.2.1">implemet-kit</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.3.3">
<td class="ltx_td ltx_align_left" id="S1.T1.1.1.3.3.1">Src 2</td>
<td class="ltx_td ltx_align_left" id="S1.T1.1.1.3.3.2"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.3.3.2.1">ge comma vocstonal ele poly…</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.4.4.1">Tgt 1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.4.4.2">allgemeiner Grundsätze für den Implementierungssatz</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.5.5">
<td class="ltx_td ltx_align_left" id="S1.T1.1.1.5.5.1">Tgt 2</td>
<td class="ltx_td ltx_align_left" id="S1.T1.1.1.5.5.2">ge Komma Gesang ele Poly…</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S1.T1.1.1.6.6.1">OCR w/ layout</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.7.7.1">Src</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.7.7.2">general principles for implementing</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.8.8">
<td class="ltx_td" id="S1.T1.1.1.8.8.1"></td>
<td class="ltx_td ltx_align_left" id="S1.T1.1.1.8.8.2">a common vocational training policy…</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.9.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.9.9.1">Tgt</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.9.9.2">allgemeiner Grundsätze für die Umsetzung</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.10.10">
<td class="ltx_td ltx_border_bb" id="S1.T1.1.1.10.10.1"></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S1.T1.1.1.10.10.2">einer gemeinsamen Berufsbildungspolitik…</td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Without layout information, the downstream translation system cannot recover from OCR errors resulting in garbled translations. With layout information, the reading order and contiguous blocks of text are preserved, resulting in improved translation usability and quality.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Visual cues represent an important yet overlooked set of features which can provide contextual clues. For instance, real world documents typically include different typeface in denoting section headings to different parts of the document which indicates to human readers that parts of a document should be treated as a logically distinct. Text blocks can also indicate that sections of text are contextually related and multi-columnar text is easily understood by human readers. Layout elements can also indicate that certain text should <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">not</span> be translated (e.g. address or name fields) or that tabular data is present. Disregarding layout and visual information can lead to catastrophic translation errors, as demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#S1.T1" title="Table 1 ‣ 1 Introduction ‣ M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><object class="ltx_graphics ltx_centering ltx_img_landscape" data="extracted/5660360/figures/Figure1.svg" height="338" id="S1.F2.g1" type="image/svg+xml" width="598"></object>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>We introduce a benchmark dataset for PDF document translation. Unlike earlier works, our benchmark that focus on sentence-level text or cleanly segmented text, ours tests models on their ability to utilize visual queues for translating text. Above from left to right, are example documents sourced from the RVL-CDIP and DocLaynet datasets demonstrating the complex layout and domains included in the benchmark. Additional examples can be found in the Appendix, Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#A1.T4" title="Table 4 ‣ Appendix A Annotation Details ‣ M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In recent years, there has been significant focus on Vision-Language (VL) architectures, which has led to notable advancements in multi-modal reasoning <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib13" title="">2022</a>; Alayrac et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib1" title="">2022</a>; Ganz et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib5" title="">2023</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib12" title="">2023</a>; Ganz et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib4" title="">2024</a>)</cite>. Specifically, recent research has dedicated considerable attention to understanding structured documents, particularly in the domain of intelligent document processing and information extraction (e.g. visual question answering) tasks <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib30" title="">2020b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib29" title="">a</a>; Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib9" title="">2022</a>; Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib23" title="">2022</a>)</cite>. In terms of translation, earlier work has explored correcting single-word translation using visual features <cite class="ltx_cite ltx_citemacro_cite">Nikolov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib17" title="">2018</a>); Salesky et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib21" title="">2021</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib27" title="">2020</a>); Biten et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib2" title="">2022</a>)</cite>.
With recent advances in multi-modal foundation models (FMs) <cite class="ltx_cite ltx_citemacro_citep">(Alayrac et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib1" title="">2022</a>; OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib18" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib15" title="">2023</a>; Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib23" title="">2022</a>)</cite>, that combine visual encoders with large language models (LLMs), development of multi-modal models to handle visually and textually challenging tasks like translation is within reach. The ability to benchmark model performance on a challenging document understanding task that takes into account long range contextual clues is of growing importance.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Publicly available datasets for benchmarking document-level MT mainly consist of plain text documents with sentence or paragraph level alignment. The widely used JRC-Acquis consists of 8,000 documents in 20 official EU languages aligned at sentence and paragraph level <cite class="ltx_cite ltx_citemacro_citep">(Steinberger et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib22" title="">2006</a>)</cite>. Similarly, the Web Fiction benchmark <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib28" title="">2023</a>)</cite> consists of manually aligned sentence pairs that capture book and chapter-level contexts. These benchmarks address the text domain, assuming perfect segmentation of document structure (e.g. document and paragraph boundaries).</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this work, we present a multi-modal benchmark dataset for evaluating MT models on translating visually rich PDF documents. Our dataset is unique in that it focuses on machine translation at the document level and test models on both their ability to translate and their ability to use visual features as contextual clues. Our contributions are: (1) We introduce <span class="ltx_text ltx_font_typewriter" id="S1.p5.1.1">M3T</span> , a new challenging benchmark testset for evaluating end-to-end <span class="ltx_text ltx_font_bold" id="S1.p5.1.2">M</span>ulti-<span class="ltx_text ltx_font_bold" id="S1.p5.1.3">M</span>odal <span class="ltx_text ltx_font_bold" id="S1.p5.1.4">MT</span> of structured documents (Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>). To further aid in research in document understanding, we have also annotated layout and reading order of the extracted text; (2) We conduct initial experiments using a multi-modal foundation model, LLaVa-v1.5 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib15" title="">2023</a>)</cite>, and find that multi-modal features improve translation quality, though there is significant room for improvement; (3) Finally, we release synthetically generated parallel data to aid future model development by the community.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Dataset and scripts can be found here: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://github.com/amazon-science/m3t-multi-modal-translation-bench" title="">http://github.com/amazon-science/m3t-multi-modal-translation-bench</a></span></span></span></p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Dataset Description</h2>
<figure class="ltx_table" id="S2.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T2.8" style="width:433.6pt;height:939.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(88.0pt,-190.6pt) scale(1.68323261558611,1.68323261558611) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T2.8.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.8.8.9.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.9.1.1"></th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.9.1.2">RVL-CDIP</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.9.1.3">DocLayNet</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.9.1.4">EUR-Lex</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.10.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="4" id="S2.T2.8.8.10.2.1">Layout Elements</th>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.11.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.8.8.11.3.1">Text</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.8.8.11.3.2">2780</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.8.8.11.3.3">6679</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.8.8.11.3.4">1325</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.12.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.12.4.1">Section-header</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.12.4.2">144</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.12.4.3">1070</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.12.4.4">999</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.13.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.13.5.1">Page-footer</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.13.5.2">92</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.13.5.3">685</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.13.5.4">0</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.14.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.14.6.1">Vertical</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.14.6.2">68</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.14.6.3">133</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.14.6.4">1088</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.15.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.15.7.1">Table-cell</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.15.7.2">1332</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.15.7.3">16292</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.15.7.4">16451</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.16.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.16.8.1">Table</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.16.8.2">44</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.16.8.3">256</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.16.8.4">445</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.17.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.17.9.1">Page-header</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.17.9.2">220</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.17.9.3">525</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.17.9.4">1532</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.18.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.18.10.1">Picture</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.18.10.2">28</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.18.10.3">444</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.18.10.4">0</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.19.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.19.11.1">List-item</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.19.11.2">204</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.19.11.3">2406</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.19.11.4">1317</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.20.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.20.12.1">Formula</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.20.12.2">0</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.20.12.3">23</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.20.12.4">2</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.21.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.21.13.1">Footnote</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.21.13.2">0</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.21.13.3">93</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.21.13.4">391</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.22.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.22.14.1">Code</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.22.14.2">0</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.22.14.3">13</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.22.14.4">0</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.23.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.23.15.1">Title</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.23.15.2">0</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.23.15.3">74</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.23.15.4">30</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.24.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.24.16.1">Caption</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.24.16.2">0</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.24.16.3">76</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.24.16.4">24</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.25.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="4" id="S2.T2.8.8.25.17.1">Language Pairs</th>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.1.1.1.1">en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T2.1.1.1.1.m1.1"><semantics id="S2.T2.1.1.1.1.m1.1a"><mo id="S2.T2.1.1.1.1.m1.1.1" stretchy="false" xref="S2.T2.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.1.m1.1b"><ci id="S2.T2.1.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T2.1.1.1.1.m1.1d">→</annotation></semantics></math>de</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.2">20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.3">126</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1.4">0</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.2.2.2.1">de<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T2.2.2.2.1.m1.1"><semantics id="S2.T2.2.2.2.1.m1.1a"><mo id="S2.T2.2.2.2.1.m1.1.1" stretchy="false" xref="S2.T2.2.2.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T2.2.2.2.1.m1.1b"><ci id="S2.T2.2.2.2.1.m1.1.1.cmml" xref="S2.T2.2.2.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.2.2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T2.2.2.2.1.m1.1d">→</annotation></semantics></math>en</th>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.2.2">0</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.2.3">34</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.2.4">54</td>
</tr>
<tr class="ltx_tr" id="S2.T2.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.3.3.3.1">en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T2.3.3.3.1.m1.1"><semantics id="S2.T2.3.3.3.1.m1.1a"><mo id="S2.T2.3.3.3.1.m1.1.1" stretchy="false" xref="S2.T2.3.3.3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T2.3.3.3.1.m1.1b"><ci id="S2.T2.3.3.3.1.m1.1.1.cmml" xref="S2.T2.3.3.3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.3.3.3.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T2.3.3.3.1.m1.1d">→</annotation></semantics></math>es</th>
<td class="ltx_td ltx_align_center" id="S2.T2.3.3.3.2">20</td>
<td class="ltx_td ltx_align_center" id="S2.T2.3.3.3.3">126</td>
<td class="ltx_td ltx_align_center" id="S2.T2.3.3.3.4">0</td>
</tr>
<tr class="ltx_tr" id="S2.T2.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.4.4.4.1">es<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T2.4.4.4.1.m1.1"><semantics id="S2.T2.4.4.4.1.m1.1a"><mo id="S2.T2.4.4.4.1.m1.1.1" stretchy="false" xref="S2.T2.4.4.4.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T2.4.4.4.1.m1.1b"><ci id="S2.T2.4.4.4.1.m1.1.1.cmml" xref="S2.T2.4.4.4.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.4.4.4.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T2.4.4.4.1.m1.1d">→</annotation></semantics></math>en</th>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.2">0</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.3">0</td>
<td class="ltx_td ltx_align_center" id="S2.T2.4.4.4.4">54</td>
</tr>
<tr class="ltx_tr" id="S2.T2.5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.5.5.5.1">en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T2.5.5.5.1.m1.1"><semantics id="S2.T2.5.5.5.1.m1.1a"><mo id="S2.T2.5.5.5.1.m1.1.1" stretchy="false" xref="S2.T2.5.5.5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T2.5.5.5.1.m1.1b"><ci id="S2.T2.5.5.5.1.m1.1.1.cmml" xref="S2.T2.5.5.5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.5.5.5.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T2.5.5.5.1.m1.1d">→</annotation></semantics></math>fr</th>
<td class="ltx_td ltx_align_center" id="S2.T2.5.5.5.2">20</td>
<td class="ltx_td ltx_align_center" id="S2.T2.5.5.5.3">126</td>
<td class="ltx_td ltx_align_center" id="S2.T2.5.5.5.4">0</td>
</tr>
<tr class="ltx_tr" id="S2.T2.6.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.6.6.6.1">fr<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T2.6.6.6.1.m1.1"><semantics id="S2.T2.6.6.6.1.m1.1a"><mo id="S2.T2.6.6.6.1.m1.1.1" stretchy="false" xref="S2.T2.6.6.6.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T2.6.6.6.1.m1.1b"><ci id="S2.T2.6.6.6.1.m1.1.1.cmml" xref="S2.T2.6.6.6.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.6.6.6.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T2.6.6.6.1.m1.1d">→</annotation></semantics></math>en</th>
<td class="ltx_td ltx_align_center" id="S2.T2.6.6.6.2">0</td>
<td class="ltx_td ltx_align_center" id="S2.T2.6.6.6.3">11</td>
<td class="ltx_td ltx_align_center" id="S2.T2.6.6.6.4">54</td>
</tr>
<tr class="ltx_tr" id="S2.T2.7.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.7.7.7.1">en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T2.7.7.7.1.m1.1"><semantics id="S2.T2.7.7.7.1.m1.1a"><mo id="S2.T2.7.7.7.1.m1.1.1" stretchy="false" xref="S2.T2.7.7.7.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T2.7.7.7.1.m1.1b"><ci id="S2.T2.7.7.7.1.m1.1.1.cmml" xref="S2.T2.7.7.7.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.7.7.7.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T2.7.7.7.1.m1.1d">→</annotation></semantics></math>zh</th>
<td class="ltx_td ltx_align_center" id="S2.T2.7.7.7.2">20</td>
<td class="ltx_td ltx_align_center" id="S2.T2.7.7.7.3">126</td>
<td class="ltx_td ltx_align_center" id="S2.T2.7.7.7.4">113</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.8.1">zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T2.8.8.8.1.m1.1"><semantics id="S2.T2.8.8.8.1.m1.1a"><mo id="S2.T2.8.8.8.1.m1.1.1" stretchy="false" xref="S2.T2.8.8.8.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T2.8.8.8.1.m1.1b"><ci id="S2.T2.8.8.8.1.m1.1.1.cmml" xref="S2.T2.8.8.8.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.8.8.8.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T2.8.8.8.1.m1.1d">→</annotation></semantics></math>en</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.8.2">0</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.8.3">19</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.8.4">0</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.26.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="4" id="S2.T2.8.8.26.18.1">Document Domains</th>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.27.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.8.8.27.19.1">Scanned</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.8.8.27.19.2">80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.8.8.27.19.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.8.8.27.19.4">-</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.28.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.28.20.1">Govt. tenders</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.28.20.2">-</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.28.20.3">102</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.28.20.4">-</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.29.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.29.21.1">Patents</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.29.21.2">-</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.29.21.3">36</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.29.21.4">-</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.30.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.30.22.1">Legal</th>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.30.22.2">-</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.30.22.3">71</td>
<td class="ltx_td ltx_align_center" id="S2.T2.8.8.30.22.4">383</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.31.23">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S2.T2.8.8.31.23.1">Fin. reports</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.8.8.31.23.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.8.8.31.23.3">344</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T2.8.8.31.23.4">-</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Distribution of the layout elements, language pairs, and document domains contained in the benchmark dataset.
Language pairs indicate the original language of the source document.
</figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_typewriter" id="S2.p1.1.1">M3T</span> focuses on PDF documents which are a commonly utilized format that pose several challenges for modern language models. Even in digitally generated PDFs, certain artifacts, such as white characters strategically placed for spacing adjustments or the duplication of characters in the absence of bold-face options, can complicate straightforward text extraction.</p>
</div>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Data Sourcing</h3>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">We sourced documents from several public datasets to cover a wide range of documents. First, we used EUR-Lex<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://eur-lex.europa.eu/homepage.html" title="">https://eur-lex.europa.eu/homepage.html</a></span></span></span> to source documents which are translated by professional translators into all the European languages while also preserving the document layout. We sample a subset of documents which we annotated with layout information described in the next section.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p2.1">Second, we sourced documents from DocLayNet <cite class="ltx_cite ltx_citemacro_cite">Pfitzmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib19" title="">2022</a>)</cite> and RVL-CDIP <cite class="ltx_cite ltx_citemacro_citep">(Harley et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib7" title="">2015</a>)</cite>, which consist of documents annotated with layout information. We sourced annotations on the documents from their respective test sets to preserve the train/test split that might be used in training layout/vision models (e.g. LayoutLM).</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p3.1">For the sampling strategy, we focused on covering a wide range of layout elements. For RVL-CDIP, were randomly selected, excluding fax cover sheets. For DocLayNet we sampled documents containing flat layouts, multi-columnar data, images and tabular information. For EUR-Lex, we used an initial set of automatically labeled layout elements for sampling documents, and selected documents containing more than just text elements (e.g. tables, captions, footnotes) that were not previously translated and represent more complex layout features. Initial automatic labelling was done using a Faster R-CNN classifier on the DocLayNet dataset using the standard <span class="ltx_text ltx_font_typewriter" id="S2.SS0.SSS0.Px1.p3.1.1">dectrctron2<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif" id="footnote3.1.1.1">3</span></span><span class="ltx_text ltx_font_serif" id="footnote3.5">https://github.com/facebookresearch/detectron2</span></span></span></span></span> following <cite class="ltx_cite ltx_citemacro_citet">Pfitzmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib19" title="">2022</a>)</cite>. Our classifier had an F1 of 73.4 averaged across all layout elements.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">Document Annotation</h3>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">We annotated the sampled documents in two stages: (1) Annotators first extracted the text and labeled the layout elements; (2) Annotators translated the text extracted in each layout element. Table <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#S2.T2" title="Table 2 ‣ 2 Dataset Description ‣ M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a> reports the final distribution of layouts and domain coverage.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p2.1">For the layout information, annotators were asked to provide bounding box coordinates and labels for each element. Bounding box labeling means grouping the elements on the page into reasonable paragraphs, headings, tables, pictures, etc. areas on the page, in non-overlapping rectangular areas (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>). For the set of labels, we extended the DocLayNet ontology to include reading order and table cells. Specifically, annotators labelled the layout elements described in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:annotation:labels</span>.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p3">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p3.1">For translating the extracted text, we provided professional translators with the original document and asked to take into account the context of the document when translating. Extracted text was then translated using a selection of commercial segment level MT systems. Translators were then instructed to post-edit these translations with the additional requirement that the length of the translation within <math alttext="\pm" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p3.1.m1.1"><semantics id="S2.SS0.SSS0.Px2.p3.1.m1.1a"><mo id="S2.SS0.SSS0.Px2.p3.1.m1.1.1" xref="S2.SS0.SSS0.Px2.p3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p3.1.m1.1b"><csymbol cd="latexml" id="S2.SS0.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p3.1.m1.1d">±</annotation></semantics></math>10% of the source text length, which is useful for evaluating how well systems may preserve the original layout of the source document. In order to facilitate length compliance, we sourced documents in de, es, and fr and asked to translate them into en. We also asked to translate en documents to into zh. Additional annotation details can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#A1" title="Appendix A Annotation Details ‣ M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h3 class="ltx_title ltx_title_paragraph">Supplementary Data</h3>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">We also provide automatically annotated EUR-Lex documents for future research (e.g., model training). First, we downloaded the plain text parallel corpus based on EUR-Lex <cite class="ltx_cite ltx_citemacro_citep">(Steinberger et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib22" title="">2006</a>, JRC-Acquis)</cite> and extracted text from EUR-Lex documents using Tesseract OCR.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tesseract-ocr/tesseract" title="">https://github.com/tesseract-ocr/tesseract</a></span></span></span> Next, we aligned text in JRC-Acquis with recognized (OCR’d) bounding boxes in EUR-Lex documents. Finally, we erased unaligned bounding boxes and produced tuples of <span class="ltx_text ltx_font_typewriter" id="S2.SS0.SSS0.Px3.p1.1.1">&lt;source-text, OCR’d-text, bounding-box, target-text&gt;</span>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Multi-Modal Document Translation</h2>
<figure class="ltx_table" id="S3.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.3" style="width:433.6pt;height:243.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(88.7pt,-49.8pt) scale(1.6918063744306,1.6918063744306) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.3.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r" id="S3.T3.3.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="3" id="S3.T3.3.1.1.1.2">Visual Features</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="2" id="S3.T3.3.1.1.1.3">doc-COMET</th>
</tr>
<tr class="ltx_tr" id="S3.T3.3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.3.1.2.2.1">Text</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T3.3.1.2.2.2">Rand.</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T3.3.1.2.2.3">Block</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.3.1.2.2.4">Doc</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.3.1.2.2.5">EUR-Lex</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.3.1.2.2.6">DocLayNet</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.3.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T3.3.1.3.1.1" rowspan="3"><span class="ltx_text" id="S3.T3.3.1.3.1.1.1">OCR</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T3.3.1.3.1.2">✓</td>
<td class="ltx_td ltx_border_t" id="S3.T3.3.1.3.1.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T3.3.1.3.1.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.3.1.3.1.5">0.0173</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.3.1.3.1.6">0.0120</td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.1.4.2">
<td class="ltx_td" id="S3.T3.3.1.4.2.1"></td>
<td class="ltx_td" id="S3.T3.3.1.4.2.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.3.1.4.2.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.3.1.4.2.4"><span class="ltx_text ltx_font_bold" id="S3.T3.3.1.4.2.4.1">0.0174</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.3.1.4.2.5"><span class="ltx_text ltx_font_bold" id="S3.T3.3.1.4.2.5.1">0.0126</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.1.5.3">
<td class="ltx_td" id="S3.T3.3.1.5.3.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T3.3.1.5.3.2">✓</td>
<td class="ltx_td ltx_border_r" id="S3.T3.3.1.5.3.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.3.1.5.3.4">0.0174</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.3.1.5.3.5">0.0122</td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.1.6.4">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.3.1.6.4.1" rowspan="3"><span class="ltx_text" id="S3.T3.3.1.6.4.1.1">Gold</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T3.3.1.6.4.2">✓</td>
<td class="ltx_td ltx_border_t" id="S3.T3.3.1.6.4.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S3.T3.3.1.6.4.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.3.1.6.4.5">0.0214</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.3.1.6.4.6">0.0193</td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.1.7.5">
<td class="ltx_td" id="S3.T3.3.1.7.5.1"></td>
<td class="ltx_td" id="S3.T3.3.1.7.5.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.3.1.7.5.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.3.1.7.5.4"><span class="ltx_text ltx_font_bold" id="S3.T3.3.1.7.5.4.1">0.0218</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.3.1.7.5.5"><span class="ltx_text ltx_font_bold" id="S3.T3.3.1.7.5.5.1">0.0194</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.1.8.6">
<td class="ltx_td ltx_border_b" id="S3.T3.3.1.8.6.1"></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T3.3.1.8.6.2">✓</td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S3.T3.3.1.8.6.3"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T3.3.1.8.6.4">0.0215</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T3.3.1.8.6.5">0.0192</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results from our experiments using LLaVa-v1.5 using different visual features such as random images (Rand.), images of text blocks (Block), and images of the whole document (Doc). We tested how these features are used to improve OCR’d text (OCR) and if they provide additional context for clean text (Gold). We found that visual features improved document level translation quality in terms of doc-COMET scores. Results are averaged over de, fr<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.T3.2.m1.1"><semantics id="S3.T3.2.m1.1b"><mo id="S3.T3.2.m1.1.1" stretchy="false" xref="S3.T3.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.m1.1c"><ci id="S3.T3.2.m1.1.1.cmml" xref="S3.T3.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.m1.1e">→</annotation></semantics></math>en language pairs. We found LLaVa is capable of using visual features to address OCR errors. It also appears to generate better translations when given the document image as context. </figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Multi-modal document translation is under explored because of a lack of annotated benchmark datasets. Given the creation of a novel benchmark dataset in this work, we conducted experiments testing how recent multi-modal FMs perform on multi-modal document translations.
To motivate future research, we experiment with a recent LLaVa-v1.5 model <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib15" title="">2023</a>)</cite>, a large multi-modal model that combines a CLIP vision encoder ViT-L <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib20" title="">2021</a>)</cite> and Vicuna-13b <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib31" title="">2023</a>)</cite> for general-purpose visual and language understanding.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We conduct experiments adding various visual and layout features to the model. For visually rich documents, evaluating OCR text translation directly is difficult since an alignment between extracted source segments with clean source segments needs to be found. We leave the development of such an alignment model to future work. Instead, our experiments focus on whether the visual features are utilized by a multi-modal FM to improve translations by (1) recovering from OCR errors or (2) understanding the context of the document. We focused our experiments on X<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><mo id="S3.p2.1.m1.1.1" stretchy="false" xref="S3.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">→</annotation></semantics></math>en documents for reasons discussed below.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">To test these hypothesizes, we controlled for the effects of incorrect layout prediction
by conducting experiments using the gold annotated layout elements (e.g., annotated bounding boxes). For hypothesis (1), we experimented with different granularity of visual features (at the text block level or at the document level). We used Tesseract for OCR in our experiments to extract text blocks and passed a single text block at a time to the LLaVA model. We also compared with using translations gold source text translation to understand the upper bound on translation quality. Finally to test hypothesis (2), we experimented with translations using the gold text to see if less granular visual features enable a multi-modal FM to understand contextual clues better. As a baseline, we tested random images from the MS COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib14" title="">2014</a>)</cite> into the LLaVa model which allows us to measure improvements from including relevant visual features. We conducted ablations using images of identified text blocks or images of the entire document. Lastly for evaluation, since the reading order of the text was annotated, we used document level COMET scores<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/amazon-science/doc-mt-metrics</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Vernikos et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib26" title="">2022</a>)</cite> with a context size of two (i.e., two previous segments as context).</p>
</div>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Results</h3>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#S3.T3" title="Table 3 ‣ 3 Multi-Modal Document Translation ‣ M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a> summarizes the results from our experiments.
We found that visual features helped the model improve translations of OCR’d text. We found evidence for this comparing performance on different languages. Tesseract OCR performance is known to vary significantly for none Latin script languages <cite class="ltx_cite ltx_citemacro_citep">(Gupte et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.08255v1#bib.bib6" title="">2021</a>)</cite>. In particular, on Chinese text, we found that including images in the model improved performance by +0.02 when including images of the entire document.</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p2.1">We found that including relevant visual features improved upon random visual features from the MS COCO dataset. Moreover, features focused on the relevant text blocks improved translation quality (as measured by doc-COMET score) on both EUR-Lex and DocLayNet subsets of data. When looking at document level visual features, we found that these universally improved translation quality beyond the baseline with random visual features.</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p3.1">While there were improvements, visual features did not help translation appreciably. This presents an opportunity for future researchers to find methods to leverage contextual clues in the visual domain more fully. One issue was that the Vicuna-13b model suffered from hallucinations especially when the inputs are short, single word, or numerical segments which is the case for many of the tabular data from both EUR-Lex and DocLayNet documents. In the case of en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p3.1.m1.1"><semantics id="S3.SS0.SSS0.Px1.p3.1.m1.1a"><mo id="S3.SS0.SSS0.Px1.p3.1.m1.1.1" stretchy="false" xref="S3.SS0.SSS0.Px1.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p3.1.m1.1b"><ci id="S3.SS0.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p3.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p3.1.m1.1d">→</annotation></semantics></math>X translation, we found that the model misinterpreted the prompt and did not produce translations.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusions</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In conclusion, we release <span class="ltx_text ltx_font_typewriter" id="S4.p1.1.1">M3T</span> , a new benchmark for assessing multi-modal machine translation of visually-rich documents. We conducted experiments using a multi-modal FM built ontop of Vicuna-13b and found that while these models attain impressive results on visual question-answering and captioning tasks, multi-modal document translation is still an area for future research.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitations</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this work, we used available open source models which were not specifically tuned for this benchmark task. Finally, while we sourced documents from large set of domains, the benchmark is ultimately limited to a few cases and heavily focused on legal and financial text in a few high resource languages (de, es, fr, and zh) which can introduce bias in future model evaluation and development.
Furthermore, we did not experiment extensively with prompts, and an interesting future direction could be an investigation into whether their are better prompts to use for multi-modal models.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank Tanya Badeka and Jen Wang for their help in curating the dataset and Yogarshi Vyas and Stanislas Lauly for their comments and suggestions early in this project.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. (2022)</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina
Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew
Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo
Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=EbMuimAbPbs" title="">Flamingo: a
visual language model for few-shot learning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biten et al. (2022)</span>
<span class="ltx_bibblock">
Ali Furkan Biten, Ron Litman, Yusheng Xie, Srikar Appalaraju, and R Manmatha.
2022.

</span>
<span class="ltx_bibblock">Latr: Layout-aware transformer for scene-text vqa.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 16548–16558.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernandes et al. (2021)</span>
<span class="ltx_bibblock">
Patrick Fernandes, Kayo Yin, Graham Neubig, and André F. T. Martins. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.505" title="">Measuring and
increasing context usage in context-aware machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 6467–6478,
Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ganz et al. (2024)</span>
<span class="ltx_bibblock">
Roy Ganz, Yair Kittenplon, Aviad Aberdam, Elad Ben Avraham, Oren Nuriel, Shai
Mazor, and Ron Litman. 2024.

</span>
<span class="ltx_bibblock">Question aware vision transformer for multimodal reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2402.05472</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ganz et al. (2023)</span>
<span class="ltx_bibblock">
Roy Ganz, Oren Nuriel, Aviad Aberdam, Yair Kittenplon, Shai Mazor, and Ron
Litman. 2023.

</span>
<span class="ltx_bibblock">Towards models that can see and read.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2301.07389</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupte et al. (2021)</span>
<span class="ltx_bibblock">
Amit Gupte, Alexey Romanov, Sahitya Mantravadi, Dalitso Banda, Jianjie Liu,
Raza Khan, Lakshmanan Ramu Meenal, Benjamin Han, and Soundar Srinivasan.
2021.

</span>
<span class="ltx_bibblock">Lights, camera, action! a framework to improve nlp accuracy over ocr
documents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Document Intelligence Workshop at KDD 2021</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harley et al. (2015)</span>
<span class="ltx_bibblock">
Adam W. Harley, Alex Ufkes, and Konstantinos G. Derpanis. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICDAR.2015.7333910" title="">Evaluation of
deep convolutional nets for document image classification and retrieval</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">2015 13th International Conference on Document Analysis and
Recognition (ICDAR)</em>, pages 991–995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassan et al. (2018)</span>
<span class="ltx_bibblock">
Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark,
Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis,
Mu Li, Shujie Liu, Tie-Yan Liu, Renqian Luo, Arul Menezes, Tao Qin, Frank
Seide, Xu Tan, Fei Tian, Lijun Wu, Shuangzhi Wu, Yingce Xia, Dongdong Zhang,
Zhirui Zhang, and Ming Zhou. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1803.05567" title="">Achieving human parity on
automatic chinese to english news translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2022)</span>
<span class="ltx_bibblock">
Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022.

</span>
<span class="ltx_bibblock">Layoutlmv3: Pre-training for document ai with unified text and image
masking.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 30th ACM International Conference on
Multimedia</em>, pages 4083–4091.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Junczys-Dowmunt (2019)</span>
<span class="ltx_bibblock">
Marcin Junczys-Dowmunt. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W19-5321" title="">Microsoft translator
at WMT 2019: Towards large-scale document-level neural machine
translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the Fourth Conference on Machine Translation
(Volume 2: Shared Task Papers, Day 1)</em>, pages 225–233, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Läubli et al. (2018)</span>
<span class="ltx_bibblock">
Samuel Läubli, Rico Sennrich, and Martin Volk. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1512" title="">Has machine translation
achieved human parity? a case for document-level evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pages 4791–4796, Brussels, Belgium.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image
encoders and large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2301.12597</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022.

</span>
<span class="ltx_bibblock">Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">International Conference on Machine Learning</em>, pages
12888–12900. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Computer Vision – ECCV 2014</em>, pages 740–755, Cham.
Springer International Publishing.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.microsoft.com/en-us/research/publication/visual-instruction-tuning/" title="">Visual instruction tuning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">NeurIPS</em>.

</span>
<span class="ltx_bibblock">Oral Presentation Project Page: https://llava-vl.github.io/.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lopes et al. (2020)</span>
<span class="ltx_bibblock">
António Lopes, M. Amin Farajian, Rachel Bawden, Michael Zhang, and
André F. T. Martins. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.eamt-1.24" title="">Document-level
neural MT: A systematic comparison</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 22nd Annual Conference of the European
Association for Machine Translation</em>, pages 225–234, Lisboa, Portugal.
European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nikolov et al. (2018)</span>
<span class="ltx_bibblock">
Nikola I. Nikolov, Yuhuang Hu, Mi Xue Tan, and Richard H.R. Hahnloser. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6302" title="">Character-level
Chinese-English translation through ASCII encoding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the Third Conference on Machine Translation:
Research Papers</em>, pages 10–16, Brussels, Belgium. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2303.08774" title="">Gpt-4 technical report</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfitzmann et al. (2022)</span>
<span class="ltx_bibblock">
Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter
Staar. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3534678.3539043" title="">Doclaynet: A large
human-annotated dataset for document-layout segmentation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining</em>, KDD ’22, page 3743–3751, New York, NY, USA.
Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v139/radford21a.html" title="">Learning
transferable visual models from natural language supervision</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 38th International Conference on Machine
Learning</em>, volume 139 of <em class="ltx_emph ltx_font_italic" id="bib.bib20.2.2">Proceedings of Machine Learning Research</em>,
pages 8748–8763. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salesky et al. (2021)</span>
<span class="ltx_bibblock">
Elizabeth Salesky, David Etter, and Matt Post. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-main.576" title="">Robust
open-vocabulary translation from visual text representations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pages 7235–7252, Online and Punta Cana,
Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Steinberger et al. (2006)</span>
<span class="ltx_bibblock">
Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaž
Erjavec, Dan Tufiş, and Dániel Varga. 2006.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://www.lrec-conf.org/proceedings/lrec2006/pdf/340_pdf.pdf" title="">The JRC-Acquis: A multilingual aligned parallel corpus with 20+
languages</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the Fifth International Conference on
Language Resources and Evaluation (LREC’06)</em>, Genoa, Italy. European
Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2022)</span>
<span class="ltx_bibblock">
Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu,
Michael Zeng, Chao-Yue Zhang, and Mohit Bansal. 2022.

</span>
<span class="ltx_bibblock">Unifying vision, text, and layout for universal document processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">2023 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, pages 19254–19264.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann and Scherrer (2017)</span>
<span class="ltx_bibblock">
Jörg Tiedemann and Yves Scherrer. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W17-4811" title="">Neural machine
translation with extended context</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the Third Workshop on Discourse in Machine
Translation</em>, pages 82–92, Copenhagen, Denmark. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toral et al. (2018)</span>
<span class="ltx_bibblock">
Antonio Toral, Sheila Castilho, Ke Hu, and Andy Way. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6312" title="">Attaining the
unattainable? reassessing claims of human parity in neural machine
translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the Third Conference on Machine Translation:
Research Papers</em>, pages 113–123, Brussels, Belgium. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vernikos et al. (2022)</span>
<span class="ltx_bibblock">
Giorgos Vernikos, Brian Thompson, Prashant Mathur, and Marcello Federico. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.6" title="">Embarrassingly easy
document-level MT metrics: How to convert any pretrained metric into a
document-level metric</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the Seventh Conference on Machine Translation
(WMT)</em>, pages 118–128, Abu Dhabi, United Arab Emirates (Hybrid). Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Haohan Wang, Peiyan Zhang, and Eric P. Xing. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2010.09997" title="">Word shape matters: Robust
machine translation with visual embedding</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Longyue Wang, Zhaopeng Tu, Yan Gu, Siyou Liu, Dian Yu, Qingsong Ma, Chenyang
Lyu, Liting Zhou, Chao-Hong Liu, Yufeng Ma, Weiyu Chen, Yvette Graham, Bonnie
Webber, Philipp Koehn, Andy Way, Yulin Yuan, and Shuming Shi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.wmt-1.3" title="">Findings of the wmt
2023 shared task on discourse-level literary translation: A fresh orb in the
cosmos of llms</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the Eighth Conference on Machine
Translation</em>, pages 55–67, Singapore. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2020a)</span>
<span class="ltx_bibblock">
Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu,
Dinei Florencio, Cha Zhang, Wanxiang Che, et al. 2020a.

</span>
<span class="ltx_bibblock">Layoutlmv2: Multi-modal pre-training for visually-rich document
understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2012.14740</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2020b)</span>
<span class="ltx_bibblock">
Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou.
2020b.

</span>
<span class="ltx_bibblock">Layoutlm: Pre-training of text and layout for document image
understanding.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E.
Gonzalez, and Ion Stoica. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=uccHPGDlao" title="">Judging
LLM-as-a-judge with MT-bench and chatbot arena</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Thirty-seventh Conference on Neural Information Processing
Systems Datasets and Benchmarks Track</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Annotation Details</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">For layout annotations, professional annotators were instructed to annotate bounding box coordinates and assign a label to the type of text given the definitions outlined in Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:annotation:labels</span>. A random selection 10% of annotation per annotators was selected and manually reviewed. If more than 5% of those labels did not follow guidelines, the set was then re-annotated.</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">For translations, we followed a similar protocol. We used multiple systems to generate initial translations for each sentence per document at the segment level. Professional translators were then asked to post-edit translations based on the context of the document. A 10% of translations were sampled and checked for errors. If there were errors, these samples were re-translated.</p>
</div>
<figure class="ltx_table" id="A1.T4">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T4.1" style="width:411.9pt;height:276.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.5pt,20.5pt) scale(0.871202884713159,0.871202884713159) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T4.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T4.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="A1.T4.1.1.1.1.1">Layout</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="A1.T4.1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T4.1.1.1.1.2.1">
<span class="ltx_p" id="A1.T4.1.1.1.1.2.1.1" style="width:381.6pt;">Description</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.2.1.1.1">Captions</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T4.1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T4.1.1.2.1.2.1">
<span class="ltx_p" id="A1.T4.1.1.2.1.2.1.1" style="width:381.6pt;">Special text outside a picture or table that describes the picture or table.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.3.2.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.3.2.1.1">Footnotes</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T4.1.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T4.1.1.3.2.2.1">
<span class="ltx_p" id="A1.T4.1.1.3.2.2.1.1" style="width:381.6pt;">Typically small text at the bottom of a page, with a number or symbol that is referred to in the text above.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.4.3.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.4.3.1.1">Formula</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T4.1.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T4.1.1.4.3.2.1">
<span class="ltx_p" id="A1.T4.1.1.4.3.2.1.1" style="width:381.6pt;">Mathematical equation or chemical formula on its own line.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.5.4.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.5.4.1.1">List-item</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T4.1.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T4.1.1.5.4.2.1">
<span class="ltx_p" id="A1.T4.1.1.5.4.2.1.1" style="width:381.6pt;">One element of a list, in a hanging shape, i.e., from the second line onwards the paragraph is indented more than the first line.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.6.5.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.6.5.1.1">Page-footer</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T4.1.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T4.1.1.6.5.2.1">
<span class="ltx_p" id="A1.T4.1.1.6.5.2.1.1" style="width:381.6pt;">Repeating elements like page number at the bottom, outside of the normal text flow.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.7.6.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.7.6.1.1">Page-header</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T4.1.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T4.1.1.7.6.2.1">
<span class="ltx_p" id="A1.T4.1.1.7.6.2.1.1" style="width:381.6pt;">Repeating elements like page number at the top, outside of the normal text flow.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.8.7.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.8.7.1.1">Picture</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T4.1.1.8.7.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T4.1.1.8.7.2.1">
<span class="ltx_p" id="A1.T4.1.1.8.7.2.1.1" style="width:381.6pt;">A graphic or photograph.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.9.8.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.9.8.1.1">Section-header</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T4.1.1.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T4.1.1.9.8.2.1">
<span class="ltx_p" id="A1.T4.1.1.9.8.2.1.1" style="width:381.6pt;">Any kind of heading in the text, except overall document title.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.10.9.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.10.9.1.1">Table</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T4.1.1.10.9.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T4.1.1.10.9.2.1">
<span class="ltx_p" id="A1.T4.1.1.10.9.2.1.1" style="width:381.6pt;">Material arranged in a grid alignment with rows and columns, often with separator lines</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.11.10.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.11.10.1.1">Table-cell</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T4.1.1.11.10.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T4.1.1.11.10.2.1">
<span class="ltx_p" id="A1.T4.1.1.11.10.2.1.1" style="width:381.6pt;">Text contained in a cell of a table.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.12.11.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.12.11.1.1">Text-block</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T4.1.1.12.11.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T4.1.1.12.11.2.1">
<span class="ltx_p" id="A1.T4.1.1.12.11.2.1.1" style="width:381.6pt;">Regular text paragraphs or blocks of text that should be logically grouped together.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.13.12.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.13.12.1.1">Title</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T4.1.1.13.12.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T4.1.1.13.12.2.1">
<span class="ltx_p" id="A1.T4.1.1.13.12.2.1.1" style="width:381.6pt;">Overall title of a document typically appearing in large font or denoted with special typeface to distinguish it from the rest of the document.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.1.14.13.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.14.13.1.1">Order</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T4.1.1.14.13.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T4.1.1.14.13.2.1">
<span class="ltx_p" id="A1.T4.1.1.14.13.2.1.1" style="width:381.6pt;">The order number that the bounding box should be read on the page (i.e. the reading order). For European languages, this is generally left to right, top to bottom. For tables, we adopt a left to right, top to bottom ordering of the cells.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="A1.T4.1.1.15.14.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.15.14.1.1">Vertical</span></th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="A1.T4.1.1.15.14.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T4.1.1.15.14.2.1">
<span class="ltx_p" id="A1.T4.1.1.15.14.2.1.1" style="width:381.6pt;">Text is oriented vertically.</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Layout elements and their labels annotated in our benchmark following the DocLayNet typology.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Detailed Results</h2>
<figure class="ltx_table" id="A2.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T5.6" style="width:368.6pt;height:158.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.8pt,1.6pt) scale(0.979973639350977,0.979973639350977) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T5.6.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T5.6.6.7.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="A2.T5.6.6.7.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="3" id="A2.T5.6.6.7.1.2">Visual Features</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="6" id="A2.T5.6.6.7.1.3">doc-COMET</td>
</tr>
<tr class="ltx_tr" id="A2.T5.6.6.8.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A2.T5.6.6.8.2.1"></th>
<td class="ltx_td ltx_border_t" id="A2.T5.6.6.8.2.2"></td>
<td class="ltx_td ltx_border_t" id="A2.T5.6.6.8.2.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="A2.T5.6.6.8.2.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="A2.T5.6.6.8.2.5">EUR-Lex</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="A2.T5.6.6.8.2.6">DocLayNet</td>
</tr>
<tr class="ltx_tr" id="A2.T5.6.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A2.T5.6.6.6.7">Text</th>
<td class="ltx_td ltx_align_left" id="A2.T5.6.6.6.8">Rand</td>
<td class="ltx_td ltx_align_left" id="A2.T5.6.6.6.9">Block</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A2.T5.6.6.6.10">Doc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.1.1.1">de<math alttext="\rightarrow" class="ltx_Math" display="inline" id="A2.T5.1.1.1.1.m1.1"><semantics id="A2.T5.1.1.1.1.m1.1a"><mo id="A2.T5.1.1.1.1.m1.1.1" stretchy="false" xref="A2.T5.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A2.T5.1.1.1.1.m1.1b"><ci id="A2.T5.1.1.1.1.m1.1.1.cmml" xref="A2.T5.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.1.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T5.1.1.1.1.m1.1d">→</annotation></semantics></math>en</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.2.2.2.2">fr<math alttext="\rightarrow" class="ltx_Math" display="inline" id="A2.T5.2.2.2.2.m1.1"><semantics id="A2.T5.2.2.2.2.m1.1a"><mo id="A2.T5.2.2.2.2.m1.1.1" stretchy="false" xref="A2.T5.2.2.2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A2.T5.2.2.2.2.m1.1b"><ci id="A2.T5.2.2.2.2.m1.1.1.cmml" xref="A2.T5.2.2.2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.2.2.2.2.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T5.2.2.2.2.m1.1d">→</annotation></semantics></math>en</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T5.3.3.3.3">es<math alttext="\rightarrow" class="ltx_Math" display="inline" id="A2.T5.3.3.3.3.m1.1"><semantics id="A2.T5.3.3.3.3.m1.1a"><mo id="A2.T5.3.3.3.3.m1.1.1" stretchy="false" xref="A2.T5.3.3.3.3.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A2.T5.3.3.3.3.m1.1b"><ci id="A2.T5.3.3.3.3.m1.1.1.cmml" xref="A2.T5.3.3.3.3.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.3.3.3.3.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T5.3.3.3.3.m1.1d">→</annotation></semantics></math>en</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.4.4.4.4">de<math alttext="\rightarrow" class="ltx_Math" display="inline" id="A2.T5.4.4.4.4.m1.1"><semantics id="A2.T5.4.4.4.4.m1.1a"><mo id="A2.T5.4.4.4.4.m1.1.1" stretchy="false" xref="A2.T5.4.4.4.4.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A2.T5.4.4.4.4.m1.1b"><ci id="A2.T5.4.4.4.4.m1.1.1.cmml" xref="A2.T5.4.4.4.4.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.4.4.4.4.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T5.4.4.4.4.m1.1d">→</annotation></semantics></math>en</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.5.5.5.5">fr<math alttext="\rightarrow" class="ltx_Math" display="inline" id="A2.T5.5.5.5.5.m1.1"><semantics id="A2.T5.5.5.5.5.m1.1a"><mo id="A2.T5.5.5.5.5.m1.1.1" stretchy="false" xref="A2.T5.5.5.5.5.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A2.T5.5.5.5.5.m1.1b"><ci id="A2.T5.5.5.5.5.m1.1.1.cmml" xref="A2.T5.5.5.5.5.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.5.5.5.5.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T5.5.5.5.5.m1.1d">→</annotation></semantics></math>en</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T5.6.6.6.6">zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="A2.T5.6.6.6.6.m1.1"><semantics id="A2.T5.6.6.6.6.m1.1a"><mo id="A2.T5.6.6.6.6.m1.1.1" stretchy="false" xref="A2.T5.6.6.6.6.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A2.T5.6.6.6.6.m1.1b"><ci id="A2.T5.6.6.6.6.m1.1.1.cmml" xref="A2.T5.6.6.6.6.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.6.6.6.6.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T5.6.6.6.6.m1.1d">→</annotation></semantics></math>en</td>
</tr>
<tr class="ltx_tr" id="A2.T5.6.6.9.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A2.T5.6.6.9.3.1" rowspan="3"><span class="ltx_text" id="A2.T5.6.6.9.3.1.1">OCR</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.6.6.9.3.2">✓</td>
<td class="ltx_td ltx_border_t" id="A2.T5.6.6.9.3.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="A2.T5.6.6.9.3.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.6.6.9.3.5">0.0159</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.6.6.9.3.6">0.0187</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T5.6.6.9.3.7">0.0226</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.6.6.9.3.8">0.0123</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.6.6.9.3.9">0.0118</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T5.6.6.9.3.10">-0.011</td>
</tr>
<tr class="ltx_tr" id="A2.T5.6.6.10.4">
<td class="ltx_td" id="A2.T5.6.6.10.4.1"></td>
<td class="ltx_td" id="A2.T5.6.6.10.4.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A2.T5.6.6.10.4.3">✓</td>
<td class="ltx_td ltx_align_center" id="A2.T5.6.6.10.4.4">0.0160</td>
<td class="ltx_td ltx_align_center" id="A2.T5.6.6.10.4.5">0.0187</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.6.6.10.4.6">0.0225</td>
<td class="ltx_td ltx_align_center" id="A2.T5.6.6.10.4.7">0.0125</td>
<td class="ltx_td ltx_align_center" id="A2.T5.6.6.10.4.8">0.0128</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.6.6.10.4.9">0.013</td>
</tr>
<tr class="ltx_tr" id="A2.T5.6.6.11.5">
<td class="ltx_td" id="A2.T5.6.6.11.5.1"></td>
<td class="ltx_td ltx_align_left" id="A2.T5.6.6.11.5.2">✓</td>
<td class="ltx_td ltx_border_r" id="A2.T5.6.6.11.5.3"></td>
<td class="ltx_td ltx_align_center" id="A2.T5.6.6.11.5.4">0.0159</td>
<td class="ltx_td ltx_align_center" id="A2.T5.6.6.11.5.5">0.0189</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.6.6.11.5.6">0.0226</td>
<td class="ltx_td ltx_align_center" id="A2.T5.6.6.11.5.7">0.0123</td>
<td class="ltx_td ltx_align_center" id="A2.T5.6.6.11.5.8">0.0120</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.6.6.11.5.9">-0.010</td>
</tr>
<tr class="ltx_tr" id="A2.T5.6.6.12.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="A2.T5.6.6.12.6.1" rowspan="3"><span class="ltx_text" id="A2.T5.6.6.12.6.1.1">Gold</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.6.6.12.6.2">✓</td>
<td class="ltx_td ltx_border_t" id="A2.T5.6.6.12.6.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="A2.T5.6.6.12.6.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.6.6.12.6.5">0.0190</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.6.6.12.6.6">0.0238</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T5.6.6.12.6.7">0.0281</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.6.6.12.6.8">0.0219</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.6.6.12.6.9">0.0166</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T5.6.6.12.6.10">0.013</td>
</tr>
<tr class="ltx_tr" id="A2.T5.6.6.13.7">
<td class="ltx_td" id="A2.T5.6.6.13.7.1"></td>
<td class="ltx_td" id="A2.T5.6.6.13.7.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A2.T5.6.6.13.7.3">✓</td>
<td class="ltx_td ltx_align_center" id="A2.T5.6.6.13.7.4">0.0195</td>
<td class="ltx_td ltx_align_center" id="A2.T5.6.6.13.7.5">0.0241</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.6.6.13.7.6">0.0282</td>
<td class="ltx_td ltx_align_center" id="A2.T5.6.6.13.7.7">0.0217</td>
<td class="ltx_td ltx_align_center" id="A2.T5.6.6.13.7.8">0.0170</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T5.6.6.13.7.9">0.014</td>
</tr>
<tr class="ltx_tr" id="A2.T5.6.6.14.8">
<td class="ltx_td ltx_border_b" id="A2.T5.6.6.14.8.1"></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="A2.T5.6.6.14.8.2">✓</td>
<td class="ltx_td ltx_border_b ltx_border_r" id="A2.T5.6.6.14.8.3"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T5.6.6.14.8.4">0.0191</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T5.6.6.14.8.5">0.0238</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A2.T5.6.6.14.8.6">0.0281</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T5.6.6.14.8.7">0.0219</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A2.T5.6.6.14.8.8">0.0166</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A2.T5.6.6.14.8.9">0.014</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Results from our experiments using LLaVa-v1.5 using different visual features such as random images (Rand.), images of text blocks (Block), and images of the whole document (Doc). We tested how these features are used to improve OCR’d text (OCR) and if they provide additional context for clean text (Gold). We found that visual features improved document level translation quality in terms of doc-COMET scores. Results are averaged over de, fr<math alttext="\rightarrow" class="ltx_Math" display="inline" id="A2.T5.8.m1.1"><semantics id="A2.T5.8.m1.1b"><mo id="A2.T5.8.m1.1.1" stretchy="false" xref="A2.T5.8.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A2.T5.8.m1.1c"><ci id="A2.T5.8.m1.1.1.cmml" xref="A2.T5.8.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.8.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T5.8.m1.1e">→</annotation></semantics></math>en language pairs. We found LLaVa is capable of using visual features to address OCR errors. It also appears to generate better translations when given the document image as context. </figcaption>
</figure>
<figure class="ltx_figure" id="A2.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F3.1"><object class="ltx_graphics ltx_img_landscape" data="extracted/5660360/figures/example_eurolex_annotations.svg" height="248" id="A2.F3.1.g1" type="image/svg+xml" width="509"></object>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F3.2"><object class="ltx_graphics ltx_img_landscape" data="extracted/5660360/figures/example_doclaynet_annotations.svg" height="248" id="A2.F3.2.g1" type="image/svg+xml" width="509"></object>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F3.3"><object class="ltx_graphics ltx_img_landscape" data="extracted/5660360/figures/example_rdl_annotations.svg" height="241" id="A2.F3.3.g1" type="image/svg+xml" width="509"></object>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>In this work, we introduce a benchmark dataset for PDF document translation. Example documents sourced from (a) EUR-Lex (b) DocLayNet and (c) RVL-CDIP datasets demonstrating the complex layout and domains included in the benchmark. Unlike earlier works, our benchmark focuses on machine translation which depends more heavily on contextual information than earlier works centered around entity extraction.</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 11 21:22:00 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
