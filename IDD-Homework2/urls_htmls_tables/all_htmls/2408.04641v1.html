<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>GPT-3 Powered Information Extraction for Building Robust Knowledge Bases</title>
<!--Generated on Wed Jul 31 14:46:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
GPT-3,  Pre-trained Language Models,  In-Context Learning,  Information Extraction,  Biomedical
" lang="en" name="keywords"/>
<base href="/html/2408.04641v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S1" title="In GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S2" title="In GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S3" title="In GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Proposed Approach</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S3.SS1" title="In III Proposed Approach ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">True Few-Shot Settings</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S3.SS2" title="In III Proposed Approach ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Context-Based Learning in GPT-3</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S3.SS2.SSS1" title="In III-B Context-Based Learning in GPT-3 ‣ III Proposed Approach ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>1 </span>Sequencing of Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S3.SS2.SSS2" title="In III-B Context-Based Learning in GPT-3 ‣ III Proposed Approach ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>2 </span>Designing the Prompt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S3.SS2.SSS3" title="In III-B Context-Based Learning in GPT-3 ‣ III Proposed Approach ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>3 </span>Implementation of Logit Bias</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S3.SS2.SSS4" title="In III-B Context-Based Learning in GPT-3 ‣ III Proposed Approach ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>4 </span>Dynamic Selection of in-Context Instances</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S4" title="In GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Datasets Details</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S4.SS1" title="In IV Datasets Details ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Analyzing comparable methods</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S5" title="In GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Results and Analysis</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S5.SS1" title="In V Results and Analysis ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">GPT-3: Analyzing the effects of removing components</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S5.SS2" title="In V Results and Analysis ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Error Analysis</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S5.SS3" title="In V Results and Analysis ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">RE Error analysis</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S5.SS3.SSS1" title="In V-C RE Error analysis ‣ V Results and Analysis ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span>1 </span>NER Error Analysis</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S6" title="In GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Implementation in Knowledge Base</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S7" title="In GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">GPT-3 Powered Information Extraction for Building Robust Knowledge Bases</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Ritabrata Roy Choudhury
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id1.1.id1">School of Computer Engineering</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id2.2.id2">Kalinga Institute of Industrial Technology</span>
<br class="ltx_break"/>Bhubaneswar, Odisha, India 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id3.3.id3">email:ritabrata2003.rrc@gmail.com</span>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Soumik Dey
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id4.1.id1">School of Computer Engineering</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id5.2.id2">Kalinga Institute of Industrial Technology</span>
<br class="ltx_break"/>Bhubaneswar, Odisha, India 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id6.3.id3">email:soumikdey2@gmail.com</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">This work uses the state-of-the-art language model GPT-3 to offer a novel method of information extraction for knowledge base development. The suggested method attempts to solve the difficulties associated with obtaining relevant entities and relationships from unstructured text in order to extract structured information. We conduct experiments on a huge corpus of text from diverse fields to assess the performance of our suggested technique. The evaluation measures, which are frequently employed in information extraction tasks, include precision, recall, and F1-score. The findings demonstrate that GPT-3 can be used to efficiently and accurately extract pertinent and correct information from text, hence increasing the precision and productivity of knowledge base creation. We also assess how well our suggested approach performs in comparison to the most advanced information extraction techniques already in use. The findings show that by utilising only a small number of instances in in-context learning, our suggested strategy yields competitive outcomes with notable savings in terms of data annotation and engineering expense. Additionally, we use our proposed method to retrieve Biomedical information, demonstrating its practicality in a real-world setting. All things considered, our suggested method offers a viable way to overcome the difficulties involved in obtaining structured data from unstructured text in order to create knowledge bases. It can greatly increase the precision and effectiveness of information extraction, which is necessary for many applications including chatbots, recommendation engines, and question-answering systems.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
GPT-3, Pre-trained Language Models, In-Context Learning, Information Extraction, Biomedical

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the rapid increase in the generation of biomedical research and clinical text , it has become more and more essential for both researchers and practitioners to convert large amounts of biomedical text into structured data. Recently, pre-trained language models (PLMs), which can be either general-purpose or specialized for biomedicine, have significantly improved the ability to extract information from the biomedical text in various tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib1" title="">1</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The development of Generative Pre-trained Transformer, GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib2" title="">2</a>]</cite>, a new pre-trained language model, represents a significant advancement in the field of natural language processing. Unlike previous models which required extensive fine-tuning for specific tasks, GPT-3 can generalize unseen cases after being provided with just a few in-context examples. This opens up many new possibilities for NLP systems, including expanding emails, entity extraction from text, and generating code based on natural language instructions with only a few demonstration examples.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Newly released pre-trained language models (PLMs), including GPT-3, Megatron-Turing NLG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib3" title="">3</a>]</cite>, and the Switch Transformer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib4" title="">4</a>]</cite>, have many thousands parameters and have demonstrated remarkable achievement in natural language processing (NLP) tasks using a new research paradigm called “in-context learning.” PLMs can utilize their natural language generation skills to complete prompts or pieces of text in a manner similar to how humans approach a given task. By utilizing in-context learning, these large models can tackle various NLP issues without requiring updates to their parameters. This approach results in significant savings in terms of data annotation and engineering costs compared to traditional model training methods. It is worth noting that GPT-3’s in-context learning produces competitive outcomes in numerous NLP tasks, even when supplied with only a limited number of demonstrative examples in the prompt.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">As there are multiple potential applications for biomedical information extraction and the cost of biomedical annotations is high, alongside the challenges in model training, in-context learning has become an appealing option for biomedical use cases. To test the feasibility of this approach, we conducted a thorough and systematic study to compare the effectiveness of GPT-3 in-context learning and BERT-sized PLM fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib5" title="">5</a>]</cite> in the few-shot setting for named entity recognition (NER) and relation extraction (RE) – both important biomedical information extraction tasks. In order to maintain consistency and comprehensiveness, we utilized all the biomedical NER and RE tasks accessible in the BLURB benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib6" title="">6</a>]</cite>. To ensure an accurate evaluation, we adopted the true few-shot setting introduced by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib7" title="">7</a>]</cite> to avoid overestimating the models’ few-shot performance through model selection on a large validation set.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The main contributions of this paper are the following:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">The paper proposes an in-context learning approach for Information Extraction in Knowledge Base Construction using GPT-3, which involves creating a structured prompt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib7" title="">7</a>]</cite>, utilizing a k-nearest neighbor module <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib8" title="">8</a>]</cite>, and integrating logit biases and contextual calibration for NER and RE. The approach is evaluated using biomedical NER and RE tasks, and the results show its potential for practical use cases compared to fine-tuned models with no additional cost.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">The proposed model presents a unique approach to the implementation of information obtained through the aforementioned extraction processes. This approach involves the organization and construction of a knowledge base in a structured and systematic manner. The model’s methodology is distinct from conventional methods, and it seeks to optimize the efficiency and accuracy of the information extraction process by utilizing an innovative approach to knowledge base construction.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Overall, the proposed approach’s novelty lies in adapting GPT-3 to the task of Information Extraction for KBC, its ability to understand the context and meaning in natural language text, and the demonstration of its potential in a real-world setting. The research opens up new avenues for using advanced natural language processing techniques to extract structured information from unstructured text and has the potential to significantly improve the efficiency and accuracy of knowledge base construction.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">The resultant knowledge base may be used to support a range of applications, including chatbots, intelligent search engines, and recommendation systems. It can also be utilized in a variety of sectors, including e-commerce, healthcare, and finance. KBC is an important field of study and development in artificial intelligence and natural language processing since the accuracy and quality of the knowledge base are essential to the functionality and success of these applications.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In a variety of tasks, such as text classification, natural language inference, machine translation, question answering, table-to-text generation, and semantic parsing, GPT-3 in-context learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib2" title="">2</a>]</cite> has been found to be competitive <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib10" title="">10</a>]</cite> against supervised baselines <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib11" title="">11</a>]</cite>. Many methods have been developed to improve its performance, such as removing biases through calibration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib12" title="">12</a>]</cite>; optimizing prompt retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib10" title="">10</a>]</cite>; prompt ordering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib9" title="">9</a>]</cite>; and optimizing prompt design<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib7" title="">7</a>]</cite>. Much research hasn’t been done on how well GPT-3 performs while learning in context for information extraction tasks. Smaller GPT-3 models are assessed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib11" title="">11</a>]</cite> using a modified slot-filling task where every sample has at least one item of interest. Moreover, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib14" title="">14</a>]</cite> assess the GPT-2’s in-context learning performance using open-domain NER datasets that have been altered to maintain a particular ratio of empty to non-empty cases. Our biological NER prompt design closely references both of these pieces. We believe that our work is among the first to thoroughly assess the in-context learning capabilities of GPT-3 on IE tasks.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Several additional research paths examine ways to reframe NLP problems as language generation challenges aside from the work on in-context learning. In order to enhance few-shot learning in smaller pre-trained language models, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib15" title="">15</a>]</cite> reformulated the text classification and natural language inference tasks using a variety of manually created cloze-style templates as prompts.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib6" title="">6</a>]</cite> investigate a comparable environment but make use of an external language model to produce such templates. Both of these highlight the value of utilizing different prompt designs. In the same vein, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib16" title="">16</a>]</cite> reformulate relation extraction benchmarks as an end-to-end sequence-to-sequence work to attain state-of-the-art performance.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib17" title="">17</a>]</cite> introduced the multi-task sequence-to-sequence paradigm, and several works in the biomedical domain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib20" title="">20</a>]</cite> follow it. These works outperform previous methods on many tasks, including side effect extraction, NER, RE, natural language inference, and question answering. Several of these initiatives to rephrase IE tasks as sequence-to-sequence activities served as major inspirations for our quick design.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">According to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib7" title="">7</a>]</cite> past research overestimates PLMs’ few-shot learning capabilities by choosing models and prompts from huge validation sets. Several research in this approach has used this setting in an effort to estimate few-shot performance more precisely <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib9" title="">9</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Using the GPT-3 API directly results in a subpar performance in the biomedical domain, according to previous research analyzing GPT-3’s in-context learning capabilities on biomedical NLP tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib23" title="">23</a>]</cite>. They provide experimental findings on five biomedical NLP datasets for various tasks, such as connection extraction. We intend to deliver a thorough and in-depth evaluation of biomedical IE in our study by utilizing a well-established multi-dataset biomedical NLP benchmark and cutting-edge in-context learning methodologies. This will allow us to achieve the best performance to the best of our knowledge and capacity. However, the inadequacy of GPT-3 in-context learning for biological IE tasks is finally supported by our results and cannot be easily remedied with current methods. GPT-3 performs well on a different set of clinical Questions, according to a parallel study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib24" title="">24</a>]</cite>, which is an interesting task, such as a clinical one on the extraction of biological evidence. The reason for this unexpected difference in IE performance across the clinical and biomedical domains for in-context learning has to be investigated further.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Proposed Approach</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In the following section, we present the two approaches we explored for named entity recognition (NER) and relation extraction (RE) utilizing the genuine few-shot scenario: adapting pre-trained language models (PLM) of BERT size and in-context learning of GPT-3. The proposed model’s conceptual foundation draws from the seminal work of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib25" title="">25</a>]</cite>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">True Few-Shot Settings</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Recent research shows concerns regarding the reliability of few-shot learning in large pre-trained language models (PLMs) such as GPT-3 and minor PLM fine-tuning. The selection of models and prompts can be influenced by large validation sets. To address these concerns and avoid misjudging the performance of pre-trained language models on a small training set, we adopt the true few-shot configuration proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib7" title="">7</a>]</cite>. In this approach, the model preferences are systematically based on the small training set rather than a large validation set. We use cross-validation on a set of 100 training samples to choose the prompt structure, the number of few-shot instances for each question, and the fine-tuning evaluation metrics for our experiments.</p>
</div>
<figure class="ltx_figure" id="S3.F1">
<p class="ltx_p ltx_align_center" id="S3.F1.1"><span class="ltx_text" id="S3.F1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="261" id="S3.F1.1.1.g1" src="extracted/5766607/model.png" width="598"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Representation of the proposed approach</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Context-Based Learning in GPT-3</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The modifications we made to the named entity recognition (NER) and relation extraction (RE) objectives to accommodate in-context learning are described in this section. We also discuss other strategies we used to improve GPT-3’s performance in biological IE through in-context learning, including our procedures for developing prompts and retrieving in-context examples..</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS1.4.1.1">III-B</span>1 </span>Sequencing of Tasks</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">To tackle NER and RE, we have transformed them into language generation tasks. We have employed a similar approach to other studies, which involves identifying all the item spans in the original phrase and merging them with a separator. Our method adds entities only once and trains the GPT-3 algorithm to generate a list of items separated by the chosen separator based on the input and its context.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">To carry out relation extraction, we adopt the method proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib16" title="">16</a>]</cite> and utilize their technique of transforming each instance into a prompt, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S3.F1" title="Figure 1 ‣ III-A True Few-Shot Settings ‣ III Proposed Approach ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_tag">1</span></a>. Our prompt templates feature the original semantic representation of both the subject and object entities.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS2.4.1.1">III-B</span>2 </span>Designing the Prompt</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">The prompts must be properly chosen if you want the GPT-3 in-context learning to perform as well as possible (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib22" title="">22</a>]</cite>). We provide a methodical, task-independent method for creating GPT-3 prompts, which is divided into three key components : task assignment commands, a phrase introduction, and a recovery message. A label verbalizer is also defined for relation extraction to make it easier to produce natural language words that map relation categories. The ideal mix of prompt alternatives and the quantity of in-context examples contained in the prompt is determined using leave-one-out cross-validation (LOOCV), which we manually build a set of alternatives for each question section. We evaluate eight prompt options for each dataset to reduce expenses.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS3.4.1.1">III-B</span>3 </span>Implementation of Logit Bias</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">To avoid GPT-3 from generating tokens that were not present in the original text, we make use of the ”logit bias” feature available in the OpenAI Completion API. This option limits the set of tokens that GPT-3 can generate by increasing the likelihood of a specific set of tokens. We increase the valuation each token in the initial text by 10, along with their chosen separator and the newline token, to be more accurate. After processing, we eliminate any forecasted entities which do not resemble any spans in the original phrase.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1">We discovered that when we provided GPT-3 with a set of few-shot in-context examples, it became biased towards certain labels regardless of the test input. To address this issue, we used a technique in which the output is calibrated using a linear transformation that evenly distributes the label probabilities produced via GPT-3 when given one null prompt (where the test input is changed to ”N/A”).. They applied this approach to their RE task to remove context-induced biases.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS4.4.1.1">III-B</span>4 </span>Dynamic Selection of in-Context Instances</h4>
<div class="ltx_para" id="S3.SS2.SSS4.p1">
<p class="ltx_p" id="S3.SS2.SSS4.p1.1">Improved performance in GPT-3 in-context learning can be achieved by dynamically selecting a small number of in-context instances for every test example. According to various research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib8" title="">8</a>]</cite>, this method entails choosing the most comparable examples from the training set to serve as a few-shot in-context prompt for every training sample using a k-nearest neighbor (kNN) retrieval module. RoBERTa-large has been chosen as the encoder for the kNN retrieval module since it has been found to be superior to other biomedical PLMs, sentence-transformer models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib26" title="">26</a>]</cite>, and a BM25 baseline <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib27" title="">27</a>]</cite>. baseline.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of Performance of Fine-tuned BERT-sized PLMs with GPT-3 In-Context</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1" rowspan="2"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">GPT-3 In-Context</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S3.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1">PubMedBERT-base</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S3.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.4.1">RoBERTa-large</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S3.T1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.5.1">BioBERT-large</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.2.1">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.2.2">Recall</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.2.3">F1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.2.4">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.2.5">Recall</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.2.6">F1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.2.7">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.2.8">Recall</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.2.9">F1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.2.10">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.2.11">Recall</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.2.12">F1</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.3.3.1">JNLPBA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.3.3.2">44.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.3.3.3">52.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.3.3.4">48.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.3.3.5">56.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.3.3.6">67.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.3.3.7">61.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.3.3.8">57.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.3.3.9">75.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.3.3.10">65.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.3.3.11">57.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.3.3.12">73.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.3.3.13">64.6</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.4.4.1">NCBI-disease</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.4.4.2">55.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.4.4.3">49.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.4.4.4">51.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.4.4.5">68.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.4.4.6">67.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.4.4.7">68.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.4.4.8">64.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.4.4.9">68.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.4.4.10">66.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.4.4.11">59.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.4.4.12">67.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.4.4.13">63.0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.5.5.1">BC5CDR-disease</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.5.5.2">57.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.5.5.3">35.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.5.5.4">43.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.5.5.5">67.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.5.5.6">67.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.5.5.7">67.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.5.5.8">66.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.5.5.9">68.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.5.5.10">67.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.5.5.11">62.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.5.5.12">69.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.5.5.13">65.8</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.6.6.1">BC2GM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.6.6.2">43.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.6.6.3">40.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.6.6.4">41.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.6.6.5">55.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.6.6.6">57.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.6.6.7">56.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.6.6.8">49.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.6.6.9">56.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.6.6.10">52.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.6.6.11">53.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.6.6.12">59.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.6.6.13">56.2</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.7.7.1">BC5CDR-chem</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.7.7.2">74.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.7.7.3">71.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.7.7.4">73.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.7.7.5">86.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.7.7.6">88.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.7.7.7">87.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.7.7.8">82.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.7.7.9">87.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.7.7.10">84.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.7.7.11">84.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.7.7.12">87.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.7.7.13">86.0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.8.8.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.8.8.1.1">NER Average</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.8.8.2">55.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.8.8.3">49.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.8.8.4">51.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.8.8.5">66.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.8.8.6">69.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.8.8.7">68.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.8.8.8">64.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.8.8.9">71.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.8.8.10">67.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.8.8.11">63.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.8.8.12">71.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.8.8.13">67.1</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" id="S3.T1.1.9.9.1">ChemProt</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.1.9.9.2">15.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.1.9.9.3">68.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.1.9.9.4">25.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.1.9.9.5">17.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.1.9.9.6">62.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.1.9.9.7">27.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.1.9.9.8">22.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.1.9.9.9">69.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.1.9.9.10">33.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.1.9.9.11">19.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.1.9.9.12">60.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.1.9.9.13">28.7</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.10.10.1">DDI</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.10.10.2">99.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.10.10.3">48.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.10.10.4">16.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.10.10.5">19.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.10.10.6">79.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.10.10.7">31.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.10.10.8">25.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.10.10.9">77.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.10.10.10">38.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.10.10.11">17.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.10.10.12">75.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.10.10.13">28.2</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.11.11">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.11.11.1">GAD</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.11.11.2">51.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.11.11.3">92.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.11.11.4">66.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.11.11.5">63.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.11.11.6">57.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.11.11.7">60.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.11.11.8">64.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.11.11.9">78.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.11.11.10">70.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.11.11.11">63.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.11.11.12">72.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.11.11.13">67.6</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.12.12">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.12.12.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.12.12.1.1">RE Average</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.12.2">25.6</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.12.3">70.0</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.12.4">36.0</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.12.5">33.8</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.12.6">66.1</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.12.7">39.9</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.12.8">37.2</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.12.9">75.4</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.12.10">47.4</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.12.11">33.2</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.12.12">69.6</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.12.12.13">41.5</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Datasets Details</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">When assessing biological information extraction, we employ the identical NER and RE datasets like the ones’ which are employed in the BLURB benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib6" title="">6</a>]</cite>. The datasets’ statistics are detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S4.T2" title="TABLE II ‣ IV Datasets Details ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_tag">II</span></a>, along with other information. As stated in Section 2.3 of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib6" title="">6</a>]</cite>, the processing and train/dev/test splits are carried out. As the premise of this paper revolves around the concept from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib25" title="">25</a>]</cite>’, we conducted a simulation on the dataset procured from the same source</p>
</div>
<div class="ltx_para" id="S4.p2">
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">NCBI-disease :</span> NCBI disease corpus<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib28" title="">28</a>]</cite> a resource for disease name recognition and concept normalization. Journal of biomedical informatics, 47, 1-10.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">BC2GM :</span> The Biocreative II Gene Mention corpus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib29" title="">29</a>]</cite> is a dataset that includes 17,500 words with annotations for gene entities from PubMed articles. Recognizing the gene and protein described in text is the dataset’s aim. It is frequently employed in tasks involving named entity recognition in biomedicine.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">JNLPBA :</span> The Joint Workshop on Natural Language Processing in Biomedicine and its Applications dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib30" title="">30</a>]</cite>is composed of 2,000 MEDLINE abstracts which have been manually selected and annotated for entities related to genes.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i4.p1.1.1">BC5CDR : </span> The BioCreative V Chemical-Disease Relation corpus<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib31" title="">31</a>]</cite>, which contains PubMed abstracts with annotations of both diseases and chemicals, is used for evaluation of models in the biomedical information extraction task. The models are evaluated separately for each entity type, following the approach used in previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i5.p1.1.1">ChemProt : </span>The ChemProt dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib32" title="">32</a>]</cite> comprises 1,820 abstracts from PubMed that are labeled with annotations for chemical-protein interactions. The dataset has six relation categories, five of which are true and one is considered vacuous.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S4.I1.i6.p1">
<p class="ltx_p" id="S4.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i6.p1.1.1">DDI : </span>The DDI dataset, as described by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib33" title="">33</a>]</cite>, includes sentences from MEDLINE and DrugBank that are labeled with information about drug-drug interactions. The interactions are categorized into four true relations and one vacuous relation.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="S4.I1.i7.p1">
<p class="ltx_p" id="S4.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i7.p1.1.1">GAD : </span>The Genetic Association Database corpus<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib34" title="">34</a>]</cite> is a collection of scientific texts, including excerpts and abstracts, that have been annotated with gene-disease associations in a distant manner, meaning that the annotations were made based on information inferred from the text rather than directly stated in it.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Dataset details and statistics</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.1.1.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.1.1.2">Task</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.1.1.3">Train</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.1.1.4">Dev</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.1.1.5">Test</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.1.1.6">Eval. Metric</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.2.2.1">BC5CDR-disease</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.2.2.2">NER</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.2.2.3">4182</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.2.2.4">4244</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.2.2.5">4424</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.2.2.6">F1 entity-level</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.3.3.1">BC5CDR-chem</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.3.3.2">NER</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.3.3.3">5203</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.3.3.4">5347</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.3.3.5">5385</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.3.3.6">F1 entity-level</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.4.4.1">NCBI-disease</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.4.4.2">NER</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.4.4.3">5134</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.4.4.4">787</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.4.4.5">960</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.4.4.6">F1 entity-level</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.5.5.1">JNLPBA</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.5.5.2">NER</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.5.5.3">46750</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.5.5.4">4551</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.5.5.5">8662</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.5.5.6">F1 entity-level</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.6.6.1">BC2GM</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.6.6.2">NER</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.6.6.3">15197</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.6.6.4">3061</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.6.6.5">6325</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.6.6.6">F1 entity-level</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt" id="S4.T2.1.7.7.1">DDI</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T2.1.7.7.2">RE</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T2.1.7.7.3">25296</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T2.1.7.7.4">2496</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T2.1.7.7.5">5716</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T2.1.7.7.6">Micro F1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.8.8.1">ChemProt</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.8.8.2">RE</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.8.8.3">18035</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.8.8.4">11268</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.8.8.5">15745</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.1.8.8.6">Micro F1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9.9">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.9.9.1">GAD</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.9.9.2">RE</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.9.9.3">4261</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.9.9.4">535</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.9.9.5">534</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.9.9.6">Micro F1</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Analyzing comparable methods</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Using 100 training instances, the researchers ran tests to compare the effectiveness of four pre-trained language models: PubMedBERT-base<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib6" title="">6</a>]</cite>, BioBERT-large<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib1" title="">1</a>]</cite>, RoBERTa-large<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib9" title="">9</a>]</cite>, and GPT-3. RoBERTa-large was previously trained on general-domain text, while PubMedBERT-base and BioBERT-large were pre-trained on a sizable collection of biomedical literature from PubMed. The in-context prompt for each test example was retrieved by the researchers for the GPT-3 by employing the identical hundred training sets. This observation has been derived from the research conducted in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib25" title="">25</a>]</cite>.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Implementation Details.</span>We chose to use 100 annotated examples for their experiments, as they considered this a reasonable number to start training an information extraction model for a new assignment. We used a balanced set of 100 examples for the relation extraction task, with an equal distribution across relation types. From the previous work, we found that the BERT-sized pre-trained language models were fine-tuned using the HuggingFace Transformers library<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib35" title="">35</a>]</cite>, while for the GPT-3 experiments, we used a maximum of 10 and 5 in-context examples for NER and RE, respectively, to stay within GPT-3’s input length limit. Because GPT-3 is expensive, we only assessed each approach on a maximum of 1,000 test cases from each dataset, sampled stratified to match the distribution of relation types in the original test set. We chose models and prompt designs using the true few-shot framework, and carried out all experiments using three distinct training sets of 100 examples, reporting the mean and standard deviation to take training data volatility into account.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Results and Analysis</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S3.T1" title="TABLE I ‣ III-B4 Dynamic Selection of in-Context Instances ‣ III-B Context-Based Learning in GPT-3 ‣ III Proposed Approach ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_tag">I</span></a> contains the major findings of our experiment. As the same dataset was used for the stimulation, the resulting findings are comparable to those reported in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib25" title="">25</a>]</cite>. Our findings demonstrate that, across all datasets, fine-tuned BERT-sized PLMs outperform GPT-3 in-context learning, frequently by a large margin (with an average improvement of 15.6-16.7% for NER and 3.9-11.4% for RE in F1 scores). As GPT-3’s recall falls by twice as much in NER despite a 10-point decline in precision, this shows that GPT-3’s under-prediction of entities plays a large role in its subpar in-context learning performance. On the other hand, GPT-3 performs poorly in the ”none” relation class in RE tasks, which contributes to the sharper decline in precision.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We found that, despite the tiny size of the training sets, BERT-sized PLMs exhibit respectable performance in NER tasks after assessing the findings of our fine-tuning experiment. On account of the great lexical regularity of names of drugs, we specifically achieved strong results in the drug extraction task (BC5CDR-chem) with scores in the mid-80s. However, due to the higher lexical variability in the names of these entities, performance in other biomedical NER tasks, such as illness and gene extraction, declines to the high and low 60s. Recent research demonstrates that PLMs tuned on complete training sets also exhibit similar performance disparity. Furthermore, we found that the basic PubMedBERT model outperformed more complex variants of the general-domain RoBERTa and the biomedicine-specific BioBERT models, suggesting that pre-training on domain-specific text and vocabulary from beginning is particularly beneficial for NER tasks. These conclusions are in line with the findings of earlier research by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Due to the assessed techniques’ increased complexity, particularly in DDI and ChemProt, which have more relation types and class imbalance, the performance of these methods diminishes in RE tasks. RoBERTa-large outperforms PubMedBERT-base and BioBERT-large in the relation extraction task, in contrast to the NER task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib6" title="">6</a>]</cite> and other studies with larger training sets. This suggests that, for activities requiring sophisticated syntactic and semantic comprehension, like RE, larger-scale general-domain pre-training can offset the advantages of domain-specific pre-training.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.4.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">GPT-3: Analyzing the effects of removing components</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We conducted ablation tests on a portion of 250 validation cases from sample datasets for every task, as shown in Tables <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S5.T3" title="TABLE III ‣ V-A GPT-3: Analyzing the effects of removing components ‣ V Results and Analysis ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_tag">III</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S5.T4" title="TABLE IV ‣ V-A GPT-3: Analyzing the effects of removing components ‣ V Results and Analysis ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_tag">IV</span></a>” in order to examine the methods employed to enhance GPT-3’s efficiency. We  replaced the kNN module for another one that arbitrarily selects instances from the training set to be the in-context prompts for each test example. The findings demonstrated that the kNN module’s removal decreased the GPT-3’s in-context learning performance, with RE showing a more pronounced performance decline than NER.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">We discovered that eliminating the logit bias option decreased recall even though precision improved in their NER-specific ablation trial. This was due to the post-processing step that removes predicted entities not present in the original sentence, leading to fewer false positives but also a decrease in the number of valid spans predicted. When the kNN module and logit bias option were both removed, the drop in performance was even greater, indicating that they complement each other.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">With or without the kNN module, we discovered in the RE-specific excision investigation that eliminating the calibration module decreased precision and recall, demonstrating the efficiency of the module.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>NER ablation study on BC5CDR-disease</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.1.1.2">Precision</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.1.1.3">F1</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.1.1.4">Recall</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.2.1.1">Best Model</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.2.1.2">42.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.2.1.3">46.3</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.2.1.4">50.9</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.3.2.1">Logit Biases</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.3.2.2">66.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.3.2.3">42.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.3.2.4">31.3</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.4.3.1">kNN Module</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.4.3.2">42.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.4.3.3">46.3</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.1.4.3.4">50.9</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.1.5.4.1">Both</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.1.5.4.2">60.2</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.1.5.4.3">38.7</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.1.5.4.4">28.5</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>RE ablation study on DDI.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.1.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.1.1.1.2">Precision</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.1.1.1.3">F1</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.1.1.1.4">Recall</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.1.2.1.1">Best Model</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.1.2.1.2">16.1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.1.2.1.3">26.1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.1.2.1.4">68.0</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.1.3.2.1">Calibration</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.1.3.2.2">14.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.1.3.2.3">23.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.1.3.2.4">62.0</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.1.4.3.1">kNN Module</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.1.4.3.2">11.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.1.4.3.3">18.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.1.4.3.4">48.0</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.1.5.4.1">Both</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.1.5.4.2">10.9</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.1.5.4.3">16.9</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.1.5.4.4">38.0</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.4.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.5.2">Error Analysis</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">In this section, we conducted a thorough analysis and discovered that in-context learning struggles with handling the null class. This refers to sentences that do not contain any entities (for NER) and entity pairs that do not have any of the target relations (for RE). We found that these difficulties are not unique to biomedical applications and are likely to be problematic for Information Extraction (IE) tasks in general.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Assessment on modified BC5CDR-disease where sentences without disease entity are not removed.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" colspan="4" id="S5.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.1">Original BC5CDR-disease</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.2.2">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S5.T5.1.2.2.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.2.2.2">Precision</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.2.2.3">F1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.2.2.4">Recall</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T5.1.3.3.1">RoBERTa-large</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.3.3.2">66.9</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.3.3.3">67.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.3.3.4">68.7</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T5.1.4.4.1">GPT-3 In-Context</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.4.4.2">57.9</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.4.4.3">43.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.4.4.4">35.0</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" colspan="4" id="S5.T5.1.5.5.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.5.5.1.1">Modified BC5CDR-disease</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.6.6">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S5.T5.1.6.6.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.6.6.2">Precision</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.6.6.3">F1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.6.6.4">Recall</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T5.1.7.7.1">RoBERTa-large</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.7.7.2">68.0</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.7.7.3">70.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.1.7.7.4">72.9</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T5.1.8.8.1">GPT-3 In-Context</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.1.8.8.2">60.3</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.1.8.8.3">59.8</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.1.8.8.4">59.3</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.4.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.5.2">RE Error analysis</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Our study also looked at the impact of the null class (called ”none relation” in the DDI dataset) in the RE task. Table <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S3.T1" title="TABLE I ‣ III-B4 Dynamic Selection of in-Context Instances ‣ III-B Context-Based Learning in GPT-3 ‣ III Proposed Approach ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_tag">I</span></a> shows that when multiple relation types exist in the dataset, such as in DDI and ChemProt, GPT-3 in-context learning achieves high recall but low precision. Upon examining the confusion matrices generated through LOOCV, we discovered that GPT-3 rarely predicts the ”none relation” in the DDI dataset. This bias against the ”none relation” significantly lowers the model’s precision, especially since the DDI dataset is heavily weighted towards this class.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>Comparison of two DDI (drug-drug interaction) examples predicted by GPT-3 in-context learning and RoBERTa-large.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T6.1.1.1.1">Label</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.1.1.1.2">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.1.1.1.3">Sample</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T6.1.2.1.1" rowspan="2"><span class="ltx_text" id="S5.T6.1.2.1.1.1">Effect</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.1.2.1.2">RoBERTa-large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.1.2.1.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T6.1.2.1.3.1">
<tr class="ltx_tr" id="S5.T6.1.2.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.2.1.3.1.1.1">Concurrent use of</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.2.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.2.1.3.1.2.1">phenothiazines may</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.2.1.3.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.2.1.3.1.3.1">antagonize the</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.2.1.3.1.4">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.2.1.3.1.4.1">anorectic effect of</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.2.1.3.1.5">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.2.1.3.1.5.1">diethylpropion</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.1.3.2.1">GPT-3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.1.3.2.2">
<table class="ltx_tabular ltx_align_middle" id="S5.T6.1.3.2.2.1">
<tr class="ltx_tr" id="S5.T6.1.3.2.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.3.2.2.1.1.1">Concurrent use of</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.3.2.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.3.2.2.1.2.1">phenothiazines may</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.3.2.2.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.3.2.2.1.3.1">antagonize the</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.3.2.2.1.4">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.3.2.2.1.4.1">anorectic effect of</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.3.2.2.1.5">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.3.2.2.1.5.1">diethylpropion.</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T6.1.4.3.1" rowspan="2"><span class="ltx_text" id="S5.T6.1.4.3.1.1">None</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.1.4.3.2">RoBERTa-large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.1.4.3.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T6.1.4.3.3.1">
<tr class="ltx_tr" id="S5.T6.1.4.3.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.4.3.3.1.1.1">Other powerful CYP3A4</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.4.3.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.4.3.3.1.2.1">inhibitors (e.g., itraconazole,</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.4.3.3.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.4.3.3.1.3.1">clarithromycin, nefazodone,</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.4.3.3.1.4">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.4.3.3.1.4.1">troleandomycin, ritonavir,</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.4.3.3.1.5">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.4.3.3.1.5.1">nelfinavir) should</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.4.3.3.1.6">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.4.3.3.1.6.1">function similarly.</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T6.1.5.4.1">GPT-3</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T6.1.5.4.2">
<table class="ltx_tabular ltx_align_middle" id="S5.T6.1.5.4.2.1">
<tr class="ltx_tr" id="S5.T6.1.5.4.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.5.4.2.1.1.1">Other powerful CYP3A4</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.5.4.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.5.4.2.1.2.1">inhibitors (e.g., itraconazole,</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.5.4.2.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.5.4.2.1.3.1">clarithromycin, nefazodone,</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.5.4.2.1.4">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.5.4.2.1.4.1">troleandomycin, ritonavir,</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.5.4.2.1.5">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.5.4.2.1.5.1">nelfinavir) are not</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.5.4.2.1.6">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T6.1.5.4.2.1.6.1">predicted to function similarly.</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">In table <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S5.T6" title="TABLE VI ‣ V-C RE Error analysis ‣ V Results and Analysis ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_tag">VI</span></a>, the evaluation of the comparison of LIME-based saliency scores for two DDI examples predicted by GPT-3 in-context learning and RoBERTa-large. involves masking out words highlighted in blue and observing the change in the model’s current prediction. The drugs shown in bold are the head and tail entities for the relation being queried. The second example highlights that GPT-3 in-context learning is more susceptible to spurious surface-level signals, which can lead to incorrect predictions, particularly in predicting the none-class. To gain a better understanding of this bias, we used LIME <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib27" title="">27</a>]</cite> to analyze the predictions made by both GPT-3 and RoBERTa on effect and none examples. The first example in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S5.T6" title="TABLE VI ‣ V-C RE Error analysis ‣ V Results and Analysis ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_tag">VI</span></a> was correctly labeled by both models by relying on the relevant signal of the ”anorectic effect”. However, correct predictions for none examples often require a more implicit understanding of the sentence’s structure rather than relying on surface-level signals, as demonstrated in the second example in <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S5.T6" title="TABLE VI ‣ V-C RE Error analysis ‣ V Results and Analysis ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_tag">VI</span></a>. Here, we noticed that RoBERTa’s prediction is strongly influenced by the phrase ”of CYP3A4 (e.g.,” which suggests that the drugs within the parentheses belong to the same class and, therefore, do not interact with each other. This indicates that RoBERTa correctly uses the linguistic structure of the sentence. On the other hand, GPT-3’s incorrect mechanism prediction seems to be supported by the phrase ”expected to behave similarly,” which is irrelevant to the relation being queried between the drugs. This suggests that GPT-3’s in-context learning is more susceptible to spurious surface-level signals and, therefore, struggles with predicting the none class.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS3.SSS1.4.1.1">V-C</span>1 </span>NER Error Analysis</h4>
<div class="ltx_para" id="S5.SS3.SSS1.p1">
<p class="ltx_p" id="S5.SS3.SSS1.p1.1">When using a NER model in real-world scenarios, there may be many instances where the input sentence contains no relevant entity, which is referred to as a null class example. This is common in datasets like BC5CDR-disease, where up to 50% of sentences contain no disease. However, previous studies on GPT-3 in-context learning have not taken this into account. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#bib.bib11" title="">11</a>]</cite> removed all examples that did not contain relevant slots from their slot-filling experiment. This approach ignores the impact of null class examples on the poor performance of in-context learning. Our study shows that null class examples significantly cause this issue.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS1.p2">
<p class="ltx_p" id="S5.SS3.SSS1.p2.1">Using a modified BC5CDR-disease dataset, where all words lacking disease entities were eliminated, we ran an experiment comparing GPT-3 in-context learning with fine-tuned RoBERTa-large to examine the effects of null samples. Table <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S5.T5" title="TABLE V ‣ V-B Error Analysis ‣ V Results and Analysis ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_tag">V</span></a> shows our results, which reveal that GPT-3’s recall increased by about 24% compared to only 4% for RoBERTa-large, demonstrating that prompts having null examples strongly biases GPT-3 to predict few entities rather than contributing them to the fine-tuning data.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS1.p3">
<p class="ltx_p" id="S5.SS3.SSS1.p3.1">We believe that GPT-3’s bias in underpredicting entities is partially due to the fact that in-context learning requires it to predict relevant entities only if they are present in the given sentence, which is different from how smaller PLMs predict entities. To test this hypothesis, we simplified our experiment by removing the k-NN retrieval module and using the same two-shot prompt with one example having no entities and another with at least one entity for all examples in the BC5CDR-disease training dataset. We then added a random example without entities to every prompt and compared the probability of a null prediction in each setting. We found that adding the second null example slightly increased the null probability more for examples without entities than those with entities, accounting for the lower initial null probability for examples with one or more entities reversed this effect. The fact that the increase in null probability was not significantly larger for examples without entities suggests that GPT-3 struggles to infer the appropriate prediction constraint for this task and instead increases the null probability somewhat uniformly across examples. This is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S5.T7" title="TABLE VII ‣ V-C1 NER Error Analysis ‣ V-C RE Error analysis ‣ V Results and Analysis ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_tag">VII</span></a>. In table <a class="ltx_ref" href="https://arxiv.org/html/2408.04641v1#S5.T7" title="TABLE VII ‣ V-C1 NER Error Analysis ‣ V-C RE Error analysis ‣ V Results and Analysis ‣ GPT-3 Powered Information Extraction for Building Robust Knowledge Bases"><span class="ltx_text ltx_ref_tag">VII</span></a>, We compare the null token probability assigned by GPT-3 to examples with zero and non-zero entities in the dataset using 2-shot and 3-shot prompts. The 3-shot prompts contain an additional null example to examine its effect. The evaluation presents the average over 3 randomly chosen prompts.</p>
</div>
<figure class="ltx_table" id="S5.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VII: </span>Evaluation of GPT-3’s performance on the BC5CDR-disease training dataset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T7.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T7.1.1.2">Entity Number</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T7.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T7.1.1.3.1">
<tr class="ltx_tr" id="S5.T7.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T7.1.1.3.1.1.1">P(null)</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T7.1.1.3.1.2.1">2-Shot</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T7.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T7.1.1.4.1">
<tr class="ltx_tr" id="S5.T7.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T7.1.1.4.1.1.1">P(null)</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T7.1.1.4.1.2.1">3-Shot</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T7.1.1.1">
<table class="ltx_tabular ltx_align_middle" id="S5.T7.1.1.1.1">
<tr class="ltx_tr" id="S5.T7.1.1.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T7.1.1.1.1.2.1">Absolute</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T7.1.1.1.1.1.1"><math alttext="\Delta" class="ltx_Math" display="inline" id="S5.T7.1.1.1.1.1.1.m1.1"><semantics id="S5.T7.1.1.1.1.1.1.m1.1a"><mi id="S5.T7.1.1.1.1.1.1.m1.1.1" mathvariant="normal" xref="S5.T7.1.1.1.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S5.T7.1.1.1.1.1.1.m1.1b"><ci id="S5.T7.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T7.1.1.1.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.1.1.1.1.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S5.T7.1.1.1.1.1.1.m1.1d">roman_Δ</annotation></semantics></math></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T7.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="S5.T7.1.1.5.1">
<tr class="ltx_tr" id="S5.T7.1.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T7.1.1.5.1.1.1">%</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T7.1.1.5.1.2.1">Increase</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T7.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T7.1.2.1.1">One or More</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T7.1.2.1.2">15.8</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T7.1.2.1.3">40.9</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T7.1.2.1.4">25.1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T7.1.2.1.5">159%</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T7.1.3.2.1">Zero(null)</th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T7.1.3.2.2">19.4</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T7.1.3.2.3">49.1</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T7.1.3.2.4">29.7</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T7.1.3.2.5">153%</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Implementation in Knowledge Base</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The Information Extracted by GPT-3 using the proposed approach is now applied in Knowledge Base Construction. For that, the following procedure should be followed:-</p>
</div>
<div class="ltx_para" id="S6.p2">
<ol class="ltx_enumerate" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i1.p1.1.1">Defining the domain and scope of the knowledge base</span>: To use prompt-based language models for Knowledge Base Construction, the knowledge base’s domain, and scope must first be defined. This will make it easier to decide what kinds of entities and relationships should be recorded in the knowledge base.</p>
</div>
<div class="ltx_para" id="S6.I1.i1.p2">
<p class="ltx_p" id="S6.I1.i1.p2.1">It is a crucial step in implementing GPT-3 for information extraction in a knowledge-based system like a knowledge base construction (KBC) system. This step involves specifying the subject matter that the knowledge base will cover and the specific types of information that the system will extract and store.</p>
</div>
<div class="ltx_para" id="S6.I1.i1.p3">
<p class="ltx_p" id="S6.I1.i1.p3.1">The domain of the knowledge base refers to the specific area of knowledge or expertise that the system will focus on. For example, the domain could be healthcare, finance, law, or any other field that requires specialized knowledge. Defining the domain is important because it will determine the type of information that the system will extract and store.</p>
</div>
<div class="ltx_para" id="S6.I1.i1.p4">
<p class="ltx_p" id="S6.I1.i1.p4.1">The scope of the knowledge base refers to the breadth and depth of the information that the system will cover. For example, the scope could be limited to a specific area within the domain, such as medical diagnosis or financial planning, or it could be broader, covering multiple areas within the domain. Defining the scope is important because it will determine the level of detail that the system will extract and store.</p>
</div>
<div class="ltx_para" id="S6.I1.i1.p5">
<p class="ltx_p" id="S6.I1.i1.p5.1">Once the domain and scope of the knowledge base have been defined, the system can be designed to extract and store relevant information using GPT-3. GPT-3 can be used to analyze text and extract key information, such as named entities, relationships between entities, and other relevant data. The extracted information can then be stored in a structured format within the knowledge base, making it easily accessible and usable for a wide range of applications.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i2.p1.1.1">Preparing prompts</span>: The next phase is preparing the prompts to produce text that may be used for information extraction.Preparing prompts is an important step in implementing GPT-3 for information extraction in a knowledge-based system like a knowledge base construction (KBC) system. Prompts are the inputs given to GPT-3 to generate outputs based on the specific task at hand. The prompts define the context and requirements for the information extraction task and guide the model to generate relevant responses.</p>
</div>
<div class="ltx_para" id="S6.I1.i2.p2">
<p class="ltx_p" id="S6.I1.i2.p2.1">To prepare prompts for GPT-3, you will first need to define the specific types of information that you want the system to extract. This could include entity types, relationships between entities, specific attributes or properties of entities, or any other relevant information. For example, if you are building a knowledge base on medical conditions, you may want the system to extract information such as symptoms, causes, treatments, and risk factors for each condition.</p>
</div>
<div class="ltx_para" id="S6.I1.i2.p3">
<p class="ltx_p" id="S6.I1.i2.p3.1">Once you have defined the information to be extracted, you can create prompts that provide context for GPT-3 to generate responses. Prompts can be simple or complex, depending on the specific requirements of the task. For example, a simple prompt might ask GPT-3 to identify the symptoms of a particular medical condition, while a more complex prompt might ask the system to identify the symptoms, causes, and treatments of a condition and their interrelationships.</p>
</div>
<div class="ltx_para" id="S6.I1.i2.p4">
<p class="ltx_p" id="S6.I1.i2.p4.1">To ensure that GPT-3 generates accurate and relevant responses, it is important to fine-tune the prompts by testing them with sample inputs and refining them based on the model’s outputs. This process of trial and error can help improve the accuracy and relevance of the information extracted by GPT-3.</p>
</div>
<div class="ltx_para" id="S6.I1.i2.p5">
<p class="ltx_p" id="S6.I1.i2.p5.1">Overall, preparing prompts is a critical step in implementing GPT-3 for information extraction in a KBC system, as it defines the context and requirements for the information extraction task and guides the model to generate relevant and accurate responses.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p" id="S6.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i3.p1.1.1">Collecting unstructured data</span>:Collecting unstructured data is a crucial step in implementing GPT-3 for information extraction in a knowledge-based system like a knowledge base construction (KBC) system. Unstructured data refers to data that is not organized in a structured format, such as text data in emails, social media posts, news articles, and other sources. Collecting unstructured data involves identifying relevant sources of information and extracting the text data from those sources.</p>
</div>
<div class="ltx_para" id="S6.I1.i3.p2">
<p class="ltx_p" id="S6.I1.i3.p2.1">There are several ways to collect unstructured data, including web scraping, APIs, and manual collection. Web scraping involves automatically extracting data from web pages using tools like web crawlers or scraping software. APIs (Application Programming Interfaces) allow developers to access data from online services such as Twitter or Facebook programmatically. The manual collection involves manually copying and pasting data from sources like news articles or emails.</p>
</div>
<div class="ltx_para" id="S6.I1.i3.p3">
<p class="ltx_p" id="S6.I1.i3.p3.1">Once the unstructured data has been collected, GPT-3 can be used to analyze the text and extract relevant information. GPT-3 is a language model that has been trained on a large corpus of text data and can generate high-quality responses to a wide range of natural language processing tasks. It can be used to identify named entities, relationships between entities, and other relevant information from unstructured text data.</p>
</div>
<div class="ltx_para" id="S6.I1.i3.p4">
<p class="ltx_p" id="S6.I1.i3.p4.1">After the relevant information has been extracted from the unstructured data, it can be stored in a structured format within the knowledge base. This structured data can then be used to answer questions, generate insights, and provide recommendations based on the knowledge stored within the system.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S6.I1.i4.p1">
<p class="ltx_p" id="S6.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i4.p1.1.1">Extracting structured data</span>: In implementing GPT-3 for information extraction in a knowledge-based system like a knowledge base construction (KBC) system, extracting structured data involves converting unstructured text data into a structured format that can be easily stored and analyzed by the system.</p>
</div>
<div class="ltx_para" id="S6.I1.i4.p2">
<p class="ltx_p" id="S6.I1.i4.p2.1">GPT-3 can be used to analyze text and extract key information, such as named entities, relationships between entities, and other relevant data. Once the information has been extracted, it can be structured into a standardized format, such as a table or a graph, that can be easily stored and analyzed.</p>
</div>
<div class="ltx_para" id="S6.I1.i4.p3">
<p class="ltx_p" id="S6.I1.i4.p3.1">There are several techniques that can be used to extract structured data from unstructured text using GPT-3. These include:</p>
</div>
<div class="ltx_para" id="S6.I1.i4.p4">
<ul class="ltx_itemize" id="S6.I1.i4.I1">
<li class="ltx_item" id="S6.I1.i4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i4.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i4.I1.i1.p1.1">Named entity recognition (NER): This technique involves identifying and extracting entities such as people, places, organizations, and other types of named entities from unstructured text.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i4.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i4.I1.i2.p1.1">Relationship extraction: This technique involves identifying and extracting relationships between entities in unstructured text. For example, if the text mentions a person and a company, the system can extract the relationship between the person and the company.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i4.I1.i3.p1">
<p class="ltx_p" id="S6.I1.i4.I1.i3.p1.1">Semantic parsing: This technique involves analyzing the syntax and semantics of a sentence or paragraph to extract structured information. For example, if the text mentions a price, a date, and a product name, the system can extract the structured data into a table format.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S6.I1.i4.p5">
<p class="ltx_p" id="S6.I1.i4.p5.1">Once the structured data has been extracted, it can be stored in a database or knowledge graph and used for a wide range of applications, such as natural language understanding, question answering, and information retrieval.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S6.I1.i5.p1">
<p class="ltx_p" id="S6.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i5.p1.1.1">Verifying and updating data</span>: To assure correctness and completeness, the retrieved data should be reviewed and updated as needed. Manual or automatic verification methods could be used.</p>
</div>
<div class="ltx_para" id="S6.I1.i5.p2">
<p class="ltx_p" id="S6.I1.i5.p2.1">Verifying and updating data is an important part of information extraction in a knowledge-based system that uses GPT-3. After the system has extracted information using GPT-3, it needs to verify the accuracy of the extracted information and update it if necessary.</p>
</div>
<div class="ltx_para" id="S6.I1.i5.p3">
<p class="ltx_p" id="S6.I1.i5.p3.1">Verifying data involves comparing the extracted information with existing data in the knowledge base to ensure that it is accurate and consistent. This can be done using various techniques, such as comparing named entities, checking relationships between entities, and cross-referencing with external sources. If the extracted information is found to be accurate, it can be stored in the knowledge base. If there are discrepancies or inconsistencies, the system can flag the information for further review or update it accordingly.</p>
</div>
<div class="ltx_para" id="S6.I1.i5.p4">
<p class="ltx_p" id="S6.I1.i5.p4.1">Updating data involves modifying existing information in the knowledge base based on new information that has been extracted using GPT-3. This can happen when new information becomes available or when existing information needs to be corrected or updated. The system can use GPT-3 to extract new information and compare it with existing data in the knowledge base. If the new information is found to be accurate, the system can update the relevant entries in the knowledge base accordingly.</p>
</div>
<div class="ltx_para" id="S6.I1.i5.p5">
<p class="ltx_p" id="S6.I1.i5.p5.1">Overall, verifying and updating data is an iterative process that involves continuously extracting new information and comparing it with existing data in the knowledge base to ensure accuracy and consistency. This helps to ensure that the knowledge base remains up-to-date and reliable, which is essential for its usefulness in knowledge-based systems like KBC.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S6.I1.i6.p1">
<p class="ltx_p" id="S6.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i6.p1.1.1">Use of the knowledge base</span>: Once GPT-3 has been implemented for information extraction in a knowledge-based system like a knowledge base construction (KBC) system, the KBC system can be used for a wide range of applications, such as:</p>
</div>
<div class="ltx_para" id="S6.I1.i6.p2">
<ul class="ltx_itemize" id="S6.I1.i6.I1">
<li class="ltx_item" id="S6.I1.i6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i6.I1.i1.p1.1">Answering questions: The KBC system can be used to answer questions related to the domain and scope of the knowledge base. Users can input a question and the system will search through the knowledge base to find relevant information and provide an answer.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i6.I1.i2.p1.1">Decision-making: The KBC system can be used to support decision-making by providing relevant information and insights. For example, in healthcare, the system can provide recommendations for diagnosis and treatment based on the patient’s symptoms and medical history.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i6.I1.i3.p1">
<p class="ltx_p" id="S6.I1.i6.I1.i3.p1.1">Research and analysis: The KBC system can be used to analyze data and provide insights that can be used for research and analysis. For example, in finance, the system can analyze financial data and provide insights into market trends and investment opportunities.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i6.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i6.I1.i4.p1">
<p class="ltx_p" id="S6.I1.i6.I1.i4.p1.1">Training and education: The KBC system can be used for training and education by providing access to a wealth of knowledge and information in the domain. For example, in law, the system can provide access to case law and legal precedents, which can be used for training and education purposes.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S6.I1.i6.p3">
<p class="ltx_p" id="S6.I1.i6.p3.1">Overall, the KBC system can support a wide range of applications and provide valuable insights and information in the domain. By leveraging GPT-3 for information extraction, the KBC system can provide accurate and relevant information, making it a valuable tool for a wide range of users, including professionals, researchers, and students.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this study, we looked into how GPT-3 in-context learning may be used for the crucial job of information extraction (IE). Considering that such a paradigm would offer important benefits for biomedical IE applications, we invested a lot of time investigating the methods that have been successfully used in other in-context learning contexts. We demonstrated, however, that using a variety of benchmark datasets for biomedical NER and RE, existing methods do not allow GPT-3 in-context learning to outperform BERT-sized PLM fine-tuning. Also, we spoke about a few potential general restrictions on in-context learning in biomedical IE that will be investigated in subsequent research, including its difficulties in managing the null class, such as entity-less NER examples and vacuous relation examples for RE. In addition to presenting this topic for additional research, we expect that our work will be able to point biomedical researchers and practitioners in the direction of more effective and affordable methods for low-resource IE, including tiny PLM fine-tuning or possibly even directly fine-tuning GPT-3.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">One of the primary benefits of using GPT-3 for extracting information for Knowledge Base Construction (KBC) is its ability to process natural language text in a more human-like way. GPT-3 can understand the context and meaning of text and can extract relevant information, such as entities, relationships, and attributes, from unstructured data sources, such as news articles, social media posts, and web pages. This allows for the construction of a more sophisticated and intelligent knowledge base that can be used to answer complex queries and perform advanced analysis. Additionally, GPT-3 can significantly improve the efficiency and accuracy of IE, reducing the need for manual curation and increasing the speed at which new information can be added to the knowledge base. Overall, the use of GPT-3 for IE in KBC can lead to a more intelligent and effective system that can provide more value to users and organizations.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, “Biobert: a pre-trained biomedical language representation model for biomedical text mining,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Bioinformatics</em>, vol. 36, no. 4, pp. 1234–1240, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">et al.</em>, “Language models are few-shot learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2">Advances in neural information processing systems</em>, vol. 33, pp. 1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">et al.</em>, “Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.2.2">arXiv preprint arXiv:2201.11990</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">J. Mach. Learn. Res</em>, vol. 23, pp. 1–40, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:1810.04805</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao, and H. Poon, “Domain-specific language model pretraining for biomedical natural language processing,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">ACM Transactions on Computing for Healthcare (HEALTH)</em>, vol. 3, no. 1, pp. 1–23, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
E. Perez, D. Kiela, and K. Cho, “True few-shot learning with language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in neural information processing systems</em>, vol. 34, pp. 11 054–11 070, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
E. Agirre, M. Apidianaki, and I. Vulić, “Proceedings of deep learning inside out (deelio 2022): The 3rd workshop on knowledge extraction and integration for deep learning architectures,” in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert pretraining approach,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:1907.11692</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
R. Shin, C. H. Lin, S. Thomson, C. Chen, S. Roy, E. A. Platanios, A. Pauls, D. Klein, J. Eisner, and B. Van Durme, “Constrained language models yield few-shot semantic parsers,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2104.08768</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh, “Calibrate before use: Improving few-shot performance of language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">International Conference on Machine Learning</em>.   PMLR, 2021, pp. 12 697–12 706.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
N. Malkin, Z. Wang, and N. Jojic, “Coherence boosting: When your pretrained language model is not paying enough attention,” in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 2022, pp. 8214–8236.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">et al.</em>, “Transformers: State-of-the-art natural language processing,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.2.2">Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations</em>, 2020, pp. 38–45.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
E. V. Epure and R. Hennequin, “Probing pre-trained auto-regressive language models for named entity typing and recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2108.11857</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
T. Schick and H. Schütze, “It’s not just size that matters: Small language models are also few-shot learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2009.07118</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
P.-L. H. Cabot and R. Navigli, “Rebel: Relation extraction by end-to-end language generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Findings of the Association for Computational Linguistics: EMNLP 2021</em>, 2021, pp. 2370–2381.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a unified text-to-text transformer,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">The Journal of Machine Learning Research</em>, vol. 21, no. 1, pp. 5485–5551, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S. Raval, H. Sedghamiz, E. Santus, T. Alhanai, M. Ghassemi, and E. Chersoni, “Exploring a unified sequence-to-sequence transformer for medical product safety monitoring in social media,” <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2109.05815</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
L. N. Phan, J. T. Anibal, H. Tran, S. Chanana, E. Bahadroglu, A. Peltekian, and G. Altan-Bonnet, “Scifive: a text-to-text transformer model for biomedical literature,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2106.03598</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M. Parmar, S. Mishra, M. Purohit, M. Luo, M. H. Murad, and C. Baral, “In-boxbart: Get instructions into biomedical multi-task learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2204.07600</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
R. L. Logan IV, I. Balažević, E. Wallace, F. Petroni, S. Singh, and S. Riedel, “Cutting down on prompts and parameters: Simple few-shot learning with language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2106.13353</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
T. Schick and H. Schütze, “True few-shot learning with prompts—a real-world perspective,” <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Transactions of the Association for Computational Linguistics</em>, vol. 10, pp. 716–731, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M. Moradi, K. Blagec, F. Haberl, and M. Samwald, “Gpt-3 models are poor few-shot learners in the biomedical domain,” <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2109.02555</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
M. Agrawal, S. Hegselmann, H. Lang, Y. Kim, and D. Sontag, “Large language models are zero-shot clinical information extractors,” <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2205.12689</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
B. J. Gutiérrez, N. McNeal, C. Washington, Y. Chen, L. Li, H. Sun, and Y. Su, “Thinking about gpt-3 in-context learning for biomedical ie? think again,” <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2203.08410</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
N. Reimers and I. Gurevych, “Sentence-bert: Sentence embeddings using siamese bert-networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:1908.10084</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
S. Robertson, H. Zaragoza <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">et al.</em>, “The probabilistic relevance framework: Bm25 and beyond,” <em class="ltx_emph ltx_font_italic" id="bib.bib27.2.2">Foundations and Trends® in Information Retrieval</em>, vol. 3, no. 4, pp. 333–389, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
R. I. Doğan, R. Leaman, and Z. Lu, “Ncbi disease corpus: a resource for disease name recognition and concept normalization,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Journal of biomedical informatics</em>, vol. 47, pp. 1–10, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
L. Smith, L. K. Tanabe, C.-J. Kuo, I. Chung, C.-N. Hsu, Y.-S. Lin, R. Klinger, C. M. Friedrich, K. Ganchev, M. Torii <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">et al.</em>, “Overview of biocreative ii gene mention recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib29.2.2">Genome biology</em>, vol. 9, no. 2, pp. 1–19, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
N. Collier and J.-D. Kim, “Introduction to the bio-entity recognition task at jnlpba,” in <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP)</em>, 2004, pp. 73–78.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
J. Li, Y. Sun, R. J. Johnson, D. Sciaky, C.-H. Wei, R. Leaman, A. P. Davis, C. J. Mattingly, T. C. Wiegers, and Z. Lu, “Biocreative v cdr task corpus: a resource for chemical disease relation extraction,” <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Database</em>, vol. 2016, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
M. Krallinger, O. Rabal, S. A. Akhondi, M. P. Pérez, J. Santamaría, G. P. Rodríguez, G. Tsatsaronis, A. Intxaurrondo, J. A. López, U. Nandal <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">et al.</em>, “Overview of the biocreative vi chemical-protein interaction track,” in <em class="ltx_emph ltx_font_italic" id="bib.bib32.2.2">Proceedings of the sixth BioCreative challenge evaluation workshop</em>, vol. 1, no. 2017, 2017, pp. 141–146.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
M. Herrero-Zazo, I. Segura-Bedmar, P. Martínez, and T. Declerck, “The ddi corpus: An annotated corpus with pharmacological substances and drug–drug interactions,” <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Journal of biomedical informatics</em>, vol. 46, no. 5, pp. 914–920, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
À. Bravo, J. Piñero, N. Queralt-Rosinach, M. Rautschka, and L. I. Furlong, “Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research,” <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">BMC bioinformatics</em>, vol. 16, pp. 1–17, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">et al.</em>, “Huggingface’s transformers: State-of-the-art natural language processing,” <em class="ltx_emph ltx_font_italic" id="bib.bib35.2.2">arXiv preprint arXiv:1910.03771</em>, 2019.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jul 31 14:46:34 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
