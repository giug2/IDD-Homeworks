<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2104.14336] Document Collection Visual Question Answering</title><meta property="og:description" content="Current tasks and methods in Document Understanding aims to process documents as single elements. However, documents are usually organized in collections (historical records, purchase invoices), that provide context us‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Document Collection Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Document Collection Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2104.14336">

<!--Generated on Sun Mar 17 06:09:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Document collection Visual Question Answering">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Computer Vision Center, UAB, Spain 
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{rperez, dimos, ernest}@cvc.uab.es</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Document Collection Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rub√®n Tito 
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dimosthenis Karatzas
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ernest Valveny
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Current tasks and methods in Document Understanding aims to process documents as single elements. However, documents are usually organized in collections (historical records, purchase invoices), that provide context useful for their interpretation.
To address this problem, we introduce Document Collection Visual Question Answering (DocCVQA) a new dataset and related task, where questions are posed over a whole collection of document images and the goal is not only to provide the answer to the given question, but also to retrieve the set of documents that contain the information needed to infer the answer. Along with the dataset we propose a new evaluation metric and baselines which provide further insights to the new dataset and task.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Document collection Visual Question Answering
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Documents are essential for humans since they have been used to store knowledge and information over the history. For this reason there has been a strong research effort on improving the machine understanding of documents. The research field of Document Analysis and Recognition (DAR) aims at the automatic extraction of information presented on paper, initially addressed to human comprehension. Some of the most widely known applications of DAR involve processing office documents by recognizing text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, tables and forms layout <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, mathematical expressions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and visual information like figures and graphics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. However, even though all these research fields have progressed immensely during the last decades, they have been agnostic to the end purpose they can be used for. Moreover, despite the fact that document collections are as ancient as documents themselves, the research in this scope has been limited to document retrieval by lexical content in word spotting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, blind to the semantics and ignoring the task of extracting higher level information from those collections.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">On the other hand, over the past few years Visual Question Answering (VQA) has been one of the major relevant tasks as a link between vision and language. Even though the works of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> start considering text in VQA by requiring the methods to read the text in the images to answer the questions, they constrained the problem to natural scenes. It was <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> who first introduced VQA on documents. However, none of those previous works consider the image collection perspective, neither from real scenes nor documents.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this regard, we present Document Collection Visual Question Answering (DocCVQA) as a step towards better understanding document collections and going beyond word spotting. The objective of DocCVQA is to extract information from a document image collection by asking questions and expecting the methods to provide the answers. Nevertheless, to ensure that those answers have been inferred using the documents that contain the necessary information, the methods must also provide the IDs of the documents used to obtain the answer in the form of a confidence list as answer evidence.
Hence, we design this task as a retrieval-answering task, for which the methods should be trained initially on other datasets and consequently, we pose only a set of 20 questions over this document collection. In addition, most of the answers in this task are actually a set of words extracted from different documents for which the order is not relevant, as we can observe in the question example in Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Document Collection Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Therefore, we define a new evaluation metric based on the Average Normalized Levenshtein Similarity (ANLS) to evaluate the answering performance of the methods in this task. Finally, we propose two baseline methods from very different perspectives which provide some insights on this task and dataset.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The dataset, the baselines code and the performance evaluation scripts with an online evaluation service are available in <a target="_blank" href="https://docvqa.org" title="" class="ltx_ref ltx_href">https://docvqa.org</a>.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<table id="S1.F1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.F1.2.2" class="ltx_tr">
<td id="S1.F1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S1.F1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.F1.1.1.1.1.1" class="ltx_p" style="width:212.5pt;"><img src="/html/2104.14336/assets/images/doc_0454_ultra_zoomed.jpg" id="S1.F1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="479" alt="Refer to caption"></span>
</span>
</td>
<td id="S1.F1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S1.F1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.F1.2.2.2.1.1" class="ltx_p" style="width:212.5pt;"><img src="/html/2104.14336/assets/images/doc_2231_ultra_zoomed.jpg" id="S1.F1.2.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="465" alt="Refer to caption"></span>
</span>
</td>
</tr>
<tr id="S1.F1.2.3.1" class="ltx_tr">
<td id="S1.F1.2.3.1.1" class="ltx_td ltx_align_justify ltx_align_top" colspan="2">Q: In which years did Anna M. Rivers run for the State senator office?</td>
</tr>
<tr id="S1.F1.2.4.2" class="ltx_tr">
<td id="S1.F1.2.4.2.1" class="ltx_td ltx_align_justify ltx_align_top" colspan="2">A: [2016, 2020]</td>
</tr>
<tr id="S1.F1.2.5.3" class="ltx_tr">
<td id="S1.F1.2.5.3.1" class="ltx_td ltx_align_justify ltx_align_top" colspan="2">E: [454, 10901]</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Top: Partial visualization of sample documents in DocCVQA. The left document corresponds to the document with ID 454, which is one of the relevant documents to answer the question below. Bottom: Example question from the sample set, its answer and their evidences. In DocCVQA the evidences are the documents where the answer can be inferred from. In this example, the correct answer are the years 2016 and 2020, and the evidences are the document images with ids 454 and 10901 which corresponds to the forms where Anna M. Rivers presented as a candidate for the State senator office.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Document understanding</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Document understanding has been largely investigated within the document analysis community with the final goal of automatically extracting relevant information from documents. Most works have focused on structured or semi-structured documents such as forms, invoices, receipts, passports or ID cards, e-mails, contracts, etc. Earlier works¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> were based on a predefined set of rules that required the definition of specific templates for each new type of document. Later on, learning-based methods¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> allowed to automatically classify the type of document and identify relevant fields of information without predefined templates. Recent advances on deep learning¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> leverage natural language processing, visual feature extraction and graph-based representations in order to have a more global view of the document that take into account word semantics and visual layout in the process of information extraction.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">All these methods mainly focus on extracting key-value pairs, following a bottom-up approach, from the document features to the relevant semantic information. The task proposed in this work takes a different top-down approach, using the visual question answering paradigm, where the goal drives the search of information in the document.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Document retrieval</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Providing tools for searching relevant information in large collections of documents has been the focus of document retrieval. Most works have addressed this task from the perspective of word spotting¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, i.e., searching for specific query words in the document collection without relying on explicit noisy OCR. Current state-of-the-art on word spotting is based on similarity search in an common embedding space¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> where both the query string and word images can be projected using deep networks¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> In order to search for the whole collection, these representations are combined with standard deep learning architectures for object detection in order to find all instances of a given word in the document¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Word spotting only allows to search for the specific instances of a given word in the collection without taking into account the semantic context where that word appears. On the contrary, the task proposed in this work does not aim to find specific isolated words, but to make a semantic retrieval of documents based on the query question.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Visual Question Answering</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Visual Question Answering (VQA) is the task where given an image and a natural language question about that image, the objective is to provide an accurate natural language answer. It was initially introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> proposed the first large scale dataset for this task. All the images from those works are real scenes and the questions mainly refer to objects present in the images. Nonetheless, the field became very popular and several new datasets were released exploring new challenges like ST-VQA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and TextVQA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, which were the first datasets that considered the text in the scene. In the former dataset, the answers are always contained within the text found in the image while the latter requires to read the text, but the answer might not be a direct transcription of the recognized text. The incorporation of text in VQA posed two main challenges. First, the number of classes as possible answers grew exponentially and second, the methods had to deal with a lot of out of vocabulary (OOV) words both as answers or as input recognized text. To address the problem of OOV words, embeddings such as Fasttext¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and PHOC¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> became more popular, while in order to predict an answer, along with the standard fixed vocabulary with the most common answers a copy mechanism was introduced by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> which allowed to propose an OCR token as an answer. Later <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> changed the classification output to a decoder that outputs a word from the fixed vocabulary or from the recognized text at each timestep, and provided more flexibility in complex and longer answers.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Concerning documents, FigureQA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and DVQA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> focused on complex figures and data representation like different kinds of charts and plots by proposing synthetic datasets and corresponding questions and answers over those figures. More recently,¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> proposed DocVQA, the first VQA dataset over document images, where the questions also refer to figures, forms or tables but also text in complex layouts. Along with the dataset they proposed some baselines based on NLP and scene text VQA models. In this sense, we go a step further extending this work for document collections.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Finally, one of the most relevant works for this paper is ISVQA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> where the questions are asked over a small set of images which consist of different perspectives of the same scene. Notice that even though the set up might seem similar, the methods to tackle this dataset and the one we propose are very different. For ISVQA all the images share the same context, which implies that finding some information in one of the images can be useful for the other images in the set. In addition, the image sets are always small sets of <math id="S2.SS3.p3.1.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S2.SS3.p3.1.m1.1a"><mn id="S2.SS3.p3.1.m1.1.1" xref="S2.SS3.p3.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.1.m1.1b"><cn type="integer" id="S2.SS3.p3.1.m1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.1.m1.1c">6</annotation></semantics></math> images, in contrast to the whole collection of DocCVQA and finally, the images are about real scenes which don‚Äôt even consider the text. As an example, the baselines they propose are based on the HME-VideoQA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and standard VQA methods stitching all the images, or the images features. Which are not suitable to our problem.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>DocCVQA Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section we describe the process for collecting images, questions and answers, an analysis of the collected data and finally, we describe the metric used for the evaluation of this task.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Collection</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Images:</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">The DocCVQA dataset comprises <math id="S3.SS1.SSS1.p1.1.m1.2" class="ltx_Math" alttext="14,362" display="inline"><semantics id="S3.SS1.SSS1.p1.1.m1.2a"><mrow id="S3.SS1.SSS1.p1.1.m1.2.3.2" xref="S3.SS1.SSS1.p1.1.m1.2.3.1.cmml"><mn id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml">14</mn><mo id="S3.SS1.SSS1.p1.1.m1.2.3.2.1" xref="S3.SS1.SSS1.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS1.SSS1.p1.1.m1.2.2" xref="S3.SS1.SSS1.p1.1.m1.2.2.cmml">362</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.2b"><list id="S3.SS1.SSS1.p1.1.m1.2.3.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.2.3.2"><cn type="integer" id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1">14</cn><cn type="integer" id="S3.SS1.SSS1.p1.1.m1.2.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.2.2">362</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.2c">14,362</annotation></semantics></math> document images sourced from the Open Data portal of the Public Disclosure Commission (PDC), an agency that aims to provide public access to information about the financing of political campaigns, lobbyist expenditures, and the financial affairs of public officials and candidates. We got the documents from this source for various reasons. First, it‚Äôs a live repository that is updated periodically and therefore, the dataset can be increased in size in the future if it‚Äôs considered necessary or beneficial for the research. In addition, it contains a type of documents in terms of layout and content that makes sense and can be interesting to reason about an entire collection. Moreover, along with the documents, they provide their transcriptions in the form of CSV files which allows us to pose a set of questions and get their answers without the costly process of annotation. From the original collection of document images, we discarded all the multi-page documents and documents for which the transcriptions were partially missing or ambiguous. Thus, all documents that were finally included in the dataset were sourced from the same document template, the US Candidate Registration form, with slight design differences due to changes over the time. However, these documents still pose some challenges since the proposed methods will need to understand its complex layout, as well as handwritten and typewritten text at the same time. We provide some document examples in Figure¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Document Collection Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Questions and Answers:</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.6" class="ltx_p">Considering that DocCVQA dataset is set up as a retrieval-answering task and documents are relatively similar we pose only a set of <math id="S3.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><mn id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">20</annotation></semantics></math> natural language questions over this collection. To gather the questions and answers, we first analyzed which are the most important fields in the document form in terms of complexity (numbers, dates, candidate‚Äôs names, checkboxes and different form field layouts) and variability (see section <a href="#S3.SS2" title="3.2 Statistics and Analysis ‚Ä£ 3 DocCVQA Dataset ‚Ä£ Document Collection Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). We also defined different types of constraints for the types of questions since limiting the questions to find a specific value would place this in a standard word spotting scenario. Thus, we defined different constraints depending on the type of field related to the question: for dates, the questions will refer to the document before, after and between specific dates, or specific years. For other textual fields the questions refer to documents with specific values (candidates from party <math id="S3.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><mi id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><ci id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1">ùëÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">P</annotation></semantics></math>), to documents that do not contain specific values (candidates which do not represent the party <math id="S3.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.SS1.SSS2.p1.3.m3.1a"><mi id="S3.SS1.SSS2.p1.3.m3.1.1" xref="S3.SS1.SSS2.p1.3.m3.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.3.m3.1b"><ci id="S3.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1">ùëÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.3.m3.1c">P</annotation></semantics></math>), or that contains a value from a set of possibilities (candidates from parties <math id="S3.SS1.SSS2.p1.4.m4.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.SS1.SSS2.p1.4.m4.1a"><mi id="S3.SS1.SSS2.p1.4.m4.1.1" xref="S3.SS1.SSS2.p1.4.m4.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.4.m4.1b"><ci id="S3.SS1.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1">ùëÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.4.m4.1c">P</annotation></semantics></math>, <math id="S3.SS1.SSS2.p1.5.m5.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS1.SSS2.p1.5.m5.1a"><mi id="S3.SS1.SSS2.p1.5.m5.1.1" xref="S3.SS1.SSS2.p1.5.m5.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.5.m5.1b"><ci id="S3.SS1.SSS2.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS2.p1.5.m5.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.5.m5.1c">Q</annotation></semantics></math> or <math id="S3.SS1.SSS2.p1.6.m6.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.SS1.SSS2.p1.6.m6.1a"><mi id="S3.SS1.SSS2.p1.6.m6.1.1" xref="S3.SS1.SSS2.p1.6.m6.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.6.m6.1b"><ci id="S3.SS1.SSS2.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS2.p1.6.m6.1.1">ùëÖ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.6.m6.1c">R</annotation></semantics></math>). For checkboxes we defined constraints regarding if a value is checked or not. Finally, according to the fields and constraints defined we posed the questions in natural language referring to the whole collection, and asking for particular values instead of the document itself. We provide the full list of questions in the test set in table¬†<a href="#S3.T1" title="Table 1 ‚Ä£ 3.1.2 Questions and Answers: ‚Ä£ 3.1 Data Collection ‚Ä£ 3 DocCVQA Dataset ‚Ä£ Document Collection Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" style="padding:2.5pt 8.0pt;">ID</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" style="padding:2.5pt 8.0pt;">
<span id="S3.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.2.1.1" class="ltx_p" style="width:298.8pt;">Question</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:2.5pt 8.0pt;">8</th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding:2.5pt 8.0pt;">
<span id="S3.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.2.1.1" class="ltx_p" style="width:298.8pt;">Which candidates in 2008 were from the Republican party?</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:2.5pt 8.0pt;">9</th>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding:2.5pt 8.0pt;">
<span id="S3.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.2.1.1" class="ltx_p" style="width:298.8pt;">Which candidates ran for the State Representative office between 06/01/2012 and 12/31/2012?</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:2.5pt 8.0pt;">10</th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding:2.5pt 8.0pt;">
<span id="S3.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.3.2.1.1" class="ltx_p" style="width:298.8pt;">In which legislative counties did Gary L. Schoessler run for County Commissioner?</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:2.5pt 8.0pt;">11</th>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding:2.5pt 8.0pt;">
<span id="S3.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.4.2.1.1" class="ltx_p" style="width:298.8pt;">For which candidates was Danielle Westbrook the treasurer?</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:2.5pt 8.0pt;">12</th>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding:2.5pt 8.0pt;">
<span id="S3.T1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.5.2.1.1" class="ltx_p" style="width:298.8pt;">Which candidates ran for election in North Bonneville who were from neither the Republican nor Democrat parties?</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<th id="S3.T1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:2.5pt 8.0pt;">13</th>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding:2.5pt 8.0pt;">
<span id="S3.T1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.6.2.1.1" class="ltx_p" style="width:298.8pt;">Did Valerie I. Quill select the full reporting option when she ran for the 11/03/2015 elections?</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.8.7" class="ltx_tr">
<th id="S3.T1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:2.5pt 8.0pt;">14</th>
<td id="S3.T1.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding:2.5pt 8.0pt;">
<span id="S3.T1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.7.2.1.1" class="ltx_p" style="width:298.8pt;">Which candidates from the Libertarian, Independent, or Green parties ran for election in Seattle?</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.9.8" class="ltx_tr">
<th id="S3.T1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:2.5pt 8.0pt;">15</th>
<td id="S3.T1.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding:2.5pt 8.0pt;">
<span id="S3.T1.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.8.2.1.1" class="ltx_p" style="width:298.8pt;">Did Suzanne G. Skaar ever run for City Council member?</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.10.9" class="ltx_tr">
<th id="S3.T1.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:2.5pt 8.0pt;">16</th>
<td id="S3.T1.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding:2.5pt 8.0pt;">
<span id="S3.T1.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.10.9.2.1.1" class="ltx_p" style="width:298.8pt;">In which election year did Stanley J Rumbaugh run for Superior Court Judge?</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.11.10" class="ltx_tr">
<th id="S3.T1.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:2.5pt 8.0pt;">17</th>
<td id="S3.T1.1.11.10.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding:2.5pt 8.0pt;">
<span id="S3.T1.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.11.10.2.1.1" class="ltx_p" style="width:298.8pt;">In which years did Dean A. Takko run for the State Representative office?</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.12.11" class="ltx_tr">
<th id="S3.T1.1.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:2.5pt 8.0pt;">18</th>
<td id="S3.T1.1.12.11.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding:2.5pt 8.0pt;">
<span id="S3.T1.1.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.12.11.2.1.1" class="ltx_p" style="width:298.8pt;">Which candidates running after 06/15/2017 were from the Libertarian party?</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.13.12" class="ltx_tr">
<th id="S3.T1.1.13.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding:2.5pt 8.0pt;">19</th>
<td id="S3.T1.1.13.12.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding:2.5pt 8.0pt;">
<span id="S3.T1.1.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.13.12.2.1.1" class="ltx_p" style="width:298.8pt;">Which reporting option did Douglas J. Fair select when he ran for district court judge in Edmonds? Mini or full?</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Questions in test set.</figcaption>
</figure>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.2" class="ltx_p">Once we had the question, we got the answer from the annotations downloaded along with the images. Then, we manually checked that those answers were correct and unambiguous, since some original annotations were wrong. Finally, we divided the questions into two different splits; the sample set with <math id="S3.SS1.SSS2.p2.1.m1.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS1.SSS2.p2.1.m1.1a"><mn id="S3.SS1.SSS2.p2.1.m1.1.1" xref="S3.SS1.SSS2.p2.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.1.m1.1b"><cn type="integer" id="S3.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.1.m1.1c">8</annotation></semantics></math> questions and the test set with the remaining <math id="S3.SS1.SSS2.p2.2.m2.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S3.SS1.SSS2.p2.2.m2.1a"><mn id="S3.SS1.SSS2.p2.2.m2.1.1" xref="S3.SS1.SSS2.p2.2.m2.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.2.m2.1b"><cn type="integer" id="S3.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p2.2.m2.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.2.m2.1c">12</annotation></semantics></math>. Given the low variability of the documents layout, we ensured that in the test set there were questions which refer to document form fields or that had some constraints that were not seen in the sample set. In addition, as depicted in figure¬†<a href="#S3.F2" title="Figure 2 ‚Ä£ 3.1.2 Questions and Answers: ‚Ä£ 3.1 Data Collection ‚Ä£ 3 DocCVQA Dataset ‚Ä£ Document Collection Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> the number of relevant documents is quite variable among the questions, which poses another challenge that methods will have to deal with.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2104.14336/assets/images/plots/Num_rel_documents.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="240" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Number of relevant documents in ground truth for each question in the sample set (blue) and the test set (red).</figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Statistics and Analysis</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We provide in table¬†<a href="#S3.T2" title="Table 2 ‚Ä£ 3.3 Evaluation metrics ‚Ä£ 3 DocCVQA Dataset ‚Ä£ Document Collection Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> a brief description of the document forms fields used to perform the questions and expected answers with a brief analysis of their variability showing the number of values and unique values in their annotations.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluation metrics</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The ultimate goal of this task is the extraction of information from a collection of documents. However, as previously demonstrated, and especially in unbalanced datasets, models can learn that specific answers are more common to specific questions. One of the clearest cases is the answer <em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">Yes</em>, to questions that are answered with <em id="S3.SS3.p1.1.2" class="ltx_emph ltx_font_italic">Yes</em> or <em id="S3.SS3.p1.1.3" class="ltx_emph ltx_font_italic">No</em>. To prevent this, we not only evaluate the answer to the question, but also if the answer has been reasoned from the document that contains the information to answer the question, which we consider as evidence. Therefore, we have two different evaluations, one for the evidence which is based on retrieval performance, and the other for the answer, based on text VQA performance.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Field</span></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Type</span></th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S3.T2.1.1.1.3.1" class="ltx_text ltx_font_bold"># Values</span></th>
<th id="S3.T2.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S3.T2.1.1.1.4.1" class="ltx_text ltx_font_bold"># Unique values</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<th id="S3.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Candidate name</th>
<th id="S3.T2.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Text</th>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_right" style="padding-left:8.0pt;padding-right:8.0pt;">14362</td>
<td id="S3.T2.1.2.1.4" class="ltx_td ltx_align_right" style="padding-left:8.0pt;padding-right:8.0pt;">9309</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<th id="S3.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Party</th>
<th id="S3.T2.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Text</th>
<td id="S3.T2.1.3.2.3" class="ltx_td ltx_align_right" style="padding-left:8.0pt;padding-right:8.0pt;">14161</td>
<td id="S3.T2.1.3.2.4" class="ltx_td ltx_align_right" style="padding-left:8.0pt;padding-right:8.0pt;">10</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<th id="S3.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Office</th>
<th id="S3.T2.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Text</th>
<td id="S3.T2.1.4.3.3" class="ltx_td ltx_align_right" style="padding-left:8.0pt;padding-right:8.0pt;">14362</td>
<td id="S3.T2.1.4.3.4" class="ltx_td ltx_align_right" style="padding-left:8.0pt;padding-right:8.0pt;">43</td>
</tr>
<tr id="S3.T2.1.5.4" class="ltx_tr">
<th id="S3.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Candidate city</th>
<th id="S3.T2.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Text</th>
<td id="S3.T2.1.5.4.3" class="ltx_td ltx_align_right" style="padding-left:8.0pt;padding-right:8.0pt;">14361</td>
<td id="S3.T2.1.5.4.4" class="ltx_td ltx_align_right" style="padding-left:8.0pt;padding-right:8.0pt;">476</td>
</tr>
<tr id="S3.T2.1.6.5" class="ltx_tr">
<th id="S3.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Candidate county</th>
<th id="S3.T2.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Text</th>
<td id="S3.T2.1.6.5.3" class="ltx_td ltx_align_right" style="padding-left:8.0pt;padding-right:8.0pt;">14343</td>
<td id="S3.T2.1.6.5.4" class="ltx_td ltx_align_right" style="padding-left:8.0pt;padding-right:8.0pt;">39</td>
</tr>
<tr id="S3.T2.1.7.6" class="ltx_tr">
<th id="S3.T2.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Election date</th>
<th id="S3.T2.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Date</th>
<td id="S3.T2.1.7.6.3" class="ltx_td ltx_align_right" style="padding-left:8.0pt;padding-right:8.0pt;">14362</td>
<td id="S3.T2.1.7.6.4" class="ltx_td ltx_align_right" style="padding-left:8.0pt;padding-right:8.0pt;">27</td>
</tr>
<tr id="S3.T2.1.8.7" class="ltx_tr">
<th id="S3.T2.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Reporting option</th>
<th id="S3.T2.1.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Checkbox</th>
<td id="S3.T2.1.8.7.3" class="ltx_td ltx_align_right" style="padding-left:8.0pt;padding-right:8.0pt;">14357</td>
<td id="S3.T2.1.8.7.4" class="ltx_td ltx_align_right" style="padding-left:8.0pt;padding-right:8.0pt;">2</td>
</tr>
<tr id="S3.T2.1.9.8" class="ltx_tr">
<th id="S3.T2.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Treasurer name</th>
<th id="S3.T2.1.9.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Text</th>
<td id="S3.T2.1.9.8.3" class="ltx_td ltx_align_right" style="padding-left:8.0pt;padding-right:8.0pt;">14362</td>
<td id="S3.T2.1.9.8.4" class="ltx_td ltx_align_right" style="padding-left:8.0pt;padding-right:8.0pt;">10197</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Description of the document forms fields with a brief analysis of their variability showing the number of values and unique values in their annotations.</figcaption>
</figure>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Evidences:</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">Following standard retrieval tasks¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> we use the Mean Average Precision (MAP) to assess the correctness of the positive evidences provided by the methods. We consider as positive evidences the documents in which the answer to the question can be found.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Answers:</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.11" class="ltx_p">Following other text based VQA tasks¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> we use the Average Normalized Levenshtein Similarity (ANLS) which captures the model‚Äôs reasoning capability while smoothly penalizing OCR recognition errors. However, in our case the answers are a set of items for which the order is not relevant, in contrast to common VQA tasks where the answer is a string. Thus, we need to adapt this metric to make it suitable to our problem. We name this adaptation as Average Normalized Levenshtein Similarity for Lists (ANLSL), formally described in equation <a href="#S3.E1" title="In 3.3.2 Answers: ‚Ä£ 3.3 Evaluation metrics ‚Ä£ 3 DocCVQA Dataset ‚Ä£ Document Collection Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Given a question <math id="S3.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS3.SSS2.p1.1.m1.1a"><mi id="S3.SS3.SSS2.p1.1.m1.1.1" xref="S3.SS3.SSS2.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.1.m1.1b"><ci id="S3.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.1.m1.1c">Q</annotation></semantics></math>, the ground truth list of answers <math id="S3.SS3.SSS2.p1.2.m2.2" class="ltx_Math" alttext="G=\{g_{1},g_{2}\dots g_{M}\}" display="inline"><semantics id="S3.SS3.SSS2.p1.2.m2.2a"><mrow id="S3.SS3.SSS2.p1.2.m2.2.2" xref="S3.SS3.SSS2.p1.2.m2.2.2.cmml"><mi id="S3.SS3.SSS2.p1.2.m2.2.2.4" xref="S3.SS3.SSS2.p1.2.m2.2.2.4.cmml">G</mi><mo id="S3.SS3.SSS2.p1.2.m2.2.2.3" xref="S3.SS3.SSS2.p1.2.m2.2.2.3.cmml">=</mo><mrow id="S3.SS3.SSS2.p1.2.m2.2.2.2.2" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.3" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.3.cmml">{</mo><msub id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.2" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.2.cmml">g</mi><mn id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.3" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.4" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.3.cmml">,</mo><mrow id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.cmml"><msub id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.2" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.2.cmml"><mi id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.2.2" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.2.2.cmml">g</mi><mn id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.2.3" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.1" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.3" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.3.cmml">‚Ä¶</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.1a" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.1.cmml">‚Äã</mo><msub id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.4" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.4.cmml"><mi id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.4.2" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.4.2.cmml">g</mi><mi id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.4.3" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.4.3.cmml">M</mi></msub></mrow><mo stretchy="false" id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.5" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.2.m2.2b"><apply id="S3.SS3.SSS2.p1.2.m2.2.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2"><eq id="S3.SS3.SSS2.p1.2.m2.2.2.3.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.3"></eq><ci id="S3.SS3.SSS2.p1.2.m2.2.2.4.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.4">ùê∫</ci><set id="S3.SS3.SSS2.p1.2.m2.2.2.2.3.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2"><apply id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.2">ùëî</ci><cn type="integer" id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2"><times id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.1"></times><apply id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.2.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.2.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.2.2">ùëî</ci><cn type="integer" id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.2.3.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.2.3">2</cn></apply><ci id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.3">‚Ä¶</ci><apply id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.4.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.4"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.4.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.4">subscript</csymbol><ci id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.4.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.4.2">ùëî</ci><ci id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.4.3.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.4.3">ùëÄ</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.2.m2.2c">G=\{g_{1},g_{2}\dots g_{M}\}</annotation></semantics></math> and a model‚Äôs list predicted answers <math id="S3.SS3.SSS2.p1.3.m3.2" class="ltx_Math" alttext="P=\{p_{1},p_{2}\dots p_{N}\}" display="inline"><semantics id="S3.SS3.SSS2.p1.3.m3.2a"><mrow id="S3.SS3.SSS2.p1.3.m3.2.2" xref="S3.SS3.SSS2.p1.3.m3.2.2.cmml"><mi id="S3.SS3.SSS2.p1.3.m3.2.2.4" xref="S3.SS3.SSS2.p1.3.m3.2.2.4.cmml">P</mi><mo id="S3.SS3.SSS2.p1.3.m3.2.2.3" xref="S3.SS3.SSS2.p1.3.m3.2.2.3.cmml">=</mo><mrow id="S3.SS3.SSS2.p1.3.m3.2.2.2.2" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.3" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.3.cmml">{</mo><msub id="S3.SS3.SSS2.p1.3.m3.1.1.1.1.1" xref="S3.SS3.SSS2.p1.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS2.p1.3.m3.1.1.1.1.1.2" xref="S3.SS3.SSS2.p1.3.m3.1.1.1.1.1.2.cmml">p</mi><mn id="S3.SS3.SSS2.p1.3.m3.1.1.1.1.1.3" xref="S3.SS3.SSS2.p1.3.m3.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.4" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.3.cmml">,</mo><mrow id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.cmml"><msub id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.2" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.2.cmml"><mi id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.2.2" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.2.2.cmml">p</mi><mn id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.2.3" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.1" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.3" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.3.cmml">‚Ä¶</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.1a" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.1.cmml">‚Äã</mo><msub id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.4" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.4.cmml"><mi id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.4.2" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.4.2.cmml">p</mi><mi id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.4.3" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.4.3.cmml">N</mi></msub></mrow><mo stretchy="false" id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.5" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.3.m3.2b"><apply id="S3.SS3.SSS2.p1.3.m3.2.2.cmml" xref="S3.SS3.SSS2.p1.3.m3.2.2"><eq id="S3.SS3.SSS2.p1.3.m3.2.2.3.cmml" xref="S3.SS3.SSS2.p1.3.m3.2.2.3"></eq><ci id="S3.SS3.SSS2.p1.3.m3.2.2.4.cmml" xref="S3.SS3.SSS2.p1.3.m3.2.2.4">ùëÉ</ci><set id="S3.SS3.SSS2.p1.3.m3.2.2.2.3.cmml" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2"><apply id="S3.SS3.SSS2.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1.1.1.1.2">ùëù</ci><cn type="integer" id="S3.SS3.SSS2.p1.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.cmml" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2"><times id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.1.cmml" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.1"></times><apply id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.2.cmml" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.2.1.cmml" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.2.2.cmml" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.2.2">ùëù</ci><cn type="integer" id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.2.3.cmml" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.2.3">2</cn></apply><ci id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.3.cmml" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.3">‚Ä¶</ci><apply id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.4.cmml" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.4"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.4.1.cmml" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.4">subscript</csymbol><ci id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.4.2.cmml" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.4.2">ùëù</ci><ci id="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.4.3.cmml" xref="S3.SS3.SSS2.p1.3.m3.2.2.2.2.2.4.3">ùëÅ</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.3.m3.2c">P=\{p_{1},p_{2}\dots p_{N}\}</annotation></semantics></math>, the ANLSL performs the Hungarian matching algorithm to obtain a <math id="S3.SS3.SSS2.p1.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.SSS2.p1.4.m4.1a"><mi id="S3.SS3.SSS2.p1.4.m4.1.1" xref="S3.SS3.SSS2.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.4.m4.1b"><ci id="S3.SS3.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS2.p1.4.m4.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.4.m4.1c">k</annotation></semantics></math> number of pairs <math id="S3.SS3.SSS2.p1.5.m5.2" class="ltx_Math" alttext="U=\{u_{1},u_{2}\dots u_{K}\}" display="inline"><semantics id="S3.SS3.SSS2.p1.5.m5.2a"><mrow id="S3.SS3.SSS2.p1.5.m5.2.2" xref="S3.SS3.SSS2.p1.5.m5.2.2.cmml"><mi id="S3.SS3.SSS2.p1.5.m5.2.2.4" xref="S3.SS3.SSS2.p1.5.m5.2.2.4.cmml">U</mi><mo id="S3.SS3.SSS2.p1.5.m5.2.2.3" xref="S3.SS3.SSS2.p1.5.m5.2.2.3.cmml">=</mo><mrow id="S3.SS3.SSS2.p1.5.m5.2.2.2.2" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.3" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.3.cmml">{</mo><msub id="S3.SS3.SSS2.p1.5.m5.1.1.1.1.1" xref="S3.SS3.SSS2.p1.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS2.p1.5.m5.1.1.1.1.1.2" xref="S3.SS3.SSS2.p1.5.m5.1.1.1.1.1.2.cmml">u</mi><mn id="S3.SS3.SSS2.p1.5.m5.1.1.1.1.1.3" xref="S3.SS3.SSS2.p1.5.m5.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.4" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.3.cmml">,</mo><mrow id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.cmml"><msub id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.2" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.2.cmml"><mi id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.2.2" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.2.2.cmml">u</mi><mn id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.2.3" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.1" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.3" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.3.cmml">‚Ä¶</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.1a" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.1.cmml">‚Äã</mo><msub id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.4" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.4.cmml"><mi id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.4.2" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.4.2.cmml">u</mi><mi id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.4.3" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.4.3.cmml">K</mi></msub></mrow><mo stretchy="false" id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.5" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.5.m5.2b"><apply id="S3.SS3.SSS2.p1.5.m5.2.2.cmml" xref="S3.SS3.SSS2.p1.5.m5.2.2"><eq id="S3.SS3.SSS2.p1.5.m5.2.2.3.cmml" xref="S3.SS3.SSS2.p1.5.m5.2.2.3"></eq><ci id="S3.SS3.SSS2.p1.5.m5.2.2.4.cmml" xref="S3.SS3.SSS2.p1.5.m5.2.2.4">ùëà</ci><set id="S3.SS3.SSS2.p1.5.m5.2.2.2.3.cmml" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2"><apply id="S3.SS3.SSS2.p1.5.m5.1.1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.5.m5.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS2.p1.5.m5.1.1.1.1.1.2">ùë¢</ci><cn type="integer" id="S3.SS3.SSS2.p1.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS2.p1.5.m5.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.cmml" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2"><times id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.1.cmml" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.1"></times><apply id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.2.cmml" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.2.1.cmml" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.2.2.cmml" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.2.2">ùë¢</ci><cn type="integer" id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.2.3.cmml" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.2.3">2</cn></apply><ci id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.3.cmml" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.3">‚Ä¶</ci><apply id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.4.cmml" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.4"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.4.1.cmml" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.4">subscript</csymbol><ci id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.4.2.cmml" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.4.2">ùë¢</ci><ci id="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.4.3.cmml" xref="S3.SS3.SSS2.p1.5.m5.2.2.2.2.2.4.3">ùêæ</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.5.m5.2c">U=\{u_{1},u_{2}\dots u_{K}\}</annotation></semantics></math> where <math id="S3.SS3.SSS2.p1.6.m6.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.SSS2.p1.6.m6.1a"><mi id="S3.SS3.SSS2.p1.6.m6.1.1" xref="S3.SS3.SSS2.p1.6.m6.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.6.m6.1b"><ci id="S3.SS3.SSS2.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS2.p1.6.m6.1.1">ùêæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.6.m6.1c">K</annotation></semantics></math> is the minimum between the ground truth and the predicted answer lists lengths. The Hungarian matching (<math id="S3.SS3.SSS2.p1.7.m7.1" class="ltx_Math" alttext="\Psi" display="inline"><semantics id="S3.SS3.SSS2.p1.7.m7.1a"><mi mathvariant="normal" id="S3.SS3.SSS2.p1.7.m7.1.1" xref="S3.SS3.SSS2.p1.7.m7.1.1.cmml">Œ®</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.7.m7.1b"><ci id="S3.SS3.SSS2.p1.7.m7.1.1.cmml" xref="S3.SS3.SSS2.p1.7.m7.1.1">Œ®</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.7.m7.1c">\Psi</annotation></semantics></math>) is performed according to the Normalized Levenshtein Similarity (<math id="S3.SS3.SSS2.p1.8.m8.1" class="ltx_Math" alttext="NLS" display="inline"><semantics id="S3.SS3.SSS2.p1.8.m8.1a"><mrow id="S3.SS3.SSS2.p1.8.m8.1.1" xref="S3.SS3.SSS2.p1.8.m8.1.1.cmml"><mi id="S3.SS3.SSS2.p1.8.m8.1.1.2" xref="S3.SS3.SSS2.p1.8.m8.1.1.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS2.p1.8.m8.1.1.1" xref="S3.SS3.SSS2.p1.8.m8.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.SSS2.p1.8.m8.1.1.3" xref="S3.SS3.SSS2.p1.8.m8.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS2.p1.8.m8.1.1.1a" xref="S3.SS3.SSS2.p1.8.m8.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.SSS2.p1.8.m8.1.1.4" xref="S3.SS3.SSS2.p1.8.m8.1.1.4.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.8.m8.1b"><apply id="S3.SS3.SSS2.p1.8.m8.1.1.cmml" xref="S3.SS3.SSS2.p1.8.m8.1.1"><times id="S3.SS3.SSS2.p1.8.m8.1.1.1.cmml" xref="S3.SS3.SSS2.p1.8.m8.1.1.1"></times><ci id="S3.SS3.SSS2.p1.8.m8.1.1.2.cmml" xref="S3.SS3.SSS2.p1.8.m8.1.1.2">ùëÅ</ci><ci id="S3.SS3.SSS2.p1.8.m8.1.1.3.cmml" xref="S3.SS3.SSS2.p1.8.m8.1.1.3">ùêø</ci><ci id="S3.SS3.SSS2.p1.8.m8.1.1.4.cmml" xref="S3.SS3.SSS2.p1.8.m8.1.1.4">ùëÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.8.m8.1c">NLS</annotation></semantics></math>) between each ground truth element <math id="S3.SS3.SSS2.p1.9.m9.1" class="ltx_Math" alttext="g_{j}\in G" display="inline"><semantics id="S3.SS3.SSS2.p1.9.m9.1a"><mrow id="S3.SS3.SSS2.p1.9.m9.1.1" xref="S3.SS3.SSS2.p1.9.m9.1.1.cmml"><msub id="S3.SS3.SSS2.p1.9.m9.1.1.2" xref="S3.SS3.SSS2.p1.9.m9.1.1.2.cmml"><mi id="S3.SS3.SSS2.p1.9.m9.1.1.2.2" xref="S3.SS3.SSS2.p1.9.m9.1.1.2.2.cmml">g</mi><mi id="S3.SS3.SSS2.p1.9.m9.1.1.2.3" xref="S3.SS3.SSS2.p1.9.m9.1.1.2.3.cmml">j</mi></msub><mo id="S3.SS3.SSS2.p1.9.m9.1.1.1" xref="S3.SS3.SSS2.p1.9.m9.1.1.1.cmml">‚àà</mo><mi id="S3.SS3.SSS2.p1.9.m9.1.1.3" xref="S3.SS3.SSS2.p1.9.m9.1.1.3.cmml">G</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.9.m9.1b"><apply id="S3.SS3.SSS2.p1.9.m9.1.1.cmml" xref="S3.SS3.SSS2.p1.9.m9.1.1"><in id="S3.SS3.SSS2.p1.9.m9.1.1.1.cmml" xref="S3.SS3.SSS2.p1.9.m9.1.1.1"></in><apply id="S3.SS3.SSS2.p1.9.m9.1.1.2.cmml" xref="S3.SS3.SSS2.p1.9.m9.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.9.m9.1.1.2.1.cmml" xref="S3.SS3.SSS2.p1.9.m9.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS2.p1.9.m9.1.1.2.2.cmml" xref="S3.SS3.SSS2.p1.9.m9.1.1.2.2">ùëî</ci><ci id="S3.SS3.SSS2.p1.9.m9.1.1.2.3.cmml" xref="S3.SS3.SSS2.p1.9.m9.1.1.2.3">ùëó</ci></apply><ci id="S3.SS3.SSS2.p1.9.m9.1.1.3.cmml" xref="S3.SS3.SSS2.p1.9.m9.1.1.3">ùê∫</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.9.m9.1c">g_{j}\in G</annotation></semantics></math> and each prediction <math id="S3.SS3.SSS2.p1.10.m10.1" class="ltx_Math" alttext="p_{i}\in P" display="inline"><semantics id="S3.SS3.SSS2.p1.10.m10.1a"><mrow id="S3.SS3.SSS2.p1.10.m10.1.1" xref="S3.SS3.SSS2.p1.10.m10.1.1.cmml"><msub id="S3.SS3.SSS2.p1.10.m10.1.1.2" xref="S3.SS3.SSS2.p1.10.m10.1.1.2.cmml"><mi id="S3.SS3.SSS2.p1.10.m10.1.1.2.2" xref="S3.SS3.SSS2.p1.10.m10.1.1.2.2.cmml">p</mi><mi id="S3.SS3.SSS2.p1.10.m10.1.1.2.3" xref="S3.SS3.SSS2.p1.10.m10.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS3.SSS2.p1.10.m10.1.1.1" xref="S3.SS3.SSS2.p1.10.m10.1.1.1.cmml">‚àà</mo><mi id="S3.SS3.SSS2.p1.10.m10.1.1.3" xref="S3.SS3.SSS2.p1.10.m10.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.10.m10.1b"><apply id="S3.SS3.SSS2.p1.10.m10.1.1.cmml" xref="S3.SS3.SSS2.p1.10.m10.1.1"><in id="S3.SS3.SSS2.p1.10.m10.1.1.1.cmml" xref="S3.SS3.SSS2.p1.10.m10.1.1.1"></in><apply id="S3.SS3.SSS2.p1.10.m10.1.1.2.cmml" xref="S3.SS3.SSS2.p1.10.m10.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.10.m10.1.1.2.1.cmml" xref="S3.SS3.SSS2.p1.10.m10.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS2.p1.10.m10.1.1.2.2.cmml" xref="S3.SS3.SSS2.p1.10.m10.1.1.2.2">ùëù</ci><ci id="S3.SS3.SSS2.p1.10.m10.1.1.2.3.cmml" xref="S3.SS3.SSS2.p1.10.m10.1.1.2.3">ùëñ</ci></apply><ci id="S3.SS3.SSS2.p1.10.m10.1.1.3.cmml" xref="S3.SS3.SSS2.p1.10.m10.1.1.3">ùëÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.10.m10.1c">p_{i}\in P</annotation></semantics></math>. Once the matching is performed, all the NLS scores of the <math id="S3.SS3.SSS2.p1.11.m11.1" class="ltx_Math" alttext="u_{z}\in U" display="inline"><semantics id="S3.SS3.SSS2.p1.11.m11.1a"><mrow id="S3.SS3.SSS2.p1.11.m11.1.1" xref="S3.SS3.SSS2.p1.11.m11.1.1.cmml"><msub id="S3.SS3.SSS2.p1.11.m11.1.1.2" xref="S3.SS3.SSS2.p1.11.m11.1.1.2.cmml"><mi id="S3.SS3.SSS2.p1.11.m11.1.1.2.2" xref="S3.SS3.SSS2.p1.11.m11.1.1.2.2.cmml">u</mi><mi id="S3.SS3.SSS2.p1.11.m11.1.1.2.3" xref="S3.SS3.SSS2.p1.11.m11.1.1.2.3.cmml">z</mi></msub><mo id="S3.SS3.SSS2.p1.11.m11.1.1.1" xref="S3.SS3.SSS2.p1.11.m11.1.1.1.cmml">‚àà</mo><mi id="S3.SS3.SSS2.p1.11.m11.1.1.3" xref="S3.SS3.SSS2.p1.11.m11.1.1.3.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.11.m11.1b"><apply id="S3.SS3.SSS2.p1.11.m11.1.1.cmml" xref="S3.SS3.SSS2.p1.11.m11.1.1"><in id="S3.SS3.SSS2.p1.11.m11.1.1.1.cmml" xref="S3.SS3.SSS2.p1.11.m11.1.1.1"></in><apply id="S3.SS3.SSS2.p1.11.m11.1.1.2.cmml" xref="S3.SS3.SSS2.p1.11.m11.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.11.m11.1.1.2.1.cmml" xref="S3.SS3.SSS2.p1.11.m11.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS2.p1.11.m11.1.1.2.2.cmml" xref="S3.SS3.SSS2.p1.11.m11.1.1.2.2">ùë¢</ci><ci id="S3.SS3.SSS2.p1.11.m11.1.1.2.3.cmml" xref="S3.SS3.SSS2.p1.11.m11.1.1.2.3">ùëß</ci></apply><ci id="S3.SS3.SSS2.p1.11.m11.1.1.3.cmml" xref="S3.SS3.SSS2.p1.11.m11.1.1.3">ùëà</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.11.m11.1c">u_{z}\in U</annotation></semantics></math> pairs are summed and divided for the maximum length of both ground truth and predicted answer lists. Therefore, if there are more or less ground truth answers than the ones predicted, the method is penalized.</p>
</div>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E1X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1X.2.1.1.m1.3" class="ltx_Math" alttext="\displaystyle U={\Psi}(NLS(G,P))" display="inline"><semantics id="S3.E1X.2.1.1.m1.3a"><mrow id="S3.E1X.2.1.1.m1.3.3" xref="S3.E1X.2.1.1.m1.3.3.cmml"><mi id="S3.E1X.2.1.1.m1.3.3.3" xref="S3.E1X.2.1.1.m1.3.3.3.cmml">U</mi><mo id="S3.E1X.2.1.1.m1.3.3.2" xref="S3.E1X.2.1.1.m1.3.3.2.cmml">=</mo><mrow id="S3.E1X.2.1.1.m1.3.3.1" xref="S3.E1X.2.1.1.m1.3.3.1.cmml"><mi mathvariant="normal" id="S3.E1X.2.1.1.m1.3.3.1.3" xref="S3.E1X.2.1.1.m1.3.3.1.3.cmml">Œ®</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.3.3.1.2" xref="S3.E1X.2.1.1.m1.3.3.1.2.cmml">‚Äã</mo><mrow id="S3.E1X.2.1.1.m1.3.3.1.1.1" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1X.2.1.1.m1.3.3.1.1.1.2" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.E1X.2.1.1.m1.3.3.1.1.1.1" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.cmml"><mi id="S3.E1X.2.1.1.m1.3.3.1.1.1.1.2" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.3.3.1.1.1.1.1" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.1.cmml">‚Äã</mo><mi id="S3.E1X.2.1.1.m1.3.3.1.1.1.1.3" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.3.3.1.1.1.1.1a" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.1.cmml">‚Äã</mo><mi id="S3.E1X.2.1.1.m1.3.3.1.1.1.1.4" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.3.3.1.1.1.1.1b" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.1.cmml">‚Äã</mo><mrow id="S3.E1X.2.1.1.m1.3.3.1.1.1.1.5.2" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.5.1.cmml"><mo stretchy="false" id="S3.E1X.2.1.1.m1.3.3.1.1.1.1.5.2.1" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.5.1.cmml">(</mo><mi id="S3.E1X.2.1.1.m1.1.1" xref="S3.E1X.2.1.1.m1.1.1.cmml">G</mi><mo id="S3.E1X.2.1.1.m1.3.3.1.1.1.1.5.2.2" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.5.1.cmml">,</mo><mi id="S3.E1X.2.1.1.m1.2.2" xref="S3.E1X.2.1.1.m1.2.2.cmml">P</mi><mo stretchy="false" id="S3.E1X.2.1.1.m1.3.3.1.1.1.1.5.2.3" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.5.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1X.2.1.1.m1.3.3.1.1.1.3" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1X.2.1.1.m1.3b"><apply id="S3.E1X.2.1.1.m1.3.3.cmml" xref="S3.E1X.2.1.1.m1.3.3"><eq id="S3.E1X.2.1.1.m1.3.3.2.cmml" xref="S3.E1X.2.1.1.m1.3.3.2"></eq><ci id="S3.E1X.2.1.1.m1.3.3.3.cmml" xref="S3.E1X.2.1.1.m1.3.3.3">ùëà</ci><apply id="S3.E1X.2.1.1.m1.3.3.1.cmml" xref="S3.E1X.2.1.1.m1.3.3.1"><times id="S3.E1X.2.1.1.m1.3.3.1.2.cmml" xref="S3.E1X.2.1.1.m1.3.3.1.2"></times><ci id="S3.E1X.2.1.1.m1.3.3.1.3.cmml" xref="S3.E1X.2.1.1.m1.3.3.1.3">Œ®</ci><apply id="S3.E1X.2.1.1.m1.3.3.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.3.3.1.1.1"><times id="S3.E1X.2.1.1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.1"></times><ci id="S3.E1X.2.1.1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.2">ùëÅ</ci><ci id="S3.E1X.2.1.1.m1.3.3.1.1.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.3">ùêø</ci><ci id="S3.E1X.2.1.1.m1.3.3.1.1.1.1.4.cmml" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.4">ùëÜ</ci><interval closure="open" id="S3.E1X.2.1.1.m1.3.3.1.1.1.1.5.1.cmml" xref="S3.E1X.2.1.1.m1.3.3.1.1.1.1.5.2"><ci id="S3.E1X.2.1.1.m1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1">ùê∫</ci><ci id="S3.E1X.2.1.1.m1.2.2.cmml" xref="S3.E1X.2.1.1.m1.2.2">ùëÉ</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.2.1.1.m1.3c">\displaystyle U={\Psi}(NLS(G,P))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
<tr id="S3.E1Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1Xa.2.1.1.m1.4" class="ltx_Math" alttext="\displaystyle ANLSL=\frac{1}{\max(M,N)}\sum_{z=1}^{K}NLS(u_{z})" display="inline"><semantics id="S3.E1Xa.2.1.1.m1.4a"><mrow id="S3.E1Xa.2.1.1.m1.4.4" xref="S3.E1Xa.2.1.1.m1.4.4.cmml"><mrow id="S3.E1Xa.2.1.1.m1.4.4.3" xref="S3.E1Xa.2.1.1.m1.4.4.3.cmml"><mi id="S3.E1Xa.2.1.1.m1.4.4.3.2" xref="S3.E1Xa.2.1.1.m1.4.4.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1Xa.2.1.1.m1.4.4.3.1" xref="S3.E1Xa.2.1.1.m1.4.4.3.1.cmml">‚Äã</mo><mi id="S3.E1Xa.2.1.1.m1.4.4.3.3" xref="S3.E1Xa.2.1.1.m1.4.4.3.3.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.E1Xa.2.1.1.m1.4.4.3.1a" xref="S3.E1Xa.2.1.1.m1.4.4.3.1.cmml">‚Äã</mo><mi id="S3.E1Xa.2.1.1.m1.4.4.3.4" xref="S3.E1Xa.2.1.1.m1.4.4.3.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E1Xa.2.1.1.m1.4.4.3.1b" xref="S3.E1Xa.2.1.1.m1.4.4.3.1.cmml">‚Äã</mo><mi id="S3.E1Xa.2.1.1.m1.4.4.3.5" xref="S3.E1Xa.2.1.1.m1.4.4.3.5.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E1Xa.2.1.1.m1.4.4.3.1c" xref="S3.E1Xa.2.1.1.m1.4.4.3.1.cmml">‚Äã</mo><mi id="S3.E1Xa.2.1.1.m1.4.4.3.6" xref="S3.E1Xa.2.1.1.m1.4.4.3.6.cmml">L</mi></mrow><mo id="S3.E1Xa.2.1.1.m1.4.4.2" xref="S3.E1Xa.2.1.1.m1.4.4.2.cmml">=</mo><mrow id="S3.E1Xa.2.1.1.m1.4.4.1" xref="S3.E1Xa.2.1.1.m1.4.4.1.cmml"><mstyle displaystyle="true" id="S3.E1Xa.2.1.1.m1.3.3" xref="S3.E1Xa.2.1.1.m1.3.3.cmml"><mfrac id="S3.E1Xa.2.1.1.m1.3.3a" xref="S3.E1Xa.2.1.1.m1.3.3.cmml"><mn id="S3.E1Xa.2.1.1.m1.3.3.5" xref="S3.E1Xa.2.1.1.m1.3.3.5.cmml">1</mn><mrow id="S3.E1Xa.2.1.1.m1.3.3.3.5" xref="S3.E1Xa.2.1.1.m1.3.3.3.4.cmml"><mi id="S3.E1Xa.2.1.1.m1.1.1.1.1" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.cmml">max</mi><mo id="S3.E1Xa.2.1.1.m1.3.3.3.5a" xref="S3.E1Xa.2.1.1.m1.3.3.3.4.cmml">‚Å°</mo><mrow id="S3.E1Xa.2.1.1.m1.3.3.3.5.1" xref="S3.E1Xa.2.1.1.m1.3.3.3.4.cmml"><mo stretchy="false" id="S3.E1Xa.2.1.1.m1.3.3.3.5.1.1" xref="S3.E1Xa.2.1.1.m1.3.3.3.4.cmml">(</mo><mi id="S3.E1Xa.2.1.1.m1.2.2.2.2" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.cmml">M</mi><mo id="S3.E1Xa.2.1.1.m1.3.3.3.5.1.2" xref="S3.E1Xa.2.1.1.m1.3.3.3.4.cmml">,</mo><mi id="S3.E1Xa.2.1.1.m1.3.3.3.3" xref="S3.E1Xa.2.1.1.m1.3.3.3.3.cmml">N</mi><mo stretchy="false" id="S3.E1Xa.2.1.1.m1.3.3.3.5.1.3" xref="S3.E1Xa.2.1.1.m1.3.3.3.4.cmml">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.E1Xa.2.1.1.m1.4.4.1.2" xref="S3.E1Xa.2.1.1.m1.4.4.1.2.cmml">‚Äã</mo><mrow id="S3.E1Xa.2.1.1.m1.4.4.1.1" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.cmml"><mstyle displaystyle="true" id="S3.E1Xa.2.1.1.m1.4.4.1.1.2" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2.cmml"><munderover id="S3.E1Xa.2.1.1.m1.4.4.1.1.2a" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2.cmml"><mo movablelimits="false" id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.2" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.2.cmml">‚àë</mo><mrow id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.3" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.3.cmml"><mi id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.3.2" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.3.2.cmml">z</mi><mo id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.3.1" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.3.3" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.3" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2.3.cmml">K</mi></munderover></mstyle><mrow id="S3.E1Xa.2.1.1.m1.4.4.1.1.1" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.cmml"><mi id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.3" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.3.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.2" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.2.cmml">‚Äã</mo><mi id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.4" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.2a" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.2.cmml">‚Äã</mo><mi id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.5" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.5.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.2b" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.2.cmml">‚Äã</mo><mrow id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.2" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.2" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.2.cmml">u</mi><mi id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.3" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.3.cmml">z</mi></msub><mo stretchy="false" id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.3" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1Xa.2.1.1.m1.4b"><apply id="S3.E1Xa.2.1.1.m1.4.4.cmml" xref="S3.E1Xa.2.1.1.m1.4.4"><eq id="S3.E1Xa.2.1.1.m1.4.4.2.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.2"></eq><apply id="S3.E1Xa.2.1.1.m1.4.4.3.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.3"><times id="S3.E1Xa.2.1.1.m1.4.4.3.1.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.3.1"></times><ci id="S3.E1Xa.2.1.1.m1.4.4.3.2.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.3.2">ùê¥</ci><ci id="S3.E1Xa.2.1.1.m1.4.4.3.3.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.3.3">ùëÅ</ci><ci id="S3.E1Xa.2.1.1.m1.4.4.3.4.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.3.4">ùêø</ci><ci id="S3.E1Xa.2.1.1.m1.4.4.3.5.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.3.5">ùëÜ</ci><ci id="S3.E1Xa.2.1.1.m1.4.4.3.6.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.3.6">ùêø</ci></apply><apply id="S3.E1Xa.2.1.1.m1.4.4.1.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1"><times id="S3.E1Xa.2.1.1.m1.4.4.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.2"></times><apply id="S3.E1Xa.2.1.1.m1.3.3.cmml" xref="S3.E1Xa.2.1.1.m1.3.3"><divide id="S3.E1Xa.2.1.1.m1.3.3.4.cmml" xref="S3.E1Xa.2.1.1.m1.3.3"></divide><cn type="integer" id="S3.E1Xa.2.1.1.m1.3.3.5.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.5">1</cn><apply id="S3.E1Xa.2.1.1.m1.3.3.3.4.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.3.5"><max id="S3.E1Xa.2.1.1.m1.1.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1"></max><ci id="S3.E1Xa.2.1.1.m1.2.2.2.2.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.2.2">ùëÄ</ci><ci id="S3.E1Xa.2.1.1.m1.3.3.3.3.cmml" xref="S3.E1Xa.2.1.1.m1.3.3.3.3">ùëÅ</ci></apply></apply><apply id="S3.E1Xa.2.1.1.m1.4.4.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1"><apply id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2"><csymbol cd="ambiguous" id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.1.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2">superscript</csymbol><apply id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2"><csymbol cd="ambiguous" id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.1.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2">subscript</csymbol><sum id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.2.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.2"></sum><apply id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.3.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.3"><eq id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.3.1.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.3.1"></eq><ci id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.3.2.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.3.2">ùëß</ci><cn type="integer" id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.3.3.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E1Xa.2.1.1.m1.4.4.1.1.2.3.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.2.3">ùêæ</ci></apply><apply id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1"><times id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.2"></times><ci id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.3.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.3">ùëÅ</ci><ci id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.4.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.4">ùêø</ci><ci id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.5.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.5">ùëÜ</ci><apply id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1">subscript</csymbol><ci id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.2">ùë¢</ci><ci id="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E1Xa.2.1.1.m1.4.4.1.1.1.1.1.1.3">ùëß</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1Xa.2.1.1.m1.4c">\displaystyle ANLSL=\frac{1}{\max(M,N)}\sum_{z=1}^{K}NLS(u_{z})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Baselines</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section describes the two baselines that are employed in the experiments. Both baselines breakdown the task into two different stages. First, they rank the documents according to the confidence of containing the information to answer a given a question and then, they get the answers from the documents with the highest confidence. The first baseline combines methods from the word spotting and NLP Question Answering fields to retrieve the relevant documents and answer the questions. We name this baseline as <em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">Text spotting + QA</em>. In contrast, the second baseline is an ad-hoc method specially designed for this task and data, which consist on extracting the information from the documents and map it in the format of key-value relations. In this sense it represents the collection similar as databases do, for which we name this baseline as <em id="S4.p1.1.2" class="ltx_emph ltx_font_italic">Database</em>. These baselines allows to appreciate the performance of two very different approaches.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Text spotting + QA</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The objective of this baseline is to set a starting performance result from the combination of two simple but generic methods that will allow to assess the improvement of future proposed methods.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Evidence retrieval:</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.6" class="ltx_p">To retrieve the relevant documents we apply a text spotting approach, which consist on ranking the documents according to a confidence given a query, which in our case is the question. To obtain this confidence, we first run a Part Of Speech (POS) tagger over the question to identify the most relevant words in it by keeping only nouns and digits, and ignore the rest of the words. Then, as described in equation <a href="#S4.E2" title="In 4.1.1 Evidence retrieval: ‚Ä£ 4.1 Text spotting + QA ‚Ä£ 4 Baselines ‚Ä£ Document Collection Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, given a question <math id="S4.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><mi id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><ci id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">Q</annotation></semantics></math>, for each relevant word in the question <math id="S4.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="qw_{i}\in Q" display="inline"><semantics id="S4.SS1.SSS1.p1.2.m2.1a"><mrow id="S4.SS1.SSS1.p1.2.m2.1.1" xref="S4.SS1.SSS1.p1.2.m2.1.1.cmml"><mrow id="S4.SS1.SSS1.p1.2.m2.1.1.2" xref="S4.SS1.SSS1.p1.2.m2.1.1.2.cmml"><mi id="S4.SS1.SSS1.p1.2.m2.1.1.2.2" xref="S4.SS1.SSS1.p1.2.m2.1.1.2.2.cmml">q</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p1.2.m2.1.1.2.1" xref="S4.SS1.SSS1.p1.2.m2.1.1.2.1.cmml">‚Äã</mo><msub id="S4.SS1.SSS1.p1.2.m2.1.1.2.3" xref="S4.SS1.SSS1.p1.2.m2.1.1.2.3.cmml"><mi id="S4.SS1.SSS1.p1.2.m2.1.1.2.3.2" xref="S4.SS1.SSS1.p1.2.m2.1.1.2.3.2.cmml">w</mi><mi id="S4.SS1.SSS1.p1.2.m2.1.1.2.3.3" xref="S4.SS1.SSS1.p1.2.m2.1.1.2.3.3.cmml">i</mi></msub></mrow><mo id="S4.SS1.SSS1.p1.2.m2.1.1.1" xref="S4.SS1.SSS1.p1.2.m2.1.1.1.cmml">‚àà</mo><mi id="S4.SS1.SSS1.p1.2.m2.1.1.3" xref="S4.SS1.SSS1.p1.2.m2.1.1.3.cmml">Q</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.2.m2.1b"><apply id="S4.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1"><in id="S4.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.1"></in><apply id="S4.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.2"><times id="S4.SS1.SSS1.p1.2.m2.1.1.2.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.2.1"></times><ci id="S4.SS1.SSS1.p1.2.m2.1.1.2.2.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.2.2">ùëû</ci><apply id="S4.SS1.SSS1.p1.2.m2.1.1.2.3.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.2.3"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.2.m2.1.1.2.3.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.2.3">subscript</csymbol><ci id="S4.SS1.SSS1.p1.2.m2.1.1.2.3.2.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.2.3.2">ùë§</ci><ci id="S4.SS1.SSS1.p1.2.m2.1.1.2.3.3.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.2.3.3">ùëñ</ci></apply></apply><ci id="S4.SS1.SSS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1.3">ùëÑ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.2.m2.1c">qw_{i}\in Q</annotation></semantics></math> we get the minimum Normalized Levenshtein Distance (<math id="S4.SS1.SSS1.p1.3.m3.1" class="ltx_Math" alttext="NLD" display="inline"><semantics id="S4.SS1.SSS1.p1.3.m3.1a"><mrow id="S4.SS1.SSS1.p1.3.m3.1.1" xref="S4.SS1.SSS1.p1.3.m3.1.1.cmml"><mi id="S4.SS1.SSS1.p1.3.m3.1.1.2" xref="S4.SS1.SSS1.p1.3.m3.1.1.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p1.3.m3.1.1.1" xref="S4.SS1.SSS1.p1.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.SSS1.p1.3.m3.1.1.3" xref="S4.SS1.SSS1.p1.3.m3.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p1.3.m3.1.1.1a" xref="S4.SS1.SSS1.p1.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.SSS1.p1.3.m3.1.1.4" xref="S4.SS1.SSS1.p1.3.m3.1.1.4.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.3.m3.1b"><apply id="S4.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS1.p1.3.m3.1.1"><times id="S4.SS1.SSS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS1.p1.3.m3.1.1.1"></times><ci id="S4.SS1.SSS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS1.p1.3.m3.1.1.2">ùëÅ</ci><ci id="S4.SS1.SSS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.SSS1.p1.3.m3.1.1.3">ùêø</ci><ci id="S4.SS1.SSS1.p1.3.m3.1.1.4.cmml" xref="S4.SS1.SSS1.p1.3.m3.1.1.4">ùê∑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.3.m3.1c">NLD</annotation></semantics></math>)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, between all recognized words <math id="S4.SS1.SSS1.p1.4.m4.1" class="ltx_Math" alttext="rw_{j}" display="inline"><semantics id="S4.SS1.SSS1.p1.4.m4.1a"><mrow id="S4.SS1.SSS1.p1.4.m4.1.1" xref="S4.SS1.SSS1.p1.4.m4.1.1.cmml"><mi id="S4.SS1.SSS1.p1.4.m4.1.1.2" xref="S4.SS1.SSS1.p1.4.m4.1.1.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p1.4.m4.1.1.1" xref="S4.SS1.SSS1.p1.4.m4.1.1.1.cmml">‚Äã</mo><msub id="S4.SS1.SSS1.p1.4.m4.1.1.3" xref="S4.SS1.SSS1.p1.4.m4.1.1.3.cmml"><mi id="S4.SS1.SSS1.p1.4.m4.1.1.3.2" xref="S4.SS1.SSS1.p1.4.m4.1.1.3.2.cmml">w</mi><mi id="S4.SS1.SSS1.p1.4.m4.1.1.3.3" xref="S4.SS1.SSS1.p1.4.m4.1.1.3.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.4.m4.1b"><apply id="S4.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1"><times id="S4.SS1.SSS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1.1"></times><ci id="S4.SS1.SSS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1.2">ùëü</ci><apply id="S4.SS1.SSS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.4.m4.1.1.3.1.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1.3">subscript</csymbol><ci id="S4.SS1.SSS1.p1.4.m4.1.1.3.2.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1.3.2">ùë§</ci><ci id="S4.SS1.SSS1.p1.4.m4.1.1.3.3.cmml" xref="S4.SS1.SSS1.p1.4.m4.1.1.3.3">ùëó</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.4.m4.1c">rw_{j}</annotation></semantics></math> extracted through an OCR in the document and the question word. Then, we average over all the distances and use the result as the confidence <math id="S4.SS1.SSS1.p1.5.m5.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S4.SS1.SSS1.p1.5.m5.1a"><mi id="S4.SS1.SSS1.p1.5.m5.1.1" xref="S4.SS1.SSS1.p1.5.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.5.m5.1b"><ci id="S4.SS1.SSS1.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS1.p1.5.m5.1.1">ùëê</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.5.m5.1c">c</annotation></semantics></math> for which the document <math id="S4.SS1.SSS1.p1.6.m6.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S4.SS1.SSS1.p1.6.m6.1a"><mi id="S4.SS1.SSS1.p1.6.m6.1.1" xref="S4.SS1.SSS1.p1.6.m6.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.6.m6.1b"><ci id="S4.SS1.SSS1.p1.6.m6.1.1.cmml" xref="S4.SS1.SSS1.p1.6.m6.1.1">ùëë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.6.m6.1c">d</annotation></semantics></math> is relevant to the question.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.5" class="ltx_Math" alttext="c_{d}=\frac{1}{|Q|}\sum_{i=1}^{|Q|}\min_{j=1}^{|OCR|}\{NLD(qw_{i},rw_{j})\}" display="block"><semantics id="S4.E2.m1.5a"><mrow id="S4.E2.m1.5.5" xref="S4.E2.m1.5.5.cmml"><msub id="S4.E2.m1.5.5.4" xref="S4.E2.m1.5.5.4.cmml"><mi id="S4.E2.m1.5.5.4.2" xref="S4.E2.m1.5.5.4.2.cmml">c</mi><mi id="S4.E2.m1.5.5.4.3" xref="S4.E2.m1.5.5.4.3.cmml">d</mi></msub><mo id="S4.E2.m1.5.5.3" xref="S4.E2.m1.5.5.3.cmml">=</mo><mrow id="S4.E2.m1.5.5.2" xref="S4.E2.m1.5.5.2.cmml"><mfrac id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mn id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml">1</mn><mrow id="S4.E2.m1.1.1.1.3" xref="S4.E2.m1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E2.m1.1.1.1.3.1" xref="S4.E2.m1.1.1.1.2.1.cmml">|</mo><mi id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml">Q</mi><mo stretchy="false" id="S4.E2.m1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="S4.E2.m1.5.5.2.3" xref="S4.E2.m1.5.5.2.3.cmml">‚Äã</mo><mrow id="S4.E2.m1.5.5.2.2" xref="S4.E2.m1.5.5.2.2.cmml"><munderover id="S4.E2.m1.5.5.2.2.3" xref="S4.E2.m1.5.5.2.2.3.cmml"><mo movablelimits="false" id="S4.E2.m1.5.5.2.2.3.2.2" xref="S4.E2.m1.5.5.2.2.3.2.2.cmml">‚àë</mo><mrow id="S4.E2.m1.5.5.2.2.3.2.3" xref="S4.E2.m1.5.5.2.2.3.2.3.cmml"><mi id="S4.E2.m1.5.5.2.2.3.2.3.2" xref="S4.E2.m1.5.5.2.2.3.2.3.2.cmml">i</mi><mo id="S4.E2.m1.5.5.2.2.3.2.3.1" xref="S4.E2.m1.5.5.2.2.3.2.3.1.cmml">=</mo><mn id="S4.E2.m1.5.5.2.2.3.2.3.3" xref="S4.E2.m1.5.5.2.2.3.2.3.3.cmml">1</mn></mrow><mrow id="S4.E2.m1.2.2.1.3" xref="S4.E2.m1.2.2.1.2.cmml"><mo stretchy="false" id="S4.E2.m1.2.2.1.3.1" xref="S4.E2.m1.2.2.1.2.1.cmml">|</mo><mi id="S4.E2.m1.2.2.1.1" xref="S4.E2.m1.2.2.1.1.cmml">Q</mi><mo stretchy="false" id="S4.E2.m1.2.2.1.3.2" xref="S4.E2.m1.2.2.1.2.1.cmml">|</mo></mrow></munderover><mrow id="S4.E2.m1.5.5.2.2.2.2" xref="S4.E2.m1.5.5.2.2.2.3.cmml"><munderover id="S4.E2.m1.4.4.1.1.1.1.1" xref="S4.E2.m1.4.4.1.1.1.1.1.cmml"><mi id="S4.E2.m1.4.4.1.1.1.1.1.2.2" xref="S4.E2.m1.4.4.1.1.1.1.1.2.2.cmml">min</mi><mrow id="S4.E2.m1.4.4.1.1.1.1.1.2.3" xref="S4.E2.m1.4.4.1.1.1.1.1.2.3.cmml"><mi id="S4.E2.m1.4.4.1.1.1.1.1.2.3.2" xref="S4.E2.m1.4.4.1.1.1.1.1.2.3.2.cmml">j</mi><mo id="S4.E2.m1.4.4.1.1.1.1.1.2.3.1" xref="S4.E2.m1.4.4.1.1.1.1.1.2.3.1.cmml">=</mo><mn id="S4.E2.m1.4.4.1.1.1.1.1.2.3.3" xref="S4.E2.m1.4.4.1.1.1.1.1.2.3.3.cmml">1</mn></mrow><mrow id="S4.E2.m1.3.3.1.1" xref="S4.E2.m1.3.3.1.2.cmml"><mo stretchy="false" id="S4.E2.m1.3.3.1.1.2" xref="S4.E2.m1.3.3.1.2.1.cmml">|</mo><mrow id="S4.E2.m1.3.3.1.1.1" xref="S4.E2.m1.3.3.1.1.1.cmml"><mi id="S4.E2.m1.3.3.1.1.1.2" xref="S4.E2.m1.3.3.1.1.1.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.3.3.1.1.1.1" xref="S4.E2.m1.3.3.1.1.1.1.cmml">‚Äã</mo><mi id="S4.E2.m1.3.3.1.1.1.3" xref="S4.E2.m1.3.3.1.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.3.3.1.1.1.1a" xref="S4.E2.m1.3.3.1.1.1.1.cmml">‚Äã</mo><mi id="S4.E2.m1.3.3.1.1.1.4" xref="S4.E2.m1.3.3.1.1.1.4.cmml">R</mi></mrow><mo stretchy="false" id="S4.E2.m1.3.3.1.1.3" xref="S4.E2.m1.3.3.1.2.1.cmml">|</mo></mrow></munderover><mo id="S4.E2.m1.5.5.2.2.2.2a" xref="S4.E2.m1.5.5.2.2.2.3.cmml">‚Å°</mo><mrow id="S4.E2.m1.5.5.2.2.2.2.2" xref="S4.E2.m1.5.5.2.2.2.3.cmml"><mo stretchy="false" id="S4.E2.m1.5.5.2.2.2.2.2.2" xref="S4.E2.m1.5.5.2.2.2.3.cmml">{</mo><mrow id="S4.E2.m1.5.5.2.2.2.2.2.1" xref="S4.E2.m1.5.5.2.2.2.2.2.1.cmml"><mi id="S4.E2.m1.5.5.2.2.2.2.2.1.4" xref="S4.E2.m1.5.5.2.2.2.2.2.1.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.5.5.2.2.2.2.2.1.3" xref="S4.E2.m1.5.5.2.2.2.2.2.1.3.cmml">‚Äã</mo><mi id="S4.E2.m1.5.5.2.2.2.2.2.1.5" xref="S4.E2.m1.5.5.2.2.2.2.2.1.5.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.5.5.2.2.2.2.2.1.3a" xref="S4.E2.m1.5.5.2.2.2.2.2.1.3.cmml">‚Äã</mo><mi id="S4.E2.m1.5.5.2.2.2.2.2.1.6" xref="S4.E2.m1.5.5.2.2.2.2.2.1.6.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.5.5.2.2.2.2.2.1.3b" xref="S4.E2.m1.5.5.2.2.2.2.2.1.3.cmml">‚Äã</mo><mrow id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.3.cmml"><mo stretchy="false" id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.3" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.3.cmml">(</mo><mrow id="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1" xref="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.cmml"><mi id="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.2" xref="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.2.cmml">q</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.1" xref="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.1.cmml">‚Äã</mo><msub id="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.3" xref="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.3.cmml"><mi id="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.3.2" xref="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.3.2.cmml">w</mi><mi id="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.3.3" xref="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.4" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.3.cmml">,</mo><mrow id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.cmml"><mi id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.2" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.1" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.1.cmml">‚Äã</mo><msub id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.3" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.3.cmml"><mi id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.3.2" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.3.2.cmml">w</mi><mi id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.3.3" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.3.3.cmml">j</mi></msub></mrow><mo stretchy="false" id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.5" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E2.m1.5.5.2.2.2.2.2.3" xref="S4.E2.m1.5.5.2.2.2.3.cmml">}</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.5b"><apply id="S4.E2.m1.5.5.cmml" xref="S4.E2.m1.5.5"><eq id="S4.E2.m1.5.5.3.cmml" xref="S4.E2.m1.5.5.3"></eq><apply id="S4.E2.m1.5.5.4.cmml" xref="S4.E2.m1.5.5.4"><csymbol cd="ambiguous" id="S4.E2.m1.5.5.4.1.cmml" xref="S4.E2.m1.5.5.4">subscript</csymbol><ci id="S4.E2.m1.5.5.4.2.cmml" xref="S4.E2.m1.5.5.4.2">ùëê</ci><ci id="S4.E2.m1.5.5.4.3.cmml" xref="S4.E2.m1.5.5.4.3">ùëë</ci></apply><apply id="S4.E2.m1.5.5.2.cmml" xref="S4.E2.m1.5.5.2"><times id="S4.E2.m1.5.5.2.3.cmml" xref="S4.E2.m1.5.5.2.3"></times><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><divide id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1"></divide><cn type="integer" id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3">1</cn><apply id="S4.E2.m1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.3"><abs id="S4.E2.m1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.3.1"></abs><ci id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1">ùëÑ</ci></apply></apply><apply id="S4.E2.m1.5.5.2.2.cmml" xref="S4.E2.m1.5.5.2.2"><apply id="S4.E2.m1.5.5.2.2.3.cmml" xref="S4.E2.m1.5.5.2.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.5.5.2.2.3.1.cmml" xref="S4.E2.m1.5.5.2.2.3">superscript</csymbol><apply id="S4.E2.m1.5.5.2.2.3.2.cmml" xref="S4.E2.m1.5.5.2.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.5.5.2.2.3.2.1.cmml" xref="S4.E2.m1.5.5.2.2.3">subscript</csymbol><sum id="S4.E2.m1.5.5.2.2.3.2.2.cmml" xref="S4.E2.m1.5.5.2.2.3.2.2"></sum><apply id="S4.E2.m1.5.5.2.2.3.2.3.cmml" xref="S4.E2.m1.5.5.2.2.3.2.3"><eq id="S4.E2.m1.5.5.2.2.3.2.3.1.cmml" xref="S4.E2.m1.5.5.2.2.3.2.3.1"></eq><ci id="S4.E2.m1.5.5.2.2.3.2.3.2.cmml" xref="S4.E2.m1.5.5.2.2.3.2.3.2">ùëñ</ci><cn type="integer" id="S4.E2.m1.5.5.2.2.3.2.3.3.cmml" xref="S4.E2.m1.5.5.2.2.3.2.3.3">1</cn></apply></apply><apply id="S4.E2.m1.2.2.1.2.cmml" xref="S4.E2.m1.2.2.1.3"><abs id="S4.E2.m1.2.2.1.2.1.cmml" xref="S4.E2.m1.2.2.1.3.1"></abs><ci id="S4.E2.m1.2.2.1.1.cmml" xref="S4.E2.m1.2.2.1.1">ùëÑ</ci></apply></apply><apply id="S4.E2.m1.5.5.2.2.2.3.cmml" xref="S4.E2.m1.5.5.2.2.2.2"><apply id="S4.E2.m1.4.4.1.1.1.1.1.cmml" xref="S4.E2.m1.4.4.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.4.4.1.1.1.1.1.1.cmml" xref="S4.E2.m1.4.4.1.1.1.1.1">superscript</csymbol><apply id="S4.E2.m1.4.4.1.1.1.1.1.2.cmml" xref="S4.E2.m1.4.4.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.4.4.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.4.4.1.1.1.1.1">subscript</csymbol><min id="S4.E2.m1.4.4.1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.4.4.1.1.1.1.1.2.2"></min><apply id="S4.E2.m1.4.4.1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.4.4.1.1.1.1.1.2.3"><eq id="S4.E2.m1.4.4.1.1.1.1.1.2.3.1.cmml" xref="S4.E2.m1.4.4.1.1.1.1.1.2.3.1"></eq><ci id="S4.E2.m1.4.4.1.1.1.1.1.2.3.2.cmml" xref="S4.E2.m1.4.4.1.1.1.1.1.2.3.2">ùëó</ci><cn type="integer" id="S4.E2.m1.4.4.1.1.1.1.1.2.3.3.cmml" xref="S4.E2.m1.4.4.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S4.E2.m1.3.3.1.2.cmml" xref="S4.E2.m1.3.3.1.1"><abs id="S4.E2.m1.3.3.1.2.1.cmml" xref="S4.E2.m1.3.3.1.1.2"></abs><apply id="S4.E2.m1.3.3.1.1.1.cmml" xref="S4.E2.m1.3.3.1.1.1"><times id="S4.E2.m1.3.3.1.1.1.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1"></times><ci id="S4.E2.m1.3.3.1.1.1.2.cmml" xref="S4.E2.m1.3.3.1.1.1.2">ùëÇ</ci><ci id="S4.E2.m1.3.3.1.1.1.3.cmml" xref="S4.E2.m1.3.3.1.1.1.3">ùê∂</ci><ci id="S4.E2.m1.3.3.1.1.1.4.cmml" xref="S4.E2.m1.3.3.1.1.1.4">ùëÖ</ci></apply></apply></apply><apply id="S4.E2.m1.5.5.2.2.2.2.2.1.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1"><times id="S4.E2.m1.5.5.2.2.2.2.2.1.3.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.3"></times><ci id="S4.E2.m1.5.5.2.2.2.2.2.1.4.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.4">ùëÅ</ci><ci id="S4.E2.m1.5.5.2.2.2.2.2.1.5.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.5">ùêø</ci><ci id="S4.E2.m1.5.5.2.2.2.2.2.1.6.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.6">ùê∑</ci><interval closure="open" id="S4.E2.m1.5.5.2.2.2.2.2.1.2.3.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.2"><apply id="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1"><times id="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.1.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.1"></times><ci id="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.2.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.2">ùëû</ci><apply id="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.3.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.3.1.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.3">subscript</csymbol><ci id="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.3.2.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.3.2">ùë§</ci><ci id="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.3.3.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.1.1.1.3.3">ùëñ</ci></apply></apply><apply id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2"><times id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.1.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.1"></times><ci id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.2.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.2">ùëü</ci><apply id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.3.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.3.1.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.3">subscript</csymbol><ci id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.3.2.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.3.2">ùë§</ci><ci id="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.3.3.cmml" xref="S4.E2.m1.5.5.2.2.2.2.2.1.2.2.2.3.3">ùëó</ci></apply></apply></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.5c">c_{d}=\frac{1}{|Q|}\sum_{i=1}^{|Q|}\min_{j=1}^{|OCR|}\{NLD(qw_{i},rw_{j})\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.SSS1.p3" class="ltx_para">
<p id="S4.SS1.SSS1.p3.1" class="ltx_p">Notice that removing only stopwords is not enough, like in the question depicted in figure <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Document Collection Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, where the verb <em id="S4.SS1.SSS1.p3.1.1" class="ltx_emph ltx_font_italic">run</em> is not considered as stopword, but can‚Äôt be found in the document and consequently would be counterproductive.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Answering:</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.12" class="ltx_p">Once the documents are ranked, to answer the given questions we make use of BERT¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> question answering model. BERT is a task agnostic language representation based on transformers¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> that can be afterwards used in other downstream tasks. In our case, we use extractive question answering BERT models which consist on predicting the answer as a text span from a context, usually a passage or paragraph by predicting the start and end indices on that context. Nonetheless, there is no such context in the DocCVQA documents that encompasses all the textual information. Therefore, we follow the approach of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> to build this context by serializing the recognized OCR tokens on the document images to a single string separated by spaces following a top-left to bottom-right order. Then, following the original implementation of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> we introduce a start vector <math id="S4.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="S\in\mathbb{R}^{H}" display="inline"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mrow id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS2.p1.1.m1.1.1.2" xref="S4.SS1.SSS2.p1.1.m1.1.1.2.cmml">S</mi><mo id="S4.SS1.SSS2.p1.1.m1.1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.1.cmml">‚àà</mo><msup id="S4.SS1.SSS2.p1.1.m1.1.1.3" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.cmml"><mi id="S4.SS1.SSS2.p1.1.m1.1.1.3.2" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.2.cmml">‚Ñù</mi><mi id="S4.SS1.SSS2.p1.1.m1.1.1.3.3" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.3.cmml">H</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><apply id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1"><in id="S4.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.1"></in><ci id="S4.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.2">ùëÜ</ci><apply id="S4.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS2.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.2">‚Ñù</ci><ci id="S4.SS1.SSS2.p1.1.m1.1.1.3.3.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.3">ùêª</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">S\in\mathbb{R}^{H}</annotation></semantics></math> and end vector <math id="S4.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="E\in\mathbb{R}^{H}" display="inline"><semantics id="S4.SS1.SSS2.p1.2.m2.1a"><mrow id="S4.SS1.SSS2.p1.2.m2.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS2.p1.2.m2.1.1.2" xref="S4.SS1.SSS2.p1.2.m2.1.1.2.cmml">E</mi><mo id="S4.SS1.SSS2.p1.2.m2.1.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.1.cmml">‚àà</mo><msup id="S4.SS1.SSS2.p1.2.m2.1.1.3" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.cmml"><mi id="S4.SS1.SSS2.p1.2.m2.1.1.3.2" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.2.cmml">‚Ñù</mi><mi id="S4.SS1.SSS2.p1.2.m2.1.1.3.3" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.3.cmml">H</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.2.m2.1b"><apply id="S4.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1"><in id="S4.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.1"></in><ci id="S4.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.2">ùê∏</ci><apply id="S4.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.2.m2.1.1.3.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS2.p1.2.m2.1.1.3.2.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.2">‚Ñù</ci><ci id="S4.SS1.SSS2.p1.2.m2.1.1.3.3.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.3">ùêª</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.2.m2.1c">E\in\mathbb{R}^{H}</annotation></semantics></math>. The probability of a word <math id="S4.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS1.SSS2.p1.3.m3.1a"><mi id="S4.SS1.SSS2.p1.3.m3.1.1" xref="S4.SS1.SSS2.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.3.m3.1b"><ci id="S4.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS2.p1.3.m3.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.3.m3.1c">i</annotation></semantics></math> being the start of the answer span is obtained as the dot product between the BERT word embedding hidden vector <math id="S4.SS1.SSS2.p1.4.m4.1" class="ltx_Math" alttext="T_{i}" display="inline"><semantics id="S4.SS1.SSS2.p1.4.m4.1a"><msub id="S4.SS1.SSS2.p1.4.m4.1.1" xref="S4.SS1.SSS2.p1.4.m4.1.1.cmml"><mi id="S4.SS1.SSS2.p1.4.m4.1.1.2" xref="S4.SS1.SSS2.p1.4.m4.1.1.2.cmml">T</mi><mi id="S4.SS1.SSS2.p1.4.m4.1.1.3" xref="S4.SS1.SSS2.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.4.m4.1b"><apply id="S4.SS1.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.4.m4.1.1.1.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p1.4.m4.1.1.2.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.2">ùëá</ci><ci id="S4.SS1.SSS2.p1.4.m4.1.1.3.cmml" xref="S4.SS1.SSS2.p1.4.m4.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.4.m4.1c">T_{i}</annotation></semantics></math> and <math id="S4.SS1.SSS2.p1.5.m5.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S4.SS1.SSS2.p1.5.m5.1a"><mi id="S4.SS1.SSS2.p1.5.m5.1.1" xref="S4.SS1.SSS2.p1.5.m5.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.5.m5.1b"><ci id="S4.SS1.SSS2.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS2.p1.5.m5.1.1">ùëÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.5.m5.1c">S</annotation></semantics></math> followed by a softmax over all the words in the paragraph. The same formula is applied to compute if the word <math id="S4.SS1.SSS2.p1.6.m6.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS1.SSS2.p1.6.m6.1a"><mi id="S4.SS1.SSS2.p1.6.m6.1.1" xref="S4.SS1.SSS2.p1.6.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.6.m6.1b"><ci id="S4.SS1.SSS2.p1.6.m6.1.1.cmml" xref="S4.SS1.SSS2.p1.6.m6.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.6.m6.1c">i</annotation></semantics></math> is the end token by replacing the start vector <math id="S4.SS1.SSS2.p1.7.m7.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S4.SS1.SSS2.p1.7.m7.1a"><mi id="S4.SS1.SSS2.p1.7.m7.1.1" xref="S4.SS1.SSS2.p1.7.m7.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.7.m7.1b"><ci id="S4.SS1.SSS2.p1.7.m7.1.1.cmml" xref="S4.SS1.SSS2.p1.7.m7.1.1">ùëÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.7.m7.1c">S</annotation></semantics></math> with the end vector <math id="S4.SS1.SSS2.p1.8.m8.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S4.SS1.SSS2.p1.8.m8.1a"><mi id="S4.SS1.SSS2.p1.8.m8.1.1" xref="S4.SS1.SSS2.p1.8.m8.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.8.m8.1b"><ci id="S4.SS1.SSS2.p1.8.m8.1.1.cmml" xref="S4.SS1.SSS2.p1.8.m8.1.1">ùê∏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.8.m8.1c">E</annotation></semantics></math>. Finally, the score of a candidate span from position <math id="S4.SS1.SSS2.p1.9.m9.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS1.SSS2.p1.9.m9.1a"><mi id="S4.SS1.SSS2.p1.9.m9.1.1" xref="S4.SS1.SSS2.p1.9.m9.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.9.m9.1b"><ci id="S4.SS1.SSS2.p1.9.m9.1.1.cmml" xref="S4.SS1.SSS2.p1.9.m9.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.9.m9.1c">i</annotation></semantics></math> to position <math id="S4.SS1.SSS2.p1.10.m10.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.SS1.SSS2.p1.10.m10.1a"><mi id="S4.SS1.SSS2.p1.10.m10.1.1" xref="S4.SS1.SSS2.p1.10.m10.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.10.m10.1b"><ci id="S4.SS1.SSS2.p1.10.m10.1.1.cmml" xref="S4.SS1.SSS2.p1.10.m10.1.1">ùëó</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.10.m10.1c">j</annotation></semantics></math> is defined as <math id="S4.SS1.SSS2.p1.11.m11.1" class="ltx_Math" alttext="S\cdot T_{i}+E\cdot T_{j}" display="inline"><semantics id="S4.SS1.SSS2.p1.11.m11.1a"><mrow id="S4.SS1.SSS2.p1.11.m11.1.1" xref="S4.SS1.SSS2.p1.11.m11.1.1.cmml"><mrow id="S4.SS1.SSS2.p1.11.m11.1.1.2" xref="S4.SS1.SSS2.p1.11.m11.1.1.2.cmml"><mi id="S4.SS1.SSS2.p1.11.m11.1.1.2.2" xref="S4.SS1.SSS2.p1.11.m11.1.1.2.2.cmml">S</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p1.11.m11.1.1.2.1" xref="S4.SS1.SSS2.p1.11.m11.1.1.2.1.cmml">‚ãÖ</mo><msub id="S4.SS1.SSS2.p1.11.m11.1.1.2.3" xref="S4.SS1.SSS2.p1.11.m11.1.1.2.3.cmml"><mi id="S4.SS1.SSS2.p1.11.m11.1.1.2.3.2" xref="S4.SS1.SSS2.p1.11.m11.1.1.2.3.2.cmml">T</mi><mi id="S4.SS1.SSS2.p1.11.m11.1.1.2.3.3" xref="S4.SS1.SSS2.p1.11.m11.1.1.2.3.3.cmml">i</mi></msub></mrow><mo id="S4.SS1.SSS2.p1.11.m11.1.1.1" xref="S4.SS1.SSS2.p1.11.m11.1.1.1.cmml">+</mo><mrow id="S4.SS1.SSS2.p1.11.m11.1.1.3" xref="S4.SS1.SSS2.p1.11.m11.1.1.3.cmml"><mi id="S4.SS1.SSS2.p1.11.m11.1.1.3.2" xref="S4.SS1.SSS2.p1.11.m11.1.1.3.2.cmml">E</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p1.11.m11.1.1.3.1" xref="S4.SS1.SSS2.p1.11.m11.1.1.3.1.cmml">‚ãÖ</mo><msub id="S4.SS1.SSS2.p1.11.m11.1.1.3.3" xref="S4.SS1.SSS2.p1.11.m11.1.1.3.3.cmml"><mi id="S4.SS1.SSS2.p1.11.m11.1.1.3.3.2" xref="S4.SS1.SSS2.p1.11.m11.1.1.3.3.2.cmml">T</mi><mi id="S4.SS1.SSS2.p1.11.m11.1.1.3.3.3" xref="S4.SS1.SSS2.p1.11.m11.1.1.3.3.3.cmml">j</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.11.m11.1b"><apply id="S4.SS1.SSS2.p1.11.m11.1.1.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1"><plus id="S4.SS1.SSS2.p1.11.m11.1.1.1.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1.1"></plus><apply id="S4.SS1.SSS2.p1.11.m11.1.1.2.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1.2"><ci id="S4.SS1.SSS2.p1.11.m11.1.1.2.1.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1.2.1">‚ãÖ</ci><ci id="S4.SS1.SSS2.p1.11.m11.1.1.2.2.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1.2.2">ùëÜ</ci><apply id="S4.SS1.SSS2.p1.11.m11.1.1.2.3.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1.2.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.11.m11.1.1.2.3.1.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1.2.3">subscript</csymbol><ci id="S4.SS1.SSS2.p1.11.m11.1.1.2.3.2.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1.2.3.2">ùëá</ci><ci id="S4.SS1.SSS2.p1.11.m11.1.1.2.3.3.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1.2.3.3">ùëñ</ci></apply></apply><apply id="S4.SS1.SSS2.p1.11.m11.1.1.3.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1.3"><ci id="S4.SS1.SSS2.p1.11.m11.1.1.3.1.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1.3.1">‚ãÖ</ci><ci id="S4.SS1.SSS2.p1.11.m11.1.1.3.2.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1.3.2">ùê∏</ci><apply id="S4.SS1.SSS2.p1.11.m11.1.1.3.3.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.11.m11.1.1.3.3.1.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1.3.3">subscript</csymbol><ci id="S4.SS1.SSS2.p1.11.m11.1.1.3.3.2.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1.3.3.2">ùëá</ci><ci id="S4.SS1.SSS2.p1.11.m11.1.1.3.3.3.cmml" xref="S4.SS1.SSS2.p1.11.m11.1.1.3.3.3">ùëó</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.11.m11.1c">S\cdot T_{i}+E\cdot T_{j}</annotation></semantics></math>, and the maximum scoring span where <math id="S4.SS1.SSS2.p1.12.m12.1" class="ltx_Math" alttext="j\geq i" display="inline"><semantics id="S4.SS1.SSS2.p1.12.m12.1a"><mrow id="S4.SS1.SSS2.p1.12.m12.1.1" xref="S4.SS1.SSS2.p1.12.m12.1.1.cmml"><mi id="S4.SS1.SSS2.p1.12.m12.1.1.2" xref="S4.SS1.SSS2.p1.12.m12.1.1.2.cmml">j</mi><mo id="S4.SS1.SSS2.p1.12.m12.1.1.1" xref="S4.SS1.SSS2.p1.12.m12.1.1.1.cmml">‚â•</mo><mi id="S4.SS1.SSS2.p1.12.m12.1.1.3" xref="S4.SS1.SSS2.p1.12.m12.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.12.m12.1b"><apply id="S4.SS1.SSS2.p1.12.m12.1.1.cmml" xref="S4.SS1.SSS2.p1.12.m12.1.1"><geq id="S4.SS1.SSS2.p1.12.m12.1.1.1.cmml" xref="S4.SS1.SSS2.p1.12.m12.1.1.1"></geq><ci id="S4.SS1.SSS2.p1.12.m12.1.1.2.cmml" xref="S4.SS1.SSS2.p1.12.m12.1.1.2">ùëó</ci><ci id="S4.SS1.SSS2.p1.12.m12.1.1.3.cmml" xref="S4.SS1.SSS2.p1.12.m12.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.12.m12.1c">j\geq i</annotation></semantics></math> is used as a prediction.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Database approach</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The objective of proposing this baseline is to showcase which is the performance of an ad-hoc method using heuristics and commercial software to achieve the best possible performance. Since obtaining a human performance analysis is near impossible
because it would mean that the people involved in the experiment should check more than <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="14k" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">14</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">14</cn><ci id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">14k</annotation></semantics></math> documents for each question, we see this baseline as a performance to beat in a medium-long term.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">This approach also breakdown the task in the same retrieval and answering stages. However, in this case the ranking of the results is binary rather indicating if a document is relevant or not. For that, we first run a commercial OCR over the document collection, extracting not only the recognized text, but also the key-value relationship between the field names and their values, including checkboxes. This is followed by a process to correct possible OCR recognition errors for the fields with low variability (field names, parties and reporting options) and normalize all the dates by parsing them to the same format. Finally, we map the key-value pairs into a database like data structure. At the time of answering a question, the fields in the query are compared with those in the stored records. If all the constraints are met, that document is considered relevant and is given a confidence of <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mn id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><cn type="integer" id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">1</annotation></semantics></math>, while otherwise it is given a confidence of <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mn id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><cn type="integer" id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">0</cn></annotation-xml></semantics></math>. Finally, the requested value in the question is extracted from the records of the relevant documents.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">It is very important to consider two relevant aspects on this baseline. First, it is a very rigid method that does not allow any modification in the data and therefore, is not generalizable at all. Moreover, it requires a preprocessing that is currently done manually to parse the query from Natural Language to a Structured Query Language (SQL).</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Evidences</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">To initially assess the retrieval performance of the methods, we first compare two different commercial OCR systems that we are going to use for text spotting, Google OCR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and Amazon Textract¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. As reported in table <a href="#S5.T3" title="Table 3 ‚Ä£ 5.1 Evidences ‚Ä£ 5 Results ‚Ä£ Document Collection Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> the performance on text spotting with the latter OCR is better than Google OCR, and is the only one capable of extracting the key-value relations for the database approach. For this reason we use this as the standard OCR for the rest of the text spotting baselines.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Retrieval method</span></th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">MAP</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<th id="S5.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Text spotting (google)</th>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">71.62</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<th id="S5.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Text spotting (textract)</th>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_center">72.84</td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<th id="S5.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Database</th>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">71.06</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance of different retrieval methods.</figcaption>
</figure>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.2" class="ltx_p">Compared to text spotting, the database retrieval average performance is similar. However, as depicted in figure¬†<a href="#S5.F3" title="Figure 3 ‚Ä£ 5.1 Evidences ‚Ä£ 5 Results ‚Ä£ Document Collection Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> we can appreciate that performs better for all the questions but the number <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="11" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mn id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">11</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><cn type="integer" id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">11</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">11</annotation></semantics></math> where it gets a MAP of <math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mn id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><cn type="integer" id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">0</cn></annotation-xml></semantics></math>. This is the result from the fact that the key-value pair extractor is not able to capture the relation between some of the forms fields, in this case the treasurer name, and consequently it catastrophically fails at retrieving documents with specific values on those fields, one of the main drawbacks of such rigid methods. On the other hand, the questions where the database approach shows a greater performance gap are those where in order to find the relevant documents the methods must search not only documents with a particular value, but understand more complex constraints such as the ones described in section¬†<a href="#S3.SS1.SSS2" title="3.1.2 Questions and Answers: ‚Ä£ 3.1 Data Collection ‚Ä£ 3 DocCVQA Dataset ‚Ä£ Document Collection Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.2</span></a>, which are finding documents between two dates (question 9), after a date (question 18), documents that do not contain a particular value (question 12), or where several values are considered as correct (question 14).</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2104.14336/assets/images/plots/Retrieval_results_by_Q_v2.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="240" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Evidence retrieval performance of the different methods reported by each question in the test set.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Answers</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.5" class="ltx_p">For the BERT QA method we use the pretrained weights bert-large-uncased-whole-wordmasking-finetuned-squad from the Transformers library¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. This is a pretrained model finetuned on SQuAD 1.1 question answering task consisting on more than <math id="S5.SS2.p1.1.m1.2" class="ltx_Math" alttext="100,000" display="inline"><semantics id="S5.SS2.p1.1.m1.2a"><mrow id="S5.SS2.p1.1.m1.2.3.2" xref="S5.SS2.p1.1.m1.2.3.1.cmml"><mn id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">100</mn><mo id="S5.SS2.p1.1.m1.2.3.2.1" xref="S5.SS2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.SS2.p1.1.m1.2.2" xref="S5.SS2.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.2b"><list id="S5.SS2.p1.1.m1.2.3.1.cmml" xref="S5.SS2.p1.1.m1.2.3.2"><cn type="integer" id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">100</cn><cn type="integer" id="S5.SS2.p1.1.m1.2.2.cmml" xref="S5.SS2.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.2c">100,000</annotation></semantics></math> questions over <math id="S5.SS2.p1.2.m2.2" class="ltx_Math" alttext="23,215" display="inline"><semantics id="S5.SS2.p1.2.m2.2a"><mrow id="S5.SS2.p1.2.m2.2.3.2" xref="S5.SS2.p1.2.m2.2.3.1.cmml"><mn id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">23</mn><mo id="S5.SS2.p1.2.m2.2.3.2.1" xref="S5.SS2.p1.2.m2.2.3.1.cmml">,</mo><mn id="S5.SS2.p1.2.m2.2.2" xref="S5.SS2.p1.2.m2.2.2.cmml">215</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.2b"><list id="S5.SS2.p1.2.m2.2.3.1.cmml" xref="S5.SS2.p1.2.m2.2.3.2"><cn type="integer" id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">23</cn><cn type="integer" id="S5.SS2.p1.2.m2.2.2.cmml" xref="S5.SS2.p1.2.m2.2.2">215</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.2c">23,215</annotation></semantics></math> paragraphs. Then, we finetune it again on the DocVQA dataset for 2 epochs following¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> to teach the model reason about document concepts as well as adapting the new context style format. Finally, we perform a third finetunning phase on the DocCVQA sample set for <math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><mn id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><cn type="integer" id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">6</annotation></semantics></math> epochs. Notice that the sample set is specially small and during these <math id="S5.SS2.p1.4.m4.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S5.SS2.p1.4.m4.1a"><mn id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><cn type="integer" id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">6</annotation></semantics></math> epochs the model only see around <math id="S5.SS2.p1.5.m5.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S5.SS2.p1.5.m5.1a"><mn id="S5.SS2.p1.5.m5.1.1" xref="S5.SS2.p1.5.m5.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.5.m5.1b"><cn type="integer" id="S5.SS2.p1.5.m5.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.5.m5.1c">80</annotation></semantics></math> samples. Nonetheless, this is sufficient to improve the answering performance without harming the previous knowledge.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Given the collection nature of DocCVQA, the answer to the question usually consists on a list of texts found in different documents considered as relevants. In our case, we consider a document as relevant when the confidence provided for the retrieval method on that document is greater than a threshold. For the text spotting methods we have fixed the threshold through an empirical study where we have found that the best threshold is 0.9. In the case of the database approach, given that the confidence provided is either 0 or 1, we consider relevant all positive documents. 
<br class="ltx_break"></p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">In the experiments we use the BERT answering baseline to answer the questions over the ranked documents from the text spotting and the database retrieval methods. But we only use the database method to answer the ranked documents from the same retrieval approach. As reported in table¬†<a href="#S5.T4" title="Table 4 ‚Ä£ 5.2 Answers ‚Ä£ 5 Results ‚Ä£ Document Collection Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> the latter is the one that performs the best. The main reason for this is that the wrong retrieval of the documents prevents the answering methods to find the necessary information to provide the correct answers. Nevertheless, the fact of having the key-value relations allows the database method to directly output the value for the requested field as an answer while BERT needs to learn to extract it from a context that has partially lost the spatial information of the recognized text when at the time of being created, the value of a field might not be close to the field name, losing the semantic connection between the key-value pair. To showcase the answering performance upper bounds of the answering methods we also provide their performance regardless of the retrieval system, where the documents are ranked according to the test ground truth.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S5.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Retrieval</span></th>
<th id="S5.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S5.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Answering</span></th>
<th id="S5.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;" rowspan="2"><span id="S5.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">MAP</span></th>
<th id="S5.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:8.0pt;padding-right:8.0pt;" rowspan="2"><span id="S5.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">ANLSL</span></th>
</tr>
<tr id="S5.T4.1.2.2" class="ltx_tr">
<th id="S5.T4.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S5.T4.1.2.2.1.1" class="ltx_text ltx_font_bold">method</span></th>
<th id="S5.T4.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;"><span id="S5.T4.1.2.2.2.1" class="ltx_text ltx_font_bold">method</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.3.1" class="ltx_tr">
<th id="S5.T4.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">Text spotting</th>
<th id="S5.T4.1.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">BERT</th>
<td id="S5.T4.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">72.84</td>
<td id="S5.T4.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:8.0pt;padding-right:8.0pt;">0.4513</td>
</tr>
<tr id="S5.T4.1.4.2" class="ltx_tr">
<th id="S5.T4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Database</th>
<th id="S5.T4.1.4.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">BERT</th>
<td id="S5.T4.1.4.2.3" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">71.06</td>
<td id="S5.T4.1.4.2.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.5411</td>
</tr>
<tr id="S5.T4.1.5.3" class="ltx_tr">
<th id="S5.T4.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Database</th>
<th id="S5.T4.1.5.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">Database</th>
<td id="S5.T4.1.5.3.3" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">71.06</td>
<td id="S5.T4.1.5.3.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.7068</td>
</tr>
<tr id="S5.T4.1.6.4" class="ltx_tr">
<th id="S5.T4.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">GT</th>
<th id="S5.T4.1.6.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:8.0pt;padding-right:8.0pt;">BERT</th>
<td id="S5.T4.1.6.4.3" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">100.00</td>
<td id="S5.T4.1.6.4.4" class="ltx_td ltx_align_center" style="padding-left:8.0pt;padding-right:8.0pt;">0.5818</td>
</tr>
<tr id="S5.T4.1.7.5" class="ltx_tr">
<th id="S5.T4.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">GT</th>
<th id="S5.T4.1.7.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">Database</th>
<td id="S5.T4.1.7.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">100.00</td>
<td id="S5.T4.1.7.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:8.0pt;padding-right:8.0pt;">0.8473</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Baselines results comparison.</figcaption>
</figure>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">As depicted in figure¬†<a href="#S5.F4" title="Figure 4 ‚Ä£ 5.2 Answers ‚Ä£ 5 Results ‚Ä£ Document Collection Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, BERT does not perform well when the answer are candidate‚Äôs names (questions 8, 9, 11, 14 and 18). However, it has a better performance when asking about dates (questions 16 and 17) or legislative counties (question 10). On the other hand, the database approach is able to provide the required answer, usually depending solely on whether the text and the key-value relationships have been correctly recognized.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">The most interesting question is the number 13, where none of the methods are able to answer the question regardless of a correct retrieval. This question asks if a candidate selected a specific checkbox value. The difference here is that the answer is <em id="S5.SS2.p5.1.1" class="ltx_emph ltx_font_italic">No</em>, in contrast to the sample question number 3. Then, BERT can‚Äôt answer because it lacks of a document collection point of view, and moreover, since it is an extractive QA method, it would require to have a <em id="S5.SS2.p5.1.2" class="ltx_emph ltx_font_italic">No</em> in the document surrounded with some context that could help to identify that word as an answer. On the other hand, the database method fails because of its logical structure. If there is a relevant document for that question, it will find the field for which the query is asking for, or will answer ‚Äô<em id="S5.SS2.p5.1.3" class="ltx_emph ltx_font_italic">Yes</em>‚Äô if the question is a Yes/No type.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2104.14336/assets/images/plots/Answering_results_by_Q_extended_v2.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="214" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Answering performance of the different methods reported by each question in the test set.</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions and Future Work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This work introduces a new and challenging task to both the VQA and DAR research fields. We presented the DocCVQA that aims to provide a new perspective to Document understanding and highlight the importance and difficulty of contemplating a whole collection of documents. We have shown the performance of two different approaches. On one hand, a text spotting with an extractive QA baseline that, although it has lower generic performance it is more generic and could achieve similar performance in other types of documents. And on the other hand, a baseline that represents the documents by their key-value relations that despite achieving quite good performance, is still far from being perfect and because of its design is very limited and can‚Äôt generalize at all when processing other types of documents. In this regard, we believe that the next steps are to propose a method that can reason about the whole collection in a single stage, being able to provide the answer and the positive evidences.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work has been supported by the UAB PIF scholarship B18P0070 and the Consolidated Research Group 2017-SGR-1783 from the Research and University Department of the Catalan Government.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:80%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:80%;">
Almaz√°n, J., Gordo, A., Forn√©s, A., Valveny, E.: Word spotting and
recognition with embedded attributes. IEEE transactions on pattern analysis
and machine intelligence </span><span id="bib.bib1.2.2" class="ltx_text ltx_font_bold" style="font-size:80%;">36</span><span id="bib.bib1.3.3" class="ltx_text" style="font-size:80%;">(12), 2552‚Äì2566 (2014)
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:80%;">
Amazon: Amazon textract (2021), </span><a target="_blank" href="https://aws.amazon.com/es/textract/" title="" class="ltx_ref ltx_url" style="font-size:80%;">https://aws.amazon.com/es/textract/</a><span id="bib.bib2.2.2" class="ltx_text" style="font-size:80%;">,
accessed on Jan 11, 2021
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:80%;">
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh,
D.: Vqa: Visual question answering. In: Proceedings of the IEEE international
conference on computer vision. pp. 2425‚Äì2433 (2015)
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:80%;">
Bansal, A., Zhang, Y., Chellappa, R.: Visual question answering on image sets.
In: European Conference on Computer Vision. pp. 51‚Äì67. Springer (2020)
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:80%;">
Biten, A.F., Tito, R., Mafla, A., Gomez, L., Rusinol, M., Mathew, M., Jawahar,
C., Valveny, E., Karatzas, D.: Icdar 2019 competition on scene text visual
question answering. In: 2019 International Conference on Document Analysis
and Recognition (ICDAR). pp. 1563‚Äì1570. IEEE (2019)
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:80%;">
Biten, A.F., Tito, R., Mafla, A., Gomez, L., Rusinol, M., Valveny, E., Jawahar,
C., Karatzas, D.: Scene text visual question answering. In: Proceedings of
the IEEE/CVF International Conference on Computer Vision. pp. 4291‚Äì4301
(2019)
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:80%;">
Bojanowski, P., Grave, E., Joulin, A., Mikolov, T.: Enriching word vectors with
subword information. Transactions of the Association for Computational
Linguistics </span><span id="bib.bib7.2.2" class="ltx_text ltx_font_bold" style="font-size:80%;">5</span><span id="bib.bib7.3.3" class="ltx_text" style="font-size:80%;">, 135‚Äì146 (2017)
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:80%;">
Co√ºasnon, B., Lemaitre, A.: Recognition of tables and forms (2014)
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:80%;">
Dengel, A.R., Klein, B.: smartfix: A requirements-driven system for document
analysis and understanding. In: Lopresti, D., Hu, J., Kashi, R. (eds.)
Document Analysis Systems V. pp. 433‚Äì444. Springer Berlin Heidelberg (2002)
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:80%;">
Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep
bidirectional transformers for language understanding. ACL (2019)
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:80%;">
Fan, C., Zhang, X., Zhang, S., Wang, W., Zhang, C., Huang, H.: Heterogeneous
memory enhanced multimodal attention model for video question answering. In:
Proceedings of the IEEE/CVF Conference on CVPR. pp. 1999‚Äì2007 (2019)
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:80%;">
Google: Google ocr (2020),
</span><a target="_blank" href="https://cloud.google.com/solutions/document-ai" title="" class="ltx_ref ltx_url" style="font-size:80%;">https://cloud.google.com/solutions/document-ai</a><span id="bib.bib12.2.2" class="ltx_text" style="font-size:80%;">, accessed on Dec 10,
2020
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:80%;">
Hu, R., Singh, A., Darrell, T., Rohrbach, M.: Iterative answer prediction with
pointer-augmented multimodal transformers for textvqa. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020)
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:80%;">
Hull, J.J.: A database for handwritten text recognition research. IEEE
Transactions on pattern analysis and machine intelligence </span><span id="bib.bib14.2.2" class="ltx_text ltx_font_bold" style="font-size:80%;">16</span><span id="bib.bib14.3.3" class="ltx_text" style="font-size:80%;">(5),
550‚Äì554 (1994)
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:80%;">
Kafle, K., Price, B., Cohen, S., Kanan, C.: Dvqa: Understanding data
visualizations via question answering. In: Proceedings of the IEEE conference
on computer vision and pattern recognition. pp. 5648‚Äì5656 (2018)
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:80%;">
Kahou, S.E., Michalski, V., Atkinson, A., K√°d√°r, √Å., Trischler, A.,
Bengio, Y.: Figureqa: An annotated figure dataset for visual reasoning. arXiv
preprint arXiv:1710.07300 (2017)
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:80%;">
Krishnan, P., Dutta, K., Jawahar, C.V.: Word spotting and recognition
using deep embedding. In: 2018 13th IAPR International Workshop on Document
Analysis Systems (DAS). pp.¬†1‚Äì6 (2018)
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:80%;">
Levenshtein, V.I.: Binary codes capable of correcting deletions, insertions,
and reversals. In: Soviet physics doklady. pp. 707‚Äì710. Soviet Union (1966)
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:80%;">
Liu, T.Y.: Learning to rank for information retrieval. Foundations and Trends
in Information Retrieval </span><span id="bib.bib19.2.2" class="ltx_text ltx_font_bold" style="font-size:80%;">3</span><span id="bib.bib19.3.3" class="ltx_text" style="font-size:80%;">(3), 225‚Äì331 (2009)
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:80%;">
Liu, X., Gao, F., Zhang, Q., Zhao, H.: Graph convolution for multimodal
information extraction from visually rich documents. In: Proceedings of the
2019 Conference of the North American Chapter on Computational Linguistics.
pp. 32‚Äì39
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:80%;">
Malinowski, M., Fritz, M.: A multi-world approach to question answering about
real-world scenes based on uncertain input. arXiv preprint arXiv:1410.0210
(2014)
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:80%;">
Manmatha, R., Croft, W.: Word spotting: Indexing handwritten archives.
Intelligent Multimedia Information Retrieval Collection pp. 43‚Äì64 (1997)
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:80%;">
Mathew, M., Karatzas, D., Jawahar, C.: Docvqa: A dataset for vqa on document
images. In: Proceedings of the IEEE/CVF WACV. pp. 2200‚Äì2209 (2021)
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:80%;">
Mathew, M., Tito, R., Karatzas, D., Manmatha, R., Jawahar, C.: Document visual
question answering challenge 2020. arXiv e-prints pp. arXiv‚Äì2008 (2020)
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:80%;">
Mouch√®re, H., Viard-Gaudin, C., Zanibbi, R., Garain, U.: Icfhr2016 crohme:
Competition on recognition of online handwritten mathematical expressions.
In: 2016 15th International Conference on Frontiers in Handwriting
Recognition (ICFHR)
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:80%;">
Palm, R.B., Winther, O., Laws, F.: Cloudscan - a configuration-free
invoice analysis system using recurrent neural networks. In: 2017 14th IAPR
International Conference on Document Analysis and Recognition (ICDAR). pp.
406‚Äì413
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:80%;">
Rath, T.M., Manmatha, R.: Word image matching using dynamic time warping. In:
2003 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, 2003. Proceedings. vol.¬†2, pp. II‚ÄìII. IEEE (2003)
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:80%;">
Ren, M., Kiros, R., Zemel, R.: Exploring models and data for image question
answering. arXiv preprint arXiv:1505.02074 (2015)
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:80%;">
Schuster, D., et¬†al.: Intellix ‚Äì end-user trained information extraction for
document archiving. In: 2013 12th ICDAR
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:80%;">
Siegel, N., Horvitz, Z., Levin, R., Divvala, S., Farhadi, A.: Figureseer:
Parsing result-figures in research papers. In: ECCV. pp. 664‚Äì680. Springer
(2016)
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:80%;">
Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D.,
Rohrbach, M.: Towards vqa models that can read. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:80%;">
Sudholt, S., Fink, G.A.: Evaluating word string embeddings and loss
functions for cnn-based word spotting. In: 2017 14th IAPR International
Conference on Document Analysis and Recognition (ICDAR). vol.¬†01, pp.
493‚Äì498 (2017)
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:80%;">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, L., Polosukhin, I.: Attention is all you need. In: NIPS (2017)
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:80%;">
Wilkinson, T., Lindstr√∂m, J., Brun, A.: Neural ctrl-f: Segmentation-free
query-by-string word spotting in handwritten manuscript collections. In: 2017
IEEE International Conference on Computer Vision (ICCV). pp. 4443‚Äì4452
(2017)
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:80%;">
Wolf, T., et¬†al.: Transformers: State-of-the-art natural language processing.
In: Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing: System Demonstrations. Association for Computational
Linguistics
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:80%;">
Xu, Y., Li, M., Cui, L., Huang, S., Wei, F., Zhou, M.: Layoutlm: Pre-training
of text and layout for document image understanding. p. 1192‚Äì1200. KDD ‚Äô20
(2020)
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:80%;">
Zhang, P., et¬†al.: TRIE: End-to-End Text Reading and Information Extraction for
Document Understanding, p. 1413‚Äì1422 (2020)
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2104.14335" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2104.14336" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2104.14336">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2104.14336" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2104.14337" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 06:09:22 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
