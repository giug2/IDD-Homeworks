<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>In Defense of RAG in the Era of Long-Context Language Models</title>
<!--Generated on Tue Sep  3 07:17:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.01666v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#S1" title="In In Defense of RAG in the Era of Long-Context Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#S2" title="In In Defense of RAG in the Era of Long-Context Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#S3" title="In In Defense of RAG in the Era of Long-Context Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Order-Preserve RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#S4" title="In In Defense of RAG in the Era of Long-Context Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#S4.SS1" title="In 4 Experiments â€£ In Defense of RAG in the Era of Long-Context Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#S4.SS2" title="In 4 Experiments â€£ In Defense of RAG in the Era of Long-Context Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Implementation details.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#S4.SS3" title="In 4 Experiments â€£ In Defense of RAG in the Era of Long-Context Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#S4.SS4" title="In 4 Experiments â€£ In Defense of RAG in the Era of Long-Context Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Main Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#S5" title="In In Defense of RAG in the Era of Long-Context Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">In Defense of RAG in the Era of Long-Context Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tan Yu 
<br class="ltx_break"/>NVIDIA 
<br class="ltx_break"/>Santa Clara, California
<br class="ltx_break"/>United States 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">tayu@nvidia.com</span>
<br class="ltx_break"/>&amp;Anbang Xu 
<br class="ltx_break"/>NVIDIA 
<br class="ltx_break"/>Santa Clara, California 
<br class="ltx_break"/>United States 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">anbangx@nvidia.com</span>
<br class="ltx_break"/>&amp;Rama Akkiraju 
<br class="ltx_break"/>NVIDIA 
<br class="ltx_break"/>Santa Clara, California 
<br class="ltx_break"/>United States 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.3.id3">rakkiraju@nvidia.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">Overcoming the limited context limitations in early-generation LLMs, retrieval-augmented generation (RAG) has been a reliable solution for context-based answer generation in the past. Recently, the emergence of long-context LLMs allows the models to incorporate much longer text sequences, making RAG less attractive. Recent studies show that long-context LLMs significantly outperform RAG in
long-context applications. Unlike the existing works favoring the long-context LLM over RAG, we argue that the extremely long context in LLMs suffers from a diminished focus on relevant information and leads to potential degradation in answer quality. This paper revisits the RAG in long-context answer generation. We propose an order-preserve retrieval-augmented generation (OP-RAG) mechanism, which significantly improves the performance of RAG for long-context question-answer applications. With OP-RAG, as the number of retrieved chunks increases, the answer quality initially rises, and then declines, forming an inverted U-shaped curve. There exist sweet points where OP-RAG could achieve higher answer quality with much less tokens than long-context LLM taking the whole context as input. Extensive experiments on public benchmark demonstrate the superiority of our OP-RAG.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">In Defense of RAG in the Era of Long-Context Language Models</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Tan Yu</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1">NVIDIA</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1">Santa Clara, California</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.4.1">United States</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.5.5">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.5.5.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.5.5.1.1">tayu@nvidia.com</span></span></span>
</span>
</span></span>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â <span class="ltx_text ltx_inline-block" id="p1.1.2.2" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.2.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.2.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.2.1.1.1.1.1">Anbang Xu</span></span></span>
<span class="ltx_tr" id="p1.1.2.2.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.2.2.1">NVIDIA</span></span>
<span class="ltx_tr" id="p1.1.2.2.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.3.3.1">Santa Clara, California</span></span>
<span class="ltx_tr" id="p1.1.2.2.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.4.4.1">United States</span></span>
<span class="ltx_tr" id="p1.1.2.2.1.5.5">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.5.5.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.2.1.5.5.1.1">anbangx@nvidia.com</span></span></span>
</span>
</span></span>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â <span class="ltx_text ltx_inline-block" id="p1.1.2.3" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.3.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.3.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.3.1.1.1.1.1">Rama Akkiraju</span></span></span>
<span class="ltx_tr" id="p1.1.2.3.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.3.1.2.2.1">NVIDIA</span></span>
<span class="ltx_tr" id="p1.1.2.3.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.3.1.3.3.1">Santa Clara, California</span></span>
<span class="ltx_tr" id="p1.1.2.3.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.3.1.4.4.1">United States</span></span>
<span class="ltx_tr" id="p1.1.2.3.1.5.5">
<span class="ltx_td ltx_align_center" id="p1.1.2.3.1.5.5.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.3.1.5.5.1.1">rakkiraju@nvidia.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Due to the limited context window length (<em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">eg</em>, 4096) of early-generation large language models (LLMs), retrieval augmented generation (RAG)Â <cite class="ltx_cite ltx_citemacro_cite">Guu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib9" title="">2020</a>); Lewis etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib10" title="">2020</a>)</cite> is an indispensable choice to handle a large-scale context corpus. Since the answer quality is heavily dependent on the performance of the retrieval model, a lot of efforts are devoted to improving the retrieval recall/precision when designing the RAG system.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S1.F1.sf1.g1" src="extracted/5829890/figures/Figure_1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>F1 score.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S1.F1.sf2.g1" src="extracted/5829890/figures/Figure_2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Input token count.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparisons between the proposed order-preserve retrieval-augmented generation (OP-RAG) and approaches using long-context LLMs without RAG on En.QA dataset of <math alttext="\infty" class="ltx_Math" display="inline" id="S1.F1.2.m1.1"><semantics id="S1.F1.2.m1.1b"><mi id="S1.F1.2.m1.1.1" mathvariant="normal" xref="S1.F1.2.m1.1.1.cmml">âˆ</mi><annotation-xml encoding="MathML-Content" id="S1.F1.2.m1.1c"><infinity id="S1.F1.2.m1.1.1.cmml" xref="S1.F1.2.m1.1.1"></infinity></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.2.m1.1d">\infty</annotation><annotation encoding="application/x-llamapun" id="S1.F1.2.m1.1e">âˆ</annotation></semantics></math>Bench. Our OP-RAG uses Llama3.1-70B as generator, which significantly outperforms its counterpart using Llama3.1-70B without RAG. </figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Recently, the state-of-art LLMs support much longer context windows. For example, GPT-4OÂ <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib15" title="">2023</a>)</cite>, Claudi-3.5Â <cite class="ltx_cite ltx_citemacro_cite">Anthropic (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib3" title="">2024</a>)</cite>, Llama3.1Â <cite class="ltx_cite ltx_citemacro_cite">Meta (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib13" title="">2024b</a>)</cite>, Phi-3Â <cite class="ltx_cite ltx_citemacro_cite">Abdin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib1" title="">2024</a>)</cite>, and Mistral-Large2Â <cite class="ltx_cite ltx_citemacro_cite">AI (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib2" title="">2024</a>)</cite> all support 128-K context. Gemini-1.5-pro even supports a 1M context window. The recent emergence of long-context LLMs naturally leads to the question: is RAG necessary in the age of long-context LLMs? <cite class="ltx_cite ltx_citemacro_citet">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib11" title="">2024</a>)</cite>
recently systematically compares RAG with long-context (LC) LLMs (w/o RAG) and demonstrates that LC LLMs consistently outperform RAG in terms of answer quality.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we re-examine the effectiveness of RAG in long-context answer generation.
We observe that the order of retrieved chunks in the context of LLM is vital for the answer quality. Different from traditional RAG which places the retrieved chunks in a relevance-descending order, we propose to preserve the order of retrieved chunks in the original text. Our experiments show that the proposed order-preserving mechanism significantly improves the answer quality of RAG.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Meanwhile, using the proposed order-preserve RAG, as the number of retrieved chunks increases, the answer quality initially rises and then declines. This is because, with more retrieved chunks, the model has access to more potentially relevant information, which improves the chances of retrieving the correct context needed to generate a high-quality answer. However, as more chunks are retrieved, the likelihood of introducing irrelevant or distracting information also increases. This excess information can confuse the model, leading to a decline in answer quality. The trade-off, therefore, is between improving recall by retrieving more context and maintaining precision by limiting distractions. The optimal point is where the balance between relevant and irrelevant information maximizes the quality of the answer. Beyond this point, the introduction of too much irrelevant information degrades the modelâ€™s performance. It explains the inferior performance of the approach taking the whole long context as the input of LLM.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.7">Different from the conclusion from <cite class="ltx_cite ltx_citemacro_citet">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib11" title="">2024</a>)</cite>, with the proposed order-preserving mechanism, RAG achieves higher answer quality compared with its counterparts that rely solely on Long-Context LLMs.
As shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#S4.F4.sf1" title="In Figure 4 â€£ 4.1 Datasets. â€£ 4 Experiments â€£ In Defense of RAG in the Era of Long-Context Language Models"><span class="ltx_text ltx_ref_tag">4(a)</span></a>,
On En.QA dataset of <math alttext="\infty" class="ltx_Math" display="inline" id="S1.p5.1.m1.1"><semantics id="S1.p5.1.m1.1a"><mi id="S1.p5.1.m1.1.1" mathvariant="normal" xref="S1.p5.1.m1.1.1.cmml">âˆ</mi><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><infinity id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1"></infinity></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">\infty</annotation><annotation encoding="application/x-llamapun" id="S1.p5.1.m1.1d">âˆ</annotation></semantics></math>BenchÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib23" title="">2024</a>)</cite>, using only <math alttext="16" class="ltx_Math" display="inline" id="S1.p5.2.m2.1"><semantics id="S1.p5.2.m2.1a"><mn id="S1.p5.2.m2.1.1" xref="S1.p5.2.m2.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S1.p5.2.m2.1b"><cn id="S1.p5.2.m2.1.1.cmml" type="integer" xref="S1.p5.2.m2.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.2.m2.1c">16</annotation><annotation encoding="application/x-llamapun" id="S1.p5.2.m2.1d">16</annotation></semantics></math>K retrieved tokens, we achieve <math alttext="44.43" class="ltx_Math" display="inline" id="S1.p5.3.m3.1"><semantics id="S1.p5.3.m3.1a"><mn id="S1.p5.3.m3.1.1" xref="S1.p5.3.m3.1.1.cmml">44.43</mn><annotation-xml encoding="MathML-Content" id="S1.p5.3.m3.1b"><cn id="S1.p5.3.m3.1.1.cmml" type="float" xref="S1.p5.3.m3.1.1">44.43</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.3.m3.1c">44.43</annotation><annotation encoding="application/x-llamapun" id="S1.p5.3.m3.1d">44.43</annotation></semantics></math> F1 score with Llama3.1-70B. In contrast, without RAG, Llama3.1-70B making full use of <math alttext="128" class="ltx_Math" display="inline" id="S1.p5.4.m4.1"><semantics id="S1.p5.4.m4.1a"><mn id="S1.p5.4.m4.1.1" xref="S1.p5.4.m4.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S1.p5.4.m4.1b"><cn id="S1.p5.4.m4.1.1.cmml" type="integer" xref="S1.p5.4.m4.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.4.m4.1c">128</annotation><annotation encoding="application/x-llamapun" id="S1.p5.4.m4.1d">128</annotation></semantics></math>K context only achieves <math alttext="34.32" class="ltx_Math" display="inline" id="S1.p5.5.m5.1"><semantics id="S1.p5.5.m5.1a"><mn id="S1.p5.5.m5.1.1" xref="S1.p5.5.m5.1.1.cmml">34.32</mn><annotation-xml encoding="MathML-Content" id="S1.p5.5.m5.1b"><cn id="S1.p5.5.m5.1.1.cmml" type="float" xref="S1.p5.5.m5.1.1">34.32</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.5.m5.1c">34.32</annotation><annotation encoding="application/x-llamapun" id="S1.p5.5.m5.1d">34.32</annotation></semantics></math> F1 score, GPT-4O achieves only <math alttext="32.36" class="ltx_Math" display="inline" id="S1.p5.6.m6.1"><semantics id="S1.p5.6.m6.1a"><mn id="S1.p5.6.m6.1.1" xref="S1.p5.6.m6.1.1.cmml">32.36</mn><annotation-xml encoding="MathML-Content" id="S1.p5.6.m6.1b"><cn id="S1.p5.6.m6.1.1.cmml" type="float" xref="S1.p5.6.m6.1.1">32.36</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.6.m6.1c">32.36</annotation><annotation encoding="application/x-llamapun" id="S1.p5.6.m6.1d">32.36</annotation></semantics></math> F1 score and Gemini-1.5-Pro obtains only <math alttext="43.08" class="ltx_Math" display="inline" id="S1.p5.7.m7.1"><semantics id="S1.p5.7.m7.1a"><mn id="S1.p5.7.m7.1.1" xref="S1.p5.7.m7.1.1.cmml">43.08</mn><annotation-xml encoding="MathML-Content" id="S1.p5.7.m7.1b"><cn id="S1.p5.7.m7.1.1.cmml" type="float" xref="S1.p5.7.m7.1.1">43.08</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.7.m7.1c">43.08</annotation><annotation encoding="application/x-llamapun" id="S1.p5.7.m7.1d">43.08</annotation></semantics></math> F1 score as evaluated by <cite class="ltx_cite ltx_citemacro_citet">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib11" title="">2024</a>)</cite>. That is, RAG could achieve a higher F1 score even with a significant reduction on input length.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Retrieval-augmented generation.</span> By incorporating the external knowledge as context, retrieval-augmented generation (RAG)Â <cite class="ltx_cite ltx_citemacro_cite">Guu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib9" title="">2020</a>); Lewis etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib10" title="">2020</a>); Mialon etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib14" title="">2023</a>)</cite> allows language model to access up-to-date and specific information, reducing hallucinations and improving factual accuracy. Before the era of long-context LLMs, RAG is a promising solution to overcoming the limitation of short context window.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Long-context LLM.</span>
To support the long sequence of language models, many efforts have been devoted to improving the computing efficiency of self-attentionÂ <cite class="ltx_cite ltx_citemacro_cite">Choromanski etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib6" title="">2020</a>); Zaheer etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib22" title="">2020</a>); Tay etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib19" title="">2020</a>); Dao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib8" title="">2022</a>); Dao (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib7" title="">2024</a>)</cite> and boosting extensibility of positional encodingÂ <cite class="ltx_cite ltx_citemacro_cite">Press etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib16" title="">2021</a>); Sun etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib18" title="">2022</a>); Chen etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib5" title="">2023</a>)</cite>. Recently, the flagship LLMs such as GPT-4OÂ <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib15" title="">2023</a>)</cite>, Gemini-1.5-ProÂ <cite class="ltx_cite ltx_citemacro_cite">Reid etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib17" title="">2024</a>)</cite>, Claudi-3.5Â <cite class="ltx_cite ltx_citemacro_cite">Anthropic (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib3" title="">2024</a>)</cite>, Grok-2Â <cite class="ltx_cite ltx_citemacro_cite">xAI (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib20" title="">2024</a>)</cite>, and Llama3.1Â <cite class="ltx_cite ltx_citemacro_cite">Meta (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib12" title="">2024a</a>)</cite> have supported extremely large context. With the existence of long-context LLMs, RAG is no longer a indispensable module for long-context question-answering task. Recently, <cite class="ltx_cite ltx_citemacro_citet">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib11" title="">2024</a>)</cite> concludes that using long-context without RAG could significantly outperforms RAG. Different from the conclusion fromÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib11" title="">2024</a>)</cite>, in this work, we demonstrate the proposed order-preserve RAG could beat the long-context LLMs without RAG.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="250" id="S2.F2.g1" src="extracted/5829890/figures/op_rag.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Vanilla RAG versus the proposed order-preserve the RAG. As shown in the example, a long document is cropped into <math alttext="13" class="ltx_Math" display="inline" id="S2.F2.3.m1.1"><semantics id="S2.F2.3.m1.1b"><mn id="S2.F2.3.m1.1.1" xref="S2.F2.3.m1.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S2.F2.3.m1.1c"><cn id="S2.F2.3.m1.1.1.cmml" type="integer" xref="S2.F2.3.m1.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.3.m1.1d">13</annotation><annotation encoding="application/x-llamapun" id="S2.F2.3.m1.1e">13</annotation></semantics></math> chunks, <math alttext="\{c_{i}\}_{i=1}^{13}" class="ltx_Math" display="inline" id="S2.F2.4.m2.1"><semantics id="S2.F2.4.m2.1b"><msubsup id="S2.F2.4.m2.1.1" xref="S2.F2.4.m2.1.1.cmml"><mrow id="S2.F2.4.m2.1.1.1.1.1" xref="S2.F2.4.m2.1.1.1.1.2.cmml"><mo id="S2.F2.4.m2.1.1.1.1.1.2" stretchy="false" xref="S2.F2.4.m2.1.1.1.1.2.cmml">{</mo><msub id="S2.F2.4.m2.1.1.1.1.1.1" xref="S2.F2.4.m2.1.1.1.1.1.1.cmml"><mi id="S2.F2.4.m2.1.1.1.1.1.1.2" xref="S2.F2.4.m2.1.1.1.1.1.1.2.cmml">c</mi><mi id="S2.F2.4.m2.1.1.1.1.1.1.3" xref="S2.F2.4.m2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.F2.4.m2.1.1.1.1.1.3" stretchy="false" xref="S2.F2.4.m2.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S2.F2.4.m2.1.1.1.3" xref="S2.F2.4.m2.1.1.1.3.cmml"><mi id="S2.F2.4.m2.1.1.1.3.2" xref="S2.F2.4.m2.1.1.1.3.2.cmml">i</mi><mo id="S2.F2.4.m2.1.1.1.3.1" xref="S2.F2.4.m2.1.1.1.3.1.cmml">=</mo><mn id="S2.F2.4.m2.1.1.1.3.3" xref="S2.F2.4.m2.1.1.1.3.3.cmml">1</mn></mrow><mn id="S2.F2.4.m2.1.1.3" xref="S2.F2.4.m2.1.1.3.cmml">13</mn></msubsup><annotation-xml encoding="MathML-Content" id="S2.F2.4.m2.1c"><apply id="S2.F2.4.m2.1.1.cmml" xref="S2.F2.4.m2.1.1"><csymbol cd="ambiguous" id="S2.F2.4.m2.1.1.2.cmml" xref="S2.F2.4.m2.1.1">superscript</csymbol><apply id="S2.F2.4.m2.1.1.1.cmml" xref="S2.F2.4.m2.1.1"><csymbol cd="ambiguous" id="S2.F2.4.m2.1.1.1.2.cmml" xref="S2.F2.4.m2.1.1">subscript</csymbol><set id="S2.F2.4.m2.1.1.1.1.2.cmml" xref="S2.F2.4.m2.1.1.1.1.1"><apply id="S2.F2.4.m2.1.1.1.1.1.1.cmml" xref="S2.F2.4.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.F2.4.m2.1.1.1.1.1.1.1.cmml" xref="S2.F2.4.m2.1.1.1.1.1.1">subscript</csymbol><ci id="S2.F2.4.m2.1.1.1.1.1.1.2.cmml" xref="S2.F2.4.m2.1.1.1.1.1.1.2">ğ‘</ci><ci id="S2.F2.4.m2.1.1.1.1.1.1.3.cmml" xref="S2.F2.4.m2.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S2.F2.4.m2.1.1.1.3.cmml" xref="S2.F2.4.m2.1.1.1.3"><eq id="S2.F2.4.m2.1.1.1.3.1.cmml" xref="S2.F2.4.m2.1.1.1.3.1"></eq><ci id="S2.F2.4.m2.1.1.1.3.2.cmml" xref="S2.F2.4.m2.1.1.1.3.2">ğ‘–</ci><cn id="S2.F2.4.m2.1.1.1.3.3.cmml" type="integer" xref="S2.F2.4.m2.1.1.1.3.3">1</cn></apply></apply><cn id="S2.F2.4.m2.1.1.3.cmml" type="integer" xref="S2.F2.4.m2.1.1.3">13</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.4.m2.1d">\{c_{i}\}_{i=1}^{13}</annotation><annotation encoding="application/x-llamapun" id="S2.F2.4.m2.1e">{ italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 13 end_POSTSUPERSCRIPT</annotation></semantics></math>. The similarity score is appended to each chunk. We retrieve top 4 chunks with the highest similarity scores.
Vanilla RAG places the chunks in a score-descending order, whereas the proposed order-preserve RAG places the chunks based on the order in the original document.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Order-Preserve RAG</h2>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="537" id="S3.F3.sf1.g1" src="extracted/5829890/figures/context_1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>EN.QA</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="532" id="S3.F3.sf2.g1" src="extracted/5829890/figures/context_2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>EN.MC</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The influence of context length on the performance of RAG. The evaluations are conducted on En.QA and EN.MC datasets of <math alttext="\infty" class="ltx_Math" display="inline" id="S3.F3.2.m1.1"><semantics id="S3.F3.2.m1.1b"><mi id="S3.F3.2.m1.1.1" mathvariant="normal" xref="S3.F3.2.m1.1.1.cmml">âˆ</mi><annotation-xml encoding="MathML-Content" id="S3.F3.2.m1.1c"><infinity id="S3.F3.2.m1.1.1.cmml" xref="S3.F3.2.m1.1.1"></infinity></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.m1.1d">\infty</annotation><annotation encoding="application/x-llamapun" id="S3.F3.2.m1.1e">âˆ</annotation></semantics></math>Bench. </figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.15">Let us denote the long textual context, <em class="ltx_emph ltx_font_italic" id="S3.p1.15.1">e.g.</em>, a long document, by <math alttext="d" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_d</annotation></semantics></math>.
We split <math alttext="d" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_d</annotation></semantics></math> into <math alttext="N" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">italic_N</annotation></semantics></math> chunks sequentially and uniformly, <math alttext="\{c_{i}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><msubsup id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><mrow id="S3.p1.4.m4.1.1.1.1.1" xref="S3.p1.4.m4.1.1.1.1.2.cmml"><mo id="S3.p1.4.m4.1.1.1.1.1.2" stretchy="false" xref="S3.p1.4.m4.1.1.1.1.2.cmml">{</mo><msub id="S3.p1.4.m4.1.1.1.1.1.1" xref="S3.p1.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.p1.4.m4.1.1.1.1.1.1.2" xref="S3.p1.4.m4.1.1.1.1.1.1.2.cmml">c</mi><mi id="S3.p1.4.m4.1.1.1.1.1.1.3" xref="S3.p1.4.m4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.p1.4.m4.1.1.1.1.1.3" stretchy="false" xref="S3.p1.4.m4.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.p1.4.m4.1.1.1.3" xref="S3.p1.4.m4.1.1.1.3.cmml"><mi id="S3.p1.4.m4.1.1.1.3.2" xref="S3.p1.4.m4.1.1.1.3.2.cmml">i</mi><mo id="S3.p1.4.m4.1.1.1.3.1" xref="S3.p1.4.m4.1.1.1.3.1.cmml">=</mo><mn id="S3.p1.4.m4.1.1.1.3.3" xref="S3.p1.4.m4.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1">superscript</csymbol><apply id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.2.cmml" xref="S3.p1.4.m4.1.1">subscript</csymbol><set id="S3.p1.4.m4.1.1.1.1.2.cmml" xref="S3.p1.4.m4.1.1.1.1.1"><apply id="S3.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.2">ğ‘</ci><ci id="S3.p1.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.p1.4.m4.1.1.1.3.cmml" xref="S3.p1.4.m4.1.1.1.3"><eq id="S3.p1.4.m4.1.1.1.3.1.cmml" xref="S3.p1.4.m4.1.1.1.3.1"></eq><ci id="S3.p1.4.m4.1.1.1.3.2.cmml" xref="S3.p1.4.m4.1.1.1.3.2">ğ‘–</ci><cn id="S3.p1.4.m4.1.1.1.3.3.cmml" type="integer" xref="S3.p1.4.m4.1.1.1.3.3">1</cn></apply></apply><ci id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">\{c_{i}\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">{ italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>.
The index <math alttext="i" class="ltx_Math" display="inline" id="S3.p1.5.m5.1"><semantics id="S3.p1.5.m5.1a"><mi id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.p1.5.m5.1d">italic_i</annotation></semantics></math> implies the sequential order of the chunk <math alttext="c_{i}" class="ltx_Math" display="inline" id="S3.p1.6.m6.1"><semantics id="S3.p1.6.m6.1a"><msub id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml"><mi id="S3.p1.6.m6.1.1.2" xref="S3.p1.6.m6.1.1.2.cmml">c</mi><mi id="S3.p1.6.m6.1.1.3" xref="S3.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><apply id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p1.6.m6.1.1.1.cmml" xref="S3.p1.6.m6.1.1">subscript</csymbol><ci id="S3.p1.6.m6.1.1.2.cmml" xref="S3.p1.6.m6.1.1.2">ğ‘</ci><ci id="S3.p1.6.m6.1.1.3.cmml" xref="S3.p1.6.m6.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">c_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.6.m6.1d">italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in <math alttext="d" class="ltx_Math" display="inline" id="S3.p1.7.m7.1"><semantics id="S3.p1.7.m7.1a"><mi id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.1b"><ci id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.p1.7.m7.1d">italic_d</annotation></semantics></math>.
That is, <math alttext="c_{i-1}" class="ltx_Math" display="inline" id="S3.p1.8.m8.1"><semantics id="S3.p1.8.m8.1a"><msub id="S3.p1.8.m8.1.1" xref="S3.p1.8.m8.1.1.cmml"><mi id="S3.p1.8.m8.1.1.2" xref="S3.p1.8.m8.1.1.2.cmml">c</mi><mrow id="S3.p1.8.m8.1.1.3" xref="S3.p1.8.m8.1.1.3.cmml"><mi id="S3.p1.8.m8.1.1.3.2" xref="S3.p1.8.m8.1.1.3.2.cmml">i</mi><mo id="S3.p1.8.m8.1.1.3.1" xref="S3.p1.8.m8.1.1.3.1.cmml">âˆ’</mo><mn id="S3.p1.8.m8.1.1.3.3" xref="S3.p1.8.m8.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p1.8.m8.1b"><apply id="S3.p1.8.m8.1.1.cmml" xref="S3.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p1.8.m8.1.1.1.cmml" xref="S3.p1.8.m8.1.1">subscript</csymbol><ci id="S3.p1.8.m8.1.1.2.cmml" xref="S3.p1.8.m8.1.1.2">ğ‘</ci><apply id="S3.p1.8.m8.1.1.3.cmml" xref="S3.p1.8.m8.1.1.3"><minus id="S3.p1.8.m8.1.1.3.1.cmml" xref="S3.p1.8.m8.1.1.3.1"></minus><ci id="S3.p1.8.m8.1.1.3.2.cmml" xref="S3.p1.8.m8.1.1.3.2">ğ‘–</ci><cn id="S3.p1.8.m8.1.1.3.3.cmml" type="integer" xref="S3.p1.8.m8.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m8.1c">c_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.8.m8.1d">italic_c start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math> denotes the chunk before <math alttext="c_{i}" class="ltx_Math" display="inline" id="S3.p1.9.m9.1"><semantics id="S3.p1.9.m9.1a"><msub id="S3.p1.9.m9.1.1" xref="S3.p1.9.m9.1.1.cmml"><mi id="S3.p1.9.m9.1.1.2" xref="S3.p1.9.m9.1.1.2.cmml">c</mi><mi id="S3.p1.9.m9.1.1.3" xref="S3.p1.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.9.m9.1b"><apply id="S3.p1.9.m9.1.1.cmml" xref="S3.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.p1.9.m9.1.1.1.cmml" xref="S3.p1.9.m9.1.1">subscript</csymbol><ci id="S3.p1.9.m9.1.1.2.cmml" xref="S3.p1.9.m9.1.1.2">ğ‘</ci><ci id="S3.p1.9.m9.1.1.3.cmml" xref="S3.p1.9.m9.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.9.m9.1c">c_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.9.m9.1d">italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> whereas <math alttext="c_{i+1}" class="ltx_Math" display="inline" id="S3.p1.10.m10.1"><semantics id="S3.p1.10.m10.1a"><msub id="S3.p1.10.m10.1.1" xref="S3.p1.10.m10.1.1.cmml"><mi id="S3.p1.10.m10.1.1.2" xref="S3.p1.10.m10.1.1.2.cmml">c</mi><mrow id="S3.p1.10.m10.1.1.3" xref="S3.p1.10.m10.1.1.3.cmml"><mi id="S3.p1.10.m10.1.1.3.2" xref="S3.p1.10.m10.1.1.3.2.cmml">i</mi><mo id="S3.p1.10.m10.1.1.3.1" xref="S3.p1.10.m10.1.1.3.1.cmml">+</mo><mn id="S3.p1.10.m10.1.1.3.3" xref="S3.p1.10.m10.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p1.10.m10.1b"><apply id="S3.p1.10.m10.1.1.cmml" xref="S3.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.p1.10.m10.1.1.1.cmml" xref="S3.p1.10.m10.1.1">subscript</csymbol><ci id="S3.p1.10.m10.1.1.2.cmml" xref="S3.p1.10.m10.1.1.2">ğ‘</ci><apply id="S3.p1.10.m10.1.1.3.cmml" xref="S3.p1.10.m10.1.1.3"><plus id="S3.p1.10.m10.1.1.3.1.cmml" xref="S3.p1.10.m10.1.1.3.1"></plus><ci id="S3.p1.10.m10.1.1.3.2.cmml" xref="S3.p1.10.m10.1.1.3.2">ğ‘–</ci><cn id="S3.p1.10.m10.1.1.3.3.cmml" type="integer" xref="S3.p1.10.m10.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.10.m10.1c">c_{i+1}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.10.m10.1d">italic_c start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT</annotation></semantics></math> denotes the chunk right after <math alttext="c_{i}" class="ltx_Math" display="inline" id="S3.p1.11.m11.1"><semantics id="S3.p1.11.m11.1a"><msub id="S3.p1.11.m11.1.1" xref="S3.p1.11.m11.1.1.cmml"><mi id="S3.p1.11.m11.1.1.2" xref="S3.p1.11.m11.1.1.2.cmml">c</mi><mi id="S3.p1.11.m11.1.1.3" xref="S3.p1.11.m11.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.11.m11.1b"><apply id="S3.p1.11.m11.1.1.cmml" xref="S3.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.p1.11.m11.1.1.1.cmml" xref="S3.p1.11.m11.1.1">subscript</csymbol><ci id="S3.p1.11.m11.1.1.2.cmml" xref="S3.p1.11.m11.1.1.2">ğ‘</ci><ci id="S3.p1.11.m11.1.1.3.cmml" xref="S3.p1.11.m11.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.11.m11.1c">c_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.11.m11.1d">italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. Given a query <math alttext="q" class="ltx_Math" display="inline" id="S3.p1.12.m12.1"><semantics id="S3.p1.12.m12.1a"><mi id="S3.p1.12.m12.1.1" xref="S3.p1.12.m12.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.12.m12.1b"><ci id="S3.p1.12.m12.1.1.cmml" xref="S3.p1.12.m12.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.12.m12.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.p1.12.m12.1d">italic_q</annotation></semantics></math>, we obtain the relevance score of the chunk <math alttext="c_{i}" class="ltx_Math" display="inline" id="S3.p1.13.m13.1"><semantics id="S3.p1.13.m13.1a"><msub id="S3.p1.13.m13.1.1" xref="S3.p1.13.m13.1.1.cmml"><mi id="S3.p1.13.m13.1.1.2" xref="S3.p1.13.m13.1.1.2.cmml">c</mi><mi id="S3.p1.13.m13.1.1.3" xref="S3.p1.13.m13.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.13.m13.1b"><apply id="S3.p1.13.m13.1.1.cmml" xref="S3.p1.13.m13.1.1"><csymbol cd="ambiguous" id="S3.p1.13.m13.1.1.1.cmml" xref="S3.p1.13.m13.1.1">subscript</csymbol><ci id="S3.p1.13.m13.1.1.2.cmml" xref="S3.p1.13.m13.1.1.2">ğ‘</ci><ci id="S3.p1.13.m13.1.1.3.cmml" xref="S3.p1.13.m13.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.13.m13.1c">c_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.13.m13.1d">italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> by computing cosine similarity between the embedding of <math alttext="q" class="ltx_Math" display="inline" id="S3.p1.14.m14.1"><semantics id="S3.p1.14.m14.1a"><mi id="S3.p1.14.m14.1.1" xref="S3.p1.14.m14.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.14.m14.1b"><ci id="S3.p1.14.m14.1.1.cmml" xref="S3.p1.14.m14.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.14.m14.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.p1.14.m14.1d">italic_q</annotation></semantics></math> and that of <math alttext="c_{i}" class="ltx_Math" display="inline" id="S3.p1.15.m15.1"><semantics id="S3.p1.15.m15.1a"><msub id="S3.p1.15.m15.1.1" xref="S3.p1.15.m15.1.1.cmml"><mi id="S3.p1.15.m15.1.1.2" xref="S3.p1.15.m15.1.1.2.cmml">c</mi><mi id="S3.p1.15.m15.1.1.3" xref="S3.p1.15.m15.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.15.m15.1b"><apply id="S3.p1.15.m15.1.1.cmml" xref="S3.p1.15.m15.1.1"><csymbol cd="ambiguous" id="S3.p1.15.m15.1.1.1.cmml" xref="S3.p1.15.m15.1.1">subscript</csymbol><ci id="S3.p1.15.m15.1.1.2.cmml" xref="S3.p1.15.m15.1.1.2">ğ‘</ci><ci id="S3.p1.15.m15.1.1.3.cmml" xref="S3.p1.15.m15.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.15.m15.1c">c_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.15.m15.1d">italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
</div>
<div class="ltx_para" id="S3.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="s_{i}=\mathrm{cos}(\mathrm{emb}(q),\mathrm{emb}(c_{i}))," class="ltx_Math" display="block" id="S3.E1.m1.2"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.4" xref="S3.E1.m1.2.2.1.1.4.cmml"><mi id="S3.E1.m1.2.2.1.1.4.2" xref="S3.E1.m1.2.2.1.1.4.2.cmml">s</mi><mi id="S3.E1.m1.2.2.1.1.4.3" xref="S3.E1.m1.2.2.1.1.4.3.cmml">i</mi></msub><mo id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml"><mi id="S3.E1.m1.2.2.1.1.2.4" xref="S3.E1.m1.2.2.1.1.2.4.cmml">cos</mi><mo id="S3.E1.m1.2.2.1.1.2.3" xref="S3.E1.m1.2.2.1.1.2.3.cmml">â¢</mo><mrow id="S3.E1.m1.2.2.1.1.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml"><mo id="S3.E1.m1.2.2.1.1.2.2.2.3" stretchy="false" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml">emb</mi><mo id="S3.E1.m1.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml">â¢</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.2.2.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">q</mi><mo id="S3.E1.m1.2.2.1.1.1.1.1.1.3.2.2" stretchy="false" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.1.1.2.2.2.4" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml">,</mo><mrow id="S3.E1.m1.2.2.1.1.2.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.1.1.2.2.2.2.3" xref="S3.E1.m1.2.2.1.1.2.2.2.2.3.cmml">emb</mi><mo id="S3.E1.m1.2.2.1.1.2.2.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.2.2.cmml">â¢</mo><mrow id="S3.E1.m1.2.2.1.1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.1.cmml"><mo id="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.2" stretchy="false" xref="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.1.cmml">(</mo><msub id="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.1.2.cmml">c</mi><mi id="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.3" stretchy="false" xref="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.1.1.2.2.2.5" stretchy="false" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3"></eq><apply id="S3.E1.m1.2.2.1.1.4.cmml" xref="S3.E1.m1.2.2.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.4.1.cmml" xref="S3.E1.m1.2.2.1.1.4">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.4.2.cmml" xref="S3.E1.m1.2.2.1.1.4.2">ğ‘ </ci><ci id="S3.E1.m1.2.2.1.1.4.3.cmml" xref="S3.E1.m1.2.2.1.1.4.3">ğ‘–</ci></apply><apply id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"><times id="S3.E1.m1.2.2.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.3"></times><ci id="S3.E1.m1.2.2.1.1.2.4.cmml" xref="S3.E1.m1.2.2.1.1.2.4">cos</ci><interval closure="open" id="S3.E1.m1.2.2.1.1.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2"><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1"></times><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2">emb</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ğ‘</ci></apply><apply id="S3.E1.m1.2.2.1.1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2"><times id="S3.E1.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2.2"></times><ci id="S3.E1.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2.3">emb</ci><apply id="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.1.2">ğ‘</ci><ci id="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2.1.1.1.3">ğ‘–</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">s_{i}=\mathrm{cos}(\mathrm{emb}(q),\mathrm{emb}(c_{i})),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.2d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_cos ( roman_emb ( italic_q ) , roman_emb ( italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p2.2">where <math alttext="\mathrm{cos}(\cdot,\cdot)" class="ltx_Math" display="inline" id="S3.p2.1.m1.2"><semantics id="S3.p2.1.m1.2a"><mrow id="S3.p2.1.m1.2.3" xref="S3.p2.1.m1.2.3.cmml"><mi id="S3.p2.1.m1.2.3.2" xref="S3.p2.1.m1.2.3.2.cmml">cos</mi><mo id="S3.p2.1.m1.2.3.1" xref="S3.p2.1.m1.2.3.1.cmml">â¢</mo><mrow id="S3.p2.1.m1.2.3.3.2" xref="S3.p2.1.m1.2.3.3.1.cmml"><mo id="S3.p2.1.m1.2.3.3.2.1" stretchy="false" xref="S3.p2.1.m1.2.3.3.1.cmml">(</mo><mo id="S3.p2.1.m1.1.1" lspace="0em" rspace="0em" xref="S3.p2.1.m1.1.1.cmml">â‹…</mo><mo id="S3.p2.1.m1.2.3.3.2.2" rspace="0em" xref="S3.p2.1.m1.2.3.3.1.cmml">,</mo><mo id="S3.p2.1.m1.2.2" lspace="0em" rspace="0em" xref="S3.p2.1.m1.2.2.cmml">â‹…</mo><mo id="S3.p2.1.m1.2.3.3.2.3" stretchy="false" xref="S3.p2.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.2b"><apply id="S3.p2.1.m1.2.3.cmml" xref="S3.p2.1.m1.2.3"><times id="S3.p2.1.m1.2.3.1.cmml" xref="S3.p2.1.m1.2.3.1"></times><ci id="S3.p2.1.m1.2.3.2.cmml" xref="S3.p2.1.m1.2.3.2">cos</ci><interval closure="open" id="S3.p2.1.m1.2.3.3.1.cmml" xref="S3.p2.1.m1.2.3.3.2"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">â‹…</ci><ci id="S3.p2.1.m1.2.2.cmml" xref="S3.p2.1.m1.2.2">â‹…</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.2c">\mathrm{cos}(\cdot,\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.2d">roman_cos ( â‹… , â‹… )</annotation></semantics></math> denotes the cosine similarity function and <math alttext="\mathrm{emb}(\cdot)" class="ltx_Math" display="inline" id="S3.p2.2.m2.1"><semantics id="S3.p2.2.m2.1a"><mrow id="S3.p2.2.m2.1.2" xref="S3.p2.2.m2.1.2.cmml"><mi id="S3.p2.2.m2.1.2.2" xref="S3.p2.2.m2.1.2.2.cmml">emb</mi><mo id="S3.p2.2.m2.1.2.1" xref="S3.p2.2.m2.1.2.1.cmml">â¢</mo><mrow id="S3.p2.2.m2.1.2.3.2" xref="S3.p2.2.m2.1.2.cmml"><mo id="S3.p2.2.m2.1.2.3.2.1" stretchy="false" xref="S3.p2.2.m2.1.2.cmml">(</mo><mo id="S3.p2.2.m2.1.1" lspace="0em" rspace="0em" xref="S3.p2.2.m2.1.1.cmml">â‹…</mo><mo id="S3.p2.2.m2.1.2.3.2.2" stretchy="false" xref="S3.p2.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.2.cmml" xref="S3.p2.2.m2.1.2"><times id="S3.p2.2.m2.1.2.1.cmml" xref="S3.p2.2.m2.1.2.1"></times><ci id="S3.p2.2.m2.1.2.2.cmml" xref="S3.p2.2.m2.1.2.2">emb</ci><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">\mathrm{emb}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.1d">roman_emb ( â‹… )</annotation></semantics></math> denotes the embedding function.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.2">We retrieve the top k chunks with the highest similarity scores with the query and
denote the indices of top k chunks by <math alttext="\mathcal{J}=\{j_{i}\}_{i=1}^{k}" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><mrow id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">ğ’¥</mi><mo id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">=</mo><msubsup id="S3.p3.1.m1.1.1.1" xref="S3.p3.1.m1.1.1.1.cmml"><mrow id="S3.p3.1.m1.1.1.1.1.1.1" xref="S3.p3.1.m1.1.1.1.1.1.2.cmml"><mo id="S3.p3.1.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.p3.1.m1.1.1.1.1.1.2.cmml">{</mo><msub id="S3.p3.1.m1.1.1.1.1.1.1.1" xref="S3.p3.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.p3.1.m1.1.1.1.1.1.1.1.2" xref="S3.p3.1.m1.1.1.1.1.1.1.1.2.cmml">j</mi><mi id="S3.p3.1.m1.1.1.1.1.1.1.1.3" xref="S3.p3.1.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.p3.1.m1.1.1.1.1.1.1.3" stretchy="false" xref="S3.p3.1.m1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.p3.1.m1.1.1.1.1.3" xref="S3.p3.1.m1.1.1.1.1.3.cmml"><mi id="S3.p3.1.m1.1.1.1.1.3.2" xref="S3.p3.1.m1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.p3.1.m1.1.1.1.1.3.1" xref="S3.p3.1.m1.1.1.1.1.3.1.cmml">=</mo><mn id="S3.p3.1.m1.1.1.1.1.3.3" xref="S3.p3.1.m1.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.p3.1.m1.1.1.1.3" xref="S3.p3.1.m1.1.1.1.3.cmml">k</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><eq id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2"></eq><ci id="S3.p3.1.m1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.3">ğ’¥</ci><apply id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.1">superscript</csymbol><apply id="S3.p3.1.m1.1.1.1.1.cmml" xref="S3.p3.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.1">subscript</csymbol><set id="S3.p3.1.m1.1.1.1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.1.1.1.1"><apply id="S3.p3.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.p3.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.p3.1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p3.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.1.1.1.1.1.2">ğ‘—</ci><ci id="S3.p3.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S3.p3.1.m1.1.1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.1.1.3"><eq id="S3.p3.1.m1.1.1.1.1.3.1.cmml" xref="S3.p3.1.m1.1.1.1.1.3.1"></eq><ci id="S3.p3.1.m1.1.1.1.1.3.2.cmml" xref="S3.p3.1.m1.1.1.1.1.3.2">ğ‘–</ci><cn id="S3.p3.1.m1.1.1.1.1.3.3.cmml" type="integer" xref="S3.p3.1.m1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.p3.1.m1.1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.1.3">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">\mathcal{J}=\{j_{i}\}_{i=1}^{k}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">caligraphic_J = { italic_j start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>. We preserve the order of chunks in the original long context <math alttext="d" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">italic_d</annotation></semantics></math>, that is, we constrain</p>
</div>
<div class="ltx_para" id="S3.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="j_{l}&gt;j_{m}\iff l&gt;m." class="ltx_Math" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml"><msub id="S3.E2.m1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.cmml">j</mi><mi id="S3.E2.m1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml">l</mi></msub><mo id="S3.E2.m1.1.1.1.1.2.1" xref="S3.E2.m1.1.1.1.1.2.1.cmml">&gt;</mo><msub id="S3.E2.m1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.2.3.2" xref="S3.E2.m1.1.1.1.1.2.3.2.cmml">j</mi><mi id="S3.E2.m1.1.1.1.1.2.3.3" xref="S3.E2.m1.1.1.1.1.2.3.3.cmml">m</mi></msub></mrow><mo id="S3.E2.m1.1.1.1.1.1" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.cmml">â‡”</mo><mrow id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">l</mi><mo id="S3.E2.m1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.3.1.cmml">&gt;</mo><mi id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml">m</mi></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" lspace="0em" xref="S3.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1">iff</csymbol><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><gt id="S3.E2.m1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.1"></gt><apply id="S3.E2.m1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2">ğ‘—</ci><ci id="S3.E2.m1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3">ğ‘™</ci></apply><apply id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.2.3.2">ğ‘—</ci><ci id="S3.E2.m1.1.1.1.1.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3.3">ğ‘š</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><gt id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1"></gt><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">ğ‘™</ci><ci id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3">ğ‘š</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">j_{l}&gt;j_{m}\iff l&gt;m.</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_j start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT &gt; italic_j start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT â‡” italic_l &gt; italic_m .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p4.1">FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#S2.F2" title="Figure 2 â€£ 2 Related Work â€£ In Defense of RAG in the Era of Long-Context Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> visualizes the difference between the vanilla RAG and the proposed order-preserve RAG. Different from vanilla RAG placing the chunks in the order of similarity descending, the proposed order-preserve RAG keep the order of chunks in the original document.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets.</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We conduct experiments on EN.QA and EN.MC datasets of <math alttext="\infty" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" mathvariant="normal" xref="S4.SS1.p1.1.m1.1.1.cmml">âˆ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><infinity id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"></infinity></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\infty</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">âˆ</annotation></semantics></math>BenchÂ <cite class="ltx_cite ltx_citemacro_cite">Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib23" title="">2024</a>)</cite> benchmark, specially designed for long-context QA evaluation. To be specific, En.QA consists of 351 human-annotated question-answer pairs. On average, the long context in En.QA contains 150,374 words.
We use F1-score as metric for evaluation on En.QA.
EN.MC consists of 224 question-answer pairs, which are annotated similarly to En.QA, but each question is provided with four answer choices. On average, the long context in En.MC contains 142,622 words.
We use accuracy as metric for evaluation on En.QA. We notice there is another benchmark termed LongBenchÂ <cite class="ltx_cite ltx_citemacro_cite">Bai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib4" title="">2023</a>)</cite>. Nevertheless, the average context length of LongBench is below 20K words, which is not long enough to evaluate the recent long-context LLMs supporting 128K-token window size.</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="473" id="S4.F4.sf1.g1" src="extracted/5829890/figures/op_1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>EN.QA</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="474" id="S4.F4.sf2.g1" src="extracted/5829890/figures/op_2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>EN.MC</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparisons between the proposed order-preserve RAG and vanilla RAG. The evaluations are conducted on En.QA and EN.MC datasets of <math alttext="\infty" class="ltx_Math" display="inline" id="S4.F4.2.m1.1"><semantics id="S4.F4.2.m1.1b"><mi id="S4.F4.2.m1.1.1" mathvariant="normal" xref="S4.F4.2.m1.1.1.cmml">âˆ</mi><annotation-xml encoding="MathML-Content" id="S4.F4.2.m1.1c"><infinity id="S4.F4.2.m1.1.1.cmml" xref="S4.F4.2.m1.1.1"></infinity></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.2.m1.1d">\infty</annotation><annotation encoding="application/x-llamapun" id="S4.F4.2.m1.1e">âˆ</annotation></semantics></math>Bench, using Llama3.1-70B model. </figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation details.</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We set the chunk size as <math alttext="128" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn id="S4.SS2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">128</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">128</annotation></semantics></math> tokens on all datasets. Chunks are non-overlapped.
We use BGE-large-en-v1.5Â <cite class="ltx_cite ltx_citemacro_cite">Xiao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib21" title="">2023</a>)</cite> to extract the embedding of queries and chunks, by default.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Study</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.5"><span class="ltx_text ltx_font_bold" id="S4.SS3.p1.5.1">The influence of context length.</span> We evaluate the influence of the context length on the performance of the proposed order-preserve RAG. Since each chunk contains <math alttext="128" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><cn id="S4.SS3.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">128</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">128</annotation></semantics></math> tokens, the context length is <math alttext="128m" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.1"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mn id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">128</mn><mo id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">â¢</mo><mi id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><times id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1"></times><cn id="S4.SS3.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.SS3.p1.2.m2.1.1.2">128</cn><ci id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">128m</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.1d">128 italic_m</annotation></semantics></math>, where <math alttext="m" class="ltx_Math" display="inline" id="S4.SS3.p1.3.m3.1"><semantics id="S4.SS3.p1.3.m3.1a"><mi id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><ci id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">m</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.3.m3.1d">italic_m</annotation></semantics></math> is the number of the retrieved chunks as the context for generating the answer. As shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#S3.F3" title="Figure 3 â€£ 3 Order-Preserve RAG â€£ In Defense of RAG in the Era of Long-Context Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>, as the context length increases, the performance initially increases. This is because more context might have a greater chance of covering the relevant chunk. Nevertheless, as the
context length further increases, the answer quality drops since more irrelevant chunks are used as distractions. To be specific, Llama3.1-8B model achieves the performance peak when the context length is 16K on both EN.QA dataset and EN.MC dataset, whereas the best performance of Llama3.1-70B model is achieved at <math alttext="48" class="ltx_Math" display="inline" id="S4.SS3.p1.4.m4.1"><semantics id="S4.SS3.p1.4.m4.1a"><mn id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml">48</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><cn id="S4.SS3.p1.4.m4.1.1.cmml" type="integer" xref="S4.SS3.p1.4.m4.1.1">48</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">48</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.4.m4.1d">48</annotation></semantics></math>K on EN.QA and <math alttext="32" class="ltx_Math" display="inline" id="S4.SS3.p1.5.m5.1"><semantics id="S4.SS3.p1.5.m5.1a"><mn id="S4.SS3.p1.5.m5.1.1" xref="S4.SS3.p1.5.m5.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m5.1b"><cn id="S4.SS3.p1.5.m5.1.1.cmml" type="integer" xref="S4.SS3.p1.5.m5.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m5.1c">32</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.5.m5.1d">32</annotation></semantics></math>K on EN.MC.
The fact that the peak point of Llama3.1-70B comes later than Llama3.1-8B model might be because the larger-scale model has a stronger capability to distinguish the relevant chunks from irrelevant distractions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.6"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.6.1">Order-preserve RAG versus vanilla RAG.</span>
As shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#S4.F4" title="Figure 4 â€£ 4.1 Datasets. â€£ 4 Experiments â€£ In Defense of RAG in the Era of Long-Context Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>, when the number of retrieved chunks are small (<em class="ltx_emph ltx_font_italic" id="S4.SS3.p2.6.2">e.g</em>, 8), the advantage of the proposed order-preserve RAG over vanilla RAG is not considerably. In contrast, when the number of retrieved chunks is large, our order-preserve RAG significantly outperforms vanilla RAG. To be specific, on EN.QA dataset, when the number of retrieved chunk is <math alttext="128" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mn id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><cn id="S4.SS3.p2.1.m1.1.1.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">128</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">128</annotation></semantics></math>, vanilla RAG only achieves <math alttext="38.40" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mn id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">38.40</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><cn id="S4.SS3.p2.2.m2.1.1.cmml" type="float" xref="S4.SS3.p2.2.m2.1.1">38.40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">38.40</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">38.40</annotation></semantics></math> F1-score whereas our order-preserve RAG achieves <math alttext="44.43" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m3.1"><semantics id="S4.SS3.p2.3.m3.1a"><mn id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml">44.43</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><cn id="S4.SS3.p2.3.m3.1.1.cmml" type="float" xref="S4.SS3.p2.3.m3.1.1">44.43</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">44.43</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.3.m3.1d">44.43</annotation></semantics></math> F1-score. On EN.MC dataset, retrieving <math alttext="192" class="ltx_Math" display="inline" id="S4.SS3.p2.4.m4.1"><semantics id="S4.SS3.p2.4.m4.1a"><mn id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml">192</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><cn id="S4.SS3.p2.4.m4.1.1.cmml" type="integer" xref="S4.SS3.p2.4.m4.1.1">192</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">192</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.4.m4.1d">192</annotation></semantics></math> chunks, vanialla RAG only achieves <math alttext="81.22" class="ltx_Math" display="inline" id="S4.SS3.p2.5.m5.1"><semantics id="S4.SS3.p2.5.m5.1a"><mn id="S4.SS3.p2.5.m5.1.1" xref="S4.SS3.p2.5.m5.1.1.cmml">81.22</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m5.1b"><cn id="S4.SS3.p2.5.m5.1.1.cmml" type="float" xref="S4.SS3.p2.5.m5.1.1">81.22</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m5.1c">81.22</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.5.m5.1d">81.22</annotation></semantics></math> accuracy whereas our order-preserve RAG obtains <math alttext="88.65" class="ltx_Math" display="inline" id="S4.SS3.p2.6.m6.1"><semantics id="S4.SS3.p2.6.m6.1a"><mn id="S4.SS3.p2.6.m6.1.1" xref="S4.SS3.p2.6.m6.1.1.cmml">88.65</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.6.m6.1b"><cn id="S4.SS3.p2.6.m6.1.1.cmml" type="float" xref="S4.SS3.p2.6.m6.1.1">88.65</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.6.m6.1c">88.65</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.6.m6.1d">88.65</annotation></semantics></math> accuracy.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Main Results</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.2">We compare the proposed order-preserve RAG with two types of baselines. The first category of approaches uses the long-context LLM without RAG. As shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#S4.T1" title="Table 1 â€£ 4.4 Main Results â€£ 4 Experiments â€£ In Defense of RAG in the Era of Long-Context Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, without RAG, LLM takes a huge number of tokens as input, which is inefficient and costly. In contrast, the proposed order-preserve RAG not only significantly reduces the number of tokens, but also significantly improves the answer quality. For instance, using Llama3.1-70B model, the approach without RAG only achieves a <math alttext="34.26" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.1"><semantics id="S4.SS4.p1.1.m1.1a"><mn id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">34.26</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><cn id="S4.SS4.p1.1.m1.1.1.cmml" type="float" xref="S4.SS4.p1.1.m1.1.1">34.26</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">34.26</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.1d">34.26</annotation></semantics></math> F1 score on EN.QA with an average of 117K tokens as input. In contrast, our OP-RAG with 48K tokens as input attains a <math alttext="47.25" class="ltx_Math" display="inline" id="S4.SS4.p1.2.m2.1"><semantics id="S4.SS4.p1.2.m2.1a"><mn id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">47.25</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><cn id="S4.SS4.p1.2.m2.1.1.cmml" type="float" xref="S4.SS4.p1.2.m2.1.1">47.25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">47.25</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.2.m2.1d">47.25</annotation></semantics></math> F1 score. The second category of baselines takes the SELF-ROUTE mechanismÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib11" title="">2024</a>)</cite>, which routes queries to RAG or long-context LLM based on the model self-reflection. As shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#S4.T1" title="Table 1 â€£ 4.4 Main Results â€£ 4 Experiments â€£ In Defense of RAG in the Era of Long-Context Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, ours significantly outperforms than using much fewer tokens in the input of LLMs.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.16">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.16.17.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.16.17.1.1" rowspan="2"><span class="ltx_text" id="S4.T1.16.17.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T1.16.17.1.2">EN.QA</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T1.16.17.1.3">EN.MC</td>
</tr>
<tr class="ltx_tr" id="S4.T1.16.18.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.16.18.2.1">F1 Score</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.16.18.2.2">Tokens</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.16.18.2.3">Acc.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.16.18.2.4">Tokens</td>
</tr>
<tr class="ltx_tr" id="S4.T1.16.19.3">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S4.T1.16.19.3.1">Long-context LLM w/o RAG</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.3">Llama3.1-70B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.1.1.1"><math alttext="34.26" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><mn id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">34.26</mn><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><cn id="S4.T1.1.1.1.m1.1.1.cmml" type="float" xref="S4.T1.1.1.1.m1.1.1">34.26</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">34.26</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">34.26</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.4">117K</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.2"><math alttext="71.62" class="ltx_Math" display="inline" id="S4.T1.2.2.2.m1.1"><semantics id="S4.T1.2.2.2.m1.1a"><mn id="S4.T1.2.2.2.m1.1.1" xref="S4.T1.2.2.2.m1.1.1.cmml">71.62</mn><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><cn id="S4.T1.2.2.2.m1.1.1.cmml" type="float" xref="S4.T1.2.2.2.m1.1.1">71.62</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">71.62</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.m1.1d">71.62</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.2.5">117K</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.3">GPT-4O</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.3.1"><math alttext="32.36" class="ltx_Math" display="inline" id="S4.T1.3.3.1.m1.1"><semantics id="S4.T1.3.3.1.m1.1a"><mn id="S4.T1.3.3.1.m1.1.1" xref="S4.T1.3.3.1.m1.1.1.cmml">32.36</mn><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b"><cn id="S4.T1.3.3.1.m1.1.1.cmml" type="float" xref="S4.T1.3.3.1.m1.1.1">32.36</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">32.36</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.1.m1.1d">32.36</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.4">117K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.2"><math alttext="78.42" class="ltx_Math" display="inline" id="S4.T1.4.4.2.m1.1"><semantics id="S4.T1.4.4.2.m1.1a"><mn id="S4.T1.4.4.2.m1.1.1" xref="S4.T1.4.4.2.m1.1.1.cmml">78.42</mn><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.2.m1.1b"><cn id="S4.T1.4.4.2.m1.1.1.cmml" type="float" xref="S4.T1.4.4.2.m1.1.1">78.42</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.2.m1.1c">78.42</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.2.m1.1d">78.42</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.5">117K</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.6.3">Gemini-1.5-Pro</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.5.5.1"><math alttext="43.08" class="ltx_Math" display="inline" id="S4.T1.5.5.1.m1.1"><semantics id="S4.T1.5.5.1.m1.1a"><mn id="S4.T1.5.5.1.m1.1.1" xref="S4.T1.5.5.1.m1.1.1.cmml">43.08</mn><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.1.m1.1b"><cn id="S4.T1.5.5.1.m1.1.1.cmml" type="float" xref="S4.T1.5.5.1.m1.1.1">43.08</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.1.m1.1c">43.08</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.5.1.m1.1d">43.08</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.6.4">196K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.6.6.2"><math alttext="85.57" class="ltx_Math" display="inline" id="S4.T1.6.6.2.m1.1"><semantics id="S4.T1.6.6.2.m1.1a"><mn id="S4.T1.6.6.2.m1.1.1" xref="S4.T1.6.6.2.m1.1.1.cmml">85.57</mn><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.2.m1.1b"><cn id="S4.T1.6.6.2.m1.1.1.cmml" type="float" xref="S4.T1.6.6.2.m1.1.1">85.57</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.2.m1.1c">85.57</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.6.2.m1.1d">85.57</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.5">188K</td>
</tr>
<tr class="ltx_tr" id="S4.T1.16.20.4">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S4.T1.16.20.4.1">SELF-ROUTEÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib11" title="">2024</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.8.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.8.8.3">GPT-4O</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.7.7.1"><math alttext="34.95" class="ltx_Math" display="inline" id="S4.T1.7.7.1.m1.1"><semantics id="S4.T1.7.7.1.m1.1a"><mn id="S4.T1.7.7.1.m1.1.1" xref="S4.T1.7.7.1.m1.1.1.cmml">34.95</mn><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.1.m1.1b"><cn id="S4.T1.7.7.1.m1.1.1.cmml" type="float" xref="S4.T1.7.7.1.m1.1.1">34.95</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.1.m1.1c">34.95</annotation><annotation encoding="application/x-llamapun" id="S4.T1.7.7.1.m1.1d">34.95</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.8.8.4">85K</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.8.8.2"><math alttext="77.29" class="ltx_Math" display="inline" id="S4.T1.8.8.2.m1.1"><semantics id="S4.T1.8.8.2.m1.1a"><mn id="S4.T1.8.8.2.m1.1.1" xref="S4.T1.8.8.2.m1.1.1.cmml">77.29</mn><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.2.m1.1b"><cn id="S4.T1.8.8.2.m1.1.1.cmml" type="float" xref="S4.T1.8.8.2.m1.1.1">77.29</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.2.m1.1c">77.29</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.8.2.m1.1d">77.29</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.5">62K</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.3">Gemini-1.5-Pro</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.9.9.1"><math alttext="37.51" class="ltx_Math" display="inline" id="S4.T1.9.9.1.m1.1"><semantics id="S4.T1.9.9.1.m1.1a"><mn id="S4.T1.9.9.1.m1.1.1" xref="S4.T1.9.9.1.m1.1.1.cmml">37.51</mn><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.1.m1.1b"><cn id="S4.T1.9.9.1.m1.1.1.cmml" type="float" xref="S4.T1.9.9.1.m1.1.1">37.51</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.1.m1.1c">37.51</annotation><annotation encoding="application/x-llamapun" id="S4.T1.9.9.1.m1.1d">37.51</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.4">83K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.2"><math alttext="76.86" class="ltx_Math" display="inline" id="S4.T1.10.10.2.m1.1"><semantics id="S4.T1.10.10.2.m1.1a"><mn id="S4.T1.10.10.2.m1.1.1" xref="S4.T1.10.10.2.m1.1.1.cmml">76.86</mn><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.2.m1.1b"><cn id="S4.T1.10.10.2.m1.1.1.cmml" type="float" xref="S4.T1.10.10.2.m1.1.1">76.86</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.2.m1.1c">76.86</annotation><annotation encoding="application/x-llamapun" id="S4.T1.10.10.2.m1.1d">76.86</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.5">62K</td>
</tr>
<tr class="ltx_tr" id="S4.T1.16.21.5">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S4.T1.16.21.5.1">Llama3.1-70B order-preserve RAG (ours)</td>
</tr>
<tr class="ltx_tr" id="S4.T1.12.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.12.12.3">OP-RAG-16K</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.11.11.1"><math alttext="44.43" class="ltx_Math" display="inline" id="S4.T1.11.11.1.m1.1"><semantics id="S4.T1.11.11.1.m1.1a"><mn id="S4.T1.11.11.1.m1.1.1" xref="S4.T1.11.11.1.m1.1.1.cmml">44.43</mn><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.1.m1.1b"><cn id="S4.T1.11.11.1.m1.1.1.cmml" type="float" xref="S4.T1.11.11.1.m1.1.1">44.43</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.1.m1.1c">44.43</annotation><annotation encoding="application/x-llamapun" id="S4.T1.11.11.1.m1.1d">44.43</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.12.12.4">16K</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.12.12.2"><math alttext="84.72" class="ltx_Math" display="inline" id="S4.T1.12.12.2.m1.1"><semantics id="S4.T1.12.12.2.m1.1a"><mn id="S4.T1.12.12.2.m1.1.1" xref="S4.T1.12.12.2.m1.1.1.cmml">84.72</mn><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.2.m1.1b"><cn id="S4.T1.12.12.2.m1.1.1.cmml" type="float" xref="S4.T1.12.12.2.m1.1.1">84.72</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.2.m1.1c">84.72</annotation><annotation encoding="application/x-llamapun" id="S4.T1.12.12.2.m1.1d">84.72</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.12.12.5">16K</td>
</tr>
<tr class="ltx_tr" id="S4.T1.14.14">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.14.14.3">OP-RAG-24K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.13.13.1"><math alttext="45.45" class="ltx_Math" display="inline" id="S4.T1.13.13.1.m1.1"><semantics id="S4.T1.13.13.1.m1.1a"><mn id="S4.T1.13.13.1.m1.1.1" xref="S4.T1.13.13.1.m1.1.1.cmml">45.45</mn><annotation-xml encoding="MathML-Content" id="S4.T1.13.13.1.m1.1b"><cn id="S4.T1.13.13.1.m1.1.1.cmml" type="float" xref="S4.T1.13.13.1.m1.1.1">45.45</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.13.1.m1.1c">45.45</annotation><annotation encoding="application/x-llamapun" id="S4.T1.13.13.1.m1.1d">45.45</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.14.14.4">24K</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.14.14.2"><math alttext="\mathbf{88.65}" class="ltx_Math" display="inline" id="S4.T1.14.14.2.m1.1"><semantics id="S4.T1.14.14.2.m1.1a"><mn class="ltx_mathvariant_bold" id="S4.T1.14.14.2.m1.1.1" mathvariant="bold" xref="S4.T1.14.14.2.m1.1.1.cmml">88.65</mn><annotation-xml encoding="MathML-Content" id="S4.T1.14.14.2.m1.1b"><cn id="S4.T1.14.14.2.m1.1.1.cmml" type="float" xref="S4.T1.14.14.2.m1.1.1">88.65</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.14.2.m1.1c">\mathbf{88.65}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.14.14.2.m1.1d">bold_88.65</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S4.T1.14.14.5">24K</td>
</tr>
<tr class="ltx_tr" id="S4.T1.16.16">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.16.16.3">OP-RAG-48K</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.15.15.1"><math alttext="\mathbf{47.25}" class="ltx_Math" display="inline" id="S4.T1.15.15.1.m1.1"><semantics id="S4.T1.15.15.1.m1.1a"><mn class="ltx_mathvariant_bold" id="S4.T1.15.15.1.m1.1.1" mathvariant="bold" xref="S4.T1.15.15.1.m1.1.1.cmml">47.25</mn><annotation-xml encoding="MathML-Content" id="S4.T1.15.15.1.m1.1b"><cn id="S4.T1.15.15.1.m1.1.1.cmml" type="float" xref="S4.T1.15.15.1.m1.1.1">47.25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.15.1.m1.1c">\mathbf{47.25}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.15.15.1.m1.1d">bold_47.25</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.16.16.4">48K</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.16.16.2"><math alttext="85.59" class="ltx_Math" display="inline" id="S4.T1.16.16.2.m1.1"><semantics id="S4.T1.16.16.2.m1.1a"><mn id="S4.T1.16.16.2.m1.1.1" xref="S4.T1.16.16.2.m1.1.1.cmml">85.59</mn><annotation-xml encoding="MathML-Content" id="S4.T1.16.16.2.m1.1b"><cn id="S4.T1.16.16.2.m1.1.1.cmml" type="float" xref="S4.T1.16.16.2.m1.1.1">85.59</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.16.16.2.m1.1c">85.59</annotation><annotation encoding="application/x-llamapun" id="S4.T1.16.16.2.m1.1d">85.59</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.16.16.5">48K</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparisons among the long-context LLM without RAG, SELF-ROUTE mechanismÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01666v1#bib.bib11" title="">2024</a>)</cite> and the proposed order-preserve (OP) RAG.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we have revisited the role of retrieval-augmented generation (RAG) in the era of long-context language models (LLMs). While recent trends have favored long-context LLMs over RAG for their ability to incorporate extensive text sequences, our research challenges this perspective. We argue that extremely long contexts in LLMs can lead to a diminished focus on relevant information, potentially degrading answer quality in question-answering tasks.
To address this issue, we proposed the order-preserve retrieval-augmented generation (OP-RAG) mechanism. Our extensive experiments on public benchmarks have demonstrated that OP-RAG significantly improves the performance of RAG for long-context question-answer applications. OP-RAGâ€™s superior performance suggests that efficient retrieval and focused context utilization can outperform the brute-force approach of processing extremely long contexts.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdin etÂ al. (2024)</span>
<span class="ltx_bibblock">
Marah Abdin, SamÂ Ade Jacobs, AmmarÂ Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, etÂ al. 2024.

</span>
<span class="ltx_bibblock">Phi-3 technical report: A highly capable language model locally on your phone.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2404.14219</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI (2024)</span>
<span class="ltx_bibblock">
Mistral AI. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://mistral.ai/news/mistral-large-2407" title="">Mistral large 2</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2024)</span>
<span class="ltx_bibblock">
Anthropic. 2024.

</span>
<span class="ltx_bibblock">Claude 3.5 sonnet.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023.

</span>
<span class="ltx_bibblock">Longbench: A bilingual, multitask benchmark for long context understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2308.14508</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2023)</span>
<span class="ltx_bibblock">
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023.

</span>
<span class="ltx_bibblock">Extending context window of large language models via positional interpolation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2306.15595</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choromanski etÂ al. (2020)</span>
<span class="ltx_bibblock">
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, etÂ al. 2020.

</span>
<span class="ltx_bibblock">Rethinking attention with performers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2009.14794</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao (2024)</span>
<span class="ltx_bibblock">
Tri Dao. 2024.

</span>
<span class="ltx_bibblock">FlashAttention-2: Faster attention with better parallelism and work partitioning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">International Conference on Learning Representations (ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao etÂ al. (2022)</span>
<span class="ltx_bibblock">
Tri Dao, DanielÂ Y. Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. 2022.

</span>
<span class="ltx_bibblock">FlashAttention: Fast and memory-efficient exact attention with IO-awareness.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Advances in Neural Information Processing Systems (NeurIPS)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020.

</span>
<span class="ltx_bibblock">Retrieval augmented language model pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">International conference on machine learning</em>, pages 3929â€“3938. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis etÂ al. (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, etÂ al. 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Advances in Neural Information Processing Systems</em>, 33:9459â€“9474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. 2024.

</span>
<span class="ltx_bibblock">Retrieval augmented generation or long-context llms? a comprehensive study and hybrid approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2407.16833</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta (2024a)</span>
<span class="ltx_bibblock">
Meta. 2024a.

</span>
<span class="ltx_bibblock">Introducing llama 3.1: Our most capable models to date.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta (2024b)</span>
<span class="ltx_bibblock">
Meta. 2024b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://llama.meta.com" title="">Llama 3.1 models.</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mialon etÂ al. (2023)</span>
<span class="ltx_bibblock">
GrÃ©goire Mialon, Roberto DessÃ¬, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste RoziÃ¨re, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, etÂ al. 2023.

</span>
<span class="ltx_bibblock">Augmented language models: a survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2302.07842</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">GPT-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">ArXiv</em>, 2303:08774.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press etÂ al. (2021)</span>
<span class="ltx_bibblock">
Ofir Press, NoahÂ A Smith, and Mike Lewis. 2021.

</span>
<span class="ltx_bibblock">Train short, test long: Attention with linear biases enables input length extrapolation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2108.12409</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid etÂ al. (2024)</span>
<span class="ltx_bibblock">
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, etÂ al. 2024.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2403.05530</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yutao Sun, LiÂ Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022.

</span>
<span class="ltx_bibblock">A length-extrapolatable transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2212.10554</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay etÂ al. (2020)</span>
<span class="ltx_bibblock">
YiÂ Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020.

</span>
<span class="ltx_bibblock">Efficient transformers: A survey.(2020).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint cs.LG/2009.06732</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">xAI (2024)</span>
<span class="ltx_bibblock">
xAI. 2024.

</span>
<span class="ltx_bibblock">Grok-2 beta release.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2309.07597" title="">C-pack: Packaged resources to advance general chinese embedding</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Preprint</em>, arXiv:2309.07597.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zaheer etÂ al. (2020)</span>
<span class="ltx_bibblock">
Manzil Zaheer, Guru Guruganesh, KumarÂ Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, LiÂ Yang, etÂ al. 2020.

</span>
<span class="ltx_bibblock">Big bird: Transformers for longer sequences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Advances in neural information processing systems</em>, 33:17283â€“17297.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, MooÂ Khai Hao, XuÂ Han, ZhenÂ Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.13718" title=""><math alttext="\infty" class="ltx_Math" display="inline" id="bib.bib23.1.1.m1.1"><semantics id="bib.bib23.1.1.m1.1a"><mi id="bib.bib23.1.1.m1.1.1" mathvariant="normal" xref="bib.bib23.1.1.m1.1.1.cmml">âˆ</mi><annotation-xml encoding="MathML-Content" id="bib.bib23.1.1.m1.1b"><infinity id="bib.bib23.1.1.m1.1.1.cmml" xref="bib.bib23.1.1.m1.1.1"></infinity></annotation-xml><annotation encoding="application/x-tex" id="bib.bib23.1.1.m1.1c">\infty</annotation><annotation encoding="application/x-llamapun" id="bib.bib23.1.1.m1.1d">âˆ</annotation></semantics></math>bench: Extending long context evaluation beyond 100k tokens</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.2.1">Preprint</em>, arXiv:2402.13718.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep  3 07:17:12 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
