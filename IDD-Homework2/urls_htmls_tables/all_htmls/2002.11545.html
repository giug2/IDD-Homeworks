<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2002.11545] Towards Utilizing Unlabeled Data in Federated Learning: A Survey and Prospective</title><meta property="og:description" content="Federated Learning (FL) proposed in recent years has received significant attention from researchers in that it can bring separate data sources together and build machine learning models in a collaborative but private …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Utilizing Unlabeled Data in Federated Learning: A Survey and Prospective">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Utilizing Unlabeled Data in Federated Learning: A Survey and Prospective">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2002.11545">

<!--Generated on Sat Mar 16 11:50:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Utilizing Unlabeled Data in Federated Learning: A Survey and Prospective</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yilun Jin<sup id="id7.2.id1" class="ltx_sup">1</sup><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Contact Author</span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiguang Wei<sup id="id8.2.id1" class="ltx_sup">2</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yang Liu<sup id="id9.5.id1" class="ltx_sup"><span id="id9.5.id1.1" class="ltx_text ltx_font_italic">2</span></sup>
&amp;Qiang Yang<sup id="id10.6.id2" class="ltx_sup"><span id="id10.6.id2.1" class="ltx_text ltx_font_italic">1,2</span></sup>

<br class="ltx_break"><sup id="id11.7.id3" class="ltx_sup">1</sup>The Hong Kong University of Science and Technology, Hong Kong SAR, China
<br class="ltx_break"><sup id="id12.8.id4" class="ltx_sup">2</sup>Webank, Shenzhen, China
<br class="ltx_break">yilun.jin@connect.ust.hk,
{xiguangwei,yangliu}@webank.com,
qyang@cse.ust.hk
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id13.id1" class="ltx_p">Federated Learning (FL) proposed in recent years has received significant attention from researchers in that it can bring separate data sources together and build machine learning models in a collaborative but private manner. Yet, in most applications of FL, such as keyboard prediction, labeling data requires virtually no additional efforts, which is not generally the case. In reality, acquiring large-scale labeled datasets can be extremely costly, which motivates research works that exploit unlabeled data to help build machine learning models. However, to the best of our knowledge, few existing works aim to utilize unlabeled data to enhance federated learning, which leaves a potentially promising research topic. In this paper, we identify the need to exploit unlabeled data in FL, and survey possible research fields that can contribute to the goal.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">There should be little doubt that the prosperity of <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">Artificial Intelligence</span> (AI) should largely be attributed to the availability of Big Data. As an example, the field of computer vision, where we witnessed numerous advances in deep learning, was significantly boosted with the advent of the comprehensive ImageNet dataset <cite class="ltx_cite ltx_citemacro_cite">Deng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib8" title="" class="ltx_ref">2009</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Yet when it comes to applications of AI in real-world scenarios, things are not exactly the case. It is often the case that corporations only possess low-quality, incomplete and insufficient data. To this end, <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">Federated Learning</span> <cite class="ltx_cite ltx_citemacro_cite">McMahan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>); Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib44" title="" class="ltx_ref">2019</a>); Kairouz <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> was proposed as an attempt to alleviate such a problem by enabling private collaboration among parties without explicit sharing of data. Up till now, FL has been widely accepted as a new learning scheme and has triggered numerous applications <cite class="ltx_cite ltx_citemacro_cite">Hard <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib13" title="" class="ltx_ref">2018</a>); Chen <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib6" title="" class="ltx_ref">2019</a>); Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib43" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Nonetheless, as we observe the existing applications of FL, we find that the majority of them require no additional efforts to label the data. For example, in next-word prediction <cite class="ltx_cite ltx_citemacro_cite">Hard <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite>, data are automatically labeled through user typing behaviors. Yet in general, raw data collected require manual labeling, which makes it hard to obtain large-scale, high-quality labeled datasets, making the application of FL limited.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<table id="S1.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.2.1.1" class="ltx_tr">
<th id="S1.T1.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.2.1.1.1.1" class="ltx_text ltx_font_bold">FL setting</span></th>
<th id="S1.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S1.T1.2.1.1.2.1" class="ltx_text ltx_font_bold">ID Space</span></th>
<th id="S1.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S1.T1.2.1.1.3.1" class="ltx_text ltx_font_bold">Feature Space</span></th>
<th id="S1.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S1.T1.2.1.1.4.1" class="ltx_text ltx_font_bold">Label Space</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.2.2.1" class="ltx_tr">
<td id="S1.T1.2.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Horizontal Federated Learning (HFL)</td>
<td id="S1.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Different</td>
<td id="S1.T1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Same</td>
<td id="S1.T1.2.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Same</td>
</tr>
<tr id="S1.T1.2.3.2" class="ltx_tr">
<td id="S1.T1.2.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Vertical Federated Learning (VFL)</td>
<td id="S1.T1.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Same/Can be aligned</td>
<td id="S1.T1.2.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Different</td>
<td id="S1.T1.2.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Different</td>
</tr>
<tr id="S1.T1.2.4.3" class="ltx_tr">
<td id="S1.T1.2.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Federated Transfer Learning (FTL)</td>
<td id="S1.T1.2.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Different</td>
<td id="S1.T1.2.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">(Generally) Different</td>
<td id="S1.T1.2.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">(Generally) Different</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S1.T1.4.2" class="ltx_text" style="font-size:90%;">FL Categorization According to Data Partition</span></figcaption>
</figure>
<figure id="S1.T2" class="ltx_table">
<table id="S1.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T2.2.3.1" class="ltx_tr">
<th id="S1.T2.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T2.2.3.1.1.1" class="ltx_text ltx_font_bold">FL setting</span></th>
<th id="S1.T2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S1.T2.2.3.1.2.1" class="ltx_text ltx_font_bold">Participants</span></th>
<th id="S1.T2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S1.T2.2.3.1.3.1" class="ltx_text ltx_font_bold"># Participants</span></th>
<th id="S1.T2.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S1.T2.2.3.1.4.1" class="ltx_text ltx_font_bold">Local Dataset Size</span></th>
<th id="S1.T2.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S1.T2.2.3.1.5.1" class="ltx_text ltx_font_bold">Consistency</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T2.1.1" class="ltx_tr">
<td id="S1.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Cross-device FL</td>
<td id="S1.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">e.g. phones, IoT devices.</td>
<td id="S1.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Massive, up to <math id="S1.T2.1.1.1.m1.1" class="ltx_Math" alttext="10^{10}" display="inline"><semantics id="S1.T2.1.1.1.m1.1a"><msup id="S1.T2.1.1.1.m1.1.1" xref="S1.T2.1.1.1.m1.1.1.cmml"><mn id="S1.T2.1.1.1.m1.1.1.2" xref="S1.T2.1.1.1.m1.1.1.2.cmml">10</mn><mn id="S1.T2.1.1.1.m1.1.1.3" xref="S1.T2.1.1.1.m1.1.1.3.cmml">10</mn></msup><annotation-xml encoding="MathML-Content" id="S1.T2.1.1.1.m1.1b"><apply id="S1.T2.1.1.1.m1.1.1.cmml" xref="S1.T2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S1.T2.1.1.1.m1.1.1.1.cmml" xref="S1.T2.1.1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S1.T2.1.1.1.m1.1.1.2.cmml" xref="S1.T2.1.1.1.m1.1.1.2">10</cn><cn type="integer" id="S1.T2.1.1.1.m1.1.1.3.cmml" xref="S1.T2.1.1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.1.1.1.m1.1c">10^{10}</annotation></semantics></math> clients</td>
<td id="S1.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Relatively small</td>
<td id="S1.T2.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Inconsistent</td>
</tr>
<tr id="S1.T2.2.2" class="ltx_tr">
<td id="S1.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Cross-silo FL</td>
<td id="S1.T2.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">e.g. corporations, institutes</td>
<td id="S1.T2.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Up to <math id="S1.T2.2.2.1.m1.1" class="ltx_Math" alttext="10^{2}" display="inline"><semantics id="S1.T2.2.2.1.m1.1a"><msup id="S1.T2.2.2.1.m1.1.1" xref="S1.T2.2.2.1.m1.1.1.cmml"><mn id="S1.T2.2.2.1.m1.1.1.2" xref="S1.T2.2.2.1.m1.1.1.2.cmml">10</mn><mn id="S1.T2.2.2.1.m1.1.1.3" xref="S1.T2.2.2.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S1.T2.2.2.1.m1.1b"><apply id="S1.T2.2.2.1.m1.1.1.cmml" xref="S1.T2.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S1.T2.2.2.1.m1.1.1.1.cmml" xref="S1.T2.2.2.1.m1.1.1">superscript</csymbol><cn type="integer" id="S1.T2.2.2.1.m1.1.1.2.cmml" xref="S1.T2.2.2.1.m1.1.1.2">10</cn><cn type="integer" id="S1.T2.2.2.1.m1.1.1.3.cmml" xref="S1.T2.2.2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.2.2.1.m1.1c">10^{2}</annotation></semantics></math>.</td>
<td id="S1.T2.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Relatively large</td>
<td id="S1.T2.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Consistent</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T2.4.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S1.T2.5.2" class="ltx_text" style="font-size:90%;">FL Categorization According to Type of Participants. The term ’consistency’ means the consistency of participants across each round. In cross-device FL, the participants are not always available (e.g. subject to network and battery status, and diurnal-nocturnal changes), making the participants for each round different, and thus ’inconsistent’. On the contrary, cross-silo FL shows much better consistency, as they use dedicated hardware, reliable networks and are much better scheduled. </span></figcaption>
</figure>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2002.11545/assets/cross-device_FL.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="237" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">An illustration of cross-device FL. The model is trained through numerous devices and is deployed to all devices throughout the world. </span></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We argue that applications of FL are in more pressing need of utilizing unlabeled data than others. On one hand, in cross-device FL <cite class="ltx_cite ltx_citemacro_cite">Kairouz <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>, where participants are individual devices, numerous unlabeled data are generated through our interaction with smart devices, such as photos taken, text inputs, and physiological indicators measured by wearables, whose sheer volume makes it impractical to require users to label them. On the other hand, in cross-silo FL where participants are corporations, the data involved are likely to require human expertise, such as finance (risk management, credit evaluation), and medical applications (disease diagnosis, health monitoring). In these cases, it would require significant human intellect and efforts to label the data. In this case, labeling all the data would be costly, and thus makes it necessary to utilize unlabeled data and learn models in a weakly supervised manner.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Nevertheless, compared to other areas, there is relatively little attention paid to this area. While techniques like transfer learning, semi-supervised learning, self-supervised learning and active learning are all popular research topics, we can only observe popularity in federated transfer learning (FTL) <cite class="ltx_cite ltx_citemacro_cite">Peng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib32" title="" class="ltx_ref">2020</a>); Liu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite>, while others are relatively ignored.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Consequently, in this paper, we seek to provide a perspective into weakly supervised approaches in federated learning. We first introduce related preliminaries, before identifying motivations that drive us to devote to this problem. Last but not least, we make a prospect into potential scenarios, research topics, as well as challenges. We hope that our efforts can be followed by researchers who come up with concrete solutions to the problem that will contribute to both the academia and the industry.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Preliminaries and Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Federated Learning</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p"><span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">Federated Learning</span>, proposed by <cite class="ltx_cite ltx_citemacro_cite">McMahan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>)</cite> and extensively surveyed by <cite class="ltx_cite ltx_citemacro_cite">Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib44" title="" class="ltx_ref">2019</a>); Kairouz <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>, is a machine learning scheme that enables aggregation of isolated data in a privacy-preserving manner. Generally speaking there are two major categorization standards proposed by previous surveys, with the first <cite class="ltx_cite ltx_citemacro_cite">Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite> focusing on data partitions and the latter <cite class="ltx_cite ltx_citemacro_cite">Kairouz <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> focusing on types of participants. We show the two categorizations in Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Towards Utilizing Unlabeled Data in Federated Learning: A Survey and Prospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Table <a href="#S1.T2" title="Table 2 ‣ 1 Introduction ‣ Towards Utilizing Unlabeled Data in Federated Learning: A Survey and Prospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, respectively.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Existing works on FL have shown significant diversity. There have been research works on federated optimization <cite class="ltx_cite ltx_citemacro_cite">McMahan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>); Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>); Wang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib42" title="" class="ltx_ref">2020</a>)</cite>, federated learning algorithms <cite class="ltx_cite ltx_citemacro_cite">Cheng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>); Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib21" title="" class="ltx_ref">2019</a>); Shokri and
Shmatikov (<a href="#bib.bib36" title="" class="ltx_ref">2015</a>)</cite>, privacy mechanisms and attacks <cite class="ltx_cite ltx_citemacro_cite">Hitaj <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib15" title="" class="ltx_ref">2017</a>); Mohassel and
Zhang (<a href="#bib.bib28" title="" class="ltx_ref">2017</a>); Bonawitz <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib2" title="" class="ltx_ref">2017</a>)</cite>, systems and communication <cite class="ltx_cite ltx_citemacro_cite">Bonawitz <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite>, etc. However, regarding FL in weakly-supervised scenarios, relatively little attention has been paid to this area.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Existing works on weakly supervised FL mostly fall into federated transfer learning (FTL), with <cite class="ltx_cite ltx_citemacro_cite">Peng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Liu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite> proposed unsupervised and supervised FTL, respectively. There are also works tackling federated self-supervised feature learning on texts <cite class="ltx_cite ltx_citemacro_cite">Jiang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib16" title="" class="ltx_ref">2019</a>); McMahan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>)</cite> by learning topic models and language models. Regarding other forms of weakly supervised algorithms, such as semi-supervised learning and active learning, we observe little prior arts <cite class="ltx_cite ltx_citemacro_cite">Goetz <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> to the best of our knowledge.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">We here discuss two prior works on federated transfer learning. <cite class="ltx_cite ltx_citemacro_cite">Liu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite> tackles the problem of semi-supervised transfer learning between two clients, where the two clients exchange gradients and intermediate results through Homomorphic Encryption (HE). As generally, HE is computationally expensive to perform, the approach may not scale to cross-device FL where maybe millions of participants exist. <cite class="ltx_cite ltx_citemacro_cite">Peng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite> focuses on unsupervised domain adaptation, that uses several source domains held by clients to facilitate classification on one target domain. The work achieves domain adaptation through novel adversarial training techniques and achieved convincing results. Yet, similar to <cite class="ltx_cite ltx_citemacro_cite">Liu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite>, this work assumes that the participants are static and constantly available, which also does not scale to the cross-device FL setting.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Weakly Supervised Learning Algorithms</h3>

<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Transfer Learning</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">Transfer Learning <cite class="ltx_cite ltx_citemacro_cite">Yang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite> aims to transfer knowledge learned from a source domain to a relevant target domain, probably with fewer labeled samples to train on. Existing popular transfer learning methods include domain adaptation <cite class="ltx_cite ltx_citemacro_cite">Long <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib24" title="" class="ltx_ref">2014</a>, <a href="#bib.bib25" title="" class="ltx_ref">2015</a>)</cite>, knowledge distillation <cite class="ltx_cite ltx_citemacro_cite">Hinton <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib14" title="" class="ltx_ref">2015</a>)</cite>, and pre-training/fine-tuning <cite class="ltx_cite ltx_citemacro_cite">Devlin <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> etc.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p">While transfer learning has achieved tremendous success in vision and language modeling, and even triggered interests in FTL, one limitation exists, that a related source domain with abundant data must be found to support transfer learning. In FL, the applications are highly diverse, which makes it hard for every one of them to find a suitable and resourceful source domain.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Semi-supervised Learning</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">Semi-supervised Learning (SSL) <cite class="ltx_cite ltx_citemacro_cite">Zhu (<a href="#bib.bib48" title="" class="ltx_ref">2005</a>)</cite> aims to learn a model under very limited labeled data and also massive unlabeled data. SSL is widely adopted in areas where labels are scarce. In most cases researchers utilize unlabeled data to improve the generalization performance and prevent overfitting caused by small datasets. Popular methods of SSL include generative models <cite class="ltx_cite ltx_citemacro_cite">Kingma <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib19" title="" class="ltx_ref">2014</a>); Robert <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib33" title="" class="ltx_ref">2018</a>)</cite>, adversarial training <cite class="ltx_cite ltx_citemacro_cite">Miyato <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib27" title="" class="ltx_ref">2018</a>); Odena (<a href="#bib.bib29" title="" class="ltx_ref">2016</a>)</cite>, regularization <cite class="ltx_cite ltx_citemacro_cite">Tarvainen and
Valpola (<a href="#bib.bib40" title="" class="ltx_ref">2017</a>)</cite>, pseudo-labeling <cite class="ltx_cite ltx_citemacro_cite">Berthelot <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib1" title="" class="ltx_ref">2019</a>)</cite>, connections between samples <cite class="ltx_cite ltx_citemacro_cite">Kipf and Welling (<a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite> and multi-view ensemble training <cite class="ltx_cite ltx_citemacro_cite">Chen <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Self-supervised Learning</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">Self-supervised learning, also known as representation learning, aims to extract indicative features from large amounts of data without label supervision. Consequently, common approaches in self-supervised learning utilize the data themselves to provide supervision, trying to capture innate structures within the data. Up till now, self-supervised learning has achieved tremendous success in natural language process (NLP) through large-scale language models <cite class="ltx_cite ltx_citemacro_cite">Devlin <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>, and also topic models <cite class="ltx_cite ltx_citemacro_cite">Jiang <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite>. Also in the area of vision, self-supervised feature learning is popular, commonly achieved by learning colorization, positioning and rotation information <cite class="ltx_cite ltx_citemacro_cite">Trinh <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib41" title="" class="ltx_ref">2019</a>); Pathak <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib31" title="" class="ltx_ref">2016</a>)</cite>, and has been used for boosting performance in semantic segmentation, clustering, and object detection <cite class="ltx_cite ltx_citemacro_cite">Jing and Tian (<a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.4 </span>Active Learning</h4>

<div id="S2.SS2.SSS4.p1" class="ltx_para">
<p id="S2.SS2.SSS4.p1.1" class="ltx_p">Active Learning aims to train a classifier on datasets with few labeled samples by making as few queries of additional label information as possible. Essentially, active learning aims to find samples that, when labeled, will provide the greatest contribution towards model learning. In existing approaches, active learning is achieved by designing label query algorithms, such as the most uncertain samples <cite class="ltx_cite ltx_citemacro_cite">Settles and
Craven (<a href="#bib.bib35" title="" class="ltx_ref">2008</a>)</cite>, most variance reduction <cite class="ltx_cite ltx_citemacro_cite">Schein and Ungar (<a href="#bib.bib34" title="" class="ltx_ref">2007</a>)</cite>, etc.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Motivations and Advantages</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we identify the motivations that drive us to the problem of FL in weakly supervised settings, and propose advantages that will arise when FL is able to utilize unlabeled samples.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Expanding Application Scenarios</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Existing application scenarios of FL generally work on problems which require little extra effort to label the data. For example, in language modeling <cite class="ltx_cite ltx_citemacro_cite">McMahan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>)</cite>, labeling is automatically achieved through user typing behaviors. In recommendation <cite class="ltx_cite ltx_citemacro_cite">Yang (<a href="#bib.bib46" title="" class="ltx_ref">2019</a>)</cite>, the labels are purchase records of users, which also require no extra labor. Yet in most applications, explicit labeling is required, such as object recognition, sentiment analysis, person re-identification, etc.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We also argue that applications of FL face even greater demands in utilizing unlabeled data.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">First, FL imposes strong privacy requirements, which rules out large-scale labeling through outsourcing, which is a common practice in corporations.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Second, in cross-device FL introduced by <cite class="ltx_cite ltx_citemacro_cite">Kairouz <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>, where participants are smart devices, huge amounts of data are generated every day, such as text inputs, images taken, and even physiological indicators measured by wearables. These data are either too large in size to require users to label, or require high-level human expertise (such as sleep monitoring, heartbeats) that few users possess. Consequently, quite often the data generated remain unlabeled.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Last but not least, in cross-silo FL, where participants are corporations, the data involved often lie within specialized domains, such as finance (risk management, credit evaluation, anti money laundering), or clinical services (medical image diagnosis, object detection and localization). In these domains, the effort required to label the data are generally prohibitive, and therefore we can only afford to label a small proportion of them, instead of the whole dataset.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Consequently, developing algorithms that effectively utilize unlabeled data to enhance training would open up extensive new applications and help build a more vibrant federated AI ecosystem.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Mitigating Domain Discrepancy</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">As a challenge identified by many researchers, non-iid data is a prominent issue in FL, and there have also been works to study such a challenge <cite class="ltx_cite ltx_citemacro_cite">Li <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite>. Generally speaking, non-iid data pose two challenges to FL. On one hand, the data owned by different parties inevitably differ in their distribution, causing difficulties in model learning. On the other hand, domain discrepancy also exists between training and testing. Chances are that the data used to train a federated model differs a lot to those owned by certain users, making the model ineffective for them. In fact, a recent empirical study <cite class="ltx_cite ltx_citemacro_cite">Yu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib47" title="" class="ltx_ref">2020</a>)</cite> demonstrated that, federated language models can be less accurate than a considerable proportion (as much as 20%) of local models trained using data from individual parties, whose data distributions differ a lot from the global distribution.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Utilizing large-scale unlabeled data, correspondingly, is able to mitigate the problem of non-iid data. Intuitively, by viewing a sufficiently large unlabeled dataset, one can get a much better understanding of the data distribution than using only a small labeled dataset alone. For example, unlabeled data can be used to train generative models that provide additional information about the data’s prior distribution <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="p(x)" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.2" xref="S3.SS2.p2.1.m1.1.2.cmml"><mi id="S3.SS2.p2.1.m1.1.2.2" xref="S3.SS2.p2.1.m1.1.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.2.1" xref="S3.SS2.p2.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS2.p2.1.m1.1.2.3.2" xref="S3.SS2.p2.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p2.1.m1.1.2.3.2.1" xref="S3.SS2.p2.1.m1.1.2.cmml">(</mo><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS2.p2.1.m1.1.2.3.2.2" xref="S3.SS2.p2.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.2"><times id="S3.SS2.p2.1.m1.1.2.1.cmml" xref="S3.SS2.p2.1.m1.1.2.1"></times><ci id="S3.SS2.p2.1.m1.1.2.2.cmml" xref="S3.SS2.p2.1.m1.1.2.2">𝑝</ci><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">p(x)</annotation></semantics></math>, thus filtering out the domain-specific features <cite class="ltx_cite ltx_citemacro_cite">Kingma <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib19" title="" class="ltx_ref">2014</a>); Robert <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib33" title="" class="ltx_ref">2018</a>)</cite>. In addition, domain adaptation that minimizes domain discrepancies can also be used on unlabeled data <cite class="ltx_cite ltx_citemacro_cite">Peng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite>, such that domain invariant representations can be learned. Last but not least, advances in disentangled representations <cite class="ltx_cite ltx_citemacro_cite">Siddharth <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib38" title="" class="ltx_ref">2017</a>)</cite> can also contribute to domain invariant models by disentangling domain-specific features from domain invariant ones.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Enhancing Robustness</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Robustness means that a model would be resilient to small variations, such as outliers and small perturbations of inputs, which is appealing in most machine learning applications. By utilizing unlabeled data to regularize the model, robustness can be achieved. For example, sensitivity towards small perturbations can be alleviated if we regularize the model to produce consistent outputs in the neighborhood of each data point. It would not be possible if only a few labeled samples are available, as they only represent a small subset over the data distribution. In addition, reliance on specific data points can be alleviated if more unlabeled data can be used to prevent overfitting on a few labeled samples.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Robustness in FL also implies attractive outcomes. On one hand, when participants of FL have a rather limited amount of data, the local trainings are likely to be noisy, and local models prone to overfitting. By utilizing available unlabeled data for regularization, local overfitting can be alleviated and therefore, a better global model can be reached. On the other hand, robustness implies resilience towards modification of the dataset, which is favorable towards private and secure machine learning models. For example, robustness against small perturbations would lead to resistance over data poisoning attacks, such as adversarial examples <cite class="ltx_cite ltx_citemacro_cite">Goodfellow <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib11" title="" class="ltx_ref">2014</a>)</cite>. As another example, as shown in <cite class="ltx_cite ltx_citemacro_cite">Shokri <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib37" title="" class="ltx_ref">2017</a>)</cite>, membership inference attacks are closely related to overfitting, and the more overfitting the model is, the more prone it is towards membership inference attacks (as the model is more likely to behave differently on samples that are used to train the model). Consequently, robustness in FL can also lead to appealing properties in security.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Potential Topics and Challenges</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section we introduce potential settings and topics, both in research and applications, that may contribute to better FL algorithms, and also potential challenges that may arise.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Transfer Learning</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Existing solutions enabling FTL have been highly sophisticated <cite class="ltx_cite ltx_citemacro_cite">Liu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2018</a>); Peng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite>. We here identify several potential topics regarding FTL.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Versatile Source Domains and Datasets.</span> As FL should support a wide range of applications, to enable FTL, it is important that adequate source domains and datasets are chosen, otherwise negative transfer <cite class="ltx_cite ltx_citemacro_cite">Cao <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib4" title="" class="ltx_ref">2010</a>)</cite> may happen. It is thus important in practice that adequate source domains must be chosen to enable FTL applications. Alternatively, it is always welcomed to develop versatile datasets that transfer to multiple domains.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Realistic Federated Datasets.</span> FL features non-iid data held by different participants, as determined by location, population, etc, and a realistic federated dataset that accurately replicates such domain discrepancies would be necessary for evaluating FTL or even broader FL algorithms. Up till now, existing FTL evaluations use artificial datasets created by manipulating existing benchmarks, which may not accurately capture real-world domain discrepancies featured by FL.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">FTL in cross-device FL.</span> Existing solutions on FTL work on relatively few participants, e.g. several, or tens, with each of them holding relatively large data, and are always available throughout the training <cite class="ltx_cite ltx_citemacro_cite">Peng <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite>. Yet, in cross-device FL, participants are much larger in size, inconsistent for each round of training, and each of them may hold much smaller amounts of data, as shown in Table <a href="#S1.T2" title="Table 2 ‣ 1 Introduction ‣ Towards Utilizing Unlabeled Data in Federated Learning: A Survey and Prospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. It is thus relatively unknown how FTL can work in the cross-device FL setting, which also shows significant domain discrepancy <cite class="ltx_cite ltx_citemacro_cite">Yu <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib47" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Semi-supervised Learning</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Semi-supervised setting in FL has received little attention, which leaves a promising potential topic, as semi-supervised learning can work on almost all types of data. For example, in medical image classification, obtaining fully annotated training datasets may not be possible, where we can resort to federated semi-supervised learning to solve the problem. As another example, it is also costly to obtain fully annotated data in financial applications, where collaborators such as banks, insurance companies would jointly train their model in a semi-supervised manner. We here point out several potential challenges that need to be resolved in this topic.</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p"><span id="S4.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Privacy Requirements. </span> In certain semi-supervised learning algorithms, connections between samples are leveraged to infer or ’propagate’ labels towards unlabeled samples <cite class="ltx_cite ltx_citemacro_cite">Kipf and Welling (<a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite>. In these approaches, it is important that privacy requirements are not breached when we leverage these connections. There are also algorithms that involve generative models, which are capable of generating artificial samples <cite class="ltx_cite ltx_citemacro_cite">Robert <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib33" title="" class="ltx_ref">2018</a>); Springenberg (<a href="#bib.bib39" title="" class="ltx_ref">2015</a>)</cite>. Whether such artificial samples are breaking the privacy requirements remains an important challenge that is yet to be resolved.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p"><span id="S4.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Domain Discrepancy</span> Non-iid data always pose significant challenges in FL. In the case of semi-supervised learning, <cite class="ltx_cite ltx_citemacro_cite">Oliver <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite> showed that when labeled data and unlabeled data belong to different domains (i.e. domains that show significant discrepancy), semi-supervised learning algorithms will significantly degrade in performance. Thus, semi-supervised learning methods in FL must be combined with techniques that tackle with domain discrepancy.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p"><span id="S4.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Extension to VFL</span> Existing studies on semi-supervised learning mainly fits in with the HFL setting, where the unlabeled data are shown intact. However, when it comes to VFL, where the data samples themselves are fragmented and cannot be brought together, more sophisticated protocols should be designed.</p>
</div>
</li>
<li id="S4.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i4.p1" class="ltx_para">
<p id="S4.I2.i4.p1.1" class="ltx_p"><span id="S4.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">Relationship between robustness and security</span> As mentioned before, model robustness (e.g. robustness to perturbations, outliers) are intuitively related with defense against attacks, such as adversarial attacks and membership inference attacks. As various regularization techniques are involved in semi-supervised learning, it is interesting to study, both empirically and theoretically how such regularization and robustness will contribute towards model security.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Self-supervised Learning</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">One significant doubt on self-supervised learning in FL is that, it may depend strongly on the data domain and the downstream task it is used for. For example, while self-supervised language modeling is competitive in a wide range of tasks, self-supervised learning in vision is not the case. As shown in <cite class="ltx_cite ltx_citemacro_cite">Goyal <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite>, self-supervised learning is competitive in object detection, but outperformed by supervised pre-training significantly in various classification tasks. Consequently, although self-supervised learning is a natural idea in FL <cite class="ltx_cite ltx_citemacro_cite">McMahan <span class="ltx_text ltx_font_italic">et al.</span> (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>)</cite>, whether it enables wider application may be doubtful and depend heavily on the specific application.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Active Learning</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Active learning seems a natural idea that can be well combined with FL. For example, in cross-device FL, the model holder may ask certain users to label several examples which are then used for training, acting in a crowd-sourcing manner. In cross-silo FL, an institute may identify several difficult examples during training, and ask its experts to label it to facilitate training.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">A key challenge that needs to be solved is how to identify data samples that contribute most to training and should be queried. In federated learning, neither the coordinator or the training server can directly observe raw data. Instead they can only observe batched, and in some cases even protected (e.g. via differential privacy) or encrypted intermediate results. Consequently, identifying individual data samples that may contribute most to training is not straightforward.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper we identify a potentially important topic in federated learning: utilizing unlabeled data for weakly supervised federated training. We introduce existing methods that effectively leverage unlabeled data for training models, and point out motivating advantages that arise if unlabeled data can be incorporated for weakly-supervised training. Finally, we make a prospect into potential topics, application scenarios and challenges that come along weakly supervised learning in FL. We hope that this paper can lead to more attempts in more effective utilization of data, better learning algorithms, and a more diverse federated ecosystem featuring a wider range of applications.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berthelot <span id="bib.bib1.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital
Oliver, and Colin A Raffel.

</span>
<span class="ltx_bibblock">Mixmatch: A holistic approach to semi-supervised learning.

</span>
<span class="ltx_bibblock">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc,
E. Fox, and R. Garnett, editors, <span id="bib.bib1.3.1" class="ltx_text ltx_font_italic">Advances in Neural Information
Processing Systems 32</span>, pages 5049–5059. Curran Associates, Inc., 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz <span id="bib.bib2.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy-preserving machine learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.3.1" class="ltx_text ltx_font_italic">Proceedings of the 2017 ACM SIGSAC Conference on Computer and
Communications Security</span>, pages 1175–1191. ACM, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz <span id="bib.bib3.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konecny, Stefano Mazzocchi,
H Brendan McMahan, et al.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System design.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1902.01046</span>, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao <span id="bib.bib4.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2010]</span>
<span class="ltx_bibblock">
Bin Cao, Sinno Jialin Pan, Yu Zhang, Dit-Yan Yeung, and Qiang Yang.

</span>
<span class="ltx_bibblock">Adaptive transfer learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.3.1" class="ltx_text ltx_font_italic">Twenty-Fourth AAAI Conference on Artificial Intelligence</span>,
2010.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen <span id="bib.bib5.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Dong-Dong Chen, Wei Wang, Wei Gao, and Zhi-Hua Zhou.

</span>
<span class="ltx_bibblock">Tri-net for semi-supervised deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.3.1" class="ltx_text ltx_font_italic">Proceedings of the 27th International Joint Conference on
Artificial Intelligence</span>, pages 2014–2020. AAAI Press, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen <span id="bib.bib6.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Mingqing Chen, Rajiv Mathews, Tom Ouyang, and Françoise Beaufays.

</span>
<span class="ltx_bibblock">Federated learning of out-of-vocabulary words.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1903.10635</span>, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng <span id="bib.bib7.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Kewei Cheng, Tao Fan, Yilun Jin, Yang Liu, Tianjian Chen, and Qiang Yang.

</span>
<span class="ltx_bibblock">Secureboost: A lossless federated learning framework.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1901.08755</span>, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng <span id="bib.bib8.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2009]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.3.1" class="ltx_text ltx_font_italic">2009 IEEE conference on computer vision and pattern
recognition</span>, pages 248–255. Ieee, 2009.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin <span id="bib.bib9.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.3.1" class="ltx_text ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</span>, pages 4171–4186, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goetz <span id="bib.bib10.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Jack Goetz, Kshitiz Malik, Duc Bui, Seungwhan Moon, Honglei Liu, and Anuj
Kumar.

</span>
<span class="ltx_bibblock">Active federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1909.12641</span>, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow <span id="bib.bib11.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2014]</span>
<span class="ltx_bibblock">
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.

</span>
<span class="ltx_bibblock">Explaining and harnessing adversarial examples.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.6572</span>, 2014.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal <span id="bib.bib12.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra.

</span>
<span class="ltx_bibblock">Scaling and benchmarking self-supervised visual representation
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.3.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</span>, pages 6391–6400, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard <span id="bib.bib13.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Andrew Hard, Kanishka Rao, Rajiv Mathews, Françoise Beaufays, Sean
Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel Ramage.

</span>
<span class="ltx_bibblock">Federated learning for mobile keyboard prediction.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1811.03604</span>, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton <span id="bib.bib14.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2015]</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1503.02531</span>, 2015.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hitaj <span id="bib.bib15.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz.

</span>
<span class="ltx_bibblock">Deep models under the gan: information leakage from collaborative
deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.3.1" class="ltx_text ltx_font_italic">Proceedings of the 2017 ACM SIGSAC Conference on Computer and
Communications Security</span>, pages 603–618. ACM, 2017.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang <span id="bib.bib16.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Di Jiang, Yuanfeng Song, Yongxin Tong, Xueyang Wu, Weiwei Zhao, Qian Xu, and
Qiang Yang.

</span>
<span class="ltx_bibblock">Federated topic modeling.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.3.1" class="ltx_text ltx_font_italic">Proceedings of the 28th ACM International Conference on
Information and Knowledge Management</span>, pages 1071–1080, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jing and Tian [2019]</span>
<span class="ltx_bibblock">
Longlong Jing and Yingli Tian.

</span>
<span class="ltx_bibblock">Self-supervised visual feature learning with deep neural networks: A
survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1902.06162</span>, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz <span id="bib.bib18.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.04977</span>, 2019.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma <span id="bib.bib19.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2014]</span>
<span class="ltx_bibblock">
Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling.

</span>
<span class="ltx_bibblock">Semi-supervised learning with deep generative models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.3.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
3581–3589, 2014.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kipf and Welling [2016]</span>
<span class="ltx_bibblock">
Thomas N Kipf and Max Welling.

</span>
<span class="ltx_bibblock">Semi-supervised classification with graph convolutional networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1609.02907</span>, 2016.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib21.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Qinbin Li, Zeyi Wen, and Bingsheng He.

</span>
<span class="ltx_bibblock">Practical federated gradient boosting decision trees.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1911.04206</span>, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li <span id="bib.bib22.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.

</span>
<span class="ltx_bibblock">On the convergence of fedavg on non-iid data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.3.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu <span id="bib.bib23.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Yang Liu, Tianjian Chen, and Qiang Yang.

</span>
<span class="ltx_bibblock">Secure federated transfer learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.03337</span>, 2018.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long <span id="bib.bib24.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2014]</span>
<span class="ltx_bibblock">
Mingsheng Long, Jianmin Wang, Jiaguang Sun, and S Yu Philip.

</span>
<span class="ltx_bibblock">Domain invariant transfer kernel learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</span>,
27(6):1519–1532, 2014.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long <span id="bib.bib25.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2015]</span>
<span class="ltx_bibblock">
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan.

</span>
<span class="ltx_bibblock">Learning transferable features with deep adaptation networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.3.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages 97–105,
2015.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan <span id="bib.bib26.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.3.1" class="ltx_text ltx_font_italic">Artificial Intelligence and Statistics</span>, pages 1273–1282,
2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miyato <span id="bib.bib27.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii.

</span>
<span class="ltx_bibblock">Virtual adversarial training: a regularization method for supervised
and semi-supervised learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>,
41(8):1979–1993, 2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohassel and
Zhang [2017]</span>
<span class="ltx_bibblock">
Payman Mohassel and Yupeng Zhang.

</span>
<span class="ltx_bibblock">Secureml: A system for scalable privacy-preserving machine learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">2017 IEEE Symposium on Security and Privacy (SP)</span>, pages
19–38. IEEE, 2017.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Odena [2016]</span>
<span class="ltx_bibblock">
Augustus Odena.

</span>
<span class="ltx_bibblock">Semi-supervised learning with generative adversarial networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.01583</span>, 2016.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oliver <span id="bib.bib30.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian
Goodfellow.

</span>
<span class="ltx_bibblock">Realistic evaluation of deep semi-supervised learning algorithms.

</span>
<span class="ltx_bibblock">In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, editors, <span id="bib.bib30.3.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing
Systems 31</span>, pages 3235–3246. Curran Associates, Inc., 2018.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pathak <span id="bib.bib31.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2016]</span>
<span class="ltx_bibblock">
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A
Efros.

</span>
<span class="ltx_bibblock">Context encoders: Feature learning by inpainting.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.3.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 2536–2544, 2016.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng <span id="bib.bib32.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Xingchao Peng, Zijun Huang, Yizhe Zhu, and Kate Saenko.

</span>
<span class="ltx_bibblock">Federated adversarial domain adaptation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.3.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robert <span id="bib.bib33.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Thomas Robert, Nicolas Thome, and Matthieu Cord.

</span>
<span class="ltx_bibblock">Hybridnet: Classification and reconstruction cooperation for
semi-supervised learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.3.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 153–169, 2018.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schein and Ungar [2007]</span>
<span class="ltx_bibblock">
Andrew I Schein and Lyle H Ungar.

</span>
<span class="ltx_bibblock">Active learning for logistic regression: an evaluation.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Machine Learning</span>, 68(3):235–265, 2007.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Settles and
Craven [2008]</span>
<span class="ltx_bibblock">
Burr Settles and Mark Craven.

</span>
<span class="ltx_bibblock">An analysis of active learning strategies for sequence labeling
tasks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2008 Conference on Empirical Methods in
Natural Language Processing</span>, pages 1070–1079, 2008.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shokri and
Shmatikov [2015]</span>
<span class="ltx_bibblock">
Reza Shokri and Vitaly Shmatikov.

</span>
<span class="ltx_bibblock">Privacy-preserving deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Proceedings of the 22nd ACM SIGSAC conference on computer and
communications security</span>, pages 1310–1321. ACM, 2015.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shokri <span id="bib.bib37.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.

</span>
<span class="ltx_bibblock">Membership inference attacks against machine learning models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.3.1" class="ltx_text ltx_font_italic">2017 IEEE Symposium on Security and Privacy (SP)</span>, pages
3–18. IEEE, 2017.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siddharth <span id="bib.bib38.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2017]</span>
<span class="ltx_bibblock">
Narayanaswamy Siddharth, Brooks Paige, Jan-Willem Van de Meent, Alban
Desmaison, Noah Goodman, Pushmeet Kohli, Frank Wood, and Philip Torr.

</span>
<span class="ltx_bibblock">Learning disentangled representations with semi-supervised deep
generative models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.3.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pages
5925–5935, 2017.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Springenberg [2015]</span>
<span class="ltx_bibblock">
Jost Tobias Springenberg.

</span>
<span class="ltx_bibblock">Unsupervised and semi-supervised learning with categorical generative
adversarial networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.06390</span>, 2015.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tarvainen and
Valpola [2017]</span>
<span class="ltx_bibblock">
Antti Tarvainen and Harri Valpola.

</span>
<span class="ltx_bibblock">Mean teachers are better role models: Weight-averaged consistency
targets improve semi-supervised deep learning results.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
1195–1204, 2017.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trinh <span id="bib.bib41.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Trieu H Trinh, Minh-Thang Luong, and Quoc V Le.

</span>
<span class="ltx_bibblock">Selfie: Self-supervised pretraining for image embedding.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1906.02940</span>, 2019.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang <span id="bib.bib42.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and
Yasaman Khazaeni.

</span>
<span class="ltx_bibblock">Federated learning with matched averaging.

</span>
<span class="ltx_bibblock">In <span id="bib.bib42.3.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2020.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang <span id="bib.bib43.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2018]</span>
<span class="ltx_bibblock">
Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas
Kong, Daniel Ramage, and Françoise Beaufays.

</span>
<span class="ltx_bibblock">Applied federated learning: Improving google keyboard query
suggestions.

</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.02903</span>, 2018.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang <span id="bib.bib44.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2019]</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</span>,
10(2):12, 2019.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang <span id="bib.bib45.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Qiang Yang, Yu Zhang, Wenyuan Dai, and Sinno Jialin Pan.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic">Transfer learning</span>.

</span>
<span class="ltx_bibblock">Cambridge University Press, 2020.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang [2019]</span>
<span class="ltx_bibblock">
Qiang Yang.

</span>
<span class="ltx_bibblock">Federated recommendation systems.

</span>
<span class="ltx_bibblock">In <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">2019 IEEE International Conference on Big Data (Big Data)</span>,
pages 1–1. IEEE, 2019.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu <span id="bib.bib47.2.2.1" class="ltx_text ltx_font_italic">et al.</span> [2020]</span>
<span class="ltx_bibblock">
Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov.

</span>
<span class="ltx_bibblock">Salvaging federated learning by local adaptation.

</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.04758</span>, 2020.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu [2005]</span>
<span class="ltx_bibblock">
Xiaojin Jerry Zhu.

</span>
<span class="ltx_bibblock">Semi-supervised learning literature survey.

</span>
<span class="ltx_bibblock">Technical report, University of Wisconsin-Madison Department of
Computer Sciences, 2005.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2002.11544" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2002.11545" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2002.11545">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2002.11545" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2002.11546" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 11:50:21 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
