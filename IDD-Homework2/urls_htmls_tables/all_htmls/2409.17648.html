<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Efficient In-Domain Question Answering for Resource-Constrained Environments</title>
<!--Generated on Mon Sep 30 22:50:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.17648v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#S1" title="In Efficient In-Domain Question Answering for Resource-Constrained Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#S2" title="In Efficient In-Domain Question Answering for Resource-Constrained Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ Efficient In-Domain Question Answering for Resource-Constrained Environments"><span class="ltx_text ltx_ref_title">Retrieval Augmented Generation (RAG)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#S2.SS0.SSS0.Px2" title="In 2 Related Work ‣ Efficient In-Domain Question Answering for Resource-Constrained Environments"><span class="ltx_text ltx_ref_title">Fine tuning for RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#S2.SS0.SSS0.Px3" title="In 2 Related Work ‣ Efficient In-Domain Question Answering for Resource-Constrained Environments"><span class="ltx_text ltx_ref_title">Parameter Efficient Finetuning (PEFT)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#S3" title="In Efficient In-Domain Question Answering for Resource-Constrained Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#S4" title="In Efficient In-Domain Question Answering for Resource-Constrained Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#S4.SS1" title="In 4 Experiments ‣ Efficient In-Domain Question Answering for Resource-Constrained Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#S4.SS2" title="In 4 Experiments ‣ Efficient In-Domain Question Answering for Resource-Constrained Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Baselines</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#S5" title="In Efficient In-Domain Question Answering for Resource-Constrained Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#S5.SS0.SSS0.Px1" title="In 5 Results ‣ Efficient In-Domain Question Answering for Resource-Constrained Environments"><span class="ltx_text ltx_ref_title">Single-Hop QA vs Multi-Hop QA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#S5.SS0.SSS0.Px2" title="In 5 Results ‣ Efficient In-Domain Question Answering for Resource-Constrained Environments"><span class="ltx_text ltx_ref_title">Comparison of RAFT and CRAFT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#S6" title="In Efficient In-Domain Question Answering for Resource-Constrained Environments"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusions and Future Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Efficient In-Domain Question Answering for Resource-Constrained Environments</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Isaac Chung
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Phat Vo
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Arman Kizilkale
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aaron Reite 
<br class="ltx_break"/>
<br class="ltx_break"/>Clarifai Inc.
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">first.last@clarifai.com</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Retrieval Augmented Generation (RAG) is a common method for integrating external knowledge into pretrained Large Language Models (LLMs) to enhance accuracy and relevancy in question answering (QA) tasks. However, prompt engineering and resource efficiency remain significant bottlenecks in developing optimal and robust RAG solutions for real-world QA applications. Recent studies have shown success in using fine tuning to address these problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5. The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution, yet remains an unexplored area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage requirements and gain faster inference times while maintaining comparable RAG performance. This results in a more compute-efficient RAFT, or <span class="ltx_text ltx_font_bold" id="id2.id1.1">CRAFT</span>, which is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited.</p>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span>Equal contributions.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In this paper, we propose <span class="ltx_text ltx_font_bold" id="S1.p1.1.1">CRAFT</span>, a resource efficient approach to enhance in-domain Question Answering (QA) tasks in resource-constrained environments by combining Retrieval Augmented Fine Tuning (RAFT) <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib26" title="">2024b</a>)</cite> and Parameter-Efficient Fine Tuning (PEFT) methods, specifically Low-Rank Adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib7" title="">2021</a>)</cite> .
Resource-constrained environments, where systems may be isolated from unsecured networks (or "air-gapped") for security or privacy reasons are prevalent in sectors such as government, healthcare, and finance <cite class="ltx_cite ltx_citemacro_cite">Byres (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib2" title="">2013</a>)</cite>. These settings pose significant challenges for deploying machine learning models due to restricted internet access and limited computational resources. Addressing these challenges is crucial to enable the practical application of advanced QA systems in such critical domains.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In-domain QA tasks are useful for extracting relevant information from vast datasets specific to a particular field. However, the deployment of QA models in resource-constrained environments faces significant limitations. Existing QA models typically demand high computational power for both training and inference and rely on externally-hosted models requiring continuous internet access <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib1" title="">2020</a>); OpenAI et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib17" title="">2024</a>); Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib5" title="">2023</a>)</cite>. These dependencies hinder the deployment of performant QA models in resource-constrained settings, where hardware capabilities are often limited and connectivity is non-existent.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To overcome these challenges, we explore the integration of RAFT and LoRA. RAFT is a technique that combines fine tuning with information retrieval, allowing the LLM to more effectively answer questions using relevant content from the retrieved data. In resource-constrained settings, RAFT compensates for reduced model capacity by expanding the model’s contextual knowledge while simultaneously reducing the model’s vulnerability to incorrect retrievals.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">On the other hand, LoRA <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib7" title="">2021</a>)</cite> is a PEFT technique designed to fine tune models efficiently by training lightweight adapters that are added into the frozen model. LoRA adapters consist of a much smaller number of trainable parameters compared to the full model, ensuring efficient storage and fine tuning while maintaining or even improving performance on QA tasks compared to fine tuning the full model (hereafter referred to as supervised fine tuning, or SFT). Furthermore, different or custom LoRA adapters can be dynamically replaced in a LLM residing in an inference server’s memory immediately prior to inference, a process referred to as adapter swapping. Both RAFT and LoRA are discussed in further detail in Section 2. The individual benefits of RAFT and LoRA suggest that their combination could provide a synergistic advantage, leading to RAG systems that are both effective, resource-efficient, and highly customizable via adapter swapping.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We focus our study on LLMs within the 7-8 billion parameter range, like Llama3 and Llama3.1 <cite class="ltx_cite ltx_citemacro_cite">Dubey et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib4" title="">2024</a>)</cite>. This specific range is chosen to balance the trade-offs between model size, performance, and resource requirements. Larger models like GPT-4 <cite class="ltx_cite ltx_citemacro_cite">OpenAI et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib17" title="">2024</a>)</cite> generally offer better performance but at the cost of increased computational demands, which are impractical in resource-constrained environments, if not impossible, to access due to firewalls or connectivity. Conversely, smaller models may not provide the necessary performance for complex QA tasks in regular RAG systems. The 7-8B parameter range strikes an optimal balance for RAFT systems, offering sufficient capability while remaining feasible for deployment in resource-constrained settings.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The primary objectives of this research are to reduce training requirements, achieve faster inference times, enable adapter swapping, and maintain or improve the performance of LLMs in RAG systems. By combining RAFT and LoRA methods, we aim to create a framework that addresses the specific needs of resource-constrained environments without compromising on the quality of the QA system.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">To assess the effectiveness of our proposed models, we evaluate them on both their QA performance and resource utilization. The choice of these metrics ensures a comprehensive evaluation of both the efficiency and reliability of the models.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">We release all models and generated datasets to facilitate further study on HuggingFace <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://huggingface.co/collections/phatvo/raft-66c44a18ac74db25de87a4dc</span></span></span>. In Section 2, we present some related work on RAG, fine tuning methods for RAG including RAFT, and PEFT techniques such as LoRA. In Section 3, we introduce our compute-efficient RAFT method, CRAFT, in detail. In Section 4, we present the experiment setup. In Section 5 we report the results, and in Section 6 we conclude the paper.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Retrieval Augmented Generation (RAG)</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">RAG <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib12" title="">2021</a>)</cite> enhances LLMs by retrieving relevant document chunks from external knowledge bases through semantic similarity calculations. This method mitigates the generation of factually incorrect content by referencing external knowledge rather than relying solely on knowledge the model learned during training, thereby improving the relevancy of the generated text while reducing "hallucinations". Despite its advantages, RAG faces challenges, particularly with domain-specific or knowledge-intensive tasks, particularly when handling queries beyond the scope of its retrieved data <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib27" title="">2023</a>)</cite>, though at a lesser extent when compared to non-retrieval-augmented LLMs. Other major challenges with RAG includes requiring a high-performing retriever model to produce representative embeddings from the document chunks and retrieval system that balances scale and accuracy. Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential <cite class="ltx_cite ltx_citemacro_cite">Yan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib23" title="">2024</a>)</cite>. RAG excels in dynamic environments by offering real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with higher latency and the possibility of added noise from extraneous contexts.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Fine tuning for RAG</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib13" title="">2024</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib22" title="">2024</a>)</cite> have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever.
RAFT <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib26" title="">2024b</a>)</cite> includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning. The entire target domain is chunked into a smaller collection of documents by a fixed token count. For each document, a larger, highly performant LLM, usually different from the LLM to be fine tuned, is used to generate a question that can be answered using the document. A Chain-of-thought (CoT)<cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib20" title="">2023</a>)</cite> style answer is then generated by providing the same, larger LLM with the context of the document and question from the previous step. This response has a full reasoning chain, i.e. a series of intermediate reasoning steps, and clearly cited sources. Then the model is fine tuned via standard supervised techniques using the generated QA pairs as well as the original, or "golden" document for context, intermixed with irrelevant or "distractor" documents. In some training instances, the "golden" document is intentionally left out, which motivates the model to directly memorize answers from the target data during fine tuning instead of relying exclusively on the provided context. This increases performance and results in a model less susceptible to incorrect RAG retrievals. Our work follows the RAFT strategy but employs LoRA instead of SFT.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Parameter Efficient Finetuning (PEFT)</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.6">PEFT is a class of methods to adapt pre-trained large models to specific tasks or domains by fine tuning a much smaller set of parameters compared to SFT <cite class="ltx_cite ltx_citemacro_cite">Han et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib6" title="">2024</a>)</cite> and may be used to enhance QA performance with or without RAG. PEFT involves selectively fine tuning a small proportion of model parameters to adapt it to a specific task or domain, or introducing new trainable parameters to the model while keeping the rest frozen. LoRA <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib7" title="">2021</a>)</cite> is a notable PEFT technique that significantly decreases computational burden while maintaining performance on a wide array of tasks comparable to SFT <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib7" title="">2021</a>); Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib25" title="">2024a</a>)</cite>. LoRA consists of a set of pairs of low-rank matrices of size <math alttext="n\times k" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.1.m1.1"><semantics id="S2.SS0.SSS0.Px3.p1.1.m1.1a"><mrow id="S2.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.2" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml">n</mi><mo id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">×</mo><mi id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.3" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1"><times id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.1"></times><ci id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.2">𝑛</ci><ci id="S2.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S2.SS0.SSS0.Px3.p1.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.1.m1.1c">n\times k</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.1.m1.1d">italic_n × italic_k</annotation></semantics></math> and <math alttext="k\times n" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.2.m2.1"><semantics id="S2.SS0.SSS0.Px3.p1.2.m2.1a"><mrow id="S2.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="S2.SS0.SSS0.Px3.p1.2.m2.1.1.2" xref="S2.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml">k</mi><mo id="S2.SS0.SSS0.Px3.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">×</mo><mi id="S2.SS0.SSS0.Px3.p1.2.m2.1.1.3" xref="S2.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.2.m2.1b"><apply id="S2.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.2.m2.1.1"><times id="S2.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.2.m2.1.1.1"></times><ci id="S2.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S2.SS0.SSS0.Px3.p1.2.m2.1.1.2">𝑘</ci><ci id="S2.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="S2.SS0.SSS0.Px3.p1.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.2.m2.1c">k\times n</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.2.m2.1d">italic_k × italic_n</annotation></semantics></math>, where <math alttext="k&lt;&lt;n" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.3.m3.1"><semantics id="S2.SS0.SSS0.Px3.p1.3.m3.1a"><mrow id="S2.SS0.SSS0.Px3.p1.3.m3.1.1" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.cmml"><mi id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.2" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml">k</mi><mo id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml">&lt;&lt;</mo><mi id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.3.m3.1b"><apply id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1"><csymbol cd="latexml" id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.1">much-less-than</csymbol><ci id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.2">𝑘</ci><ci id="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="S2.SS0.SSS0.Px3.p1.3.m3.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.3.m3.1c">k&lt;&lt;n</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.3.m3.1d">italic_k &lt; &lt; italic_n</annotation></semantics></math>. The multiple of each pair is added into a single corresponding matrix of size <math alttext="n\times n" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.4.m4.1"><semantics id="S2.SS0.SSS0.Px3.p1.4.m4.1a"><mrow id="S2.SS0.SSS0.Px3.p1.4.m4.1.1" xref="S2.SS0.SSS0.Px3.p1.4.m4.1.1.cmml"><mi id="S2.SS0.SSS0.Px3.p1.4.m4.1.1.2" xref="S2.SS0.SSS0.Px3.p1.4.m4.1.1.2.cmml">n</mi><mo id="S2.SS0.SSS0.Px3.p1.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">×</mo><mi id="S2.SS0.SSS0.Px3.p1.4.m4.1.1.3" xref="S2.SS0.SSS0.Px3.p1.4.m4.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.4.m4.1b"><apply id="S2.SS0.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.1.1"><times id="S2.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.1.1.1"></times><ci id="S2.SS0.SSS0.Px3.p1.4.m4.1.1.2.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.1.1.2">𝑛</ci><ci id="S2.SS0.SSS0.Px3.p1.4.m4.1.1.3.cmml" xref="S2.SS0.SSS0.Px3.p1.4.m4.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.4.m4.1c">n\times n</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.4.m4.1d">italic_n × italic_n</annotation></semantics></math> within the frozen transformer model. LoRA adapters may be added into any subset of matrices in the transformer while maintaining differentiability, allowing for efficient fine-tuning as each pair contains a total of <math alttext="2(n\times k)" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.5.m5.1"><semantics id="S2.SS0.SSS0.Px3.p1.5.m5.1a"><mrow id="S2.SS0.SSS0.Px3.p1.5.m5.1.1" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.cmml"><mn id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.3" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.3.cmml">2</mn><mo id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.2" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.2.cmml">⁢</mo><mrow id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.cmml"><mo id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.2" stretchy="false" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.cmml"><mi id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.2" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.2.cmml">n</mi><mo id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.cmml">×</mo><mi id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.3" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.3.cmml">k</mi></mrow><mo id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.3" stretchy="false" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.5.m5.1b"><apply id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1"><times id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.2.cmml" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.2"></times><cn id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.3.cmml" type="integer" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.3">2</cn><apply id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1"><times id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.1"></times><ci id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.2.cmml" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.2">𝑛</ci><ci id="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.3.cmml" xref="S2.SS0.SSS0.Px3.p1.5.m5.1.1.1.1.1.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.5.m5.1c">2(n\times k)</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.5.m5.1d">2 ( italic_n × italic_k )</annotation></semantics></math> trainable parameters, much fewer than the <math alttext="n^{2}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px3.p1.6.m6.1"><semantics id="S2.SS0.SSS0.Px3.p1.6.m6.1a"><msup id="S2.SS0.SSS0.Px3.p1.6.m6.1.1" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1.cmml"><mi id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.2" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1.2.cmml">n</mi><mn id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px3.p1.6.m6.1b"><apply id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.1.cmml" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1">superscript</csymbol><ci id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.2.cmml" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1.2">𝑛</ci><cn id="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3.cmml" type="integer" xref="S2.SS0.SSS0.Px3.p1.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px3.p1.6.m6.1c">n^{2}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px3.p1.6.m6.1d">italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> parameters of the frozen matrix. This approach significantly reduces memory usage and computational requirements compared to fine tuning the full model, making it suitable for adapting LLMs to a new domain in resource-constrained environments with limited resources. Moreover, multiple LoRA adapters may be trained on a single frozen LLM: for example, different LoRA adapters may be trained for different datasets, or even users, and then quickly added into a frozen LLM already in an inference servers’ memory, a process referred to as adapter swapping. In contrast, fully fine tuning the entire LLM for a multitude of differing use cases requires much more compute and storage, and swapping an entire LLM into an inference server’s memory prior to inference introduces unacceptable latency when compared to adapter swapping.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we detail our method to train a LoRA adapter for RAFT. We largely follow the setup described in RAFT <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib26" title="">2024b</a>)</cite> and report the major differences. We use the original recipe to generate training data from the target in-domain data; then we use LoRA to fine tune an adapter on the generated dataset instead of SFT. In the remainder, the term RAFT refers to a model fine tuned with SFT, while CRAFT refers to a model fine tuned with LoRA.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Fine Tuning:</span> At the data generation stage, we follow the RAFT recipe, but we substitute Llama3-70B-instruct for GPT-4 <cite class="ltx_cite ltx_citemacro_cite">OpenAI et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib17" title="">2024</a>)</cite> to generate one question per context that can be answered using that context. We chose Llama3-70B-instruct as a GPT-4 replacement as other proprietary models may not be accessible in resource-constrained environments. Since using this 70B model (FP16) for inference requires around 130GB, we used vLLM <cite class="ltx_cite ltx_citemacro_cite">Kwon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib11" title="">2023</a>)</cite> as the inference server over 4x48GB NVIDIA RTX 6000 Ada GPUs. Note that this inference server is only needed at the data generation stage and not in the fine tuning or inference stages.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We include 3 distractor documents instead of the 4 used in the original RAFT paper to further reduce the number of tokens used during training, resulting in slightly more efficiency. The golden and distractor documents, along with the question and the CoT answer, are then formatted to form one instruction.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.p4.1.1">LoRA:</span> We tune the hyperparameters of the LoRA training configurations. Specifically, we tune the rank of the adapters (<math alttext="r=[4,16,64]" class="ltx_Math" display="inline" id="S3.p4.1.m1.3"><semantics id="S3.p4.1.m1.3a"><mrow id="S3.p4.1.m1.3.4" xref="S3.p4.1.m1.3.4.cmml"><mi id="S3.p4.1.m1.3.4.2" xref="S3.p4.1.m1.3.4.2.cmml">r</mi><mo id="S3.p4.1.m1.3.4.1" xref="S3.p4.1.m1.3.4.1.cmml">=</mo><mrow id="S3.p4.1.m1.3.4.3.2" xref="S3.p4.1.m1.3.4.3.1.cmml"><mo id="S3.p4.1.m1.3.4.3.2.1" stretchy="false" xref="S3.p4.1.m1.3.4.3.1.cmml">[</mo><mn id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">4</mn><mo id="S3.p4.1.m1.3.4.3.2.2" xref="S3.p4.1.m1.3.4.3.1.cmml">,</mo><mn id="S3.p4.1.m1.2.2" xref="S3.p4.1.m1.2.2.cmml">16</mn><mo id="S3.p4.1.m1.3.4.3.2.3" xref="S3.p4.1.m1.3.4.3.1.cmml">,</mo><mn id="S3.p4.1.m1.3.3" xref="S3.p4.1.m1.3.3.cmml">64</mn><mo id="S3.p4.1.m1.3.4.3.2.4" stretchy="false" xref="S3.p4.1.m1.3.4.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.3b"><apply id="S3.p4.1.m1.3.4.cmml" xref="S3.p4.1.m1.3.4"><eq id="S3.p4.1.m1.3.4.1.cmml" xref="S3.p4.1.m1.3.4.1"></eq><ci id="S3.p4.1.m1.3.4.2.cmml" xref="S3.p4.1.m1.3.4.2">𝑟</ci><list id="S3.p4.1.m1.3.4.3.1.cmml" xref="S3.p4.1.m1.3.4.3.2"><cn id="S3.p4.1.m1.1.1.cmml" type="integer" xref="S3.p4.1.m1.1.1">4</cn><cn id="S3.p4.1.m1.2.2.cmml" type="integer" xref="S3.p4.1.m1.2.2">16</cn><cn id="S3.p4.1.m1.3.3.cmml" type="integer" xref="S3.p4.1.m1.3.3">64</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.3c">r=[4,16,64]</annotation><annotation encoding="application/x-llamapun" id="S3.p4.1.m1.3d">italic_r = [ 4 , 16 , 64 ]</annotation></semantics></math>) and report the best result in the Results section.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.p5.1.1">Model Selection:</span> We focus on pretrained LLMs within the 7-8B parameter range. These models offer a balanced trade-off between performance and computational feasibility, making them suitable for deployment in resource-constrained environments. As air-gapped environments may not be able to access external APIs, we focus our study on Llama3.1-8B-Instruct <cite class="ltx_cite ltx_citemacro_cite">Dubey et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib4" title="">2024</a>)</cite>, which can be deployed locally.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we describe our experimental setup and our results, offer a detailed analysis, and highlight the importance of PEFT in minimizing GPU memory usage during training for resource-constrained systems.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">For our experiments, we selected the following datasets:</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">HotPotQA dataset <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib24" title="">2018</a>)</cite>, which provides a diverse set of multi-hop QA pairs across various topics from Wikipedia. This differs from the other listed datasets here where the questions require finding and reasoning over multiple supporting documents to answer as opposed to only a single document (single-hop).</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">NarrativeQA <cite class="ltx_cite ltx_citemacro_cite">Kočiský et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib10" title="">2018</a>)</cite>, which provides QA pairs derived from stories based on books and movie scripts.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">NewsQA <cite class="ltx_cite ltx_citemacro_cite">Trischler et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib18" title="">2017</a>)</cite>, which provides QA pairs based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1">PubMedQA <cite class="ltx_cite ltx_citemacro_cite">Jin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib8" title="">2019</a>)</cite>, which provides QA pairs for reasoning over biomedical research texts. It mainly focuses on answering yes/no medical and biology questions based on a given set of documents.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1">WebGLM-QA <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib14" title="">2023</a>)</cite>, a long-formed and properly cited QA dataset curated via LLM in-context bootstrapping to train the WebGLM generator module.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Data Preprocessing:</span> Each dataset is segmented into context chunks, each containing relevant information that could be used to generate question-answer pairs. All of the selected datasets already have pre-determined contexts.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Sample Selection:</span> To manage computational constraints, a sample of 100 chunks is selected for generating question-answer pairs for fine tuning. The evaluation set is also subsampled to only 1000 rows to speed up computations. This selection ensures that the training process remains feasible within the resource limitations of resource-constrained environments, allowing us to focus on optimizing the model’s performance without exceeding hardware capabilities.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Baselines</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We consider the following baselines for our experiments:</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i1.p1.1.1">Llama3.1-8B-Instruct + Golden</span> model represents an idealized RAG setup. In order to eliminate the effects of retrieval errors on the benchmarks the model is always provided with the golden context.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.I2.i2.p1.1.1">Llama3.1-8B-Instruct + RAG</span> model represents a realistic setup where the retriever is operational; hence golden context is not guaranteed to be in the context.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">For the RAG setup, we use a naive RAG pipeline, also known as a retrieve-read framework <cite class="ltx_cite ltx_citemacro_cite">Ma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib15" title="">2023</a>)</cite> that only involves retrieval and generation, and does not include any pre-/post-processing or advanced techniques. We use the <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p3.1.1">BAAI/bge-small-en-v1.5</span> text embedding model <cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib21" title="">2023</a>)</cite>, one of the top models on MTEB <cite class="ltx_cite ltx_citemacro_cite">Muennighoff et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib16" title="">2023</a>)</cite> with &lt;100M parameters, to generate embeddings. The index is generated with FAISS <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib9" title="">2019</a>)</cite> and deployed as the retriever in the RAG pipeline, with the <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p3.1.2">top_k</span> retrieval set to 5.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We find that the CRAFT (a LoRA/RAFT finetuned version of Llama3.1-8B-Instruct) is better at reading and extracting information from in-domain documents, compared to a general-purpose model with RAG.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1.2">HotPotQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1.3">NewsQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1.4">NarrativeQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1.5">PubMedQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1.6">WebGLM-QA</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.2.1.1">Llama3.1-8B + Golden</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.2.1.2">41.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.2.1.3">46.93</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.2.1.4">59.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.2.1.5">77.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.2.1.6">18.34</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.1.3.2.1">CRAFT + Golden</th>
<td class="ltx_td ltx_align_center" id="S5.T1.1.3.2.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.3.2.2.1">48.21</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.3.2.3.1">47.14</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.3.2.4"><span class="ltx_text ltx_font_bold" id="S5.T1.1.3.2.4.1">64.54</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.3.2.5"><span class="ltx_text ltx_font_bold" id="S5.T1.1.3.2.5.1">79.67</span></td>
<td class="ltx_td ltx_align_center" id="S5.T1.1.3.2.6"><span class="ltx_text ltx_font_bold" id="S5.T1.1.3.2.6.1">36.59</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.1.4.3.1">Llama3.1-8B + RAG</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.4.3.2">19.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.4.3.3">20.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.4.3.4">35.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.4.3.5">73.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.1.4.3.6">22.00</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T1.1.5.4.1">CRAFT + RAG</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.1.5.4.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.5.4.2.1">44.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.1.5.4.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.5.4.3.1">30.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.1.5.4.4"><span class="ltx_text ltx_font_bold" id="S5.T1.1.5.4.4.1">47.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.1.5.4.5"><span class="ltx_text ltx_font_bold" id="S5.T1.1.5.4.5.1">75.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.1.5.4.6"><span class="ltx_text ltx_font_bold" id="S5.T1.1.5.4.6.1">39.00</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>F1 scores on HotPotQA, NewsQA, NarrativeQA, PubMedQA, and WebGLM-QA comparing CRAFT and the baseline models: (i) an idealized RAG (Golden) and (ii) a realistic RAG. Bold numbers denote the best score in each comparison, where higher is better.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S5.T2.1.1.1.2">HotPotQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S5.T2.1.1.1.3">NewsQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S5.T2.1.1.1.4">NarrativeQA</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S5.T2.1.2.2.1"></th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.2.2.2">(a)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.2.2.3">(b)</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.2.2.4">(a)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.2.2.5">(b)</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.2.2.6">(a)</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.2.2.7">(b)</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.3.3.1">Trainable Params (M)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.3.2">8100</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.3.3.3">168</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.3.4">8100</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.3.3.5">168</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.3.6">8100</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.3.7">168</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.4.4.1">Training time (min)</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.2">180</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.4.4.3">9</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.4">240</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.4.4.5">41</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.6">100</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.7">19</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.5.5.1">GPU Memory (GB)</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.2">40.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.5.5.3">21.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.4">40.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.5.5.5">32.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.6">40.5</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.7">26.0</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T2.1.6.6.1">Perplexity</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.1.6.6.2">1.20</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.1.6.6.3">1.24</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.1.6.6.4">1.075</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.1.6.6.5">1.12</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.1.6.6.6">1.65</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.1.6.6.7">1.57</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of a) RAFT and b) CRAFT using generated RAFT data from HotPotQA, NewsQA, and NarrativeQA datasets. We also report perplexity over selected datasets. Lower perplexity is better. </figcaption>
</figure>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Using the above datasets and baselines, we evaluate our proposed method and demonstrate the effectiveness of CRAFT in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#S5.T1" title="Table 1 ‣ 5 Results ‣ Efficient In-Domain Question Answering for Resource-Constrained Environments"><span class="ltx_text ltx_ref_tag">1</span></a>.
Overall, the Llama3.1-8B-instruct model performs well due to its pre-training and instruction-tuned answering style.
We see that CRAFT consistently outperforms the baselines. Compared to the Llama3.1-8B-instruct model, CRAFT does much better in terms of extracting information, yielding an average of 13.41% gains over the evaluated datasets.</p>
</div>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Single-Hop QA vs Multi-Hop QA</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">CRAFT enhances QA scenarious requiring multi-hop reasoning substantially more than those requiring only single-hop reasoning. The performance gains over the baseline model when provided the golden context are 14.9% for HotPotQA, 4.47% for NewsQA, and 8.93% for NarrativeQA. These gains are dramatically amplified in RAG scenarios, to 128%, 44.7% and 35.6% for HotPotQA, NewsQA, and NarrativeQA respectively.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Comparison of RAFT and CRAFT</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">conduct an analysis to illustrate the resource efficiencies achieved by using LoRA compared to SFT for fine tuning on the same generated dataset. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#S5.T2" title="Table 2 ‣ 5 Results ‣ Efficient In-Domain Question Answering for Resource-Constrained Environments"><span class="ltx_text ltx_ref_tag">2</span></a>, using CRAFT reduces the number of trainable parameters to just 2% of those required by SFT. This results in a nearly 35% decrease in GPU memory usage during training and an average of speedup of 7.5x. In terms of perplexity, for HotPotQA and NewsQA, SFT offered marginally lower perplexity (by 4 and 0.045, respectively), where LoRA achieved a lower perplexity for NarrativeQA (by 0.08). Overall, the average perplexity between the two methods is nearly identical, indicating very similar model performance.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions and Future Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we introduced CRAFT, a method that combines RAFT and LoRA to efficiently adapt LLMs in resource-constrained environments while still retaining competitive performance in knowledge-intensive QA tasks.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Notably, CRAFT still requires a relatively large LLM for generating questions and CoT answers. A quantized model can certainly reduce memory requirements. For example, a 4-bit quantized Llama3-70B-instruct model would only require around 35GB and would fit in a 1x48GB setting.
A potential avenue to avoid using any large models could be an ensemble of smaller models using an mixture-of-agents approach <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib19" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Investigations on how to combine retrieval augmented fine tuning methods and methods that further reduce memory usage for fine tuning or leverage quantization, such as QLORA <cite class="ltx_cite ltx_citemacro_cite">Dettmers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.17648v2#bib.bib3" title="">2023</a>)</cite>, is left for future work.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We would like to thank Mark Lowell for providing feedback on the paper.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, et al. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2005.14165" title="">Language models are few-shot learners</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Byres (2013)</span>
<span class="ltx_bibblock">
Eric Byres. 2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/2492007.2492018" title="">The air gap: Scada’s enduring security myth</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Commun. ACM</em>, 56(8):29–31.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et al. (2023)</span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.14314" title="">Qlora: Efficient finetuning of quantized llms</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, et al. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2407.21783" title="">The llama 3 herd of models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:266359151" title="">Retrieval-augmented generation for large language models: A survey</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. (2024)</span>
<span class="ltx_bibblock">
Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2403.14608" title="">Parameter-efficient fine-tuning for large models: A comprehensive survey</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2106.09685" title="">Lora: Low-rank adaptation of large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2019)</span>
<span class="ltx_bibblock">
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1259" title="">PubMedQA: A dataset for biomedical research question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 2567–2577, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2019)</span>
<span class="ltx_bibblock">
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.

</span>
<span class="ltx_bibblock">Billion-scale similarity search with GPUs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">IEEE Transactions on Big Data</em>, 7(3):535–547.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kočiský et al. (2018)</span>
<span class="ltx_bibblock">
Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00023" title="">The NarrativeQA reading comprehension challenge</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Transactions of the Association for Computational Linguistics</em>, 6:317–328.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al. (2023)</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2021)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2005.11401" title="">Retrieval-augmented generation for knowledge-intensive nlp tasks</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2024)</span>
<span class="ltx_bibblock">
Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.01352" title="">Ra-dit: Retrieval-augmented dual instruction tuning</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.07906" title="">Webglm: Towards an efficient web-enhanced question answering system with human preferences</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2023)</span>
<span class="ltx_bibblock">
Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.14283" title="">Query rewriting for retrieval-augmented large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et al. (2023)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2210.07316" title="">Mteb: Massive text embedding benchmark</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI et al. (2024)</span>
<span class="ltx_bibblock">
OpenAI, Josh Achiam, et al. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2303.08774" title="">Gpt-4 technical report</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trischler et al. (2017)</span>
<span class="ltx_bibblock">
Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1611.09830" title="">Newsqa: A machine comprehension dataset</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2406.04692" title="">Mixture-of-agents enhances large language model capabilities</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2023)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2201.11903" title="">Chain-of-thought prompting elicits reasoning in large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2023)</span>
<span class="ltx_bibblock">
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2309.07597" title="">C-pack: Packaged resources to advance general chinese embedding</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024)</span>
<span class="ltx_bibblock">
Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.03025" title="">Retrieval meets long context large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2024)</span>
<span class="ltx_bibblock">
Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2401.15884" title="">Corrective retrieval augmented generation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1809.09600" title="">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024a)</span>
<span class="ltx_bibblock">
Ruiyi Zhang, Rushi Qiang, Sai Ashish Somayajula, and Pengtao Xie. 2024a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2403.09113" title="">Autolora: Automatically tuning matrix ranks in low-rank adaptation based on meta learning</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024b)</span>
<span class="ltx_bibblock">
Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E. Gonzalez. 2024b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2403.10131" title="">Raft: Adapting language model to domain specific rag</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2309.01219" title="">Siren’s song in the ai ocean: A survey on hallucination in large language models</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 30 22:50:47 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
