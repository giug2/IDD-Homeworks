<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.07805] Fairness and Privacy in Federated Learning and Their Implications in Healthcare</title><meta property="og:description" content="Currently, many contexts exist where distributed learning is difficult or otherwise constrained by security and communication limitations. One common domain where this is a consideration is in Healthcare where data is …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fairness and Privacy in Federated Learning and Their Implications in Healthcare">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Fairness and Privacy in Federated Learning and Their Implications in Healthcare">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.07805">

<!--Generated on Wed Feb 28 12:43:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated Learning,  Healthcare,  Medical Imaging,  Fairness
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Fairness and Privacy in Federated Learning and Their Implications in Healthcare</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Navya Annapareddy<sup id="id5.5.id1" class="ltx_sup"><span id="id5.5.id1.1" class="ltx_text ltx_font_italic">1,∗</span></sup>, Jade Preston<sup id="id6.6.id2" class="ltx_sup"><span id="id6.6.id2.1" class="ltx_text ltx_font_italic">1,∗</span></sup>, Judy Fox
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><sup id="id7.7.id1" class="ltx_sup"><span id="id7.7.id1.1" class="ltx_text ltx_font_italic">1</span></sup>School of Data Science
University of Virginia, Charlottesville, VA 
<br class="ltx_break"><sup id="id8.8.id2" class="ltx_sup"><span id="id8.8.id2.1" class="ltx_text ltx_font_italic">∗</span></sup>Authors contributed equally to this research
<br class="ltx_break">e-mail: {na3au, yry2jy, ckw9mp}@virginia.edu

</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">Currently, many contexts exist where distributed learning is difficult or otherwise constrained by security and communication limitations. One common domain where this is a consideration is in Healthcare where data is often governed by data-use-ordinances like HIPAA. On the other hand, larger sample sizes and shared data models are necessary to allow models to better generalize on account of the potential for more variability and balancing underrepresented classes.</p>
<p id="id10.id2" class="ltx_p">Federated learning is a type of distributed learning model that allows data to be trained in a decentralized manner. This, in turn, addresses data security, privacy, and vulnerability considerations as data itself is not shared across a given learning network nodes. Three main challenges to federated learning include node data is not independent and identically distributed (iid), clients requiring high levels of communication overhead between peers, and there is the heterogeneity of different clients within a network with respect to dataset bias and size. As the field has grown, the notion of fairness in federated learning has also been introduced through novel implementations. Fairness approaches differ from the standard form of federated learning and also have distinct challenges and considerations for the healthcare domain. This paper endeavors to outline the typical lifecycle of fair federated learning in research as well as provide an updated taxonomy to account for the current state of fairness in implementations. Lastly, this paper provides added insight into the implications and challenges of implementing and supporting fairness in federated learning in the healthcare domain.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated Learning, Healthcare, Medical Imaging, Fairness

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Privacy and security have been a growing concern with Machine Learning systems. Having huge amounts of data in the same place is a security risk. Therefore, many domains such as healthcare regulate the centralization of data. Addressing this concern of centrally located data and privacy, McMahan et al. developed a concept called federated learning (FL). FL responds to the security issue of centralized information by having datasets at separate locations. It is a system where decentralized clients load and train data on their own devices and communicate their weights and predictions to a global server. Therefore, FL has been used to respond to security issues arising from clinical datasets. Though FL is prime for the incorporation of many devices, it has four challenges listed by McMahan et al. To start, the data in a FL framework is not independent and identically distributed because each device has different data types and represents a subset of the entire population. Additionally, some clients have more data than others resulting in an imbalance of information. Thirdly, there is an abundance of device or node connectivity - at times client participation is in the millions. Lastly, because of the large number of clients, communication between devices is difficult and the “cost” to the framework increases. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite></p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS1.4.1.1" class="ltx_text">I-A</span> </span><span id="S1.SS1.5.2" class="ltx_text ltx_font_italic">Fairness in Federated Learning</span>
</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Unfairness in FL can come from the way the clients are chosen, the methodology used to optimize the model, the way the clients are rewarded for contributions and the means by which the contributions to the model are defined. Typically, when choosing clients, speed and model accuracy are considered which can cause unfairness because some clients are always chosen, some are seldom chosen, and some are never chosen. Implementing fair FL techniques to individual clients can create an unfair federated model because of the lack of homogeneity of the clients and differences of data distribution for protected classes.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">To combat unfairness within their FL frameworks many studies incorporate fairness tactics into their datasets or methodology; these tactics are driven by underlying ideas of how unfairness is brought into the framework. Some authors strive to ensure that each group within their system achieves the same or close to the same level or performance accuracy. Others seek to achieve fairer models through ensuring every group receives the same rewards and incentives no matter their contribution to the model. Continuing with this idea, some authors work to balance return of rewards for all groups. Rather than on the back-end of the study with rewards, others believe ensuring fairness begins with client submission to the study and make an effort to motivate a diverse participation rate. Some authors believe that fairness is defined by the implementations within the algorithm; they work to modify weights or assumptions in the model setup to ensure every group receives appropriate metrics. Other researchers singularly strive to mitigate the loss within protected groups.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite></p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS2.4.1.1" class="ltx_text">I-B</span> </span><span id="S1.SS2.5.2" class="ltx_text ltx_font_italic">Fair Federated Learning Assumptions</span>
</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Shi et al. outline nine different assumptions to fair FL: client data has privacy requirements, clients and servers are “self-serving”, logical, and honest, invitations to join FL are always accepted, clients always have prior information, and their data never changes, clients reserve all their resources for FL, and there is only one FL server operating. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite></p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS3.4.1.1" class="ltx_text">I-C</span> </span><span id="S1.SS3.5.2" class="ltx_text ltx_font_italic">Federated Learning in Healthcare</span>
</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">The healthcare domain is marked by special considerations for security and privacy that machine learning in healthcare (MLHC), including FL, must address. Specifically, most institutions subscribe to by ordinances that ensure patient privacy and confidentiality, commonly through compliance fixtures like the Healthcare Insurance Portability and Accountability Act (HIPAA). Even excluding such ordinances, it is generally preferable in healthcare contexts to isolate patient data from external access to preserve confidentiality and limit liability from unwanted access outside of clinical providers within a specific institution. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> This creates difficulty in conducting multi organizational or longitudinal studies as clinical information is siloed. This type of expansion is necessary as clinical studies often focus on a limited set of inclusion criteria leading to smaller sample sizes and a higher chance of carrying institutional bias if the data scope is limited.
Thus, FL for healthcare systems is proposed as a method to enable institutional collaboration and enable secure sharing of model parameters without compromising patient confidentiality. Notably, the resulting shared model weights are free of personally identifiable information (PII) as well as patient demographics.
Implementations of FL in healthcare typically address the imbalanced nature of clinical patient samples as well as bias within different groups. Notably Quy et al. found significant demographic bias when performing a broad benchmark on clinical data ranging from signal data to medical imaging results. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
Considerations for the use and fairness of FL in healthcare also must consider the secure environments within data exchanges must often occur. These can range from trusted execution environments (TEE) to secure encryption and handshake protocols that must be integrated into FL mechanisms for use in healthcare contexts. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></p>
</div>
</section>
<section id="S1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS4.4.1.1" class="ltx_text">I-D</span> </span><span id="S1.SS4.5.2" class="ltx_text ltx_font_italic">Gaps in Research</span>
</h3>

<div id="S1.SS4.p1" class="ltx_para">
<p id="S1.SS4.p1.1" class="ltx_p">There are four significant gaps within the research. Categorization of existing implementation are metric and resource allocation based. Fairness surveys focus on tabular data and ultimately lack considerations for image and high dimensional datasets, including ones commonly or exclusively found in healthcare such as signal and medical imaging datasets. Additionally, there has been no comprehensive lifecyle view to-date with an emphasis on fair FL. This makes it difficult to identify model process considerations or opportunities for security and privacy preservation across client nodes and servers. Lastly, there is no existing surveys with a specific focus on the relationship between fair FL implementations and its usage in healthcare context.</p>
</div>
<div id="S1.SS4.p2" class="ltx_para">
<p id="S1.SS4.p2.1" class="ltx_p">Therefore the contributions of this paper will include: 1) an updated survey of current federated learning literature and implementations which incorporate fairness, 2) an updated taxonomy of fairness approaches resulting from the before mentioned survey, 3) a discrete mapping of a generalized FL model lifecyle process, and 4) provided insights on the healthcare implications drawn from the research completed in this paper.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Fairness Mechanisms in Federated Learning</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Recognizing the potential for improvements to fairness in ML using FL, many authors developed studies to address the concern. Shi et al developed a detailed classification for model fairness approaches. The outlined high level categories are as follows: client section, model optimization, contribution evaluation and incentive mechanism. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> These approaches as well as a few others will be expounded within this paper.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Quy et al. identified several fairness approaches that were not included in the taxonomy approach listed by Shi et al. such as multi-discrimination, temporal fairness, and distributional fairness. In their paper ‘A survey on datasets for fairness-aware machine learning’ Quy et al. reviewed domain considerations of fairness. Specifically, they examine bias in FL datasets in the financial, health, and social domains. In each domain, the authors explored challenges to fairness such as data cardinality, imbalance, and the validity of assumptions of independence. Quy et al. concluded that: fairness-centric research defines fairness differently with every study/datasets, careful evaluation of protected classes needs to be conducted, and newer datasets need to be implemented to remain relevant. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">We will later discuss how the following papers should be classified in terms of fairness approaches. Huang et al. authored a paper dealing specifically with FL and an incorporation of fairness, ‘Fairness and Accuracy in Federated Learning’- introducing FedFa. FedFa is a FL optimization algorithm incorporating a double momentum gradient methodology and fairness weighting scheme. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> In another paper ’Ditto: Fair and Robust Federated Learning Through Personalization’, Li et al. asserts that robustness and fairness are competing concepts in many FL frameworks. They assert that recent papers attempt to ensure fairness while also remaining robust, but don’t realize that fairness and robustness are in fact competing concepts. By addressing this idea the authors introduce a methodology with an objective that makes FL more personalized.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
Hu et al. propose a different methodology for FL that adheres to group fairness requirements. They do this by expanding on the concept of bounded group loss (BGL). BGL is a methodology in which the average loss across all data points for each group are held the same. Continuing with this, the authors introduce an optimization formulation guaranteeing adherence to implemented group fairness requirements.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2308.07805/assets/lifecycle.png" id="S2.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="312" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Discrete generalized federated learning lifecycle with client node level and server level processes and interactions. After model training and inference, key client server interactions include sending and downloading weights during aggregation steps.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Fairness in Context of Healthcare</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Fairness in healthcare is a broad concept that spans considerations for bias handling, representativeness, inclusion, generalizability, resource allocation, and disparity in results among the included groups. Specifically, given the history of algorithmic discrimination, MLHC must address both differential performance within and between patient groups as well as emergent fairness issues and appropriate responses.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Regulatory guidelines on defining and enforcing fairness within MLHC remain minimal. Notable guidelines include Standard Protocol Items: Recommendations for Interventional Trials-AI (SPIRIT-AI) and Consolidated Standards of Reporting Trials-AI (CONSORT-AI) which both mention encourage exploring bias as a consideration of model fairness but do not offer any format definition or response guidelines for doing so. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite></p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">As engagement on fairness within MLHC remains preliminary, fairness within the context of FL in healthcare (FLHC) remains especially minimal.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Security and Privacy in Healthcare</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The use of FL in healthcare has unique security and privacy considerations at each step of the model lifecycle. Trusted execution environments (TEEs) in healthcare are typically constrained in communication capabilities, especially between external organizations leading to the use of specialized servers and communication protocols to exchange model weights or enable client server interactions. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">FLHC is also uniquely characterized by the participation of medical devices, wearable sensors, and Internet of Thing (IoT) enabled instruments. When learning infrastructure is split between client and server or multiple devices, the collaborative global model is vulnerable to attacks such as training-hijacking and targeted inversion attacks. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite></p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Differential privacy is highly relevant in healthcare contexts as it enables utilization of relevant features while minimizing available information on individuals in a group.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">Finally, healthcare, as a domain, hosts a wide variety of datasets ranging from continuous signal inputs, spatio-temporal metadata, and emitting instruments with minimal encryption or privacy enabled features. The lack of standard usage for FLHC remains a challenge as privacy implementations remain context specific rather than generalizable.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2308.07805/assets/taxonomy.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="290" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Generalized taxonomy of fairness approaches in federated learning (FAFL) detailing specific groups of high level classifications (client selection, model optimization, contribution evaluation, incentive mechanism, and spatio-temporal fairness) as well as each group’s representative implementations.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To better understand the FL model process, a generalized view of the fairness FL lifecycle stages is depicted in Figure 1. The model representation consists of two stages of the process driven by the client nodes and global server respectively. Events known as client-server interaction (CSIs) and sequential events are present in each sub-process.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Federated Learning Lifecycle</span>
</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.4.1.1" class="ltx_text">III-A</span>1 </span>Client Level</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">The lifecycle beginning at the node or client level begin very similarly across the studies we reviewed. Most federated frameworks have some form of model training at the node level. In ‘Fairness and Accuracy in Federated Learning’ by Huang et al., the authors implement a momentum gradient to train the model at the node level. They eventually incorporate a second momentum gradient a the server level resulting in faster convergence. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> ’Ditto: Fair and Robust Federated Learning Through Personalization’ and ’Fair Federated Learning via Bounded Group Loss’ are other examples of model training as part of the federated lifecycle. However, with these papers model training is conducted via minimizing an objective function. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">The next stage of the lifecycle at the node level usually involves the process of downloading weights. In ’A Reputation Mechanism Is All You Need: Collaborative Fairness and Adversarial Robustness in Federated Learning’, the clients download the most recent global gradient and incorporate it to their local calculations. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> Alternatively, with ’Ditto: Fair and Robust Federated Learning Through Personalization’, a random sample of nodes are selected and are sent global weights from the server.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> In addition to updating the weights,in ’Fair Federated Learning via Bounded Group Loss’, the node receives a global fairness constraint from the server.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">The final overarching stage in the lifecycle is sending the updated weights to the server. No matter the methodology and in every study reviewed, the final process has weights sent from the local client level to the server. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> An example of this stage can be observed in ’Federated Optimization in Heterogeneous Networks’ by Li et al. In this paper, the authors modify a well known FL implementation FedAvg introduced in the original FL paper by McMahan et al. To accomplish this, Li et al. introduce FedProx and use an alternative local objective to minimize the impact of local updates and account for heterogeneous systems. FedProx assumes each device has their individual workload rather than the same. To account for the dissimilarity when weights are updated each round of FL, the authors incorporate variables into the algorithm. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite></p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.4.1.1" class="ltx_text">III-A</span>2 </span>Server Level</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">FL schemes typically consist of transfer of local gradient and model updates from client nodes to a global server where aggregation steps and feedback loops between client and server can be enabled.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">Model aggregation is a two-fold process with global aggregation of weights and/or gradients occurring before global models are ensembled and propagated back to clients. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.1" class="ltx_p">Throughout this process, notions such as client contribution and participation are enforced. A given client’s contribution can be minimized or maximized on the basis of a reputation objective function, as introduced by Xu et al. through their novel adversarial robustness metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Federated Learning Taxonomy</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We propose a comprehensive generalized FL taxonomy from relevant literature to expand current hierarchical understandings of the field to newer fairness mechanisms. In doing so, we will also expand the scope of previous reviews to text, image, and other high dimensional implementations commonly found in healthcare. We aim to differentiate the fairness approach and update the earlier taxonomy or pursue other organization methods based on developments in the field. In conjunction with a generalized FL lifecycle view, we review security and scaling considerations in the context of fairness, such as the security of metric aggregation, model sending, and scalability mechanisms such as peer selection and addition. The combined use of these discrete views help to expand past the assumption of single fairness mechanism systems and instead provide a basis to build a more complex, multi-fairness mechanisms for federated learning approaches.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Fairness Notions</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Performance parity fairness</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Shi et al. defines performance parity as the requirement of model accuracy equality for each group. This means that within the FL framework each group (protected or unprotected) will have a basic assured level of accuracy. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> A close comparison to this would be with the paper by Hu et al, ’Fair Federated Learning via Bounded Group Loss’. Their methodology aims to satisfy global model fairness requirements through saddle point optimization and Conditional Bounded Group Loss (CBGL). The idea of CBGL is specifically novel to this paper because it expounds on the concept BGL. With BGL, researchers ensure each group meets a minimum fairness baseline but does not require their fairness score to be equal. Hu et al. proposed BGL because
(by observing other similar techniques) they concluded that ensuring equal fairness scores causes a drop in prediction accuracy across all the groups. The authors state that CBGL is similar to the idea of equalized odds – true and false positive rate is made equal for each group. However, instead of requiring equality, CBGL sets an upper bound for each group. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Good Intent Fairness</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Good intent notions of fairness typically introduce concepts to prevent overfitting and minimize model bias, especially towards sensitive or protected data features. One interpretation of a model assuming good intent is one that minimizes losses for individual client nodes equally to avoid overfitting the model for the benefit of a subset of nodes, thereby minimizing bias to larger or more influential clients in the network.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">One of the most significant implementations of good intent learning is the agnostic loss metric and corresponding agnostic federated learning (AFL) models.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Agnostic loss is a generalizable loss function able to optimize for a target distribution consisting of variable mixtures of clients, rather than a set distribution at runtime. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> AFL models are more robust, while traditional FL schemes assume distribution parity between target and client distributions. While this is true, AFL models have been found to be most effective for smaller FL networks and struggle to generalize accurately with large amounts of clients.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Contribution Fairness</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Contribution fairness is a concept where the focus has shifted from ensuring a certain level of accuracy across groups to rewarding clients for their contribution to the model. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> Xu et. al focuses on two important concepts in ’A Reputation Mechanism Is All You Need: Collaborative Fairness and Adversarial Robustness in Federated Learning’: collaborative fairness and adversarial robustness. Collaborative fairness is defined in the paper as the model’s ability to reward participants based on their individual and different contributions to the framework. Adversarial robustness is defined as the framework’s ability to block “free riders” or malicious participants from reaping the benefits of the global model rewards. To implement collaborative fairness, Xu et al. reward participants based on the test accuracy of the client’s model. Adversarial robustness was addressed based on detection of model poisoning and the potential for free riders. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite></p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Regret Fairness</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In FL, regret fairness aims to minimize the difference between a node’s expected input and the input it has actually received as well as the length at which the node has been in waiting for a given input.
This notion relates to incentive schemes in that nodes follow an objective function that balances waiting and latency with incentive payout. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite></p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.4.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.5.2" class="ltx_text ltx_font_italic">Expectation Fairness</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Similar to regret fairness, the notion of expectation fairness aims to minimize inequity in client nodes as incentive payouts occur. Unlike regret fairness, expectation fairness assumes a gradual incentive payout such expectation values can be calculated. Since these payouts happen at different periods rather than all at once, it is useful for FL schemes where incentive budgets are not fixed or otherwise variable. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite></p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS6.4.1.1" class="ltx_text">IV-F</span> </span><span id="S4.SS6.5.2" class="ltx_text ltx_font_italic">Spatio-temporal Fairness</span>
</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">In ‘A survey on datasets for fairness-aware machine learning’, Quy et al. include the term temporal into their fairness notions. Specifically they discuss datasets with temporal and spacial information that should be removed to ensure the study is fair for all groups. Mashhadi et al. surveys the current approaches and challenges to fairness for special temporal frameworks in ’Fairness in Federated Learning for Spatial-Temporal Applications’. The authors assert that due to the rise of smart devices, research interest has progressed in the study of trends in human mobility. Specifically, temporal and spatial fairness is a concern in response to the many problems with collecting large amounts of human mobility data in often unsecure data environments. There are privacy concerns to location information; this makes people less willing to share or upload their location. Second, the data repositories have missing or outdated information. Lastly, many of the datasets are discriminatory which would make any algorithm output biased toward certain groups. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Fairness Approaches</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Approaches to FAFL can also be classified discretely to standardize groups of implementations separate to fairness notions. This taxonomy depicted in Figure 2 outlines a hierarchy for the major schools of applications of fairness in federated learning as well as characteristic or relevant implementations.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Client Selection</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In an attempt to incorporate fairness Shi et al. suggests there are multiple approaches that fall into key categories. The first approach they classify is client selection. With client selection, nodes are allowed into the framework based on their potential. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> We assert that client selection is more accurately specified through additional specifications for client filtering and has strong ties to contribution evaluation through baseline constraints of accuracy or speed. With these contribution constraints, many clients are filtered out before the first round. Clients can also not only be rewarded but also removed every round of the study depending on their model performance. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite></p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Incentive Mechanisms</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Typical incentive schemes in FL distribute rewards (monetary and otherwise) to clients in a standardized manner. This form does not indicate the vast differences client nodes may have in terms of local computational and physical resources (e.g. memory, bandwidth, battery power, and communication capacity and speed) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite></p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Because the standard form of an incentive scheme ignores that variation and inconsistency in local
updates of clients can affect the characteristics of the global model, it mimics the free-rider problem and prevents high quality contributors from being represented.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Model Optimization</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Model optimization focuses on making changes to the algorithm to introduce fairness to the framework. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> Often the changes incorporated are the weights or some additional parameter. A great example of this concept is in ’Fairness and Accuracy in Federated Learning’. The authors Huang et al. modify the the FedAvg algorithm developed by McMahan et al. to generate FedFa. In their methodology, Huang et al. update the algorithm from employing stochastic gradient decent. They incorporate momentum into the gradient at both the system node and server level to speed up convergence. The second modification to FedAvg was the weighting system. With FedFa, weights were updated and applied every round of the FL. To obtain the weights a random subset of the clients was selected; the weights were then calculated by a combination of the clients training accuracy and the number of times the client was selected. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite></p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.4.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.5.2" class="ltx_text ltx_font_italic">Contribution Evaluation</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">FL approaches that consider contribution evaluation typically consider fairness in the node selection, client filtering, and reward distribution stages. This approach is also key for differential privacy, as information that exposes individual level features can be removed for the sake of de-identification. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite></p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">Contribution evaluation procedures are often based on features such as self reported sparsity, size, and weight improvements.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Discussion: Implications In Healthcare</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The use of fair FL in healthcare is governed by existing data usage and execution practices. While FL has the opportunity to increase in organizational collaboration by offering a secure channel for distributed weight sharing, it also has unique security challenges and overhead in communications.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Client selection is commonly a function of nodes receiving weights for their potential additives to the model and different clients from different locations will thus provide data features characteristic to their area or group’s demographics. Having a multitude of clients for various places will contribute to model accuracy for more groups within healthcare. Moreover, healthcare networks face frequent threats making the additional feedback loops and client server interactions (CSIs) necessary to enable aggregation methods like double momentum, local personalization, and reputation metrics less ideal than other approaches that minimize CSIs. On the other hand, these approaches incorporate essential elements of differential privacy by unburdening a given FL network from free rider or unhelpful clients and thus minimizing the use and propagation of unnecessary information.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Finally, an increasing amount of individual-level healthcare data is now sourced from wearable devices, sensors, or cloud enabled devices and smartphones that are responsible for tracking location and longitudinal health data. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> These edge devices are often less secured than traditional data collection devices and prone to vulnerabilities of users’ local networks which is further compounded by lack of standardized use and security norms.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Overall, FLHC will be enabled by further consideration low resourced edge devices, minimization of communication overhead and organizational silos, and standardization of client and peer selection practices when considering FL in the context of existing biases and fairness implications in healthcare.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we present a survey on the current state of fairness in federating learning as it pertains to healthcare. FAFL, especially in the healthcare domain, is not a zero-sum and as fairness approaches in federated learning evolve, approaches will converge to include more complex or multi-notion implementations. We expanded on existing taxonomy representations of FAFL by adding new fairness notions and considering implementations in the context of multiple categories and high level methodology groups. We also noted new FL paradigms such as bilevel optimization; this optimization is distinct from standard FL workflows in their continuous feedback loops between client and server. Finally, in this paper we also develop a discrete lifecycle representation of FL which can be paired with domain and execution environment knowledge to further model implementation and security considerations.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">FLHC is a natural extension of the standard FL architecture and is aligned with the healthcare domain’s focuses and ordinances pertaining to security, individual’s privacy, confidentiality, and group fairness. Applying FL to healthcare also brings many challenges to federated learning including high dimensionality, high communication overhead and complexity, and enforced security standards that may impair or otherwise complicate normal FL communication procedures. These challenges will continue to be relevant as the field of federated learning, fairness implementations, and their use in healthcare evolve.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors would like to thank the School of Data Science at the University of Virginia for their support during this research and providing access to publication catalogues. The authors also acknowledge the Research Computing at UVA for providing technical support that have contributed to the results reported within this publication.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Referenced code</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">All the referenced and supporting code and survey iterations will be stored within the corresponding GitHub repository (https://github.com/UVA-MLSys/DS7406/tree/main/Projects/Team%204).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
W. Huang, T. Li, D. Wang, S. Du, and J. Zhang, “Fairness and accuracy in
federated learning,” <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol. abs/2012.10069, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
G. H. Lee and S.-Y. Shin, “Federated learning on clinical benchmark data:
performance assessment,” <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Journal of medical Internet research</span>,
vol. 22, no. 10, p. e20891, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y. Shi, H. Yu, and C. Leung, “Towards fairness-aware federated learning,”
2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
T. L. Quy, A. Roy, V. Iosifidis, and E. Ntoutsi, “A survey on datasets for
fairness-aware machine learning,” <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol. abs/2110.00530, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M. A. Ahmad, A. Patel, C. Eckert, V. Kumar, and A. Teredesai, “Fairness in
machine learning for healthcare,” in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of the 26th acm sigkdd
international conference on knowledge discovery &amp; data mining</span>,
pp. 3529–3530, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
M. Javaid, A. Haleem, R. Pratap Singh, R. Suman, and S. Rab, “Significance
of machine learning in healthcare: Features, pillars and applications,” <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">International Journal of Intelligent Networks</span>, vol. 3, pp. 58–73, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
T. Li, S. Hu, A. Beirami, and V. Smith, “Ditto: Fair and robust federated
learning through personalization,” in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings of the 38th
International Conference on Machine Learning</span> (M. Meila and T. Zhang, eds.),
vol. 139 of <span id="bib.bib8.2.2" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>, pp. 6357–6368,
PMLR, 18–24 Jul 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S. Hu, Z. S. Wu, and V. Smith, “Fair federated learning via bounded group
loss,” 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
X. Liu, S. C. Rivera, D. Moher, M. J. Calvert, and A. K. Denniston, “Reporting
guidelines for clinical trial reports for interventions involving artificial
intelligence: the consort-ai extension,” <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">BMJ</span>, vol. 370, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. R. Javed, I. Razzak, and G. Xu, “Federated learning for privacy
preservation of healthcare data from smartphone-based side-channel attacks,”
<span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">IEEE journal of biomedical and health informatics</span>, vol. PP, 05 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
X. Xu and L. Lyu, “A reputation mechanism is all you need: Collaborative
fairness and adversarial robustness in federated learning,” 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith,
“Federated optimization in heterogeneous networks,” 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. Mohri, G. Sivek, and A. T. Suresh, “Agnostic federated learning,” <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol. abs/1902.00146, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. Fitzsimons, M. Osborne, and S. Roberts, “Intersectionality: Multiple group
fairness in expectation constraints,” 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A. Mashhadi, A. Kyllo, and R. M. Parizi, “Fairness in federated learning for
spatial-temporal applications,” 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
H. Yu, Z. Liu, Y. Liu, T. Chen, M. Cong, X. Weng, D. Niyato, and Q. Yang, “A
fairness-aware incentive scheme for federated learning,” in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proceedings
of the AAAI/ACM Conference on AI, Ethics, and Society</span>, AIES ’20, (New York,
NY, USA), p. 393–399, Association for Computing Machinery, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S. K. Shyn, D. Kim, and K. Kim, “Fedccea : A practical approach of client
contribution evaluation for federated learning,” 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.07804" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.07805" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.07805">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.07805" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.07806" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 12:43:42 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
