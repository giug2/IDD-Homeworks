{
    "id_table_1": {
        "caption": "TABLE I :  Total pipeline runtimes for one trial in  experiment A , where  | D t  r  a  i  n | = 1 , 000 subscript D t r a i n 1 000 |D_{train}|=1,000 | italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT | = 1 , 000  records of SNAKE data.",
        "table": "S4.T1.5",
        "footnotes": [],
        "references": [
            "As illustrated in Figure  1 , a synthetic data generator is trained on real data  D t  r  a  i  n subscript D t r a i n D_{train} italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT  and subsequently used to generate synthetic data  D s  y  n  t  h subscript D s y n t h D_{synth} italic_D start_POSTSUBSCRIPT italic_s italic_y italic_n italic_t italic_h end_POSTSUBSCRIPT . The goal of a membership inference attack (MIA) is to determine whether a given record was used to train the generator that produced the synthetic records without looking at the training data. We refer to the data records on which we make such inferences as targets,  D t  a  r  g  e  t subscript D t a r g e t D_{target} italic_D start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT . MIAs can be conducted in other contexts too, such as on the output of a machine learning model instead of a synthetic dataset  [ 30 ] . But in this paper, we focus on the former, known as Inference-on-Synthetic attacks  [ 16 ] .",
            "We set up the problem as follows (and as shown in Figure  1 ). Given a population, or auxiliary, tabular dataset  D a  u  x subscript D a u x D_{aux} italic_D start_POSTSUBSCRIPT italic_a italic_u italic_x end_POSTSUBSCRIPT , given an    \\varepsilon italic_ -DP SDG algorithm  A A \\mathcal{A} caligraphic_A  and a synthetic dataset  D s  y  n  t  h subscript D s y n t h D_{synth} italic_D start_POSTSUBSCRIPT italic_s italic_y italic_n italic_t italic_h end_POSTSUBSCRIPT  produced by running  A A \\mathcal{A} caligraphic_A  on training dataset  D t  r  a  i  n  D a  u  x subscript D t r a i n subscript D a u x D_{train}\\subset D_{aux} italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT  italic_D start_POSTSUBSCRIPT italic_a italic_u italic_x end_POSTSUBSCRIPT  (i.e.  A  ( D t  r  a  i  n ) = D s  y  n  t  h A subscript D t r a i n subscript D s y n t h \\mathcal{A}(D_{train})=D_{synth} caligraphic_A ( italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT ) = italic_D start_POSTSUBSCRIPT italic_s italic_y italic_n italic_t italic_h end_POSTSUBSCRIPT ), and given some subset of targets from the population data  D t  a  r  g  e  t  D a  u  x subscript D t a r g e t subscript D a u x D_{target}\\subset D_{aux} italic_D start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT  italic_D start_POSTSUBSCRIPT italic_a italic_u italic_x end_POSTSUBSCRIPT , the goal is to determine which of  D t  a  r  g  e  t subscript D t a r g e t D_{target} italic_D start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT  were members of the training data,  D m  e  m  b  e  r = D t  a  r  g  e  t  D t  r  a  i  n subscript D m e m b e r subscript D t a r g e t subscript D t r a i n D_{member}=D_{target}\\cap D_{train} italic_D start_POSTSUBSCRIPT italic_m italic_e italic_m italic_b italic_e italic_r end_POSTSUBSCRIPT = italic_D start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT  italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT . Always,  D t  r  a  i  n subscript D t r a i n D_{train} italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT  is hidden.",
            "This custom density estimation    \\zeta italic_  is depicted in Algorithm  1 . Lines 12 are made unique to each SDG algorithm (see Section  III-C ). The overall aggregation of focal-point measurements into    \\zeta italic_  is the same for all.",
            "During one shadow modelling run, we will observe that the amount of FPs chosen is  l  1 l 1 l-1 italic_l - 1 , where  l l l italic_l  is the number of features in  D t  r  a  i  n subscript D t r a i n D_{train} italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT , since thats how many edges are in a tree of  l l l italic_l  nodes. In Algorithm  1 , we input the counts of all focal-points  F F \\mathcal{F} caligraphic_F  observed during shadow modelling as weights,  w F subscript w F w_{\\mathbf{F}} italic_w start_POSTSUBSCRIPT bold_F end_POSTSUBSCRIPT  for  F  F F F \\mathbf{F}\\in\\mathcal{F} bold_F  caligraphic_F . So the total of all of the weights   F  F w F = 50  ( l  1 ) subscript F F subscript w F  50 l 1 \\sum_{\\mathbf{F}\\in\\mathcal{F}}w_{\\mathbf{F}}=50\\cdot(l-1)  start_POSTSUBSCRIPT bold_F  caligraphic_F end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT bold_F end_POSTSUBSCRIPT = 50  ( italic_l - 1 ) , with some FPs being observed more frequently than others.",
            "MAMA-MIA finds success against Private-GSD following the same process as before. We first simulate Private-GSD on subsets  D ^ t  r  a  i  n subscript ^ D t r a i n \\hat{D}_{train} over^ start_ARG italic_D end_ARG start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT  of  D a  u  x subscript D a u x D_{aux} italic_D start_POSTSUBSCRIPT italic_a italic_u italic_x end_POSTSUBSCRIPT  and record which FPs (i.e. marginals) are chosen each time. Across 50 runs, some FPs are chosen consistently and others are not. Once we have the FPs  F  F F F \\mathbf{F}\\in\\mathcal{F} bold_F  caligraphic_F  and their respective frequencies  w F subscript w F w_{\\mathbf{F}} italic_w start_POSTSUBSCRIPT bold_F end_POSTSUBSCRIPT , these can be inputted into    \\zeta italic_  in Algorithm  1 .",
            "An alternative    \\zeta italic_  Various aggregations based on measuring the predicted focal-points in  D s  y  n  t  h subscript D s y n t h D_{synth} italic_D start_POSTSUBSCRIPT italic_s italic_y italic_n italic_t italic_h end_POSTSUBSCRIPT  and  D a  u  x subscript D a u x D_{aux} italic_D start_POSTSUBSCRIPT italic_a italic_u italic_x end_POSTSUBSCRIPT  could be used for    \\zeta italic_ . One such aggregation that could replace line 5 of Algorithm  1  is the mathematically-derived",
            "where    \\omega italic_  is some threshold for how many times the focal-point  F F \\mathbf{F} bold_F  was chosen out of 50 runs (  = 40  40 \\omega=40 italic_ = 40 , for instance). However, we chose the aggregation in Algorithm  1  because the resulting density estimation achieved slightly better attack accuracy throughout.",
            "However, we do not include it in the main text because we found that this SDG algorithm did not produce high quality data, both for the SNAKE dataset and the housing dataset. See, for example, our distance measurement (described in Section  IV-D ) of RAPs  D s  y  n  t  h subscript D s y n t h D_{synth} italic_D start_POSTSUBSCRIPT italic_s italic_y italic_n italic_t italic_h end_POSTSUBSCRIPT  to its training data in Figure  11 , compared with those generated by the other SDG algorithms. Despite our best efforts to bolster the quality of the RAP data through many training parameters (of course beginning with its defaults), we could not improve its quality much. This result is consistent with other scholarship ranking the qualities of MST, PrivBayes, and RAP  [ 32 ] . If our simple measurement yields such a great divergence between the real data and RAPs synthetic data, it is certainly unlikely that RAP could maintain other important properties, like marginals, between the real and the synthetic data.",
            "The best result MAMA-MIA achieved on RAP data was when  D t  r  a  i  n subscript D t r a i n D_{train} italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT  had only 100 rows of SNAKE data, and when performing set MI on it, shown in Figure  10 . Notice how, in this more susceptible setting, our attack on other SDG algorithms can get much closer to perfect accuracy.",
            "This was also the only setting where DOMIAS, using KDE, performed noticeably better than random guessing on MST and PrivBayes. For these investigations in Figures  11  &  10 , we include thirteen values of    { 10 i / 3 |  3  i  9 }  conditional-set superscript 10 i 3 3 i 9 \\varepsilon\\in\\{10^{i/3}|-3\\leq i\\leq 9\\} italic_  { 10 start_POSTSUPERSCRIPT italic_i / 3 end_POSTSUPERSCRIPT | - 3  italic_i  9 } , but do not have results for the TAPAS or DOMIAS-using-BNAF attacks due to computational limitations.",
            "So RAP expects the scientist to specify a workloadthat is, hand-select a subset of features to be considered in the query selection processusing domain- and task-specific knowledge. This greatly narrows the amount of possible focal-points considered, and so poses a challenge for us during shadow-modelling; since knowledge of the workload is not included in our threat model, and since we only have access to default information, the FPs we observe will likely be wildly different from those actually measured on  D t  r  a  i  n subscript D t r a i n D_{train} italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT . Even when no workload is chosen by the scientist, and default behavior is used to train the generator (as is done in our experiments), the shear amount of FPs considered causes the fitting of  D s  y  n  t  h subscript D s y n t h D_{synth} italic_D start_POSTSUBSCRIPT italic_s italic_y italic_n italic_t italic_h end_POSTSUBSCRIPT  to be highly volatile, even when    \\varepsilon italic_  is large. This notwithstanding, our density estimation    \\zeta italic_  of RAP synthetic data uses the frequencies of queries chosen during shadow modelling, and we use those as the weighted focal-points in Algorithm  1 .",
            "The results on RAP from Figure  10  deviate from the default parameters, in that, instead of selecting 50 queries per epoch, we select 70 queries per epoch, and increase the maximum possible updates per epoch to 2600 from 1000. We do this as a counter measure to induce fitting on the chosen focal-points, selected from a much larger domain of queries (773,239 possible for the one-hot encoded SNAKE data), rather than fitting to a specified workload as RAP intends. Otherwise, all default values for MST, PrivBayes, Private-GSD, and RAP are used in our experiments.",
            "We show in Figure  12  the accuracy of our MAMA-MIA attack as described in Section  III  (using shadow modelling to select focal-points that we predict were used by the generator), and for when we do not do this step, and instead choose the same amount of focal-points  arbitrarily . The accuracy of this latter configuration is still notable, but is substantially improved by our novel use of shadow modelling.",
            "As expected, we observed that the variability increased as the privacy-loss budget    \\varepsilon italic_  decreased, because a smaller    \\varepsilon italic_  amounts to a higher amount of randomness in the selection of focal-points by the SDG. For MST, we visualize these findings as a bar graph in Figure  13 , where each bar represents a percentage of shadow runs FPs were reselected, across different values of    \\varepsilon italic_ . Notice that, when   = 0.1  0.1 \\varepsilon=0.1 italic_ = 0.1  most of the marginals selected by MST were only chosen less than  50 % percent 50 50\\% 50 %  of the time, boding poorly for our confidence in which FPs to measure. On the other hand, with higher    \\varepsilon italic_ , a vast majority of marginals were chosen more than  75 % percent 75 75\\% 75 %  of the time during shadow modelling, which makes our density estimation of  D s  y  n  t  h subscript D s y n t h D_{synth} italic_D start_POSTSUBSCRIPT italic_s italic_y italic_n italic_t italic_h end_POSTSUBSCRIPT  closer to that of the hidden  D t  r  a  i  n subscript D t r a i n D_{train} italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT .",
            "Moreover, PrivBayes  preferred  different FPs for different    \\varepsilon italic_ . Figure  14  shows a histogram of how frequent conditionals parents sizes were selected, and the trends for different    \\varepsilon italic_ . When   = 0.1  0.1 \\varepsilon=0.1 italic_ = 0.1 , PrivBayes only ever allows for conditionals to have one or zero parents. But when   = 1000  1000 \\varepsilon=1000 italic_ = 1000 , its graphical estimation is constructed by favoring conditionals with two, three, and up to four parents. This was expected because of the way PrivBayes changes its maximum parent size to make the most effective use of its privacy budget.",
            "Set MI experiments  We show results for set MI experiments in Figures  15  and  16 . These are conducted using the same settings as in experiment A, i.e. with  | D t  r  a  i  n | subscript D t r a i n |D_{train}| | italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT |  fixed at 1,000 records, trialling both the SNAKE and the housing data. We only present results for DOMIAS, when using KDE, and we omit set MI results for the attack from the TAPAS library because it does not offer set MI in its API. Sets are groups of at least five target records, and instead of making  | D t  a  r  g  e  t | subscript D t a r g e t |D_{target}| | italic_D start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT | , and  | D m  e  m  b  e  r | subscript D m e m b e r |D_{member}| | italic_D start_POSTSUBSCRIPT italic_m italic_e italic_m italic_b italic_e italic_r end_POSTSUBSCRIPT |  equal to 32 and 16 records, respectively, we use 32 and 16  sets . The results are unsurprising, where MAMA-MIAs accuracy improves over the single MI setting."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II :  Dataset size configurations used in  experiment B  and others. Each trial uses configurations entirely from one row.",
        "table": "S4.T2.4",
        "footnotes": [],
        "references": [
            "A simple illustration of this idea is given in Figure  2  where, in the second graph, areas in red suggest where there might be a high concentration of training data values. (This idea is discussed in depth in  [ 35 ] .) DOMIAS uses  generic  density estimators to this end, such as KDE  [ 29 ]  and BNAF  [ 7 ] . Then it simply computes   = S  ( D s  y  n  t  h ) / S  ( D a  u  x )  S subscript D s y n t h S subscript D a u x \\Lambda=S(D_{synth})/S(D_{aux}) roman_ = italic_S ( italic_D start_POSTSUBSCRIPT italic_s italic_y italic_n italic_t italic_h end_POSTSUBSCRIPT ) / italic_S ( italic_D start_POSTSUBSCRIPT italic_a italic_u italic_x end_POSTSUBSCRIPT )  for each target. 7 7 7 The threshold for inferring membership is when   > 1.0  1.0 \\Lambda>1.0 roman_ > 1.0 . But it neednt be. Normalization allows for any threshold to identify density concentrations meaningfully.",
            "We show in Figure  12  the accuracy of our MAMA-MIA attack as described in Section  III  (using shadow modelling to select focal-points that we predict were used by the generator), and for when we do not do this step, and instead choose the same amount of focal-points  arbitrarily . The accuracy of this latter configuration is still notable, but is substantially improved by our novel use of shadow modelling."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III :  Experiment C  The TAPAS attack when generating and using 4,000 shadow datasets compared with MAMA-MIA, which uses only 50 shadow runs.  | D t  r  a  i  n | = 1 , 000 subscript D t r a i n 1 000 |D_{train}|=1,000 | italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT | = 1 , 000  and   = 10  10 \\varepsilon=10 italic_ = 10  on SNAKE data. Private-GSD uses its default 20,000 generations.",
        "table": "S4.T3.17",
        "footnotes": [],
        "references": [
            "Results are shown in Figure  3 . Our MAMA-MIA attacks accuracy is at par with or better than the other MIAs on data generated by all three SDG algorithms. Not surprisingly, the accuracy improves when the privacy budget   italic- \\epsilon italic_  is larger, i.e. when the privacy guarantees offered by the DP SDG algorithms are lower. DOMIAS did not perform significantly above random guessing, neither with BNAF nor with KDE, the two density estimators used by Van Breugel at al.  [ 35 ] . 13 13 13 There were settings in which DOMIAS performed better than random guessing, but still far worse than MAMA-MIA, and only when the datasets used were small (see appendix section  A-B ).  We also provide a tabled version of our full results, with standard deviations, in the Appendix  A-E .",
            "Main results in greater detail  We offer AUC results in Tables  V , and  VI  for experiment A, conducted on the SNAKE data and the housing data. These tables include MAMA-MIA and TAPAS accuracies, averaged over 30 trials with standard deviations. These correspond to Figures  3  and  5  respectively. The runtimes of MAMA-MIA for this experiment, now separated into its shadow modelling and its density estimation step, are given in Table  VII .",
            "As expected, we observed that the variability increased as the privacy-loss budget    \\varepsilon italic_  decreased, because a smaller    \\varepsilon italic_  amounts to a higher amount of randomness in the selection of focal-points by the SDG. For MST, we visualize these findings as a bar graph in Figure  13 , where each bar represents a percentage of shadow runs FPs were reselected, across different values of    \\varepsilon italic_ . Notice that, when   = 0.1  0.1 \\varepsilon=0.1 italic_ = 0.1  most of the marginals selected by MST were only chosen less than  50 % percent 50 50\\% 50 %  of the time, boding poorly for our confidence in which FPs to measure. On the other hand, with higher    \\varepsilon italic_ , a vast majority of marginals were chosen more than  75 % percent 75 75\\% 75 %  of the time during shadow modelling, which makes our density estimation of  D s  y  n  t  h subscript D s y n t h D_{synth} italic_D start_POSTSUBSCRIPT italic_s italic_y italic_n italic_t italic_h end_POSTSUBSCRIPT  closer to that of the hidden  D t  r  a  i  n subscript D t r a i n D_{train} italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_4": {
        "caption": "TABLE IV :  Podium results for the SNAKE Challenge, evaluated using MA.",
        "table": "S4.T4.3",
        "footnotes": [],
        "references": [
            "This experiment, shown in Figure  4 , is about demonstrating how much more  consistently  strong our MIA is as the datasets become large, as opposed to the TAPAS MIA, which only appears to perform well on smaller dataset sizes. On the other hand, MAMA-MIA appears to far outperform all others at smaller sizes on Private-GSD.",
            "One noticable trend from Figure  4  is that the accuracies of all MIAs tend to degrade for the larger configurations used. We rule out the quality of the synthetic data as an explanation (see Figure  9  where we show that the data quality actually  improves  when increasing the dataset sizes). So the decreasing accuracies can possibly be explained by the diminishing proportion of targets to training data, ostensibly lowering the impact had on the resulting synthetic data.",
            "Moreover, PrivBayes  preferred  different FPs for different    \\varepsilon italic_ . Figure  14  shows a histogram of how frequent conditionals parents sizes were selected, and the trends for different    \\varepsilon italic_ . When   = 0.1  0.1 \\varepsilon=0.1 italic_ = 0.1 , PrivBayes only ever allows for conditionals to have one or zero parents. But when   = 1000  1000 \\varepsilon=1000 italic_ = 1000 , its graphical estimation is constructed by favoring conditionals with two, three, and up to four parents. This was expected because of the way PrivBayes changes its maximum parent size to make the most effective use of its privacy budget."
        ]
    },
    "id_table_5": {
        "caption": "TABLE V :  Average AUC scores (  plus-or-minus \\pm   one standard deviation) over 30 trials for MAMA-MIA and TAPAS attacks on the  SNAKE data .  D t  r  a  i  n = 1 , 000 subscript D t r a i n 1 000 D_{train}=1,000 italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT = 1 , 000 , and TAPAS uses 500 shadow datasets.",
        "table": "A1.T5.59",
        "footnotes": [],
        "references": [
            "Extending experiments to the California Housing Dataset  We extend our experiments from Section  IV-A  to the California Housing Dataset, containing only 20,640 records (as opposed to SNAKEs 201,279) and nine continuous features (SNAKE has fifteen categorical). Since all of these SDG algorithms operate over discrete values, we encode the values into twenty  equal-depth  bins. Again,  | D t  r  a  i  n | = 1 , 000 subscript D t r a i n 1 000 |D_{train}|=1,000 | italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT | = 1 , 000  records. The results are shown in Figure  5 . MAMA-MIAs performance is even more distinguished on this data.",
            "Main results in greater detail  We offer AUC results in Tables  V , and  VI  for experiment A, conducted on the SNAKE data and the housing data. These tables include MAMA-MIA and TAPAS accuracies, averaged over 30 trials with standard deviations. These correspond to Figures  3  and  5  respectively. The runtimes of MAMA-MIA for this experiment, now separated into its shadow modelling and its density estimation step, are given in Table  VII .",
            "Set MI experiments  We show results for set MI experiments in Figures  15  and  16 . These are conducted using the same settings as in experiment A, i.e. with  | D t  r  a  i  n | subscript D t r a i n |D_{train}| | italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT |  fixed at 1,000 records, trialling both the SNAKE and the housing data. We only present results for DOMIAS, when using KDE, and we omit set MI results for the attack from the TAPAS library because it does not offer set MI in its API. Sets are groups of at least five target records, and instead of making  | D t  a  r  g  e  t | subscript D t a r g e t |D_{target}| | italic_D start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT | , and  | D m  e  m  b  e  r | subscript D m e m b e r |D_{member}| | italic_D start_POSTSUBSCRIPT italic_m italic_e italic_m italic_b italic_e italic_r end_POSTSUBSCRIPT |  equal to 32 and 16 records, respectively, we use 32 and 16  sets . The results are unsurprising, where MAMA-MIAs accuracy improves over the single MI setting."
        ]
    },
    "id_table_6": {
        "caption": "TABLE VI :  Average AUC scores (  plus-or-minus \\pm   one standard deviation) over 30 trials for MAMA-MIA and TAPAS attacks on the  housing data .  D t  r  a  i  n = 1 , 000 subscript D t r a i n 1 000 D_{train}=1,000 italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT = 1 , 000 , and TAPAS uses 500 shadow datasets.",
        "table": "A1.T6.59",
        "footnotes": [],
        "references": [
            "Exploring a more strenuous setting  We conduct experiment A, though now increasing  | D t  r  a  i  n | , | D s  y  n  t  h | subscript D t r a i n subscript D s y n t h |D_{train}|,|D_{synth}| | italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT | , | italic_D start_POSTSUBSCRIPT italic_s italic_y italic_n italic_t italic_h end_POSTSUBSCRIPT |  from  1 , 000 1 000 1,000 1 , 000  records to  10 , 000 10 000 10,000 10 , 000  records, and decreasing the proportion of targets from 3.2% to 1% of  D t  r  a  i  n subscript D t r a i n D_{train} italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT  (see configuration (v) from Table  II ). Made clear in Figure  6 , the accuracies are lower for this setting because of the reduced proportion of targets, as explained in section  IV-B . However, MAMA-MIA more definitively outperforms the other attacks when the datasets are large. In fact, our PrivBayes attack accuracy barely diminishes at all from the smaller setting. It is unclear why this happens, but is likely due to PrivBayes use of conditionals as FPs, which have up to four parents, and so hold greater specificity than the 2-way marginals chosen by MST and the 3-way marginals in Private-GSD. (See our extended results on PrivBayes FP selection in Appendix section  A-E  for a discussion).",
            "Main results in greater detail  We offer AUC results in Tables  V , and  VI  for experiment A, conducted on the SNAKE data and the housing data. These tables include MAMA-MIA and TAPAS accuracies, averaged over 30 trials with standard deviations. These correspond to Figures  3  and  5  respectively. The runtimes of MAMA-MIA for this experiment, now separated into its shadow modelling and its density estimation step, are given in Table  VII .",
            "Set MI experiments  We show results for set MI experiments in Figures  15  and  16 . These are conducted using the same settings as in experiment A, i.e. with  | D t  r  a  i  n | subscript D t r a i n |D_{train}| | italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT |  fixed at 1,000 records, trialling both the SNAKE and the housing data. We only present results for DOMIAS, when using KDE, and we omit set MI results for the attack from the TAPAS library because it does not offer set MI in its API. Sets are groups of at least five target records, and instead of making  | D t  a  r  g  e  t | subscript D t a r g e t |D_{target}| | italic_D start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT | , and  | D m  e  m  b  e  r | subscript D m e m b e r |D_{member}| | italic_D start_POSTSUBSCRIPT italic_m italic_e italic_m italic_b italic_e italic_r end_POSTSUBSCRIPT |  equal to 32 and 16 records, respectively, we use 32 and 16  sets . The results are unsurprising, where MAMA-MIAs accuracy improves over the single MI setting."
        ]
    },
    "id_table_7": {
        "caption": "TABLE VII :  Runtimes of MAMA-MIA on SNAKE data when   = 10  10 \\varepsilon=10 italic_ = 10  (i.e. experiment B), separated into the shadow modelling stage, where we select FPs, and the inference stage, where we use them to compute    \\zeta italic_ . The runtime of the select FP stage is for 50 shadow modelling runs. The runtime of the compute    \\zeta italic_  stage given is the average runtime for one trial, attacking all targets in  D t  a  r  g  e  t subscript D t a r g e t D_{target} italic_D start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT . Results for Private-GSD on 31,623 records are omitted because of computational limitations.",
        "table": "A1.T7.12",
        "footnotes": [],
        "references": [
            "Since the TAPAS attack is our most competitive contender, we strengthen its training conditions, feeding it 4,000 shadow datasets instead of 500 as in the previous experiments. Because of computational costs, we only do this for  | D t  r  a  i  n | = 1 , 000 subscript D t r a i n 1 000 |D_{train}|=1,000 | italic_D start_POSTSUBSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT | = 1 , 000  and   = 10  10 \\varepsilon=10 italic_ = 10 , on the SNAKE data. Its accuracy improves slightly, but it still does not beat the accuracy of MAMA-MIA, which is still only using 50 shadow runs. These results are shown in Table  III  along with corresponding runtimes. (The differences in performance when TAPAS uses 500 versus 4,000 shadow datasets can be seen in Figure  7 ).",
            "Evaluating a much more efficient TAPAS attack  Since the computational costs associated with the TAPAS MIA were substantial, perhaps such costs arent entirely necessary. So instead of training the attack with 500 and 4,000 shadow datasets, we train it with only 50, similar to the 50 shadow modelling runs MAMA-MIA uses. This yields comparable, though still worse, runtimes because MAMA-MIA terminates the SDG process early. This notwithstanding, we find that the TAPAS attack accuracy significantly degrades when using 50 shadow datasets versus 500 (shown in Figure  7 ), suggesting that high computational costs are inherent to that attacks success.",
            "Runtimes are given in Table  VII  for this compute    \\zeta italic_  stage separately. PrivBayes takes slightly longer than MST, which is likely due to the fact that it chooses conditionals as focal-points, which offer higher specificity than MSTs marginals proper, and so presumably stress the hashing capability. Computing    \\zeta italic_  in Private-GSD appears to take much longer than on the other two SDG algorithms. This can be explained by  | F | F |\\mathcal{F}| | caligraphic_F |  being much larger in Private-GSD, which selects thousands of focal-points over one-hot encoded data, as opposed to the dozen or so FPs on un-encoded data chosen by MST and PrivBayes. The reason why the time to compute    \\zeta italic_  is relatively constant, despite the growing dataset size, is because the    \\zeta italic_  also performs computations over the entire  D a  u  x subscript D a u x D_{aux} italic_D start_POSTSUBSCRIPT italic_a italic_u italic_x end_POSTSUBSCRIPT , which is much larger (i.e. over 200,000 rows) than any  D s  y  n  t  h subscript D s y n t h D_{synth} italic_D start_POSTSUBSCRIPT italic_s italic_y italic_n italic_t italic_h end_POSTSUBSCRIPT  tried, and so dominates that portion of the runtime.",
            "Main results in greater detail  We offer AUC results in Tables  V , and  VI  for experiment A, conducted on the SNAKE data and the housing data. These tables include MAMA-MIA and TAPAS accuracies, averaged over 30 trials with standard deviations. These correspond to Figures  3  and  5  respectively. The runtimes of MAMA-MIA for this experiment, now separated into its shadow modelling and its density estimation step, are given in Table  VII .",
            "Showing the impact of shadow modelling in MAMA-MIA  Shadow modelling to select focal-points (FPs) is a time consuming step in MAMA-MIA, especially when attacking synthetic data generated by SDGs that are in themselves computationally expensive to train (see Table  VII ). One may wonder what the effect is of this careful selection of focal-points on the accuracy of the attack, i.e. whether it is worth the computational effort."
        ]
    }
}