{
    "id_table_1": {
        "caption": "Table 1.  Statistics of user study on reconstructed 3D hair by different methods.  R  a  t . R a t Rat. italic_R italic_a italic_t .  and  T  e  x . T e x Tex. italic_T italic_e italic_x .  stand for the rationality and texture quality.",
        "table": "A5.EGx1",
        "footnotes": [],
        "references": [
            "In the process of Gaussian optimization, we observe that directly applying Score Distillation Sampling (SDS) loss  (Poole et al . ,  2022 )  against  HairSynthesizer  leads to noisy and blurry hair texture. To reconstruct 3D hair with fine-grained textures, we carefully design two different level Gaussian refinements. We first perform a view-level Gaussian refinement with the supervision of dense views generated by  HairSynthesizer . Although the noise can be clearly removed, the texture is still blurry due to the inherent view-inconsistency of  HairSynthesizer . Thus, we further conduct a pixel-level Gaussian refinement against  HairEnhancer  and obtain a satisfactory 3D hair with fine texture, as shown in  Fig.   1 .",
            "To further demonstrate the merits of our system on single-view hair modeling, we compare our strategy with most recent generative Image-to-3D works DreamGaussian  (Tang et al . ,  2023 ) , One-2-3-45  (Liu et al . ,  2023c )  and One-2-3-45++  (Liu et al . ,  2023a ) , given front-view condition on in-the-wild real images. For the visual comparison in   Fig.   7 , the results of previous methods are full of various artifacts, such as irrational hairstyle, undesired shape and blurry texture. On the contrast, our method achieves faithful 3D hair reconstruction. Moreover, although our priors  HairSynthesizer  and  HairEnhancer  are trained on synthetic data, our system has fine performance on in-the-wild images involving highly diverse hairstyles with a non-negligible domain gap, as shown in  Fig.   7  and  Fig.   10 . To demonstrate the superior of our method, we conduct a user study on 10 randomly selected hairstyles, with total 32 volunteers involved. In the questionnaires, they are asked to evaluate the rationality and the texture quality of randomly sorted reconstructed 3D hairstyles of different approaches, by giving integer scores from 0 to 10. The final average scores of all methods are listed in  Tab.   1 , where ours is ranked the best, supporting the analysis of the visual results. More results and visualization of 3D Gaussians on real-world images can be found in the supplementary video.",
            "Our strategy has the ability to generate consistent and high-quality multi-view hair images from a single input. As a result, with the help of our method, the single-view 3D strand reconstruction can be converted to maturer multi-view reconstruction  (Sklyarova et al . ,  2023 ) . We feed 180 images rendered by our enhanced 3D Gaussian   2 superscript  2 \\Theta^{2} roman_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  to multi-view reconstruction methods  (Sklyarova et al . ,  2023 ; Wu et al . ,  2024 )  and finally obtain reconstructed 3D hair strands which is more reasonable than the results of the state-of-the-at approach  (Zheng et al . ,  2023 )  as shown in  Fig.   11 .",
            "Our method is based on good facial landmark detection which is quite simple for frontal view images but difficult for side views. As shown in  Fig.   12 , to make our method work for side-view input, we first replace the body of the input image with grey mask, then we generate the reference view, i.e.  R = T = 0 R T 0 R=T=\\textbf{0} italic_R = italic_T = 0 , using  HairSynthesizer  conditioned on this masked hair image. Finally, we input the synthesized aligned hair image of input side-view image to our system and reconstruct 3D hair. Rendered images of outputted 3D Gaussian from two random views are given in  Fig.   12  (d-e).",
            "As shown in  Fig.   13 , given a 3D avatar, we can customize its hairstyle to match the styles of reference images. We first reconstruct the target 3D hair from the single image via our method, which is aligned with our 3D body template. This hairstyle can then be applied to the given 3D avatar, which is also aligned with our template.",
            "Although our approach yields realistic results on a wide range of input images, it may fail in certain cases when tested on challenging portraits. For instance, as shown in  Fig.   14 , our method relies on successful intermediate stages such as (a) hair-head alignment and (b) hair mask segmentation. Inaccuracies in these processes can adversely affect the final results. Also, our method may struggle with complex real-world lighting, which can impact hair texture (see the top of the hair in  Fig.   14 (c)). Hair accessories may cause inconsistencies between different views ( Fig.   14 (d)). Our method does not yet perform well on wavy, curly, and coily hair, which we hope to improve in the future. Additionally, although our method shows generalization capability on real portraits, it still needs improvement to achieve high fidelity. We believe this is mainly caused by the usage of limited synthetic data. A promising direction is leveraging priors from massive high-quality 2D real images for 3D hair reconstruction.",
            "We compare our  SynMvHair  with previous 3D hair dataset in  Tab.   S1 , including USC-HairSalon  (Hu et al . ,  2015 ) , following datasets from recent hair modeling works  (Wu et al . ,  2022 ; Zhou et al . ,  2023 ; Shen et al . ,  2023 )  and commercial website DataGen  1 1 1 {https://datagen.tech/}  on human-centric data synthesis. So far, the largest open-source 3D hair dataset USC-HairSalon contains 343 hairstyles. It can only cover basic categories, such as long or short, straight or wavy, but fail to include complex types like extremely curly hair or braided hair. Following works on hair modeling also suffer from data shortage. To tackle on this, we construct our large-scale highly diverse synthetic hair dataset  SynMvHair  of calibrated multi-view images. The scale of both geometries and textures are much larger than previous dataset.",
            "We argue that although we do not provide strand-level geometry, the rich-textured and shape-diverse  SynMvHair  in multi-view representation is capable to support the learning of diffusion prior of 3D hairstyle. We show more hair samples in our  SynMvHair  with various styles ( Fig.   S1 ) , texture ( Fig.   S2 ) and rendering angles ( Fig.   S3 )."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Ablation on Gaussian refinement.  P  e  r  c . P e r c Perc. italic_P italic_e italic_r italic_c .  denotes perceptual errors.",
        "table": "A5.EGx2",
        "footnotes": [],
        "references": [
            "As shown in  Fig.   2 , given an input portrait image, our method first enables the reconstruction of its corresponding 3D hair in the representation of 3D Gaussian. Our method supports input of various hairstyles, ranging from un-braided Bobs and Waves to knotted braids and buns, etc. The obtained 3D hair is aligned with a template body and can be rendered into high-quality images from arbitrary views. To support our method, we construct a novel dataset  SynMvHair  and build two diffusion priors  HairSynthesizer  and  HairEnhancer . We describe our  SynMvHair  in  Sec.   4  and how the pipeline works in  Sec.   5 .",
            "As shown in Fig.  2 , we first wears the hair of the input image  I I I italic_I  on the rendered template body to align the format of the condition in  HairSynthesizer . Then a coarse 3D Gaussian   0 superscript  0 \\Theta^{0} roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  will be distilled from  HairSynthesizer  via an optimization guided by SDS loss  (Poole et al . ,  2022 )  conditioning on the aligned image  I a subscript I a I_{a} italic_I start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT . However, the hair texture provided by   0 superscript  0 \\Theta^{0} roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  is noisy and blurry. Thus, we further perform view-wise and pixel-wise Gaussian refinements leveraging the prior of  HairSynthesizer  and  HairEnhancer , to obtain refined 3D Gaussian   1 superscript  1 \\Theta^{1} roman_ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  and enhanced 3D Gaussian   2 superscript  2 \\Theta^{2} roman_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , respectively. Thanks to the two diffusion priors based on our novel dataset  SynMvHair  and the level-wise refinement module, our method can finally output a Gaussian-based 3D hair which enabling multi-view rendering with fine hair textures.",
            "To reconstructing 3D hair shown in  I a subscript I a I_{a} italic_I start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  using  SynMvHair , we follow  (Liu et al . ,  2023b )  to build a diffusion-based module  HairSynthesizer  for novel view synthesis of hair images. Although  HairSynthesizer  can generate plausible novel views of a given hair image, the consistency between different views cannot be guaranteed. Thus, based on  HairSynthesizer , we optimize an underlying 3D Gaussian   0 superscript  0 \\Theta^{0} roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  via SDS loss  (Poole et al . ,  2022 )  against  HairSynthesizer  with the condition of  I a subscript I a I_{a} italic_I start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT , as shown in the left-bottom of  Fig.   2 .",
            "To better analyze the design of different modules of our pipeline, we compare the results of  HairSynthesizer  and the rendering results of   0 superscript  0 \\Theta^{0} roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ,   1 superscript  1 \\Theta^{1} roman_ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  and   2 superscript  2 \\Theta^{2} roman_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  from real inputs in  Fig.   8 , qualitatively. From the comparisons between coarse Gaussian   0 superscript  0 \\Theta^{0} roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  (b) and refined Gaussian   1 superscript  1 \\Theta^{1} roman_ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  (c) in  Fig.   8 , we can easily see the effectiveness of view-wise Gaussian refinement. Furthermore, after the process of pixel-wise Gaussian refinement, the texture of   2 superscript  2 \\Theta^{2} roman_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  (d) will be clearly improved, comparing with   1 superscript  1 \\Theta^{1} roman_ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  (c). Please check the zoom-in details in (e). We also show the results of our diffusion prior  HairSynthesizer  in Fig.   8 (a), where view-inconsistency occurs. For each example, images from two very near view angles are given in the first and the second row, respectively. In the red boxes of the upper example, inconsistent hair geometries are presented, the first view shows ridge-like shape while the second view shows contrary valley-like shape. In terms of the lower example, the comparisons between the yellow boxes and red boxes present inconsistent texture and shape, respectively. With the help of the underlying 3D Gaussian, our results performs well in view-consistency.  Tab.   2  provides quantitative supports by evaluating on the test subset of  SynMvHair .",
            "In the view-wise Gaussian refinement, we gradually enlarging the weight    \\gamma italic_  of the images rendered from the being optimized coarse Gaussian   0 superscript  0 \\Theta^{0} roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT , and add less noise into the input of diffusion prior  HairSynthesizer . As shown in  Tab.   3 , starting from 0.5, we add 0.15 to    \\gamma italic_  every 200 steps. From the comparisons in  Fig.   9 , the noising pattern of the texture will gradually removed using this strategy. Comparisons in  Tab.   2  also verifies our observation.",
            "Our method is based on good facial landmark detection which is quite simple for frontal view images but difficult for side views. As shown in  Fig.   12 , to make our method work for side-view input, we first replace the body of the input image with grey mask, then we generate the reference view, i.e.  R = T = 0 R T 0 R=T=\\textbf{0} italic_R = italic_T = 0 , using  HairSynthesizer  conditioned on this masked hair image. Finally, we input the synthesized aligned hair image of input side-view image to our system and reconstruct 3D hair. Rendered images of outputted 3D Gaussian from two random views are given in  Fig.   12  (d-e).",
            "We argue that although we do not provide strand-level geometry, the rich-textured and shape-diverse  SynMvHair  in multi-view representation is capable to support the learning of diffusion prior of 3D hairstyle. We show more hair samples in our  SynMvHair  with various styles ( Fig.   S1 ) , texture ( Fig.   S2 ) and rendering angles ( Fig.   S3 ).",
            "We compare the rendering results with (w/) and without (w/o) using perceptual loss  (Johnson et al . ,  2016 )  (features of layer 4, 9, 16 and 23 of VGG16  (Simonyan and Zisserman,  2014 ) ). As shown in  Fig.   S4 , the perceptual loss helps a lot to obtain fine textures of   1 superscript  1 \\Theta^{1} roman_ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  and   2 superscript  2 \\Theta^{2} roman_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  in the process of view-wise Gaussian refinement and pixel-wise Gaussian refinement.  Tab.   S2  displays the quantitative comparisons supports this analysis."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Comparisons on enlarging    \\gamma italic_  during view-wise Gaussian refinement.  P  e  r  c . P e r c Perc. italic_P italic_e italic_r italic_c .  denotes perceptual errors.",
        "table": "A5.EGx3",
        "footnotes": [],
        "references": [
            "We first collect 10,320 different raw synthetic hair models from The Sims Resource 1 1 1 {https://www.thesimsresource.com/} . After taking efforts to check and remove the poor data with coarse shape and blurry texture, we finally obtain 2,396 3D hair models and 82,682 textures in total. All of these 3D hair models are composed of hundreds to thousands independent thin polygon-strips and each strip represents a coherent hair wisp. Furthermore, all 3D hair models are canonical and normalized by artists, in other words, they are aligned onto the head of an identical template body. As the inner structure of 3D hairs are not as good as desired, we cannot convert them to 3D strands or strips connected with hair roots within an allowable budget. Inspired by  (Liu et al . ,  2023b ) , we render dense multi-view images with high-quality hair textures to enable the learning of 3D-aware diffusion priors. The camera can randomly move on a upper hemisphere whose center is roughly set at the center of neck and whose radius is set to contain the upper-half body for shooting most of hairstyles. A data sample of  SynMvHair  is illustrated in  Fig.   3 .",
            "In the view-wise Gaussian refinement, we gradually enlarging the weight    \\gamma italic_  of the images rendered from the being optimized coarse Gaussian   0 superscript  0 \\Theta^{0} roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT , and add less noise into the input of diffusion prior  HairSynthesizer . As shown in  Tab.   3 , starting from 0.5, we add 0.15 to    \\gamma italic_  every 200 steps. From the comparisons in  Fig.   9 , the noising pattern of the texture will gradually removed using this strategy. Comparisons in  Tab.   2  also verifies our observation.",
            "As shown in  Fig.   13 , given a 3D avatar, we can customize its hairstyle to match the styles of reference images. We first reconstruct the target 3D hair from the single image via our method, which is aligned with our 3D body template. This hairstyle can then be applied to the given 3D avatar, which is also aligned with our template.",
            "We argue that although we do not provide strand-level geometry, the rich-textured and shape-diverse  SynMvHair  in multi-view representation is capable to support the learning of diffusion prior of 3D hairstyle. We show more hair samples in our  SynMvHair  with various styles ( Fig.   S1 ) , texture ( Fig.   S2 ) and rendering angles ( Fig.   S3 ).",
            "We compare our method with the related method DreamGaussian  (Tang et al . ,  2023 )  in  Fig.   S5  and  Tab.   S3 . From the comparisons, we find the results of DreamGaussian has lots of artifacts as shown in  Fig.   S5  (a), mainly caused by the inappropriate general prior of Zero-1-to-3. With the help of our hair-specified prior  HairSynthesizer , the results of DreamGaussian turn better ( Fig.   S5  (b)), but still unsatisfactory. We believe that this is because of the inflexibility of 3D mesh representation used in the refinement stage of  (Tang et al . ,  2023 ) . Also, the extraction of the coarse mesh from 3D Gaussian has lost quite amount of geometric details. The reconstructed 3D hair of our method is shown in  Fig.   S5  (c). It indicates that 3D Gaussian brings large flexibility in both shape and texture optimization, which makes the reconstruction of 3D hair a higher quality."
        ]
    },
    "id_table_4": {
        "caption": "Table S1.  Comparisons of current 3D hair datasets.",
        "table": "A5.EGx4",
        "footnotes": [],
        "references": [
            "As shown in  Fig.   2 , given an input portrait image, our method first enables the reconstruction of its corresponding 3D hair in the representation of 3D Gaussian. Our method supports input of various hairstyles, ranging from un-braided Bobs and Waves to knotted braids and buns, etc. The obtained 3D hair is aligned with a template body and can be rendered into high-quality images from arbitrary views. To support our method, we construct a novel dataset  SynMvHair  and build two diffusion priors  HairSynthesizer  and  HairEnhancer . We describe our  SynMvHair  in  Sec.   4  and how the pipeline works in  Sec.   5 .",
            "The coarse Gaussian optimization can only recover the rough 3D hair because of the inherent view-inconsistency of  HairSynthesizer . Especially, the quality of rendering of   0 superscript  0 \\Theta^{0} roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  is at a very low level. The texture of  I  0 subscript I superscript  0 I_{\\Theta^{0}} italic_I start_POSTSUBSCRIPT roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT  shows very noisy patterns (see  Fig.   4 ). Inspired by the diffusion-based image editing  (Meng et al . ,  2022 ) , we believe combining the advantages of obtained 3D hair   0 superscript  0 \\Theta^{0} roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  and the prior of  HairSynthesizer  can alleviate the issue of texture.   0 superscript  0 \\Theta^{0} roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  provides consistent 3D information but bad texture, while  HairSynthesizer  can generate multi-view images with plausible texture but bad view-consistency. Thus, to improve the texture quality of obtained 3D hair, we perform an view-wise Gaussian refinement using   0 superscript  0 \\Theta^{0} roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  and  HairSynthesizer  jointly.",
            "The view-wise Gaussian refinement is conducted in optimization manner. As shown in  Fig.   4 , at each step, we first render   0 superscript  0 \\Theta^{0} roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  into an image  I  0 subscript I superscript  0 I_{\\Theta^{0}} italic_I start_POSTSUBSCRIPT roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT  from a random view  ( R , T ) R T (R,T) ( italic_R , italic_T ) . Then we blend this image with random noises and obtain its refined version  I r  e  f  i  n  e subscript I r e f i n e I_{refine} italic_I start_POSTSUBSCRIPT italic_r italic_e italic_f italic_i italic_n italic_e end_POSTSUBSCRIPT  through the multi-step denoising of  HairSynthesizer  conditioned on  I a subscript I a I_{a} italic_I start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  and the camera pose  ( R , T ) R T (R,T) ( italic_R , italic_T ) :",
            "where    (  ) italic-  \\phi(\\cdot) italic_ (  )  denotes the operation of extracting VGG features  (Simonyan and Zisserman,  2014 ) . Please note that we describe the process with batch size 1 for convenience, but we render several views each step in practice for stable optimization and also add the supervision of  I a subscript I a I_{a} italic_I start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  for achieving better quality of frontal views. After the view-wise Gaussian refinement, we obtain a refined 3D Gaussian   1 superscript  1 \\Theta^{1} roman_ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT . As shown in the zoom-in rendering comparison of  Fig.   4 , the texture quality of   1 superscript  1 \\Theta^{1} roman_ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  is improved a lot than   0 superscript  0 \\Theta^{0} roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT , where noisy patterns are basically cleaned.",
            "Although our approach yields realistic results on a wide range of input images, it may fail in certain cases when tested on challenging portraits. For instance, as shown in  Fig.   14 , our method relies on successful intermediate stages such as (a) hair-head alignment and (b) hair mask segmentation. Inaccuracies in these processes can adversely affect the final results. Also, our method may struggle with complex real-world lighting, which can impact hair texture (see the top of the hair in  Fig.   14 (c)). Hair accessories may cause inconsistencies between different views ( Fig.   14 (d)). Our method does not yet perform well on wavy, curly, and coily hair, which we hope to improve in the future. Additionally, although our method shows generalization capability on real portraits, it still needs improvement to achieve high fidelity. We believe this is mainly caused by the usage of limited synthetic data. A promising direction is leveraging priors from massive high-quality 2D real images for 3D hair reconstruction.",
            "We compare the rendering results with (w/) and without (w/o) using perceptual loss  (Johnson et al . ,  2016 )  (features of layer 4, 9, 16 and 23 of VGG16  (Simonyan and Zisserman,  2014 ) ). As shown in  Fig.   S4 , the perceptual loss helps a lot to obtain fine textures of   1 superscript  1 \\Theta^{1} roman_ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  and   2 superscript  2 \\Theta^{2} roman_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  in the process of view-wise Gaussian refinement and pixel-wise Gaussian refinement.  Tab.   S2  displays the quantitative comparisons supports this analysis."
        ]
    },
    "id_table_5": {
        "caption": "Table S2.  Ablation study on perceptual loss.",
        "table": "A5.EGx5",
        "footnotes": [],
        "references": [
            "As shown in  Fig.   2 , given an input portrait image, our method first enables the reconstruction of its corresponding 3D hair in the representation of 3D Gaussian. Our method supports input of various hairstyles, ranging from un-braided Bobs and Waves to knotted braids and buns, etc. The obtained 3D hair is aligned with a template body and can be rendered into high-quality images from arbitrary views. To support our method, we construct a novel dataset  SynMvHair  and build two diffusion priors  HairSynthesizer  and  HairEnhancer . We describe our  SynMvHair  in  Sec.   4  and how the pipeline works in  Sec.   5 .",
            "As shown in  Fig.   5 , similar to view-wise refinement, we perform the pixel-wise Gaussian refinement through optimizing the   1 superscript  1 \\Theta^{1} roman_ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  via minimizing the loss:",
            "We compare our method with the related method DreamGaussian  (Tang et al . ,  2023 )  in  Fig.   S5  and  Tab.   S3 . From the comparisons, we find the results of DreamGaussian has lots of artifacts as shown in  Fig.   S5  (a), mainly caused by the inappropriate general prior of Zero-1-to-3. With the help of our hair-specified prior  HairSynthesizer , the results of DreamGaussian turn better ( Fig.   S5  (b)), but still unsatisfactory. We believe that this is because of the inflexibility of 3D mesh representation used in the refinement stage of  (Tang et al . ,  2023 ) . Also, the extraction of the coarse mesh from 3D Gaussian has lost quite amount of geometric details. The reconstructed 3D hair of our method is shown in  Fig.   S5  (c). It indicates that 3D Gaussian brings large flexibility in both shape and texture optimization, which makes the reconstruction of 3D hair a higher quality."
        ]
    },
    "id_table_6": {
        "caption": "Table S3.  Comparisons with DreamGaussian  (Tang et al . ,  2023 ) .",
        "table": "A5.EGx6",
        "footnotes": [
            ""
        ],
        "references": [
            "We compare our method with single-view hair reconstruction methods  (Zheng et al . ,  2023 ) ,  (Hu et al . ,  2017 )  and  (Sun et al . ,  2021 )  on braided styles in  Fig.   6 .   (Zheng et al . ,  2023 )  fails on the hairstyle of braids and buns (see  Fig.   6  (a)), because of lacking corresponding 3D strand-level braided data in its underlying hair prior, which hinders its generalization capability to diverse real-world hairstyles. As shown in  Fig.   6  (b), the method of  (Hu et al . ,  2017 )  can produce textured 3D hair strips. However, it fails to recover the structure of braids and the texture of their results are lack of realism. We also compare our strategy with the braid-specific method  (Sun et al . ,  2021 ) . Our method can successfully reconstruct the double-braid 3D hairstyle faithfully, while  (Sun et al . ,  2021 )  fails to recover the invisible parts of the hair,which is emphasized by the red box in  Fig.   6  (c). Also, the texture quality of our result is much better than  (Sun et al . ,  2021 ) . Please note that,  (Sun et al . ,  2021 )  is not an automatic approach and our alignment method based on facial landmarks cannot work for the input from back-view. In this example, we use a same semi-automatic method to do the alignment as  (Sun et al . ,  2021 ) .",
            "In this section, we provide more in-the-wild results of our approach in  Fig.   S6  and  Fig.   S7 , which demonstrates that our method has good generalization ability on real portraits, although using priors learned from synthetic data."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "S6.T1.4",
        "footnotes": [],
        "references": [
            "To further demonstrate the merits of our system on single-view hair modeling, we compare our strategy with most recent generative Image-to-3D works DreamGaussian  (Tang et al . ,  2023 ) , One-2-3-45  (Liu et al . ,  2023c )  and One-2-3-45++  (Liu et al . ,  2023a ) , given front-view condition on in-the-wild real images. For the visual comparison in   Fig.   7 , the results of previous methods are full of various artifacts, such as irrational hairstyle, undesired shape and blurry texture. On the contrast, our method achieves faithful 3D hair reconstruction. Moreover, although our priors  HairSynthesizer  and  HairEnhancer  are trained on synthetic data, our system has fine performance on in-the-wild images involving highly diverse hairstyles with a non-negligible domain gap, as shown in  Fig.   7  and  Fig.   10 . To demonstrate the superior of our method, we conduct a user study on 10 randomly selected hairstyles, with total 32 volunteers involved. In the questionnaires, they are asked to evaluate the rationality and the texture quality of randomly sorted reconstructed 3D hairstyles of different approaches, by giving integer scores from 0 to 10. The final average scores of all methods are listed in  Tab.   1 , where ours is ranked the best, supporting the analysis of the visual results. More results and visualization of 3D Gaussians on real-world images can be found in the supplementary video.",
            "In this section, we provide more in-the-wild results of our approach in  Fig.   S6  and  Fig.   S7 , which demonstrates that our method has good generalization ability on real portraits, although using priors learned from synthetic data."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "S6.T2.9",
        "footnotes": [],
        "references": [
            "To better analyze the design of different modules of our pipeline, we compare the results of  HairSynthesizer  and the rendering results of   0 superscript  0 \\Theta^{0} roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ,   1 superscript  1 \\Theta^{1} roman_ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  and   2 superscript  2 \\Theta^{2} roman_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  from real inputs in  Fig.   8 , qualitatively. From the comparisons between coarse Gaussian   0 superscript  0 \\Theta^{0} roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  (b) and refined Gaussian   1 superscript  1 \\Theta^{1} roman_ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  (c) in  Fig.   8 , we can easily see the effectiveness of view-wise Gaussian refinement. Furthermore, after the process of pixel-wise Gaussian refinement, the texture of   2 superscript  2 \\Theta^{2} roman_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  (d) will be clearly improved, comparing with   1 superscript  1 \\Theta^{1} roman_ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  (c). Please check the zoom-in details in (e). We also show the results of our diffusion prior  HairSynthesizer  in Fig.   8 (a), where view-inconsistency occurs. For each example, images from two very near view angles are given in the first and the second row, respectively. In the red boxes of the upper example, inconsistent hair geometries are presented, the first view shows ridge-like shape while the second view shows contrary valley-like shape. In terms of the lower example, the comparisons between the yellow boxes and red boxes present inconsistent texture and shape, respectively. With the help of the underlying 3D Gaussian, our results performs well in view-consistency.  Tab.   2  provides quantitative supports by evaluating on the test subset of  SynMvHair ."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "S6.T3.13",
        "footnotes": [],
        "references": [
            "In the view-wise Gaussian refinement, we gradually enlarging the weight    \\gamma italic_  of the images rendered from the being optimized coarse Gaussian   0 superscript  0 \\Theta^{0} roman_ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT , and add less noise into the input of diffusion prior  HairSynthesizer . As shown in  Tab.   3 , starting from 0.5, we add 0.15 to    \\gamma italic_  every 200 steps. From the comparisons in  Fig.   9 , the noising pattern of the texture will gradually removed using this strategy. Comparisons in  Tab.   2  also verifies our observation."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A2.T1.1",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "To further demonstrate the merits of our system on single-view hair modeling, we compare our strategy with most recent generative Image-to-3D works DreamGaussian  (Tang et al . ,  2023 ) , One-2-3-45  (Liu et al . ,  2023c )  and One-2-3-45++  (Liu et al . ,  2023a ) , given front-view condition on in-the-wild real images. For the visual comparison in   Fig.   7 , the results of previous methods are full of various artifacts, such as irrational hairstyle, undesired shape and blurry texture. On the contrast, our method achieves faithful 3D hair reconstruction. Moreover, although our priors  HairSynthesizer  and  HairEnhancer  are trained on synthetic data, our system has fine performance on in-the-wild images involving highly diverse hairstyles with a non-negligible domain gap, as shown in  Fig.   7  and  Fig.   10 . To demonstrate the superior of our method, we conduct a user study on 10 randomly selected hairstyles, with total 32 volunteers involved. In the questionnaires, they are asked to evaluate the rationality and the texture quality of randomly sorted reconstructed 3D hairstyles of different approaches, by giving integer scores from 0 to 10. The final average scores of all methods are listed in  Tab.   1 , where ours is ranked the best, supporting the analysis of the visual results. More results and visualization of 3D Gaussians on real-world images can be found in the supplementary video."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "A3.T2.14.14",
        "footnotes": [],
        "references": [
            "Our strategy has the ability to generate consistent and high-quality multi-view hair images from a single input. As a result, with the help of our method, the single-view 3D strand reconstruction can be converted to maturer multi-view reconstruction  (Sklyarova et al . ,  2023 ) . We feed 180 images rendered by our enhanced 3D Gaussian   2 superscript  2 \\Theta^{2} roman_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  to multi-view reconstruction methods  (Sklyarova et al . ,  2023 ; Wu et al . ,  2024 )  and finally obtain reconstructed 3D hair strands which is more reasonable than the results of the state-of-the-at approach  (Zheng et al . ,  2023 )  as shown in  Fig.   11 ."
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "A4.T3.6.6",
        "footnotes": [],
        "references": [
            "Our method is based on good facial landmark detection which is quite simple for frontal view images but difficult for side views. As shown in  Fig.   12 , to make our method work for side-view input, we first replace the body of the input image with grey mask, then we generate the reference view, i.e.  R = T = 0 R T 0 R=T=\\textbf{0} italic_R = italic_T = 0 , using  HairSynthesizer  conditioned on this masked hair image. Finally, we input the synthesized aligned hair image of input side-view image to our system and reconstruct 3D hair. Rendered images of outputted 3D Gaussian from two random views are given in  Fig.   12  (d-e)."
        ]
    }
}