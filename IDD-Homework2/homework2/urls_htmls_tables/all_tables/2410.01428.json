{
    "id_table_1": {
        "caption": "Table 6:  Details of state types and action spaces.",
        "table": "S3.T1.3.1",
        "footnotes": [],
        "references": [
            "While the above methods have shown prospects, they are generally limited to handling problems with relatively simple reasoning processes, such as answering two-hop questions like What year was the Argentine actor who directed El Tio Disparate born?  These methods often fail to solve  domain-knowledge-intensive  and  reasoning-heavy  problems, such as competitive programming problems  (Shi et al.,  2024 )  which require the model to possess rich algorithmic knowledge and strong reasoning capability.  Specifically, these methods often struggle with two significant types of errors, as shown in Figure  1  (a).  The first is  reasoning error . When presented with the problem Given a string  s s \\mathbf{s} bold_s , find the length of the longest substring without repeating characters in optimal time complexity, a CoT approach may incorrectly generate that The optimal time complexity is  O  ( n 2 ) O superscript n 2 O(n^{2}) italic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )  in its initial reasoning step.  This erroneous reasoning step then cascades through subsequent steps, leading to an incorrect final answer.  The second type of error is  retrieving error .  The effectiveness of the retrieval process depends on the accuracy of the generated search queries and the selection of the retrieved documents. If the preceding reasoning step is flawed, the query generator could be misguided, leading the retriever to return misinformation, as shown in Figure  1  (a).  Additionally, the selection of retrieved documents could be erroneous. Thus, the subsequent reasoning will be grounded on a wrong prior.",
            "To address these errors, we present critic-guided planning with retrieval-augmentation (CR-Planner),  a framework designed to tackle reasoning-heavy problems requiring extensive domain knowledge. CR-Planner systematically plans both reasoning and retrieval processes with specially fine-tuned critic models.  An example of CR-Planner in action is illustrated in Figure  1  (b), using the question mentioned above.  CR-Planner begins with  Sub-Goal Selection , where it selects a sub-goal from three options:  Reason  (generating rationales),  GenQuery  (generating search queries), and  Retrieve  (retrieving documents), based on reward scores estimated by a critic model,  the sub-goal critic .  After choosing the sub-goal of  Reason  in Step 1, CR-Planner proceeds to  Execution Selection , where it samples several candidate rationales for the next step. Another critic model,  the execution critic  is then employed to select the optimal rationale, which in this case is The optimal time complexity is  O  ( n ) O n O(n) italic_O ( italic_n ) .  In Step 3, CR-Planner returns to sub-goal selection to determine the next best sub-goal.  This iterative process of alternating between sub-goal selection and execution selection continues until the final answer is reached, with each step effectively guided by the corresponding critic model.  Regarding the implementation, CR-Planner incorporates two types of LLMs: a large general generator model ( e.g.,  GPT-4) and small critic models ( e.g.,  Llama-3-8B) fine-tuned with domain-specific  (critiquing)  knowledge. Specifically, when executing a sub-goal, the generator model generates multiple candidate executions ( e.g.,  rationales or search queries, depending on the current sub-goal type). Then, an execution critic corresponding to the sub-goal type performs planning by selecting the most prospective option.  Such a design allows CR-Planner to leverage the generation and reasoning strengths of large generalist LLMs and meanwhile, its small critic models are easier to train with domain-specific  (critiquing)  knowledge.",
            "A s subscript A s \\mathcal{A}_{s} caligraphic_A start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  represents the actions available at each state. For example, the actions available at the sub-goal selection stage,  i.e.,  at the  Root  state or after observing an outcome of an execution selection are:  reasoning ,  querying , and  retrieving .  The possible actions available at the execution selection stage arise from the sampling for the corresponding sub-goal ( i.e.,  temperature sampling for  Reason  and  GenQuery , and top-k candidates for  Retrieve ). For example, Steps 1 and 2 in Figure  1  (b) illustrate the  Reason  and  Rationale  observations generated following the sub-goal selection and execution selection stages, respectively.",
            "The action space  A s t subscript A subscript s t \\mathcal{A}_{s_{t}} caligraphic_A start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT  varies depending on  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .  As previously discussed in Section  2.1  and outlined in Table  6 , for a state in the sub-goal stage, the action space leads to possible executions of that sub-goal, while for a state in the execution stage, the action space leads to the possible subsequent sub-goals.   R  ( s t , a ) R subscript s t a \\mathcal{R}(s_{t},a) caligraphic_R ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a )  is the expected reward when taking action  a a a italic_a  in state  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and estimated by the critic models:",
            "As mentioned in Section  2.1 , given the current state  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and action  a t subscript a t a_{t} italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , we employ three specific functions to generate different types of outcomes.  The generator  f g  e  n  (  ) subscript f g e n  f_{gen}(\\cdot) italic_f start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT (  )  generates either a  Rationale  or  Query . The retriever  f r  e  t  r  (  ) subscript f r e t r  f_{retr}(\\cdot) italic_f start_POSTSUBSCRIPT italic_r italic_e italic_t italic_r end_POSTSUBSCRIPT (  )  outputs a  Doc .  Last but not least, the rule-based function  f r  u  l  e  (  ) subscript f r u l e  f_{rule}(\\cdot) italic_f start_POSTSUBSCRIPT italic_r italic_u italic_l italic_e end_POSTSUBSCRIPT (  )  outputs a  SubGoal .  The  SubGoal  is a predefined natural language. For example, a  Reason  thought is The next step is to generate a rationale.",
            "(1) CR-Planner outperforms all baselines consistently.   Table  1  presents the results for USACO using various methods.  CR-Planner significantly outperforms all baseline methods, achieving a 7.49% improvement in overall performance compared to standard prompting.  This highlights the effectiveness of CR-Planner.   (2) Reasoning-driven methods offer limited improvements.   We observe that reasoning-driven methods like CoT and Reflexion do improve the performances of the standard prompting method on bronze, silver, and gold problems, reaffirming that intermediate rationales and critique-based reasoning aid in solving reasoning tasks  (Wei et al.,  2022 ; Shinn et al.,  2023 ) .  However, the improvements are trivial, and these methods fail to improve performance on platinum-level problems.  We attribute this to the models limited knowledge of the tasks or the generation of faulty rationales and critiques.   (3) Faulty retrieval hinders performance.   We observe that both standard RAG and CoK perform worse than the standard prompting method, consistent with the findings of  Yao et al. ( 2023b )  and  Shi et al. ( 2024 ) .  This decline in performance can be attributed to the quality of retrieval. As demonstrated in Figure  1 , if the retrieved example is irrelevant to the original problem, it may mislead the model into generating an incorrect answer.  Additionally, we notice that CoK performs worse than RAG due to its reliance on multiple retrievals at individual steps, increasing the likelihood of misleading information being introduced and leading to a faulty final answer.   (4) CR-Planner improves harder problems.   CR-Planner notably boosts the performances on gold- and platinum-level problems. As aforementioned, while CoT offers minor improvements, it falls short on more difficult problems, and retrieval can hinder performance due to irrelevant knowledge.  In contrast, CR-Planner employs critic models to guide both the reasoning and retrieval through the process, leading to non-trivial improvements at the two highest levels of programming problems.   (5) CR-Planner is orthogonal with other methods.   Reflexion executes the initially generated code and uses the execution results of a few test cases as linguistic feedback to revise the code. CR-Planner works orthogonal with such methods, leading to a significant improvement of 9.44%, further highlighting the effectiveness of critic-guided planning with retrieval-augmentation."
        ]
    },
    "id_table_2": {
        "caption": "",
        "table": "S3.T2.1.1",
        "footnotes": [],
        "references": [
            "We introduce the critic-guided planning with retrieval-augmentation framework (CR-Planner)   to address challenging tasks that are both domain-knowledge-intensive and reasoning-heavy.   As shown in Figure  2 , CR-Planner operates with two key components during inference:   (1) Sub-Goal Selection : Given the current state, it employs a sub-goal critic model to determine the sub-goal among  Reason ,  GenQuery , and  Retrieve  that leads towards the desired answer.   (2) Execution Selection : Upon selecting a sub-goal, CR-Planner undertakes multiple possible executions to realize the sub-goal. For instance, it may generate multiple search queries to achieve the  GenQuery  sub-goal. Then, an execution critic model specifically designed to assess the executions for the sub-goal is employed to select the optimal execution among these candidates.  In the above process, a general generator model collaborates with multiple specialized critic models to address the task effectively. We leverage the strengths of the generator model to generate initial plans, while the specialized critic models are fine-tuned to guide optimal routing.  To ensure that the training data for the critic models is comprehensive and represents global reward information, we employ Monte Carlo Tree Search (MCTS) to collect the training data.",
            "The reward function  R  ( s t , a ) R subscript s t a \\mathcal{R}(s_{t},a) caligraphic_R ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a )  specifies the expected reward received after taking an action  a t subscript a t a_{t} italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  at state  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .  In our context, fine-tuned critic models estimate the rewards and guide the decision-making process by encouraging actions that contribute the most towards solving the MDP. Details of the critic models are provided in Section  2.2 .",
            "The action space  A s t subscript A subscript s t \\mathcal{A}_{s_{t}} caligraphic_A start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT  varies depending on  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .  As previously discussed in Section  2.1  and outlined in Table  6 , for a state in the sub-goal stage, the action space leads to possible executions of that sub-goal, while for a state in the execution stage, the action space leads to the possible subsequent sub-goals.   R  ( s t , a ) R subscript s t a \\mathcal{R}(s_{t},a) caligraphic_R ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a )  is the expected reward when taking action  a a a italic_a  in state  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and estimated by the critic models:",
            "Specifically, distinct critic models are utilized for different state types:   g g  (  ) superscript g g  g^{g}(\\cdot) italic_g start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT (  )  is for determining the next sub-goal at the current execution state ( i.e.,  the inference Steps 1 in Figure  2 ), and  g e  (  ) superscript g e  g^{e}(\\cdot) italic_g start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT (  )  is for evaluating different execution candidates at the current sub-goal state ( i.e.,  the inference Step 2 in Figure  2 ).  Additionally, according to the sub-goal states,   g e  (  ) superscript g e  g^{e}(\\cdot) italic_g start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT (  )  has three variants  g Rationale e subscript superscript g e Rationale g^{e}_{\\textsc{Rationale}} italic_g start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT start_POSTSUBSCRIPT Rationale end_POSTSUBSCRIPT ,  g Query e subscript superscript g e Query g^{e}_{\\textsc{Query}} italic_g start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT start_POSTSUBSCRIPT Query end_POSTSUBSCRIPT , and  g Doc e subscript superscript g e Doc g^{e}_{\\textsc{Doc}} italic_g start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT start_POSTSUBSCRIPT Doc end_POSTSUBSCRIPT , correspondingly evaluating rationales, queries and the retrieved documents.",
            "As mentioned in Section  2.1 , given the current state  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and action  a t subscript a t a_{t} italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , we employ three specific functions to generate different types of outcomes.  The generator  f g  e  n  (  ) subscript f g e n  f_{gen}(\\cdot) italic_f start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT (  )  generates either a  Rationale  or  Query . The retriever  f r  e  t  r  (  ) subscript f r e t r  f_{retr}(\\cdot) italic_f start_POSTSUBSCRIPT italic_r italic_e italic_t italic_r end_POSTSUBSCRIPT (  )  outputs a  Doc .  Last but not least, the rule-based function  f r  u  l  e  (  ) subscript f r u l e  f_{rule}(\\cdot) italic_f start_POSTSUBSCRIPT italic_r italic_u italic_l italic_e end_POSTSUBSCRIPT (  )  outputs a  SubGoal .  The  SubGoal  is a predefined natural language. For example, a  Reason  thought is The next step is to generate a rationale.",
            "As shown in Figure  2 , MCTS consists of the four key steps:   (1) Selection.   Starting from the  root  state  s 0 subscript s 0 s_{0} italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , the algorithm selects child node (with observation) recursively based on the Upper Confidence Bound (UCB1) that balances exploration and exploitation.  The UCB1 value for  o i subscript o i o_{i} italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is computed as  v i n i + c  ln  n p n i subscript v i subscript n i c subscript n p subscript n i \\frac{v_{i}}{n_{i}}+c\\sqrt{\\frac{\\ln n_{p}}{n_{i}}} divide start_ARG italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG + italic_c square-root start_ARG divide start_ARG roman_ln italic_n start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_ARG start_ARG italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG end_ARG , where  v i subscript v i v_{i} italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the cumulative rewards of  o i subscript o i o_{i} italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,  n i subscript n i n_{i} italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the number of times  o i subscript o i o_{i} italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  has been visited, and  n p subscript n p n_{p} italic_n start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  denotes the number of visits to the parent thought of  o i subscript o i o_{i} italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .  This process continues until it reaches a node that is not fully expanded or a terminal node.   (2) Expansion.   If the selected  o i subscript o i o_{i} italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is not terminal and has unexplored child nodes, MCTS expands the tree by adding one or more of these unexplored child nodes. This represents exploring new actions available from the current action space  A s t subscript A subscript s t \\mathcal{A}_{s_{t}} caligraphic_A start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT .   (3) Simulation.   From the newly added observation, MCTS simulates a playthrough to a terminal state by employing a generative model  f g  e  n  (  ) subscript f g e n  f_{gen}(\\cdot) italic_f start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT (  )  to generate the final answer based on existing observations. This simulation estimates the potential outcome from the observation.   (4) Backpropagation.   The result of the simulation is then propagated back up the tree. Each node along the path to the root updates its statistics, including visit counts and total reward, which informs future selection decisions by reflecting the observed outcomes.  For each data point in the training dataset, we run MCTS for  N N N italic_N  steps and collect pairwise data from the final state for each observation type.  In particular, a chosen observation  o i subscript o i o_{i} italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the one with the highest score, while a rejected observation  o i  superscript subscript o i  o_{i}^{\\prime} italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is one of the observations sharing the same parent node but a lower score.  For critic model  g Rationale e  (  ) subscript superscript g e Rationale  g^{e}_{\\textsc{Rationale}}(\\cdot) italic_g start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT start_POSTSUBSCRIPT Rationale end_POSTSUBSCRIPT (  ) , we collect  D Rationale = { ( O i Rationale , o i , o i  )  ... } superscript D Rationale subscript superscript O Rationale i subscript o i superscript subscript o i  ... \\mathcal{D}^{\\textsc{Rationale}}=\\{(O^{\\textsc{Rationale}}_{i},o_{i},o_{i}^{%  \\prime})...\\} caligraphic_D start_POSTSUPERSCRIPT Rationale end_POSTSUPERSCRIPT = { ( italic_O start_POSTSUPERSCRIPT Rationale end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) ... } , where  O i Rationale subscript superscript O Rationale i O^{\\textsc{Rationale}}_{i} italic_O start_POSTSUPERSCRIPT Rationale end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  represents previous  Rationale s along the trajectory before the current  Rationale   o i subscript o i o_{i} italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .  It is crucial to evaluate  o i subscript o i o_{i} italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  considering all prior rationales.  The critic model  g Query e  (  ) subscript superscript g e Query  g^{e}_{\\textsc{Query}}(\\cdot) italic_g start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT start_POSTSUBSCRIPT Query end_POSTSUBSCRIPT (  )  uses  D Query = { ( o i Rationale , o i , o i  )  ... } superscript D Query subscript superscript o Rationale i subscript o i superscript subscript o i  ... \\mathcal{D}^{\\textsc{Query}}=\\{(o^{\\textsc{Rationale}}_{i},o_{i},o_{i}^{\\prime%  })...\\} caligraphic_D start_POSTSUPERSCRIPT Query end_POSTSUPERSCRIPT = { ( italic_o start_POSTSUPERSCRIPT Rationale end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) ... } , where  o i Rationale subscript superscript o Rationale i o^{\\textsc{Rationale}}_{i} italic_o start_POSTSUPERSCRIPT Rationale end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is one immediately preceding  Rationale  of  Query   o i subscript o i o_{i} italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .  For the critic model  g Doc e  (  ) subscript superscript g e Doc  g^{e}_{\\textsc{Doc}}(\\cdot) italic_g start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT start_POSTSUBSCRIPT Doc end_POSTSUBSCRIPT (  ) , we have  D i Doc = { ( o i Rationale , o i Query , o i , o i  )  ... } subscript superscript D Doc i subscript superscript o Rationale i subscript superscript o Query i subscript o i superscript subscript o i  ... \\mathcal{D}^{\\textsc{Doc}}_{i}=\\{(o^{\\textsc{Rationale}}_{i},o^{\\textsc{Query}%  }_{i},o_{i},o_{i}^{\\prime})...\\} caligraphic_D start_POSTSUPERSCRIPT Doc end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { ( italic_o start_POSTSUPERSCRIPT Rationale end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_o start_POSTSUPERSCRIPT Query end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) ... } , where  o i Rationale subscript superscript o Rationale i o^{\\textsc{Rationale}}_{i} italic_o start_POSTSUPERSCRIPT Rationale end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  o i Query subscript superscript o Query i o^{\\textsc{Query}}_{i} italic_o start_POSTSUPERSCRIPT Query end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  are the immediately preceding  Rationale  and  Query  of  Doc   o i subscript o i o_{i} italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .  Lastly, the  SubGoal  critic model  g g  (  ) superscript g g  g^{g}(\\cdot) italic_g start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT (  )  uses  D SubGoal = { ( O i , o i , o i  )  ... } superscript D SubGoal subscript O i subscript o i superscript subscript o i  ... \\mathcal{D}^{\\textsc{SubGoal}}=\\{(O_{i},o_{i},o_{i}^{\\prime})...\\} caligraphic_D start_POSTSUPERSCRIPT SubGoal end_POSTSUPERSCRIPT = { ( italic_O start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) ... } , where  O i subscript O i O_{i} italic_O start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  represents all previous observations of any type along the trajectory.",
            "For each of the collected training datasets described above, we train a dedicated critic model as shown in Figure  2 .  Following  Burges et al. ( 2005 )  and  Ouyang et al. ( 2022 ) , we employ pairwise ranking loss to optimize the parameters.",
            "Similar to competitive programming, as shown in Table  2 , we observe a notable performance improvement from CR-Planner, with 13.59% on TheoremQA-Math compared to standard prompting method.  This further demonstrates the effectiveness of CR-Planner in tasks requiring knowledge retrieval and complex reasoning.  Furthermore, in contrast to their behavior in the USACO benchmark, retrieval methods, such as standard RAG and CoK, do enhance performance in this task.  We attribute this to the shorter context of the retrieved documents in the math domain. With shorter retrieved documents, the base model is easier to determine which information to incorporate or discard.  Nevertheless, CR-Planner maximizes the benefits of both retrieval and reasoning, leading to the greatest performance improvement.",
            "Tackling challenging domain-specific tasks such as competitive programming requires extensive reasoning as well as advanced algorithmic knowledge, which base models may not inherently possess.  In this section, we examine the importance of accurately retrieving external knowledge to assist in solving competitive programming problems.  We instruct the model to concentrate solely on reasoning, employing the reasoning critic model  g Reason g subscript superscript g g Reason g^{g}_{\\textsc{Reason}} italic_g start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT Reason end_POSTSUBSCRIPT  to select a rationale for each reasoning step.  As shown in Table  5 , the performance without retrieval is lower. However, as discussed in Section  3.2  and by  Shi et al. ( 2024 ) , inaccurate retrieval could impair performance. This emphasizes the critical role of accurate retrieval and the overall effectiveness of CR-Planner."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "S3.T3.1.1",
        "footnotes": [],
        "references": [
            "As shown in Table  3 , CR-Planner consistently improves over the standard BM25 method by 10.31% and 7.9% on StackBio and StackEcon, respectively.  CoK improves the standard BM25 method, which indicates that reasoning before retrieval is crucial in such reasoning-heavy domain retrieval tasks.  However, CoK does not consistently enhance performance; for instance, it performs worse than CoT on StackBio.  We attribute this to the potential noise introduced by multiple suboptimal retrieval results.  These observations further highlight the effectiveness of the critic models in RC-Planner.",
            "The critic models play a key role in CR-Planner by guiding the selection of sub-goals and executions through inference.  Previous works often adopt proprietary LLMs as critic models ( e.g.,  GPT-4 and Claude), utilizing in-context learning to evaluate actions  (Gou et al.,  2024 ; Zhao et al.,  2024 ) .  In this sub-section, we compare CR-Planners performance when using either fine-tuned models or GPT-4 ( gpt-4o-2024-05-13 ) as critics on the USACO and StackBio datasets.  The results are presented in Figure  3 .  Although employing GPT-4 as the critic yields improvements over the baseline, CR-Planner consistently performs better with fine-tuned critic models.  Notably, the fine-tuned critic models lead to larger gains in tasks that require domain knowledge, such as StackBio.  This underscores the significance of domain-specific fine-tuning and the rationale behind CR-Planners use of fine-tuned critic models.",
            "Tackling challenging domain-specific tasks such as competitive programming requires extensive reasoning as well as advanced algorithmic knowledge, which base models may not inherently possess.  In this section, we examine the importance of accurately retrieving external knowledge to assist in solving competitive programming problems.  We instruct the model to concentrate solely on reasoning, employing the reasoning critic model  g Reason g subscript superscript g g Reason g^{g}_{\\textsc{Reason}} italic_g start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT Reason end_POSTSUBSCRIPT  to select a rationale for each reasoning step.  As shown in Table  5 , the performance without retrieval is lower. However, as discussed in Section  3.2  and by  Shi et al. ( 2024 ) , inaccurate retrieval could impair performance. This emphasizes the critical role of accurate retrieval and the overall effectiveness of CR-Planner."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "S4.T4.1.1",
        "footnotes": [],
        "references": [
            "Compared to previous methods like Self-RAG  (Asai et al.,  2024 ) , CR-Planner does not require fine-tuning the base model.  This flexibility allows CR-Planner to be applied across various base models, whether open-source or closed-source.  In this subsection, we showcase the effectiveness of our critic models on another closed-source model, Claude-3.5 ( claude-3-5-sonnet ), and an open-source model, Llama-3.1 ( Llama-3.1-70B-Instruct ).  As demonstrated in Table  4 , CR-Planner enhances both Claude-3.5 and Llama-3.1.  However, the improvements, 4.56% for Claude-3.5 and 2.61% for Llama-3.1, are smaller compared to the 7.49% boost seen with GPT-4.  We believe this is due to the critic models being trained on data collected from GPT-4, making them more attuned to GPT-4 during inference and potentially less optimized for other models.  Nonetheless, the plug-and-play nature of critic models in our CR-Planner presents a promising approach to distill planning capabilities from powerful LLMs. This planning ability can be utilized to directly guide smaller LLMs, which lack the strength to generate high-quality MCTS trajectories on their own.",
            "Throughout both the training and inference phases of CR-Planner, executing sub-goals involves sampling several possible candidates.  By increasing the number of candidates, the likelihood of identifying a better option may improve.  In this subsection, we study how changing the number of candidates sampled for sub-goal execution during inference affects performance.  However, due to cost concerns, we do not conduct ablation studies for the training phase.  As illustrated in Figure  4 , the improvements on USACO are substantial when increasing from one to two, but converge around three. We believe this is due to the limitations of the generator models reasoning capabilities and the retrievers accuracy. Without fine-tuning both generator and retriever models,  further performance gains would be challenging to achieve.  Therefore, to balance between performance and cost, we select three as the sampling number for the inference of main experiments."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S4.T5.1.1",
        "footnotes": [],
        "references": [
            "Tackling challenging domain-specific tasks such as competitive programming requires extensive reasoning as well as advanced algorithmic knowledge, which base models may not inherently possess.  In this section, we examine the importance of accurately retrieving external knowledge to assist in solving competitive programming problems.  We instruct the model to concentrate solely on reasoning, employing the reasoning critic model  g Reason g subscript superscript g g Reason g^{g}_{\\textsc{Reason}} italic_g start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT Reason end_POSTSUBSCRIPT  to select a rationale for each reasoning step.  As shown in Table  5 , the performance without retrieval is lower. However, as discussed in Section  3.2  and by  Shi et al. ( 2024 ) , inaccurate retrieval could impair performance. This emphasizes the critical role of accurate retrieval and the overall effectiveness of CR-Planner."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A2.T6.3.3",
        "footnotes": [],
        "references": [
            "Solving the MDP requires generating an optimal plan in the form of a trajectory:    = ( s 0 , a 0 , ... , s t , a t , ... , s T  1 , a T  1 , s T ) \\tau*=(s_{0},a_{0},...,s_{t},a_{t},...,s_{T-1},a_{T-1},s_{T}) italic_  = ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , ... , italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , ... , italic_s start_POSTSUBSCRIPT italic_T - 1 end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_T - 1 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT )  that maximizes the total expected rewards.  2 2 2 Details of state types and action spaces are in Appendix  C  Table  6 .",
            "The action space  A s t subscript A subscript s t \\mathcal{A}_{s_{t}} caligraphic_A start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT  varies depending on  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .  As previously discussed in Section  2.1  and outlined in Table  6 , for a state in the sub-goal stage, the action space leads to possible executions of that sub-goal, while for a state in the execution stage, the action space leads to the possible subsequent sub-goals.   R  ( s t , a ) R subscript s t a \\mathcal{R}(s_{t},a) caligraphic_R ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a )  is the expected reward when taking action  a a a italic_a  in state  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and estimated by the critic models:",
            "We provide detailed information on state types and action spaces for CR-Planner in Table  6 ."
        ]
    },
    "global_footnotes": [
        "We will make our code and data publicly available.",
        "Details of state types and action spaces are in Appendix",
        "Table",
        ".",
        "We exclude Self-RAG as a baseline because it requires training the base model, which is not feasible in our setup. This further highlights the flexibility of CR-Planner."
    ]
}