{
    "id_table_1": {
        "caption": "Table 1 :  Comparison of LLM-assisted thematic analysis workflows",
        "table": "S2.T1.4",
        "footnotes": [],
        "references": [
            "While some papers focus on the application of LLMs to specific use cases (i.e., data sets), others undertake establishing workflows for utilizing LLMs for QDA  [ 31 ,  32 ,  29 ] . As this papers methods employ thematic analysis, we center our literature review on that particular method of QDA. De Paoli (2024) and Katz et al. (2023) focused on human-in-the-loop LLM-assisted thematic analysis with an individual human colder, while Gao et al. (2024) developed an LLM-assisted collaborative coding platform for multiple researchers. Table  1  shows a comparison of these three workflows and the approaches they are grounded in.",
            "The objective for generating synthetic data was to generate data that were as realistic as possible to data that one might actually encounter when running studies in organizational research. We achieved that objective in a multi-step process. Step one involved generating backstories and data generation criteria for the model to use. Step two involved using generative text models to actually simulate data according to those generation criteria. There were two categories of criteria used in this process: those generated by a generative text model and those manually specified by the researchers. The model-specified criteria included: personas, contexts, themes, and sub-themes. The manually-specified criteria included: data type, data collection context, writing style, and writing length. The process for data simulation is shown in Figure  1 .",
            "One important objective of the GATOS workflow is to generate codes that capture the ideas in the data while avoiding redundancy in the codes. To track this, we noted the rate of code creation over time as each cluster was scrutinized by the generative text model. In the worst-case scenario, the model would create one (or more) new codes with each cluster it encountered. This would be unideal because there would almost certainly be thematically redundant clusters, which would lead to redundant codes being created and defeat the purpose of the codebook checking step. Instead, if the model actually were checking the codes in the codebook to decide to create a new code or not, we would expect to see the model generate fewer new codes over time as it encounters more clusters and the codebook becomes saturated. This would suggest that the model is finding existing codes in the codebook that can describe the ideas in the new clusters. The rate of code creation for the teammate feedback is shown in Figure  9 , for the organizational culture of ethics dataset in Figure  10 , and for the return to workplace dataset in Figure  11 .",
            "Once more, we also present the instances where there were no ideal matches between the GATOS workflow themes and the original sub-themes in Table  10 . The specific examples of worst fit between the GATOS workflow-generated themes for this dataset and original sub-themes included financial rewards for whistleblowing, and media scrutiny and public opinion. The nearest matches for the original whistleblower sub-theme alluded to financial rewards whereas the theme generated from the workflow was more general about whistleblowing and retaliation and omitted reference to rewards. Likewise, the media scrutiny and public opinion sub-theme was matched with the themes of (1) public perception and image and (2) media and public relations, which were close but not individually exact matches; however, together the two workflow-generated themes could capture the central idea of the original sub-theme.",
            "Finally, with the synthetic dataset of returning to the workplace after the pandemic, there were 63 sub-themes used to generate the original synthetic data. The codebook generated by the GATOS workflow contained 314 codes and 110 themes. An example comparison of the codebook generated by the GATOS workflow with the original sub-themes is shown in Table  11 . As with the other two datasets, there were several examples of near-perfect matches between the original sub-themes and the GATOS workflow-generated themes. For example, the original sub-theme resistance to traditional office hours was most closely matched with the GATOS workflow theme resistance to traditional office hours. Another near-perfect match was for the sub-theme concerns about micromanaging and surveillance, which was matched with the GATOS workflow theme micromanagement and surveillance concerns. In those instances, even the syntax of the sub-theme was captured in the GATOS workflow-generated theme along with the semantics.",
            "As with the other two datasets, we also present the instances where there were no ideal matches between the GATOS workflow themes and the original sub-themes in Table  12 . Unlike the first two synthetic datasets, however, there were no instances where the GATOS workflow themes did not match the original sub-themes. The worst match was for the sub-theme confusion over new policies and procedures, which was matched with the GATOS workflow theme uncertainty about specific post-pandemic office aspects. Those are close matches, with the difference possibly being the lack of confusion being mentioned in the GATOS workflow theme. The second sub-optimal match was for the sub-theme worry about being judged or evaluated, which was matched with the GATOS workflow themes social expectation pressure anxiety. The aspect possibly missing in the workflow-generated theme is an element of judgment.",
            "The plots of code generation rates over time for each dataset (Figures  9 ,  10 , and  11 ) show that the GATOS workflow was able to generate fewer new codes as it encountered more clusters. For us, this was only a recently observed phenomena with newer generative text models because prior models would not consistently generate internally coherent reasoning traces to decide whether or not to create a new code. Anecdotally, in our experience with prior versions of generative text models trained on fewer data, the models would display faulty reasoning 10-20% of the time. That error rate was unacceptable for applications in research. However, the newer generations of models (e.g., Mistral-small-22b) have improved to the point where they can consistently generate plausible and consistent reasoning. This level of reliability is crucial for any method that is intended to be used in research  [ 41 ,  42 ] ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Data Generation Criteria Generated by Text Model",
        "table": "S3.T2.4",
        "footnotes": [],
        "references": [
            "A listing of the types of data generation criteria used in this process that were generated by a language model is shown in Table  2 , and a listing of the manually specified data generation criteria is shown in Table  3 .",
            "After removing invalid responses, the simulated teammate feedback dataset consists of 854 written responses to that data collection context prompt. The distribution of length of the written responses is shown in Figure  2 . The average number of words in the responses was 194 and the median was 180. As shown in the figure, the responses demonstrated a bimodal distribution. This distribution was likely a function of the way the data were generated by forcing variety in response length through the writing style and writing length criteria.",
            "The first step in the GATOS workflow is to summarize the original data into distinct ideas. This step is necessary because the raw data may contain multiple ideas in a single response. For example, a participant may write a long response that contains multiple ideas about their perspective of their organizations culture of ethics. The goal of this step is to extract the key ideas from the raw data to make them easier to analyze in the subsequent steps. One can use a relatively small language model (e.g., 7 to 14 billion parameters) for this step. Here, we used the mistral-nemo-12b model for this information extraction task because it is small enough to run quickly but performant enough not to miss parts of the original data to summarize (along with being an open-source model and released under an Apache-2.0 license). The prompt used for summarization is given in the Appendix  8.2 . An example of the input and output for this information extraction is shown in table  4 .",
            "As with the other two datasets, we also present the instances where there were no ideal matches between the GATOS workflow themes and the original sub-themes in Table  12 . Unlike the first two synthetic datasets, however, there were no instances where the GATOS workflow themes did not match the original sub-themes. The worst match was for the sub-theme confusion over new policies and procedures, which was matched with the GATOS workflow theme uncertainty about specific post-pandemic office aspects. Those are close matches, with the difference possibly being the lack of confusion being mentioned in the GATOS workflow theme. The second sub-optimal match was for the sub-theme worry about being judged or evaluated, which was matched with the GATOS workflow themes social expectation pressure anxiety. The aspect possibly missing in the workflow-generated theme is an element of judgment."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Manually Specified Data Generation Criteria",
        "table": "S3.T3.4",
        "footnotes": [],
        "references": [
            "A listing of the types of data generation criteria used in this process that were generated by a language model is shown in Table  2 , and a listing of the manually specified data generation criteria is shown in Table  3 .",
            "After removing invalid responses, there were 823 responses in this dataset. The distribution of the length of those written responses for this scenario is shown in Figure  3 . The average number of words in the responses was 129 and the median was 95. The responses demonstrated a long tail distribution, with a few responses being much longer than the rest. This was a deviation from the bimodal distribution of first dataset.",
            "At a philosophical level, at this point in the process, it is still an open question whether those most similar codes actually do capture the idea(s) expressed in that cluster. To investigate this question, we instruct a generative text model to look at the cluster of ( n n n italic_n ) summary points, the existing codes in the codebook (as represented by the   n  k absent n k \\leq n*k  italic_n  italic_k  nearest neighbor codes), and decide whether or not a new code is needed based on whether the existing codes in the codebook provide sufficient thematic coverage of the cluster of summary points. If the model decides that a new code (or codes) is (are) needed, then it generates a new code and definition for that code, which are added to the codebook. If the model decides that a new code is not needed, then the process simply moves on to the next cluster of data points. The full prompt for this code consideration step is given in the Appendix  8.3 .",
            "The next step in the GATOS workflow is to create the actual codebook by iteratively reading the clusters of summary points and deciding whether to generate a new code or not. To start the process, however, we prompt a generative text model to create 20 hypothetical codes that one might expect to appear in a study of whatever topic we simulated. For example, for the teammate feedback study, we prompted the model to generate 20 hypothetical codes one would expect to describe some data collected in a study of teammate feedback. These initial synthetic codes are used to help the process get started by providing the process with some initial codes to consider from the nearest neighbor matching when generating new codes. The specific prompt is given in the Appendix  8.3 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Example of Information Extraction from Return to Work Synthetic Dataset",
        "table": "S4.T4.4",
        "footnotes": [],
        "references": [
            "The distribution of the length of the 1,110 written responses for scenario number three is shown in Figure  4 . The average number of words in the responses was 131 and the median was 97. The responses demonstrated a more realistic long tail distribution similar to the second dataset but with fewer responses being much longer than the rest in comparison with dataset number two. This distribution is more similar to our observations from prior work  [ 30 ]  where we found that responses to open-ended survey questions often have a long tail distribution.",
            "The first step in the GATOS workflow is to summarize the original data into distinct ideas. This step is necessary because the raw data may contain multiple ideas in a single response. For example, a participant may write a long response that contains multiple ideas about their perspective of their organizations culture of ethics. The goal of this step is to extract the key ideas from the raw data to make them easier to analyze in the subsequent steps. One can use a relatively small language model (e.g., 7 to 14 billion parameters) for this step. Here, we used the mistral-nemo-12b model for this information extraction task because it is small enough to run quickly but performant enough not to miss parts of the original data to summarize (along with being an open-source model and released under an Apache-2.0 license). The prompt used for summarization is given in the Appendix  8.2 . An example of the input and output for this information extraction is shown in table  4 .",
            "The next step in our process runs  C C C italic_C  many times, where  C C C italic_C  is the number of clusters from step two.  As mentioned before, at a high level we instruct the generative text model to read the summary points in a single cluster, read the nearest neighbor codes from the codebook that  might  describe the ideas in that cluster, and then decide whether to generate a new code or not. We used Mistral-22b-2409 for this step because it is sufficiently large to be able to exhibit reasoning steps while being small enough to run quickly on common consumer hardware. The prompt used for this step is given in the Appendix  8.4 .",
            "An example of the input and output from the information extraction step is given in table  4 . We ran this step for each entry in the datasets to extract the key ideas from each response, which makes the data more manageable for the next steps in the method. As shown here, the input is a medium-length simulated response from the return-to-office dataset, and the output is a five-point list of key ideas extracted from that response. The specified theme and sub-theme for the original synthesized data were Expectations for Hybrid Work Arrangements and Fears about Negative Impact on Career Advancement, respectively. Despite being given a single theme and sub-theme to discuss, the generative model included more than just those themes and sub-themes in the simulated response. This can be seen in the summary points extracted from the response, which include a mix of the specified theme and sub-theme as well as additional information that was not part of the original specified criteria (but was in the simulated data as a byproduct of the simulation)."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Example of Theme Identification from Initial Codebook",
        "table": "S4.T5.4",
        "footnotes": [],
        "references": [
            "By the end of step five, we have gone through each cluster of summary points and either identified a new code or decided that no new code was needed, thereby balancing the goals of identifying the recurring patterns in the summary points while also not generating too many redundant codes. In practice, there are still many near redundancies that exist, so we proceed to a final step in which we cluster these newly generated codes and prompt a model to identify the distinct themes. The process therefore culminates with a list of themes and codes belonging to those themes. This entire workflow is shown in Figure  5 . We call this workflow the Generative AI-enabled Theme Organization and Structuring (GATOS) method because it is designed to mimic parts of thematic analysis while also being distinct from actual traditional thematic analysis.",
            "The final step in the method is to simplify the codebook by trying to identify themes. This step is necessary because the generative text model may generate redundant codes in the preceding step and codes that belong together at a more abstract level. To address these issues, we used a step in which we clustered similar codes together and gave those to the model along with the instructions to identify higher-level themes in the clusters of codes where possible. The specific prompt used in this step is given in the Appendix  8.5 . The final output of the GATOS workflow is a set of themes and codes that should describe the common semantic patterns in the data.",
            "An example of the input cluster of codes and output suggested themes from the theme creation step are given in table  5 . The input codes contained some redundancies related to flexible work arrangements and work hours, so the generative text model identified those as part of the theme Flexibility in Work Arrangements. The model also identified themes related to uncertainty in work schedules and breaks, so those were combined into the theme Uncertainty in Work Schedules. This example also illustrates how individual codes could also persist in the theme identification process as in the case of the code Control Over Work Hours. The example also illustrates how there is a thin line between a code and a theme, as the code Control Over Work Hours could be seen as a theme in its own right, or it could have been subsumed into the theme Flexibility in Work Arrangements."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  Summary of GATOS Workflow",
        "table": "S4.T6.4",
        "footnotes": [],
        "references": [
            "The distributions of the number of summary points extracted for the teammate feedback dataset is shown in Figure  6 , for the organizational culture of ethics synthetic dataset in Figure  7 , and for the return to workplace dataset in Figure  8 . Coincidentally, the average number of summary points extracted from reach response for the three datasets was 6.04, 5.07, and 4.28, respectively. These values give readers a sense of the amount of information extracted from each response and how the original data (i.e., full response) can expand 4x-6x as it is broken into summary points for the next step in the process. Additionally, the distributions of these summary points more closely resembled normal distributions, which was a notable contrast compared to the long-tailed and bimodal distributions of the length of the original synthesized data.",
            "As the figures show, the GATOS workflow met the minimal success criterion of not always generating a new code for each cluster. This can be seen by the deviation of the blue line from the red 45-degree line in the figures. The blue line represents the cumulative number of new codes generated by the model, while the 45-degree line represents the trajectory one would expect to see the blue line follow if there were a new code created for each cluster. The blue line bending below the red line is a quick way to see that sometimes the model does not create a new code for a cluster. The figures also show how many overall codes are initially created for the three datasets. The summary table of the starting number of simulated data points, summary points, codes, and themes for each dataset is shown in Table  6 .",
            "In this section we present the answer to the central question of this paper:  how well do the codebooks generated through the GATOS workflow capture the themes and sub-themes used to generate the original synthetic data? . We present the answer to this question for each of the three datasets in their respective subsections. In each of those subsections, we present examples where the workflow nearly matches the original themes one-to-one and examples where the workflow only partially captures the original sub-theme. It is important to note that we are only presenting the comparisons for the GATOS workflow-generated themes and the original sub-themes. We do not present the comparisons for the individual codes generated by the GATOS workflow because the codes are not the final output of the workflow; however, we did find in our testing that those codes always matched an original sub-theme perfectly. One might expect that result given the number of codes generated by the workflow (in the range of 246 to 314, as shown in table  6 ) and the number of sub-themes used to generate the original synthetic data."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :  Best Examples of Comparison of Codebooks for Teammate Feedback Synthetic Dataset",
        "table": "S4.T7.4",
        "footnotes": [],
        "references": [
            "The distributions of the number of summary points extracted for the teammate feedback dataset is shown in Figure  6 , for the organizational culture of ethics synthetic dataset in Figure  7 , and for the return to workplace dataset in Figure  8 . Coincidentally, the average number of summary points extracted from reach response for the three datasets was 6.04, 5.07, and 4.28, respectively. These values give readers a sense of the amount of information extracted from each response and how the original data (i.e., full response) can expand 4x-6x as it is broken into summary points for the next step in the process. Additionally, the distributions of these summary points more closely resembled normal distributions, which was a notable contrast compared to the long-tailed and bimodal distributions of the length of the original synthesized data.",
            "There were 60 sub-themes (after removing a few redundant sub-themes) used to generate the original synthetic data for the teammate feedback dataset. The codebook generated by the GATOS workflow contained 249 codes and 89 themes. An example comparison of the codebook generated by the GATOS workflow with the original sub-themes is shown in Table  7 . In this table, we have selected the best examples of how the GATOS workflow themes aligned with the sub-themes in the original synthetic data. There were many examples where the GATOS workflow-generated themes closely matched the original sub-themes. For example, the original sub-theme effective use of feedback mechanisms was most closely matched with the GATOS workflow theme feedback mechanism effectiveness. Another near-perfect match was for the sub-theme embracing feedback as a learning opportunity, which was matched with the GATOS workflow theme openness to feedback. The latter example demonstrates how the GATOS workflow can capture the semantic meaning of a sub-theme without using the exact same words (i.e., openness and embracing)."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 :  Worst Comparisons of Codebooks for Teammate Feedback Synthetic Dataset",
        "table": "S4.T8.4",
        "footnotes": [],
        "references": [
            "At a philosophical level, at this point in the process, it is still an open question whether those most similar codes actually do capture the idea(s) expressed in that cluster. To investigate this question, we instruct a generative text model to look at the cluster of ( n n n italic_n ) summary points, the existing codes in the codebook (as represented by the   n  k absent n k \\leq n*k  italic_n  italic_k  nearest neighbor codes), and decide whether or not a new code is needed based on whether the existing codes in the codebook provide sufficient thematic coverage of the cluster of summary points. If the model decides that a new code (or codes) is (are) needed, then it generates a new code and definition for that code, which are added to the codebook. If the model decides that a new code is not needed, then the process simply moves on to the next cluster of data points. The full prompt for this code consideration step is given in the Appendix  8.3 .",
            "The first step in the GATOS workflow is to summarize the original data into distinct ideas. This step is necessary because the raw data may contain multiple ideas in a single response. For example, a participant may write a long response that contains multiple ideas about their perspective of their organizations culture of ethics. The goal of this step is to extract the key ideas from the raw data to make them easier to analyze in the subsequent steps. One can use a relatively small language model (e.g., 7 to 14 billion parameters) for this step. Here, we used the mistral-nemo-12b model for this information extraction task because it is small enough to run quickly but performant enough not to miss parts of the original data to summarize (along with being an open-source model and released under an Apache-2.0 license). The prompt used for summarization is given in the Appendix  8.2 . An example of the input and output for this information extraction is shown in table  4 .",
            "The next step in the GATOS workflow is to create the actual codebook by iteratively reading the clusters of summary points and deciding whether to generate a new code or not. To start the process, however, we prompt a generative text model to create 20 hypothetical codes that one might expect to appear in a study of whatever topic we simulated. For example, for the teammate feedback study, we prompted the model to generate 20 hypothetical codes one would expect to describe some data collected in a study of teammate feedback. These initial synthetic codes are used to help the process get started by providing the process with some initial codes to consider from the nearest neighbor matching when generating new codes. The specific prompt is given in the Appendix  8.3 .",
            "The next step in our process runs  C C C italic_C  many times, where  C C C italic_C  is the number of clusters from step two.  As mentioned before, at a high level we instruct the generative text model to read the summary points in a single cluster, read the nearest neighbor codes from the codebook that  might  describe the ideas in that cluster, and then decide whether to generate a new code or not. We used Mistral-22b-2409 for this step because it is sufficiently large to be able to exhibit reasoning steps while being small enough to run quickly on common consumer hardware. The prompt used for this step is given in the Appendix  8.4 .",
            "The final step in the method is to simplify the codebook by trying to identify themes. This step is necessary because the generative text model may generate redundant codes in the preceding step and codes that belong together at a more abstract level. To address these issues, we used a step in which we clustered similar codes together and gave those to the model along with the instructions to identify higher-level themes in the clusters of codes where possible. The specific prompt used in this step is given in the Appendix  8.5 . The final output of the GATOS workflow is a set of themes and codes that should describe the common semantic patterns in the data.",
            "The distributions of the number of summary points extracted for the teammate feedback dataset is shown in Figure  6 , for the organizational culture of ethics synthetic dataset in Figure  7 , and for the return to workplace dataset in Figure  8 . Coincidentally, the average number of summary points extracted from reach response for the three datasets was 6.04, 5.07, and 4.28, respectively. These values give readers a sense of the amount of information extracted from each response and how the original data (i.e., full response) can expand 4x-6x as it is broken into summary points for the next step in the process. Additionally, the distributions of these summary points more closely resembled normal distributions, which was a notable contrast compared to the long-tailed and bimodal distributions of the length of the original synthesized data.",
            "To avoid the temptation to cherry-pick the data, we also present the worst matches or unmatched themes for the teammate feedback synthetic dataset in Table  8 . In this table, we show the instances where there was no clear theme that perfectly aligned with a sub-theme used in the original data generation step. For example, the simulated sub-theme setting realistic goals for self-improvement was only matched by personal growth and self-reflection in the GATOS-generated themes. Those themes are missing an aspect of goal setting that was present in the original sub-theme. The worst match of these poor matches was for the sub-theme avoiding blame, which was matched with the themes personal bias management and negative behaviors and their impact. While these themes might indirectly relate to avoiding blame, they miss the essence of blame avoidance. It should be noted that the original codes generated by the GATOS workflow (before they were combined into themes) did indeed have a closer match of dismissive behavior, though this, too, misses the aspect of blame. All other 56 sub-themes had at least one good match with the GATOS workflow-generated themes."
        ]
    },
    "id_table_9": {
        "caption": "Table 9 :  Best Comparisons of Codebooks for Organizational Culture of Ethics Synthetic Dataset",
        "table": "S4.T9.4",
        "footnotes": [],
        "references": [
            "One important objective of the GATOS workflow is to generate codes that capture the ideas in the data while avoiding redundancy in the codes. To track this, we noted the rate of code creation over time as each cluster was scrutinized by the generative text model. In the worst-case scenario, the model would create one (or more) new codes with each cluster it encountered. This would be unideal because there would almost certainly be thematically redundant clusters, which would lead to redundant codes being created and defeat the purpose of the codebook checking step. Instead, if the model actually were checking the codes in the codebook to decide to create a new code or not, we would expect to see the model generate fewer new codes over time as it encounters more clusters and the codebook becomes saturated. This would suggest that the model is finding existing codes in the codebook that can describe the ideas in the new clusters. The rate of code creation for the teammate feedback is shown in Figure  9 , for the organizational culture of ethics dataset in Figure  10 , and for the return to workplace dataset in Figure  11 .",
            "With the synthetic dataset of organizational cultures of ethical behavior, there were 61 sub-themes used to generate the original synthetic data. The codebook generated by the GATOS workflow contained 246 codes and 75 themes. An example comparison of the codebook generated by the GATOS workflow with the original sub-themes is shown in Table  9 . As with the first dataset, for this initial table we have selected the best examples of how the GATOS workflow themes are most clearly aligned with the sub-themes in the original synthetic data. For example, the original sub-theme visible consequences for leadership misconduct was most closely matched with the GATOS workflow theme leadership misconduct and consequences. Another near-perfect match was for the sub-theme regulatory environment constraints, which was matched with the GATOS workflow theme regulatory environment dynamics.",
            "The plots of code generation rates over time for each dataset (Figures  9 ,  10 , and  11 ) show that the GATOS workflow was able to generate fewer new codes as it encountered more clusters. For us, this was only a recently observed phenomena with newer generative text models because prior models would not consistently generate internally coherent reasoning traces to decide whether or not to create a new code. Anecdotally, in our experience with prior versions of generative text models trained on fewer data, the models would display faulty reasoning 10-20% of the time. That error rate was unacceptable for applications in research. However, the newer generations of models (e.g., Mistral-small-22b) have improved to the point where they can consistently generate plausible and consistent reasoning. This level of reliability is crucial for any method that is intended to be used in research  [ 41 ,  42 ] ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10 :  Worst Comparisons of Codebooks for Organizational Culture of Ethics Synthetic Dataset",
        "table": "S4.T10.4",
        "footnotes": [],
        "references": [
            "One important objective of the GATOS workflow is to generate codes that capture the ideas in the data while avoiding redundancy in the codes. To track this, we noted the rate of code creation over time as each cluster was scrutinized by the generative text model. In the worst-case scenario, the model would create one (or more) new codes with each cluster it encountered. This would be unideal because there would almost certainly be thematically redundant clusters, which would lead to redundant codes being created and defeat the purpose of the codebook checking step. Instead, if the model actually were checking the codes in the codebook to decide to create a new code or not, we would expect to see the model generate fewer new codes over time as it encounters more clusters and the codebook becomes saturated. This would suggest that the model is finding existing codes in the codebook that can describe the ideas in the new clusters. The rate of code creation for the teammate feedback is shown in Figure  9 , for the organizational culture of ethics dataset in Figure  10 , and for the return to workplace dataset in Figure  11 .",
            "Once more, we also present the instances where there were no ideal matches between the GATOS workflow themes and the original sub-themes in Table  10 . The specific examples of worst fit between the GATOS workflow-generated themes for this dataset and original sub-themes included financial rewards for whistleblowing, and media scrutiny and public opinion. The nearest matches for the original whistleblower sub-theme alluded to financial rewards whereas the theme generated from the workflow was more general about whistleblowing and retaliation and omitted reference to rewards. Likewise, the media scrutiny and public opinion sub-theme was matched with the themes of (1) public perception and image and (2) media and public relations, which were close but not individually exact matches; however, together the two workflow-generated themes could capture the central idea of the original sub-theme.",
            "The plots of code generation rates over time for each dataset (Figures  9 ,  10 , and  11 ) show that the GATOS workflow was able to generate fewer new codes as it encountered more clusters. For us, this was only a recently observed phenomena with newer generative text models because prior models would not consistently generate internally coherent reasoning traces to decide whether or not to create a new code. Anecdotally, in our experience with prior versions of generative text models trained on fewer data, the models would display faulty reasoning 10-20% of the time. That error rate was unacceptable for applications in research. However, the newer generations of models (e.g., Mistral-small-22b) have improved to the point where they can consistently generate plausible and consistent reasoning. This level of reliability is crucial for any method that is intended to be used in research  [ 41 ,  42 ] ."
        ]
    },
    "id_table_11": {
        "caption": "Table 11 :  Best Comparisons of Codebooks for Returning to Workplace After the Pandemic Synthetic Dataset",
        "table": "S4.T11.4",
        "footnotes": [],
        "references": [
            "One important objective of the GATOS workflow is to generate codes that capture the ideas in the data while avoiding redundancy in the codes. To track this, we noted the rate of code creation over time as each cluster was scrutinized by the generative text model. In the worst-case scenario, the model would create one (or more) new codes with each cluster it encountered. This would be unideal because there would almost certainly be thematically redundant clusters, which would lead to redundant codes being created and defeat the purpose of the codebook checking step. Instead, if the model actually were checking the codes in the codebook to decide to create a new code or not, we would expect to see the model generate fewer new codes over time as it encounters more clusters and the codebook becomes saturated. This would suggest that the model is finding existing codes in the codebook that can describe the ideas in the new clusters. The rate of code creation for the teammate feedback is shown in Figure  9 , for the organizational culture of ethics dataset in Figure  10 , and for the return to workplace dataset in Figure  11 .",
            "Finally, with the synthetic dataset of returning to the workplace after the pandemic, there were 63 sub-themes used to generate the original synthetic data. The codebook generated by the GATOS workflow contained 314 codes and 110 themes. An example comparison of the codebook generated by the GATOS workflow with the original sub-themes is shown in Table  11 . As with the other two datasets, there were several examples of near-perfect matches between the original sub-themes and the GATOS workflow-generated themes. For example, the original sub-theme resistance to traditional office hours was most closely matched with the GATOS workflow theme resistance to traditional office hours. Another near-perfect match was for the sub-theme concerns about micromanaging and surveillance, which was matched with the GATOS workflow theme micromanagement and surveillance concerns. In those instances, even the syntax of the sub-theme was captured in the GATOS workflow-generated theme along with the semantics.",
            "The plots of code generation rates over time for each dataset (Figures  9 ,  10 , and  11 ) show that the GATOS workflow was able to generate fewer new codes as it encountered more clusters. For us, this was only a recently observed phenomena with newer generative text models because prior models would not consistently generate internally coherent reasoning traces to decide whether or not to create a new code. Anecdotally, in our experience with prior versions of generative text models trained on fewer data, the models would display faulty reasoning 10-20% of the time. That error rate was unacceptable for applications in research. However, the newer generations of models (e.g., Mistral-small-22b) have improved to the point where they can consistently generate plausible and consistent reasoning. This level of reliability is crucial for any method that is intended to be used in research  [ 41 ,  42 ] ."
        ]
    },
    "id_table_12": {
        "caption": "Table 12 :  Worst Comparisons of Codebooks for Returning to Workplace After the Pandemic Synthetic Dataset",
        "table": "S4.T12.4",
        "footnotes": [],
        "references": [
            "As with the other two datasets, we also present the instances where there were no ideal matches between the GATOS workflow themes and the original sub-themes in Table  12 . Unlike the first two synthetic datasets, however, there were no instances where the GATOS workflow themes did not match the original sub-themes. The worst match was for the sub-theme confusion over new policies and procedures, which was matched with the GATOS workflow theme uncertainty about specific post-pandemic office aspects. Those are close matches, with the difference possibly being the lack of confusion being mentioned in the GATOS workflow theme. The second sub-optimal match was for the sub-theme worry about being judged or evaluated, which was matched with the GATOS workflow themes social expectation pressure anxiety. The aspect possibly missing in the workflow-generated theme is an element of judgment."
        ]
    },
    "global_footnotes": []
}