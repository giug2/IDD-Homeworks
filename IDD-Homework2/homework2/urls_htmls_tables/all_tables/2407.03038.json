{
    "PAPER'S NUMBER OF TABLES": 10,
    "S6.T1": {
        "caption": "Table 1: Performce under summarization task. All values here indicate their best performance within 500 communication rounds of training.",
        "table": "<table id=\"S6.T1.3.3\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S6.T1.3.3.4\" class=\"ltx_tr\">\n<td id=\"S6.T1.3.3.4.1\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S6.T1.3.3.4.2\" class=\"ltx_td ltx_align_center\" colspan=\"2\">Selector</td>\n<td id=\"S6.T1.3.3.4.3\" class=\"ltx_td\"></td>\n<td id=\"S6.T1.3.3.4.4\" class=\"ltx_td ltx_align_center\" colspan=\"2\">LLM</td>\n</tr>\n<tr id=\"S6.T1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.1.1.2\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S6.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">Agreement</td>\n<td id=\"S6.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Best-of-<math id=\"S6.T1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"n\" display=\"inline\"><semantics id=\"S6.T1.1.1.1.1.m1.1a\"><mi id=\"S6.T1.1.1.1.1.m1.1.1\" xref=\"S6.T1.1.1.1.1.m1.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.1.1.1.1.m1.1b\"><ci id=\"S6.T1.1.1.1.1.m1.1.1.cmml\" xref=\"S6.T1.1.1.1.1.m1.1.1\">ùëõ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.1.1.1.1.m1.1c\">n</annotation></semantics></math>\n</td>\n<td id=\"S6.T1.1.1.1.4\" class=\"ltx_td\"></td>\n<td id=\"S6.T1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">Rating</td>\n<td id=\"S6.T1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">Win Rate</td>\n</tr>\n<tr id=\"S6.T1.3.3.5\" class=\"ltx_tr\">\n<td id=\"S6.T1.3.3.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">SFT</td>\n<td id=\"S6.T1.3.3.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td id=\"S6.T1.3.3.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td id=\"S6.T1.3.3.5.4\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S6.T1.3.3.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">5.028</td>\n<td id=\"S6.T1.3.3.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\">29.71%</td>\n</tr>\n<tr id=\"S6.T1.3.3.6\" class=\"ltx_tr\">\n<td id=\"S6.T1.3.3.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Centralized</td>\n<td id=\"S6.T1.3.3.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T1.3.3.6.2.1\" class=\"ltx_text ltx_font_bold\">73.10%</span></td>\n<td id=\"S6.T1.3.3.6.3\" class=\"ltx_td ltx_align_center\">5.302</td>\n<td id=\"S6.T1.3.3.6.4\" class=\"ltx_td\"></td>\n<td id=\"S6.T1.3.3.6.5\" class=\"ltx_td ltx_align_center\">5.688</td>\n<td id=\"S6.T1.3.3.6.6\" class=\"ltx_td ltx_align_center\">78.89%</td>\n</tr>\n<tr id=\"S6.T1.2.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T1.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T1.2.2.2.1.1\" class=\"ltx_text ltx_markedasmath\">FedBis</span></td>\n<td id=\"S6.T1.2.2.2.2\" class=\"ltx_td ltx_align_center\">70.44%</td>\n<td id=\"S6.T1.2.2.2.3\" class=\"ltx_td ltx_align_center\">5.274</td>\n<td id=\"S6.T1.2.2.2.4\" class=\"ltx_td\"></td>\n<td id=\"S6.T1.2.2.2.5\" class=\"ltx_td ltx_align_center\">5.661</td>\n<td id=\"S6.T1.2.2.2.6\" class=\"ltx_td ltx_align_center\">71.35%</td>\n</tr>\n<tr id=\"S6.T1.3.3.3\" class=\"ltx_tr\">\n<td id=\"S6.T1.3.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T1.3.3.3.1.1\" class=\"ltx_text ltx_markedasmath\">FedBiscuit</span></td>\n<td id=\"S6.T1.3.3.3.2\" class=\"ltx_td ltx_align_center\">70.52%</td>\n<td id=\"S6.T1.3.3.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T1.3.3.3.3.1\" class=\"ltx_text ltx_font_bold\">5.305</span></td>\n<td id=\"S6.T1.3.3.3.4\" class=\"ltx_td\"></td>\n<td id=\"S6.T1.3.3.3.5\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T1.3.3.3.5.1\" class=\"ltx_text ltx_font_bold\">5.703</span></td>\n<td id=\"S6.T1.3.3.3.6\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T1.3.3.3.6.1\" class=\"ltx_text ltx_font_bold\">80.65%</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In this section, the evaluation data originates from the TL;DR dataset, as mentioned in Section 5. The dataset comprises two disjoint parts: one ranked by a group of labelers for model-generated responses, and the other written by users to summarize the key content of a post. We use the former to compute the consistency between the selector and the human annotator. For the latter, we apply various metrics, including best-of-nùëõn, rating, and win rate. The results are presented in Table 1 and Figure 3."
        ]
    },
    "A1.T2": {
        "caption": "Table 2: Hyperparameter Settings for the Summarization Task",
        "table": "<table id=\"A1.T2.3.3\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A1.T2.3.3.4\" class=\"ltx_tr\">\n<td id=\"A1.T2.3.3.4.1\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"A1.T2.3.3.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">SFT</td>\n<td id=\"A1.T2.3.3.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">Selector Training</td>\n<td id=\"A1.T2.3.3.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">RLFT</td>\n</tr>\n<tr id=\"A1.T2.3.3.5\" class=\"ltx_tr\">\n<td id=\"A1.T2.3.3.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Participation Rate</td>\n<td id=\"A1.T2.3.3.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td id=\"A1.T2.3.3.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">5/53</td>\n<td id=\"A1.T2.3.3.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr id=\"A1.T2.3.3.6\" class=\"ltx_tr\">\n<td id=\"A1.T2.3.3.6.1\" class=\"ltx_td ltx_align_left\">Local Iterations</td>\n<td id=\"A1.T2.3.3.6.2\" class=\"ltx_td ltx_align_center\">30</td>\n<td id=\"A1.T2.3.3.6.3\" class=\"ltx_td ltx_align_center\">30</td>\n<td id=\"A1.T2.3.3.6.4\" class=\"ltx_td ltx_align_center\">30</td>\n</tr>\n<tr id=\"A1.T2.3.3.7\" class=\"ltx_tr\">\n<td id=\"A1.T2.3.3.7.1\" class=\"ltx_td ltx_align_left\">Batch Size</td>\n<td id=\"A1.T2.3.3.7.2\" class=\"ltx_td ltx_align_center\">32</td>\n<td id=\"A1.T2.3.3.7.3\" class=\"ltx_td ltx_align_center\">16</td>\n<td id=\"A1.T2.3.3.7.4\" class=\"ltx_td ltx_align_center\">32</td>\n</tr>\n<tr id=\"A1.T2.3.3.8\" class=\"ltx_tr\">\n<td id=\"A1.T2.3.3.8.1\" class=\"ltx_td ltx_align_left\">Rounds</td>\n<td id=\"A1.T2.3.3.8.2\" class=\"ltx_td ltx_align_center\">1000</td>\n<td id=\"A1.T2.3.3.8.3\" class=\"ltx_td ltx_align_center\">500</td>\n<td id=\"A1.T2.3.3.8.4\" class=\"ltx_td ltx_align_center\">500</td>\n</tr>\n<tr id=\"A1.T2.3.3.9\" class=\"ltx_tr\">\n<td id=\"A1.T2.3.3.9.1\" class=\"ltx_td ltx_align_left\">Optimizer</td>\n<td id=\"A1.T2.3.3.9.2\" class=\"ltx_td ltx_align_center\">AdamW</td>\n<td id=\"A1.T2.3.3.9.3\" class=\"ltx_td ltx_align_center\">AdamW</td>\n<td id=\"A1.T2.3.3.9.4\" class=\"ltx_td ltx_align_center\">RMSprop</td>\n</tr>\n<tr id=\"A1.T2.3.3.10\" class=\"ltx_tr\">\n<td id=\"A1.T2.3.3.10.1\" class=\"ltx_td ltx_align_left\">Hyperparameters</td>\n<td id=\"A1.T2.3.3.10.2\" class=\"ltx_td ltx_align_center\">(0.9, 0.95)</td>\n<td id=\"A1.T2.3.3.10.3\" class=\"ltx_td ltx_align_center\">(0.9, 0.95)</td>\n<td id=\"A1.T2.3.3.10.4\" class=\"ltx_td ltx_align_center\">‚Äì</td>\n</tr>\n<tr id=\"A1.T2.3.3.3\" class=\"ltx_tr\">\n<td id=\"A1.T2.3.3.3.4\" class=\"ltx_td ltx_align_left ltx_border_b\">Learning rate</td>\n<td id=\"A1.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_b\"><math id=\"A1.T2.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"1e-4\" display=\"inline\"><semantics id=\"A1.T2.1.1.1.1.m1.1a\"><mrow id=\"A1.T2.1.1.1.1.m1.1.1\" xref=\"A1.T2.1.1.1.1.m1.1.1.cmml\"><mrow id=\"A1.T2.1.1.1.1.m1.1.1.2\" xref=\"A1.T2.1.1.1.1.m1.1.1.2.cmml\"><mn id=\"A1.T2.1.1.1.1.m1.1.1.2.2\" xref=\"A1.T2.1.1.1.1.m1.1.1.2.2.cmml\">1</mn><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T2.1.1.1.1.m1.1.1.2.1\" xref=\"A1.T2.1.1.1.1.m1.1.1.2.1.cmml\">‚Äã</mo><mi id=\"A1.T2.1.1.1.1.m1.1.1.2.3\" xref=\"A1.T2.1.1.1.1.m1.1.1.2.3.cmml\">e</mi></mrow><mo id=\"A1.T2.1.1.1.1.m1.1.1.1\" xref=\"A1.T2.1.1.1.1.m1.1.1.1.cmml\">‚àí</mo><mn id=\"A1.T2.1.1.1.1.m1.1.1.3\" xref=\"A1.T2.1.1.1.1.m1.1.1.3.cmml\">4</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.T2.1.1.1.1.m1.1b\"><apply id=\"A1.T2.1.1.1.1.m1.1.1.cmml\" xref=\"A1.T2.1.1.1.1.m1.1.1\"><minus id=\"A1.T2.1.1.1.1.m1.1.1.1.cmml\" xref=\"A1.T2.1.1.1.1.m1.1.1.1\"></minus><apply id=\"A1.T2.1.1.1.1.m1.1.1.2.cmml\" xref=\"A1.T2.1.1.1.1.m1.1.1.2\"><times id=\"A1.T2.1.1.1.1.m1.1.1.2.1.cmml\" xref=\"A1.T2.1.1.1.1.m1.1.1.2.1\"></times><cn type=\"integer\" id=\"A1.T2.1.1.1.1.m1.1.1.2.2.cmml\" xref=\"A1.T2.1.1.1.1.m1.1.1.2.2\">1</cn><ci id=\"A1.T2.1.1.1.1.m1.1.1.2.3.cmml\" xref=\"A1.T2.1.1.1.1.m1.1.1.2.3\">ùëí</ci></apply><cn type=\"integer\" id=\"A1.T2.1.1.1.1.m1.1.1.3.cmml\" xref=\"A1.T2.1.1.1.1.m1.1.1.3\">4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T2.1.1.1.1.m1.1c\">1e-4</annotation></semantics></math></td>\n<td id=\"A1.T2.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_b\"><math id=\"A1.T2.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"1e-5\" display=\"inline\"><semantics id=\"A1.T2.2.2.2.2.m1.1a\"><mrow id=\"A1.T2.2.2.2.2.m1.1.1\" xref=\"A1.T2.2.2.2.2.m1.1.1.cmml\"><mrow id=\"A1.T2.2.2.2.2.m1.1.1.2\" xref=\"A1.T2.2.2.2.2.m1.1.1.2.cmml\"><mn id=\"A1.T2.2.2.2.2.m1.1.1.2.2\" xref=\"A1.T2.2.2.2.2.m1.1.1.2.2.cmml\">1</mn><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T2.2.2.2.2.m1.1.1.2.1\" xref=\"A1.T2.2.2.2.2.m1.1.1.2.1.cmml\">‚Äã</mo><mi id=\"A1.T2.2.2.2.2.m1.1.1.2.3\" xref=\"A1.T2.2.2.2.2.m1.1.1.2.3.cmml\">e</mi></mrow><mo id=\"A1.T2.2.2.2.2.m1.1.1.1\" xref=\"A1.T2.2.2.2.2.m1.1.1.1.cmml\">‚àí</mo><mn id=\"A1.T2.2.2.2.2.m1.1.1.3\" xref=\"A1.T2.2.2.2.2.m1.1.1.3.cmml\">5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.T2.2.2.2.2.m1.1b\"><apply id=\"A1.T2.2.2.2.2.m1.1.1.cmml\" xref=\"A1.T2.2.2.2.2.m1.1.1\"><minus id=\"A1.T2.2.2.2.2.m1.1.1.1.cmml\" xref=\"A1.T2.2.2.2.2.m1.1.1.1\"></minus><apply id=\"A1.T2.2.2.2.2.m1.1.1.2.cmml\" xref=\"A1.T2.2.2.2.2.m1.1.1.2\"><times id=\"A1.T2.2.2.2.2.m1.1.1.2.1.cmml\" xref=\"A1.T2.2.2.2.2.m1.1.1.2.1\"></times><cn type=\"integer\" id=\"A1.T2.2.2.2.2.m1.1.1.2.2.cmml\" xref=\"A1.T2.2.2.2.2.m1.1.1.2.2\">1</cn><ci id=\"A1.T2.2.2.2.2.m1.1.1.2.3.cmml\" xref=\"A1.T2.2.2.2.2.m1.1.1.2.3\">ùëí</ci></apply><cn type=\"integer\" id=\"A1.T2.2.2.2.2.m1.1.1.3.cmml\" xref=\"A1.T2.2.2.2.2.m1.1.1.3\">5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T2.2.2.2.2.m1.1c\">1e-5</annotation></semantics></math></td>\n<td id=\"A1.T2.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\"><math id=\"A1.T2.3.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"1e-6\" display=\"inline\"><semantics id=\"A1.T2.3.3.3.3.m1.1a\"><mrow id=\"A1.T2.3.3.3.3.m1.1.1\" xref=\"A1.T2.3.3.3.3.m1.1.1.cmml\"><mrow id=\"A1.T2.3.3.3.3.m1.1.1.2\" xref=\"A1.T2.3.3.3.3.m1.1.1.2.cmml\"><mn id=\"A1.T2.3.3.3.3.m1.1.1.2.2\" xref=\"A1.T2.3.3.3.3.m1.1.1.2.2.cmml\">1</mn><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T2.3.3.3.3.m1.1.1.2.1\" xref=\"A1.T2.3.3.3.3.m1.1.1.2.1.cmml\">‚Äã</mo><mi id=\"A1.T2.3.3.3.3.m1.1.1.2.3\" xref=\"A1.T2.3.3.3.3.m1.1.1.2.3.cmml\">e</mi></mrow><mo id=\"A1.T2.3.3.3.3.m1.1.1.1\" xref=\"A1.T2.3.3.3.3.m1.1.1.1.cmml\">‚àí</mo><mn id=\"A1.T2.3.3.3.3.m1.1.1.3\" xref=\"A1.T2.3.3.3.3.m1.1.1.3.cmml\">6</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.T2.3.3.3.3.m1.1b\"><apply id=\"A1.T2.3.3.3.3.m1.1.1.cmml\" xref=\"A1.T2.3.3.3.3.m1.1.1\"><minus id=\"A1.T2.3.3.3.3.m1.1.1.1.cmml\" xref=\"A1.T2.3.3.3.3.m1.1.1.1\"></minus><apply id=\"A1.T2.3.3.3.3.m1.1.1.2.cmml\" xref=\"A1.T2.3.3.3.3.m1.1.1.2\"><times id=\"A1.T2.3.3.3.3.m1.1.1.2.1.cmml\" xref=\"A1.T2.3.3.3.3.m1.1.1.2.1\"></times><cn type=\"integer\" id=\"A1.T2.3.3.3.3.m1.1.1.2.2.cmml\" xref=\"A1.T2.3.3.3.3.m1.1.1.2.2\">1</cn><ci id=\"A1.T2.3.3.3.3.m1.1.1.2.3.cmml\" xref=\"A1.T2.3.3.3.3.m1.1.1.2.3\">ùëí</ci></apply><cn type=\"integer\" id=\"A1.T2.3.3.3.3.m1.1.1.3.cmml\" xref=\"A1.T2.3.3.3.3.m1.1.1.3\">6</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T2.3.3.3.3.m1.1c\">1e-6</annotation></semantics></math></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In our work, we fine-tune all models using LoRA, which is consistently set to rank 8, ",
                "Œ±",
                "=",
                "16",
                "ùõº",
                "16",
                "\\alpha=16",
                ", and the dropout rate 0.0. For the generation, we apply with these parameters:",
                "If it is required to generate multiple completions, then we set the temperature to 1.0.",
                "If it is required to generate a single completion, then we adopt greedy search by setting the temperature to 0.0.",
                "In the following part, we show the hyperparameter setting for different tasks:"
            ]
        ]
    },
    "A1.T3": {
        "caption": "Table 3: Hyperparameter Settings for the QA Task",
        "table": "<table id=\"A1.T3.2.2\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A1.T3.2.2.3\" class=\"ltx_tr\">\n<td id=\"A1.T3.2.2.3.1\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"A1.T3.2.2.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Selector Training</td>\n<td id=\"A1.T3.2.2.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">RLFT</td>\n</tr>\n<tr id=\"A1.T3.2.2.4\" class=\"ltx_tr\">\n<td id=\"A1.T3.2.2.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Participation Rate</td>\n<td id=\"A1.T3.2.2.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">10/300</td>\n<td id=\"A1.T3.2.2.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr id=\"A1.T3.2.2.5\" class=\"ltx_tr\">\n<td id=\"A1.T3.2.2.5.1\" class=\"ltx_td ltx_align_left\">Local Iterations</td>\n<td id=\"A1.T3.2.2.5.2\" class=\"ltx_td ltx_align_center\">10</td>\n<td id=\"A1.T3.2.2.5.3\" class=\"ltx_td ltx_align_center\">10</td>\n</tr>\n<tr id=\"A1.T3.2.2.6\" class=\"ltx_tr\">\n<td id=\"A1.T3.2.2.6.1\" class=\"ltx_td ltx_align_left\">Batch Size</td>\n<td id=\"A1.T3.2.2.6.2\" class=\"ltx_td ltx_align_center\">16</td>\n<td id=\"A1.T3.2.2.6.3\" class=\"ltx_td ltx_align_center\">16</td>\n</tr>\n<tr id=\"A1.T3.2.2.7\" class=\"ltx_tr\">\n<td id=\"A1.T3.2.2.7.1\" class=\"ltx_td ltx_align_left\">Rounds</td>\n<td id=\"A1.T3.2.2.7.2\" class=\"ltx_td ltx_align_center\">200</td>\n<td id=\"A1.T3.2.2.7.3\" class=\"ltx_td ltx_align_center\">200</td>\n</tr>\n<tr id=\"A1.T3.2.2.8\" class=\"ltx_tr\">\n<td id=\"A1.T3.2.2.8.1\" class=\"ltx_td ltx_align_left\">Optimizer</td>\n<td id=\"A1.T3.2.2.8.2\" class=\"ltx_td ltx_align_center\">AdamW</td>\n<td id=\"A1.T3.2.2.8.3\" class=\"ltx_td ltx_align_center\">RMSprop</td>\n</tr>\n<tr id=\"A1.T3.2.2.9\" class=\"ltx_tr\">\n<td id=\"A1.T3.2.2.9.1\" class=\"ltx_td ltx_align_left\">Hyperparameters</td>\n<td id=\"A1.T3.2.2.9.2\" class=\"ltx_td ltx_align_center\">(0.9, 0.95)</td>\n<td id=\"A1.T3.2.2.9.3\" class=\"ltx_td ltx_align_center\">‚Äì</td>\n</tr>\n<tr id=\"A1.T3.2.2.2\" class=\"ltx_tr\">\n<td id=\"A1.T3.2.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_b\">Learning rate</td>\n<td id=\"A1.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_b\"><math id=\"A1.T3.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"1e-5\" display=\"inline\"><semantics id=\"A1.T3.1.1.1.1.m1.1a\"><mrow id=\"A1.T3.1.1.1.1.m1.1.1\" xref=\"A1.T3.1.1.1.1.m1.1.1.cmml\"><mrow id=\"A1.T3.1.1.1.1.m1.1.1.2\" xref=\"A1.T3.1.1.1.1.m1.1.1.2.cmml\"><mn id=\"A1.T3.1.1.1.1.m1.1.1.2.2\" xref=\"A1.T3.1.1.1.1.m1.1.1.2.2.cmml\">1</mn><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T3.1.1.1.1.m1.1.1.2.1\" xref=\"A1.T3.1.1.1.1.m1.1.1.2.1.cmml\">‚Äã</mo><mi id=\"A1.T3.1.1.1.1.m1.1.1.2.3\" xref=\"A1.T3.1.1.1.1.m1.1.1.2.3.cmml\">e</mi></mrow><mo id=\"A1.T3.1.1.1.1.m1.1.1.1\" xref=\"A1.T3.1.1.1.1.m1.1.1.1.cmml\">‚àí</mo><mn id=\"A1.T3.1.1.1.1.m1.1.1.3\" xref=\"A1.T3.1.1.1.1.m1.1.1.3.cmml\">5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.T3.1.1.1.1.m1.1b\"><apply id=\"A1.T3.1.1.1.1.m1.1.1.cmml\" xref=\"A1.T3.1.1.1.1.m1.1.1\"><minus id=\"A1.T3.1.1.1.1.m1.1.1.1.cmml\" xref=\"A1.T3.1.1.1.1.m1.1.1.1\"></minus><apply id=\"A1.T3.1.1.1.1.m1.1.1.2.cmml\" xref=\"A1.T3.1.1.1.1.m1.1.1.2\"><times id=\"A1.T3.1.1.1.1.m1.1.1.2.1.cmml\" xref=\"A1.T3.1.1.1.1.m1.1.1.2.1\"></times><cn type=\"integer\" id=\"A1.T3.1.1.1.1.m1.1.1.2.2.cmml\" xref=\"A1.T3.1.1.1.1.m1.1.1.2.2\">1</cn><ci id=\"A1.T3.1.1.1.1.m1.1.1.2.3.cmml\" xref=\"A1.T3.1.1.1.1.m1.1.1.2.3\">ùëí</ci></apply><cn type=\"integer\" id=\"A1.T3.1.1.1.1.m1.1.1.3.cmml\" xref=\"A1.T3.1.1.1.1.m1.1.1.3\">5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T3.1.1.1.1.m1.1c\">1e-5</annotation></semantics></math></td>\n<td id=\"A1.T3.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_b\"><math id=\"A1.T3.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"1e-6\" display=\"inline\"><semantics id=\"A1.T3.2.2.2.2.m1.1a\"><mrow id=\"A1.T3.2.2.2.2.m1.1.1\" xref=\"A1.T3.2.2.2.2.m1.1.1.cmml\"><mrow id=\"A1.T3.2.2.2.2.m1.1.1.2\" xref=\"A1.T3.2.2.2.2.m1.1.1.2.cmml\"><mn id=\"A1.T3.2.2.2.2.m1.1.1.2.2\" xref=\"A1.T3.2.2.2.2.m1.1.1.2.2.cmml\">1</mn><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T3.2.2.2.2.m1.1.1.2.1\" xref=\"A1.T3.2.2.2.2.m1.1.1.2.1.cmml\">‚Äã</mo><mi id=\"A1.T3.2.2.2.2.m1.1.1.2.3\" xref=\"A1.T3.2.2.2.2.m1.1.1.2.3.cmml\">e</mi></mrow><mo id=\"A1.T3.2.2.2.2.m1.1.1.1\" xref=\"A1.T3.2.2.2.2.m1.1.1.1.cmml\">‚àí</mo><mn id=\"A1.T3.2.2.2.2.m1.1.1.3\" xref=\"A1.T3.2.2.2.2.m1.1.1.3.cmml\">6</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.T3.2.2.2.2.m1.1b\"><apply id=\"A1.T3.2.2.2.2.m1.1.1.cmml\" xref=\"A1.T3.2.2.2.2.m1.1.1\"><minus id=\"A1.T3.2.2.2.2.m1.1.1.1.cmml\" xref=\"A1.T3.2.2.2.2.m1.1.1.1\"></minus><apply id=\"A1.T3.2.2.2.2.m1.1.1.2.cmml\" xref=\"A1.T3.2.2.2.2.m1.1.1.2\"><times id=\"A1.T3.2.2.2.2.m1.1.1.2.1.cmml\" xref=\"A1.T3.2.2.2.2.m1.1.1.2.1\"></times><cn type=\"integer\" id=\"A1.T3.2.2.2.2.m1.1.1.2.2.cmml\" xref=\"A1.T3.2.2.2.2.m1.1.1.2.2\">1</cn><ci id=\"A1.T3.2.2.2.2.m1.1.1.2.3.cmml\" xref=\"A1.T3.2.2.2.2.m1.1.1.2.3\">ùëí</ci></apply><cn type=\"integer\" id=\"A1.T3.2.2.2.2.m1.1.1.3.cmml\" xref=\"A1.T3.2.2.2.2.m1.1.1.3\">6</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T3.2.2.2.2.m1.1c\">1e-6</annotation></semantics></math></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In our work, we fine-tune all models using LoRA, which is consistently set to rank 8, ",
                "Œ±",
                "=",
                "16",
                "ùõº",
                "16",
                "\\alpha=16",
                ", and the dropout rate 0.0. For the generation, we apply with these parameters:",
                "If it is required to generate multiple completions, then we set the temperature to 1.0.",
                "If it is required to generate a single completion, then we adopt greedy search by setting the temperature to 0.0.",
                "In the following part, we show the hyperparameter setting for different tasks:"
            ]
        ]
    },
    "A1.SS2.SSS0.Px1.tab1": {
        "caption": "",
        "table": "<table id=\"A1.SS2.SSS0.Px1.tab1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A1.SS2.SSS0.Px1.tab1.1.1\" class=\"ltx_tr\">\n<td id=\"A1.SS2.SSS0.Px1.tab1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_t\">\n<span id=\"A1.SS2.SSS0.Px1.tab1.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.SS2.SSS0.Px1.tab1.1.1.1.1.1\" class=\"ltx_p\">Below is a forum post. Write a precise and concise summary that includes the most important points of the post.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">### SUBREDDIT: r/{subreddit}\n<br class=\"ltx_break\">### TITLE: {title}\n<br class=\"ltx_break\">### POST: {post}\n<br class=\"ltx_break\">### TL;DR:</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Previous works offer a number of human preference datasets, but none of them address the FL setting. This is the first work to discuss the possible data partition approaches to build a heterogeneous human preference dataset. To this end, we introduce a benchmark that includes several human preference datasets suitable for FL.",
            "RLHF typically involves supervised fine-tuning, reward modeling, and reward optimization, initially popularized by Christiano et¬†al. (2017). Proximal Policy Optimization (PPO) Schulman et¬†al. (2017) is a common RLHF algorithm, yet it struggles with instability, inefficiency, and high resource demands Choshen et¬†al. (2019); Engstrom et¬†al. (2020). These challenges have led to the development of alternative methods, such as Direct Preference Optimization (DPO) Rafailov et¬†al. (2023) and others Dong et¬†al. (2023); Zhao et¬†al. (2023); Azar et¬†al. (2024); Ethayarajh et¬†al. (2024); Gulcehre et¬†al. (2023), which offer more stable and efficient solutions. However, these methods typically operate within a centralized training framework, where the LLM owner retains control over the preference data. In contrast, our work seeks to expand data sources and incorporate real user preferences during the fine-tuning of the LLM.",
            "As demonstrated in experiments, FedBis‚Äôs performance improves first but may later decline with the increase of training rounds. This phenomenon, known as reward hacking, is discussed by Skalse et¬†al. (2022) as an inevitable issue in training a reward proxy model, which is used to enhance the performance of a policy model (e.g., LLM). However, we can mitigate this impact by delaying the inflection point, allowing the reward proxy model to continue improving performance for more training rounds and ultimately achieve a higher rating.",
            "It is noted that performance degradation occurs when a model is trained by clients with time-varying sizes in FedAvg Gu et¬†al. (2021); Wang and Ji (2023). In other words, Equation (4) is no longer suitable for multi-selector aggregation due to the fluctuation in the number of clients training a selector in each communication round. Therefore, FedBiscuit adopts a new aggregation rule as formulated in Equation (8).",
            "In this section, the evaluation data originates from the TL;DR dataset, as mentioned in Section 5. The dataset comprises two disjoint parts: one ranked by a group of labelers for model-generated responses, and the other written by users to summarize the key content of a post. We use the former to compute the consistency between the selector and the human annotator. For the latter, we apply various metrics, including best-of-nùëõn, rating, and win rate. The results are presented in Table 1 and Figure 3.",
            "The table shows that conventional centralized training outperforms the proposed FedBiscuit in terms of agreement. This is because the agreement evaluation data have a similar distribution to the training dataset, as their outputs are generated from the same language models and labeled by the same group of labelers Stiennon et¬†al. (2020). Consequently, centralized training performs better than the proposed FedBis and FedBiscuit, which are affected by data heterogeneity.",
            "One notable limitation of our work lies in the construction of the preference dataset, which relies solely on publicly available data rather than gathering information directly from real clients. By doing so, we miss out on the nuances and intricacies of individual preferences that can only be captured through firsthand data collection. As a result, our dataset may lack the depth and breadth necessary to fully comprehend the true heterogeneity of preferences among clients. Without access to authentic client data, we may inadvertently overlook important variations in preferences, potentially limiting the applicability and robustness of our findings.",
            "The table above may lead to conclusions different from those drawn from the summarization task. First, FedBis achieves better performance than FedBiscuit. This is within our expectations. First, these selectors are trained for a total of 200 rounds. As presented in Figure 3, FedBiscuit surpasses FedBis after 300 communication rounds. This is because the selectors of FedBiscuit are trained for 100 rounds only, while the selector of FedBis has been fully trained for 200 rounds. When the inflection point appears in FedBis, we can hypothesize that the dominance of FedBiscuit still exists.",
            "Another comparison arises between different numbers of generations to a given prompt. From the table, we notice that \"2-completion\" can achieve better performance than \"4-completion,\" meaning that the performance may not be relevant to the size of the RLFT training set. Instead, it may rely on the quality of the training data. As we can see, Alpaca-7B hardly generates high-quality data, leading to limited improvement with training with these generated data. In other words, if we generate more low-quality completions, the improvement of the model would be more limited. At the same time, we can hypothesize that if the generated data are of high quality, \"4-completion\" may outperform \"2-completion\" in terms of the final rating.",
            "In this section, we prepare two examples to illustrate the performance of our proposed FedBis and FedBiscuit and compare them with other baselines. Refer to Table 5 and Table 6 for the demonstration of the examples."
        ]
    },
    "A1.SS2.SSS0.Px1.tab2": {
        "caption": "",
        "table": "<table id=\"A1.SS2.SSS0.Px1.tab2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A1.SS2.SSS0.Px1.tab2.1.1\" class=\"ltx_tr\">\n<td id=\"A1.SS2.SSS0.Px1.tab2.1.1.1\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_t\">\n<span id=\"A1.SS2.SSS0.Px1.tab2.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.SS2.SSS0.Px1.tab2.1.1.1.1.1\" class=\"ltx_p\">Below is a forum post followed by two summaries. Pick a more precise and concise one that summarizes the most important points in the given forum post, without including unimportant or irrelevant details. State your choice with a single capital letter, i.e., √Ñ√Øf SUMMARY A is better, BÃà√Øf SUMMARY B is better.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">### SUBREDDIT: r/{subreddit}\n<br class=\"ltx_break\">### TITLE: {title}\n<br class=\"ltx_break\">### POST: {post}\n<br class=\"ltx_break\">### SUMMARY A: {output_A}\n<br class=\"ltx_break\">### SUMMARY B: {output_B}\n<br class=\"ltx_break\">### YOUR CHOICE:</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Previous works offer a number of human preference datasets, but none of them address the FL setting. This is the first work to discuss the possible data partition approaches to build a heterogeneous human preference dataset. To this end, we introduce a benchmark that includes several human preference datasets suitable for FL.",
            "RLHF typically involves supervised fine-tuning, reward modeling, and reward optimization, initially popularized by Christiano et¬†al. (2017). Proximal Policy Optimization (PPO) Schulman et¬†al. (2017) is a common RLHF algorithm, yet it struggles with instability, inefficiency, and high resource demands Choshen et¬†al. (2019); Engstrom et¬†al. (2020). These challenges have led to the development of alternative methods, such as Direct Preference Optimization (DPO) Rafailov et¬†al. (2023) and others Dong et¬†al. (2023); Zhao et¬†al. (2023); Azar et¬†al. (2024); Ethayarajh et¬†al. (2024); Gulcehre et¬†al. (2023), which offer more stable and efficient solutions. However, these methods typically operate within a centralized training framework, where the LLM owner retains control over the preference data. In contrast, our work seeks to expand data sources and incorporate real user preferences during the fine-tuning of the LLM.",
            "As demonstrated in experiments, FedBis‚Äôs performance improves first but may later decline with the increase of training rounds. This phenomenon, known as reward hacking, is discussed by Skalse et¬†al. (2022) as an inevitable issue in training a reward proxy model, which is used to enhance the performance of a policy model (e.g., LLM). However, we can mitigate this impact by delaying the inflection point, allowing the reward proxy model to continue improving performance for more training rounds and ultimately achieve a higher rating.",
            "It is noted that performance degradation occurs when a model is trained by clients with time-varying sizes in FedAvg Gu et¬†al. (2021); Wang and Ji (2023). In other words, Equation (4) is no longer suitable for multi-selector aggregation due to the fluctuation in the number of clients training a selector in each communication round. Therefore, FedBiscuit adopts a new aggregation rule as formulated in Equation (8).",
            "In this section, the evaluation data originates from the TL;DR dataset, as mentioned in Section 5. The dataset comprises two disjoint parts: one ranked by a group of labelers for model-generated responses, and the other written by users to summarize the key content of a post. We use the former to compute the consistency between the selector and the human annotator. For the latter, we apply various metrics, including best-of-nùëõn, rating, and win rate. The results are presented in Table 1 and Figure 3.",
            "The table shows that conventional centralized training outperforms the proposed FedBiscuit in terms of agreement. This is because the agreement evaluation data have a similar distribution to the training dataset, as their outputs are generated from the same language models and labeled by the same group of labelers Stiennon et¬†al. (2020). Consequently, centralized training performs better than the proposed FedBis and FedBiscuit, which are affected by data heterogeneity.",
            "One notable limitation of our work lies in the construction of the preference dataset, which relies solely on publicly available data rather than gathering information directly from real clients. By doing so, we miss out on the nuances and intricacies of individual preferences that can only be captured through firsthand data collection. As a result, our dataset may lack the depth and breadth necessary to fully comprehend the true heterogeneity of preferences among clients. Without access to authentic client data, we may inadvertently overlook important variations in preferences, potentially limiting the applicability and robustness of our findings.",
            "The table above may lead to conclusions different from those drawn from the summarization task. First, FedBis achieves better performance than FedBiscuit. This is within our expectations. First, these selectors are trained for a total of 200 rounds. As presented in Figure 3, FedBiscuit surpasses FedBis after 300 communication rounds. This is because the selectors of FedBiscuit are trained for 100 rounds only, while the selector of FedBis has been fully trained for 200 rounds. When the inflection point appears in FedBis, we can hypothesize that the dominance of FedBiscuit still exists.",
            "Another comparison arises between different numbers of generations to a given prompt. From the table, we notice that \"2-completion\" can achieve better performance than \"4-completion,\" meaning that the performance may not be relevant to the size of the RLFT training set. Instead, it may rely on the quality of the training data. As we can see, Alpaca-7B hardly generates high-quality data, leading to limited improvement with training with these generated data. In other words, if we generate more low-quality completions, the improvement of the model would be more limited. At the same time, we can hypothesize that if the generated data are of high quality, \"4-completion\" may outperform \"2-completion\" in terms of the final rating.",
            "In this section, we prepare two examples to illustrate the performance of our proposed FedBis and FedBiscuit and compare them with other baselines. Refer to Table 5 and Table 6 for the demonstration of the examples."
        ]
    },
    "A1.SS2.SSS0.Px2.tab1": {
        "caption": "",
        "table": "<table id=\"A1.SS2.SSS0.Px2.tab1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A1.SS2.SSS0.Px2.tab1.1.1\" class=\"ltx_tr\">\n<td id=\"A1.SS2.SSS0.Px2.tab1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_t\">\n<span id=\"A1.SS2.SSS0.Px2.tab1.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.SS2.SSS0.Px2.tab1.1.1.1.1.1\" class=\"ltx_p\">Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">### Instruction:\n<br class=\"ltx_break\">{instruction}\n\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">### Input:\n<br class=\"ltx_break\">{input}\n\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">### Response:</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Previous works offer a number of human preference datasets, but none of them address the FL setting. This is the first work to discuss the possible data partition approaches to build a heterogeneous human preference dataset. To this end, we introduce a benchmark that includes several human preference datasets suitable for FL.",
            "RLHF typically involves supervised fine-tuning, reward modeling, and reward optimization, initially popularized by Christiano et¬†al. (2017). Proximal Policy Optimization (PPO) Schulman et¬†al. (2017) is a common RLHF algorithm, yet it struggles with instability, inefficiency, and high resource demands Choshen et¬†al. (2019); Engstrom et¬†al. (2020). These challenges have led to the development of alternative methods, such as Direct Preference Optimization (DPO) Rafailov et¬†al. (2023) and others Dong et¬†al. (2023); Zhao et¬†al. (2023); Azar et¬†al. (2024); Ethayarajh et¬†al. (2024); Gulcehre et¬†al. (2023), which offer more stable and efficient solutions. However, these methods typically operate within a centralized training framework, where the LLM owner retains control over the preference data. In contrast, our work seeks to expand data sources and incorporate real user preferences during the fine-tuning of the LLM.",
            "As demonstrated in experiments, FedBis‚Äôs performance improves first but may later decline with the increase of training rounds. This phenomenon, known as reward hacking, is discussed by Skalse et¬†al. (2022) as an inevitable issue in training a reward proxy model, which is used to enhance the performance of a policy model (e.g., LLM). However, we can mitigate this impact by delaying the inflection point, allowing the reward proxy model to continue improving performance for more training rounds and ultimately achieve a higher rating.",
            "It is noted that performance degradation occurs when a model is trained by clients with time-varying sizes in FedAvg Gu et¬†al. (2021); Wang and Ji (2023). In other words, Equation (4) is no longer suitable for multi-selector aggregation due to the fluctuation in the number of clients training a selector in each communication round. Therefore, FedBiscuit adopts a new aggregation rule as formulated in Equation (8).",
            "In this section, the evaluation data originates from the TL;DR dataset, as mentioned in Section 5. The dataset comprises two disjoint parts: one ranked by a group of labelers for model-generated responses, and the other written by users to summarize the key content of a post. We use the former to compute the consistency between the selector and the human annotator. For the latter, we apply various metrics, including best-of-nùëõn, rating, and win rate. The results are presented in Table 1 and Figure 3.",
            "The table shows that conventional centralized training outperforms the proposed FedBiscuit in terms of agreement. This is because the agreement evaluation data have a similar distribution to the training dataset, as their outputs are generated from the same language models and labeled by the same group of labelers Stiennon et¬†al. (2020). Consequently, centralized training performs better than the proposed FedBis and FedBiscuit, which are affected by data heterogeneity.",
            "One notable limitation of our work lies in the construction of the preference dataset, which relies solely on publicly available data rather than gathering information directly from real clients. By doing so, we miss out on the nuances and intricacies of individual preferences that can only be captured through firsthand data collection. As a result, our dataset may lack the depth and breadth necessary to fully comprehend the true heterogeneity of preferences among clients. Without access to authentic client data, we may inadvertently overlook important variations in preferences, potentially limiting the applicability and robustness of our findings.",
            "The table above may lead to conclusions different from those drawn from the summarization task. First, FedBis achieves better performance than FedBiscuit. This is within our expectations. First, these selectors are trained for a total of 200 rounds. As presented in Figure 3, FedBiscuit surpasses FedBis after 300 communication rounds. This is because the selectors of FedBiscuit are trained for 100 rounds only, while the selector of FedBis has been fully trained for 200 rounds. When the inflection point appears in FedBis, we can hypothesize that the dominance of FedBiscuit still exists.",
            "Another comparison arises between different numbers of generations to a given prompt. From the table, we notice that \"2-completion\" can achieve better performance than \"4-completion,\" meaning that the performance may not be relevant to the size of the RLFT training set. Instead, it may rely on the quality of the training data. As we can see, Alpaca-7B hardly generates high-quality data, leading to limited improvement with training with these generated data. In other words, if we generate more low-quality completions, the improvement of the model would be more limited. At the same time, we can hypothesize that if the generated data are of high quality, \"4-completion\" may outperform \"2-completion\" in terms of the final rating.",
            "In this section, we prepare two examples to illustrate the performance of our proposed FedBis and FedBiscuit and compare them with other baselines. Refer to Table 5 and Table 6 for the demonstration of the examples."
        ]
    },
    "A1.SS2.SSS0.Px2.tab2": {
        "caption": "",
        "table": "<table id=\"A1.SS2.SSS0.Px2.tab2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A1.SS2.SSS0.Px2.tab2.1.1\" class=\"ltx_tr\">\n<td id=\"A1.SS2.SSS0.Px2.tab2.1.1.1\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_t\">\n<span id=\"A1.SS2.SSS0.Px2.tab2.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.SS2.SSS0.Px2.tab2.1.1.1.1.1\" class=\"ltx_p\">Below is a query followed by two responses. Pick a helpful response that is precise, concise, and casual. State your choice with a single capital letter, i.e., √Ñ√Øf RESPONSE A is better, BÃà√Øf RESPONSE B is better.\n<br class=\"ltx_break\">\n<br class=\"ltx_break\">### QUERY: {instruction}\n<br class=\"ltx_break\">### RESPONSE A: {output_A}\n<br class=\"ltx_break\">### RESPONSE B: {output_B}\n<br class=\"ltx_break\">### YOUR CHOICE:</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Previous works offer a number of human preference datasets, but none of them address the FL setting. This is the first work to discuss the possible data partition approaches to build a heterogeneous human preference dataset. To this end, we introduce a benchmark that includes several human preference datasets suitable for FL.",
            "RLHF typically involves supervised fine-tuning, reward modeling, and reward optimization, initially popularized by Christiano et¬†al. (2017). Proximal Policy Optimization (PPO) Schulman et¬†al. (2017) is a common RLHF algorithm, yet it struggles with instability, inefficiency, and high resource demands Choshen et¬†al. (2019); Engstrom et¬†al. (2020). These challenges have led to the development of alternative methods, such as Direct Preference Optimization (DPO) Rafailov et¬†al. (2023) and others Dong et¬†al. (2023); Zhao et¬†al. (2023); Azar et¬†al. (2024); Ethayarajh et¬†al. (2024); Gulcehre et¬†al. (2023), which offer more stable and efficient solutions. However, these methods typically operate within a centralized training framework, where the LLM owner retains control over the preference data. In contrast, our work seeks to expand data sources and incorporate real user preferences during the fine-tuning of the LLM.",
            "As demonstrated in experiments, FedBis‚Äôs performance improves first but may later decline with the increase of training rounds. This phenomenon, known as reward hacking, is discussed by Skalse et¬†al. (2022) as an inevitable issue in training a reward proxy model, which is used to enhance the performance of a policy model (e.g., LLM). However, we can mitigate this impact by delaying the inflection point, allowing the reward proxy model to continue improving performance for more training rounds and ultimately achieve a higher rating.",
            "It is noted that performance degradation occurs when a model is trained by clients with time-varying sizes in FedAvg Gu et¬†al. (2021); Wang and Ji (2023). In other words, Equation (4) is no longer suitable for multi-selector aggregation due to the fluctuation in the number of clients training a selector in each communication round. Therefore, FedBiscuit adopts a new aggregation rule as formulated in Equation (8).",
            "In this section, the evaluation data originates from the TL;DR dataset, as mentioned in Section 5. The dataset comprises two disjoint parts: one ranked by a group of labelers for model-generated responses, and the other written by users to summarize the key content of a post. We use the former to compute the consistency between the selector and the human annotator. For the latter, we apply various metrics, including best-of-nùëõn, rating, and win rate. The results are presented in Table 1 and Figure 3.",
            "The table shows that conventional centralized training outperforms the proposed FedBiscuit in terms of agreement. This is because the agreement evaluation data have a similar distribution to the training dataset, as their outputs are generated from the same language models and labeled by the same group of labelers Stiennon et¬†al. (2020). Consequently, centralized training performs better than the proposed FedBis and FedBiscuit, which are affected by data heterogeneity.",
            "One notable limitation of our work lies in the construction of the preference dataset, which relies solely on publicly available data rather than gathering information directly from real clients. By doing so, we miss out on the nuances and intricacies of individual preferences that can only be captured through firsthand data collection. As a result, our dataset may lack the depth and breadth necessary to fully comprehend the true heterogeneity of preferences among clients. Without access to authentic client data, we may inadvertently overlook important variations in preferences, potentially limiting the applicability and robustness of our findings.",
            "The table above may lead to conclusions different from those drawn from the summarization task. First, FedBis achieves better performance than FedBiscuit. This is within our expectations. First, these selectors are trained for a total of 200 rounds. As presented in Figure 3, FedBiscuit surpasses FedBis after 300 communication rounds. This is because the selectors of FedBiscuit are trained for 100 rounds only, while the selector of FedBis has been fully trained for 200 rounds. When the inflection point appears in FedBis, we can hypothesize that the dominance of FedBiscuit still exists.",
            "Another comparison arises between different numbers of generations to a given prompt. From the table, we notice that \"2-completion\" can achieve better performance than \"4-completion,\" meaning that the performance may not be relevant to the size of the RLFT training set. Instead, it may rely on the quality of the training data. As we can see, Alpaca-7B hardly generates high-quality data, leading to limited improvement with training with these generated data. In other words, if we generate more low-quality completions, the improvement of the model would be more limited. At the same time, we can hypothesize that if the generated data are of high quality, \"4-completion\" may outperform \"2-completion\" in terms of the final rating.",
            "In this section, we prepare two examples to illustrate the performance of our proposed FedBis and FedBiscuit and compare them with other baselines. Refer to Table 5 and Table 6 for the demonstration of the examples."
        ]
    },
    "A2.T4": {
        "caption": "Table 4: Performce under QA.",
        "table": "<table id=\"A2.T4.2.2\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A2.T4.2.2.3\" class=\"ltx_tr\">\n<td id=\"A2.T4.2.2.3.1\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"A2.T4.2.2.3.2\" class=\"ltx_td ltx_align_center\">2-completion</td>\n<td id=\"A2.T4.2.2.3.3\" class=\"ltx_td\"></td>\n<td id=\"A2.T4.2.2.3.4\" class=\"ltx_td ltx_align_center\">4-completion</td>\n</tr>\n<tr id=\"A2.T4.2.2.4\" class=\"ltx_tr\">\n<td id=\"A2.T4.2.2.4.1\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"A2.T4.2.2.4.2\" class=\"ltx_td ltx_align_center\">Rating</td>\n<td id=\"A2.T4.2.2.4.3\" class=\"ltx_td\"></td>\n<td id=\"A2.T4.2.2.4.4\" class=\"ltx_td ltx_align_center\">Rating</td>\n</tr>\n<tr id=\"A2.T4.2.2.5\" class=\"ltx_tr\">\n<td id=\"A2.T4.2.2.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Alpaca-7B</td>\n<td id=\"A2.T4.2.2.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">3.752</td>\n<td id=\"A2.T4.2.2.5.3\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"A2.T4.2.2.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr id=\"A2.T4.1.1.1\" class=\"ltx_tr\">\n<td id=\"A2.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"A2.T4.1.1.1.1.1\" class=\"ltx_text ltx_markedasmath\">FedBis</span></td>\n<td id=\"A2.T4.1.1.1.2\" class=\"ltx_td ltx_align_center\">4.140</td>\n<td id=\"A2.T4.1.1.1.3\" class=\"ltx_td\"></td>\n<td id=\"A2.T4.1.1.1.4\" class=\"ltx_td ltx_align_center\">4.113</td>\n</tr>\n<tr id=\"A2.T4.2.2.2\" class=\"ltx_tr\">\n<td id=\"A2.T4.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"A2.T4.2.2.2.1.1\" class=\"ltx_text ltx_markedasmath\">FedBiscuit</span></td>\n<td id=\"A2.T4.2.2.2.2\" class=\"ltx_td ltx_align_center\">4.094</td>\n<td id=\"A2.T4.2.2.2.3\" class=\"ltx_td\"></td>\n<td id=\"A2.T4.2.2.2.4\" class=\"ltx_td ltx_align_center\">3.830</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, the test dataset comes from AlpacaFarm ",
                "Dubois et¬†al. (",
                "2024",
                "); Li et¬†al. (",
                "2023b",
                ")",
                ". The results are as follows:",
                "As presented in Section ",
                "4",
                ", the LLM owner will generate a set of responses to a given instruction before building a preference dataset. Therefore, the column \"2-completion\" means the owner prepares 2 completions for each instruction, while \"4-completion\" means 4 completions for each instruction and forms 6 pairs. The row \"Alpaca-7B\" acts as a baseline to help us understand the performance of the proposed ",
                "FedBiscuit",
                " and ",
                "FedBis",
                ". All the rating comes from Auto-J ",
                "Li et¬†al. (",
                "2023a",
                ")",
                ", which would be different from the ratings reported by ",
                "Li et¬†al. (",
                "2023b",
                ")",
                " because it evaluates with GPT-4 ",
                "Achiam et¬†al. (",
                "2023",
                ")",
                ".",
                "The table above may lead to conclusions different from those drawn from the summarization task. First, ",
                "FedBis",
                " achieves better performance than ",
                "FedBiscuit",
                ". This is within our expectations. First, these selectors are trained for a total of 200 rounds. As presented in Figure ",
                "3",
                ", ",
                "FedBiscuit",
                " surpasses ",
                "FedBis",
                " after 300 communication rounds. This is because the selectors of ",
                "FedBiscuit",
                " are trained for 100 rounds only, while the selector of ",
                "FedBis",
                " has been fully trained for 200 rounds. When the inflection point appears in ",
                "FedBis",
                ", we can hypothesize that the dominance of ",
                "FedBiscuit",
                " still exists.",
                "Another comparison arises between different numbers of generations to a given prompt. From the table, we notice that \"2-completion\" can achieve better performance than \"4-completion,\" meaning that the performance may not be relevant to the size of the RLFT training set. Instead, it may rely on the quality of the training data. As we can see, Alpaca-7B hardly generates high-quality data, leading to limited improvement with training with these generated data. In other words, if we generate more low-quality completions, the improvement of the model would be more limited. At the same time, we can hypothesize that if the generated data are of high quality, \"4-completion\" may outperform \"2-completion\" in terms of the final rating."
            ]
        ]
    },
    "A2.T5": {
        "caption": "Table 5: An example for summarization task. Caution: The example data that may be offensive or harmful.",
        "table": "<table id=\"A2.T5.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A2.T5.2.3\" class=\"ltx_tr\">\n<td id=\"A2.T5.2.3.1\" class=\"ltx_td ltx_align_justify\">\n<span id=\"A2.T5.2.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T5.2.3.1.1.1\" class=\"ltx_p\"><span class=\"ltx_rule\" style=\"width:100%;height:2.0pt;background:black;display:inline-block;\">¬†</span></span>\n</span>\nSubreddit: r/relationships\nTitle:\nMe [25F] with my SO [27 M] of 3.5 years, I went through his texts and can‚Äôt stop thinking about something I saw.\nPost:\nI admit that going through his text messages was wrong and I do feel incredibly guilty about it now but the bigger issue for me is that I found something that I can‚Äôt stop thinking about. He was talking to his good female friend and I saw a text that said \"am I being an idiot for leaving [my name]\"? . I stopped reading after that because I started having a panic attack. The idea of breaking up was not even on my radar and I thought he was more than happy. Again, I know I shouldn‚Äôt have looked but whats done is done and now I can‚Äôt stop worrying about when it‚Äôs going to happen. And now i‚Äôm being clingy just because of my worry which i‚Äôm sure doesn‚Äôt help. I love him and don‚Äôt want to lose him.</td>\n</tr>\n<tr id=\"A2.T5.2.4\" class=\"ltx_tr\">\n<td id=\"A2.T5.2.4.1\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"A2.T5.2.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T5.2.4.1.1.1\" class=\"ltx_p\"><span id=\"A2.T5.2.4.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Human Summary:</span></span>\n<span id=\"A2.T5.2.4.1.1.2\" class=\"ltx_p\">I saw a text from my SO to his friend about leaving me, can‚Äôt stop freaking out. Need serious advice.</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T5.2.5\" class=\"ltx_tr\">\n<td id=\"A2.T5.2.5.1\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"A2.T5.2.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T5.2.5.1.1.1\" class=\"ltx_p\"><span id=\"A2.T5.2.5.1.1.1.1\" class=\"ltx_text ltx_font_bold\">SFT:</span></span>\n<span id=\"A2.T5.2.5.1.1.2\" class=\"ltx_p\">I went through my SO‚Äôs texts and found something I wasn‚Äôt supposed to. Now I‚Äôm being clingy and I don‚Äôt want to lose him.</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T5.2.6\" class=\"ltx_tr\">\n<td id=\"A2.T5.2.6.1\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"A2.T5.2.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T5.2.6.1.1.1\" class=\"ltx_p\"><span id=\"A2.T5.2.6.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Centralized:</span></span>\n<span id=\"A2.T5.2.6.1.1.2\" class=\"ltx_p\">went through my boyfriends texts and found something that i‚Äôm now worrying about when/if he‚Äôs going to break up with me. I love him and don‚Äôt want to lose him. Help.</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T5.1.1\" class=\"ltx_tr\">\n<td id=\"A2.T5.1.1.1\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"A2.T5.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T5.1.1.1.1.1\" class=\"ltx_p\"><span id=\"A2.T5.1.1.1.1.1.1\" class=\"ltx_text ltx_markedasmath ltx_font_bold\">FedBis</span><span id=\"A2.T5.1.1.1.1.1.2\" class=\"ltx_text ltx_font_bold\">:</span></span>\n<span id=\"A2.T5.1.1.1.1.2\" class=\"ltx_p\">went through SO‚Äôs texts and found a text that said \"am I being an idot for leaving [my name]\"? and now I‚Äôm worried he‚Äôs going to break up with me and can‚Äôt stop thinking about it.</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T5.2.2\" class=\"ltx_tr\">\n<td id=\"A2.T5.2.2.1\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"A2.T5.2.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T5.2.2.1.1.1\" class=\"ltx_p\"><span id=\"A2.T5.2.2.1.1.1.1\" class=\"ltx_text ltx_markedasmath ltx_font_bold\">FedBiscuit</span><span id=\"A2.T5.2.2.1.1.1.2\" class=\"ltx_text ltx_font_bold\">:</span></span>\n<span id=\"A2.T5.2.2.1.1.2\" class=\"ltx_p\">went through SO‚Äôs texts, found something that i‚Äôm now worrying about when the inevitable breakup is going to happen. Now being clingy and can‚Äôt stop thinking about it. Help!</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T5.2.7\" class=\"ltx_tr\">\n<td id=\"A2.T5.2.7.1\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"A2.T5.2.7.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T5.2.7.1.1.1\" class=\"ltx_p\"><span class=\"ltx_rule\" style=\"width:100%;height:2.0pt;background:black;display:inline-block;\">¬†</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In this section, we prepare two examples to illustrate the performance of our proposed FedBis and FedBiscuit and compare them with other baselines. Refer to Table 5 and Table 6 for the demonstration of the examples."
        ]
    },
    "A2.T6": {
        "caption": "Table 6: An example for QA task. Caution: The example data that may be offensive or harmful.",
        "table": "<table id=\"A2.T6.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A2.T6.2.3\" class=\"ltx_tr\">\n<td id=\"A2.T6.2.3.1\" class=\"ltx_td ltx_align_justify\">\n<span id=\"A2.T6.2.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T6.2.3.1.1.1\" class=\"ltx_p\"><span class=\"ltx_rule\" style=\"width:100%;height:2.0pt;background:black;display:inline-block;\">¬†</span></span>\n</span>\nList the layers of the TCP/IP model and for each layer identify one protocol.</td>\n</tr>\n<tr id=\"A2.T6.2.4\" class=\"ltx_tr\">\n<td id=\"A2.T6.2.4.1\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"A2.T6.2.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T6.2.4.1.1.1\" class=\"ltx_p\"><span id=\"A2.T6.2.4.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Alpaca-7B:</span></span>\n<span id=\"A2.T6.2.4.1.1.2\" class=\"ltx_p\">The TCP/IP model consists of four layers: the application layer, transport layer, internet layer and network access layer.</span>\n<span id=\"A2.T6.2.4.1.1.3\" class=\"ltx_p\">Application Layer: HTTP\n<br class=\"ltx_break\">Transport Layer: TCP\n<br class=\"ltx_break\">Internet Layer: IP\n<br class=\"ltx_break\">Network Access Layer: Ethernet\n<br class=\"ltx_break\"></span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T6.1.1\" class=\"ltx_tr\">\n<td id=\"A2.T6.1.1.1\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"A2.T6.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T6.1.1.1.1.1\" class=\"ltx_p\"><span id=\"A2.T6.1.1.1.1.1.1\" class=\"ltx_text ltx_markedasmath ltx_font_bold\">FedBis</span><span id=\"A2.T6.1.1.1.1.1.2\" class=\"ltx_text ltx_font_bold\">:</span></span>\n<span id=\"A2.T6.1.1.1.1.2\" class=\"ltx_p\">The TCP/IP model has four layers: the application layer, transport layer, internet layer and network access layer. At the application layer, the protocol is TCP. At the transport layer, the protocol is UDP. At the internet layer, the protocol is IP. At the network access layer, the protocol is Ethernet.</span>\n<span id=\"A2.T6.1.1.1.1.3\" class=\"ltx_p\">1. Application layer: TCP\n<br class=\"ltx_break\">2. Transport layer: UDP\n<br class=\"ltx_break\">3. Internet layer: IP\n<br class=\"ltx_break\">4. Network access layer: Ethernet</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T6.2.2\" class=\"ltx_tr\">\n<td id=\"A2.T6.2.2.1\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"A2.T6.2.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T6.2.2.1.1.1\" class=\"ltx_p\"><span id=\"A2.T6.2.2.1.1.1.1\" class=\"ltx_text ltx_markedasmath ltx_font_bold\">FedBiscuit</span><span id=\"A2.T6.2.2.1.1.1.2\" class=\"ltx_text ltx_font_bold\">:</span></span>\n<span id=\"A2.T6.2.2.1.1.2\" class=\"ltx_p\">The TCP/IP model has four layers: the application layer, transport layer, internet layer and network access layer. At the application layer, the HTTP protocol is used. The transport layer uses the TCP protocol. At the internet layer, the IP protocol is used. Finally, the network access layer uses the Ethernet protocol.</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T6.2.5\" class=\"ltx_tr\">\n<td id=\"A2.T6.2.5.1\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"A2.T6.2.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T6.2.5.1.1.1\" class=\"ltx_p\"><span class=\"ltx_rule\" style=\"width:100%;height:2.0pt;background:black;display:inline-block;\">¬†</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In this section, we prepare two examples to illustrate the performance of our proposed FedBis and FedBiscuit and compare them with other baselines. Refer to Table 5 and Table 6 for the demonstration of the examples."
        ]
    }
}