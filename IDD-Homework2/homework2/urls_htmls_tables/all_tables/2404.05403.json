{
    "PAPER'S NUMBER OF TABLES": 4,
    "S4.T1": {
        "caption": "TABLE I: GIA against Local Updates (LPIPS↓↓\\downarrow).",
        "table": "<table id=\"S4.T1.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S4.T1.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-2.5pt 3.4pt;\" rowspan=\"2\"><span id=\"S4.T1.3.1.2.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S4.T1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-2.5pt 3.4pt;\" rowspan=\"2\"><span id=\"S4.T1.3.1.3.1\" class=\"ltx_text ltx_font_bold\">GIA</span></td>\n<td id=\"S4.T1.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-2.5pt 3.4pt;\" colspan=\"5\"><span id=\"S4.T1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Number of Update (<math id=\"S4.T1.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"U\" display=\"inline\"><semantics id=\"S4.T1.3.1.1.1.m1.1a\"><mi id=\"S4.T1.3.1.1.1.m1.1.1\" xref=\"S4.T1.3.1.1.1.m1.1.1.cmml\">U</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.3.1.1.1.m1.1b\"><ci id=\"S4.T1.3.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.3.1.1.1.m1.1.1\">𝑈</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.3.1.1.1.m1.1c\">U</annotation></semantics></math>)</span></td>\n</tr>\n<tr id=\"S4.T1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\">1</td>\n<td id=\"S4.T1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\">2</td>\n<td id=\"S4.T1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\">4</td>\n<td id=\"S4.T1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\">6</td>\n<td id=\"S4.T1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\">8</td>\n</tr>\n<tr id=\"S4.T1.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\" rowspan=\"2\"><span id=\"S4.T1.3.3.1.1\" class=\"ltx_text\">CIFAR10</span></td>\n<td id=\"S4.T1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\">GIA-O</td>\n<td id=\"S4.T1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\">0.0374</td>\n<td id=\"S4.T1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\">0.0933</td>\n<td id=\"S4.T1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\"><span id=\"S4.T1.3.3.5.1\" class=\"ltx_text ltx_font_bold\">0.1508</span></td>\n<td id=\"S4.T1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\"><span id=\"S4.T1.3.3.6.1\" class=\"ltx_text ltx_font_bold\">0.1917</span></td>\n<td id=\"S4.T1.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\"><span id=\"S4.T1.3.3.7.1\" class=\"ltx_text ltx_font_bold\">0.2089</span></td>\n</tr>\n<tr id=\"S4.T1.3.4\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\">GIA-L</td>\n<td id=\"S4.T1.3.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\">0.0608</td>\n<td id=\"S4.T1.3.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\"><span id=\"S4.T1.3.4.3.1\" class=\"ltx_text ltx_font_bold\">0.104</span></td>\n<td id=\"S4.T1.3.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\"><span id=\"S4.T1.3.4.4.1\" class=\"ltx_text ltx_font_bold\">0.1332</span></td>\n<td id=\"S4.T1.3.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\"><span id=\"S4.T1.3.4.5.1\" class=\"ltx_text ltx_font_bold\">0.1891</span></td>\n<td id=\"S4.T1.3.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\"><span id=\"S4.T1.3.4.6.1\" class=\"ltx_text ltx_font_bold\">0.1917</span></td>\n</tr>\n<tr id=\"S4.T1.3.5\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.5.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\" rowspan=\"2\"><span id=\"S4.T1.3.5.1.1\" class=\"ltx_text\">CIFAR100</span></td>\n<td id=\"S4.T1.3.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\">GIA-O</td>\n<td id=\"S4.T1.3.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\">0.0061</td>\n<td id=\"S4.T1.3.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\">0.0455</td>\n<td id=\"S4.T1.3.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\"><span id=\"S4.T1.3.5.5.1\" class=\"ltx_text ltx_font_bold\">0.1248</span></td>\n<td id=\"S4.T1.3.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\"><span id=\"S4.T1.3.5.6.1\" class=\"ltx_text ltx_font_bold\">0.163</span></td>\n<td id=\"S4.T1.3.5.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\"><span id=\"S4.T1.3.5.7.1\" class=\"ltx_text ltx_font_bold\">0.2305</span></td>\n</tr>\n<tr id=\"S4.T1.3.6\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.6.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\">GIA-L</td>\n<td id=\"S4.T1.3.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\">0.0382</td>\n<td id=\"S4.T1.3.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\">0.0578</td>\n<td id=\"S4.T1.3.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\"><span id=\"S4.T1.3.6.4.1\" class=\"ltx_text ltx_font_bold\">0.1222</span></td>\n<td id=\"S4.T1.3.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\"><span id=\"S4.T1.3.6.5.1\" class=\"ltx_text ltx_font_bold\">0.1712</span></td>\n<td id=\"S4.T1.3.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-2.5pt 3.4pt;\"><span id=\"S4.T1.3.6.6.1\" class=\"ltx_text ltx_font_bold\">0.1886</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We consider a scenario where a client possesses ",
                "N",
                "𝑁",
                "N",
                " data samples and configures ",
                "B",
                "=",
                "N",
                "𝐵",
                "𝑁",
                "B=N",
                ". Performing ",
                "U",
                "𝑈",
                "U",
                " updates on ",
                "𝐖",
                "0",
                "subscript",
                "𝐖",
                "0",
                "\\mathbf{W}_{0}",
                ", the client uploads ",
                "𝐖",
                "U",
                "subscript",
                "𝐖",
                "𝑈",
                "\\mathbf{W}_{U}",
                " to the server. We start by conducting a theoretical analysis of the GIA concerning multiple local updates within the context of a binary classification problem. Subsequently, we provide empirical evaluations on practical classification tasks.",
                "Theoretical Analysis.",
                " Consider a binary classification task (",
                "y",
                "∈",
                "{",
                "−",
                "1",
                ",",
                "1",
                "}",
                "𝑦",
                "1",
                "1",
                "y\\in\\{-1,1\\}",
                ") and a fully connected model with a single layer (ignoring the bias):",
                "where ",
                "𝐖",
                "𝐖",
                "\\mathbf{W}",
                " represents the weight matrix and ",
                "x",
                " denotes the input vector, ",
                "𝐖",
                ",",
                "𝐱",
                "T",
                "∈",
                "ℝ",
                "1",
                "×",
                "n",
                "𝐖",
                "superscript",
                "𝐱",
                "𝑇",
                "superscript",
                "ℝ",
                "1",
                "𝑛",
                "\\mathbf{W},\\mathbf{x}^{T}\\in\\mathbb{R}^{1\\times n}",
                ". The loss function ",
                "ℓ",
                "ℓ",
                "\\ell",
                " can be expressed as:",
                "The gradient can be expressed as:",
                "The dependence between the gradient ",
                "∂",
                "ℓ",
                "∂",
                "𝐖",
                "ℓ",
                "𝐖",
                "\\frac{\\partial\\ell}{\\partial\\mathbf{W}}",
                " and ",
                "μ",
                "𝜇",
                "\\mu",
                " can be deduced as follows:",
                "Fig. ",
                "3",
                " illustrates the dependence between gradient ",
                "∂",
                "ℓ",
                "∂",
                "𝐖",
                "ℓ",
                "𝐖",
                "\\frac{\\partial\\ell}{\\partial\\mathbf{W}}",
                ", ",
                "μ",
                "𝜇",
                "\\mu",
                ", and input ",
                "𝐱",
                "𝐱",
                "\\mathbf{x}",
                ". This implies that ",
                "μ",
                "𝜇",
                "\\mu",
                " can be determined by the gradient ",
                "∂",
                "ℓ",
                "∂",
                "𝐖",
                "ℓ",
                "𝐖",
                "\\frac{\\partial\\ell}{\\partial\\mathbf{W}}",
                " and weight ",
                "𝐖",
                "𝐖",
                "\\mathbf{W}",
                " by Eq. (",
                "6",
                "), then ",
                "𝐱",
                "𝐱",
                "\\mathbf{x}",
                " can be determined as follows:",
                "(1) If ",
                "U",
                "=",
                "1",
                "𝑈",
                "1",
                "U=1",
                ":",
                "The adversary can calculate the gradient ",
                "∂",
                "ℓ",
                "∂",
                "𝐖",
                "0",
                "ℓ",
                "subscript",
                "𝐖",
                "0",
                "\\frac{\\partial\\ell}{\\partial\\mathbf{W}_{0}}",
                " by ",
                "𝐖",
                "1",
                "−",
                "𝐖",
                "0",
                "η",
                "subscript",
                "𝐖",
                "1",
                "subscript",
                "𝐖",
                "0",
                "𝜂",
                "\\frac{\\mathbf{W}_{1}-\\mathbf{W}_{0}}{\\eta}",
                ". Then, ",
                "μ",
                "𝜇",
                "\\mu",
                " and ",
                "𝐱",
                "𝐱",
                "\\mathbf{x}",
                " can be determined based on Eq. (",
                "6",
                ") and (",
                "7",
                "), respectively, as illustrated in Fig. ",
                "3",
                " with ",
                "μ",
                "0",
                "subscript",
                "𝜇",
                "0",
                "\\mu_{0}",
                " and ",
                "𝐱",
                "0",
                "subscript",
                "𝐱",
                "0",
                "\\mathbf{x}_{0}",
                ". This indicates that when the local training of clients involves only one update (",
                "U",
                "=",
                "1",
                "𝑈",
                "1",
                "U=1",
                "), a definite correspondence exists between the gradient and the input, enabling the adversary to deduce ",
                "x",
                "𝑥",
                "x",
                " using an optimizer (e.g., Adam",
                "[",
                "45",
                "]",
                ") with gradient similarity as the loss function.",
                "(2) If ",
                "U",
                ">",
                "1",
                "𝑈",
                "1",
                "U>1",
                ":",
                "In this case, the adversary can obtain ",
                "𝐖",
                "U",
                "subscript",
                "𝐖",
                "𝑈",
                "\\mathbf{W}_{U}",
                " and ",
                "𝐖",
                "1",
                "subscript",
                "𝐖",
                "1",
                "\\mathbf{W}_{1}",
                ", but does not know ",
                "U",
                "𝑈",
                "U",
                ". As shown in Eq. (",
                "8",
                "), the update after ",
                "U",
                "𝑈",
                "U",
                " steps of mini-batch SGD is equal to:",
                "However, the gradient ",
                "𝐖",
                "U",
                "−",
                "𝐖",
                "0",
                "η",
                "subscript",
                "𝐖",
                "𝑈",
                "subscript",
                "𝐖",
                "0",
                "𝜂",
                "\\frac{\\mathbf{W}_{U}-\\mathbf{W}_{0}}{\\eta}",
                " includes ",
                "U",
                "−",
                "1",
                "𝑈",
                "1",
                "U-1",
                " redundant terms compared to the ground-truth gradient ",
                "∂",
                "ℓ",
                "∂",
                "𝐖",
                "0",
                "ℓ",
                "subscript",
                "𝐖",
                "0",
                "\\frac{\\partial\\ell}{\\partial\\mathbf{W}_{0}}",
                ", thus leading to obfuscated ",
                "μ",
                "𝜇",
                "\\mu",
                " and ",
                "𝐱",
                "𝐱",
                "\\mathbf{x}",
                ". Fig. ",
                "3",
                " gives two examples where the gradients are incorrect due to the accumulated terms, which results in the solved ",
                "μ",
                "1",
                "subscript",
                "𝜇",
                "1",
                "\\mu_{1}",
                " and ",
                "μ",
                "2",
                "subscript",
                "𝜇",
                "2",
                "\\mu_{2}",
                " drifting from ",
                "μ",
                "0",
                "subscript",
                "𝜇",
                "0",
                "\\mu_{0}",
                ". Thus, the reconstructed ",
                "𝐱",
                "1",
                "subscript",
                "𝐱",
                "1",
                "\\mathbf{x}_{1}",
                " and ",
                "𝐱",
                "2",
                "subscript",
                "𝐱",
                "2",
                "\\mathbf{x}_{2}",
                " also differ from ",
                "𝐱",
                "0",
                "subscript",
                "𝐱",
                "0",
                "\\mathbf{x}_{0}",
                ". ",
                "Hence, multiple local updates obfuscate the GIA",
                ".",
                "Empirical Analysis.",
                " To empirically validate our theoretical conclusion, we evaluate the performance of the two SOTA GIAs (GIA-O with ",
                "p",
                "T",
                "​",
                "V",
                "subscript",
                "𝑝",
                "𝑇",
                "𝑉",
                "p_{TV}",
                " and GIA-L with pretrained DcGAN",
                "[",
                "43",
                "]",
                ") as the number of local updates (",
                "U",
                "𝑈",
                "U",
                ") increases on the datasets CIFAR10 (",
                "N",
                "=",
                "B",
                "=",
                "4",
                "𝑁",
                "𝐵",
                "4",
                "N=B=4",
                ") and CIFAR100 (",
                "N",
                "=",
                "B",
                "=",
                "8",
                "𝑁",
                "𝐵",
                "8",
                "N=B=8",
                "). We use the metric ",
                "Learned Perceptual Image Patch Similarity (LPIPS",
                ")",
                "[",
                "49",
                "]",
                " to measure GIA’s performance, where a smaller value represents higher reconstruction quality. The detailed setup is described in Appendix. Tab. ",
                "I",
                " demonstrates that when ",
                "U",
                "𝑈",
                "U",
                " is 1 or 2, the adversary can still reconstruct the data. However, as ",
                "U",
                "𝑈",
                "U",
                " increases, the ",
                "LPIPS",
                " value exceeds ",
                "0.1",
                "0.1",
                "0.1",
                " (",
                "bold text",
                " in Tab. ",
                "I",
                "), indicating that the reconstructed images are completely unrecognizable. Furthermore, the reconstruction quality deteriorates as ",
                "U",
                "𝑈",
                "U",
                " increases, which indicates that the redundant terms in Eq. (",
                "8",
                ") gradually accumulate, and inaccuracy in the adversary’s update approximation grows."
            ]
        ]
    },
    "S4.T2": {
        "caption": "TABLE II: GIA against Mini-Batch SGD (LPIPS↓↓\\downarrow).",
        "table": "<table id=\"S4.T2.4\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S4.T2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-3.5pt 2.4pt;\" rowspan=\"2\"><span id=\"S4.T2.4.2.3.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S4.T2.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-3.5pt 2.4pt;\" rowspan=\"2\"><span id=\"S4.T2.4.2.4.1\" class=\"ltx_text ltx_font_bold\">GIA</span></td>\n<td id=\"S4.T2.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-3.5pt 2.4pt;\" rowspan=\"2\"><span id=\"S4.T2.3.1.1.1\" class=\"ltx_text\"><math id=\"S4.T2.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"B\" display=\"inline\"><semantics id=\"S4.T2.3.1.1.1.m1.1a\"><mi id=\"S4.T2.3.1.1.1.m1.1.1\" xref=\"S4.T2.3.1.1.1.m1.1.1.cmml\">B</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.3.1.1.1.m1.1b\"><ci id=\"S4.T2.3.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.3.1.1.1.m1.1.1\">𝐵</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.3.1.1.1.m1.1c\">B</annotation></semantics></math></span></td>\n<td id=\"S4.T2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-3.5pt 2.4pt;\" colspan=\"5\"><span id=\"S4.T2.4.2.2.1\" class=\"ltx_text ltx_font_bold\">Number of Mini-batch (<math id=\"S4.T2.4.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"N/B\" display=\"inline\"><semantics id=\"S4.T2.4.2.2.1.m1.1a\"><mrow id=\"S4.T2.4.2.2.1.m1.1.1\" xref=\"S4.T2.4.2.2.1.m1.1.1.cmml\"><mi id=\"S4.T2.4.2.2.1.m1.1.1.2\" xref=\"S4.T2.4.2.2.1.m1.1.1.2.cmml\">N</mi><mo id=\"S4.T2.4.2.2.1.m1.1.1.1\" xref=\"S4.T2.4.2.2.1.m1.1.1.1.cmml\">/</mo><mi id=\"S4.T2.4.2.2.1.m1.1.1.3\" xref=\"S4.T2.4.2.2.1.m1.1.1.3.cmml\">B</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.4.2.2.1.m1.1b\"><apply id=\"S4.T2.4.2.2.1.m1.1.1.cmml\" xref=\"S4.T2.4.2.2.1.m1.1.1\"><divide id=\"S4.T2.4.2.2.1.m1.1.1.1.cmml\" xref=\"S4.T2.4.2.2.1.m1.1.1.1\"></divide><ci id=\"S4.T2.4.2.2.1.m1.1.1.2.cmml\" xref=\"S4.T2.4.2.2.1.m1.1.1.2\">𝑁</ci><ci id=\"S4.T2.4.2.2.1.m1.1.1.3.cmml\" xref=\"S4.T2.4.2.2.1.m1.1.1.3\">𝐵</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.4.2.2.1.m1.1c\">N/B</annotation></semantics></math>)</span></td>\n</tr>\n<tr id=\"S4.T2.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">1</td>\n<td id=\"S4.T2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">2</td>\n<td id=\"S4.T2.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">3</td>\n<td id=\"S4.T2.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">4</td>\n<td id=\"S4.T2.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">5</td>\n</tr>\n<tr id=\"S4.T2.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\" rowspan=\"4\"><span id=\"S4.T2.4.4.1.1\" class=\"ltx_text\">CIFAR10</span></td>\n<td id=\"S4.T2.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\" rowspan=\"2\"><span id=\"S4.T2.4.4.2.1\" class=\"ltx_text\">GIA-O</span></td>\n<td id=\"S4.T2.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">known</td>\n<td id=\"S4.T2.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.066</td>\n<td id=\"S4.T2.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.096</td>\n<td id=\"S4.T2.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0696</td>\n<td id=\"S4.T2.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0943</td>\n<td id=\"S4.T2.4.4.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0931</td>\n</tr>\n<tr id=\"S4.T2.4.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.4.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\"><span id=\"S4.T2.4.5.1.1\" class=\"ltx_text ltx_font_bold\">unknown</span></td>\n<td id=\"S4.T2.4.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.066</td>\n<td id=\"S4.T2.4.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0961</td>\n<td id=\"S4.T2.4.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\"><span id=\"S4.T2.4.5.4.1\" class=\"ltx_text ltx_font_bold\">0.1034</span></td>\n<td id=\"S4.T2.4.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\"><span id=\"S4.T2.4.5.5.1\" class=\"ltx_text ltx_font_bold\">0.1142</span></td>\n<td id=\"S4.T2.4.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\"><span id=\"S4.T2.4.5.6.1\" class=\"ltx_text ltx_font_bold\">0.1158</span></td>\n</tr>\n<tr id=\"S4.T2.4.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.4.6.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\" rowspan=\"2\"><span id=\"S4.T2.4.6.1.1\" class=\"ltx_text\">GIA-L</span></td>\n<td id=\"S4.T2.4.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">known</td>\n<td id=\"S4.T2.4.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0678</td>\n<td id=\"S4.T2.4.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0534</td>\n<td id=\"S4.T2.4.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0791</td>\n<td id=\"S4.T2.4.6.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0647</td>\n<td id=\"S4.T2.4.6.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0715</td>\n</tr>\n<tr id=\"S4.T2.4.7\" class=\"ltx_tr\">\n<td id=\"S4.T2.4.7.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\"><span id=\"S4.T2.4.7.1.1\" class=\"ltx_text ltx_font_bold\">unknown</span></td>\n<td id=\"S4.T2.4.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0678</td>\n<td id=\"S4.T2.4.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.086</td>\n<td id=\"S4.T2.4.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\"><span id=\"S4.T2.4.7.4.1\" class=\"ltx_text ltx_font_bold\">0.1089</span></td>\n<td id=\"S4.T2.4.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\"><span id=\"S4.T2.4.7.5.1\" class=\"ltx_text ltx_font_bold\">0.105</span></td>\n<td id=\"S4.T2.4.7.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\"><span id=\"S4.T2.4.7.6.1\" class=\"ltx_text ltx_font_bold\">0.1127</span></td>\n</tr>\n<tr id=\"S4.T2.4.8\" class=\"ltx_tr\">\n<td id=\"S4.T2.4.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\" rowspan=\"4\"><span id=\"S4.T2.4.8.1.1\" class=\"ltx_text\">CIFAR100</span></td>\n<td id=\"S4.T2.4.8.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\" rowspan=\"2\"><span id=\"S4.T2.4.8.2.1\" class=\"ltx_text\">GIA-O</span></td>\n<td id=\"S4.T2.4.8.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">known</td>\n<td id=\"S4.T2.4.8.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0079</td>\n<td id=\"S4.T2.4.8.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0534</td>\n<td id=\"S4.T2.4.8.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0523</td>\n<td id=\"S4.T2.4.8.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0395</td>\n<td id=\"S4.T2.4.8.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0692</td>\n</tr>\n<tr id=\"S4.T2.4.9\" class=\"ltx_tr\">\n<td id=\"S4.T2.4.9.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\"><span id=\"S4.T2.4.9.1.1\" class=\"ltx_text ltx_font_bold\">unknown</span></td>\n<td id=\"S4.T2.4.9.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0079</td>\n<td id=\"S4.T2.4.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0431</td>\n<td id=\"S4.T2.4.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.058</td>\n<td id=\"S4.T2.4.9.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\"><span id=\"S4.T2.4.9.5.1\" class=\"ltx_text ltx_font_bold\">0.1022</span></td>\n<td id=\"S4.T2.4.9.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\"><span id=\"S4.T2.4.9.6.1\" class=\"ltx_text ltx_font_bold\">0.1145</span></td>\n</tr>\n<tr id=\"S4.T2.4.10\" class=\"ltx_tr\">\n<td id=\"S4.T2.4.10.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\" rowspan=\"2\"><span id=\"S4.T2.4.10.1.1\" class=\"ltx_text\">GIA-L</span></td>\n<td id=\"S4.T2.4.10.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">known</td>\n<td id=\"S4.T2.4.10.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0325</td>\n<td id=\"S4.T2.4.10.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0742</td>\n<td id=\"S4.T2.4.10.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0409</td>\n<td id=\"S4.T2.4.10.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.051</td>\n<td id=\"S4.T2.4.10.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0692</td>\n</tr>\n<tr id=\"S4.T2.4.11\" class=\"ltx_tr\">\n<td id=\"S4.T2.4.11.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\"><span id=\"S4.T2.4.11.1.1\" class=\"ltx_text ltx_font_bold\">unknown</span></td>\n<td id=\"S4.T2.4.11.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0325</td>\n<td id=\"S4.T2.4.11.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0774</td>\n<td id=\"S4.T2.4.11.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\">0.0821</td>\n<td id=\"S4.T2.4.11.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\"><span id=\"S4.T2.4.11.5.1\" class=\"ltx_text ltx_font_bold\">0.1016</span></td>\n<td id=\"S4.T2.4.11.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-3.5pt 2.4pt;\"><span id=\"S4.T2.4.11.6.1\" class=\"ltx_text ltx_font_bold\">0.1091</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Here, we discuss a more practical scenario where the client has ",
                "N",
                "𝑁",
                "N",
                " data samples and shares the ",
                "W",
                "U",
                "subscript",
                "W",
                "𝑈",
                "\\textbf{W}_{U}",
                " after several local mini-batch SGD updates (",
                "B",
                "<",
                "N",
                ",",
                "U",
                "=",
                "N",
                "/",
                "B",
                "formulae-sequence",
                "𝐵",
                "𝑁",
                "𝑈",
                "𝑁",
                "𝐵",
                "B<N,U=N/B",
                "). It is crucial for the adversary to ascertain the value of ",
                "B",
                "𝐵",
                "B",
                ". ",
                "Case 1:",
                " In the literature, it is assumed that the adversary knows ",
                "B",
                "𝐵",
                "B",
                ", enabling them to simulate the client’s mini-batch update using ",
                "N",
                "𝑁",
                "N",
                " random initializations to more closely match ",
                "𝐖",
                "U",
                "−",
                "𝐖",
                "0",
                "η",
                "subscript",
                "𝐖",
                "𝑈",
                "subscript",
                "𝐖",
                "0",
                "𝜂",
                "\\frac{\\mathbf{W}_{U}-\\mathbf{W}_{0}}{\\eta}",
                ". ",
                "Case 2:",
                " However, in practice, the client is not obligated to provide local training details to the server, thus the adversary can only approximate the update by conducting SGD (",
                "B",
                "=",
                "N",
                ",",
                "U",
                "=",
                "1",
                "formulae-sequence",
                "𝐵",
                "𝑁",
                "𝑈",
                "1",
                "B=N,U=1",
                ") with ",
                "N",
                "𝑁",
                "N",
                " samples. We evaluate GIAs in both cases, where the adversary has known or ",
                "unknown",
                " ",
                "B",
                "𝐵",
                "B",
                ", as shown in Tab. ",
                "II",
                ". We adjust the number of mini-batches by varying the value of ",
                "N",
                "/",
                "B",
                "𝑁",
                "𝐵",
                "N/B",
                " on datasets CIFAR10 (",
                "B",
                "=",
                "4",
                "𝐵",
                "4",
                "B=4",
                ") and CIFAR100 (",
                "B",
                "=",
                "8",
                "𝐵",
                "8",
                "B=8",
                ").",
                "The results show that under the same setting, successful reconstruction occurs if the adversary knows ",
                "B",
                "𝐵",
                "B",
                " and executes the attack by simulating the local update process of the client (as described in ",
                "Case 1",
                "). If B is unknown, reconstruction is still feasible with a limited number of mini-batches (e.g., ",
                "N",
                "/",
                "B",
                "=",
                "𝑁",
                "𝐵",
                "absent",
                "N/B=",
                " ",
                "1",
                "1",
                "1",
                " or ",
                "2",
                "2",
                "2",
                "). However, as the number increases, reconstruction fails (indicated by the bold ",
                "LPIPS",
                " value exceeding ",
                "0.1",
                "0.1",
                "0.1",
                "), because solely relying on a single step of SGD to approximate the entire local training is insufficient (as described in ",
                "Case 2",
                ")."
            ]
        ]
    },
    "S5.T3": {
        "caption": "TABLE III: The Performance of GIA Using Full or a Single NIN Block’s gradient.",
        "table": "<table id=\"S5.T3.2\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T3.2.3\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.3.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-3.15pt 3.0pt;\" rowspan=\"2\"><span id=\"S5.T3.2.3.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td id=\"S5.T3.2.3.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-3.15pt 3.0pt;\" rowspan=\"2\"><span id=\"S5.T3.2.3.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Metric</span></td>\n<td id=\"S5.T3.2.3.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-3.15pt 3.0pt;\" colspan=\"5\"><span id=\"S5.T3.2.3.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Used Gradient</span></td>\n</tr>\n<tr id=\"S5.T3.2.4\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Full</span></td>\n<td id=\"S5.T3.2.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">#1 NIN Block’s</span></td>\n<td id=\"S5.T3.2.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">#2 NIN Block’s</span></td>\n<td id=\"S5.T3.2.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">#3 NIN Block’s</span></td>\n<td id=\"S5.T3.2.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.4.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">#4 NIN Block’s</span></td>\n</tr>\n<tr id=\"S5.T3.2.5\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\" rowspan=\"2\"><span id=\"S5.T3.2.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">GoogleNet</span></td>\n<td id=\"S5.T3.2.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.5.2.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Params(Ratio)</span></td>\n<td id=\"S5.T3.2.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">54,909,86(100.00%)</span></td>\n<td id=\"S5.T3.2.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.5.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">149,376(2.72%)</span></td>\n<td id=\"S5.T3.2.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">640,848(11.68%)</span></td>\n<td id=\"S5.T3.2.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.5.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">60,928(6.58%)</span></td>\n<td id=\"S5.T3.2.5.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.5.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">508,032(9.25%)</span></td>\n</tr>\n<tr id=\"S5.T3.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.1\" class=\"ltx_td ltx_align_center\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.1.1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LPIPS<math id=\"S5.T3.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S5.T3.1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S5.T3.1.1.1.1.m1.1.1\" xref=\"S5.T3.1.1.1.1.m1.1.1.cmml\">↓</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.1.1.1.1.m1.1b\"><ci id=\"S5.T3.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1\">↓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.1.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math></span></td>\n<td id=\"S5.T3.1.1.2\" class=\"ltx_td ltx_align_center\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0614±0.003</span></td>\n<td id=\"S5.T3.1.1.3\" class=\"ltx_td ltx_align_center\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0654+0.004</span></td>\n<td id=\"S5.T3.1.1.4\" class=\"ltx_td ltx_align_center\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0667±0.001</span></td>\n<td id=\"S5.T3.1.1.5\" class=\"ltx_td ltx_align_center\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0710±0.005</span></td>\n<td id=\"S5.T3.1.1.6\" class=\"ltx_td ltx_align_center\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0620±0.006</span></td>\n</tr>\n<tr id=\"S5.T3.2.6\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.6.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\" rowspan=\"2\"><span id=\"S5.T3.2.6.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Inception V3</span></td>\n<td id=\"S5.T3.2.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.6.2.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Params(Ratio)</span></td>\n<td id=\"S5.T3.2.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.6.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">21,638,954(100.00%)</span></td>\n<td id=\"S5.T3.2.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.6.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">246,688(1.14%)</span></td>\n<td id=\"S5.T3.2.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.6.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">336,640(1.56%)</span></td>\n<td id=\"S5.T3.2.6.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.6.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">381,440(1.76%)</span></td>\n<td id=\"S5.T3.2.6.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.6.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">1,138,528(5.26%)</span></td>\n</tr>\n<tr id=\"S5.T3.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.2.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">LPIPS<math id=\"S5.T3.2.2.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S5.T3.2.2.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S5.T3.2.2.1.1.m1.1.1\" xref=\"S5.T3.2.2.1.1.m1.1.1.cmml\">↓</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.2.2.1.1.m1.1b\"><ci id=\"S5.T3.2.2.1.1.m1.1.1.cmml\" xref=\"S5.T3.2.2.1.1.m1.1.1\">↓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.2.2.1.1.m1.1c\">\\downarrow</annotation></semantics></math></span></td>\n<td id=\"S5.T3.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0332±0.004</span></td>\n<td id=\"S5.T3.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0472±0.001</span></td>\n<td id=\"S5.T3.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0435±0.002</span></td>\n<td id=\"S5.T3.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0433±0.003</span></td>\n<td id=\"S5.T3.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-3.15pt 3.0pt;\"><span id=\"S5.T3.2.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0360±0.005</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Model structure refers to the way layers are connected to each other. Early models such as Vgg",
                "[",
                "50",
                "]",
                " were sequentially connected. Later, in order to mitigate gradient vanishing",
                "[",
                "48",
                "]",
                " and enhance feature extraction capabilities, skip",
                "[",
                "48",
                "]",
                " and parallel",
                "[",
                "68",
                "]",
                " structures began to appear and gradually became the backbone of model designs. In this subsection, we explore the effect of model structures on the GIA with two widely used cases in practice: ",
                "skip connection",
                "[",
                "48",
                "]",
                " (used in models like ResNet and DenseNet families) and ",
                "net-in-net",
                "[",
                "68",
                "]",
                " (used in models like GoogleNet and other InceptionNet families).",
                "i@. Skip connection",
                " is a widely used structure in deep neural networks that helps address gradient vanishing during training",
                "[",
                "48",
                "]",
                ". It enables the flow of features from one layer to another by creating direct connections between non-adjacent layers. Generally, there are two common types of skip connections, derived from ResNet and DenseNet, as shown in Fig. ",
                "11",
                ". In ResNet, skip connections take the form of identity mappings, where the input to a layer is ",
                "added directly",
                " to the output of the subsequent layer. In contrast, DenseNet takes a more aggressive approach by densely ",
                "concatenating all previous layers within a block",
                ", enhancing feature reuse ",
                "[",
                "67",
                "]",
                ".",
                "To illustrate how skip connections affect the GIA, we start with a derivation on how they affect backpropagation. Fig. ",
                "12",
                " illustrates a particular layer (",
                "𝐈",
                "l",
                "subscript",
                "𝐈",
                "𝑙",
                "\\mathbf{I}_{l}",
                " and ",
                "𝐎",
                "l",
                "subscript",
                "𝐎",
                "𝑙",
                "\\mathbf{O}_{l}",
                " represent the input and output of layer ",
                "l",
                "𝑙",
                "l",
                ", respectively), which may have three types of connections: normal (sequential), ResNet-like, and DenseNet-like. Then, the process of backpropagation can be presented in three cases:",
                "For ",
                "normal case",
                ", ",
                "𝐎",
                "l",
                "∗",
                "=",
                "𝐎",
                "l",
                "subscript",
                "superscript",
                "𝐎",
                "𝑙",
                "subscript",
                "𝐎",
                "𝑙",
                "\\mathbf{O}^{*}_{l}=\\mathbf{O}_{l}",
                ":",
                "For ",
                "ResNet",
                " and ",
                "DenseNet",
                ", the gradient in the deeper layers is passed to the front layers by adding or concatenation ( \n",
                "\n",
                "⊕",
                "direct-sum",
                "\\boldsymbol{\\oplus}",
                "\n",
                " represents both operations in Eq. (",
                "12",
                ")):",
                "Compared with Eq. (",
                "11",
                "), there is one more ",
                "residual term",
                " in the gradient of Eq. (",
                "12",
                "). For models with skip connections, residual terms multiply cumulatively with backpropagation so that the gradient could contain more combinations, which prevents the gradient from vanishing and likewise ",
                "benefits GIAs by allowing them to utilize more information",
                ".",
                "We then verify the effect of skip connections on GIA based on ResNets and DenseNets, respectively. First, we evaluate how cutting skip connections at different positions affects the GIA on ResNet-18 and ResNet-34, as shown in Fig. ",
                "13",
                ". We find that ",
                "(1) the presence of skip connections enhances the performance of the GIA",
                ". Fig. ",
                "13",
                " demonstrates that the original models (i.e., N) are much more vulnerable to the GIA than others with connection cuts. Moreover, cutting the skip connection at any position significantly worsens the effectiveness of the GIA, even resulting in failed reconstruction (",
                "LPIPS",
                " ",
                ">",
                "0.1",
                "absent",
                "0.1",
                ">0.1",
                "). In addition, we find that ",
                "(2) skip connections close to shallow or deep layers have a greater impact on the GIA",
                ". Fig. ",
                "13(b)",
                " shows that cutting shallow connections (",
                "#",
                "​",
                "0",
                ",",
                "1",
                ",",
                "2",
                "#",
                "0",
                "1",
                "2",
                "\\#0,1,2",
                ") and deep connections (",
                "#",
                "​",
                "12",
                ",",
                "14",
                ",",
                "15",
                "#",
                "12",
                "14",
                "15",
                "\\#12,14,15",
                ") drastically worsen the performance of the GIA. For shallower connections, recent works ",
                "[",
                "71",
                ", ",
                "72",
                ", ",
                "73",
                "]",
                " regarding information flow in deep neural networks state that shallow layers are more sensitive to the input, and so are their gradients, which benefits GIAs. For deeper connections, inspired by Eq. (",
                "12",
                "), since the gradients are computed from deep to shallow, the absence of deeper residual terms has a more significant effect on the whole gradient and so on the GIA, like dominoes.",
                "Besides, we explore the impact of the number of skip connections on the GIA with DenseNets, considering their dense nature. We select DenseNet-43 and DenseNet-53",
                "[",
                "74",
                "]",
                " as baselines and obtain two variants for each by employing different cutting strategies",
                "[",
                "74",
                "]",
                ". As shown in Fig. ",
                "14",
                ", ",
                "reducing the number of skip connections greatly affects the performance of GIAs",
                ". Images that can be easily inverted in ",
                "baselines",
                " are largely unrecognizable in ",
                "variants-1",
                ". Moreover, GIAs are completely unable to invert any information from ",
                "variants-2",
                ". Reducing the number of skip connections in a neural network model decreases both the backpropagation paths and the residual terms. This leads to the gradients becoming less informative, which in turn limits the effectiveness of GIAs.",
                "ii@. Net-in-net",
                " (NIN), as proposed in ",
                "[",
                "68",
                "]",
                ", is a module that integrates multi-scale convolutional kernels within a single block, as illustrated in Fig. ",
                "11(b)",
                ". Essentially, NIN works like a widening layer, where the multi-scale convolutional kernel can capture richer features from the input. However, it is evident that wider layers and multi-scale kernels also yield more informative gradients through backpropagation, consequently enhancing the effectiveness of GIAs.",
                "To further demonstrate NIN’s enhancement in GIA’s performance, we conduct ablation studies on GoogleNet and InceptionNet-V3, as detailed in Tab. ",
                "III",
                ". Specifically, we compare the reconstruction results obtained using the complete gradients versus solely the gradients from a single NIN block. These findings validate that ",
                "the gradients from NIN blocks are crucial to the model’s vulnerability to the GIA",
                ". For example, as shown in Tab. ",
                "III",
                ", although we only use the gradient of the ",
                "#",
                "​",
                "1",
                "#",
                "1",
                "\\#1",
                " NIN block in GoogleNet, which constitutes merely ",
                "2.27",
                "%",
                "percent",
                "2.27",
                "2.27\\%",
                " of the total parameters, we achieve a result nearly equivalent to that obtained with the full gradient (a comparison of ",
                "0.0654",
                "0.0654",
                "0.0654",
                " to ",
                "0.0614",
                "0.0614",
                "0.0614",
                "). This observation indicates that the gradient from a single NIN block poses a privacy risk comparable to the entire gradients despite its relatively minor proportion, suggesting the presence of a NIN block is essential for GIAs."
            ]
        ]
    },
    "S5.T4": {
        "caption": "TABLE IV: The Performance of GIA with Various Modifications to ConvNet. (+): Modifications improve the quality of reconstruction, (–): Modifications reduce the quality of reconstruction. (R)emove, (I)ncrease, (D)ecrease.",
        "table": "<table id=\"S5.T4.5\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T4.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-3.15pt 0.4pt;\"><span id=\"S5.T4.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Modifications</span></td>\n<td id=\"S5.T4.1.1.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-3.15pt 0.4pt;\"><span id=\"S5.T4.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">None</span></td>\n<td id=\"S5.T4.1.1.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-3.15pt 0.4pt;\">\n<span id=\"S5.T4.1.1.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R</span><span id=\"S5.T4.1.1.4.2\" class=\"ltx_text\" style=\"font-size:90%;\"> ReLU (+)</span>\n</td>\n<td id=\"S5.T4.1.1.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-3.15pt 0.4pt;\">\n<span id=\"S5.T4.1.1.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R</span><span id=\"S5.T4.1.1.5.2\" class=\"ltx_text\" style=\"font-size:90%;\"> DropOut (+)</span>\n</td>\n<td id=\"S5.T4.1.1.6\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-3.15pt 0.4pt;\">\n<span id=\"S5.T4.1.1.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R</span><span id=\"S5.T4.1.1.6.2\" class=\"ltx_text\" style=\"font-size:90%;\"> MaxPool2d (+)</span>\n</td>\n<td id=\"S5.T4.1.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding:-3.15pt 0.4pt;\">\n<span id=\"S5.T4.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">I</span><span id=\"S5.T4.1.1.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> kernel to </span><math id=\"S5.T4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"4\\times 4\" display=\"inline\"><semantics id=\"S5.T4.1.1.1.m1.1a\"><mrow id=\"S5.T4.1.1.1.m1.1.1\" xref=\"S5.T4.1.1.1.m1.1.1.cmml\"><mn mathsize=\"90%\" id=\"S5.T4.1.1.1.m1.1.1.2\" xref=\"S5.T4.1.1.1.m1.1.1.2.cmml\">4</mn><mo lspace=\"0.222em\" mathsize=\"90%\" rspace=\"0.222em\" id=\"S5.T4.1.1.1.m1.1.1.1\" xref=\"S5.T4.1.1.1.m1.1.1.1.cmml\">×</mo><mn mathsize=\"90%\" id=\"S5.T4.1.1.1.m1.1.1.3\" xref=\"S5.T4.1.1.1.m1.1.1.3.cmml\">4</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.1.1.1.m1.1b\"><apply id=\"S5.T4.1.1.1.m1.1.1.cmml\" xref=\"S5.T4.1.1.1.m1.1.1\"><times id=\"S5.T4.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T4.1.1.1.m1.1.1.1\"></times><cn type=\"integer\" id=\"S5.T4.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T4.1.1.1.m1.1.1.2\">4</cn><cn type=\"integer\" id=\"S5.T4.1.1.1.m1.1.1.3.cmml\" xref=\"S5.T4.1.1.1.m1.1.1.3\">4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.1.1.1.m1.1c\">4\\times 4</annotation></semantics></math><span id=\"S5.T4.1.1.1.3\" class=\"ltx_text\" style=\"font-size:90%;\"> (+)</span>\n</td>\n</tr>\n<tr id=\"S5.T4.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T4.2.2.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\"><span id=\"S5.T4.2.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LPIPS<math id=\"S5.T4.2.2.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S5.T4.2.2.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S5.T4.2.2.1.1.m1.1.1\" xref=\"S5.T4.2.2.1.1.m1.1.1.cmml\">↓</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.2.2.1.1.m1.1b\"><ci id=\"S5.T4.2.2.1.1.m1.1.1.cmml\" xref=\"S5.T4.2.2.1.1.m1.1.1\">↓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.2.2.1.1.m1.1c\">\\downarrow</annotation></semantics></math></span></td>\n<td id=\"S5.T4.2.2.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\"><span id=\"S5.T4.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0080±0.0005</span></td>\n<td id=\"S5.T4.2.2.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\"><span id=\"S5.T4.2.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0008±0.0003</span></td>\n<td id=\"S5.T4.2.2.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\"><span id=\"S5.T4.2.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0048±0.0007</span></td>\n<td id=\"S5.T4.2.2.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\"><span id=\"S5.T4.2.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0003±0.0001</span></td>\n<td id=\"S5.T4.2.2.6\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\"><span id=\"S5.T4.2.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0022±0.0007</span></td>\n</tr>\n<tr id=\"S5.T4.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T4.4.4.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\"><span id=\"S5.T4.4.4.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Modifications</span></td>\n<td id=\"S5.T4.4.4.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\">\n<span id=\"S5.T4.4.4.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R</span><span id=\"S5.T4.4.4.4.2\" class=\"ltx_text\" style=\"font-size:90%;\"> bias (–)</span>\n</td>\n<td id=\"S5.T4.3.3.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\">\n<span id=\"S5.T4.3.3.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">D</span><span id=\"S5.T4.3.3.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> kernel to </span><math id=\"S5.T4.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"2\\times 2\" display=\"inline\"><semantics id=\"S5.T4.3.3.1.m1.1a\"><mrow id=\"S5.T4.3.3.1.m1.1.1\" xref=\"S5.T4.3.3.1.m1.1.1.cmml\"><mn mathsize=\"90%\" id=\"S5.T4.3.3.1.m1.1.1.2\" xref=\"S5.T4.3.3.1.m1.1.1.2.cmml\">2</mn><mo lspace=\"0.222em\" mathsize=\"90%\" rspace=\"0.222em\" id=\"S5.T4.3.3.1.m1.1.1.1\" xref=\"S5.T4.3.3.1.m1.1.1.1.cmml\">×</mo><mn mathsize=\"90%\" id=\"S5.T4.3.3.1.m1.1.1.3\" xref=\"S5.T4.3.3.1.m1.1.1.3.cmml\">2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.3.3.1.m1.1b\"><apply id=\"S5.T4.3.3.1.m1.1.1.cmml\" xref=\"S5.T4.3.3.1.m1.1.1\"><times id=\"S5.T4.3.3.1.m1.1.1.1.cmml\" xref=\"S5.T4.3.3.1.m1.1.1.1\"></times><cn type=\"integer\" id=\"S5.T4.3.3.1.m1.1.1.2.cmml\" xref=\"S5.T4.3.3.1.m1.1.1.2\">2</cn><cn type=\"integer\" id=\"S5.T4.3.3.1.m1.1.1.3.cmml\" xref=\"S5.T4.3.3.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.3.3.1.m1.1c\">2\\times 2</annotation></semantics></math><span id=\"S5.T4.3.3.1.3\" class=\"ltx_text\" style=\"font-size:90%;\"> (–)</span>\n</td>\n<td id=\"S5.T4.4.4.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\">\n<span id=\"S5.T4.4.4.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">D</span><span id=\"S5.T4.4.4.2.2\" class=\"ltx_text\" style=\"font-size:90%;\"> kernel to </span><math id=\"S5.T4.4.4.2.m1.1\" class=\"ltx_Math\" alttext=\"1\\times 1\" display=\"inline\"><semantics id=\"S5.T4.4.4.2.m1.1a\"><mrow id=\"S5.T4.4.4.2.m1.1.1\" xref=\"S5.T4.4.4.2.m1.1.1.cmml\"><mn mathsize=\"90%\" id=\"S5.T4.4.4.2.m1.1.1.2\" xref=\"S5.T4.4.4.2.m1.1.1.2.cmml\">1</mn><mo lspace=\"0.222em\" mathsize=\"90%\" rspace=\"0.222em\" id=\"S5.T4.4.4.2.m1.1.1.1\" xref=\"S5.T4.4.4.2.m1.1.1.1.cmml\">×</mo><mn mathsize=\"90%\" id=\"S5.T4.4.4.2.m1.1.1.3\" xref=\"S5.T4.4.4.2.m1.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.4.4.2.m1.1b\"><apply id=\"S5.T4.4.4.2.m1.1.1.cmml\" xref=\"S5.T4.4.4.2.m1.1.1\"><times id=\"S5.T4.4.4.2.m1.1.1.1.cmml\" xref=\"S5.T4.4.4.2.m1.1.1.1\"></times><cn type=\"integer\" id=\"S5.T4.4.4.2.m1.1.1.2.cmml\" xref=\"S5.T4.4.4.2.m1.1.1.2\">1</cn><cn type=\"integer\" id=\"S5.T4.4.4.2.m1.1.1.3.cmml\" xref=\"S5.T4.4.4.2.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.4.4.2.m1.1c\">1\\times 1</annotation></semantics></math><span id=\"S5.T4.4.4.2.3\" class=\"ltx_text\" style=\"font-size:90%;\"> (–)</span>\n</td>\n<td id=\"S5.T4.4.4.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\">\n<span id=\"S5.T4.4.4.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">I</span><span id=\"S5.T4.4.4.5.2\" class=\"ltx_text\" style=\"font-size:90%;\"> padding to 2 (–)</span>\n</td>\n<td id=\"S5.T4.4.4.6\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\">\n<span id=\"S5.T4.4.4.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">I</span><span id=\"S5.T4.4.4.6.2\" class=\"ltx_text\" style=\"font-size:90%;\"> padding to 3 (–)</span>\n</td>\n</tr>\n<tr id=\"S5.T4.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T4.5.5.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\"><span id=\"S5.T4.5.5.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LPIPS<math id=\"S5.T4.5.5.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S5.T4.5.5.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S5.T4.5.5.1.1.m1.1.1\" xref=\"S5.T4.5.5.1.1.m1.1.1.cmml\">↓</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.5.5.1.1.m1.1b\"><ci id=\"S5.T4.5.5.1.1.m1.1.1.cmml\" xref=\"S5.T4.5.5.1.1.m1.1.1\">↓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.5.5.1.1.m1.1c\">\\downarrow</annotation></semantics></math></span></td>\n<td id=\"S5.T4.5.5.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\"><span id=\"S5.T4.5.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0280±0.0004</span></td>\n<td id=\"S5.T4.5.5.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\"><span id=\"S5.T4.5.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0251±0.0006</span></td>\n<td id=\"S5.T4.5.5.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\"><span id=\"S5.T4.5.5.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.2139±0.0008</span></td>\n<td id=\"S5.T4.5.5.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\"><span id=\"S5.T4.5.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0091±0.0002</span></td>\n<td id=\"S5.T4.5.5.6\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding:-3.15pt 0.4pt;\"><span id=\"S5.T4.5.5.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0203±0.0005</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Micro designs are subtle techniques ubiquitous in nearly all modern models. To investigate their impact on the GIA, we evaluate six prevalent micro designs: ",
                "bias",
                ", ",
                "activation function (ReLU)",
                ", ",
                "dropout",
                ", ",
                "max pooling",
                ", ",
                "convolutional kernels (size)",
                ", and ",
                "padding",
                ". In particular, we make a series of modifications to a configurable model, ConvNet",
                "[",
                "2",
                "]",
                ", which only includes components related to micro designs. In this way, we can control variables and check the resistance of the specific micro design to the GIA. The standard ConvNet incorporates bias, employs ReLU functions, and includes two max pooling layers and one dropout layer. Additionally, all convolutional layers are equipped with ",
                "3",
                "×",
                "3",
                "3",
                "3",
                "3\\times 3",
                " kernels and padding (",
                "1",
                "1",
                "1",
                "). Details are provided in Appendix.",
                "Our findings indicate that ",
                "micro designs significantly impact the model’s resistance to the GIA",
                ". As shown in Tab. ",
                "IV",
                ", removing ",
                "ReLU",
                ", dropout, ",
                "max pooling",
                " layers, or increasing the ",
                "kernel size",
                " substantially exacerbates the model’s vulnerability to GIAs. In contrast, removing ",
                "bias",
                ", decreasing the ",
                "kernel size",
                " or expanding ",
                "padding",
                " enhances the model’s resilience to GIAs.",
                "Essentially, micro design affects ",
                "the amount of information available in the feature map",
                ". Specifically, ",
                "reducing the information content of the feature map related to the input would render GIAs less effective",
                ". (1) Feature map sparsification. The ",
                "activation function",
                " induces a nonlinear transformation of elements in the feature map, ",
                "dropout",
                " zeroes out certain elements, and ",
                "padding",
                " effectively “dilutes” the original feature map. (2) Feature map aggregation. The ",
                "max pooling",
                " layer selects representative elements, and a smaller ",
                "kernel size",
                " focuses on more localized features. These designs reduce the correlation between the input and the feature maps, and thus it is difficult to invert the accurate input data through the gradient.\nHowever, ",
                "enhancing the information content of the feature map benefits GIAs",
                ". The increase of ",
                "kernel size",
                " would promote the model in extracting features on a broader scale, thereby augmenting the information within the feature map, while ",
                "bias",
                " provides extra parameters, which indirectly benefits the GIAs.\n\n",
                "\n",
                "(Insight ",
                "5.2.2",
                ")",
                " Micro designs influence the amount of information shared between the model’s feature maps and the input, consequently affecting the gradient and substantially impacting the performance of GIAs.",
                "\n"
            ]
        ]
    }
}