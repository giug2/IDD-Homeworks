{
    "S3.T1.2.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" colspan=\"3\" id=\"S3.T1.2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.2.1.1.1.1.1\">(a) Comparison with Previous Works</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T1.2.1.2.2.1\">Previous works</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S3.T1.2.1.2.2.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.2.2.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.2.1.1.1\">1. MLLM quantization, Pruning, and KG distillation.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.2.1.2.1\">2. ViT pruning without LLM guidance.</td>\n</tr>\n</table>\n</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T1.2.1.3.3.1\">This work</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S3.T1.2.1.3.3.2\">ViT pruning under LLM guidance.</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" colspan=\"3\" id=\"S3.T1.2.1.4.4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.2.1.4.4.1.1\">(b) Latency Breakdown (single image and single text)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.5.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T1.2.1.5.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.2.1.5.1.1.1\">MLLM (LLaVA 7B)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S3.T1.2.1.5.1.2\">Segment Anything (SAM)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.6.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.1.6.2.1\">ViT-H</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.1.6.2.2\">Mask Decoder</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.7.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T1.2.1.7.3.1\">71.0 ms</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.1.7.3.2\">97.7 ms</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.1.7.3.3\">8.9 ms</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.8.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T1.2.1.8.4.1\">40%</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T1.2.1.8.4.2\">55%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T1.2.1.8.4.3\">5%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 1 :  (a) Comparison with previous works. (b) MLLM guide segmentation model latency breakdown. We conudct the benchmark using an RTX A6000 GPU with the RIO dataset  [ 38 ] .",
        "footnotes": [
            "[38] \nMengxue Qu, Yu Wu, Wu Liu, Xiaodan Liang, Jingkuan Song, Yao Zhao, and Yunchao Wei.\n \n Rio: A benchmark for reasoning intention-oriented objects in open environments.\n \n Advances in Neural Information Processing Systems , 36, 2024.\n \n"
        ],
        "references": [
            "Nevertheless, a key challenge remains: the computational expense of two-stage framework is extremely high. Previous research [41, 51, 59] has primarily focused on MLLM’s compression and pruning, largely overlooking the segmentation model, as depicted in Tab. 1.(a). The segmentation model, as shown in Tab. 1(b), requires even more computational resources than MLLM. Following this, we will explore ways to enhance the efficiency of the segmentation model under the guidance of MLLM’s instruction token ⟨⟨\\langle⟨SEG⟩⟩\\rangle⟩.",
            "As illustrated in Tab. 1(b), the ViT’s computation latency constitutes more than 90% of the total latency of the entire segmentation model. Thus, enhancing the ViT’s efficiency is essential for optimizing the segmentation model’s performance. This work introduces a vision-language guided token pruning method, which accelerates ViT when handling TOS. We detail the mechanism in the following sections.",
            "Refer to Fig. 2, a segmentation model Fs⁢e⁢gsubscriptF𝑠𝑒𝑔\\textbf{F}_{seg}F start_POSTSUBSCRIPT italic_s italic_e italic_g end_POSTSUBSCRIPT typically consists of two components: a vision backbone Fe⁢n⁢csubscriptF𝑒𝑛𝑐\\textbf{F}_{enc}F start_POSTSUBSCRIPT italic_e italic_n italic_c end_POSTSUBSCRIPT and a mask decoder Fd⁢e⁢csubscriptF𝑑𝑒𝑐\\textbf{F}_{dec}F start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT. We illustrate this with SAM. The vision backbone in SAM is a ViT [9, 12]. The ViT extracts features from the input image and passes the feature map to the mask decoder. The mask decoder then predicts the target mask based on the feature map and prompt embeddings. Within our system, the prompt embedding is represented by the ⟨⟨\\langle⟨SEG⟩⟩\\rangle⟩ token embedding from MLLM, which serves as an ambiguous indicator towards the intended affordance to solve the task.\nAs depicted in Tab. 1(b), given the large latency of vision backbone, the segmentation model will benefit significantly from our proposed VLTP."
        ]
    },
    "S3.T1.2.1.2.2.2.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.2.2.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.2.1.1.1\">1. MLLM quantization, Pruning, and KG distillation.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.2.1.2.1\">2. ViT pruning without LLM guidance.</td>\n</tr>\n</table>\n\n",
        "caption": "Table 1 :  (a) Comparison with previous works. (b) MLLM guide segmentation model latency breakdown. We conudct the benchmark using an RTX A6000 GPU with the RIO dataset  [ 38 ] .",
        "footnotes": [
            "[38] \nMengxue Qu, Yu Wu, Wu Liu, Xiaodan Liang, Jingkuan Song, Yao Zhao, and Yunchao Wei.\n \n Rio: A benchmark for reasoning intention-oriented objects in open environments.\n \n Advances in Neural Information Processing Systems , 36, 2024.\n \n"
        ],
        "references": [
            "Nevertheless, a key challenge remains: the computational expense of two-stage framework is extremely high. Previous research [41, 51, 59] has primarily focused on MLLM’s compression and pruning, largely overlooking the segmentation model, as depicted in Tab. 1.(a). The segmentation model, as shown in Tab. 1(b), requires even more computational resources than MLLM. Following this, we will explore ways to enhance the efficiency of the segmentation model under the guidance of MLLM’s instruction token ⟨⟨\\langle⟨SEG⟩⟩\\rangle⟩.",
            "As illustrated in Tab. 1(b), the ViT’s computation latency constitutes more than 90% of the total latency of the entire segmentation model. Thus, enhancing the ViT’s efficiency is essential for optimizing the segmentation model’s performance. This work introduces a vision-language guided token pruning method, which accelerates ViT when handling TOS. We detail the mechanism in the following sections.",
            "Refer to Fig. 2, a segmentation model Fs⁢e⁢gsubscriptF𝑠𝑒𝑔\\textbf{F}_{seg}F start_POSTSUBSCRIPT italic_s italic_e italic_g end_POSTSUBSCRIPT typically consists of two components: a vision backbone Fe⁢n⁢csubscriptF𝑒𝑛𝑐\\textbf{F}_{enc}F start_POSTSUBSCRIPT italic_e italic_n italic_c end_POSTSUBSCRIPT and a mask decoder Fd⁢e⁢csubscriptF𝑑𝑒𝑐\\textbf{F}_{dec}F start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT. We illustrate this with SAM. The vision backbone in SAM is a ViT [9, 12]. The ViT extracts features from the input image and passes the feature map to the mask decoder. The mask decoder then predicts the target mask based on the feature map and prompt embeddings. Within our system, the prompt embedding is represented by the ⟨⟨\\langle⟨SEG⟩⟩\\rangle⟩ token embedding from MLLM, which serves as an ambiguous indicator towards the intended affordance to solve the task.\nAs depicted in Tab. 1(b), given the large latency of vision backbone, the segmentation model will benefit significantly from our proposed VLTP."
        ]
    },
    "S5.T2.4.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T2.4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S5.T2.4.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T2.4.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.4.1.1.1.2\">Common</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.4.1.1.1.3\">Uncommon</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T2.4.1.2.2.1\">mIoU (%)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T2.4.1.2.2.2\">mIoU (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S5.T2.4.1.3.1.1\">MDETR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08464v1#bib.bib19\" title=\"\">19</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.4.1.3.1.2\">44.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.4.1.3.1.3\">22.03</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T2.4.1.4.2.1\">TOIST&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08464v1#bib.bib25\" title=\"\">25</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.4.2.2\">45.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.4.2.3\">19.41</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T2.4.1.5.3.1\">Polyformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08464v1#bib.bib29\" title=\"\">29</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.5.3.2\">48.75</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.5.3.3\">26.77</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T2.4.1.6.4.1\">GROUNDHOG&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08464v1#bib.bib56\" title=\"\">56</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.6.4.2\">57.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.6.4.3\">33.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T2.4.1.7.5.1\">SAM ViT-H</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.7.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.4.1.7.5.2.1\">60.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.7.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.4.1.7.5.3.1\">37.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T2.4.1.8.6.1\">SAM ViT-L</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.8.6.2\">55.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.8.6.3\">32.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T2.4.1.9.7.1\">SAM ViT-B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.4.1.9.7.2\">50.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.4.1.9.7.3\">27.9</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 2 :  Comparison with previous works on RIO  [ 38 ]  dataset.",
        "footnotes": [
            "[38] \nMengxue Qu, Yu Wu, Wu Liu, Xiaodan Liang, Jingkuan Song, Yao Zhao, and Yunchao Wei.\n \n Rio: A benchmark for reasoning intention-oriented objects in open environments.\n \n Advances in Neural Information Processing Systems , 36, 2024.\n \n",
            "[19] \nAishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion.\n \n Mdetr-modulated detection for end-to-end multi-modal understanding.\n \n In  Proceedings of the IEEE/CVF international conference on computer vision , pages 1780–1790, 2021.\n \n",
            "[25] \nPengfei Li, Beiwen Tian, Yongliang Shi, Xiaoxue Chen, Hao Zhao, Guyue Zhou, and Ya-Qin Zhang.\n \n Toist: Task oriented instance segmentation transformer with noun-pronoun distillation.\n \n Advances in Neural Information Processing Systems , 35:17597–17611, 2022.\n \n",
            "[29] \nJiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Kumar Satzoda, Vijay Mahadevan, and R Manmatha.\n \n Polyformer: Referring image segmentation as sequential polygon generation.\n \n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 18653–18663, 2023.\n \n",
            "[56] \nYichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai.\n \n Groundhog: Grounding large language models to holistic segmentation.\n \n In  Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 14227–14238, 2024.\n \n"
        ],
        "references": [
            "We selected LLaVA 7B [28] to serve as MLLM. LLaVA processes both textual tasks and images as inputs to produce reasoning guidance ⟨⟨\\langle⟨SEG⟩⟩\\rangle⟩. For the segmentation model, we opted for SAM, a well-known foundation model that enables prompt-guided segmentation. Following previous research [24, 49, 37], we perform end-to-end visual instruction fine-tuning on the entire system. For the LLaVA module, we utilize LoRA [14] for efficient fine-tuning, while for the SAM module, we freeze the ViT backbone and only train the mask decoder. Table 2 shows the reasoning segmentation accuracy of the entire system on the RIO dataset, along with a comparison to earlier studies. Compared to the SOTA method [56], guided by MLLM, the SAM with the ViT-H backbone model, exceeds it by 2.5% on RIO common parts and 3.3% on RIO uncommon parts. This result highlights the importance of integrating MLLM with the segmentation model in visual reasoning tasks."
        ]
    },
    "S5.T3.3.3": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T3.3.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T3.3.3.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.3.3.4.1.1\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.3.3.4.1.2\">GFLOPs</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.3.3.4.1.3\">mIoU (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T3.3.3.5.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T3.3.3.5.1.1\">Baseline</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T3.3.3.5.1.2\">2976</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T3.3.3.5.1.3\">60.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.3.6.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.3.3.6.2.1\">+VLTP@Direct (&#9824;0)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.3.6.2.2\">2227</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.3.6.2.3\">39.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.1\">+VLTP@Finetune (<math alttext=\"F_{prune}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.1.1.1.1.m1.1\"><semantics id=\"S5.T3.1.1.1.1.m1.1a\"><msub id=\"S5.T3.1.1.1.1.m1.1.1\" xref=\"S5.T3.1.1.1.1.m1.1.1.cmml\"><mi id=\"S5.T3.1.1.1.1.m1.1.1.2\" xref=\"S5.T3.1.1.1.1.m1.1.1.2.cmml\">F</mi><mrow id=\"S5.T3.1.1.1.1.m1.1.1.3\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.cmml\"><mi id=\"S5.T3.1.1.1.1.m1.1.1.3.2\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.2.cmml\">p</mi><mo id=\"S5.T3.1.1.1.1.m1.1.1.3.1\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.1.1.1.1.m1.1.1.3.3\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.3.cmml\">r</mi><mo id=\"S5.T3.1.1.1.1.m1.1.1.3.1a\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.1.1.1.1.m1.1.1.3.4\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.4.cmml\">u</mi><mo id=\"S5.T3.1.1.1.1.m1.1.1.3.1b\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.1.1.1.1.m1.1.1.3.5\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.5.cmml\">n</mi><mo id=\"S5.T3.1.1.1.1.m1.1.1.3.1c\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.1.1.1.1.m1.1.1.3.6\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.6.cmml\">e</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.1.1.1.1.m1.1b\"><apply id=\"S5.T3.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T3.1.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S5.T3.1.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.2\">&#119865;</ci><apply id=\"S5.T3.1.1.1.1.m1.1.1.3.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.3\"><times id=\"S5.T3.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.1\"/><ci id=\"S5.T3.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.2\">&#119901;</ci><ci id=\"S5.T3.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.3\">&#119903;</ci><ci id=\"S5.T3.1.1.1.1.m1.1.1.3.4.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.4\">&#119906;</ci><ci id=\"S5.T3.1.1.1.1.m1.1.1.3.5.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.5\">&#119899;</ci><ci id=\"S5.T3.1.1.1.1.m1.1.1.3.6.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.6\">&#119890;</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.1.1.1.1.m1.1c\">F_{prune}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T3.1.1.1.1.m1.1d\">italic_F start_POSTSUBSCRIPT italic_p italic_r italic_u italic_n italic_e end_POSTSUBSCRIPT</annotation></semantics></math>) (&#9824;5)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.1.2\">2227</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.1.3\">50.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T3.3.3.3.2\">+VLTP@Finetune (<math alttext=\"F_{dec}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.2.2.2.1.m1.1\"><semantics id=\"S5.T3.2.2.2.1.m1.1a\"><msub id=\"S5.T3.2.2.2.1.m1.1.1\" xref=\"S5.T3.2.2.2.1.m1.1.1.cmml\"><mi id=\"S5.T3.2.2.2.1.m1.1.1.2\" xref=\"S5.T3.2.2.2.1.m1.1.1.2.cmml\">F</mi><mrow id=\"S5.T3.2.2.2.1.m1.1.1.3\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.cmml\"><mi id=\"S5.T3.2.2.2.1.m1.1.1.3.2\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.2.cmml\">d</mi><mo id=\"S5.T3.2.2.2.1.m1.1.1.3.1\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.2.2.2.1.m1.1.1.3.3\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.3.cmml\">e</mi><mo id=\"S5.T3.2.2.2.1.m1.1.1.3.1a\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.2.2.2.1.m1.1.1.3.4\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.4.cmml\">c</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.2.2.2.1.m1.1b\"><apply id=\"S5.T3.2.2.2.1.m1.1.1.cmml\" xref=\"S5.T3.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T3.2.2.2.1.m1.1.1.1.cmml\" xref=\"S5.T3.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S5.T3.2.2.2.1.m1.1.1.2.cmml\" xref=\"S5.T3.2.2.2.1.m1.1.1.2\">&#119865;</ci><apply id=\"S5.T3.2.2.2.1.m1.1.1.3.cmml\" xref=\"S5.T3.2.2.2.1.m1.1.1.3\"><times id=\"S5.T3.2.2.2.1.m1.1.1.3.1.cmml\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.1\"/><ci id=\"S5.T3.2.2.2.1.m1.1.1.3.2.cmml\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.2\">&#119889;</ci><ci id=\"S5.T3.2.2.2.1.m1.1.1.3.3.cmml\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.3\">&#119890;</ci><ci id=\"S5.T3.2.2.2.1.m1.1.1.3.4.cmml\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.4\">&#119888;</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.2.2.2.1.m1.1c\">F_{dec}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T3.2.2.2.1.m1.1d\">italic_F start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT</annotation></semantics></math> + <math alttext=\"F_{prune}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.3.3.3.2.m2.1\"><semantics id=\"S5.T3.3.3.3.2.m2.1a\"><msub id=\"S5.T3.3.3.3.2.m2.1.1\" xref=\"S5.T3.3.3.3.2.m2.1.1.cmml\"><mi id=\"S5.T3.3.3.3.2.m2.1.1.2\" xref=\"S5.T3.3.3.3.2.m2.1.1.2.cmml\">F</mi><mrow id=\"S5.T3.3.3.3.2.m2.1.1.3\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.cmml\"><mi id=\"S5.T3.3.3.3.2.m2.1.1.3.2\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.2.cmml\">p</mi><mo id=\"S5.T3.3.3.3.2.m2.1.1.3.1\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.3.3.3.2.m2.1.1.3.3\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.3.cmml\">r</mi><mo id=\"S5.T3.3.3.3.2.m2.1.1.3.1a\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.3.3.3.2.m2.1.1.3.4\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.4.cmml\">u</mi><mo id=\"S5.T3.3.3.3.2.m2.1.1.3.1b\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.3.3.3.2.m2.1.1.3.5\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.5.cmml\">n</mi><mo id=\"S5.T3.3.3.3.2.m2.1.1.3.1c\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.3.3.3.2.m2.1.1.3.6\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.6.cmml\">e</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.3.3.3.2.m2.1b\"><apply id=\"S5.T3.3.3.3.2.m2.1.1.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T3.3.3.3.2.m2.1.1.1.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1\">subscript</csymbol><ci id=\"S5.T3.3.3.3.2.m2.1.1.2.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1.2\">&#119865;</ci><apply id=\"S5.T3.3.3.3.2.m2.1.1.3.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1.3\"><times id=\"S5.T3.3.3.3.2.m2.1.1.3.1.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.1\"/><ci id=\"S5.T3.3.3.3.2.m2.1.1.3.2.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.2\">&#119901;</ci><ci id=\"S5.T3.3.3.3.2.m2.1.1.3.3.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.3\">&#119903;</ci><ci id=\"S5.T3.3.3.3.2.m2.1.1.3.4.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.4\">&#119906;</ci><ci id=\"S5.T3.3.3.3.2.m2.1.1.3.5.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.5\">&#119899;</ci><ci id=\"S5.T3.3.3.3.2.m2.1.1.3.6.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.6\">&#119890;</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.3.3.3.2.m2.1c\">F_{prune}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T3.3.3.3.2.m2.1d\">italic_F start_POSTSUBSCRIPT italic_p italic_r italic_u italic_n italic_e end_POSTSUBSCRIPT</annotation></semantics></math>) (&#9824;9)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.3.3.3.3\">2227</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.3.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.3.3.4.1\">60.1</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 3 :  Comparison of training scheme. All results are reported based on RIO common dataset. ♠ represents the extra finetune epochs.",
        "footnotes": [],
        "references": [
            "Table 3 demonstrates the impact of different training strategies on final pruning outcomes. We assume a pruning rate of 50%, inserting the prune decoder at layers 16 and 24. This implies that starting from layer 16, only 50% of image tokens will be involved in the SAM ViT computation. The results indicate that when both Fp⁢r⁢u⁢n⁢esubscript𝐹𝑝𝑟𝑢𝑛𝑒F_{prune}italic_F start_POSTSUBSCRIPT italic_p italic_r italic_u italic_n italic_e end_POSTSUBSCRIPT and Fd⁢e⁢csubscript𝐹𝑑𝑒𝑐F_{dec}italic_F start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT are fine-tuned simultaneously, VLTP reduces the SAM ViT’s computation from 2976 GFLOPs to 2227 GFLOPs, yielding approximately a 25% reduction in GFLOPs with only a 0.3% drop in mIoU. Consequently, in the subsequent sections, both the prune decoder and mask decoder will be fine-tuned to achieve optimal pruning results.",
            "Table 4 illustrates the effects of pruning as the pruning rate rmsubscript𝑟𝑚r_{m}italic_r start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT increases. In this study, we set the pruning positions of SAM ViT at layers 16 and 24. Instead of finetuning the pruning and mask decoders from scratch, we base our experiments on a pre-trained model with a 50% pruning rate, as depicted in Tab. 3. Initially, we increase the pruning rate without any finetuning, and the results in Tab. 4 indicate an accuracy drop. Consequently, we subsequently finetune both the prune decoder and mask decoder together after adjusting the pruning rate. The experiment demonstrates that VLTP achieves approximately 35% GFLOPs with just a 0.8% mIoU reduction at a 70% pruning rate, and around 40% GFLOPs with only a 1% mIoU reduction at an 80% pruning rate, compared to the original SAM ViT-H. Additionally, we display the image patch-dropping visual results in  Fig. 4. Under MLLM guidance, VLTP retains the most essential image patches while freezing irrelevant ones, in comparison to the ground truth."
        ]
    },
    "S5.T4.4.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T4.4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T4.4.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T4.4.1.1.1.1\">Pruning Rate (%)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.4.1.1.1.2\">50</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.4.1.1.1.3\">70</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.4.1.1.1.4\">70</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.4.1.1.1.5\">80</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.4.1.1.1.6\">80</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.4.1.1.1.7\">90</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.4.1.1.1.8\">90</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.4.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S5.T4.4.1.2.2.1\">Training scheme</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.4.1.2.2.2\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.4.1.2.2.3\">zero shot</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.4.1.2.2.4\">finetune</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.4.1.2.2.5\">zero shot</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.4.1.2.2.6\">finetune</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.4.1.2.2.7\">zero shot</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.4.1.2.2.8\">finetune</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.4.1.3.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T4.4.1.3.1.1\">GFLOPs</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.1.3.1.2\">2227</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.1.3.1.3\">1930</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.1.3.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.4.1.3.1.4.1\">1930</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.1.3.1.5\">1782</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.1.3.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.4.1.3.1.6.1\">1782</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.1.3.1.7\">1636</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.1.3.1.8\">1636</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.4.1.4.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T4.4.1.4.2.1\">Common (%)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.1.4.2.2\">60.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.1.4.2.3\">57.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.1.4.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.4.1.4.2.4.1\">59.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.1.4.2.5\">52.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.1.4.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.4.1.4.2.6.1\">59.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.1.4.2.7\">39.76</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.1.4.2.8\">51.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.4.1.5.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S5.T4.4.1.5.3.1\">Uncommon (%)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.4.1.5.3.2\">37.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.4.1.5.3.3\">35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.4.1.5.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.4.1.5.3.4.1\">37.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.4.1.5.3.5\">32.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.4.1.5.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.4.1.5.3.6.1\">37.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.4.1.5.3.7\">19.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.4.1.5.3.8\">28.4</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 4 :  Pruning ratio exploration for SAM ViT-H. Here we assume the pruning position is at layer 16 and layer 24 and both layer’s pruning ratio is the same. All results are reported based on RIO common dataset.",
        "footnotes": [],
        "references": [
            "Table 4 illustrates the effects of pruning as the pruning rate rmsubscript𝑟𝑚r_{m}italic_r start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT increases. In this study, we set the pruning positions of SAM ViT at layers 16 and 24. Instead of finetuning the pruning and mask decoders from scratch, we base our experiments on a pre-trained model with a 50% pruning rate, as depicted in Tab. 3. Initially, we increase the pruning rate without any finetuning, and the results in Tab. 4 indicate an accuracy drop. Consequently, we subsequently finetune both the prune decoder and mask decoder together after adjusting the pruning rate. The experiment demonstrates that VLTP achieves approximately 35% GFLOPs with just a 0.8% mIoU reduction at a 70% pruning rate, and around 40% GFLOPs with only a 1% mIoU reduction at an 80% pruning rate, compared to the original SAM ViT-H. Additionally, we display the image patch-dropping visual results in  Fig. 4. Under MLLM guidance, VLTP retains the most essential image patches while freezing irrelevant ones, in comparison to the ground truth."
        ]
    },
    "S5.T5.4.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T5.4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.4.1.1.1.1\">Position</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.4.1.1.1.2\">Pruning Rate</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.4.1.1.1.3\">GFLOPS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.4.1.1.1.4\">mIoU (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.4.1.2.1.1\">Baseline</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.4.1.2.1.2\">{0}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.4.1.2.1.3\">2976</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.4.1.2.1.4\">60.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.3.2.1\">{8}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.3.2.2\">{20}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.3.2.3\">2529</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.3.2.4\">59.8 (-0.6)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.4.3.1\">{8}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.4.3.2\">{40}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.4.3.3\">2083</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.4.3.4\">53.1 (-7.3)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.5.4.1\">{8, 16}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.5.4.2\">{20, 40}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.5.4.3\">2232</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.5.4.4\">58.8 (-1.6)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.6.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.6.5.1\">{8, 16}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.6.5.2\">{20, 60}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.6.5.3\">1783</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.6.5.4\">53.7 (-6.7)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.7.6.1\">{16, 24}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.7.6.2\">{50, 50}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.7.6.3\">2227</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.7.6.4\">60.1 (-0.3)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.8.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.8.7.1\">{16, 24}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.8.7.2\">{80, 80}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.8.7.3\">1782</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.8.7.4\">58.9 (-1.2)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.9.8.1\">{8, 16, 24}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.9.8.2\">{20, 40, 40}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.9.8.3\">2232</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.9.8.4\">58.9 (-1.5)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.10.9\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.10.9.1\">{8, 16, 24}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.10.9.2\">{20, 40, 60}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.10.9.3\">1853</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.10.9.4\">53 (-7.4)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.11.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.4.1.11.10.1\">{8, 16, 24}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.4.1.11.10.2\">{50, 50, 50}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.4.1.11.10.3\">1860</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.4.1.11.10.4\">53.3 (-7.1)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 5 :  Investigation of SAM ViT-H pruning locations. The table presents the maximum accuracy and the most efficient outcome (measured in GFLOPs) for each pruning position combination. All outcomes are derived from the RIO common dataset. Each invocation of the prune decoder adds an additional 6 GFLOPs.",
        "footnotes": [],
        "references": [
            "In Tab. 5, we illustrate the influence of different pruning positions on the ultimate pruning accuracy. The findings suggest that initiating pruning from layer 8 leads to a greater accuracy reduction compared to starting from layer 16, assuming the same level of ViT computation reduction. We attribute this to inadequate image feature extraction. Image patches that aren’t directly pertinent to reasoning tasks still contribute to the final mask generation. Consequently, removing these patches at an earlier layer can negatively impact the final mask generation.  Figure 5 visualizes the patch dropping at various layer positions."
        ]
    },
    "S5.T6.4.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T6.4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T6.4.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T6.4.1.1.1.1\">Position</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T6.4.1.1.1.2\">Pruning Rate</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T6.4.1.1.1.3\">GFLOPS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T6.4.1.1.1.4\">mIoU (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T6.4.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.4.1.2.1.1\">{16}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.4.1.2.1.2\">{50}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.4.1.2.1.3\">2221</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.4.1.2.1.4\">59.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.4.1.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.4.1.3.2.1\">{16, 24}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.4.1.3.2.2\">{50, 50}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.4.1.3.2.3\">2227</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.4.1.3.2.4\">60.1 (+0.3)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.4.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.1.4.3.1\">{16}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.1.4.3.2\">{70}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.1.4.3.3\">1923</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.1.4.3.4\">58.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.4.1.5.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.4.1.5.4.1\">{16, 24}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.4.1.5.4.2\">{70, 70}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.4.1.5.4.3\">1930</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.4.1.5.4.4\">59.6 (+1.4)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.4.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.1.6.5.1\">{16}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.1.6.5.2\">{80}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.1.6.5.3\">1775</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.1.6.5.4\">56.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.4.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.4.1.7.6.1\">{16, 24}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.4.1.7.6.2\">{80, 80}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.4.1.7.6.3\">1782</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.4.1.7.6.4\">59.4 (+2.6)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 6 :  Ablation study for effect’s of pruned tokens reevaluation. Here the model is SAM ViT-H and dataset is RIO common. Each invo-\ncation of the prune decoder adds an additional 6 GFLOPs.",
        "footnotes": [],
        "references": [
            "Table 6 illustrates the impact of reevaluating pruned image tokens. We compare the final reasoning segmentation accuracy between pruning only at layer 16 and pruning at both layer 16 and layer 24. Reassessing token relevance at layer 24 enhances the accuracy of the final reasoning. This is due to the prune decoder potentially making incorrect predictions about the relevance of image tokens to the reasoning task at earlier stages. Hence, it is crucial to reassess the importance of these tokens at deeper ViT layers."
        ]
    },
    "S5.T7.4.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T7.4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T7.4.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T7.4.1.1.1.1\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T7.4.1.1.1.2\">VLM support</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T7.4.1.1.1.3\">GFLOPs</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T7.4.1.1.1.4\">mIoU</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T7.4.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.4.1.2.1.1\">Baseline</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.4.1.2.1.2\">No</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.4.1.2.1.3\">2976</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.4.1.2.1.4\">60.4 (-0)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.4.1.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.3.2.1\">Random 50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.3.2.2\">No</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.3.2.3\">2297</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.3.2.4\">38.2 (-22.2)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.4.1.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.4.3.1\">CTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08464v1#bib.bib31\" title=\"\">31</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.4.3.2\">No</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.4.3.3\">2232</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.4.3.4\">35.7 (-24.7)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.4.1.5.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.5.4.1\">DToP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08464v1#bib.bib44\" title=\"\">44</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.5.4.2\">No</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.5.4.3\">1892</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.5.4.4\">36.3 (-24.1)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.4.1.6.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.6.5.1\">SViT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08464v1#bib.bib30\" title=\"\">30</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.6.5.2\">No</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.6.5.3\">1935</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.6.5.4\">33.5 (-26.9)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.4.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T7.4.1.7.6.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.1.7.6.1.1\">VLTP (this work)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T7.4.1.7.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.1.7.6.2.1\">Yes</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T7.4.1.7.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.1.7.6.3.1\">1782</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T7.4.1.7.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.1.7.6.4.1\">59.4 (-1.0)</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 7 :  A comparison of various pruning techniques. Each method is applied on SAM ViT-H. Random 50% indicates randomly dropping 50% of image tokens starting from layer 16.",
        "footnotes": [
            "[31] \nChenyang Lu, Daan de Geus, and Gijs Dubbelman.\n \n Content-aware token sharing for efficient semantic segmentation with vision transformers.\n \n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 23631–23640, 2023.\n \n",
            "[44] \nQuan Tang, Bowen Zhang, Jiajun Liu, Fagui Liu, and Yifan Liu.\n \n Dynamic token pruning in plain vision transformers for semantic segmentation.\n \n In  Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 777–786, 2023.\n \n",
            "[30] \nYifei Liu, Mathias Gehrig, Nico Messikommer, Marco Cannici, and Davide Scaramuzza.\n \n Revisiting token pruning for object detection and instance segmentation.\n \n In  Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 2658–2668, 2024.\n \n"
        ],
        "references": [
            "In Tab. 7, we compare various pruning methods for task-oriented reasoning tasks. Although prior research [31, 44, 30] has shown effectiveness in pruning ViTs for traditional semantic or instance segmentation tasks, these methods are not suitable for TOS as they lack vision-language guidance. For instance, the policy network in CTS [31] is unable to accurately predict whether four image tokens belong to the same semantic class because each token’s class can change depending on the reasoning task. Consequently, the pruning networks proposed in DToP [44] and SViT [30] are also ineffective as they do not incorporate reasoning guidance. VLTP addresses these limitations by integrating both ViT image tokens and MLLM reasoning guidance, thus achieving superior performance."
        ]
    },
    "S5.T8.4.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T8.4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T8.4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T8.4.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T8.4.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T8.4.1.1.1.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T8.4.1.1.1.2.1\">pruning rate</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S5.T8.4.1.1.1.3\">RIO common</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S5.T8.4.1.1.1.4\">RIO uncommon</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S5.T8.4.1.1.1.5\">COCO-Tasks</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.4.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T8.4.1.2.2.1\">mIoU(%)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T8.4.1.2.2.2\">GFLOPs</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T8.4.1.2.2.3\">mIoU(%)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T8.4.1.2.2.4\">GFLOPs</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T8.4.1.2.2.5\">mIoU(%)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T8.4.1.2.2.6\">GFLOPs</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T8.4.1.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T8.4.1.3.1.1\">SAM ViT-H</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.4.1.3.1.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.4.1.3.1.3\">60.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.4.1.3.1.4\">2976</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.4.1.3.1.5\">37.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.4.1.3.1.6\">2976</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.4.1.3.1.7\">44.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.4.1.3.1.8\">2976</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.4.1.4.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.4.1.4.2.1\">VLTP@Finetune</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.4.2.2\">0.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.4.2.3\">60.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.4.2.4\">2227</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.4.2.5\">37.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.4.2.6\">2224</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.4.2.7\">44.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.4.2.8\">2219</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.4.1.5.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.4.1.5.3.1\">VLTP@Finetune</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.5.3.2\">0.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.5.3.3\">59.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.5.3.4\">1782</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.5.3.5\">37.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.5.3.6\">1774</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.5.3.7\">43.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.5.3.8\">1771</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.4.1.6.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T8.4.1.6.4.1\">SAM ViT-L</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.4.1.6.4.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.4.1.6.4.3\">55.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.4.1.6.4.4\">1491</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.4.1.6.4.5\">32.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.4.1.6.4.6\">1491</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.4.1.6.4.7\">40.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.4.1.6.4.8\">1491</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.4.1.7.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.4.1.7.5.1\">VLTP@Finetune</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.7.5.2\">0.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.7.5.3\">55.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.7.5.4\">1118</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.7.5.5\">32.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.7.5.6\">1121</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.7.5.7\">39.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.7.5.8\">1127</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.4.1.8.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T8.4.1.8.6.1\">VLTP@Finetune</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.4.1.8.6.2\">0.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.4.1.8.6.3\">54.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.4.1.8.6.4\">895</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.4.1.8.6.5\">31.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.4.1.8.6.6\">901</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.4.1.8.6.7\">38.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.4.1.8.6.8\">905</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 8 :  Main results on two different TOS benchmarks.",
        "footnotes": [],
        "references": [
            "In Tab. 8, we provide a summary of the results when applying VLTP to SAM ViT-H and SAM ViT-L for the RIO common, RIO uncommon, and COCO-Tasks datasets. VLTP effectively maintains task-relevant image tokens while significantly reducing SAM ViT’s computational costs. On average, VLTP reduces SAM ViT’s computation by 40% with only a 1% loss in mIoU. Figure 6 showcases our framework’s segmentation visualized results on the RIO and COCO-Task datasets."
        ]
    }
}