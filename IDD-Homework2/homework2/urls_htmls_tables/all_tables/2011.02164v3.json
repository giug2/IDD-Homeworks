{
    "S4.T1": {
        "caption": "Table 1:  Experimental results with different L𝐿L. Here we use a range of values from 2 to 8 on validation set. Best performance is achieved with L=6𝐿6L=6. Therefore, in this paper we choose L=6𝐿6L=6 for our work.",
        "table": "<table id=\"S4.T1.8\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.8.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.8.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.8.1.1.1.1\" class=\"ltx_text ltx_font_bold\">L</span></th>\n<th id=\"S4.T1.8.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.8.1.1.2.1\" class=\"ltx_text ltx_font_bold\">All</span></th>\n<th id=\"S4.T1.8.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.8.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Other</span></th>\n<th id=\"S4.T1.8.1.1.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.8.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Y/N</span></th>\n<th id=\"S4.T1.8.1.1.5\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.8.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Num</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.8.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.8.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">L = 2</td>\n<td id=\"S4.T1.8.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">81.88</td>\n<td id=\"S4.T1.8.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">74.47</td>\n<td id=\"S4.T1.8.2.1.4\" class=\"ltx_td ltx_align_left ltx_border_t\">96.11</td>\n<td id=\"S4.T1.8.2.1.5\" class=\"ltx_td ltx_align_left ltx_border_t\">69.00</td>\n</tr>\n<tr id=\"S4.T1.8.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.8.3.2.1\" class=\"ltx_td ltx_align_left\">L = 4</td>\n<td id=\"S4.T1.8.3.2.2\" class=\"ltx_td ltx_align_left\">83.34</td>\n<td id=\"S4.T1.8.3.2.3\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.8.3.2.3.1\" class=\"ltx_text ltx_font_bold\">76.48</span></td>\n<td id=\"S4.T1.8.3.2.4\" class=\"ltx_td ltx_align_left\">96.65</td>\n<td id=\"S4.T1.8.3.2.5\" class=\"ltx_td ltx_align_left\">71.00</td>\n</tr>\n<tr id=\"S4.T1.8.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.8.4.3.1\" class=\"ltx_td ltx_align_left\">L = 6</td>\n<td id=\"S4.T1.8.4.3.2\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.8.4.3.2.1\" class=\"ltx_text ltx_font_bold\">83.45</span></td>\n<td id=\"S4.T1.8.4.3.3\" class=\"ltx_td ltx_align_left\">76.45</td>\n<td id=\"S4.T1.8.4.3.4\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.8.4.3.4.1\" class=\"ltx_text ltx_font_bold\">96.83</span></td>\n<td id=\"S4.T1.8.4.3.5\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.8.4.3.5.1\" class=\"ltx_text ltx_font_bold\">71.44</span></td>\n</tr>\n<tr id=\"S4.T1.8.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T1.8.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_b\">L = 8</td>\n<td id=\"S4.T1.8.5.4.2\" class=\"ltx_td ltx_align_left ltx_border_b\">82.20</td>\n<td id=\"S4.T1.8.5.4.3\" class=\"ltx_td ltx_align_left ltx_border_b\">75.42</td>\n<td id=\"S4.T1.8.5.4.4\" class=\"ltx_td ltx_align_left ltx_border_b\">95.87</td>\n<td id=\"S4.T1.8.5.4.5\" class=\"ltx_td ltx_align_left ltx_border_b\">68.53</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We run a number of experiments to show the effectiveness of our proposed method and results of these experiments are presented in Table 1 and 2.",
            "Number of Cascaded Layer (L𝐿L): MCAoA layers consist of L𝐿L number of stacked SAoA and GAoA units. From Table 1, we can see, initially, with the increasing value of L𝐿L, performance of the model is also increasing – up to L=6𝐿6L=6. After that the performance is saturated. We use L=6𝐿6L=6 in our final model. We use validation set for this experiment with the default hyperparameters of  [34]."
        ]
    },
    "S4.T2": {
        "caption": "Table 2:  Visual Question Answering results using VQA-v2 dataset. Comparison of our proposed approach with state-of-the-art method on validation set. Here we also show each component in our proposed method contribute to increase the performance of VQA system.",
        "table": "<table id=\"S4.T2.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Methods</span></th>\n<th id=\"S4.T2.2.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.2.1.1.2.1\" class=\"ltx_text ltx_font_bold\">All</span></th>\n<th id=\"S4.T2.2.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.2.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Other</span></th>\n<th id=\"S4.T2.2.1.1.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.2.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Y/N</span></th>\n<th id=\"S4.T2.2.1.1.5\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.2.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Num</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.2.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">MCAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">34</a>]</cite>\n</td>\n<td id=\"S4.T2.2.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">81.20</td>\n<td id=\"S4.T2.2.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">73.73</td>\n<td id=\"S4.T2.2.2.1.4\" class=\"ltx_td ltx_align_left ltx_border_t\">95.86</td>\n<td id=\"S4.T2.2.2.1.5\" class=\"ltx_td ltx_align_left ltx_border_t\">67.30</td>\n</tr>\n<tr id=\"S4.T2.2.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.3.2.1\" class=\"ltx_td ltx_align_left\">Ours (MCAoAN)</td>\n<td id=\"S4.T2.2.3.2.2\" class=\"ltx_td ltx_align_left\">82.91</td>\n<td id=\"S4.T2.2.3.2.3\" class=\"ltx_td ltx_align_left\">75.92</td>\n<td id=\"S4.T2.2.3.2.4\" class=\"ltx_td ltx_align_left\">96.47</td>\n<td id=\"S4.T2.2.3.2.5\" class=\"ltx_td ltx_align_left\">70.38</td>\n</tr>\n<tr id=\"S4.T2.2.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.4.3.1\" class=\"ltx_td ltx_align_left\">Ours (MCAoAN + Mutan)</td>\n<td id=\"S4.T2.2.4.3.2\" class=\"ltx_td ltx_align_left\">83.00</td>\n<td id=\"S4.T2.2.4.3.3\" class=\"ltx_td ltx_align_left\">76.13</td>\n<td id=\"S4.T2.2.4.3.4\" class=\"ltx_td ltx_align_left\">96.36</td>\n<td id=\"S4.T2.2.4.3.5\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T2.2.4.3.5.1\" class=\"ltx_text ltx_font_bold\">70.42</span></td>\n</tr>\n<tr id=\"S4.T2.2.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_b\">Ours (MCAoAN + Multi-modal Attention Fusion)</td>\n<td id=\"S4.T2.2.5.4.2\" class=\"ltx_td ltx_align_left ltx_border_b\"><span id=\"S4.T2.2.5.4.2.1\" class=\"ltx_text ltx_font_bold\">83.25</span></td>\n<td id=\"S4.T2.2.5.4.3\" class=\"ltx_td ltx_align_left ltx_border_b\"><span id=\"S4.T2.2.5.4.3.1\" class=\"ltx_text ltx_font_bold\">76.51</span></td>\n<td id=\"S4.T2.2.5.4.4\" class=\"ltx_td ltx_align_left ltx_border_b\"><span id=\"S4.T2.2.5.4.4.1\" class=\"ltx_text ltx_font_bold\">96.58</span></td>\n<td id=\"S4.T2.2.5.4.5\" class=\"ltx_td ltx_align_left ltx_border_b\">70.40</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We run a number of experiments to show the effectiveness of our proposed method and results of these experiments are presented in Table 1 and 2.",
            "Effectiveness of Each Individual Component:  In this paper, our improved architecture has two important components: (1) MCAoAN network which consists of SAoA module and GAoA module and (2) Multi-modal fusion to incorporate image and language features. Here, we describe two different fusion mechanism : Mutan fusion and Multi-modal attention fusion. Table 2 shows experimental results of these individual components and compare with existing MCAN [34] on  validation set. From the table, we see that incorporating SAoA and GAoA module with MCAN improves the performance of VQA system.",
            "Moreover, we argue that a sophisticated way to aggregate language and visual features to support multi-modal reasoning is essential to further boost the performance. Table 2 also shows the comparison of different fusions with the MCAoA only where the former achieves better performance. More specifically, our proposed MCAoAN with both multi-modal fusion modules outperforms the baseline about 2%percent22\\% accuracy on the whole validation set. This shows that the fusion module is important to combine vision and language representations. The proposed both fusion modules are suitable for VQA tasks. Among them multi-modal attention fusion performs the best. Beside that, Table 2 also shows that each individual component within our proposed method is important to increase the performance of VQA system.",
            "We evaluate our model on VQA-v2 dataset and compare with other state-of-the-art methods. We re-run the PyTorch implementation provided by [34]111https://github.com/MILVLG/mcan-vqa and compare the results with our proposed method. Table 3 and 4 shows experimental results using test-dev and test-std respectively using online evaluation 222https://evalai.cloudcv.org/web/challenges/challenge-page/163/overview. Offline evaluation only supports on validation split (see table 2). Figure 6, shows some qualitative results using our method on validation set. From the experimental results, we can see that our proposed method outperforms other baseline methods on VQA. In Figure 7, we also visualize multi-modal fusion to compare how correctly MCAN [34] and our proposed multi-modal attention fusion can able to focus on image and question elements. The brighter bounding-box along with green color within the image and darker color in question represents higher attention score. We can see that our proposed method is able to focus more on correct answer. Beside that, Figure 8 shows typical failure cases using our method."
        ]
    },
    "S4.T3": {
        "caption": "Table 3:  Experimental results with other state-of-the-art models on Test-dev.",
        "table": "<table id=\"S4.T3.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T3.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Methods</span></th>\n<th id=\"S4.T3.2.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T3.2.1.1.2.1\" class=\"ltx_text ltx_font_bold\">All</span></th>\n<th id=\"S4.T3.2.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T3.2.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Other</span></th>\n<th id=\"S4.T3.2.1.1.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T3.2.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Y/N</span></th>\n<th id=\"S4.T3.2.1.1.5\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T3.2.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Num</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.2.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Bottom-up <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">29</a>]</cite>\n</td>\n<td id=\"S4.T3.2.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">65.32</td>\n<td id=\"S4.T3.2.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">56.05</td>\n<td id=\"S4.T3.2.2.1.4\" class=\"ltx_td ltx_align_left ltx_border_t\">81.82</td>\n<td id=\"S4.T3.2.2.1.5\" class=\"ltx_td ltx_align_left ltx_border_t\">44.21</td>\n</tr>\n<tr id=\"S4.T3.2.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.2.3.2.1\" class=\"ltx_td ltx_align_left\">MFH <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">36</a>]</cite>\n</td>\n<td id=\"S4.T3.2.3.2.2\" class=\"ltx_td ltx_align_left\">68.76</td>\n<td id=\"S4.T3.2.3.2.3\" class=\"ltx_td ltx_align_left\">59.89</td>\n<td id=\"S4.T3.2.3.2.4\" class=\"ltx_td ltx_align_left\">84.27</td>\n<td id=\"S4.T3.2.3.2.5\" class=\"ltx_td ltx_align_left\">49.56</td>\n</tr>\n<tr id=\"S4.T3.2.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.2.4.3.1\" class=\"ltx_td ltx_align_left\">BAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>\n</td>\n<td id=\"S4.T3.2.4.3.2\" class=\"ltx_td ltx_align_left\">69.52</td>\n<td id=\"S4.T3.2.4.3.3\" class=\"ltx_td ltx_align_left\">60.26</td>\n<td id=\"S4.T3.2.4.3.4\" class=\"ltx_td ltx_align_left\">85.31</td>\n<td id=\"S4.T3.2.4.3.5\" class=\"ltx_td ltx_align_left\">50.93</td>\n</tr>\n<tr id=\"S4.T3.2.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.2.5.4.1\" class=\"ltx_td ltx_align_left\">BAN+Counter <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>\n</td>\n<td id=\"S4.T3.2.5.4.2\" class=\"ltx_td ltx_align_left\">70.04</td>\n<td id=\"S4.T3.2.5.4.3\" class=\"ltx_td ltx_align_left\">60.52</td>\n<td id=\"S4.T3.2.5.4.4\" class=\"ltx_td ltx_align_left\">85.42</td>\n<td id=\"S4.T3.2.5.4.5\" class=\"ltx_td ltx_align_left\">54.04</td>\n</tr>\n<tr id=\"S4.T3.2.6.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.2.6.5.1\" class=\"ltx_td ltx_align_left\">MuRel <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">4</a>]</cite>\n</td>\n<td id=\"S4.T3.2.6.5.2\" class=\"ltx_td ltx_align_left\">68.03</td>\n<td id=\"S4.T3.2.6.5.3\" class=\"ltx_td ltx_align_left\">57.85</td>\n<td id=\"S4.T3.2.6.5.4\" class=\"ltx_td ltx_align_left\">84.77</td>\n<td id=\"S4.T3.2.6.5.5\" class=\"ltx_td ltx_align_left\">49.84</td>\n</tr>\n<tr id=\"S4.T3.2.7.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.2.7.6.1\" class=\"ltx_td ltx_align_left\">MCAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">34</a>]</cite>\n</td>\n<td id=\"S4.T3.2.7.6.2\" class=\"ltx_td ltx_align_left\">70.63</td>\n<td id=\"S4.T3.2.7.6.3\" class=\"ltx_td ltx_align_left\">60.72</td>\n<td id=\"S4.T3.2.7.6.4\" class=\"ltx_td ltx_align_left\">86.82</td>\n<td id=\"S4.T3.2.7.6.5\" class=\"ltx_td ltx_align_left\">53.26</td>\n</tr>\n<tr id=\"S4.T3.2.8.7\" class=\"ltx_tr\">\n<td id=\"S4.T3.2.8.7.1\" class=\"ltx_td ltx_align_left ltx_border_b\">Ours (MCAoA)</td>\n<td id=\"S4.T3.2.8.7.2\" class=\"ltx_td ltx_align_left ltx_border_b\"><span id=\"S4.T3.2.8.7.2.1\" class=\"ltx_text ltx_font_bold\">70.90</span></td>\n<td id=\"S4.T3.2.8.7.3\" class=\"ltx_td ltx_align_left ltx_border_b\"><span id=\"S4.T3.2.8.7.3.1\" class=\"ltx_text ltx_font_bold\">60.97</span></td>\n<td id=\"S4.T3.2.8.7.4\" class=\"ltx_td ltx_align_left ltx_border_b\"><span id=\"S4.T3.2.8.7.4.1\" class=\"ltx_text ltx_font_bold\">87.05</span></td>\n<td id=\"S4.T3.2.8.7.5\" class=\"ltx_td ltx_align_left ltx_border_b\"><span id=\"S4.T3.2.8.7.5.1\" class=\"ltx_text ltx_font_bold\">53.81</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We evaluate our model on VQA-v2 dataset and compare with other state-of-the-art methods. We re-run the PyTorch implementation provided by [34]111https://github.com/MILVLG/mcan-vqa and compare the results with our proposed method. Table 3 and 4 shows experimental results using test-dev and test-std respectively using online evaluation 222https://evalai.cloudcv.org/web/challenges/challenge-page/163/overview. Offline evaluation only supports on validation split (see table 2). Figure 6, shows some qualitative results using our method on validation set. From the experimental results, we can see that our proposed method outperforms other baseline methods on VQA. In Figure 7, we also visualize multi-modal fusion to compare how correctly MCAN [34] and our proposed multi-modal attention fusion can able to focus on image and question elements. The brighter bounding-box along with green color within the image and darker color in question represents higher attention score. We can see that our proposed method is able to focus more on correct answer. Beside that, Figure 8 shows typical failure cases using our method."
        ]
    },
    "S4.T4": {
        "caption": "Table 4:  Experimental results with other state-of-the-art models on Test-std.",
        "table": "<table id=\"S4.T4.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T4.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T4.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Methods</span></th>\n<th id=\"S4.T4.2.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T4.2.1.1.2.1\" class=\"ltx_text ltx_font_bold\">All</span></th>\n<th id=\"S4.T4.2.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T4.2.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Other</span></th>\n<th id=\"S4.T4.2.1.1.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T4.2.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Y/N</span></th>\n<th id=\"S4.T4.2.1.1.5\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T4.2.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Num</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.2.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T4.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Bottom-up <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">29</a>]</cite>\n</td>\n<td id=\"S4.T4.2.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">65.67</td>\n<td id=\"S4.T4.2.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">56.26</td>\n<td id=\"S4.T4.2.2.1.4\" class=\"ltx_td ltx_align_left ltx_border_t\">82.20</td>\n<td id=\"S4.T4.2.2.1.5\" class=\"ltx_td ltx_align_left ltx_border_t\">43.90</td>\n</tr>\n<tr id=\"S4.T4.2.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T4.2.3.2.1\" class=\"ltx_td ltx_align_left\">BAN+Counter <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>\n</td>\n<td id=\"S4.T4.2.3.2.2\" class=\"ltx_td ltx_align_left\">70.35</td>\n<td id=\"S4.T4.2.3.2.3\" class=\"ltx_td ltx_align_left\">-</td>\n<td id=\"S4.T4.2.3.2.4\" class=\"ltx_td ltx_align_left\">-</td>\n<td id=\"S4.T4.2.3.2.5\" class=\"ltx_td ltx_align_left\">-</td>\n</tr>\n<tr id=\"S4.T4.2.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T4.2.4.3.1\" class=\"ltx_td ltx_align_left\">MuRel <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">4</a>]</cite>\n</td>\n<td id=\"S4.T4.2.4.3.2\" class=\"ltx_td ltx_align_left\">68.41</td>\n<td id=\"S4.T4.2.4.3.3\" class=\"ltx_td ltx_align_left\">-</td>\n<td id=\"S4.T4.2.4.3.4\" class=\"ltx_td ltx_align_left\">-</td>\n<td id=\"S4.T4.2.4.3.5\" class=\"ltx_td ltx_align_left\">-</td>\n</tr>\n<tr id=\"S4.T4.2.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T4.2.5.4.1\" class=\"ltx_td ltx_align_left\">MCAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">34</a>]</cite>\n</td>\n<td id=\"S4.T4.2.5.4.2\" class=\"ltx_td ltx_align_left\">70.90</td>\n<td id=\"S4.T4.2.5.4.3\" class=\"ltx_td ltx_align_left\">-</td>\n<td id=\"S4.T4.2.5.4.4\" class=\"ltx_td ltx_align_left\">-</td>\n<td id=\"S4.T4.2.5.4.5\" class=\"ltx_td ltx_align_left\">-</td>\n</tr>\n<tr id=\"S4.T4.2.6.5\" class=\"ltx_tr\">\n<td id=\"S4.T4.2.6.5.1\" class=\"ltx_td ltx_align_left ltx_border_b\">Ours (MCAoA)</td>\n<td id=\"S4.T4.2.6.5.2\" class=\"ltx_td ltx_align_left ltx_border_b\"><span id=\"S4.T4.2.6.5.2.1\" class=\"ltx_text ltx_font_bold\">71.14</span></td>\n<td id=\"S4.T4.2.6.5.3\" class=\"ltx_td ltx_align_left ltx_border_b\"><span id=\"S4.T4.2.6.5.3.1\" class=\"ltx_text ltx_font_bold\">61.18</span></td>\n<td id=\"S4.T4.2.6.5.4\" class=\"ltx_td ltx_align_left ltx_border_b\"><span id=\"S4.T4.2.6.5.4.1\" class=\"ltx_text ltx_font_bold\">87.25</span></td>\n<td id=\"S4.T4.2.6.5.5\" class=\"ltx_td ltx_align_left ltx_border_b\"><span id=\"S4.T4.2.6.5.5.1\" class=\"ltx_text ltx_font_bold\">53.36</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We evaluate our model on VQA-v2 dataset and compare with other state-of-the-art methods. We re-run the PyTorch implementation provided by [34]111https://github.com/MILVLG/mcan-vqa and compare the results with our proposed method. Table 3 and 4 shows experimental results using test-dev and test-std respectively using online evaluation 222https://evalai.cloudcv.org/web/challenges/challenge-page/163/overview. Offline evaluation only supports on validation split (see table 2). Figure 6, shows some qualitative results using our method on validation set. From the experimental results, we can see that our proposed method outperforms other baseline methods on VQA. In Figure 7, we also visualize multi-modal fusion to compare how correctly MCAN [34] and our proposed multi-modal attention fusion can able to focus on image and question elements. The brighter bounding-box along with green color within the image and darker color in question represents higher attention score. We can see that our proposed method is able to focus more on correct answer. Beside that, Figure 8 shows typical failure cases using our method."
        ]
    }
}