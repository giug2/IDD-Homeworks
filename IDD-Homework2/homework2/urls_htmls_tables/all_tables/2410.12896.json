{
    "id_table_1": {
        "caption": "Table 1 .  Data synthesis and augmentation in data preparation. In the table, method outlines the techniques presented by each research. Data source and synthetic data indicate the original data used to generate synthetic data and the synthetic data created for training purposes, respectively. A dash (-) in any cell denotes that the respective content was not mentioned in the cited literature.",
        "table": "S3.T1.4",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "To overcome these challenges, researchers have increasingly turned to LLM-oriented data synthesis and augmentation techniques, recognizing the ability of LLMs to model complex patterns from large datasets and generate synthetic data that closely mirror real-world distributions while introducing valuable variations  (Zhang et al . ,  2023b ; Dai et al . ,  2023a ; Samuel et al . ,  2023 ) . These studies reduce the reliance on manually curated datasets and enable the generation of high-quality, diverse data that meets the evolving demands of LLMs throughout their lifecycle and functions. To capture the breadth of these efforts, we collected papers related to LLM-oriented data synthesis and augmentation by searching Google Scholar using keywords such as data synthesis, data augmentation, and large models. Figure  1  illustrates the publication trends by year and venue, reflecting the increasing interest in this field. As of October 2024, we identified  250 250 250 250  unique publications covering diverse research topics and venues. Summarizing these efforts provides critical insights into the progress and challenges that remain, offering a foundation for future research. Despite these advancements, several key challenges remain in LLM-oriented data synthesis and augmentation. The misuse of synthetic data poses risks, particularly in spreading misinformation and raising ethical concerns around manipulating public opinion. Additionally, synthetic data often introduces ambiguity when aligning AI models with human values, potentially leading to biased outcomes. Evaluating models trained on synthetic data is also complex, as traditional benchmarks may not fully capture the nuances of this data. Ensuring reliability is another concern, as biases and inaccuracies from original datasets can persist in synthetic data, limiting its generalization across domains. Moreover, the computational demands of LLMs, along with challenges in handling less common languages or novel instructions, complicate broader applications. Finally, the lack of a unified framework for organizing and comparing the methods proposed in both academia and industry remains a barrier for researchers navigating this rapidly evolving field.",
            "for tree= grow=east, reversed=true, anchor=base west, parent anchor=east, child anchor=west, base=left, font=, rectangle, draw=hidden-draw, rounded corners, align=left, minimum width=4em, edge+=darkgray, line width=1pt, s sep=3pt, inner xsep=2pt, inner ysep=3pt, edge path= [draw,  \\forestoption edge] (!u.parent anchor)  ++(1.5mm,0) - (.child anchor)  \\forestoption edge label;, ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center, , where level=1text width=7.2em,font= , , where level=2text width=8.0em,font= , , where level=3text width=10.0em,font= , , where level=4text width=9.0em,font= , , where level=5text width=6.4em,font= , , [ Data Synthesis and Augmentation for Large Language Models: A Survey, ver, color=bounding!100, fill=0!15, text=black, font=, text width=32.0em, text centered [ Taxonomy ( 2 ), color=1!100, fill=1!80, text=black [ Data Augmentation   ( 2.1 ), color=1!100, fill=1!65, text=black [ Data Labeling, color=1!100, fill=1_1!55, text=black [  T-SciQ  (Wang et al . ,  2024d ) , ChatGPT-based  (Zhu et al . ,  2023b ; Gilardi et al . ,  2023 ; Alizadeh et al . ,  2023 ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] [ Data Reformation, color=1!100, fill=1_1!55, text=black [  Mosaic (Jocher,  2020 ) , CORE  (Dixit et al . ,  2022 ) , ALIA   (Dunlap et al . ,  2023 ) , ChatAug   (Dai et al . ,  2023a ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] [ Co-Annotation, color=1!100, fill=1_1!55, text=black [  Co-annotating  (Li et al . ,  2023d ) , ToolCoder  (Zhang et al . ,  2023e ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] ] [ Data Synthesis ( 2.2 ), color=1!100, fill=1!65, text=black [ General Model   Distillation, color=1!100, fill=1_1!55, text=black [  TinyStories (Eldan and Li,  2023 ) , Phi-1 (Gunasekar et al . ,  2023 ; Li et al . ,  2023b ) , Alpagasus  (Chen et al . ,  2023b ) , WizardLM  (Xu et al . ,  2023b ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] [ Domain Model   Distillation, color=1!100, fill=1_1!55, text=black [  Minerva (Lewkowycz et al . ,  2022 ) , DeepSeek-Prover (Xin et al . ,  2024 ) , WizardCoder (Luo et al . ,  2024 ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=1!100, fill=1_1!55, text=black [  Rephrasing (Maini et al . ,  2024 ) , Self-instruct (Wang et al . ,  2023b ) , SPIN  (Chen et al . ,  2024a ) , SelTDA  (Khan et al . ,  2023 ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] ] ] [ Full Lifecycle   of LM ( 3 ), color=2!100, fill=2!85, text=black [ Data Preparation   ( 3.1 ), color=orange!70, fill=orange!35, text=black [ General Model Distillation, color=orange!70, fill=orange!18, text=black [  Dialogic  (Li et al . ,  2022a ) , MathInstruct  (Yue et al . ,  2023b ) , Genixer  (Zhao et al . ,  2023a ) , Magpie  (Xu et al . ,  2024b ) ,   MMIQC  (Liu et al . ,  2024f ) , Genie  (Yehudai et al . ,  2024 ) , Case2Code  (Shao et al . ,  2024a ) , UltraChat  (Ding et al . ,  2023 ) , color=bounding!70, fill=orange!7, text=black, text width=24.0em ] ] [ Data Augmentation, color=orange!70, fill=orange!18, text=black [  Disco  (Chen et al . ,  2022 ) , GPT3Mix  (Yoo et al . ,  2021 ) , CoAnnotating  (Li et al . ,  2023d ) , ALIA  (Dunlap et al . ,  2023 ) ,   FullAnno  (Hao et al . ,  2024 ) , Dialgen  (Lu et al . ,  2023 ) , TinyGSM  (Liu et al . ,  2023a ) , AMPS  (Hendrycks et al . ,  2021 ) , color=bounding!70, fill=orange!7, text=black, text width=24.0em ] ] ] [ Pretraining ( 3.2 ), color=blue!40, fill=blue!20, text=black [ General Model Distillation, color=blue!40, fill=blue!10, text=black [  Phi-1  (Gunasekar et al . ,  2023 ) , SciLitLLM  (Li et al . ,  2024c ) , TRAIT  (Liang et al . ,  2024 ) , AnyGPT  (Zhan et al . ,  2024 ) ,   Phi-1.5  (Li et al . ,  2023b ) , TinyDialogues  (Feng et al . ,  2024 ) , color=bounding!70, fill=blue!5, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=blue!40, fill=blue!10, text=black [  VILA-2  (Fang et al . ,  2024 ) , color=bounding!70, fill=blue!5, text=black, text width=24.0em ] ] [ Data Augmentation, color=blue!40, fill=blue!10, text=black [  WRAP  (Maini et al . ,  2024 ) , KMLM  (Liu et al . ,  2021 ) , bioR  (Zhu and Li,  2023 ) , Physics-based  (Liu et al . ,  2024a ) , color=bounding!70, fill=blue!5, text=black, text width=24.0em ] ] ] [ Finetuning ( 3.3 ), color=purple!60, fill=purple!20, text=black [ General Model Distillation, color=purple!60, fill=purple!10, text=black [  LAB  (Sudalairaj et al . ,  2024 ) , LLM2LLM  (Lee et al . ,  2024 ) , GLAN  (Li et al . ,  2024b ) , Clingen  (Xu et al . ,  2024a ) ,   Baize  (Xu et al . ,  2023a ) , Evol-Instruct  (Xu et al . ,  2023b ) , HuaTuo  (Wang et al . ,  2023c ) , NExT-GPT  (Wu et al . ,  2023a ) , color=bounding!70, fill=purple!5, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=purple!60, fill=purple!10, text=black [  STaR  (Zelikman et al . ,  2022 ) , REST  (Gulcehre et al . ,  2023 ) , Self-Translate  (Ri et al . ,  2024 ) , Self-Instruct  (Wang et al . ,  2023b ) ,   RFT  (Yuan et al . ,  2023 ) , CodeRL  (Le et al . ,  2022 ) , REST-EM  (Singh et al . ,  2023 ) , DeepSeekProver  (Xin et al . ,  2024 ) , color=bounding!70, fill=purple!5, text=black, text width=24.0em ] ] [ Data Augmentation, color=purple!60, fill=purple!10, text=black [  MathGenie  (Lu et al . ,  2024a ) , DISC-MedLLM  (Bao et al . ,  2023 ) , MetaMath  (Yu et al . ,  2024 ) ,   Symbol tuning  (Wei et al . ,  2023a ) , Llama-3-UltraMedical  (Zhang et al . ,  2024g ) , Llemma  (Azerbayev et al . ,  2023 ) , color=bounding!70, fill=purple!5, text=black, text width=24.0em ] ] ] [ Instruction-Tuning  ( 3.4 ), color=BlueGreen!150, fill=BlueGreen!90, text=black [ General Model Distillation, color=BlueGreen!120, fill=BlueGreen!50, text=black [  Alpaca  (Taori et al . ,  2023 ) , Vicuna  (Chiang et al . ,  2023 ) , Orca   (Mukherjee et al . ,  2023 ) ,Baize  (Xu et al . ,  2023a ) , LLaVA  (Liu et al . ,  2024c ) , color=bounding!70, fill=BlueGreen!30, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=BlueGreen!120, fill=BlueGreen!50, text=black [  Self-Instruct  (Wang et al . ,  2023b ) , SPIN  (Chen et al . ,  2024a ) , CAI  (Bai et al . ,  2022b ) , Toolformer  (Schick et al . ,  2024 ) , color=bounding!70, fill=BlueGreen!30, text=black, text width=24.0em ] ] [ Data Augmentation, color=BlueGreen!120, fill=BlueGreen!50, text=black [  T-SciQ  (Wang et al . ,  2024d ) , CORE  (Dixit et al . ,  2022 ) , ChatAug   (Dai et al . ,  2023a ) , ToolCoder  (Zhang et al . ,  2023e ) , color=bounding!70, fill=BlueGreen!30, text=black, text width=24.0em ] ] ] [ Preference   Alignment( 3.5 ) , color=BlueViolet!60, fill=BlueViolet!25, text=black [ General Model Distillation, color=BlueViolet!60, fill=BlueViolet!15, text=black [  ULTRAFEEDBACK  (Cui et al . ,  2023b ) , HelpSteer  (Wang et al . ,  2023a ) , LEMA  (An et al . ,  2023 ) ,color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em ] ] [ Domain Model Distillation, color=BlueViolet!60, fill=BlueViolet!15, text=black [  BAD   (Xu et al . ,  2021 ) , BEAVERTAILS  (Ji et al . ,  2024 ) , PRM800K  (Lightman et al . ,  2023 ) , WebGPT  (Nakano et al . ,  2021 ) ,color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=BlueViolet!60, fill=BlueViolet!15, text=black [  OAIF  (Guo et al . ,  2024 ) , SELF-JUDGE  (Ye and Ng,  2024 ) , SALMON  (Sun et al . ,  2024b ) , SteerLM  (Dong et al . ,  2023 ) , color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em ] ] [ Data Augmentation, color=BlueViolet!60, fill=BlueViolet!15, text=black [   Starling-7B  (Zhu et al . ,  2023a ) , UltraInteract   (Yuan et al . ,  2024a ) , CriticBench  (Lin et al . ,  2024a ) , color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em ] ] ] [ Applications ( 3.6 ), color=red!40, fill=red!25, text=black [ Math, color=4!80, fill=4_1!70, text=black [  MetaMath  (Yu et al . ,  2024 ) , MammoTH  (Yue et al . ,  2023b ) , STaR  (Zelikman et al . ,  2022 ) , Galactia  (Taylor et al . ,  2022 ) ,    DeepSeekProver  (Xin et al . ,  2024 ) , WizardMath  (Luo et al . ,  2023 ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] [ Science, color=4!80, fill=4_1!70, text=black [  SciLitLLM  (Li et al . ,  2024c ) , ChemLLM  (Zhang et al . ,  2024f ) , SciGLM  (Zhang et al . ,  2024d ) , Galactia  (Taylor et al . ,  2022 ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] [ Code, color=4!80, fill=4_1!70, text=black [  WizardCoder  (Luo et al . ,  2024 ) , MagicCoder  (Wei et al . ,  2024 ) , Code Alpaca  (Chaudhary,  2023 ) ,    Code LLama  (Roziere et al . ,  2023 ) , Phi-1  (Gunasekar et al . ,  2023 ) , Phi-1.5  (Li et al . ,  2023b ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] [ Medical, color=4!80, fill=4_1!70, text=black [  DISC-MedLLM  (Bao et al . ,  2023 ) , HuatuoGPT  (Zhang et al . ,  2023a ; Chen et al . ,  1 16 ) , ChatCounselor  (Liu et al . ,  9 27 ) ,   ClinGen  (Xu et al . ,  2024a ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] [ Law, color=4!80, fill=4_1!70, text=black [  DISC-LawLLM  (Yue et al . ,  2023a ) , LawyerLLaMA  (Huang et al . ,  2023 ) , LawGPT  (Zhou et al . ,  2024b ) ,    WisdomInterrogatory  (zhihaiLLM,  847Z ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] ] ] [ Functionality ( 4 ), color=SlateBlue!80, fill=SlateBlue!40, text=black [ Understanding ( 4.1 ), color=brown!60, fill=brown!30, text=black [  Alpaca  (Taori et al . ,  2023 ) , WizardLM  (Xu et al . ,  2023b ) , WRAP (Maini et al . ,  2024 ) , LLaVA  (Liu et al . ,  2024c ) , ChartLlama (Han et al . ,  2023 ) ,Genixer (Zhao et al . ,  2023a ) , color=bounding!70, fill=brown!10, text=black, text width=35.8em ] ] [ Logic ( 4.2 ), color=2_2_2!100, fill=2_2_2!60, text=black [  ReST EM   (Singh et al . ,  2023 ) , Case2Code (Shao et al . ,  2024a ) , MathInstruct (Yue et al . ,  2023b ) , MMIQC (Liu et al . ,  2024f ) , STaR (Zelikman et al . ,  2022 ) ,SelTDA  (Khan et al . ,  2023 ) , color=bounding!70, fill=2_2_2!20, text=black, text width=35.8em ] ] [ Memory ( 4.3 ), color=LightGreen!100, fill=LightGreen!40, text=black [  Quiet-STaR  (Zelikman et al . ,  2024 ) , AutoKG  (Zhu et al . ,  2024 ) , Persona Hub  (Chan et al . ,  2024 ) , AceCoder  (Li et al . ,  2023g ) , RepoCoder  (Zhang et al . ,  2023c ) , color=bounding!70, fill=LightGreen!10, text=black, text width=35.8em ] ] [ Generation ( 4.4 ), color=Turquoise!80, fill=Turquoise!30, text=black [  Genie (Yehudai et al . ,  2024 ) , UltraMedical (Zhang et al . ,  2024g ) , HuaTuo (Wang et al . ,  2023c ) , TinyStories (Eldan and Li,  2023 ) , DIALOGIC (Li et al . ,  2022a ) , ALIA  (Dunlap et al . ,  2023 ) , color=bounding!70, fill=Turquoise!10, text=black, text width=35.8em ] ] ] [ Challenges and   Limitations ( 5 ), color=6!100, fill=6!50, text=black [ Synthesizing and   Augmenting Method   ( 5.1 ), color=6!100, fill=6!35, text=black [  d-RLAIF (Lee et al . ,  2023 ) , LLM2LLM (Lee et al . ,  2024 ) , Wizardmath (Luo et al . ,  2023 ) , STaR (Zelikman et al . ,  2022 ) , SciGLM (Zhang et al . ,  2024d ) , ChemLLM (Zhang et al . ,  2024f )  , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] [ Data Quality ( 5.2 ), color=6!100, fill=6!35, text=black [   LLMs4Synthesis (Giglou et al . ,  2024 ) , CoRAL (Wu et al . ,  2024a ) , FORD (Xiong et al . ,  2023 ) ,LTGC (Zhao et al . ,  2024a ) , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] [ Impact of Data   Synthesis and   Augmentation ( 5.3 ), color=6!100, fill=6!35, text=black [   DataDreamer (Patel et al . ,  2024 ) ,HARMONIC (Wang et al . ,  2024c ) , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] [ Impact on Different   Applications and   Tasks ( LABEL:sec:Impact_on_Different_Applications_and_Tasks ), color=6!100, fill=6!35, text=black [  PANDA (Liu et al . ,  2024e ) ,REGA (Wang et al . ,  2024e ) , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] [ Future Directions   ( 5.5 ), color=6!100, fill=6!35, text=black [  TabSynDex (Chundawat et al . ,  2022 ) ,CoLa-Diff (Jiang et al . ,  2023b ) ,WizardCoder (Luo et al . ,  2024 ) , WebGPT (Nakano et al . ,  2021 ) , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] ] ]"
        ]
    },
    "id_table_2": {
        "caption": "Table 2 .  Data synthesis and augmentation in pre-training. Method outlines the techniques presented by each research. Data source and synthetic data indicate the original data used to generate synthetic data and the synthetic data created for pre-training, respectively. Base model and pre-trained model indicate the foundational models and the models that have undergone pre-training, respectively. A dash (-) in any cell denotes that the respective content was not mentioned in the cited literature.",
        "table": "S3.T2.4",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "This survey aims to address these gaps by providing a comprehensive overview of LLM-oriented data synthesis and augmentation techniques. As shown in Figure  2 , unlike previous surveys  (Long et al . ,  2024 ; Ma et al . ,  2024 ; Zhou et al . ,  2024a ; Wei et al . ,  2023b ; Ding et al . ,  2024 ) , which primarily focus on applying these methods to support specific downstream tasks or particular stages of LLMs, our work emphasizes the direct role of LLM-oriented techniques in improving the overall performance of LLMs across various stages of their lifecycle and core functions. In contrast to the work  (Liu et al . ,  2024d ) , which focuses on practices for synthetic data generation to address challenges like data scarcity and privacy, our survey extends beyond practical guidance by categorizing methods aimed at improving LLM performance holistically. We examine not only data generation but also how these techniques enhance LLMs across all stages and functions, offering a more integrated, data-centric framework for advancing LLMs. Specifically, we systematically review and categorize existing research from two key perspectives: the lifecycle of LLMs (from pre-training to fine-tuning and application) and their core functions (understanding, logic, memory, and generation). By framing the discussion around these dual perspectives, we offer clearer insights into the development, interconnections, and practical applications of different approaches. Moreover, we identify critical challenges, explore emerging research directions, and highlight potential breakthroughs that could further drive advancements in LLM performance through data-centric methods.",
            "for tree= grow=east, reversed=true, anchor=base west, parent anchor=east, child anchor=west, base=left, font=, rectangle, draw=hidden-draw, rounded corners, align=left, minimum width=4em, edge+=darkgray, line width=1pt, s sep=3pt, inner xsep=2pt, inner ysep=3pt, edge path= [draw,  \\forestoption edge] (!u.parent anchor)  ++(1.5mm,0) - (.child anchor)  \\forestoption edge label;, ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center, , where level=1text width=7.2em,font= , , where level=2text width=8.0em,font= , , where level=3text width=10.0em,font= , , where level=4text width=9.0em,font= , , where level=5text width=6.4em,font= , , [ Data Synthesis and Augmentation for Large Language Models: A Survey, ver, color=bounding!100, fill=0!15, text=black, font=, text width=32.0em, text centered [ Taxonomy ( 2 ), color=1!100, fill=1!80, text=black [ Data Augmentation   ( 2.1 ), color=1!100, fill=1!65, text=black [ Data Labeling, color=1!100, fill=1_1!55, text=black [  T-SciQ  (Wang et al . ,  2024d ) , ChatGPT-based  (Zhu et al . ,  2023b ; Gilardi et al . ,  2023 ; Alizadeh et al . ,  2023 ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] [ Data Reformation, color=1!100, fill=1_1!55, text=black [  Mosaic (Jocher,  2020 ) , CORE  (Dixit et al . ,  2022 ) , ALIA   (Dunlap et al . ,  2023 ) , ChatAug   (Dai et al . ,  2023a ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] [ Co-Annotation, color=1!100, fill=1_1!55, text=black [  Co-annotating  (Li et al . ,  2023d ) , ToolCoder  (Zhang et al . ,  2023e ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] ] [ Data Synthesis ( 2.2 ), color=1!100, fill=1!65, text=black [ General Model   Distillation, color=1!100, fill=1_1!55, text=black [  TinyStories (Eldan and Li,  2023 ) , Phi-1 (Gunasekar et al . ,  2023 ; Li et al . ,  2023b ) , Alpagasus  (Chen et al . ,  2023b ) , WizardLM  (Xu et al . ,  2023b ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] [ Domain Model   Distillation, color=1!100, fill=1_1!55, text=black [  Minerva (Lewkowycz et al . ,  2022 ) , DeepSeek-Prover (Xin et al . ,  2024 ) , WizardCoder (Luo et al . ,  2024 ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=1!100, fill=1_1!55, text=black [  Rephrasing (Maini et al . ,  2024 ) , Self-instruct (Wang et al . ,  2023b ) , SPIN  (Chen et al . ,  2024a ) , SelTDA  (Khan et al . ,  2023 ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] ] ] [ Full Lifecycle   of LM ( 3 ), color=2!100, fill=2!85, text=black [ Data Preparation   ( 3.1 ), color=orange!70, fill=orange!35, text=black [ General Model Distillation, color=orange!70, fill=orange!18, text=black [  Dialogic  (Li et al . ,  2022a ) , MathInstruct  (Yue et al . ,  2023b ) , Genixer  (Zhao et al . ,  2023a ) , Magpie  (Xu et al . ,  2024b ) ,   MMIQC  (Liu et al . ,  2024f ) , Genie  (Yehudai et al . ,  2024 ) , Case2Code  (Shao et al . ,  2024a ) , UltraChat  (Ding et al . ,  2023 ) , color=bounding!70, fill=orange!7, text=black, text width=24.0em ] ] [ Data Augmentation, color=orange!70, fill=orange!18, text=black [  Disco  (Chen et al . ,  2022 ) , GPT3Mix  (Yoo et al . ,  2021 ) , CoAnnotating  (Li et al . ,  2023d ) , ALIA  (Dunlap et al . ,  2023 ) ,   FullAnno  (Hao et al . ,  2024 ) , Dialgen  (Lu et al . ,  2023 ) , TinyGSM  (Liu et al . ,  2023a ) , AMPS  (Hendrycks et al . ,  2021 ) , color=bounding!70, fill=orange!7, text=black, text width=24.0em ] ] ] [ Pretraining ( 3.2 ), color=blue!40, fill=blue!20, text=black [ General Model Distillation, color=blue!40, fill=blue!10, text=black [  Phi-1  (Gunasekar et al . ,  2023 ) , SciLitLLM  (Li et al . ,  2024c ) , TRAIT  (Liang et al . ,  2024 ) , AnyGPT  (Zhan et al . ,  2024 ) ,   Phi-1.5  (Li et al . ,  2023b ) , TinyDialogues  (Feng et al . ,  2024 ) , color=bounding!70, fill=blue!5, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=blue!40, fill=blue!10, text=black [  VILA-2  (Fang et al . ,  2024 ) , color=bounding!70, fill=blue!5, text=black, text width=24.0em ] ] [ Data Augmentation, color=blue!40, fill=blue!10, text=black [  WRAP  (Maini et al . ,  2024 ) , KMLM  (Liu et al . ,  2021 ) , bioR  (Zhu and Li,  2023 ) , Physics-based  (Liu et al . ,  2024a ) , color=bounding!70, fill=blue!5, text=black, text width=24.0em ] ] ] [ Finetuning ( 3.3 ), color=purple!60, fill=purple!20, text=black [ General Model Distillation, color=purple!60, fill=purple!10, text=black [  LAB  (Sudalairaj et al . ,  2024 ) , LLM2LLM  (Lee et al . ,  2024 ) , GLAN  (Li et al . ,  2024b ) , Clingen  (Xu et al . ,  2024a ) ,   Baize  (Xu et al . ,  2023a ) , Evol-Instruct  (Xu et al . ,  2023b ) , HuaTuo  (Wang et al . ,  2023c ) , NExT-GPT  (Wu et al . ,  2023a ) , color=bounding!70, fill=purple!5, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=purple!60, fill=purple!10, text=black [  STaR  (Zelikman et al . ,  2022 ) , REST  (Gulcehre et al . ,  2023 ) , Self-Translate  (Ri et al . ,  2024 ) , Self-Instruct  (Wang et al . ,  2023b ) ,   RFT  (Yuan et al . ,  2023 ) , CodeRL  (Le et al . ,  2022 ) , REST-EM  (Singh et al . ,  2023 ) , DeepSeekProver  (Xin et al . ,  2024 ) , color=bounding!70, fill=purple!5, text=black, text width=24.0em ] ] [ Data Augmentation, color=purple!60, fill=purple!10, text=black [  MathGenie  (Lu et al . ,  2024a ) , DISC-MedLLM  (Bao et al . ,  2023 ) , MetaMath  (Yu et al . ,  2024 ) ,   Symbol tuning  (Wei et al . ,  2023a ) , Llama-3-UltraMedical  (Zhang et al . ,  2024g ) , Llemma  (Azerbayev et al . ,  2023 ) , color=bounding!70, fill=purple!5, text=black, text width=24.0em ] ] ] [ Instruction-Tuning  ( 3.4 ), color=BlueGreen!150, fill=BlueGreen!90, text=black [ General Model Distillation, color=BlueGreen!120, fill=BlueGreen!50, text=black [  Alpaca  (Taori et al . ,  2023 ) , Vicuna  (Chiang et al . ,  2023 ) , Orca   (Mukherjee et al . ,  2023 ) ,Baize  (Xu et al . ,  2023a ) , LLaVA  (Liu et al . ,  2024c ) , color=bounding!70, fill=BlueGreen!30, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=BlueGreen!120, fill=BlueGreen!50, text=black [  Self-Instruct  (Wang et al . ,  2023b ) , SPIN  (Chen et al . ,  2024a ) , CAI  (Bai et al . ,  2022b ) , Toolformer  (Schick et al . ,  2024 ) , color=bounding!70, fill=BlueGreen!30, text=black, text width=24.0em ] ] [ Data Augmentation, color=BlueGreen!120, fill=BlueGreen!50, text=black [  T-SciQ  (Wang et al . ,  2024d ) , CORE  (Dixit et al . ,  2022 ) , ChatAug   (Dai et al . ,  2023a ) , ToolCoder  (Zhang et al . ,  2023e ) , color=bounding!70, fill=BlueGreen!30, text=black, text width=24.0em ] ] ] [ Preference   Alignment( 3.5 ) , color=BlueViolet!60, fill=BlueViolet!25, text=black [ General Model Distillation, color=BlueViolet!60, fill=BlueViolet!15, text=black [  ULTRAFEEDBACK  (Cui et al . ,  2023b ) , HelpSteer  (Wang et al . ,  2023a ) , LEMA  (An et al . ,  2023 ) ,color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em ] ] [ Domain Model Distillation, color=BlueViolet!60, fill=BlueViolet!15, text=black [  BAD   (Xu et al . ,  2021 ) , BEAVERTAILS  (Ji et al . ,  2024 ) , PRM800K  (Lightman et al . ,  2023 ) , WebGPT  (Nakano et al . ,  2021 ) ,color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=BlueViolet!60, fill=BlueViolet!15, text=black [  OAIF  (Guo et al . ,  2024 ) , SELF-JUDGE  (Ye and Ng,  2024 ) , SALMON  (Sun et al . ,  2024b ) , SteerLM  (Dong et al . ,  2023 ) , color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em ] ] [ Data Augmentation, color=BlueViolet!60, fill=BlueViolet!15, text=black [   Starling-7B  (Zhu et al . ,  2023a ) , UltraInteract   (Yuan et al . ,  2024a ) , CriticBench  (Lin et al . ,  2024a ) , color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em ] ] ] [ Applications ( 3.6 ), color=red!40, fill=red!25, text=black [ Math, color=4!80, fill=4_1!70, text=black [  MetaMath  (Yu et al . ,  2024 ) , MammoTH  (Yue et al . ,  2023b ) , STaR  (Zelikman et al . ,  2022 ) , Galactia  (Taylor et al . ,  2022 ) ,    DeepSeekProver  (Xin et al . ,  2024 ) , WizardMath  (Luo et al . ,  2023 ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] [ Science, color=4!80, fill=4_1!70, text=black [  SciLitLLM  (Li et al . ,  2024c ) , ChemLLM  (Zhang et al . ,  2024f ) , SciGLM  (Zhang et al . ,  2024d ) , Galactia  (Taylor et al . ,  2022 ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] [ Code, color=4!80, fill=4_1!70, text=black [  WizardCoder  (Luo et al . ,  2024 ) , MagicCoder  (Wei et al . ,  2024 ) , Code Alpaca  (Chaudhary,  2023 ) ,    Code LLama  (Roziere et al . ,  2023 ) , Phi-1  (Gunasekar et al . ,  2023 ) , Phi-1.5  (Li et al . ,  2023b ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] [ Medical, color=4!80, fill=4_1!70, text=black [  DISC-MedLLM  (Bao et al . ,  2023 ) , HuatuoGPT  (Zhang et al . ,  2023a ; Chen et al . ,  1 16 ) , ChatCounselor  (Liu et al . ,  9 27 ) ,   ClinGen  (Xu et al . ,  2024a ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] [ Law, color=4!80, fill=4_1!70, text=black [  DISC-LawLLM  (Yue et al . ,  2023a ) , LawyerLLaMA  (Huang et al . ,  2023 ) , LawGPT  (Zhou et al . ,  2024b ) ,    WisdomInterrogatory  (zhihaiLLM,  847Z ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] ] ] [ Functionality ( 4 ), color=SlateBlue!80, fill=SlateBlue!40, text=black [ Understanding ( 4.1 ), color=brown!60, fill=brown!30, text=black [  Alpaca  (Taori et al . ,  2023 ) , WizardLM  (Xu et al . ,  2023b ) , WRAP (Maini et al . ,  2024 ) , LLaVA  (Liu et al . ,  2024c ) , ChartLlama (Han et al . ,  2023 ) ,Genixer (Zhao et al . ,  2023a ) , color=bounding!70, fill=brown!10, text=black, text width=35.8em ] ] [ Logic ( 4.2 ), color=2_2_2!100, fill=2_2_2!60, text=black [  ReST EM   (Singh et al . ,  2023 ) , Case2Code (Shao et al . ,  2024a ) , MathInstruct (Yue et al . ,  2023b ) , MMIQC (Liu et al . ,  2024f ) , STaR (Zelikman et al . ,  2022 ) ,SelTDA  (Khan et al . ,  2023 ) , color=bounding!70, fill=2_2_2!20, text=black, text width=35.8em ] ] [ Memory ( 4.3 ), color=LightGreen!100, fill=LightGreen!40, text=black [  Quiet-STaR  (Zelikman et al . ,  2024 ) , AutoKG  (Zhu et al . ,  2024 ) , Persona Hub  (Chan et al . ,  2024 ) , AceCoder  (Li et al . ,  2023g ) , RepoCoder  (Zhang et al . ,  2023c ) , color=bounding!70, fill=LightGreen!10, text=black, text width=35.8em ] ] [ Generation ( 4.4 ), color=Turquoise!80, fill=Turquoise!30, text=black [  Genie (Yehudai et al . ,  2024 ) , UltraMedical (Zhang et al . ,  2024g ) , HuaTuo (Wang et al . ,  2023c ) , TinyStories (Eldan and Li,  2023 ) , DIALOGIC (Li et al . ,  2022a ) , ALIA  (Dunlap et al . ,  2023 ) , color=bounding!70, fill=Turquoise!10, text=black, text width=35.8em ] ] ] [ Challenges and   Limitations ( 5 ), color=6!100, fill=6!50, text=black [ Synthesizing and   Augmenting Method   ( 5.1 ), color=6!100, fill=6!35, text=black [  d-RLAIF (Lee et al . ,  2023 ) , LLM2LLM (Lee et al . ,  2024 ) , Wizardmath (Luo et al . ,  2023 ) , STaR (Zelikman et al . ,  2022 ) , SciGLM (Zhang et al . ,  2024d ) , ChemLLM (Zhang et al . ,  2024f )  , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] [ Data Quality ( 5.2 ), color=6!100, fill=6!35, text=black [   LLMs4Synthesis (Giglou et al . ,  2024 ) , CoRAL (Wu et al . ,  2024a ) , FORD (Xiong et al . ,  2023 ) ,LTGC (Zhao et al . ,  2024a ) , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] [ Impact of Data   Synthesis and   Augmentation ( 5.3 ), color=6!100, fill=6!35, text=black [   DataDreamer (Patel et al . ,  2024 ) ,HARMONIC (Wang et al . ,  2024c ) , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] [ Impact on Different   Applications and   Tasks ( LABEL:sec:Impact_on_Different_Applications_and_Tasks ), color=6!100, fill=6!35, text=black [  PANDA (Liu et al . ,  2024e ) ,REGA (Wang et al . ,  2024e ) , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] [ Future Directions   ( 5.5 ), color=6!100, fill=6!35, text=black [  TabSynDex (Chundawat et al . ,  2022 ) ,CoLa-Diff (Jiang et al . ,  2023b ) ,WizardCoder (Luo et al . ,  2024 ) , WebGPT (Nakano et al . ,  2021 ) , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] ] ]"
        ]
    },
    "id_table_3": {
        "caption": "Table 3 .  Data synthesis and augmentation in fine-tuning. In the table, method outlines the techniques presented by each research. Data source and synthetic data indicate the original data used to generate synthetic data and the synthetic data created for fine-tuning, respectively. Base model and fine-tuned model indicate the foundational models and the models that have undergone fine-tuning, respectively. A dash (-) in any cell denotes that the respective content was not mentioned in the cited literature.",
        "table": "S3.T3.4",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "for tree= grow=east, reversed=true, anchor=base west, parent anchor=east, child anchor=west, base=left, font=, rectangle, draw=hidden-draw, rounded corners, align=left, minimum width=4em, edge+=darkgray, line width=1pt, s sep=3pt, inner xsep=2pt, inner ysep=3pt, edge path= [draw,  \\forestoption edge] (!u.parent anchor)  ++(1.5mm,0) - (.child anchor)  \\forestoption edge label;, ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center, , where level=1text width=7.2em,font= , , where level=2text width=8.0em,font= , , where level=3text width=10.0em,font= , , where level=4text width=9.0em,font= , , where level=5text width=6.4em,font= , , [ Data Synthesis and Augmentation for Large Language Models: A Survey, ver, color=bounding!100, fill=0!15, text=black, font=, text width=32.0em, text centered [ Taxonomy ( 2 ), color=1!100, fill=1!80, text=black [ Data Augmentation   ( 2.1 ), color=1!100, fill=1!65, text=black [ Data Labeling, color=1!100, fill=1_1!55, text=black [  T-SciQ  (Wang et al . ,  2024d ) , ChatGPT-based  (Zhu et al . ,  2023b ; Gilardi et al . ,  2023 ; Alizadeh et al . ,  2023 ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] [ Data Reformation, color=1!100, fill=1_1!55, text=black [  Mosaic (Jocher,  2020 ) , CORE  (Dixit et al . ,  2022 ) , ALIA   (Dunlap et al . ,  2023 ) , ChatAug   (Dai et al . ,  2023a ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] [ Co-Annotation, color=1!100, fill=1_1!55, text=black [  Co-annotating  (Li et al . ,  2023d ) , ToolCoder  (Zhang et al . ,  2023e ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] ] [ Data Synthesis ( 2.2 ), color=1!100, fill=1!65, text=black [ General Model   Distillation, color=1!100, fill=1_1!55, text=black [  TinyStories (Eldan and Li,  2023 ) , Phi-1 (Gunasekar et al . ,  2023 ; Li et al . ,  2023b ) , Alpagasus  (Chen et al . ,  2023b ) , WizardLM  (Xu et al . ,  2023b ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] [ Domain Model   Distillation, color=1!100, fill=1_1!55, text=black [  Minerva (Lewkowycz et al . ,  2022 ) , DeepSeek-Prover (Xin et al . ,  2024 ) , WizardCoder (Luo et al . ,  2024 ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=1!100, fill=1_1!55, text=black [  Rephrasing (Maini et al . ,  2024 ) , Self-instruct (Wang et al . ,  2023b ) , SPIN  (Chen et al . ,  2024a ) , SelTDA  (Khan et al . ,  2023 ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] ] ] [ Full Lifecycle   of LM ( 3 ), color=2!100, fill=2!85, text=black [ Data Preparation   ( 3.1 ), color=orange!70, fill=orange!35, text=black [ General Model Distillation, color=orange!70, fill=orange!18, text=black [  Dialogic  (Li et al . ,  2022a ) , MathInstruct  (Yue et al . ,  2023b ) , Genixer  (Zhao et al . ,  2023a ) , Magpie  (Xu et al . ,  2024b ) ,   MMIQC  (Liu et al . ,  2024f ) , Genie  (Yehudai et al . ,  2024 ) , Case2Code  (Shao et al . ,  2024a ) , UltraChat  (Ding et al . ,  2023 ) , color=bounding!70, fill=orange!7, text=black, text width=24.0em ] ] [ Data Augmentation, color=orange!70, fill=orange!18, text=black [  Disco  (Chen et al . ,  2022 ) , GPT3Mix  (Yoo et al . ,  2021 ) , CoAnnotating  (Li et al . ,  2023d ) , ALIA  (Dunlap et al . ,  2023 ) ,   FullAnno  (Hao et al . ,  2024 ) , Dialgen  (Lu et al . ,  2023 ) , TinyGSM  (Liu et al . ,  2023a ) , AMPS  (Hendrycks et al . ,  2021 ) , color=bounding!70, fill=orange!7, text=black, text width=24.0em ] ] ] [ Pretraining ( 3.2 ), color=blue!40, fill=blue!20, text=black [ General Model Distillation, color=blue!40, fill=blue!10, text=black [  Phi-1  (Gunasekar et al . ,  2023 ) , SciLitLLM  (Li et al . ,  2024c ) , TRAIT  (Liang et al . ,  2024 ) , AnyGPT  (Zhan et al . ,  2024 ) ,   Phi-1.5  (Li et al . ,  2023b ) , TinyDialogues  (Feng et al . ,  2024 ) , color=bounding!70, fill=blue!5, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=blue!40, fill=blue!10, text=black [  VILA-2  (Fang et al . ,  2024 ) , color=bounding!70, fill=blue!5, text=black, text width=24.0em ] ] [ Data Augmentation, color=blue!40, fill=blue!10, text=black [  WRAP  (Maini et al . ,  2024 ) , KMLM  (Liu et al . ,  2021 ) , bioR  (Zhu and Li,  2023 ) , Physics-based  (Liu et al . ,  2024a ) , color=bounding!70, fill=blue!5, text=black, text width=24.0em ] ] ] [ Finetuning ( 3.3 ), color=purple!60, fill=purple!20, text=black [ General Model Distillation, color=purple!60, fill=purple!10, text=black [  LAB  (Sudalairaj et al . ,  2024 ) , LLM2LLM  (Lee et al . ,  2024 ) , GLAN  (Li et al . ,  2024b ) , Clingen  (Xu et al . ,  2024a ) ,   Baize  (Xu et al . ,  2023a ) , Evol-Instruct  (Xu et al . ,  2023b ) , HuaTuo  (Wang et al . ,  2023c ) , NExT-GPT  (Wu et al . ,  2023a ) , color=bounding!70, fill=purple!5, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=purple!60, fill=purple!10, text=black [  STaR  (Zelikman et al . ,  2022 ) , REST  (Gulcehre et al . ,  2023 ) , Self-Translate  (Ri et al . ,  2024 ) , Self-Instruct  (Wang et al . ,  2023b ) ,   RFT  (Yuan et al . ,  2023 ) , CodeRL  (Le et al . ,  2022 ) , REST-EM  (Singh et al . ,  2023 ) , DeepSeekProver  (Xin et al . ,  2024 ) , color=bounding!70, fill=purple!5, text=black, text width=24.0em ] ] [ Data Augmentation, color=purple!60, fill=purple!10, text=black [  MathGenie  (Lu et al . ,  2024a ) , DISC-MedLLM  (Bao et al . ,  2023 ) , MetaMath  (Yu et al . ,  2024 ) ,   Symbol tuning  (Wei et al . ,  2023a ) , Llama-3-UltraMedical  (Zhang et al . ,  2024g ) , Llemma  (Azerbayev et al . ,  2023 ) , color=bounding!70, fill=purple!5, text=black, text width=24.0em ] ] ] [ Instruction-Tuning  ( 3.4 ), color=BlueGreen!150, fill=BlueGreen!90, text=black [ General Model Distillation, color=BlueGreen!120, fill=BlueGreen!50, text=black [  Alpaca  (Taori et al . ,  2023 ) , Vicuna  (Chiang et al . ,  2023 ) , Orca   (Mukherjee et al . ,  2023 ) ,Baize  (Xu et al . ,  2023a ) , LLaVA  (Liu et al . ,  2024c ) , color=bounding!70, fill=BlueGreen!30, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=BlueGreen!120, fill=BlueGreen!50, text=black [  Self-Instruct  (Wang et al . ,  2023b ) , SPIN  (Chen et al . ,  2024a ) , CAI  (Bai et al . ,  2022b ) , Toolformer  (Schick et al . ,  2024 ) , color=bounding!70, fill=BlueGreen!30, text=black, text width=24.0em ] ] [ Data Augmentation, color=BlueGreen!120, fill=BlueGreen!50, text=black [  T-SciQ  (Wang et al . ,  2024d ) , CORE  (Dixit et al . ,  2022 ) , ChatAug   (Dai et al . ,  2023a ) , ToolCoder  (Zhang et al . ,  2023e ) , color=bounding!70, fill=BlueGreen!30, text=black, text width=24.0em ] ] ] [ Preference   Alignment( 3.5 ) , color=BlueViolet!60, fill=BlueViolet!25, text=black [ General Model Distillation, color=BlueViolet!60, fill=BlueViolet!15, text=black [  ULTRAFEEDBACK  (Cui et al . ,  2023b ) , HelpSteer  (Wang et al . ,  2023a ) , LEMA  (An et al . ,  2023 ) ,color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em ] ] [ Domain Model Distillation, color=BlueViolet!60, fill=BlueViolet!15, text=black [  BAD   (Xu et al . ,  2021 ) , BEAVERTAILS  (Ji et al . ,  2024 ) , PRM800K  (Lightman et al . ,  2023 ) , WebGPT  (Nakano et al . ,  2021 ) ,color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=BlueViolet!60, fill=BlueViolet!15, text=black [  OAIF  (Guo et al . ,  2024 ) , SELF-JUDGE  (Ye and Ng,  2024 ) , SALMON  (Sun et al . ,  2024b ) , SteerLM  (Dong et al . ,  2023 ) , color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em ] ] [ Data Augmentation, color=BlueViolet!60, fill=BlueViolet!15, text=black [   Starling-7B  (Zhu et al . ,  2023a ) , UltraInteract   (Yuan et al . ,  2024a ) , CriticBench  (Lin et al . ,  2024a ) , color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em ] ] ] [ Applications ( 3.6 ), color=red!40, fill=red!25, text=black [ Math, color=4!80, fill=4_1!70, text=black [  MetaMath  (Yu et al . ,  2024 ) , MammoTH  (Yue et al . ,  2023b ) , STaR  (Zelikman et al . ,  2022 ) , Galactia  (Taylor et al . ,  2022 ) ,    DeepSeekProver  (Xin et al . ,  2024 ) , WizardMath  (Luo et al . ,  2023 ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] [ Science, color=4!80, fill=4_1!70, text=black [  SciLitLLM  (Li et al . ,  2024c ) , ChemLLM  (Zhang et al . ,  2024f ) , SciGLM  (Zhang et al . ,  2024d ) , Galactia  (Taylor et al . ,  2022 ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] [ Code, color=4!80, fill=4_1!70, text=black [  WizardCoder  (Luo et al . ,  2024 ) , MagicCoder  (Wei et al . ,  2024 ) , Code Alpaca  (Chaudhary,  2023 ) ,    Code LLama  (Roziere et al . ,  2023 ) , Phi-1  (Gunasekar et al . ,  2023 ) , Phi-1.5  (Li et al . ,  2023b ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] [ Medical, color=4!80, fill=4_1!70, text=black [  DISC-MedLLM  (Bao et al . ,  2023 ) , HuatuoGPT  (Zhang et al . ,  2023a ; Chen et al . ,  1 16 ) , ChatCounselor  (Liu et al . ,  9 27 ) ,   ClinGen  (Xu et al . ,  2024a ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] [ Law, color=4!80, fill=4_1!70, text=black [  DISC-LawLLM  (Yue et al . ,  2023a ) , LawyerLLaMA  (Huang et al . ,  2023 ) , LawGPT  (Zhou et al . ,  2024b ) ,    WisdomInterrogatory  (zhihaiLLM,  847Z ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] ] ] [ Functionality ( 4 ), color=SlateBlue!80, fill=SlateBlue!40, text=black [ Understanding ( 4.1 ), color=brown!60, fill=brown!30, text=black [  Alpaca  (Taori et al . ,  2023 ) , WizardLM  (Xu et al . ,  2023b ) , WRAP (Maini et al . ,  2024 ) , LLaVA  (Liu et al . ,  2024c ) , ChartLlama (Han et al . ,  2023 ) ,Genixer (Zhao et al . ,  2023a ) , color=bounding!70, fill=brown!10, text=black, text width=35.8em ] ] [ Logic ( 4.2 ), color=2_2_2!100, fill=2_2_2!60, text=black [  ReST EM   (Singh et al . ,  2023 ) , Case2Code (Shao et al . ,  2024a ) , MathInstruct (Yue et al . ,  2023b ) , MMIQC (Liu et al . ,  2024f ) , STaR (Zelikman et al . ,  2022 ) ,SelTDA  (Khan et al . ,  2023 ) , color=bounding!70, fill=2_2_2!20, text=black, text width=35.8em ] ] [ Memory ( 4.3 ), color=LightGreen!100, fill=LightGreen!40, text=black [  Quiet-STaR  (Zelikman et al . ,  2024 ) , AutoKG  (Zhu et al . ,  2024 ) , Persona Hub  (Chan et al . ,  2024 ) , AceCoder  (Li et al . ,  2023g ) , RepoCoder  (Zhang et al . ,  2023c ) , color=bounding!70, fill=LightGreen!10, text=black, text width=35.8em ] ] [ Generation ( 4.4 ), color=Turquoise!80, fill=Turquoise!30, text=black [  Genie (Yehudai et al . ,  2024 ) , UltraMedical (Zhang et al . ,  2024g ) , HuaTuo (Wang et al . ,  2023c ) , TinyStories (Eldan and Li,  2023 ) , DIALOGIC (Li et al . ,  2022a ) , ALIA  (Dunlap et al . ,  2023 ) , color=bounding!70, fill=Turquoise!10, text=black, text width=35.8em ] ] ] [ Challenges and   Limitations ( 5 ), color=6!100, fill=6!50, text=black [ Synthesizing and   Augmenting Method   ( 5.1 ), color=6!100, fill=6!35, text=black [  d-RLAIF (Lee et al . ,  2023 ) , LLM2LLM (Lee et al . ,  2024 ) , Wizardmath (Luo et al . ,  2023 ) , STaR (Zelikman et al . ,  2022 ) , SciGLM (Zhang et al . ,  2024d ) , ChemLLM (Zhang et al . ,  2024f )  , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] [ Data Quality ( 5.2 ), color=6!100, fill=6!35, text=black [   LLMs4Synthesis (Giglou et al . ,  2024 ) , CoRAL (Wu et al . ,  2024a ) , FORD (Xiong et al . ,  2023 ) ,LTGC (Zhao et al . ,  2024a ) , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] [ Impact of Data   Synthesis and   Augmentation ( 5.3 ), color=6!100, fill=6!35, text=black [   DataDreamer (Patel et al . ,  2024 ) ,HARMONIC (Wang et al . ,  2024c ) , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] [ Impact on Different   Applications and   Tasks ( LABEL:sec:Impact_on_Different_Applications_and_Tasks ), color=6!100, fill=6!35, text=black [  PANDA (Liu et al . ,  2024e ) ,REGA (Wang et al . ,  2024e ) , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] [ Future Directions   ( 5.5 ), color=6!100, fill=6!35, text=black [  TabSynDex (Chundawat et al . ,  2022 ) ,CoLa-Diff (Jiang et al . ,  2023b ) ,WizardCoder (Luo et al . ,  2024 ) , WebGPT (Nakano et al . ,  2021 ) , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] ] ]"
        ]
    },
    "id_table_4": {
        "caption": "Table 4 .  Data synthesis and augmentation in Instruction-Tuning. A dash (-) indicates no relevant content",
        "table": "S3.T4.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "for tree= grow=east, reversed=true, anchor=base west, parent anchor=east, child anchor=west, base=left, font=, rectangle, draw=hidden-draw, rounded corners, align=left, minimum width=4em, edge+=darkgray, line width=1pt, s sep=3pt, inner xsep=2pt, inner ysep=3pt, edge path= [draw,  \\forestoption edge] (!u.parent anchor)  ++(1.5mm,0) - (.child anchor)  \\forestoption edge label;, ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center, , where level=1text width=7.2em,font= , , where level=2text width=8.0em,font= , , where level=3text width=10.0em,font= , , where level=4text width=9.0em,font= , , where level=5text width=6.4em,font= , , [ Data Synthesis and Augmentation for Large Language Models: A Survey, ver, color=bounding!100, fill=0!15, text=black, font=, text width=32.0em, text centered [ Taxonomy ( 2 ), color=1!100, fill=1!80, text=black [ Data Augmentation   ( 2.1 ), color=1!100, fill=1!65, text=black [ Data Labeling, color=1!100, fill=1_1!55, text=black [  T-SciQ  (Wang et al . ,  2024d ) , ChatGPT-based  (Zhu et al . ,  2023b ; Gilardi et al . ,  2023 ; Alizadeh et al . ,  2023 ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] [ Data Reformation, color=1!100, fill=1_1!55, text=black [  Mosaic (Jocher,  2020 ) , CORE  (Dixit et al . ,  2022 ) , ALIA   (Dunlap et al . ,  2023 ) , ChatAug   (Dai et al . ,  2023a ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] [ Co-Annotation, color=1!100, fill=1_1!55, text=black [  Co-annotating  (Li et al . ,  2023d ) , ToolCoder  (Zhang et al . ,  2023e ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] ] [ Data Synthesis ( 2.2 ), color=1!100, fill=1!65, text=black [ General Model   Distillation, color=1!100, fill=1_1!55, text=black [  TinyStories (Eldan and Li,  2023 ) , Phi-1 (Gunasekar et al . ,  2023 ; Li et al . ,  2023b ) , Alpagasus  (Chen et al . ,  2023b ) , WizardLM  (Xu et al . ,  2023b ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] [ Domain Model   Distillation, color=1!100, fill=1_1!55, text=black [  Minerva (Lewkowycz et al . ,  2022 ) , DeepSeek-Prover (Xin et al . ,  2024 ) , WizardCoder (Luo et al . ,  2024 ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=1!100, fill=1_1!55, text=black [  Rephrasing (Maini et al . ,  2024 ) , Self-instruct (Wang et al . ,  2023b ) , SPIN  (Chen et al . ,  2024a ) , SelTDA  (Khan et al . ,  2023 ) , color=bounding!70, fill=1_1!20, text=black, text width=24.0em ] ] ] ] [ Full Lifecycle   of LM ( 3 ), color=2!100, fill=2!85, text=black [ Data Preparation   ( 3.1 ), color=orange!70, fill=orange!35, text=black [ General Model Distillation, color=orange!70, fill=orange!18, text=black [  Dialogic  (Li et al . ,  2022a ) , MathInstruct  (Yue et al . ,  2023b ) , Genixer  (Zhao et al . ,  2023a ) , Magpie  (Xu et al . ,  2024b ) ,   MMIQC  (Liu et al . ,  2024f ) , Genie  (Yehudai et al . ,  2024 ) , Case2Code  (Shao et al . ,  2024a ) , UltraChat  (Ding et al . ,  2023 ) , color=bounding!70, fill=orange!7, text=black, text width=24.0em ] ] [ Data Augmentation, color=orange!70, fill=orange!18, text=black [  Disco  (Chen et al . ,  2022 ) , GPT3Mix  (Yoo et al . ,  2021 ) , CoAnnotating  (Li et al . ,  2023d ) , ALIA  (Dunlap et al . ,  2023 ) ,   FullAnno  (Hao et al . ,  2024 ) , Dialgen  (Lu et al . ,  2023 ) , TinyGSM  (Liu et al . ,  2023a ) , AMPS  (Hendrycks et al . ,  2021 ) , color=bounding!70, fill=orange!7, text=black, text width=24.0em ] ] ] [ Pretraining ( 3.2 ), color=blue!40, fill=blue!20, text=black [ General Model Distillation, color=blue!40, fill=blue!10, text=black [  Phi-1  (Gunasekar et al . ,  2023 ) , SciLitLLM  (Li et al . ,  2024c ) , TRAIT  (Liang et al . ,  2024 ) , AnyGPT  (Zhan et al . ,  2024 ) ,   Phi-1.5  (Li et al . ,  2023b ) , TinyDialogues  (Feng et al . ,  2024 ) , color=bounding!70, fill=blue!5, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=blue!40, fill=blue!10, text=black [  VILA-2  (Fang et al . ,  2024 ) , color=bounding!70, fill=blue!5, text=black, text width=24.0em ] ] [ Data Augmentation, color=blue!40, fill=blue!10, text=black [  WRAP  (Maini et al . ,  2024 ) , KMLM  (Liu et al . ,  2021 ) , bioR  (Zhu and Li,  2023 ) , Physics-based  (Liu et al . ,  2024a ) , color=bounding!70, fill=blue!5, text=black, text width=24.0em ] ] ] [ Finetuning ( 3.3 ), color=purple!60, fill=purple!20, text=black [ General Model Distillation, color=purple!60, fill=purple!10, text=black [  LAB  (Sudalairaj et al . ,  2024 ) , LLM2LLM  (Lee et al . ,  2024 ) , GLAN  (Li et al . ,  2024b ) , Clingen  (Xu et al . ,  2024a ) ,   Baize  (Xu et al . ,  2023a ) , Evol-Instruct  (Xu et al . ,  2023b ) , HuaTuo  (Wang et al . ,  2023c ) , NExT-GPT  (Wu et al . ,  2023a ) , color=bounding!70, fill=purple!5, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=purple!60, fill=purple!10, text=black [  STaR  (Zelikman et al . ,  2022 ) , REST  (Gulcehre et al . ,  2023 ) , Self-Translate  (Ri et al . ,  2024 ) , Self-Instruct  (Wang et al . ,  2023b ) ,   RFT  (Yuan et al . ,  2023 ) , CodeRL  (Le et al . ,  2022 ) , REST-EM  (Singh et al . ,  2023 ) , DeepSeekProver  (Xin et al . ,  2024 ) , color=bounding!70, fill=purple!5, text=black, text width=24.0em ] ] [ Data Augmentation, color=purple!60, fill=purple!10, text=black [  MathGenie  (Lu et al . ,  2024a ) , DISC-MedLLM  (Bao et al . ,  2023 ) , MetaMath  (Yu et al . ,  2024 ) ,   Symbol tuning  (Wei et al . ,  2023a ) , Llama-3-UltraMedical  (Zhang et al . ,  2024g ) , Llemma  (Azerbayev et al . ,  2023 ) , color=bounding!70, fill=purple!5, text=black, text width=24.0em ] ] ] [ Instruction-Tuning  ( 3.4 ), color=BlueGreen!150, fill=BlueGreen!90, text=black [ General Model Distillation, color=BlueGreen!120, fill=BlueGreen!50, text=black [  Alpaca  (Taori et al . ,  2023 ) , Vicuna  (Chiang et al . ,  2023 ) , Orca   (Mukherjee et al . ,  2023 ) ,Baize  (Xu et al . ,  2023a ) , LLaVA  (Liu et al . ,  2024c ) , color=bounding!70, fill=BlueGreen!30, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=BlueGreen!120, fill=BlueGreen!50, text=black [  Self-Instruct  (Wang et al . ,  2023b ) , SPIN  (Chen et al . ,  2024a ) , CAI  (Bai et al . ,  2022b ) , Toolformer  (Schick et al . ,  2024 ) , color=bounding!70, fill=BlueGreen!30, text=black, text width=24.0em ] ] [ Data Augmentation, color=BlueGreen!120, fill=BlueGreen!50, text=black [  T-SciQ  (Wang et al . ,  2024d ) , CORE  (Dixit et al . ,  2022 ) , ChatAug   (Dai et al . ,  2023a ) , ToolCoder  (Zhang et al . ,  2023e ) , color=bounding!70, fill=BlueGreen!30, text=black, text width=24.0em ] ] ] [ Preference   Alignment( 3.5 ) , color=BlueViolet!60, fill=BlueViolet!25, text=black [ General Model Distillation, color=BlueViolet!60, fill=BlueViolet!15, text=black [  ULTRAFEEDBACK  (Cui et al . ,  2023b ) , HelpSteer  (Wang et al . ,  2023a ) , LEMA  (An et al . ,  2023 ) ,color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em ] ] [ Domain Model Distillation, color=BlueViolet!60, fill=BlueViolet!15, text=black [  BAD   (Xu et al . ,  2021 ) , BEAVERTAILS  (Ji et al . ,  2024 ) , PRM800K  (Lightman et al . ,  2023 ) , WebGPT  (Nakano et al . ,  2021 ) ,color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em ] ] [ Model Self-Improvement, color=BlueViolet!60, fill=BlueViolet!15, text=black [  OAIF  (Guo et al . ,  2024 ) , SELF-JUDGE  (Ye and Ng,  2024 ) , SALMON  (Sun et al . ,  2024b ) , SteerLM  (Dong et al . ,  2023 ) , color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em ] ] [ Data Augmentation, color=BlueViolet!60, fill=BlueViolet!15, text=black [   Starling-7B  (Zhu et al . ,  2023a ) , UltraInteract   (Yuan et al . ,  2024a ) , CriticBench  (Lin et al . ,  2024a ) , color=bounding!70, fill=BlueViolet!7, text=black, text width=24.0em ] ] ] [ Applications ( 3.6 ), color=red!40, fill=red!25, text=black [ Math, color=4!80, fill=4_1!70, text=black [  MetaMath  (Yu et al . ,  2024 ) , MammoTH  (Yue et al . ,  2023b ) , STaR  (Zelikman et al . ,  2022 ) , Galactia  (Taylor et al . ,  2022 ) ,    DeepSeekProver  (Xin et al . ,  2024 ) , WizardMath  (Luo et al . ,  2023 ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] [ Science, color=4!80, fill=4_1!70, text=black [  SciLitLLM  (Li et al . ,  2024c ) , ChemLLM  (Zhang et al . ,  2024f ) , SciGLM  (Zhang et al . ,  2024d ) , Galactia  (Taylor et al . ,  2022 ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] [ Code, color=4!80, fill=4_1!70, text=black [  WizardCoder  (Luo et al . ,  2024 ) , MagicCoder  (Wei et al . ,  2024 ) , Code Alpaca  (Chaudhary,  2023 ) ,    Code LLama  (Roziere et al . ,  2023 ) , Phi-1  (Gunasekar et al . ,  2023 ) , Phi-1.5  (Li et al . ,  2023b ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] [ Medical, color=4!80, fill=4_1!70, text=black [  DISC-MedLLM  (Bao et al . ,  2023 ) , HuatuoGPT  (Zhang et al . ,  2023a ; Chen et al . ,  1 16 ) , ChatCounselor  (Liu et al . ,  9 27 ) ,   ClinGen  (Xu et al . ,  2024a ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] [ Law, color=4!80, fill=4_1!70, text=black [  DISC-LawLLM  (Yue et al . ,  2023a ) , LawyerLLaMA  (Huang et al . ,  2023 ) , LawGPT  (Zhou et al . ,  2024b ) ,    WisdomInterrogatory  (zhihaiLLM,  847Z ) , color=bounding!70, fill=4_1_1!20, text=black, text width=24.0em ] ] ] ] [ Functionality ( 4 ), color=SlateBlue!80, fill=SlateBlue!40, text=black [ Understanding ( 4.1 ), color=brown!60, fill=brown!30, text=black [  Alpaca  (Taori et al . ,  2023 ) , WizardLM  (Xu et al . ,  2023b ) , WRAP (Maini et al . ,  2024 ) , LLaVA  (Liu et al . ,  2024c ) , ChartLlama (Han et al . ,  2023 ) ,Genixer (Zhao et al . ,  2023a ) , color=bounding!70, fill=brown!10, text=black, text width=35.8em ] ] [ Logic ( 4.2 ), color=2_2_2!100, fill=2_2_2!60, text=black [  ReST EM   (Singh et al . ,  2023 ) , Case2Code (Shao et al . ,  2024a ) , MathInstruct (Yue et al . ,  2023b ) , MMIQC (Liu et al . ,  2024f ) , STaR (Zelikman et al . ,  2022 ) ,SelTDA  (Khan et al . ,  2023 ) , color=bounding!70, fill=2_2_2!20, text=black, text width=35.8em ] ] [ Memory ( 4.3 ), color=LightGreen!100, fill=LightGreen!40, text=black [  Quiet-STaR  (Zelikman et al . ,  2024 ) , AutoKG  (Zhu et al . ,  2024 ) , Persona Hub  (Chan et al . ,  2024 ) , AceCoder  (Li et al . ,  2023g ) , RepoCoder  (Zhang et al . ,  2023c ) , color=bounding!70, fill=LightGreen!10, text=black, text width=35.8em ] ] [ Generation ( 4.4 ), color=Turquoise!80, fill=Turquoise!30, text=black [  Genie (Yehudai et al . ,  2024 ) , UltraMedical (Zhang et al . ,  2024g ) , HuaTuo (Wang et al . ,  2023c ) , TinyStories (Eldan and Li,  2023 ) , DIALOGIC (Li et al . ,  2022a ) , ALIA  (Dunlap et al . ,  2023 ) , color=bounding!70, fill=Turquoise!10, text=black, text width=35.8em ] ] ] [ Challenges and   Limitations ( 5 ), color=6!100, fill=6!50, text=black [ Synthesizing and   Augmenting Method   ( 5.1 ), color=6!100, fill=6!35, text=black [  d-RLAIF (Lee et al . ,  2023 ) , LLM2LLM (Lee et al . ,  2024 ) , Wizardmath (Luo et al . ,  2023 ) , STaR (Zelikman et al . ,  2022 ) , SciGLM (Zhang et al . ,  2024d ) , ChemLLM (Zhang et al . ,  2024f )  , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] [ Data Quality ( 5.2 ), color=6!100, fill=6!35, text=black [   LLMs4Synthesis (Giglou et al . ,  2024 ) , CoRAL (Wu et al . ,  2024a ) , FORD (Xiong et al . ,  2023 ) ,LTGC (Zhao et al . ,  2024a ) , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] [ Impact of Data   Synthesis and   Augmentation ( 5.3 ), color=6!100, fill=6!35, text=black [   DataDreamer (Patel et al . ,  2024 ) ,HARMONIC (Wang et al . ,  2024c ) , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] [ Impact on Different   Applications and   Tasks ( LABEL:sec:Impact_on_Different_Applications_and_Tasks ), color=6!100, fill=6!35, text=black [  PANDA (Liu et al . ,  2024e ) ,REGA (Wang et al . ,  2024e ) , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] [ Future Directions   ( 5.5 ), color=6!100, fill=6!35, text=black [  TabSynDex (Chundawat et al . ,  2022 ) ,CoLa-Diff (Jiang et al . ,  2023b ) ,WizardCoder (Luo et al . ,  2024 ) , WebGPT (Nakano et al . ,  2021 ) , color=bounding!70, fill=6!20, text=black, text width=35.8em ] ] ] ]",
            "Data generation methods play a pivotal role in addressing data scarcity and imbalance, thereby improving model performance and generalization. As shown in Fig.  4 , we summarize the development and evolution of data augmentation and synthesis techniques in recent years. This section primarily introduces the current classification of data generation methods, distinguishing between  data augmentation , which enhances existing data samples through transformations, and  data synthesis , which creates entirely new samples from scratch or based on generative models. Both methods differ in their approach to acquiring data but aim to expand datasets. Furthermore, data augmentation and synthesis methods can be categorized into subclasses from multiple dimensions. Each approach has unique strengths and applications, enabling researchers to tailor their data generation strategies to specific needs and goals.",
            "In the instruction tuning phase, data synthetic aims at exploring synthetic instruction or prompt contents to generate instruction-following high-quality data via LLMs. According to the way of synthetic data, they consist of the following categories: (1) general model distillation, (2) model self-improvement, and (3) data augmentation, as shown in Table  4 ."
        ]
    }
}