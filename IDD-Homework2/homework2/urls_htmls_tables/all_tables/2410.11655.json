{
    "id_table_1": {
        "caption": "Table 1:  Results for zero-shot LLMs. The largest models achieve the best results, and Mistral-7B achieves excellent results for its size.   The size given for Claude-3-sonnet is a guess based on the comparative performance of the model relative to others of known size.",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "Table  1  reports baseline results for a range of different LLMs used without any retrieval. The prompt template used for this is the same one as in Section  3.3  without the  context  field and its mentions in the  Instruction  section. The top-performing model by a large margin is Mixtral-47B, followed by Claude-3-sonnet (  \\approx  70B, estimated). The much smaller Mistral-7B model  Jiang et al. ( 2023 )  is reasonably competitive, though, and it represents a better balance of costs and performance, especially for high-volume services like spelling correction.",
            "The overall best-performing setting is with ColBERT indexing 572K documents and providing retrieval results to Claude-3-sonnet. This configuration results in 39.3 F1 vs. 34.7 for Clause-3-sonnet used without retrieval (Table  1 ): a 4.6 point increase. Strikingly, the next best system is one that uses Mistral-7B, again with ColBERT indexing the larger document collection. This configuration achieves 35.9 vs. 28.1 for Mistral-7B used without retrieval: a 7.8 point increase.",
            "Table  4  summarizes our experiments involving LLM fine-tuning, using both Basic and Contextual variants of this method (Section  3.4 ). For these experiments, we adopt Mistral-7B as our base LLM. To facilitate comparisons, the top row in the first section of the table is repeated from Table  1 , and the top rows from the middle and bottom sections are repeated from Table  2 . We include the precision/recall breakdown here to support further analysis of the trade-offs."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Results for RAG with frozen LLMs. The best configurations use ColBERT and a larger document index. Both LLMs are substantially improved by RAG.",
        "table": "S4.T2.1.1",
        "footnotes": [],
        "references": [
            "Table  2  provides our primary results. We consider Mistral-7B and Clause-3-sonnet as the base LLMs. For each LLM, we evaluate our three different retrieval models. We also explore the role of the size of the product catalog by considering two different pools of documents: one with 572K documents (132K unique brands) and one with 60K documents (29K unique brands).",
            "Table  4  summarizes our experiments involving LLM fine-tuning, using both Basic and Contextual variants of this method (Section  3.4 ). For these experiments, we adopt Mistral-7B as our base LLM. To facilitate comparisons, the top row in the first section of the table is repeated from Table  1 , and the top rows from the middle and bottom sections are repeated from Table  2 . We include the precision/recall breakdown here to support further analysis of the trade-offs.",
            "In this paper, we introduced a fine-tuned Retrieval Augmented Generation (RAG) framework tailored for e-commerce spelling correction, specifically addressing the complexities posed by brand names and other non-standard lexicons. We showed that this approach is highly effective even with a frozen retriever and frozen large language model (Table  2 ). In addition, we showed that fine-tuning the LLM with retrieved context leads to even larger gains (Table  4 ), particularly for spelling corrections involving evolving brand names (Table  5 ). These results underscore the value of incorporating retrieval and allowing the model to dynamically adapt to context in a way that standalone LLMs or RAG with frozen components cannot achieve."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Qualitative evaluation of RAG-generated spelling corrections across various examples. In 14, correctly spelled retrieved items lead to accurate corrections. In 56, the retrieved items do not involve misspelled spans of the input query, leading RAG generation to rely on the LLMs internal knowledge. In 7, the LLM generates the correct spelling despite misspelled retrieved items, while in 8, the model is misled by the incorrect retrieval. Finally, in 9, la roche and laroche are both real brands. The retriever does not correctly consider the context b5 to distinguish the brands (b5 is a specific item that is only associated with brand la roche).",
        "table": "S4.T3.1.1",
        "footnotes": [],
        "references": [
            "We experiment primarily with  Mistral-7B   Jiang et al. ( 2023 )  (specifically,  open-mistral-7b  v0.1) and Claude-3-sonnet ( claude-3-sonnet-20240229  v1:0). We chose Mistral-7B because it is highly effective for its size (see Section  4.3 ), and we chose Claude-3-sonnet as a representative of a much larger class of LLMs. We would expect to see similar results for other LLMs, even larger and more capable ones, because our central challenge is making accurate predictions about novel brand names.",
            "For  Basic Fine-Tuning , the training dataset consists of 50K  <input query, label query>  pairs derived from search logs to create a training dataset. Here,  label query  reflects user-validated corrections. For this fine-tuning, we remove the referring to the provided context if its relevant wording from  Instruction  of the prompt in Section  3.3 , with the  context  field also removed and the desired output appended to the end followed by  ###  on a newline.",
            "For  Contextual Fine-Tuning , we begin with the same dataset of 50K pairs, but now we retrieve context for each  <input query> , forming  <input query, context, label query>  triples. This added context, derived through the same retrieval mechanism used in RAG, helps teach the model how to make use of the  context  field. The prompt has the same format as the one in Section  3.3  but with the desired output appended to the end followed by  ###  on a newline.",
            "Table  1  reports baseline results for a range of different LLMs used without any retrieval. The prompt template used for this is the same one as in Section  3.3  without the  context  field and its mentions in the  Instruction  section. The top-performing model by a large margin is Mixtral-47B, followed by Claude-3-sonnet (  \\approx  70B, estimated). The much smaller Mistral-7B model  Jiang et al. ( 2023 )  is reasonably competitive, though, and it represents a better balance of costs and performance, especially for high-volume services like spelling correction.",
            "Table  3  provides a more qualitative analysis that reveals nuanced interactions between retrieved context and LLM responses. When relevant context is retrieved (Examples 14), the LLM provides accurate corrections aligned with expected outputs. Conversely, in instances where context is absent (Examples 56), incorrect (Examples 78), or misleading (Example 9), the LLMs performance varied, highlighting the complex balance between the LLMs parameterized knowledge and the information retrieved. These examples illustrate that, while retrievers enrich context, they can also introduce noise or irrelevant data that might detract from accuracy if not carefully managed.",
            "Table  4  summarizes our experiments involving LLM fine-tuning, using both Basic and Contextual variants of this method (Section  3.4 ). For these experiments, we adopt Mistral-7B as our base LLM. To facilitate comparisons, the top row in the first section of the table is repeated from Table  1 , and the top rows from the middle and bottom sections are repeated from Table  2 . We include the precision/recall breakdown here to support further analysis of the trade-offs.",
            "Our primary goal is to improve performance on brands that are novel from the perspective of the LLM. Table  5  seeks to quantify the extent to which Contextual Fine-Tuning marks progress in this area, as compared to Basic Fine-Tuning. In both cases, we train on the same set of examples (Section  3.4 ). The table shows that Contextual Fine-Tuning leads to a 6.9 point increase in overall F1 and a 16.7 point increase in queries containing brands. While brands remain very challenging, our approach certainly alleviates the challenge.",
            "Our qualitative analysis further revealed challenges inherent in using real-world data, such as the presence of misspellings in indexed documents, which can mislead the LLM during generation (Table  3 ). This highlights the importance of ensuring the quality of retrieved contexts. Practical improvements include refining the contextual data through heuristic signals, like user interactions and engagement metrics, to enhance relevance and accuracy. Another promising avenue is to diversify the styles and noise levels within the retrieved context to bolster the models robustness."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Performance comparison across different retrieval configurations and fine-tuning setups. All experiments used an indexed pool of 572K documents (132K unique brands). The highest F1 score of 71.0 was achieved with ColBERT in RAG, demonstrating the added benefit of Contextual Fine-Tuning. These results indicate that fine-tuning with context-specific instructions is extremely effective.",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": [
            "We experiment primarily with  Mistral-7B   Jiang et al. ( 2023 )  (specifically,  open-mistral-7b  v0.1) and Claude-3-sonnet ( claude-3-sonnet-20240229  v1:0). We chose Mistral-7B because it is highly effective for its size (see Section  4.3 ), and we chose Claude-3-sonnet as a representative of a much larger class of LLMs. We would expect to see similar results for other LLMs, even larger and more capable ones, because our central challenge is making accurate predictions about novel brand names.",
            "For both variants, the fine-tuning process is simply additional language model training using strings formatted from our prompt templates. Further evaluation details, including metrics, are covered in Section  4 .",
            "Table  4  summarizes our experiments involving LLM fine-tuning, using both Basic and Contextual variants of this method (Section  3.4 ). For these experiments, we adopt Mistral-7B as our base LLM. To facilitate comparisons, the top row in the first section of the table is repeated from Table  1 , and the top rows from the middle and bottom sections are repeated from Table  2 . We include the precision/recall breakdown here to support further analysis of the trade-offs.",
            "Our primary goal is to improve performance on brands that are novel from the perspective of the LLM. Table  5  seeks to quantify the extent to which Contextual Fine-Tuning marks progress in this area, as compared to Basic Fine-Tuning. In both cases, we train on the same set of examples (Section  3.4 ). The table shows that Contextual Fine-Tuning leads to a 6.9 point increase in overall F1 and a 16.7 point increase in queries containing brands. While brands remain very challenging, our approach certainly alleviates the challenge.",
            "In this paper, we introduced a fine-tuned Retrieval Augmented Generation (RAG) framework tailored for e-commerce spelling correction, specifically addressing the complexities posed by brand names and other non-standard lexicons. We showed that this approach is highly effective even with a frozen retriever and frozen large language model (Table  2 ). In addition, we showed that fine-tuning the LLM with retrieved context leads to even larger gains (Table  4 ), particularly for spelling corrections involving evolving brand names (Table  5 ). These results underscore the value of incorporating retrieval and allowing the model to dynamically adapt to context in a way that standalone LLMs or RAG with frozen components cannot achieve."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Performance improvements from Contextual Fine-Tuning and RAG as compared to Basic Fine-Tuning without RAG, broken down by overall performance and performance on queries containing brands. The LLM is Mistral-7B and the retriever used for RAG is ColBERT over the 572K document collection.",
        "table": "S4.T5.1.1",
        "footnotes": [],
        "references": [
            "Our primary goal is to improve performance on brands that are novel from the perspective of the LLM. Table  5  seeks to quantify the extent to which Contextual Fine-Tuning marks progress in this area, as compared to Basic Fine-Tuning. In both cases, we train on the same set of examples (Section  3.4 ). The table shows that Contextual Fine-Tuning leads to a 6.9 point increase in overall F1 and a 16.7 point increase in queries containing brands. While brands remain very challenging, our approach certainly alleviates the challenge.",
            "In this paper, we introduced a fine-tuned Retrieval Augmented Generation (RAG) framework tailored for e-commerce spelling correction, specifically addressing the complexities posed by brand names and other non-standard lexicons. We showed that this approach is highly effective even with a frozen retriever and frozen large language model (Table  2 ). In addition, we showed that fine-tuning the LLM with retrieved context leads to even larger gains (Table  4 ), particularly for spelling corrections involving evolving brand names (Table  5 ). These results underscore the value of incorporating retrieval and allowing the model to dynamically adapt to context in a way that standalone LLMs or RAG with frozen components cannot achieve."
        ]
    },
    "global_footnotes": []
}