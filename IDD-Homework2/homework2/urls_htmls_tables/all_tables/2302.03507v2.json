{
    "S1.T1": {
        "caption": "Table 1: An example for 3-way 2-shot text classification on the Huffpost dataset, where only two support instances are given in each of the three classes. The ground-truth label of the query instance is Class B.\nExternal\nknowledge\non class labels includes class names and related descriptive texts from Wikipedia,\nwhich are used to generate prototype vectors in Meta-SN.",
        "table": null,
        "footnotes": [],
        "references": [
            "In this paper,\nto address the issues,\nwe propose a Meta-Learning Siamese Network, namely, Meta-SN.\nInstead of estimating prototype vectors from the sampled support sets,\nwe compute these vectors by\nutilizing external knowledge on\nthe class labels (see Figure 1b),\nwhich\nincludes class names and\nrelated\ndescriptive texts\n(e.g., Wiki titles and Wiki texts) as shown in Table 1.\nThis eliminates the dependence of\nprototype vector estimation on\nthe sampled support sets\nand also the adverse impact of randomness.\nAfter that,\nwe further refine these prototype vectors with a Siamese Network.\nIn particular,\nwe map both samples and prototype vectors into a low-dimensional space,\nwhere the inter-class distance between different prototype vectors is enlarged and the intra-class distance between samples and their corresponding prototype vectors is shortened (see Figure 1c).\nFurther,\nwe learn the importance of a sample in the support set based on its average distance to the query set.\nThe closer a sample is to the query set,\nthe more important the sample is for label prediction,\nand\nthe larger the weight should be assigned.\nFinally,\nwe adopt a novel sampling strategy to construct meta-tasks which assigns higher sampling probability to the hard-to-classify samples.\nOn the one hand,\nthe closer the distance between different prototype vectors,\nthe more difficult the corresponding classes can be separated.\nOn the other hand,\nthe more distant a sample in the support set is to the prototype vector,\nthe more difficult the classification task will be.\nTherefore,\nwe give higher sampling probabilities to hard-to-classify tasks to help generalize our model.\nThe main contributions\nof the paper are summarized as follows:"
        ]
    },
    "S5.T2": {
        "caption": "Table 2: \nStatistics of datasets.",
        "table": "<table id=\"S5.T2.6\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.6.6\" class=\"ltx_tr\">\n<th id=\"S5.T2.6.6.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\">Dataset</th>\n<th id=\"S5.T2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S5.T2.2.2.2.1\" class=\"ltx_text ltx_markedasmath ltx_font_typewriter\">#</span> tokens<math id=\"S5.T2.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"/\" display=\"inline\"><semantics id=\"S5.T2.2.2.2.m2.1a\"><mo id=\"S5.T2.2.2.2.m2.1.1\" xref=\"S5.T2.2.2.2.m2.1.1.cmml\">/</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.2.2.2.m2.1b\"><divide id=\"S5.T2.2.2.2.m2.1.1.cmml\" xref=\"S5.T2.2.2.2.m2.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.2.2.2.m2.1c\">/</annotation></semantics></math>example</th>\n<th id=\"S5.T2.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S5.T2.3.3.3.1\" class=\"ltx_text ltx_markedasmath ltx_font_typewriter\">#</span> samples</th>\n<th id=\"S5.T2.4.4.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S5.T2.4.4.4.1\" class=\"ltx_text ltx_markedasmath ltx_font_typewriter\">#</span> train cls</th>\n<th id=\"S5.T2.5.5.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S5.T2.5.5.5.1\" class=\"ltx_text ltx_markedasmath ltx_font_typewriter\">#</span> val cls</th>\n<th id=\"S5.T2.6.6.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S5.T2.6.6.6.1\" class=\"ltx_text ltx_markedasmath ltx_font_typewriter\">#</span> test cls</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.6.7.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.6.7.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">HuffPost</th>\n<td id=\"S5.T2.6.7.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">11</td>\n<td id=\"S5.T2.6.7.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">36900</td>\n<td id=\"S5.T2.6.7.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">20</td>\n<td id=\"S5.T2.6.7.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">5</td>\n<td id=\"S5.T2.6.7.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">16</td>\n</tr>\n<tr id=\"S5.T2.6.8.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.6.8.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">Amazon</th>\n<td id=\"S5.T2.6.8.2.2\" class=\"ltx_td ltx_align_center\">140</td>\n<td id=\"S5.T2.6.8.2.3\" class=\"ltx_td ltx_align_center\">24000</td>\n<td id=\"S5.T2.6.8.2.4\" class=\"ltx_td ltx_align_center\">10</td>\n<td id=\"S5.T2.6.8.2.5\" class=\"ltx_td ltx_align_center\">5</td>\n<td id=\"S5.T2.6.8.2.6\" class=\"ltx_td ltx_align_center\">9</td>\n</tr>\n<tr id=\"S5.T2.6.9.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.6.9.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">Reuters</th>\n<td id=\"S5.T2.6.9.3.2\" class=\"ltx_td ltx_align_center\">168</td>\n<td id=\"S5.T2.6.9.3.3\" class=\"ltx_td ltx_align_center\">620</td>\n<td id=\"S5.T2.6.9.3.4\" class=\"ltx_td ltx_align_center\">15</td>\n<td id=\"S5.T2.6.9.3.5\" class=\"ltx_td ltx_align_center\">5</td>\n<td id=\"S5.T2.6.9.3.6\" class=\"ltx_td ltx_align_center\">11</td>\n</tr>\n<tr id=\"S5.T2.6.10.4\" class=\"ltx_tr\">\n<th id=\"S5.T2.6.10.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">20 News</th>\n<td id=\"S5.T2.6.10.4.2\" class=\"ltx_td ltx_align_center\">340</td>\n<td id=\"S5.T2.6.10.4.3\" class=\"ltx_td ltx_align_center\">18820</td>\n<td id=\"S5.T2.6.10.4.4\" class=\"ltx_td ltx_align_center\">8</td>\n<td id=\"S5.T2.6.10.4.5\" class=\"ltx_td ltx_align_center\">5</td>\n<td id=\"S5.T2.6.10.4.6\" class=\"ltx_td ltx_align_center\">7</td>\n</tr>\n<tr id=\"S5.T2.6.11.5\" class=\"ltx_tr\">\n<th id=\"S5.T2.6.11.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">RCV1</th>\n<td id=\"S5.T2.6.11.5.2\" class=\"ltx_td ltx_align_center\">372</td>\n<td id=\"S5.T2.6.11.5.3\" class=\"ltx_td ltx_align_center\">1420</td>\n<td id=\"S5.T2.6.11.5.4\" class=\"ltx_td ltx_align_center\">37</td>\n<td id=\"S5.T2.6.11.5.5\" class=\"ltx_td ltx_align_center\">10</td>\n<td id=\"S5.T2.6.11.5.6\" class=\"ltx_td ltx_align_center\">24</td>\n</tr>\n<tr id=\"S5.T2.6.12.6\" class=\"ltx_tr\">\n<th id=\"S5.T2.6.12.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\">FewRel</th>\n<td id=\"S5.T2.6.12.6.2\" class=\"ltx_td ltx_align_center ltx_border_b\">24</td>\n<td id=\"S5.T2.6.12.6.3\" class=\"ltx_td ltx_align_center ltx_border_b\">56000</td>\n<td id=\"S5.T2.6.12.6.4\" class=\"ltx_td ltx_align_center ltx_border_b\">65</td>\n<td id=\"S5.T2.6.12.6.5\" class=\"ltx_td ltx_align_center ltx_border_b\">5</td>\n<td id=\"S5.T2.6.12.6.6\" class=\"ltx_td ltx_align_center ltx_border_b\">10</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We use six benchmark datasets:\nHuffPost [28, 29],\nAmazon [14],\nReuters-21578 [23],\n20 Newsgroups [21],\nRCV1 [24]\nand\nFewRel [12].\nIn particular,\nthe first five are for text classification while the last one is for few-shot relation classification.\nStatistics of these datasets are summarized in Table 2.\nAll processed datasets and their splits are publicly available."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Mean accuracy (%) of 5-way 1-shot classification and 5-way 5-shot classification over all the datasets. We highlight the best results in bold.",
        "table": null,
        "footnotes": [],
        "references": [
            "We report the results of\n5-way 1-shot classification and 5-way 5-shot classification\nin Table 3.\nFrom the table,\nMeta-SN achieves the best results across all the datasets.\nFor example,\nin the fastText-based comparison,\nMeta-SN achieves\nan average accuracy of 69.1%percent69.169.1\\% in 1-shot classification and 85.2%percent85.285.2\\% in 5-shot classification, respectively.\nIn particular,\nit outperforms the runner-up model MLADA by\na notable 3.8%percent3.83.8\\% and 2.4%percent2.42.4\\% improvement in both cases.\nWhen compared against the PROTO model, Meta-SN leads by 27.0%percent27.027.0\\% and 34.1%percent34.134.1\\% on average in 1-shot and 5-shot classification, respectively.\nThese results clearly demonstrate that our model is very effective in improving PROTO.\nWhile\nHatt-Proto upgrades PROTO by learning the importance of labeled samples,\nit disregards the\nrandomness of the sampled support sets when computing prototype vectors and\nconstructs meta-tasks\nrandomly,\nwhich degrades its performance.\nFurther,\nin the BERT-based comparison,\nMeta-SN also outperforms ContrastNet over all the datasets.\nAll\nthese results show that\nMeta-SN,\nwhich generates prototype vectors from external knowledge,\nlearns sample weights\nand constructs hard-to-classify meta-tasks,\ncan perform reasonably well."
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Ablation study: mean accuracy (%percent\\%) of 5-way 1-shot classification and 5-way\n5-shot classification over all the datasets. We highlight the best results in bold. All the results are based on fastText.",
        "table": null,
        "footnotes": [],
        "references": [
            "We conduct an ablation study to understand the characteristics of the main components of Meta-SN.\nOne variant\nignores the randomness of the sampled support sets and\ndirectly uses the mean embedding vectors of samples in the support sets as the prototype vectors.\nWe call this variant Meta-SN-rpv (random prototype vectors).\nTo show the importance of weight learning for samples,\nwe set equal weights for all sample pairs and call this variant Meta-SN-ew (equal weights).\nAnother variant\nremoves the task sampler and constructs meta-tasks in a completely random way.\nWe call this variant Meta-SN-rts (random task sampler).\nMoreover,\nto study how external knowledge affects the classification results,\nwe only extract knowledge from label names to construct prototype vectors\nand call this variant Meta-SN-ln (label names).\nThis helps to study the model robustness towards the richness of external descriptive information.\nThe results of the ablation study are reported in Table 4.\nFrom the table,\nwe observe:\n(1)\nMeta-SN significantly outperforms\nMeta-SN-rpv in all the comparisons across datasets.\nThis shows the importance of eliminating the adverse impact induced by the randomness of sampled support sets when calculating prototype vectors.\n(2)\nMeta-SN also beats Meta-SN-ew clearly.\nFor example,\nin 5 shot classification task on Amazon, the accuracy of Meta-SN is 87.7%percent87.787.7\\% while that of Meta-SN-ew is only 83.3%percent83.383.3\\%.\nThis shows the importance of weight learning for sample pairs.\n(3) Meta-SN leads Meta-SN-rts in all the 1-shot classification and 5-shot classification tasks.\nThis is because\nMeta-SN-rts constructs meta-tasks randomly while Meta-SN focuses more on the hard-to-classify meta-tasks to boost the model training.\n(4)\nThe average performance gaps between\nMeta-SN-ln and Meta-SN on both 1-shot and 5-shot classification tasks are 0.7%percent0.70.7\\%.\nThis shows that the inclusion of external descriptive texts for class labels can bring only marginal improvement on the classification results.\nAlthough we use only\nclass names to derive the\ninitial embeddings of class prototype vectors,\nMeta-SN can progressively refine these embeddings and thus lead to superior performance."
        ]
    }
}