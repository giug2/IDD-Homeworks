{
    "PAPER'S NUMBER OF TABLES": 6,
    "S4.T1": {
        "caption": "Table 1: Tradeoff between gradient inversion (GI) loss and computing time with different sizes of Dr​e​csubscript𝐷𝑟𝑒𝑐D_{rec} after 15k iterations, with the MNIST dataset",
        "table": "<table id=\"S4.T1.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S4.T1.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Size</td>\n<td id=\"S4.T1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1/64</td>\n<td id=\"S4.T1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1/16</td>\n<td id=\"S4.T1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1/4</td>\n<td id=\"S4.T1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1/2</td>\n<td id=\"S4.T1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S4.T1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10</td>\n</tr>\n<tr id=\"S4.T1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Time(s)</td>\n<td id=\"S4.T1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">193</td>\n<td id=\"S4.T1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">207</td>\n<td id=\"S4.T1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">214</td>\n<td id=\"S4.T1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">219</td>\n<td id=\"S4.T1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">564</td>\n<td id=\"S4.T1.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2097</td>\n</tr>\n<tr id=\"S4.T1.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">GI loss</td>\n<td id=\"S4.T1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">27</td>\n<td id=\"S4.T1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">4.1</td>\n<td id=\"S4.T1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">2.56</td>\n<td id=\"S4.T1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.74</td>\n<td id=\"S4.T1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.62</td>\n<td id=\"S4.T1.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.47</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "At time ",
                "t",
                "𝑡",
                "t",
                ", when the server receives a stale model update ",
                "w",
                "i",
                "t",
                "−",
                "τ",
                "subscript",
                "superscript",
                "𝑤",
                "𝑡",
                "𝜏",
                "𝑖",
                "w^{t-\\tau}_{i}",
                " from client ",
                "i",
                "𝑖",
                "i",
                " with staleness ",
                "τ",
                "𝜏",
                "\\tau",
                ", we adopt gradient inversion described in Eq. (",
                "1",
                ") into FL, to recover an intermediate dataset ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " from ",
                "w",
                "i",
                "t",
                "−",
                "τ",
                "subscript",
                "superscript",
                "𝑤",
                "𝑡",
                "𝜏",
                "𝑖",
                "w^{t-\\tau}_{i}",
                ". We expect that ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " represents the similar data distribution with the client ",
                "i",
                "𝑖",
                "i",
                "’s original training dataset ",
                "D",
                "i",
                "subscript",
                "𝐷",
                "𝑖",
                "D_{i}",
                ". To achieve so, we first fix the size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " and randomly initialize each data sample and label in ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                ". Then, we iteratively update ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " by minimizing",
                "using gradient descent, where ",
                "D",
                "​",
                "i",
                "​",
                "s",
                "​",
                "p",
                "​",
                "a",
                "​",
                "r",
                "​",
                "i",
                "​",
                "t",
                "​",
                "y",
                "​",
                "[",
                "⋅",
                "]",
                "𝐷",
                "𝑖",
                "𝑠",
                "𝑝",
                "𝑎",
                "𝑟",
                "𝑖",
                "𝑡",
                "𝑦",
                "delimited-[]",
                "⋅",
                "Disparity[\\cdot]",
                " is a metric to evaluate how much ",
                "w",
                "i",
                "t",
                "−",
                "τ",
                "subscript",
                "superscript",
                "𝑤",
                "𝑡",
                "𝜏",
                "𝑖",
                "w^{t-\\tau}_{i}",
                " changes if being retrained using ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                ". In FL, a client’s model update comprises multiple local training steps instead of a single gradient. Hence, to use gradient inversion for data recovery in FL, we substitute the single gradient computed from ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " in Eq. (",
                "1",
                ") with the local training outcome using ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                ". In this way, since the loss surface in the model’s weight space computed using ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " is similar to that using ",
                "D",
                "i",
                "subscript",
                "𝐷",
                "𝑖",
                "D_{i}",
                ", we can expect a similar gradient being computed. To verify this, we conducted preliminary experiments by using the MNIST dataset to train the LeNet model. Results in Figure ",
                "4",
                " show that, the loss surface computed using ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " is highly similar to that using ",
                "D",
                "i",
                "subscript",
                "𝐷",
                "𝑖",
                "D_{i}",
                ", in the proximity of the current global model (",
                "w",
                "g",
                "​",
                "l",
                "​",
                "o",
                "​",
                "b",
                "​",
                "a",
                "​",
                "l",
                "t",
                "−",
                "τ",
                "subscript",
                "superscript",
                "𝑤",
                "𝑡",
                "𝜏",
                "𝑔",
                "𝑙",
                "𝑜",
                "𝑏",
                "𝑎",
                "𝑙",
                "w^{t-\\tau}_{global}",
                "), and the computed gradient is hence very similar, too.",
                "A key issue is how to decide the proper size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                ". Since gradient inversion is equivalent to data resampling in the original training data’s distribution, a sufficiently large size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " would be necessary to ensure unbiased data sampling and sufficient minimization of gradient loss through iterations. On the other hand, when the size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " is too large, the computational overhead of each iteration would be unnecessarily too high. We experimentally investigated such tradeoff by using the MNIST and CIFAR-10 [",
                "10",
                "] datasets to train a LeNet model. Results in Tables 1 and 2, where the size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " is represented by its ratio to the size of original training data, show that when the size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " is larger than 1/2 of the size of the original training data, further increasing the size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " only results in little extra reduction of the gradient inversion loss but dramatically increase the computational overhead. Hence, we believe that it is a suitable size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " for FL. Considering that clients’ local dataset in FL contain at least hundreds of samples, we expect a big size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " in most FL scenarios.",
                "Such a big size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " directly decides our choice of how to evaluate the change of ",
                "w",
                "i",
                "t",
                "−",
                "τ",
                "subscript",
                "superscript",
                "𝑤",
                "𝑡",
                "𝜏",
                "𝑖",
                "w^{t-\\tau}_{i}",
                " in Eq. (",
                "2",
                "). Most existing works use cosine similarity between ",
                "L",
                "​",
                "o",
                "​",
                "c",
                "​",
                "a",
                "​",
                "l",
                "​",
                "U",
                "​",
                "p",
                "​",
                "d",
                "​",
                "a",
                "​",
                "t",
                "​",
                "e",
                "​",
                "(",
                "w",
                "g",
                "​",
                "l",
                "​",
                "o",
                "​",
                "b",
                "​",
                "a",
                "​",
                "l",
                "t",
                "−",
                "τ",
                ";",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                ")",
                "𝐿",
                "𝑜",
                "𝑐",
                "𝑎",
                "𝑙",
                "𝑈",
                "𝑝",
                "𝑑",
                "𝑎",
                "𝑡",
                "𝑒",
                "subscript",
                "superscript",
                "𝑤",
                "𝑡",
                "𝜏",
                "𝑔",
                "𝑙",
                "𝑜",
                "𝑏",
                "𝑎",
                "𝑙",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "LocalUpdate(w^{t-\\tau}_{global};D_{rec})",
                " and ",
                "w",
                "i",
                "t",
                "−",
                "τ",
                "subscript",
                "superscript",
                "𝑤",
                "𝑡",
                "𝜏",
                "𝑖",
                "w^{t-\\tau}_{i}",
                " to evaluate their difference in the direction of gradients, so as to maximize the quality of individual data samples in ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " [",
                "3",
                "]. However, since we aim to recover a large ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                ", this metric is not applicable, and instead we use L1-norm as the metric to evaluate how using ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " to retrain ",
                "w",
                "g",
                "​",
                "l",
                "​",
                "o",
                "​",
                "b",
                "​",
                "a",
                "​",
                "l",
                "t",
                "−",
                "τ",
                "subscript",
                "superscript",
                "𝑤",
                "𝑡",
                "𝜏",
                "𝑔",
                "𝑙",
                "𝑜",
                "𝑏",
                "𝑎",
                "𝑙",
                "w^{t-\\tau}_{global}",
                " will change its magnitude of gradient, to make sure that ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " incurs the minimum impact on the state of training.",
                "With such a big ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                ", the similarity between data samples in ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " and ",
                "D",
                "i",
                "subscript",
                "𝐷",
                "𝑖",
                "D_{i}",
                " is the minimum, hence protecting the client ",
                "i",
                "𝑖",
                "i",
                "’s local data privacy. To verify this, we did experiments with the CIFAR-10 dataset and ResNet-18 model, and match each data sample in ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " with the most similar data sample in ",
                "D",
                "i",
                "subscript",
                "𝐷",
                "𝑖",
                "D_{i}",
                " by computing their LPIPS perceptual similarity score [",
                "23",
                "]. As shown in Figure ",
                "5",
                ", these matching data samples are highly dissimilar, and the recovered data samples in ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " are mostly meaningless to humans.",
                "As shown in Tables 1 and 2, gradient inversion is computationally expensive because it needs a large amount of iterations to converge. In our approach, we reduce this high computational overhead in the following two ways. First, we simplify ",
                "L",
                "​",
                "o",
                "​",
                "c",
                "​",
                "a",
                "​",
                "l",
                "​",
                "U",
                "​",
                "p",
                "​",
                "d",
                "​",
                "a",
                "​",
                "t",
                "​",
                "e",
                "​",
                "[",
                "⋅",
                "]",
                "𝐿",
                "𝑜",
                "𝑐",
                "𝑎",
                "𝑙",
                "𝑈",
                "𝑝",
                "𝑑",
                "𝑎",
                "𝑡",
                "𝑒",
                "delimited-[]",
                "⋅",
                "LocalUpdate[\\cdot]",
                " in Eq. (",
                "2",
                "). In the original ",
                "L",
                "​",
                "o",
                "​",
                "c",
                "​",
                "a",
                "​",
                "l",
                "​",
                "U",
                "​",
                "p",
                "​",
                "d",
                "​",
                "a",
                "​",
                "t",
                "​",
                "e",
                "​",
                "[",
                "⋅",
                "]",
                "𝐿",
                "𝑜",
                "𝑐",
                "𝑎",
                "𝑙",
                "𝑈",
                "𝑝",
                "𝑑",
                "𝑎",
                "𝑡",
                "𝑒",
                "delimited-[]",
                "⋅",
                "LocalUpdate[\\cdot]",
                " used in FL, the client performs training epochs via mini-batch SGD, and a random data augmentation is usually applied to client data before each epoch. To reduce such overhead, in gradient inversion only full-batch gradient descent will be performed. Second, in most FL scenarios, the clients’ local datasets remain fixed, implying that ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " will also be invariant over time. Therefore, instead of starting iterations from a random initialization, we can optimize ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " from those calculated in previous training epochs. In our experiments using the MNIST dataset and the LeNet model, when the client data remains fixed, we observe a reduction in the required iterations in gradient inversion by 43%. When the client data is only partially fixed, Figure ",
                "6",
                " that we can still achieve more than 15% reduction when 20% of client data is changing in every epoch."
            ]
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Tradeoff between gradient inversion (GI) loss and computing time with different sizes of Dr​e​csubscript𝐷𝑟𝑒𝑐D_{rec} after 15k iterations, with the CIFAR-10 dataset",
        "table": "<table id=\"S4.T2.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S4.T2.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Size</td>\n<td id=\"S4.T2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1/64</td>\n<td id=\"S4.T2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1/16</td>\n<td id=\"S4.T2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1/4</td>\n<td id=\"S4.T2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1/2</td>\n<td id=\"S4.T2.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S4.T2.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10</td>\n</tr>\n<tr id=\"S4.T2.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Time(s)</td>\n<td id=\"S4.T2.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">423</td>\n<td id=\"S4.T2.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">440</td>\n<td id=\"S4.T2.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">452</td>\n<td id=\"S4.T2.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">474</td>\n<td id=\"S4.T2.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1330</td>\n<td id=\"S4.T2.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4637</td>\n</tr>\n<tr id=\"S4.T2.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">GI Loss</td>\n<td id=\"S4.T2.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.97</td>\n<td id=\"S4.T2.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.29</td>\n<td id=\"S4.T2.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.16</td>\n<td id=\"S4.T2.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.15</td>\n<td id=\"S4.T2.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.15</td>\n<td id=\"S4.T2.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.12</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "At time ",
                "t",
                "𝑡",
                "t",
                ", when the server receives a stale model update ",
                "w",
                "i",
                "t",
                "−",
                "τ",
                "subscript",
                "superscript",
                "𝑤",
                "𝑡",
                "𝜏",
                "𝑖",
                "w^{t-\\tau}_{i}",
                " from client ",
                "i",
                "𝑖",
                "i",
                " with staleness ",
                "τ",
                "𝜏",
                "\\tau",
                ", we adopt gradient inversion described in Eq. (",
                "1",
                ") into FL, to recover an intermediate dataset ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " from ",
                "w",
                "i",
                "t",
                "−",
                "τ",
                "subscript",
                "superscript",
                "𝑤",
                "𝑡",
                "𝜏",
                "𝑖",
                "w^{t-\\tau}_{i}",
                ". We expect that ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " represents the similar data distribution with the client ",
                "i",
                "𝑖",
                "i",
                "’s original training dataset ",
                "D",
                "i",
                "subscript",
                "𝐷",
                "𝑖",
                "D_{i}",
                ". To achieve so, we first fix the size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " and randomly initialize each data sample and label in ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                ". Then, we iteratively update ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " by minimizing",
                "using gradient descent, where ",
                "D",
                "​",
                "i",
                "​",
                "s",
                "​",
                "p",
                "​",
                "a",
                "​",
                "r",
                "​",
                "i",
                "​",
                "t",
                "​",
                "y",
                "​",
                "[",
                "⋅",
                "]",
                "𝐷",
                "𝑖",
                "𝑠",
                "𝑝",
                "𝑎",
                "𝑟",
                "𝑖",
                "𝑡",
                "𝑦",
                "delimited-[]",
                "⋅",
                "Disparity[\\cdot]",
                " is a metric to evaluate how much ",
                "w",
                "i",
                "t",
                "−",
                "τ",
                "subscript",
                "superscript",
                "𝑤",
                "𝑡",
                "𝜏",
                "𝑖",
                "w^{t-\\tau}_{i}",
                " changes if being retrained using ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                ". In FL, a client’s model update comprises multiple local training steps instead of a single gradient. Hence, to use gradient inversion for data recovery in FL, we substitute the single gradient computed from ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " in Eq. (",
                "1",
                ") with the local training outcome using ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                ". In this way, since the loss surface in the model’s weight space computed using ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " is similar to that using ",
                "D",
                "i",
                "subscript",
                "𝐷",
                "𝑖",
                "D_{i}",
                ", we can expect a similar gradient being computed. To verify this, we conducted preliminary experiments by using the MNIST dataset to train the LeNet model. Results in Figure ",
                "4",
                " show that, the loss surface computed using ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " is highly similar to that using ",
                "D",
                "i",
                "subscript",
                "𝐷",
                "𝑖",
                "D_{i}",
                ", in the proximity of the current global model (",
                "w",
                "g",
                "​",
                "l",
                "​",
                "o",
                "​",
                "b",
                "​",
                "a",
                "​",
                "l",
                "t",
                "−",
                "τ",
                "subscript",
                "superscript",
                "𝑤",
                "𝑡",
                "𝜏",
                "𝑔",
                "𝑙",
                "𝑜",
                "𝑏",
                "𝑎",
                "𝑙",
                "w^{t-\\tau}_{global}",
                "), and the computed gradient is hence very similar, too.",
                "A key issue is how to decide the proper size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                ". Since gradient inversion is equivalent to data resampling in the original training data’s distribution, a sufficiently large size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " would be necessary to ensure unbiased data sampling and sufficient minimization of gradient loss through iterations. On the other hand, when the size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " is too large, the computational overhead of each iteration would be unnecessarily too high. We experimentally investigated such tradeoff by using the MNIST and CIFAR-10 [",
                "10",
                "] datasets to train a LeNet model. Results in Tables 1 and 2, where the size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " is represented by its ratio to the size of original training data, show that when the size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " is larger than 1/2 of the size of the original training data, further increasing the size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " only results in little extra reduction of the gradient inversion loss but dramatically increase the computational overhead. Hence, we believe that it is a suitable size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " for FL. Considering that clients’ local dataset in FL contain at least hundreds of samples, we expect a big size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " in most FL scenarios.",
                "Such a big size of ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " directly decides our choice of how to evaluate the change of ",
                "w",
                "i",
                "t",
                "−",
                "τ",
                "subscript",
                "superscript",
                "𝑤",
                "𝑡",
                "𝜏",
                "𝑖",
                "w^{t-\\tau}_{i}",
                " in Eq. (",
                "2",
                "). Most existing works use cosine similarity between ",
                "L",
                "​",
                "o",
                "​",
                "c",
                "​",
                "a",
                "​",
                "l",
                "​",
                "U",
                "​",
                "p",
                "​",
                "d",
                "​",
                "a",
                "​",
                "t",
                "​",
                "e",
                "​",
                "(",
                "w",
                "g",
                "​",
                "l",
                "​",
                "o",
                "​",
                "b",
                "​",
                "a",
                "​",
                "l",
                "t",
                "−",
                "τ",
                ";",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                ")",
                "𝐿",
                "𝑜",
                "𝑐",
                "𝑎",
                "𝑙",
                "𝑈",
                "𝑝",
                "𝑑",
                "𝑎",
                "𝑡",
                "𝑒",
                "subscript",
                "superscript",
                "𝑤",
                "𝑡",
                "𝜏",
                "𝑔",
                "𝑙",
                "𝑜",
                "𝑏",
                "𝑎",
                "𝑙",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "LocalUpdate(w^{t-\\tau}_{global};D_{rec})",
                " and ",
                "w",
                "i",
                "t",
                "−",
                "τ",
                "subscript",
                "superscript",
                "𝑤",
                "𝑡",
                "𝜏",
                "𝑖",
                "w^{t-\\tau}_{i}",
                " to evaluate their difference in the direction of gradients, so as to maximize the quality of individual data samples in ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " [",
                "3",
                "]. However, since we aim to recover a large ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                ", this metric is not applicable, and instead we use L1-norm as the metric to evaluate how using ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " to retrain ",
                "w",
                "g",
                "​",
                "l",
                "​",
                "o",
                "​",
                "b",
                "​",
                "a",
                "​",
                "l",
                "t",
                "−",
                "τ",
                "subscript",
                "superscript",
                "𝑤",
                "𝑡",
                "𝜏",
                "𝑔",
                "𝑙",
                "𝑜",
                "𝑏",
                "𝑎",
                "𝑙",
                "w^{t-\\tau}_{global}",
                " will change its magnitude of gradient, to make sure that ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " incurs the minimum impact on the state of training.",
                "With such a big ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                ", the similarity between data samples in ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " and ",
                "D",
                "i",
                "subscript",
                "𝐷",
                "𝑖",
                "D_{i}",
                " is the minimum, hence protecting the client ",
                "i",
                "𝑖",
                "i",
                "’s local data privacy. To verify this, we did experiments with the CIFAR-10 dataset and ResNet-18 model, and match each data sample in ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " with the most similar data sample in ",
                "D",
                "i",
                "subscript",
                "𝐷",
                "𝑖",
                "D_{i}",
                " by computing their LPIPS perceptual similarity score [",
                "23",
                "]. As shown in Figure ",
                "5",
                ", these matching data samples are highly dissimilar, and the recovered data samples in ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " are mostly meaningless to humans.",
                "As shown in Tables 1 and 2, gradient inversion is computationally expensive because it needs a large amount of iterations to converge. In our approach, we reduce this high computational overhead in the following two ways. First, we simplify ",
                "L",
                "​",
                "o",
                "​",
                "c",
                "​",
                "a",
                "​",
                "l",
                "​",
                "U",
                "​",
                "p",
                "​",
                "d",
                "​",
                "a",
                "​",
                "t",
                "​",
                "e",
                "​",
                "[",
                "⋅",
                "]",
                "𝐿",
                "𝑜",
                "𝑐",
                "𝑎",
                "𝑙",
                "𝑈",
                "𝑝",
                "𝑑",
                "𝑎",
                "𝑡",
                "𝑒",
                "delimited-[]",
                "⋅",
                "LocalUpdate[\\cdot]",
                " in Eq. (",
                "2",
                "). In the original ",
                "L",
                "​",
                "o",
                "​",
                "c",
                "​",
                "a",
                "​",
                "l",
                "​",
                "U",
                "​",
                "p",
                "​",
                "d",
                "​",
                "a",
                "​",
                "t",
                "​",
                "e",
                "​",
                "[",
                "⋅",
                "]",
                "𝐿",
                "𝑜",
                "𝑐",
                "𝑎",
                "𝑙",
                "𝑈",
                "𝑝",
                "𝑑",
                "𝑎",
                "𝑡",
                "𝑒",
                "delimited-[]",
                "⋅",
                "LocalUpdate[\\cdot]",
                " used in FL, the client performs training epochs via mini-batch SGD, and a random data augmentation is usually applied to client data before each epoch. To reduce such overhead, in gradient inversion only full-batch gradient descent will be performed. Second, in most FL scenarios, the clients’ local datasets remain fixed, implying that ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " will also be invariant over time. Therefore, instead of starting iterations from a random initialization, we can optimize ",
                "D",
                "r",
                "​",
                "e",
                "​",
                "c",
                "subscript",
                "𝐷",
                "𝑟",
                "𝑒",
                "𝑐",
                "D_{rec}",
                " from those calculated in previous training epochs. In our experiments using the MNIST dataset and the LeNet model, when the client data remains fixed, we observe a reduction in the required iterations in gradient inversion by 43%. When the client data is only partially fixed, Figure ",
                "6",
                " that we can still achieve more than 15% reduction when 20% of client data is changing in every epoch."
            ]
        ]
    },
    "S5.T3": {
        "caption": "Table 3: The trained model accuracy and amount of training time spent with different amounts of staleness, measured in the number of delayed epochs",
        "table": "<table id=\"S5.T3.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T3.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T3.1.1.1.1\" class=\"ltx_text\"><span id=\"S5.T3.1.1.1.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T3.1.1.1.1.2\" class=\"ltx_text\">\n<span id=\"S5.T3.1.1.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.1.1.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.1.1.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T3.1.1.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">training time</span></span></span>\n<span id=\"S5.T3.1.1.1.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T3.1.1.1.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T3.1.1.1.1.2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">accuracy</span></span></span>\n</span></span> <span id=\"S5.T3.1.1.1.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"S5.T3.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span id=\"S5.T3.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Staleness (epoch)</span></td>\n</tr>\n<tr id=\"S5.T3.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.2.1\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.1.2.1.1\" class=\"ltx_text ltx_font_bold\">10</span></td>\n<td id=\"S5.T3.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.1.2.2.1\" class=\"ltx_text ltx_font_bold\">20</span></td>\n<td id=\"S5.T3.1.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.1.2.3.1\" class=\"ltx_text ltx_font_bold\">40</span></td>\n</tr>\n<tr id=\"S5.T3.1.3\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\">\n<span id=\"S5.T3.1.3.1.1\" class=\"ltx_text\"><span id=\"S5.T3.1.3.1.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T3.1.3.1.1.2\" class=\"ltx_text\">\n<span id=\"S5.T3.1.3.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.1.3.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.1.3.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T3.1.3.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">GI-based Estimation</span></span></span>\n</span></span> <span id=\"S5.T3.1.3.1.1.3\" class=\"ltx_text\"></span></span>\n</td>\n<td id=\"S5.T3.1.3.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.1.3.2.1\" class=\"ltx_text ltx_font_bold\">720</span></td>\n<td id=\"S5.T3.1.3.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.1.3.3.1\" class=\"ltx_text ltx_font_bold\">660</span></td>\n<td id=\"S5.T3.1.3.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.1.3.4.1\" class=\"ltx_text ltx_font_bold\">580</span></td>\n</tr>\n<tr id=\"S5.T3.1.4\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.4.1\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.1.4.1.1\" class=\"ltx_text ltx_font_bold\">73.3%</span></td>\n<td id=\"S5.T3.1.4.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.1.4.2.1\" class=\"ltx_text ltx_font_bold\">75.2%</span></td>\n<td id=\"S5.T3.1.4.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.1.4.3.1\" class=\"ltx_text ltx_font_bold\">77.7%</span></td>\n</tr>\n<tr id=\"S5.T3.1.5\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span id=\"S5.T3.1.5.1.1\" class=\"ltx_text\"><span id=\"S5.T3.1.5.1.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T3.1.5.1.1.2\" class=\"ltx_text\">\n<span id=\"S5.T3.1.5.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.1.5.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.1.5.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T3.1.5.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Direct Aggregation</span></span></span>\n</span></span> <span id=\"S5.T3.1.5.1.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"S5.T3.1.5.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.1.5.2.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n<td id=\"S5.T3.1.5.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.1.5.3.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n<td id=\"S5.T3.1.5.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.1.5.4.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n</tr>\n<tr id=\"S5.T3.1.6\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.6.1\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.1.6.1.1\" class=\"ltx_text ltx_font_bold\">72.6%</span></td>\n<td id=\"S5.T3.1.6.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.1.6.2.1\" class=\"ltx_text ltx_font_bold\">73.1%</span></td>\n<td id=\"S5.T3.1.6.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.1.6.3.1\" class=\"ltx_text ltx_font_bold\">73.9%</span></td>\n</tr>\n<tr id=\"S5.T3.1.7\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.7.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"2\"><span id=\"S5.T3.1.7.1.1\" class=\"ltx_text\"><span id=\"S5.T3.1.7.1.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T3.1.7.1.1.2\" class=\"ltx_text\">\n<span id=\"S5.T3.1.7.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.1.7.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.1.7.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T3.1.7.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">First Order compensation</span></span></span>\n</span></span> <span id=\"S5.T3.1.7.1.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"S5.T3.1.7.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.1.7.2.1\" class=\"ltx_text ltx_font_bold\">790</span></td>\n<td id=\"S5.T3.1.7.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.1.7.3.1\" class=\"ltx_text ltx_font_bold\">780</span></td>\n<td id=\"S5.T3.1.7.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.1.7.4.1\" class=\"ltx_text ltx_font_bold\">820</span></td>\n</tr>\n<tr id=\"S5.T3.1.8\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.8.1\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T3.1.8.1.1\" class=\"ltx_text ltx_font_bold\">72.8%</span></td>\n<td id=\"S5.T3.1.8.2\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T3.1.8.2.1\" class=\"ltx_text ltx_font_bold\">73.1%</span></td>\n<td id=\"S5.T3.1.8.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T3.1.8.3.1\" class=\"ltx_text ltx_font_bold\">73.7%</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In the fixed data scenario, we conduct experiments in two FL settings: 1) the MNIST [",
                "11",
                "] dataset to train a LeNet model and 2) the CIFAR-10 [",
                "10",
                "] dataset to train a ResNet-8 model.",
                "When staleness is 40 epochs, Figure ",
                "11",
                " and ",
                "12",
                " show that our proposed technique results in much better model accuracy in both FL settings. At the early stage of FL, the model accuracy achieved by our technique of ",
                "gradient inversion based model estimation",
                " is very close to that of FL without staleness, indicating that our technique can fully remove the impact of staleness. In contrast, directly aggregating stale model updates could result in 7.5% drop in model accuracy, and using weighted aggregation with staleness could even increase such model accuracy drop to up to 20%.",
                "Our technique can also greatly speed up the training progress. As shown in Figure ",
                "11",
                " and ",
                "12",
                ", compared to direct aggregation or weighted aggregation with staleness, to achieve the same model accuracy in the early stage of training (before 400 epochs in Figure ",
                "11",
                " and before 1000 epochs in Figure ",
                "12",
                "), our technique can reduce the required training time by 27.5% and 35%, for the MNIST and CIFAR-10 dataset, respectively. This speedup is particularly important in many time-sensitive and mission-critical applications such as embedded sensing, where coarse-grained but fast model ability is needed.",
                "Furthermore, we also conducted experiments with different amounts of data heterogeneity and device heterogeneity (a.k.a., staleness) using the MNIST dataset and the LeNet model. Results in Tables 3 and 4 show that, compared with the existing schemes including direct aggregation and first-order compensation, our proposed gradient inversion (GI)-based estimation can generally achieve higher model accuracy using a smaller amount of training epochs. Especially when the amount of staleness increases to a unlimited level (e.g., 20-40 epochs) or the data distributions among different clients are highly heterogeneous, the improvement of model accuracy could be up to 5% with ",
                ">",
                ">",
                "30% less training epochs. These results demonstrate that our proposed method can be widely applied to different FL scenarios with unlimited amount of staleness and data heterogeneity."
            ]
        ]
    },
    "S5.T4": {
        "caption": "Table 4: The trained model accuracy and amount of training time spent with different amounts of data heterogeneity, controlled by the tunable parameter α𝛼\\alpha)",
        "table": "<table id=\"S5.T4.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T4.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T4.1.1.2.1\" class=\"ltx_text\"><span id=\"S5.T4.1.1.2.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T4.1.1.2.1.2\" class=\"ltx_text\">\n<span id=\"S5.T4.1.1.2.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T4.1.1.2.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T4.1.1.2.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T4.1.1.2.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">training time</span></span></span>\n<span id=\"S5.T4.1.1.2.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T4.1.1.2.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T4.1.1.2.1.2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">accuracy</span></span></span>\n</span></span> <span id=\"S5.T4.1.1.2.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"S5.T4.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><math id=\"S5.T4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha\" display=\"inline\"><semantics id=\"S5.T4.1.1.1.m1.1a\"><mi id=\"S5.T4.1.1.1.m1.1.1\" xref=\"S5.T4.1.1.1.m1.1.1.cmml\">α</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.1.1.1.m1.1b\"><ci id=\"S5.T4.1.1.1.m1.1.1.cmml\" xref=\"S5.T4.1.1.1.m1.1.1\">𝛼</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.1.1.1.m1.1c\">\\alpha</annotation></semantics></math></td>\n</tr>\n<tr id=\"S5.T4.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.2.1\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.1.2.1.1\" class=\"ltx_text ltx_font_bold\">100</span></td>\n<td id=\"S5.T4.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.1.2.2.1\" class=\"ltx_text ltx_font_bold\">1</span></td>\n<td id=\"S5.T4.1.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.1.2.3.1\" class=\"ltx_text ltx_font_bold\">0.1</span></td>\n</tr>\n<tr id=\"S5.T4.1.3\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\">\n<span id=\"S5.T4.1.3.1.1\" class=\"ltx_text\"><span id=\"S5.T4.1.3.1.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T4.1.3.1.1.2\" class=\"ltx_text\">\n<span id=\"S5.T4.1.3.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T4.1.3.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T4.1.3.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T4.1.3.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">GI-based Estimation</span></span></span>\n</span></span> <span id=\"S5.T4.1.3.1.1.3\" class=\"ltx_text\"></span></span>\n</td>\n<td id=\"S5.T4.1.3.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.1.3.2.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n<td id=\"S5.T4.1.3.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.1.3.3.1\" class=\"ltx_text ltx_font_bold\">760</span></td>\n<td id=\"S5.T4.1.3.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.1.3.4.1\" class=\"ltx_text ltx_font_bold\">580</span></td>\n</tr>\n<tr id=\"S5.T4.1.4\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.4.1\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T4.1.4.1.1\" class=\"ltx_text ltx_font_bold\">82.3%</span></td>\n<td id=\"S5.T4.1.4.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T4.1.4.2.1\" class=\"ltx_text ltx_font_bold\">78.1%</span></td>\n<td id=\"S5.T4.1.4.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T4.1.4.3.1\" class=\"ltx_text ltx_font_bold\">78.3%</span></td>\n</tr>\n<tr id=\"S5.T4.1.5\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span id=\"S5.T4.1.5.1.1\" class=\"ltx_text\"><span id=\"S5.T4.1.5.1.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T4.1.5.1.1.2\" class=\"ltx_text\">\n<span id=\"S5.T4.1.5.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T4.1.5.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T4.1.5.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T4.1.5.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Direct Aggregation</span></span></span>\n</span></span> <span id=\"S5.T4.1.5.1.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"S5.T4.1.5.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.1.5.2.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n<td id=\"S5.T4.1.5.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.1.5.3.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n<td id=\"S5.T4.1.5.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.1.5.4.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n</tr>\n<tr id=\"S5.T4.1.6\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.6.1\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T4.1.6.1.1\" class=\"ltx_text ltx_font_bold\">82.3%</span></td>\n<td id=\"S5.T4.1.6.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T4.1.6.2.1\" class=\"ltx_text ltx_font_bold\">77.2%</span></td>\n<td id=\"S5.T4.1.6.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T4.1.6.3.1\" class=\"ltx_text ltx_font_bold\">73.2%</span></td>\n</tr>\n<tr id=\"S5.T4.1.7\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.7.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"2\"><span id=\"S5.T4.1.7.1.1\" class=\"ltx_text\"><span id=\"S5.T4.1.7.1.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T4.1.7.1.1.2\" class=\"ltx_text\">\n<span id=\"S5.T4.1.7.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T4.1.7.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T4.1.7.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T4.1.7.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">First Order compensation</span></span></span>\n</span></span> <span id=\"S5.T4.1.7.1.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"S5.T4.1.7.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.1.7.2.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n<td id=\"S5.T4.1.7.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.1.7.3.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n<td id=\"S5.T4.1.7.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.1.7.4.1\" class=\"ltx_text ltx_font_bold\">820</span></td>\n</tr>\n<tr id=\"S5.T4.1.8\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.8.1\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T4.1.8.1.1\" class=\"ltx_text ltx_font_bold\">82.3%</span></td>\n<td id=\"S5.T4.1.8.2\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T4.1.8.2.1\" class=\"ltx_text ltx_font_bold\">77.2%</span></td>\n<td id=\"S5.T4.1.8.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T4.1.8.3.1\" class=\"ltx_text ltx_font_bold\">72.7%</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In the fixed data scenario, we conduct experiments in two FL settings: 1) the MNIST [",
                "11",
                "] dataset to train a LeNet model and 2) the CIFAR-10 [",
                "10",
                "] dataset to train a ResNet-8 model.",
                "When staleness is 40 epochs, Figure ",
                "11",
                " and ",
                "12",
                " show that our proposed technique results in much better model accuracy in both FL settings. At the early stage of FL, the model accuracy achieved by our technique of ",
                "gradient inversion based model estimation",
                " is very close to that of FL without staleness, indicating that our technique can fully remove the impact of staleness. In contrast, directly aggregating stale model updates could result in 7.5% drop in model accuracy, and using weighted aggregation with staleness could even increase such model accuracy drop to up to 20%.",
                "Our technique can also greatly speed up the training progress. As shown in Figure ",
                "11",
                " and ",
                "12",
                ", compared to direct aggregation or weighted aggregation with staleness, to achieve the same model accuracy in the early stage of training (before 400 epochs in Figure ",
                "11",
                " and before 1000 epochs in Figure ",
                "12",
                "), our technique can reduce the required training time by 27.5% and 35%, for the MNIST and CIFAR-10 dataset, respectively. This speedup is particularly important in many time-sensitive and mission-critical applications such as embedded sensing, where coarse-grained but fast model ability is needed.",
                "Furthermore, we also conducted experiments with different amounts of data heterogeneity and device heterogeneity (a.k.a., staleness) using the MNIST dataset and the LeNet model. Results in Tables 3 and 4 show that, compared with the existing schemes including direct aggregation and first-order compensation, our proposed gradient inversion (GI)-based estimation can generally achieve higher model accuracy using a smaller amount of training epochs. Especially when the amount of staleness increases to a unlimited level (e.g., 20-40 epochs) or the data distributions among different clients are highly heterogeneous, the improvement of model accuracy could be up to 5% with ",
                ">",
                ">",
                "30% less training epochs. These results demonstrate that our proposed method can be widely applied to different FL scenarios with unlimited amount of staleness and data heterogeneity."
            ]
        ]
    },
    "S5.T5": {
        "caption": "Table 5: The trained model accuracy and amount of training time spent with different amounts of staleness",
        "table": "<table id=\"S5.T5.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T5.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T5.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T5.1.1.1.1\" class=\"ltx_text\"><span id=\"S5.T5.1.1.1.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T5.1.1.1.1.2\" class=\"ltx_text\">\n<span id=\"S5.T5.1.1.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T5.1.1.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T5.1.1.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T5.1.1.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">training time</span></span></span>\n<span id=\"S5.T5.1.1.1.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T5.1.1.1.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T5.1.1.1.1.2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">accuracy</span></span></span>\n</span></span> <span id=\"S5.T5.1.1.1.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"S5.T5.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span id=\"S5.T5.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Staleness (epoch)</span></td>\n</tr>\n<tr id=\"S5.T5.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T5.1.2.1\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T5.1.2.1.1\" class=\"ltx_text ltx_font_bold\">10</span></td>\n<td id=\"S5.T5.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T5.1.2.2.1\" class=\"ltx_text ltx_font_bold\">20</span></td>\n<td id=\"S5.T5.1.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T5.1.2.3.1\" class=\"ltx_text ltx_font_bold\">40</span></td>\n</tr>\n<tr id=\"S5.T5.1.3\" class=\"ltx_tr\">\n<td id=\"S5.T5.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\">\n<span id=\"S5.T5.1.3.1.1\" class=\"ltx_text\"><span id=\"S5.T5.1.3.1.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T5.1.3.1.1.2\" class=\"ltx_text\">\n<span id=\"S5.T5.1.3.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T5.1.3.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T5.1.3.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T5.1.3.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">GI-based Estimation</span></span></span>\n</span></span> <span id=\"S5.T5.1.3.1.1.3\" class=\"ltx_text\"></span></span>\n</td>\n<td id=\"S5.T5.1.3.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T5.1.3.2.1\" class=\"ltx_text ltx_font_bold\">760</span></td>\n<td id=\"S5.T5.1.3.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T5.1.3.3.1\" class=\"ltx_text ltx_font_bold\">710</span></td>\n<td id=\"S5.T5.1.3.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T5.1.3.4.1\" class=\"ltx_text ltx_font_bold\">440</span></td>\n</tr>\n<tr id=\"S5.T5.1.4\" class=\"ltx_tr\">\n<td id=\"S5.T5.1.4.1\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T5.1.4.1.1\" class=\"ltx_text ltx_font_bold\">54.0%</span></td>\n<td id=\"S5.T5.1.4.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T5.1.4.2.1\" class=\"ltx_text ltx_font_bold\">52.4%</span></td>\n<td id=\"S5.T5.1.4.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T5.1.4.3.1\" class=\"ltx_text ltx_font_bold\">60.2%</span></td>\n</tr>\n<tr id=\"S5.T5.1.5\" class=\"ltx_tr\">\n<td id=\"S5.T5.1.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span id=\"S5.T5.1.5.1.1\" class=\"ltx_text\"><span id=\"S5.T5.1.5.1.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T5.1.5.1.1.2\" class=\"ltx_text\">\n<span id=\"S5.T5.1.5.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T5.1.5.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T5.1.5.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T5.1.5.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Direct Aggregation</span></span></span>\n</span></span> <span id=\"S5.T5.1.5.1.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"S5.T5.1.5.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T5.1.5.2.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n<td id=\"S5.T5.1.5.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T5.1.5.3.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n<td id=\"S5.T5.1.5.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T5.1.5.4.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n</tr>\n<tr id=\"S5.T5.1.6\" class=\"ltx_tr\">\n<td id=\"S5.T5.1.6.1\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T5.1.6.1.1\" class=\"ltx_text ltx_font_bold\">51.4%</span></td>\n<td id=\"S5.T5.1.6.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T5.1.6.2.1\" class=\"ltx_text ltx_font_bold\">46.6%</span></td>\n<td id=\"S5.T5.1.6.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T5.1.6.3.1\" class=\"ltx_text ltx_font_bold\">40.7%</span></td>\n</tr>\n<tr id=\"S5.T5.1.7\" class=\"ltx_tr\">\n<td id=\"S5.T5.1.7.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"2\"><span id=\"S5.T5.1.7.1.1\" class=\"ltx_text\"><span id=\"S5.T5.1.7.1.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T5.1.7.1.1.2\" class=\"ltx_text\">\n<span id=\"S5.T5.1.7.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T5.1.7.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T5.1.7.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T5.1.7.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">First Order compensation</span></span></span>\n</span></span> <span id=\"S5.T5.1.7.1.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"S5.T5.1.7.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T5.1.7.2.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n<td id=\"S5.T5.1.7.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T5.1.7.3.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n<td id=\"S5.T5.1.7.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T5.1.7.4.1\" class=\"ltx_text ltx_font_bold\">820</span></td>\n</tr>\n<tr id=\"S5.T5.1.8\" class=\"ltx_tr\">\n<td id=\"S5.T5.1.8.1\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T5.1.8.1.1\" class=\"ltx_text ltx_font_bold\">51.4%</span></td>\n<td id=\"S5.T5.1.8.2\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T5.1.8.2.1\" class=\"ltx_text ltx_font_bold\">46.5%</span></td>\n<td id=\"S5.T5.1.8.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T5.1.8.3.1\" class=\"ltx_text ltx_font_bold\">40.9%</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "To continuously vary the data distributions of clients’ local datasets, we use two public datasets, namely MNIST and SVHN [",
                "15",
                "], which are for the same learning task (i.e., handwriting digit recognition) but with different feature representations as shown in Figure 13. Each client’s local dataset is initialized as the MNIST dataset in the same way as in the fixed data scenario. Afterwards, during training, each client continuously replaces random data samples in its local dataset with new data samples in the SVHN dataset.",
                "Experiment results in Figure ",
                "14",
                " show that in such variant data scenario, since clients’ local data distributions continuously change, the FL training will never converge. Hence, the model accuracy improvements by the existing FL training strategies, including both direct aggregation with staleness and first-order compensation, exhibit significant fluctuations over time and stay low (",
                "<",
                "<",
                "40%). In comparison, our proposed gradient inversion based estimation can better depict the variant data patterns and hence achieve much higher model accuracy, which is comparable to FL without staleness and 20% higher than those in existing FL schemes.",
                "In addition, we also conducted experiments with different amounts of staleness and rates of data variation. Results in Tables ",
                "5",
                " and ",
                "6",
                " demonstrated that our proposed method outperformed the existing FL strategies in different scenarios with different dynamics of local data patterns."
            ]
        ]
    },
    "S5.T6": {
        "caption": "Table 6: Model accuracy and amount of training time with different rates of clients’ data variation, measured as the number of local data samples being replaced in each epoch (e.g., 1/2 is to replace one data sample every 2 epochs)",
        "table": "<table id=\"S5.T6.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T6.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T6.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T6.1.1.1.1\" class=\"ltx_text\"><span id=\"S5.T6.1.1.1.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T6.1.1.1.1.2\" class=\"ltx_text\">\n<span id=\"S5.T6.1.1.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T6.1.1.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T6.1.1.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T6.1.1.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">training time</span></span></span>\n<span id=\"S5.T6.1.1.1.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T6.1.1.1.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T6.1.1.1.1.2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">accuracy</span></span></span>\n</span></span> <span id=\"S5.T6.1.1.1.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"S5.T6.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span id=\"S5.T6.1.1.2.1\" class=\"ltx_text ltx_font_bold\">streaming rate</span></td>\n</tr>\n<tr id=\"S5.T6.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T6.1.2.1\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T6.1.2.1.1\" class=\"ltx_text ltx_font_bold\">1/4</span></td>\n<td id=\"S5.T6.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T6.1.2.2.1\" class=\"ltx_text ltx_font_bold\">1/3</span></td>\n<td id=\"S5.T6.1.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T6.1.2.3.1\" class=\"ltx_text ltx_font_bold\">1/2</span></td>\n</tr>\n<tr id=\"S5.T6.1.3\" class=\"ltx_tr\">\n<td id=\"S5.T6.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\">\n<span id=\"S5.T6.1.3.1.1\" class=\"ltx_text\"><span id=\"S5.T6.1.3.1.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T6.1.3.1.1.2\" class=\"ltx_text\">\n<span id=\"S5.T6.1.3.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T6.1.3.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T6.1.3.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T6.1.3.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">GI-based Estimation</span></span></span>\n</span></span> <span id=\"S5.T6.1.3.1.1.3\" class=\"ltx_text\"></span></span>\n</td>\n<td id=\"S5.T6.1.3.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T6.1.3.2.1\" class=\"ltx_text ltx_font_bold\">330</span></td>\n<td id=\"S5.T6.1.3.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T6.1.3.3.1\" class=\"ltx_text ltx_font_bold\">330</span></td>\n<td id=\"S5.T6.1.3.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T6.1.3.4.1\" class=\"ltx_text ltx_font_bold\">440</span></td>\n</tr>\n<tr id=\"S5.T6.1.4\" class=\"ltx_tr\">\n<td id=\"S5.T6.1.4.1\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T6.1.4.1.1\" class=\"ltx_text ltx_font_bold\">32.2%</span></td>\n<td id=\"S5.T6.1.4.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T6.1.4.2.1\" class=\"ltx_text ltx_font_bold\">41.9%</span></td>\n<td id=\"S5.T6.1.4.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T6.1.4.3.1\" class=\"ltx_text ltx_font_bold\">60.2%</span></td>\n</tr>\n<tr id=\"S5.T6.1.5\" class=\"ltx_tr\">\n<td id=\"S5.T6.1.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span id=\"S5.T6.1.5.1.1\" class=\"ltx_text\"><span id=\"S5.T6.1.5.1.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T6.1.5.1.1.2\" class=\"ltx_text\">\n<span id=\"S5.T6.1.5.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T6.1.5.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T6.1.5.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T6.1.5.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Direct Aggregation</span></span></span>\n</span></span> <span id=\"S5.T6.1.5.1.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"S5.T6.1.5.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T6.1.5.2.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n<td id=\"S5.T6.1.5.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T6.1.5.3.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n<td id=\"S5.T6.1.5.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T6.1.5.4.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n</tr>\n<tr id=\"S5.T6.1.6\" class=\"ltx_tr\">\n<td id=\"S5.T6.1.6.1\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T6.1.6.1.1\" class=\"ltx_text ltx_font_bold\">27.8%</span></td>\n<td id=\"S5.T6.1.6.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T6.1.6.2.1\" class=\"ltx_text ltx_font_bold\">32.6%</span></td>\n<td id=\"S5.T6.1.6.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T6.1.6.3.1\" class=\"ltx_text ltx_font_bold\">40.7%</span></td>\n</tr>\n<tr id=\"S5.T6.1.7\" class=\"ltx_tr\">\n<td id=\"S5.T6.1.7.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"2\"><span id=\"S5.T6.1.7.1.1\" class=\"ltx_text\"><span id=\"S5.T6.1.7.1.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T6.1.7.1.1.2\" class=\"ltx_text\">\n<span id=\"S5.T6.1.7.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T6.1.7.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T6.1.7.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T6.1.7.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">First Oreder Compensation</span></span></span>\n</span></span> <span id=\"S5.T6.1.7.1.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"S5.T6.1.7.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T6.1.7.2.1\" class=\"ltx_text ltx_font_bold\">830</span></td>\n<td id=\"S5.T6.1.7.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T6.1.7.3.1\" class=\"ltx_text ltx_font_bold\">800</span></td>\n<td id=\"S5.T6.1.7.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T6.1.7.4.1\" class=\"ltx_text ltx_font_bold\">820</span></td>\n</tr>\n<tr id=\"S5.T6.1.8\" class=\"ltx_tr\">\n<td id=\"S5.T6.1.8.1\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T6.1.8.1.1\" class=\"ltx_text ltx_font_bold\">27.5%</span></td>\n<td id=\"S5.T6.1.8.2\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T6.1.8.2.1\" class=\"ltx_text ltx_font_bold\">32.6%</span></td>\n<td id=\"S5.T6.1.8.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T6.1.8.3.1\" class=\"ltx_text ltx_font_bold\">40.9%</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "To continuously vary the data distributions of clients’ local datasets, we use two public datasets, namely MNIST and SVHN [",
                "15",
                "], which are for the same learning task (i.e., handwriting digit recognition) but with different feature representations as shown in Figure 13. Each client’s local dataset is initialized as the MNIST dataset in the same way as in the fixed data scenario. Afterwards, during training, each client continuously replaces random data samples in its local dataset with new data samples in the SVHN dataset.",
                "Experiment results in Figure ",
                "14",
                " show that in such variant data scenario, since clients’ local data distributions continuously change, the FL training will never converge. Hence, the model accuracy improvements by the existing FL training strategies, including both direct aggregation with staleness and first-order compensation, exhibit significant fluctuations over time and stay low (",
                "<",
                "<",
                "40%). In comparison, our proposed gradient inversion based estimation can better depict the variant data patterns and hence achieve much higher model accuracy, which is comparable to FL without staleness and 20% higher than those in existing FL schemes.",
                "In addition, we also conducted experiments with different amounts of staleness and rates of data variation. Results in Tables ",
                "5",
                " and ",
                "6",
                " demonstrated that our proposed method outperformed the existing FL strategies in different scenarios with different dynamics of local data patterns."
            ]
        ]
    }
}