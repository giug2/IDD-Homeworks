{
    "PAPER'S NUMBER OF TABLES": 6,
    "S4.T1": {
        "caption": "Table 1:  Results on the SuperGLUE dev set.",
        "table": "",
        "footnotes": "\n\n\n\nModel\nModel Size\nReCoRD\nCOPA\nWSC\nRTE\nBoolQ\nWiC\nCB\nMultiRC\nAvg\n\nF1/Acc.\nAcc.\nAcc.\nAcc.\nAcc.\nAcc.\nF1/Acc.\nF1a/EM\n\n\n\nT5largesubscriptT5large\\text{T5}_{\\text{large}}Â (Du etÂ al., 2022)\n770M\n85.7/85.0\n78.0\n84.6\n84.8\n84.3\n71.6\n96.4/98.2\n80.9/46.6\n81.2\n\nBARTLargesubscriptBARTLarge\\text{BART}_{\\text{Large}}Â (Du etÂ al., 2022)\n409M\n88.3/87.8\n60.0\n65.4\n84.5\n84.3\n69.0\n90.5/92.9\n81.8/48.0\n76.0\n\nRoBERTaLargesubscriptRoBERTaLarge\\text{RoBERTa}_{\\text{Large}}Â (Du etÂ al., 2022)\n335M\n89.0/88.4\n90.0\n63.5\n87.0\n86.1\n72.6\n96.1/94.6\n84.4/52.9\n81.5\n\nGLMRoBERTasubscriptGLMRoBERTa\\text{GLM}_{\\text{RoBERTa}}Â (Du etÂ al., 2022)\n335M\n89.6/89.0\n82.0\n83.7\n87.7\n84.7\n71.2\n98.7/98.2\n82.4/50.1\n82.9\n\nChatGLM-6BÂ (Zeng etÂ al., 2022)\n6B\n80.2/78.7\n85.0\n71.2\n81.6\n83.4\n71.0\n85.7/83.9\n78.2/45.6\n79.6\n\n\\hdashlineFL-GLM\n6B\n79.8/78.4\n85.0\n71.2\n80.1\n81.9\n69.6\n85.7/83.9\n79.3/46.1\n79.1\n\n",
        "references": [
            "The quantitative evaluation results on SuperGLUE are shown in TableÂ 1."
        ]
    },
    "S4.T2": {
        "caption": "Table 2:  Results of abstractive summarization on the CNN/DailyMail and XSum test sets.",
        "table": "",
        "footnotes": "\n\n\n\nModel\nModel Size\nCNN/DailyMail\nXSum\n\nROUGE-1\nROUGE-2\nROUGE-L\nROUGE-1\nROUGE-2\nROUGE-L\n\n\n\nBERTSumAbsÂ (Liu and Lapata, 2019)\n110M\n41.7\n19.4\n38.8\n38.8\n16.3\n31.2\n\nUniLMv2BasesubscriptUniLMv2Base\\text{UniLMv2}_{\\text{Base}}Â (Bao etÂ al., 2020)\n110M\n43.2\n20.4\n40.1\n44.0\n21.1\n36.1\n\nT5LargesubscriptT5Large\\text{T5}_{\\text{Large}}Â (Raffel etÂ al., 2020)\n770M\n42.5\n20.7\n39.8\n40.9\n17.3\n33.0\n\nBARTLargesubscriptBARTLarge\\text{BART}_{\\text{Large}} Â (Lewis etÂ al., 2020)\n409M\n44.2\n21.3\n40.9\n45.1\n22.3\n37.3\n\nGLMRoBERTasubscriptGLMRoBERTa\\text{GLM}_{\\text{RoBERTa}}Â (Du etÂ al., 2022)\n335M\n43.8\n21.0\n40.5\n45.5\n23.5\n37.3\n\nChatGLM-6BÂ (Zeng etÂ al., 2022)\n6B\n40.4\n17.0\n28.0\n37.6\n12.5\n30.1\n\n\\hdashlineFL-GLM\n6B\n39.6\n16.9\n28.0\n37.0\n11.9\n29.4\n\n",
        "references": [
            "From the results on CNN/DialyMail and XSum datasets in TableÂ 2, shiFL-GLM can obtain 39.6 ROUGE-1, 16.9 ROUGE-2, and 28.0 ROUGE-L on the CNN/DailyMail dataset, 37.0 ROUGE-1, 11.9 ROUGE-2, and 29.4 ROUGE-L on the XSum dataset. Not more than 1.0 lower than the results of the centralized ChatGLM-6B model."
        ]
    },
    "S4.T3": {
        "caption": "Table 3:  Comparison of training time between different training strategies",
        "table": "",
        "footnotes": "\n\n\n\nStrategy\nCentralized\nserial\nclient-batch parallel\nserver-hierarchical\n\n\nnum. of clients\nNone\n2\n2\n4\n8\n2\n3\n5\n10\n\n\n\\hdashlinetime(s)\n166.4Â±9.2\n175.2Â±10.1\n85.3Â±4.1\n43.0Â±2.5\n22.5Â±1.7\n87.3Â±4.9\n65.5Â±3.2\n34.5Â±1.9\n17.3Â±0.9\n\n\n",
        "references": [
            [
                "Since the NLU tasks are reformulated as blank infilling tasks, the model performance can be evaluated using the generated probability of the ground-truth answer ",
                "a",
                "â€‹",
                "(",
                "y",
                ")",
                "ğ‘",
                "ğ‘¦",
                "a(y)",
                ". For the RTE, BoolQ, WiC, CB, and MultiRC datasets, the generated answer may contain a single word. Therefore, we compute the logit of the corresponding answer token as the evaluation score, defined as:",
                "where ",
                "Y",
                "ğ‘Œ",
                "Y",
                " is the ground-truth label set.",
                "For the ReCoRD, COPA, and WSC datasets, the answers may contain multiple words; therefore, we compute the sum of the log-probabilities of the answer tokens as the evaluation metrics, which is defined as",
                "For the summarization task, we use ROUGE-1, ROUGE-2, and ROUGE-L as quantitative metrics, which are widely used in NLP tasksÂ ",
                "(Liu etÂ al., ",
                "2021a",
                "; Chen and Yang, ",
                "2020",
                "; Fang etÂ al., ",
                "2022",
                ")",
                "."
            ]
        ]
    },
    "S4.T4": {
        "caption": "Table 4:  Security Analysis ",
        "table": "",
        "footnotes": "\n\n\n\nFğ¹F\nFâˆ’1superscriptğ¹1F^{-1}\nRouge-1\nRouge-2\nRouge-l\nBleu-4\n\n\n\nEmbedding(FebBert)\nLinear\n33.29\n7.053\n26.732\n28.57\n\nFL-GLM client-side part A\nTransformer\n0.135\n0.002\n0.473\n0.335\n\n",
        "references": [
            "The experimental results are shown in Table 4. When the client only has the embedding layer like FedBert model, Fâˆ’1superscriptğ¹1F^{-1} is a single Transformer block, the attack model can achieve a BLEU-4 score of 28.570 and a ROUGE-1 score of 33.290, while in the FL-GLM framework, where the client contains the embedding layer and an LLM-Block, Fâˆ’1superscriptğ¹1F^{-1} is a single layer Transfomer, all the metrics of the attack model are all close to 0. Therefore, the security of FL-GLM could be proven in experiments. Additionally, we find that the attack metricsâ€™ performance of a single-block Transformer is similar to that of a multi-block Transformer. Therefore, the optimal split point, based on experimental results, might be a single-block Transformer, even though it is challenging to prove theoretically."
        ]
    },
    "A0.T5": {
        "caption": "Table 5:  Cloze questions and answers for the 8 SuperGLUE tasks",
        "table": "",
        "footnotes": "\n\n\n\nDataset\nTask\nCloze Question\nAnswers\n\nReCoRD\nQuestion answering\n[passage p] [cloze question q]\nAnswer candidates\n\nCOPA\nCausal reasoning\nâ€œ[choice c1]â€ orâ€œ[choice c2]â€? [premise p], so [M].\nc1/c2\n\nWSC\nCoreference resolution\n[sentence s] The pronoun â€˜*p*â€™ refers to [M].\nNoun n\n\nRTE\nTextual entailment\nâ€œ[hypothesis h]â€? [M] â€œ[premise p]â€\n\n\n\nâ€œyesâ€/â€œnoâ€\n\n\nBoolQ\nQuestion answering\n[passage p]. Question: q? Answer:[M].\nâ€œyesâ€ / â€œnoâ€\n\nWiC\nWord sense disambiguation\n\n\n\nâ€œ[sentence s1]â€/â€œ[sentence s2]â€Similar sense\n\nof [word w]? [M].\n\nâ€œyesâ€/â€œnoâ€\n\nCB\nTextual entailment\nâ€œ[hypothesis h]â€? [M], â€œ[premise p]â€\n\n\n\nâ€œyesâ€/â€œnoâ€/â€œmaybeâ€\n\n\nMultiRC\nQuestion answering\n[passage p]. Question: q? Is it [answer a]? [M].\nâ€œyesâ€/â€œnoâ€\n\n",
        "references": [
            "TableÂ 5 shows the cloze questions and answers for SuperGLUE tasks, and the detailed corresponding description of SuperGLUE benchmark are as below:"
        ]
    },
    "A2.T6": {
        "caption": "Table 6:  Impact of different average period",
        "table": "",
        "footnotes": "\n\n\n\nDatasets\nAverage Period\nSequential\nclient-batch parallel\nserver-hierarchical\n\n\n\nCOPA\n50\n85\n85\n85\n\n100\n85\n85\n85\n\nWiC\n50\n69.1\n66.6\n68.2\n\n100\n69.0\n65.5\n67.2\n\nRTE\n50\n80.1\n80.1\n78.3\n\n100\n79.8\n79.4\n77.6\n\nBoolQ\n50\n81.6\n79.9\n81.0\n\n100\n81.9\n80.5\n81.3\n\nMultiRC\n50\n79.3\n76.2\n77.5\n\n100\n77.5\n76.6\n77.1\n\nCB\n50\n85.7\n85.7\n85.7\n\n100\n85.7\n85.7\n85.7\n\nWSC\n50\n71.2\n63.5\n63.5\n\n100\n66.3\n65.4\n63.5\n\n",
        "references": [
            [
                "TableÂ ",
                "5",
                " shows the cloze questions and answers for SuperGLUE tasks, and the detailed corresponding description of SuperGLUE benchmark are as below:",
                "ReCoRD(Reading Comprehension with Commonsense Reasoning and Disambiguation): In this task, models are required to answer questions by extracting information from a given passage, while also employing commonsense reasoning and resolving ambiguous pronouns.",
                "COPA(Choice of Plausible Alternatives): This task assesses causal reasoning abilities by providing a premise and two alternative hypotheses, where the model must choose the correct causal relationship.",
                "WSC(Winograd Schema Challenge): This task evaluates pronoun resolution and coreference resolution abilities, where the model must identify the correct referent for a pronoun in a given sentence.",
                "RTE(Recognizing Textual Entailment): The task requires determining if one sentence entails, contradicts, or remains neutral with respect to another sentence.",
                "BoolQ(Boolean Questions): Models must answer boolean questions, i.e., questions that require a yes or no answer, based on a given context.",
                "WiC(Word-in-Context): In this task, models must determine if a word has the same sense in two different contexts, requiring fine-grained lexical semantics understanding.",
                "CB(CommitmentBank): It is a famous corpus of short texts for textual entailment task, in which at least one sentence contains an embedded clause.",
                "MultiRC(Multiple-Choice Reading Comprehension): This task involves answering multiple-choice questions based on multiple passages, which tests the ability to comprehend complex documents."
            ]
        ]
    }
}