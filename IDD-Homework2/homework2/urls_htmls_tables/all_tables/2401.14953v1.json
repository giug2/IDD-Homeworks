{
    "A4.T1": {
        "caption": "Table 1: Architectures",
        "table": "<table id=\"A4.T1.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A4.T1.4.1.1\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span id=\"A4.T1.4.1.1.1.1\" class=\"ltx_text ltx_font_bold\">RNN and LSTMs</span></th>\n<td id=\"A4.T1.4.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"A4.T1.4.1.1.2.1\" class=\"ltx_text ltx_font_bold\">S</span></td>\n<td id=\"A4.T1.4.1.1.3\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"A4.T1.4.1.1.3.1\" class=\"ltx_text ltx_font_bold\">M</span></td>\n<td id=\"A4.T1.4.1.1.4\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"A4.T1.4.1.1.4.1\" class=\"ltx_text ltx_font_bold\">L</span></td>\n</tr>\n<tr id=\"A4.T1.4.2.2\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">RNN Hidden size</th>\n<td id=\"A4.T1.4.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_t\">16</td>\n<td id=\"A4.T1.4.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_t\">32</td>\n<td id=\"A4.T1.4.2.2.4\" class=\"ltx_td ltx_align_left ltx_border_t\">128</td>\n</tr>\n<tr id=\"A4.T1.4.3.3\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Number of RNN layers</th>\n<td id=\"A4.T1.4.3.3.2\" class=\"ltx_td ltx_align_left\">1</td>\n<td id=\"A4.T1.4.3.3.3\" class=\"ltx_td ltx_align_left\">2</td>\n<td id=\"A4.T1.4.3.3.4\" class=\"ltx_td ltx_align_left\">3</td>\n</tr>\n<tr id=\"A4.T1.4.4.4\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MLP before RNN layers</th>\n<td id=\"A4.T1.4.4.4.2\" class=\"ltx_td ltx_align_left\">(16,)</td>\n<td id=\"A4.T1.4.4.4.3\" class=\"ltx_td ltx_align_left\">(32, 32)</td>\n<td id=\"A4.T1.4.4.4.4\" class=\"ltx_td ltx_align_left\">(128, 128, 128)</td>\n</tr>\n<tr id=\"A4.T1.4.5.5\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MLP after RNN layers</th>\n<td id=\"A4.T1.4.5.5.2\" class=\"ltx_td ltx_align_left\">(16,)</td>\n<td id=\"A4.T1.4.5.5.3\" class=\"ltx_td ltx_align_left\">(32, 32)</td>\n<td id=\"A4.T1.4.5.5.4\" class=\"ltx_td ltx_align_left\">(128, 128, 128)</td>\n</tr>\n<tr id=\"A4.T1.4.6.6\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"A4.T1.4.6.6.1.1\" class=\"ltx_text ltx_font_bold\">Transformer SINCOS</span></th>\n<td id=\"A4.T1.4.6.6.2\" class=\"ltx_td ltx_border_t\"/>\n<td id=\"A4.T1.4.6.6.3\" class=\"ltx_td ltx_border_t\"/>\n<td id=\"A4.T1.4.6.6.4\" class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr id=\"A4.T1.4.7.7\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Embedding dimension</th>\n<td id=\"A4.T1.4.7.7.2\" class=\"ltx_td ltx_align_left\">16</td>\n<td id=\"A4.T1.4.7.7.3\" class=\"ltx_td ltx_align_left\">64</td>\n<td id=\"A4.T1.4.7.7.4\" class=\"ltx_td ltx_align_left\">256</td>\n</tr>\n<tr id=\"A4.T1.4.8.8\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Number of heads</th>\n<td id=\"A4.T1.4.8.8.2\" class=\"ltx_td ltx_align_left\">2</td>\n<td id=\"A4.T1.4.8.8.3\" class=\"ltx_td ltx_align_left\">4</td>\n<td id=\"A4.T1.4.8.8.4\" class=\"ltx_td ltx_align_left\">4</td>\n</tr>\n<tr id=\"A4.T1.4.9.9\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Number of layers</th>\n<td id=\"A4.T1.4.9.9.2\" class=\"ltx_td ltx_align_left ltx_border_bb\">2</td>\n<td id=\"A4.T1.4.9.9.3\" class=\"ltx_td ltx_align_left ltx_border_bb\">4</td>\n<td id=\"A4.T1.4.9.9.4\" class=\"ltx_td ltx_align_left ltx_border_bb\">6</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "A vanilla multi-layer RNNÂ (Elman, 1990) with hidden sizes and multi-layer perceptron (MLP) before and after the RNN layers as described in TableÂ 1.",
            "A multi-layer RNN controller with hidden sizes and MLP exactly the same as the RNN and LSTMs on TableÂ 1 with access to a differentiable stackÂ (Joulin and Mikolov, 2015).\nThe controller can perform any linear combination of push, pop, and no-op on the stack of size according to TableÂ 1, with action weights given by a softmax over a linear readout of the RNN output. Each cell of the stack contains a real vector of dimension 6 and the stack size is 64 for all (S, M and L) sizes.",
            "A multi-layer RNN controller with hidden sizes according to the TableÂ 1 with access to a differentiable tape, inspired by the Baby-NTM architectureÂ (Suzgun etÂ al., 2019).\nThe controller can perform any linear combination of write-right, write-left, write-stay, jump-left, and jump-right on the tape, with action weights given by a softmax.\nThe actions correspond to: writing at the current position and moving to the right (write-right), writing at the current position and moving to the left (write-left), writing at the current position (write-stay), jumping â„“â„“\\ell steps to the right without writing (jump-right), where â„“â„“\\ell is the length of the input, and jumping â„“â„“\\ell steps to the left without writing (jump-left).\nAs in the Stack-RNN, each cell of the tape contains a real vector of dimension 6 and the tape size is 64 for all (S, M and L) sizes.",
            "A multi-layer LSTMÂ (Hochreiter and Schmidhuber, 1997) of hidden sizes according to TableÂ 1.",
            "A vanilla Transformer decoderÂ (Vaswani etÂ al., 2017). See TableÂ 1 for the embedding dimension, number of heads and number of layers for each model size (S, M and L). Each layer is composed of an attention layer, two dense layers, and a layer normalization. We add a residual connections as in the original architectureÂ (Vaswani etÂ al., 2017).\nWe consider the standard sin/cosÂ (Vaswani etÂ al., 2017) positional encoding."
        ]
    },
    "A4.T2": {
        "caption": "Table 2: \nTable taken from (Deletang etÂ al., 2022). Tasks with their level in the Chomsky hierarchy and example input/output pairs.\nThe â€ â€ \\dagger denotes permutation-invariant tasks; the â‹†â‹†\\star denotes counting tasks; the âˆ˜\\circ denotes tasks that require a nondeterministic controller; and the Ã—\\times denotes tasks that require superlinear running time in terms of the input length.",
        "table": null,
        "footnotes": [],
        "references": []
    },
    "A5.T3": {
        "caption": "Table 3: \nPre-trained BP program sampling probabilities\nInstead of sampling programs uniformly, we can sample them w.r.t.Â any\nprobability distribution Qğ‘„Q that satisfies TheoremÂ 9.\nWe initially sampled programs uniformly and filtered out â€˜boringâ€™ sequences.\nThen we trained Qğ‘„Q via cross-entropy to mimic the distribution of â€˜interestingâ€™ sequences.\nWe used a 2nd-order Markov process as a model for Qğ‘„Q.\nWhile uniform sampling resulted in only 0.02% interesting sequences,\nsampling from Qğ‘„Q increased it to 2.5%, a 137-fold improvement.\nThe table on the left shows the 0th, 1st, and 2nd order Markov processes\nQâ€‹(pt)ğ‘„subscriptğ‘ğ‘¡Q(p_{t}),\nQâ€‹(pt|ptâˆ’1)ğ‘„conditionalsubscriptğ‘ğ‘¡subscriptğ‘ğ‘¡1Q(p_{t}|p_{t-1}), and\nQâ€‹(pt|ptâˆ’2â€‹ptâˆ’1)ğ‘„conditionalsubscriptğ‘ğ‘¡subscriptğ‘ğ‘¡2subscriptğ‘ğ‘¡1Q(p_{t}|p_{t-2}p_{t-1}) from which BP programs are sampled, for pâ‹…âˆˆ{<>+-[]{.}subscriptğ‘â‹…<>+-[]{.p_{\\cdot}\\in\\{\\texttt{<>+-[]\\{.}\\},\nbut where results for [ and { have been merged.\nEach row corresponds to a context (none or ptâˆ’1subscriptğ‘ğ‘¡1p_{t-1} or ptâˆ’2â€‹ptâˆ’1subscriptğ‘ğ‘¡2subscriptğ‘ğ‘¡1p_{t-2}p_{t-1}).\nWe also included Qâ€‹(p1|p0:=_)ğ‘„assignconditionalsubscriptğ‘1subscriptğ‘0_Q(p_{1}|p_{0}\\!\\!:=\\!\\!\\texttt{\\_}) and Qâ€‹(p1|pâˆ’1â€‹p0:=__)ğ‘„assignconditionalsubscriptğ‘1subscriptğ‘1subscriptğ‘0__Q(p_{1}|p_{-1}p_{0}\\!\\!:=\\!\\!\\texttt{\\_\\_}).\nThe entries in each column correspond to the sampling probability of ptsubscriptğ‘ğ‘¡p_{t} in the corresponding row-context.\nTraining on interesting sequences has led to a non-uniform distribution Qğ‘„Q. Universality is preserved for any kğ‘˜k-order Markov process, provided all transition probabilities are non-zero.\nThe probability Qâ€‹(.)ğ‘„.Q(\\texttt{.}) of outputting a symbol has nearly doubled from 0.14 to 0.27 on average,\nwhile the probability of loop brackets ([,]) reduced to 0.07 each on average.\nThe marginal probabilities Qâ€‹(<)â‰ˆQâ€‹(>)â‰ˆQâ€‹(+)â‰ˆQâ€‹(-)â‰ˆ1/7ğ‘„<ğ‘„>ğ‘„+ğ‘„-17Q(\\texttt{<})\\approx Q(\\texttt{>})\\approx Q(\\texttt{+})\\approx Q(\\texttt{-})\\approx 1/7 have not changed much,\nbut many of the conditional ones have.\nCertain combination of instructions are now blocked:\nFor instance +- and -+ and <> and >< have probability close to 00,\nsince they cancel each other and hence are redundant.\nSome triples such as ][- and <+ and >- and others are enhanced.\n\nCaveat: We did not have time to retrain our NN models on these newly generated sequences (experiments are still running).\nBut since the statistics is improved, we expect the results in FiguresÂ 4 and 5 to improve or at least not deteriorate.",
        "table": null,
        "footnotes": [],
        "references": [
            "For practical purposes, sampling from non-uniform (possibly learned) distribution over programs can be advantageous for efficiency. For our BrainPhoque language (that we use in our experiments later) it increases the yield of â€˜interestingâ€™ programs by a factor of 137 (see Appendix TableÂ 3). Below we show this can be done without any concerns on losing universality."
        ]
    }
}