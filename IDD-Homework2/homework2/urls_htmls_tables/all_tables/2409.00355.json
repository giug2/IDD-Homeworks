{
    "id_table_1": {
        "caption": "Table 1:  G-Eval result between YATA and other models. The best results for each base model are  bolded  and the second-best result is  underlined .",
        "table": "S2.T1.1.1",
        "footnotes": [],
        "references": [
            "To achieve this, we propose Dual Retrieval-augmented Knowledge Fusion ( DRaKe ), which 1) concurrently retrieves  K I subscript K I K_{I} italic_K start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT  and  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  and 2) integrates them in the response via knowledge fusion module  f  (  ) f  f(\\cdot) italic_f (  ) . We explain the data setup process ( section   2.1 ),  DRaKe  framework ( section   2.2 ), and the user interface ( section   2.3 ) in this section. The overview of YA-TA is illustrated in Figure  2 .",
            "Table  1  shows that retrieving information from just one side outperforms dual retrieval, which highlights the challenge of achieving personalization on both sides. Additionally, the highest performance achieved by the  DRaKe  framework when both sides are considered together demonstrates that our framework excels at integrating knowledge from both perspectives."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  A sample translated response from YA-TA and GPT-4o.",
        "table": "S3.T2.5",
        "footnotes": [],
        "references": [
            "To achieve this, we propose Dual Retrieval-augmented Knowledge Fusion ( DRaKe ), which 1) concurrently retrieves  K I subscript K I K_{I} italic_K start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT  and  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  and 2) integrates them in the response via knowledge fusion module  f  (  ) f  f(\\cdot) italic_f (  ) . We explain the data setup process ( section   2.1 ),  DRaKe  framework ( section   2.2 ), and the user interface ( section   2.3 ) in this section. The overview of YA-TA is illustrated in Figure  2 .",
            "We retrieve knowledge from both instructor and student sides to equip YA-TA with resources to generate reliable and comprehensive responses.  In the  instructor knowledge  retrieval step, we first fetch segments corresponding to a particular course from the  instructor course database . Among them, we select top  k k k italic_k  segments by ensembling a sparse and a dense retriever to account for both lexical and semantic similarities when forming  K I subscript K I K_{I} italic_K start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT . 3 3 3 Implementation details are provided in Appendix  A .  In the  student knowledge  retrieval step, we fetch a students transcript from the  academic information system  by the student ID and query record from the  student query database  by the course ID. We combine them to form  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  to encompass both the overall and course-specific performances of the student. For example, as shown in Figure  2 , when a student named Kelly asks a question about course CS50, YA-TA retrieves a number of relevant segments from CS50 as the  instructor knowledge , along with Kellys transcript and query record as the  student knowledge .",
            "The main goal of the knowledge fusion module  f  (  ) f  f(\\cdot) italic_f (  )  is to generate a reliable and comprehensible response to the query by integrating  instructor knowledge  and  student knowledge . However, simply injecting them into the response generator may not produce the best response as each knowledge is composed of raw data. Therefore, we abstract each knowledge to a higher level utilizing the reasoning ability of an LLM, before passing it to the response generator. For the  instructor knowledge , we use an LLM to extract useful segments as  evidence  that provide necessary information to answer the query. We extract rather than interpret to minimize any deviation from the instructors exact words, which reflect their principles regarding the course topic. As the  student knowledge  contains raw information such as a transcript or a query record ( i.e. , list of past queries), we use an LLM to transform it into a  plan . This  plan  serves as a helpful guide for the response generator, enabling it to personalize the response for the student. Finally, we feed  evidence  and  plan  into the response generator, which then effectively integrates them to produce a response that is both grounded in the lecture and tailored to the student. As illustrated in Figure  2 , YA-TAs response, processed through the  DRaKe  framework, demonstrates a seamless fusion of  instructor knowledge  and  student knowledge .",
            "As illustrated in Table  2  and Table  3 , YA-TA produces responses that are personalized for both the instructor and the student. The similarity between the evidence and YA-TAs responses shows that YA-TA bases its answers on the lecture. Furthermore, YA-TA uses examples relevant to the students background, which demonstrates its ability to tailor responses based on the students academic background."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  A sample response from YA-TA and GPT-4o.",
        "table": "S3.T3.6",
        "footnotes": [],
        "references": [
            "To achieve this, we propose Dual Retrieval-augmented Knowledge Fusion ( DRaKe ), which 1) concurrently retrieves  K I subscript K I K_{I} italic_K start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT  and  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  and 2) integrates them in the response via knowledge fusion module  f  (  ) f  f(\\cdot) italic_f (  ) . We explain the data setup process ( section   2.1 ),  DRaKe  framework ( section   2.2 ), and the user interface ( section   2.3 ) in this section. The overview of YA-TA is illustrated in Figure  2 .",
            "As illustrated in Table  2  and Table  3 , YA-TA produces responses that are personalized for both the instructor and the student. The similarity between the evidence and YA-TAs responses shows that YA-TA bases its answers on the lecture. Furthermore, YA-TA uses examples relevant to the students background, which demonstrates its ability to tailor responses based on the students academic background."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  10 high-quality questions from the questions extracted by GPT-3.5-Turbo.",
        "table": "A2.T4.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "Video:",
        "Implementation details are provided in Appendix",
        ".",
        "We obtained permission to use the lecture videos for this paper from the lecturer. The videos can be accessed at the following link:",
        "In this section, we utilize",
        "and",
        ".",
        "For convenience, we named the course identifier GPP6003. The videos can be accessed at the following link:"
    ]
}