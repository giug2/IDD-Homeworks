{
    "id_table_1": {
        "caption": "TABLE I:  Automatically Computed Accuracy Results",
        "table": "S6.T1.3",
        "footnotes": [],
        "references": [
            "Figure  1  presents the steps we follow for transforming Ericssons CI/CD documents into a (domain-specific) corpus.",
            "Step 3: Indexing.   In this step, we embed and store the preprocessed documents obtained from Step 2 (Figure  1 ) in a vector database. This vector database serves as the domain-specific corpus for the information retrieval step of our chatbot (Step 2 in Figure  2 , discussed later). Since the preprocessed documents are ultimately supplied to the LLM as relevant context, our objective is to maximize their length. However, two important considerations arise when determining the ideal length. On the one hand, we must ensure that the documents fit within the context length (token limit) accepted by the LLM, as exceeding this limit could result in context loss, runtime errors, or incoherent output. On the other hand, supplying long documents to the LLM can lead to a needle in the haystack problem  [ 17 ] , where relevant information is lost amongst the noise. Therefore, determining the optimal length of embedded documents becomes an important factor in maximizing chatbot efficacy, as we aim to maximize the amount of relevant information while limiting the noise.",
            "Following the splitting of the Teams and Confluence data, we send the resulting chunks to an embedding function to generate text embeddings. To ensure consistent terminology, we refer to these chunks as  context items  rather than documents, to avoid ambiguity between the chunks and the original (Confluence) documents. We store the embeddings for each context item in the vector database alongside the items original text. The domain-specific corpus depicted in Figures  1  and  2  is realized by this vector database.",
            "(c) Embeddings-based retriever.  This retriever uses the embedding of the user query and the embeddings stored in the domain corpus (Section  V-A ) to retrieve the top- k k k italic_k  relevant items. The retrieval process has three main steps. First, the retriever embeds the query using the same embedding function as that used for embedding the domain-specific corpus during the indexing process (Step 3 of Figure  1 ). Second, the retriever computes cosine similarity scores between the embedded query and the items in the vector database. Finally, the retriever selects and returns the top- k k k italic_k  most semantically similar items."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  Results of Manual Analysis",
        "table": "S6.T2.3",
        "footnotes": [],
        "references": [
            "Step 3: Indexing.   In this step, we embed and store the preprocessed documents obtained from Step 2 (Figure  1 ) in a vector database. This vector database serves as the domain-specific corpus for the information retrieval step of our chatbot (Step 2 in Figure  2 , discussed later). Since the preprocessed documents are ultimately supplied to the LLM as relevant context, our objective is to maximize their length. However, two important considerations arise when determining the ideal length. On the one hand, we must ensure that the documents fit within the context length (token limit) accepted by the LLM, as exceeding this limit could result in context loss, runtime errors, or incoherent output. On the other hand, supplying long documents to the LLM can lead to a needle in the haystack problem  [ 17 ] , where relevant information is lost amongst the noise. Therefore, determining the optimal length of embedded documents becomes an important factor in maximizing chatbot efficacy, as we aim to maximize the amount of relevant information while limiting the noise.",
            "Following the splitting of the Teams and Confluence data, we send the resulting chunks to an embedding function to generate text embeddings. To ensure consistent terminology, we refer to these chunks as  context items  rather than documents, to avoid ambiguity between the chunks and the original (Confluence) documents. We store the embeddings for each context item in the vector database alongside the items original text. The domain-specific corpus depicted in Figures  1  and  2  is realized by this vector database.",
            "Figure  2  shows the steps implemented in our chatbot. We will discuss each of these steps below.",
            "Step 1: Query Rewriting.   Given that user queries may not always be clear or well-structured  [ 33 ] , our query rewriting module prompts an LLM  1 1 1 Note that the LLM used for query rewriting does not necessarily have to be the same one used for answer generation in Step 4 of Figure  2 , which will be discussed later. In our current approach, nevertheless, we use Llama 2 as the LLM of choice to implement both Steps 1 and 4 of our chatbot design.  to enhance the query into a more effective search query. To achieve this, we use the query enhancement guidelines of Ma et al.s  [ 33 ] . Specifically, given a (frozen) LLM and a user query, we prompt the LLM to restate the query in more precise terms before the query is used for question answering. We augment Ma et al.s prompt template for query rewriting by instructing the model to also analyze the user query in the context of the conversation history and determine if the current query is a follow-up.  In response, the LLM either forms an improved question or returns the user query  verbatim as the question to pose to the LLM.",
            "To build the domain corpus used as input for the retrieval step of our chatbot (Step 2 in Figure  2 ), we followed the corpus creation process described in Section  V-A . The corpus, which forms the basis for our evaluation in this section, was generated in May 2024. It includes 4,169 Microsoft Teams messages, 18,389 responses to these messages, and 240 Confluence web pages. These CI/CD resources collectively resulted in a total of 4985 context items, which were then stored in our vector database along with their embeddings.",
            "In our experiments, we found that the execution time is overwhelmingly dominated by the LLM during the query rewriting and answer generation phases (Steps 1 and 4 in Figure  2 ). On average, these steps account for 99.91% of the total execution time. In contrast, the retrieval and prompt formation steps (Steps 2 and 3) take negligible time, contributing less than one-tenth of a percent to the overall execution. Based on the findings from RQ1, the ensemble-retriever-based pipeline has the highest accuracy. Given that the retrieval step has virtually no impact on execution time, and considering that the overall execution time of the ensemble-retriever-based pipeline is comparable to (or slightly better than) alternatives, as shown in Figure  9 , we conclude that the ensemble-retriever-based pipeline is the optimal choice for our chatbot."
        ]
    },
    "global_footnotes": [
        "Note that the LLM used for query rewriting does not necessarily have to be the same one used for answer generation in Step 4 of Figure",
        ", which will be discussed later. In our current approach, nevertheless, we use Llama 2 as the LLM of choice to implement both Steps 1 and 4 of our chatbot design."
    ]
}