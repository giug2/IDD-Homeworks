{
    "Sx3.T1.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"Sx3.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"Sx3.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T1.1.1.1.1.1\">Anatomy</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T1.1.1.1.2.1\">Pixel Instances</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T1.1.1.1.3.1\">Instances across Images</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"Sx3.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.2.1.1\">Dorsal Space</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.2.1.2\">397,350,611</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.2.1.3\">10,223</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.3.2.1\">Dura</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.3.2.2\">142,857,955</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.3.2.3\">9,814</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.4.3.1\">Pia</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.4.3.2\">142,721,894</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.4.3.3\">9,839</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T1.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.5.4.1\">CSF</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.5.4.2\">118,161,757</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.5.4.3\">9,613</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T1.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.6.5.1\">Dura/Pia complex</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.6.5.2\">57,124,376</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.6.5.3\">2,732</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T1.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.7.6.1\">Spinal cord</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.7.6.2\">812,894,511</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.7.6.3\">10,223</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T1.1.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.8.7.1\">Hematoma</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.8.7.2\">69,727,644</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.8.7.3\">5,756</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T1.1.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.9.8.1\">Dura/Ventral Complex</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.9.8.2\">58,152,053</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.9.8.3\">2,671</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T1.1.10.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.10.9.1\">Ventral Space</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.10.9.2\">89,571,059</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.10.9.3\">6,099</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T1.1.11.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.11.10.1\">Background</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.11.10.2\">18,992,200</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx3.T1.1.11.10.3\">6,385</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 1:  Pixel- and image-wise instances of each anatomical structure.",
        "footnotes": [],
        "references": [
            "Images were labeled by a team of medical and graduate students trained by a board-certified radiologist. Each image in which there was a lack of clear delineation in anatomical boundaries was verified by the radiologist to ensure high quality and accurate labels. Once all the images were labeled, the masks were then validated by a neurosurgery spine fellow at the Johns Hopkins Hospital to provide a second round of verification and further ensure the robustness of the labels. Table 1 provides an overview of the image and pixel instances for each of these labels. These images and their corresponding annotations are available electronically on our GitHub Respository."
        ]
    },
    "Sx3.T2.1.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"Sx3.T2.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.2.1\">Encoder</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.1.1.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.1.1.3.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.3.1.1.1.1\"># of</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.1.1.3.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.3.1.2.1.1\">Parameters</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.1.1.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.1.1.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.4.1.1.1.1\">Learning</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.1.1.4.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.4.1.2.1.1\">Rate</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.1.1.5\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.1.1.5.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1.5.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.1.1.5.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.5.1.1.1.1\">Batch</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1.5.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.1.1.5.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.5.1.2.1.1\">Size</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.1.1.6\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.1.1.6.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1.6.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.1.1.6.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.6.1.1.1.1\">Training</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1.6.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.1.1.6.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.6.1.2.1.1\">Epochs</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.7.1\">Optimizer</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.1.1.8\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.8.1\">Loss Function</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.2.1.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.2.1.1.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.2.1.1.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.2.1.1.1.1.1\">Faster</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.2.1.1.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.2.1.1.1.2.1\">RCNN</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.2.1.2\">ResNet50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.2.1.3\">41,299,161</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.2.1.4\">0.004935</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.2.1.5\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.2.1.6\">60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.2.1.7\">SGD</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.2.1.8\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.2.1.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.2.1.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.2.1.8.1.1.1\">Cross Entropy + Smooth L1</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.3.2.1\">SSD300</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.3.2.2\">ResNet50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.3.2.3\">24,641,780</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.3.2.4\">0.002571</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.3.2.5\">32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.3.2.6\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.3.2.7\">SGD</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.3.2.8\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.3.2.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.3.2.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.3.2.8.1.1.1\">Cross Entropy + Smooth L1</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.4.3.1\">SSD512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.4.3.2\">ResNet50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.4.3.3\">24,641,780</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.4.3.4\">0.000251</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.4.3.5\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.4.3.6\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.4.3.7\">SGD</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.4.3.8\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.4.3.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.4.3.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.4.3.8.1.1.1\">Cross Entropy + Smooth L1</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.5.4.1\">RetinaNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.5.4.2\">ResNet50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.5.4.3\">36,352,630</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.5.4.4\">0.000099</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.5.4.5\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.5.4.6\">60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.5.4.7\">SGD</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.5.4.8\">Focal Loss</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.6.5.1\">DETR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.6.5.2\">ResNet50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.6.5.3\">41,524,954</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.6.5.4\">0.000011</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.6.5.5\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.6.5.6\">200</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.6.5.7\">AdamW</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.6.5.8\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.6.5.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.6.5.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.6.5.8.1.1.1\">Cross Entropy + Smooth L1</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.7.6.1\">YOLOv7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.7.6.2\">E-ELAN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.7.6.3\">37,196,556</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.7.6.4\">0.000123</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.7.6.5\">32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.7.6.6\">75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.7.6.7\">Adam</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.7.6.8\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.7.6.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.7.6.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.7.6.8.1.1.1\">Binary Cross Entropy + Mean Square Error</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.8.7.1\">YOLOv8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.8.7.2\">CSPNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.8.7.3\">25,856,899</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.8.7.4\">0.000422</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.8.7.5\">48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.8.7.6\">80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.8.7.7\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.8.7.7.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.8.7.7.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.8.7.7.1.1.1\">AdamW</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.8.7.7.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.8.7.7.1.2.1\">+ SGD</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx3.T2.1.1.8.7.8\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.8.7.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.8.7.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.8.7.8.1.1.1\">Binary Cross Entropy + Distribution Focal</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.8.7.8.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.8.7.8.1.2.1\">Loss + Complete Intersection over Union</td>\n</tr>\n</table>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 2:  Characteristics of object detection models to detect the site of injury on porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The injury localization dataset was divided into 3 splits with roughly 80% of both pre- and post-injury images for training, 10% for validation, and 10% for testing. We ensured that all images taken from each subject were placed in the same group to prevent overfitting during training, and we balanced the distribution of healthy and injured spinal cord images in each subset. To improve the robustness of the models, we performed a number of data augmentations in the training process. This includes image transformations (e.g., rotating the samples with a degree range of ±plus-or-minus\\pm± 25, horizontal flipping with a probability of 0.5, and randomly zooming in or out of the image by 30%) and image degradation (e.g., blurring the image by a factor of 3 with a probability of 20%). We also adjusted image appearance by randomly manipulating the statistical characteristics of the image intensities such as brightness, saturation, hue, and contrast by a factor of ±plus-or-minus\\pm± 0.2. To optimize computational resources and time, object detection models uniformly resize images to dimensions of 256 x 256 pixels. These models were trained on a Windows 11 Machine (8 GB RAM) with 24 GB NVIDIA GeForce RTX 3090 graphics unit and 14th Gen Intel Core i5-14600 processor (14 cores, 20 threads, 2.7GHZ to 5.2GHz turbo frequency). The hyperparameters of the model (i.e., batch size, learning rate, training epochs) were tuned to ensure optimal performance using the Neural Network Intelligence software (Supplementary Table 1-7) [48]. The main characteristics of each model are described in Table 2."
        ]
    },
    "Sx3.T2.1.1.1.1.3.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.1.1.3.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.3.1.1.1.1\"># of</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.1.1.3.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.3.1.2.1.1\">Parameters</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Characteristics of object detection models to detect the site of injury on porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The injury localization dataset was divided into 3 splits with roughly 80% of both pre- and post-injury images for training, 10% for validation, and 10% for testing. We ensured that all images taken from each subject were placed in the same group to prevent overfitting during training, and we balanced the distribution of healthy and injured spinal cord images in each subset. To improve the robustness of the models, we performed a number of data augmentations in the training process. This includes image transformations (e.g., rotating the samples with a degree range of ±plus-or-minus\\pm± 25, horizontal flipping with a probability of 0.5, and randomly zooming in or out of the image by 30%) and image degradation (e.g., blurring the image by a factor of 3 with a probability of 20%). We also adjusted image appearance by randomly manipulating the statistical characteristics of the image intensities such as brightness, saturation, hue, and contrast by a factor of ±plus-or-minus\\pm± 0.2. To optimize computational resources and time, object detection models uniformly resize images to dimensions of 256 x 256 pixels. These models were trained on a Windows 11 Machine (8 GB RAM) with 24 GB NVIDIA GeForce RTX 3090 graphics unit and 14th Gen Intel Core i5-14600 processor (14 cores, 20 threads, 2.7GHZ to 5.2GHz turbo frequency). The hyperparameters of the model (i.e., batch size, learning rate, training epochs) were tuned to ensure optimal performance using the Neural Network Intelligence software (Supplementary Table 1-7) [48]. The main characteristics of each model are described in Table 2."
        ]
    },
    "Sx3.T2.1.1.1.1.4.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.1.1.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.4.1.1.1.1\">Learning</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.1.1.4.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.4.1.2.1.1\">Rate</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Characteristics of object detection models to detect the site of injury on porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The injury localization dataset was divided into 3 splits with roughly 80% of both pre- and post-injury images for training, 10% for validation, and 10% for testing. We ensured that all images taken from each subject were placed in the same group to prevent overfitting during training, and we balanced the distribution of healthy and injured spinal cord images in each subset. To improve the robustness of the models, we performed a number of data augmentations in the training process. This includes image transformations (e.g., rotating the samples with a degree range of ±plus-or-minus\\pm± 25, horizontal flipping with a probability of 0.5, and randomly zooming in or out of the image by 30%) and image degradation (e.g., blurring the image by a factor of 3 with a probability of 20%). We also adjusted image appearance by randomly manipulating the statistical characteristics of the image intensities such as brightness, saturation, hue, and contrast by a factor of ±plus-or-minus\\pm± 0.2. To optimize computational resources and time, object detection models uniformly resize images to dimensions of 256 x 256 pixels. These models were trained on a Windows 11 Machine (8 GB RAM) with 24 GB NVIDIA GeForce RTX 3090 graphics unit and 14th Gen Intel Core i5-14600 processor (14 cores, 20 threads, 2.7GHZ to 5.2GHz turbo frequency). The hyperparameters of the model (i.e., batch size, learning rate, training epochs) were tuned to ensure optimal performance using the Neural Network Intelligence software (Supplementary Table 1-7) [48]. The main characteristics of each model are described in Table 2."
        ]
    },
    "Sx3.T2.1.1.1.1.5.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.1.1.5.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1.5.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.1.1.5.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.5.1.1.1.1\">Batch</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1.5.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.1.1.5.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.5.1.2.1.1\">Size</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Characteristics of object detection models to detect the site of injury on porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The injury localization dataset was divided into 3 splits with roughly 80% of both pre- and post-injury images for training, 10% for validation, and 10% for testing. We ensured that all images taken from each subject were placed in the same group to prevent overfitting during training, and we balanced the distribution of healthy and injured spinal cord images in each subset. To improve the robustness of the models, we performed a number of data augmentations in the training process. This includes image transformations (e.g., rotating the samples with a degree range of ±plus-or-minus\\pm± 25, horizontal flipping with a probability of 0.5, and randomly zooming in or out of the image by 30%) and image degradation (e.g., blurring the image by a factor of 3 with a probability of 20%). We also adjusted image appearance by randomly manipulating the statistical characteristics of the image intensities such as brightness, saturation, hue, and contrast by a factor of ±plus-or-minus\\pm± 0.2. To optimize computational resources and time, object detection models uniformly resize images to dimensions of 256 x 256 pixels. These models were trained on a Windows 11 Machine (8 GB RAM) with 24 GB NVIDIA GeForce RTX 3090 graphics unit and 14th Gen Intel Core i5-14600 processor (14 cores, 20 threads, 2.7GHZ to 5.2GHz turbo frequency). The hyperparameters of the model (i.e., batch size, learning rate, training epochs) were tuned to ensure optimal performance using the Neural Network Intelligence software (Supplementary Table 1-7) [48]. The main characteristics of each model are described in Table 2."
        ]
    },
    "Sx3.T2.1.1.1.1.6.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.1.1.6.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1.6.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.1.1.6.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.6.1.1.1.1\">Training</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.1.1.6.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.1.1.6.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T2.1.1.1.1.6.1.2.1.1\">Epochs</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Characteristics of object detection models to detect the site of injury on porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The injury localization dataset was divided into 3 splits with roughly 80% of both pre- and post-injury images for training, 10% for validation, and 10% for testing. We ensured that all images taken from each subject were placed in the same group to prevent overfitting during training, and we balanced the distribution of healthy and injured spinal cord images in each subset. To improve the robustness of the models, we performed a number of data augmentations in the training process. This includes image transformations (e.g., rotating the samples with a degree range of ±plus-or-minus\\pm± 25, horizontal flipping with a probability of 0.5, and randomly zooming in or out of the image by 30%) and image degradation (e.g., blurring the image by a factor of 3 with a probability of 20%). We also adjusted image appearance by randomly manipulating the statistical characteristics of the image intensities such as brightness, saturation, hue, and contrast by a factor of ±plus-or-minus\\pm± 0.2. To optimize computational resources and time, object detection models uniformly resize images to dimensions of 256 x 256 pixels. These models were trained on a Windows 11 Machine (8 GB RAM) with 24 GB NVIDIA GeForce RTX 3090 graphics unit and 14th Gen Intel Core i5-14600 processor (14 cores, 20 threads, 2.7GHZ to 5.2GHz turbo frequency). The hyperparameters of the model (i.e., batch size, learning rate, training epochs) were tuned to ensure optimal performance using the Neural Network Intelligence software (Supplementary Table 1-7) [48]. The main characteristics of each model are described in Table 2."
        ]
    },
    "Sx3.T2.1.1.2.1.1.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.2.1.1.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.2.1.1.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.2.1.1.1.1.1\">Faster</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.2.1.1.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.2.1.1.1.2.1\">RCNN</td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Characteristics of object detection models to detect the site of injury on porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The injury localization dataset was divided into 3 splits with roughly 80% of both pre- and post-injury images for training, 10% for validation, and 10% for testing. We ensured that all images taken from each subject were placed in the same group to prevent overfitting during training, and we balanced the distribution of healthy and injured spinal cord images in each subset. To improve the robustness of the models, we performed a number of data augmentations in the training process. This includes image transformations (e.g., rotating the samples with a degree range of ±plus-or-minus\\pm± 25, horizontal flipping with a probability of 0.5, and randomly zooming in or out of the image by 30%) and image degradation (e.g., blurring the image by a factor of 3 with a probability of 20%). We also adjusted image appearance by randomly manipulating the statistical characteristics of the image intensities such as brightness, saturation, hue, and contrast by a factor of ±plus-or-minus\\pm± 0.2. To optimize computational resources and time, object detection models uniformly resize images to dimensions of 256 x 256 pixels. These models were trained on a Windows 11 Machine (8 GB RAM) with 24 GB NVIDIA GeForce RTX 3090 graphics unit and 14th Gen Intel Core i5-14600 processor (14 cores, 20 threads, 2.7GHZ to 5.2GHz turbo frequency). The hyperparameters of the model (i.e., batch size, learning rate, training epochs) were tuned to ensure optimal performance using the Neural Network Intelligence software (Supplementary Table 1-7) [48]. The main characteristics of each model are described in Table 2."
        ]
    },
    "Sx3.T2.1.1.2.1.8.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.2.1.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.2.1.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.2.1.8.1.1.1\">Cross Entropy + Smooth L1</td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Characteristics of object detection models to detect the site of injury on porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The injury localization dataset was divided into 3 splits with roughly 80% of both pre- and post-injury images for training, 10% for validation, and 10% for testing. We ensured that all images taken from each subject were placed in the same group to prevent overfitting during training, and we balanced the distribution of healthy and injured spinal cord images in each subset. To improve the robustness of the models, we performed a number of data augmentations in the training process. This includes image transformations (e.g., rotating the samples with a degree range of ±plus-or-minus\\pm± 25, horizontal flipping with a probability of 0.5, and randomly zooming in or out of the image by 30%) and image degradation (e.g., blurring the image by a factor of 3 with a probability of 20%). We also adjusted image appearance by randomly manipulating the statistical characteristics of the image intensities such as brightness, saturation, hue, and contrast by a factor of ±plus-or-minus\\pm± 0.2. To optimize computational resources and time, object detection models uniformly resize images to dimensions of 256 x 256 pixels. These models were trained on a Windows 11 Machine (8 GB RAM) with 24 GB NVIDIA GeForce RTX 3090 graphics unit and 14th Gen Intel Core i5-14600 processor (14 cores, 20 threads, 2.7GHZ to 5.2GHz turbo frequency). The hyperparameters of the model (i.e., batch size, learning rate, training epochs) were tuned to ensure optimal performance using the Neural Network Intelligence software (Supplementary Table 1-7) [48]. The main characteristics of each model are described in Table 2."
        ]
    },
    "Sx3.T2.1.1.3.2.8.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.3.2.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.3.2.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.3.2.8.1.1.1\">Cross Entropy + Smooth L1</td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Characteristics of object detection models to detect the site of injury on porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The injury localization dataset was divided into 3 splits with roughly 80% of both pre- and post-injury images for training, 10% for validation, and 10% for testing. We ensured that all images taken from each subject were placed in the same group to prevent overfitting during training, and we balanced the distribution of healthy and injured spinal cord images in each subset. To improve the robustness of the models, we performed a number of data augmentations in the training process. This includes image transformations (e.g., rotating the samples with a degree range of ±plus-or-minus\\pm± 25, horizontal flipping with a probability of 0.5, and randomly zooming in or out of the image by 30%) and image degradation (e.g., blurring the image by a factor of 3 with a probability of 20%). We also adjusted image appearance by randomly manipulating the statistical characteristics of the image intensities such as brightness, saturation, hue, and contrast by a factor of ±plus-or-minus\\pm± 0.2. To optimize computational resources and time, object detection models uniformly resize images to dimensions of 256 x 256 pixels. These models were trained on a Windows 11 Machine (8 GB RAM) with 24 GB NVIDIA GeForce RTX 3090 graphics unit and 14th Gen Intel Core i5-14600 processor (14 cores, 20 threads, 2.7GHZ to 5.2GHz turbo frequency). The hyperparameters of the model (i.e., batch size, learning rate, training epochs) were tuned to ensure optimal performance using the Neural Network Intelligence software (Supplementary Table 1-7) [48]. The main characteristics of each model are described in Table 2."
        ]
    },
    "Sx3.T2.1.1.4.3.8.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.4.3.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.4.3.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.4.3.8.1.1.1\">Cross Entropy + Smooth L1</td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Characteristics of object detection models to detect the site of injury on porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The injury localization dataset was divided into 3 splits with roughly 80% of both pre- and post-injury images for training, 10% for validation, and 10% for testing. We ensured that all images taken from each subject were placed in the same group to prevent overfitting during training, and we balanced the distribution of healthy and injured spinal cord images in each subset. To improve the robustness of the models, we performed a number of data augmentations in the training process. This includes image transformations (e.g., rotating the samples with a degree range of ±plus-or-minus\\pm± 25, horizontal flipping with a probability of 0.5, and randomly zooming in or out of the image by 30%) and image degradation (e.g., blurring the image by a factor of 3 with a probability of 20%). We also adjusted image appearance by randomly manipulating the statistical characteristics of the image intensities such as brightness, saturation, hue, and contrast by a factor of ±plus-or-minus\\pm± 0.2. To optimize computational resources and time, object detection models uniformly resize images to dimensions of 256 x 256 pixels. These models were trained on a Windows 11 Machine (8 GB RAM) with 24 GB NVIDIA GeForce RTX 3090 graphics unit and 14th Gen Intel Core i5-14600 processor (14 cores, 20 threads, 2.7GHZ to 5.2GHz turbo frequency). The hyperparameters of the model (i.e., batch size, learning rate, training epochs) were tuned to ensure optimal performance using the Neural Network Intelligence software (Supplementary Table 1-7) [48]. The main characteristics of each model are described in Table 2."
        ]
    },
    "Sx3.T2.1.1.6.5.8.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.6.5.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.6.5.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.6.5.8.1.1.1\">Cross Entropy + Smooth L1</td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Characteristics of object detection models to detect the site of injury on porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The injury localization dataset was divided into 3 splits with roughly 80% of both pre- and post-injury images for training, 10% for validation, and 10% for testing. We ensured that all images taken from each subject were placed in the same group to prevent overfitting during training, and we balanced the distribution of healthy and injured spinal cord images in each subset. To improve the robustness of the models, we performed a number of data augmentations in the training process. This includes image transformations (e.g., rotating the samples with a degree range of ±plus-or-minus\\pm± 25, horizontal flipping with a probability of 0.5, and randomly zooming in or out of the image by 30%) and image degradation (e.g., blurring the image by a factor of 3 with a probability of 20%). We also adjusted image appearance by randomly manipulating the statistical characteristics of the image intensities such as brightness, saturation, hue, and contrast by a factor of ±plus-or-minus\\pm± 0.2. To optimize computational resources and time, object detection models uniformly resize images to dimensions of 256 x 256 pixels. These models were trained on a Windows 11 Machine (8 GB RAM) with 24 GB NVIDIA GeForce RTX 3090 graphics unit and 14th Gen Intel Core i5-14600 processor (14 cores, 20 threads, 2.7GHZ to 5.2GHz turbo frequency). The hyperparameters of the model (i.e., batch size, learning rate, training epochs) were tuned to ensure optimal performance using the Neural Network Intelligence software (Supplementary Table 1-7) [48]. The main characteristics of each model are described in Table 2."
        ]
    },
    "Sx3.T2.1.1.7.6.8.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.7.6.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.7.6.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.7.6.8.1.1.1\">Binary Cross Entropy + Mean Square Error</td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Characteristics of object detection models to detect the site of injury on porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The injury localization dataset was divided into 3 splits with roughly 80% of both pre- and post-injury images for training, 10% for validation, and 10% for testing. We ensured that all images taken from each subject were placed in the same group to prevent overfitting during training, and we balanced the distribution of healthy and injured spinal cord images in each subset. To improve the robustness of the models, we performed a number of data augmentations in the training process. This includes image transformations (e.g., rotating the samples with a degree range of ±plus-or-minus\\pm± 25, horizontal flipping with a probability of 0.5, and randomly zooming in or out of the image by 30%) and image degradation (e.g., blurring the image by a factor of 3 with a probability of 20%). We also adjusted image appearance by randomly manipulating the statistical characteristics of the image intensities such as brightness, saturation, hue, and contrast by a factor of ±plus-or-minus\\pm± 0.2. To optimize computational resources and time, object detection models uniformly resize images to dimensions of 256 x 256 pixels. These models were trained on a Windows 11 Machine (8 GB RAM) with 24 GB NVIDIA GeForce RTX 3090 graphics unit and 14th Gen Intel Core i5-14600 processor (14 cores, 20 threads, 2.7GHZ to 5.2GHz turbo frequency). The hyperparameters of the model (i.e., batch size, learning rate, training epochs) were tuned to ensure optimal performance using the Neural Network Intelligence software (Supplementary Table 1-7) [48]. The main characteristics of each model are described in Table 2."
        ]
    },
    "Sx3.T2.1.1.8.7.7.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.8.7.7.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.8.7.7.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.8.7.7.1.1.1\">AdamW</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.8.7.7.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.8.7.7.1.2.1\">+ SGD</td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Characteristics of object detection models to detect the site of injury on porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The injury localization dataset was divided into 3 splits with roughly 80% of both pre- and post-injury images for training, 10% for validation, and 10% for testing. We ensured that all images taken from each subject were placed in the same group to prevent overfitting during training, and we balanced the distribution of healthy and injured spinal cord images in each subset. To improve the robustness of the models, we performed a number of data augmentations in the training process. This includes image transformations (e.g., rotating the samples with a degree range of ±plus-or-minus\\pm± 25, horizontal flipping with a probability of 0.5, and randomly zooming in or out of the image by 30%) and image degradation (e.g., blurring the image by a factor of 3 with a probability of 20%). We also adjusted image appearance by randomly manipulating the statistical characteristics of the image intensities such as brightness, saturation, hue, and contrast by a factor of ±plus-or-minus\\pm± 0.2. To optimize computational resources and time, object detection models uniformly resize images to dimensions of 256 x 256 pixels. These models were trained on a Windows 11 Machine (8 GB RAM) with 24 GB NVIDIA GeForce RTX 3090 graphics unit and 14th Gen Intel Core i5-14600 processor (14 cores, 20 threads, 2.7GHZ to 5.2GHz turbo frequency). The hyperparameters of the model (i.e., batch size, learning rate, training epochs) were tuned to ensure optimal performance using the Neural Network Intelligence software (Supplementary Table 1-7) [48]. The main characteristics of each model are described in Table 2."
        ]
    },
    "Sx3.T2.1.1.8.7.8.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T2.1.1.8.7.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.8.7.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.8.7.8.1.1.1\">Binary Cross Entropy + Distribution Focal</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T2.1.1.8.7.8.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T2.1.1.8.7.8.1.2.1\">Loss + Complete Intersection over Union</td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Characteristics of object detection models to detect the site of injury on porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The injury localization dataset was divided into 3 splits with roughly 80% of both pre- and post-injury images for training, 10% for validation, and 10% for testing. We ensured that all images taken from each subject were placed in the same group to prevent overfitting during training, and we balanced the distribution of healthy and injured spinal cord images in each subset. To improve the robustness of the models, we performed a number of data augmentations in the training process. This includes image transformations (e.g., rotating the samples with a degree range of ±plus-or-minus\\pm± 25, horizontal flipping with a probability of 0.5, and randomly zooming in or out of the image by 30%) and image degradation (e.g., blurring the image by a factor of 3 with a probability of 20%). We also adjusted image appearance by randomly manipulating the statistical characteristics of the image intensities such as brightness, saturation, hue, and contrast by a factor of ±plus-or-minus\\pm± 0.2. To optimize computational resources and time, object detection models uniformly resize images to dimensions of 256 x 256 pixels. These models were trained on a Windows 11 Machine (8 GB RAM) with 24 GB NVIDIA GeForce RTX 3090 graphics unit and 14th Gen Intel Core i5-14600 processor (14 cores, 20 threads, 2.7GHZ to 5.2GHz turbo frequency). The hyperparameters of the model (i.e., batch size, learning rate, training epochs) were tuned to ensure optimal performance using the Neural Network Intelligence software (Supplementary Table 1-7) [48]. The main characteristics of each model are described in Table 2."
        ]
    },
    "Sx3.T3.1.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"Sx3.T3.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.2.1\">Encoder</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.1.1.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T3.1.1.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.1.1.3.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.3.1.1.1.1\"># of</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.1.1.3.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.3.1.2.1.1\">Parameters</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.1.1.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T3.1.1.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.1.1.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.4.1.1.1.1\">Learning</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.1.1.4.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.4.1.2.1.1\">Rate</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.1.1.5\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T3.1.1.1.1.5.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1.5.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.1.1.5.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.5.1.1.1.1\">Batch</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1.5.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.1.1.5.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.5.1.2.1.1\">Size</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.1.1.6\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T3.1.1.1.1.6.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1.6.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.1.1.6.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.6.1.1.1.1\">Training</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1.6.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.1.1.6.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.6.1.2.1.1\">Epochs</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.7.1\">Optimizer</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.1.1.8\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.8.1\">Loss Function</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.2.1.1\">SegFormer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.2.1.2\">MiT-B5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.2.1.3\">84,601,034</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.2.1.4\">0.000972</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.2.1.5\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.2.1.6\">75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.2.1.7\">AdamW</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.2.1.8\">Cross Entropy</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.3.2.1\">U-Net</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.3.2.2\">ResNet50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.3.2.3\">31,044,106</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.3.2.4\">0.003011</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.3.2.5\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.3.2.6\">50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.3.2.7\">SGD</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.3.2.8\">Cross Entropy</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.4.3.1\">DeepLabv3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.4.3.2\">ResNet50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.4.3.3\">41,998,420</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.4.3.4\">0.000334</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.4.3.5\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.4.3.6\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.4.3.7\">AdamW</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.4.3.8\">Cross Entropy</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.5.4.1\">TransUNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.5.4.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T3.1.1.5.4.2.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.5.4.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.5.4.2.1.1.1\">ResNet50 + ViT_B16</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.5.4.3\">105,323,306</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.5.4.4\">0.004468</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.5.4.5\">24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.5.4.6\">200</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.5.4.7\">SGD</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.5.4.8\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T3.1.1.5.4.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.5.4.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.5.4.8.1.1.1\">Cross Entropy + Dice</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.6.5.1\">Swin-UNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.6.5.2\">Swin-T</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.6.5.3\">27,153,156</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.6.5.4\">0.061411</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.6.5.5\">16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.6.5.6\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.6.5.7\">SGD</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.6.5.8\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T3.1.1.6.5.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.6.5.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.6.5.8.1.1.1\">Cross Entropy + Dice</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.7.6.1\">SAMed</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.7.6.2\">SAM ViT-B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.7.6.3\">91,866,903</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.7.6.4\">0.002840</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.7.6.5\">24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.7.6.6\">200</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.7.6.7\">AdamW</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx3.T3.1.1.7.6.8\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T3.1.1.7.6.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.7.6.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.7.6.8.1.1.1\">Cross Entropy + Dice</td>\n</tr>\n</table>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 3:  Characteristics of semantic segmentation models for segmenting porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The segmentation dataset was again randomly subdivided into a roughly 80-10-10 split for training, validation, and testing, respectively. All images taken from the same porcine spine were included in a single subset (train, validation, or test) to avoid overlap and prevent artificially inflated results. Additionally, each subset contained an approximately equal number of healthy and injured spinal cords. The training dataset was augmented to improve the models’ generalizability with image transformations (e.g., rotation [±plus-or-minus\\pm± 50-degree limit], horizontal flipping [p = 0.5], image compression [p = 0.2]), and image degradation (random brightness contrast [p = 0.2], random fog [p = 0.2]). For computational and time efficiency, all segmentation models resize the images to 256 × 256 pixels. The hyperparameters (i.e., batch size, learning rate, training epochs) were tuned in the same manner as the object detectors, using the same Windows machine for training (Supplementary Table 8-13) [48]. Table 3 describes the main characteristics of each semantic segmentation model."
        ]
    },
    "Sx3.T3.1.1.1.1.3.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T3.1.1.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.1.1.3.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.3.1.1.1.1\"># of</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.1.1.3.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.3.1.2.1.1\">Parameters</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 3:  Characteristics of semantic segmentation models for segmenting porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The segmentation dataset was again randomly subdivided into a roughly 80-10-10 split for training, validation, and testing, respectively. All images taken from the same porcine spine were included in a single subset (train, validation, or test) to avoid overlap and prevent artificially inflated results. Additionally, each subset contained an approximately equal number of healthy and injured spinal cords. The training dataset was augmented to improve the models’ generalizability with image transformations (e.g., rotation [±plus-or-minus\\pm± 50-degree limit], horizontal flipping [p = 0.5], image compression [p = 0.2]), and image degradation (random brightness contrast [p = 0.2], random fog [p = 0.2]). For computational and time efficiency, all segmentation models resize the images to 256 × 256 pixels. The hyperparameters (i.e., batch size, learning rate, training epochs) were tuned in the same manner as the object detectors, using the same Windows machine for training (Supplementary Table 8-13) [48]. Table 3 describes the main characteristics of each semantic segmentation model."
        ]
    },
    "Sx3.T3.1.1.1.1.4.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T3.1.1.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.1.1.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.4.1.1.1.1\">Learning</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.1.1.4.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.4.1.2.1.1\">Rate</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 3:  Characteristics of semantic segmentation models for segmenting porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The segmentation dataset was again randomly subdivided into a roughly 80-10-10 split for training, validation, and testing, respectively. All images taken from the same porcine spine were included in a single subset (train, validation, or test) to avoid overlap and prevent artificially inflated results. Additionally, each subset contained an approximately equal number of healthy and injured spinal cords. The training dataset was augmented to improve the models’ generalizability with image transformations (e.g., rotation [±plus-or-minus\\pm± 50-degree limit], horizontal flipping [p = 0.5], image compression [p = 0.2]), and image degradation (random brightness contrast [p = 0.2], random fog [p = 0.2]). For computational and time efficiency, all segmentation models resize the images to 256 × 256 pixels. The hyperparameters (i.e., batch size, learning rate, training epochs) were tuned in the same manner as the object detectors, using the same Windows machine for training (Supplementary Table 8-13) [48]. Table 3 describes the main characteristics of each semantic segmentation model."
        ]
    },
    "Sx3.T3.1.1.1.1.5.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T3.1.1.1.1.5.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1.5.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.1.1.5.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.5.1.1.1.1\">Batch</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1.5.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.1.1.5.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.5.1.2.1.1\">Size</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 3:  Characteristics of semantic segmentation models for segmenting porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The segmentation dataset was again randomly subdivided into a roughly 80-10-10 split for training, validation, and testing, respectively. All images taken from the same porcine spine were included in a single subset (train, validation, or test) to avoid overlap and prevent artificially inflated results. Additionally, each subset contained an approximately equal number of healthy and injured spinal cords. The training dataset was augmented to improve the models’ generalizability with image transformations (e.g., rotation [±plus-or-minus\\pm± 50-degree limit], horizontal flipping [p = 0.5], image compression [p = 0.2]), and image degradation (random brightness contrast [p = 0.2], random fog [p = 0.2]). For computational and time efficiency, all segmentation models resize the images to 256 × 256 pixels. The hyperparameters (i.e., batch size, learning rate, training epochs) were tuned in the same manner as the object detectors, using the same Windows machine for training (Supplementary Table 8-13) [48]. Table 3 describes the main characteristics of each semantic segmentation model."
        ]
    },
    "Sx3.T3.1.1.1.1.6.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T3.1.1.1.1.6.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1.6.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.1.1.6.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.6.1.1.1.1\">Training</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.1.1.6.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.1.1.6.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx3.T3.1.1.1.1.6.1.2.1.1\">Epochs</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 3:  Characteristics of semantic segmentation models for segmenting porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The segmentation dataset was again randomly subdivided into a roughly 80-10-10 split for training, validation, and testing, respectively. All images taken from the same porcine spine were included in a single subset (train, validation, or test) to avoid overlap and prevent artificially inflated results. Additionally, each subset contained an approximately equal number of healthy and injured spinal cords. The training dataset was augmented to improve the models’ generalizability with image transformations (e.g., rotation [±plus-or-minus\\pm± 50-degree limit], horizontal flipping [p = 0.5], image compression [p = 0.2]), and image degradation (random brightness contrast [p = 0.2], random fog [p = 0.2]). For computational and time efficiency, all segmentation models resize the images to 256 × 256 pixels. The hyperparameters (i.e., batch size, learning rate, training epochs) were tuned in the same manner as the object detectors, using the same Windows machine for training (Supplementary Table 8-13) [48]. Table 3 describes the main characteristics of each semantic segmentation model."
        ]
    },
    "Sx3.T3.1.1.5.4.2.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T3.1.1.5.4.2.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.5.4.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.5.4.2.1.1.1\">ResNet50 + ViT_B16</td>\n</tr>\n</table>\n\n",
        "caption": "Table 3:  Characteristics of semantic segmentation models for segmenting porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The segmentation dataset was again randomly subdivided into a roughly 80-10-10 split for training, validation, and testing, respectively. All images taken from the same porcine spine were included in a single subset (train, validation, or test) to avoid overlap and prevent artificially inflated results. Additionally, each subset contained an approximately equal number of healthy and injured spinal cords. The training dataset was augmented to improve the models’ generalizability with image transformations (e.g., rotation [±plus-or-minus\\pm± 50-degree limit], horizontal flipping [p = 0.5], image compression [p = 0.2]), and image degradation (random brightness contrast [p = 0.2], random fog [p = 0.2]). For computational and time efficiency, all segmentation models resize the images to 256 × 256 pixels. The hyperparameters (i.e., batch size, learning rate, training epochs) were tuned in the same manner as the object detectors, using the same Windows machine for training (Supplementary Table 8-13) [48]. Table 3 describes the main characteristics of each semantic segmentation model."
        ]
    },
    "Sx3.T3.1.1.5.4.8.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T3.1.1.5.4.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.5.4.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.5.4.8.1.1.1\">Cross Entropy + Dice</td>\n</tr>\n</table>\n\n",
        "caption": "Table 3:  Characteristics of semantic segmentation models for segmenting porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The segmentation dataset was again randomly subdivided into a roughly 80-10-10 split for training, validation, and testing, respectively. All images taken from the same porcine spine were included in a single subset (train, validation, or test) to avoid overlap and prevent artificially inflated results. Additionally, each subset contained an approximately equal number of healthy and injured spinal cords. The training dataset was augmented to improve the models’ generalizability with image transformations (e.g., rotation [±plus-or-minus\\pm± 50-degree limit], horizontal flipping [p = 0.5], image compression [p = 0.2]), and image degradation (random brightness contrast [p = 0.2], random fog [p = 0.2]). For computational and time efficiency, all segmentation models resize the images to 256 × 256 pixels. The hyperparameters (i.e., batch size, learning rate, training epochs) were tuned in the same manner as the object detectors, using the same Windows machine for training (Supplementary Table 8-13) [48]. Table 3 describes the main characteristics of each semantic segmentation model."
        ]
    },
    "Sx3.T3.1.1.6.5.8.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T3.1.1.6.5.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.6.5.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.6.5.8.1.1.1\">Cross Entropy + Dice</td>\n</tr>\n</table>\n\n",
        "caption": "Table 3:  Characteristics of semantic segmentation models for segmenting porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The segmentation dataset was again randomly subdivided into a roughly 80-10-10 split for training, validation, and testing, respectively. All images taken from the same porcine spine were included in a single subset (train, validation, or test) to avoid overlap and prevent artificially inflated results. Additionally, each subset contained an approximately equal number of healthy and injured spinal cords. The training dataset was augmented to improve the models’ generalizability with image transformations (e.g., rotation [±plus-or-minus\\pm± 50-degree limit], horizontal flipping [p = 0.5], image compression [p = 0.2]), and image degradation (random brightness contrast [p = 0.2], random fog [p = 0.2]). For computational and time efficiency, all segmentation models resize the images to 256 × 256 pixels. The hyperparameters (i.e., batch size, learning rate, training epochs) were tuned in the same manner as the object detectors, using the same Windows machine for training (Supplementary Table 8-13) [48]. Table 3 describes the main characteristics of each semantic segmentation model."
        ]
    },
    "Sx3.T3.1.1.7.6.8.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx3.T3.1.1.7.6.8.1\">\n<tr class=\"ltx_tr\" id=\"Sx3.T3.1.1.7.6.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx3.T3.1.1.7.6.8.1.1.1\">Cross Entropy + Dice</td>\n</tr>\n</table>\n\n",
        "caption": "Table 3:  Characteristics of semantic segmentation models for segmenting porcine spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "The segmentation dataset was again randomly subdivided into a roughly 80-10-10 split for training, validation, and testing, respectively. All images taken from the same porcine spine were included in a single subset (train, validation, or test) to avoid overlap and prevent artificially inflated results. Additionally, each subset contained an approximately equal number of healthy and injured spinal cords. The training dataset was augmented to improve the models’ generalizability with image transformations (e.g., rotation [±plus-or-minus\\pm± 50-degree limit], horizontal flipping [p = 0.5], image compression [p = 0.2]), and image degradation (random brightness contrast [p = 0.2], random fog [p = 0.2]). For computational and time efficiency, all segmentation models resize the images to 256 × 256 pixels. The hyperparameters (i.e., batch size, learning rate, training epochs) were tuned in the same manner as the object detectors, using the same Windows machine for training (Supplementary Table 8-13) [48]. Table 3 describes the main characteristics of each semantic segmentation model."
        ]
    },
    "Sx4.T4.1.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T4.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.1.1.1.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.1.1.2\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.1.1.2.1\">mAP50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.1.1.3\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.1.1.3.1\">mAP50-95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.1.1.4\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.1.1.4.1\">AR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\" id=\"Sx4.T4.1.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.1.1.5.1\">CPU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\" id=\"Sx4.T4.1.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.1.1.6.1\">GPU</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.2.2.1.1\">FPS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.2.2.2.1\">Load (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.2.2.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T4.1.1.2.2.3.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.2.2.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T4.1.1.2.2.3.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.2.2.3.1.1.1.1\">Implantability</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.2.2.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T4.1.1.2.2.3.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.2.2.3.1.2.1.1\">Score</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.2.2.4.1\">FPS</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.2.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.2.2.5.1\">Load (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.2.2.6\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T4.1.1.2.2.6.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.2.2.6.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T4.1.1.2.2.6.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.2.2.6.1.1.1.1\">Implantability</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.2.2.6.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T4.1.1.2.2.6.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.2.2.6.1.2.1.1\">Score</span></td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.3.3.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T4.1.1.3.3.1.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.3.3.1.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T4.1.1.3.3.1.1.1.1\">Faster RCNN</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.3.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.3.3.2.1\">0.985</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.3.3.3\">0.524</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.3.3.4\">0.594</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.3.3.5\">1.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.3.3.6\">33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.3.3.7\">0.676</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.3.3.8\">14.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.3.3.9\">62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.3.3.10\">0.613</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.4.4.1\">SSD300</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.4.4.2\">0.669</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.4.4.3\">0.207</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.4.4.4\">0.249</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.4.4.5\">23.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.4.4.6\">36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.4.4.7\">0.735</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.4.4.8\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.4.4.8.1\">147.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.4.4.9\">27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.4.4.10\">0.766</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.5.5.1\">SSD512</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.5.5.2\">0.874</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.5.5.3\">0.274</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.5.5.4\">0.324</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.5.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.5.5.5.1\">23.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.5.5.6\">26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.5.5.7\">0.866</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.5.5.8\">125.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.5.5.9\">18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.5.5.10\">0.853</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.6.6.1\">RetinaNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.6.6.2\">0.912</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.6.6.3\">0.264</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.6.6.4\">0.426</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.6.6.5\">1.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.6.6.6\">32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.6.6.7\">0.646</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.6.6.8\">17.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.6.6.9\">48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.6.6.10\">0.616</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.7.7.1\">DETR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.7.7.2\">0.787</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.7.7.3\">0.251</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.7.7.4\">0.453</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.7.7.5\">19.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.7.7.6\">15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.7.7.7\">0.812</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.7.7.8\">114.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.7.7.9\">26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.7.7.10\">0.772</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.8.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.8.8.1\">YOLOv7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.8.8.2\">0.923</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.8.8.3\">0.439</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.8.8.4\">0.499</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.8.8.5\">19.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.8.8.6\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.8.8.7\">0.865</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.8.8.8\">80.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.8.8.9\">28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.8.8.10\">0.777</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.9.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.9.9.1\">YOLOv8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.9.9.2\">0.979</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.9.9.3\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.9.9.3.1\">0.606</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.9.9.4\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.9.9.4.1\">0.644</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.9.9.5\">17.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.9.9.6\">22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.9.9.7\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.9.9.7.1\">0.870</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.9.9.8\">115.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.9.9.9\">27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T4.1.1.9.9.10\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.9.9.10.1\">0.867</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 4:  Performance of object detection models on unseen porcine spinal cord ultrasound images to detect the site of injury.",
        "footnotes": [],
        "references": [
            "After hyperparameter tuning, the performance of each model during inference is shown in Table 4, including the mean Average Precision (mAP) value, the Average Recall (AR) value, and speed using frames per second (FPS). mAP is a popular metric ranging from 0 to 1 that measures the accuracy of object detectors as it provides a comprehensive assessment of their performance, considering both precision (how accurate the detected objected are) and recall (how many relevant objects are detected) at multiple levels of confidence (i.e., 50%, and 50-95% at intervals of 5%). The levels of confidence, known as the Intersection over Union (IoU) thresholds, consider a detection accurate if the IoU of the ground truth bounding box and the predicted bounding box is greater than the set threshold (e.g., 0.5 for mAP50). The AR value is the recall metric averaged over a range of IoU threshold (0.5 - 1.0). From these results, it is evident that Faster RCNN and YOLOv8 show the strongest performance, achieving a mAP50 score of 0.985 and 0.979, respectively. These models also achieve the highest mAP50-95 score, which is a much more stringent metric for assessing model performance compared to mAP50, at 0.524 for Faster RCNN and 0.606 for YOLOv8. YOLOv8 also attains the highest AR score at 0.644."
        ]
    },
    "Sx4.T4.1.1.2.2.3.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T4.1.1.2.2.3.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.2.2.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T4.1.1.2.2.3.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.2.2.3.1.1.1.1\">Implantability</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.2.2.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T4.1.1.2.2.3.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.2.2.3.1.2.1.1\">Score</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 4:  Performance of object detection models on unseen porcine spinal cord ultrasound images to detect the site of injury.",
        "footnotes": [],
        "references": [
            "After hyperparameter tuning, the performance of each model during inference is shown in Table 4, including the mean Average Precision (mAP) value, the Average Recall (AR) value, and speed using frames per second (FPS). mAP is a popular metric ranging from 0 to 1 that measures the accuracy of object detectors as it provides a comprehensive assessment of their performance, considering both precision (how accurate the detected objected are) and recall (how many relevant objects are detected) at multiple levels of confidence (i.e., 50%, and 50-95% at intervals of 5%). The levels of confidence, known as the Intersection over Union (IoU) thresholds, consider a detection accurate if the IoU of the ground truth bounding box and the predicted bounding box is greater than the set threshold (e.g., 0.5 for mAP50). The AR value is the recall metric averaged over a range of IoU threshold (0.5 - 1.0). From these results, it is evident that Faster RCNN and YOLOv8 show the strongest performance, achieving a mAP50 score of 0.985 and 0.979, respectively. These models also achieve the highest mAP50-95 score, which is a much more stringent metric for assessing model performance compared to mAP50, at 0.524 for Faster RCNN and 0.606 for YOLOv8. YOLOv8 also attains the highest AR score at 0.644."
        ]
    },
    "Sx4.T4.1.1.2.2.6.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T4.1.1.2.2.6.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.2.2.6.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T4.1.1.2.2.6.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.2.2.6.1.1.1.1\">Implantability</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.2.2.6.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T4.1.1.2.2.6.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T4.1.1.2.2.6.1.2.1.1\">Score</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 4:  Performance of object detection models on unseen porcine spinal cord ultrasound images to detect the site of injury.",
        "footnotes": [],
        "references": [
            "After hyperparameter tuning, the performance of each model during inference is shown in Table 4, including the mean Average Precision (mAP) value, the Average Recall (AR) value, and speed using frames per second (FPS). mAP is a popular metric ranging from 0 to 1 that measures the accuracy of object detectors as it provides a comprehensive assessment of their performance, considering both precision (how accurate the detected objected are) and recall (how many relevant objects are detected) at multiple levels of confidence (i.e., 50%, and 50-95% at intervals of 5%). The levels of confidence, known as the Intersection over Union (IoU) thresholds, consider a detection accurate if the IoU of the ground truth bounding box and the predicted bounding box is greater than the set threshold (e.g., 0.5 for mAP50). The AR value is the recall metric averaged over a range of IoU threshold (0.5 - 1.0). From these results, it is evident that Faster RCNN and YOLOv8 show the strongest performance, achieving a mAP50 score of 0.985 and 0.979, respectively. These models also achieve the highest mAP50-95 score, which is a much more stringent metric for assessing model performance compared to mAP50, at 0.524 for Faster RCNN and 0.606 for YOLOv8. YOLOv8 also attains the highest AR score at 0.644."
        ]
    },
    "Sx4.T4.1.1.3.3.1.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T4.1.1.3.3.1.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T4.1.1.3.3.1.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T4.1.1.3.3.1.1.1.1\">Faster RCNN</td>\n</tr>\n</table>\n\n",
        "caption": "Table 4:  Performance of object detection models on unseen porcine spinal cord ultrasound images to detect the site of injury.",
        "footnotes": [],
        "references": [
            "After hyperparameter tuning, the performance of each model during inference is shown in Table 4, including the mean Average Precision (mAP) value, the Average Recall (AR) value, and speed using frames per second (FPS). mAP is a popular metric ranging from 0 to 1 that measures the accuracy of object detectors as it provides a comprehensive assessment of their performance, considering both precision (how accurate the detected objected are) and recall (how many relevant objects are detected) at multiple levels of confidence (i.e., 50%, and 50-95% at intervals of 5%). The levels of confidence, known as the Intersection over Union (IoU) thresholds, consider a detection accurate if the IoU of the ground truth bounding box and the predicted bounding box is greater than the set threshold (e.g., 0.5 for mAP50). The AR value is the recall metric averaged over a range of IoU threshold (0.5 - 1.0). From these results, it is evident that Faster RCNN and YOLOv8 show the strongest performance, achieving a mAP50 score of 0.985 and 0.979, respectively. These models also achieve the highest mAP50-95 score, which is a much more stringent metric for assessing model performance compared to mAP50, at 0.524 for Faster RCNN and 0.606 for YOLOv8. YOLOv8 also attains the highest AR score at 0.644."
        ]
    },
    "Sx4.T5.1.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"Sx4.T5.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" id=\"Sx4.T5.1.1.1.1.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T5.1.1.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.1.1.2.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.2.1.1.1.1\">Porcine</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.1.1.2.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.2.1.2.1.1\">anatomy</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" id=\"Sx4.T5.1.1.1.1.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T5.1.1.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.1.1.3.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.3.1.1.1.1\">Porcine</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.1.1.3.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.3.1.2.1.1\">spinal cord</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" id=\"Sx4.T5.1.1.1.1.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T5.1.1.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.1.1.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.4.1.1.1.1\">Human</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.1.1.4.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.4.1.2.1.1\">anatomy</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" id=\"Sx4.T5.1.1.1.1.5\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T5.1.1.1.1.5.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1.5.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.1.1.5.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.5.1.1.1.1\">Human</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1.5.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.1.1.5.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.5.1.2.1.1\">spinal cord</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\" id=\"Sx4.T5.1.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.6.1\">CPU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\" id=\"Sx4.T5.1.1.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.7.1\">GPU</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.1.1\">MIoU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.2.1\">Dice</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.2.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.3.1\">IoU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.4.1\">Dice</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.2.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.5.1\">MIoU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.2.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.6.1\">Dice</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.2.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.7.1\">IoU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.2.2.8\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.8.1\">Dice</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.2.2.9\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.9.1\">FPS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.2.2.10\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T5.1.1.2.2.10.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2.10.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.2.2.10.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.10.1.1.1.1\">Load</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2.10.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.2.2.10.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.10.1.2.1.1\">(%)</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.2.2.11\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T5.1.1.2.2.11.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2.11.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.2.2.11.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.11.1.1.1.1\">Implantability</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2.11.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.2.2.11.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.11.1.2.1.1\">Score</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.2.2.12\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.12.1\">FPS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.2.2.13\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T5.1.1.2.2.13.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2.13.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.2.2.13.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.13.1.1.1.1\">Load</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2.13.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.2.2.13.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.13.1.2.1.1\">(%)</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.2.2.14\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T5.1.1.2.2.14.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2.14.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.2.2.14.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.14.1.1.1.1\">Implantability</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2.14.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.2.2.14.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.14.1.2.1.1\">Score</span></td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.3.3.1\">SegFormer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.3.3.2\">0.493</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.3.3.3\">0.570</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.3.3.4\">0.906</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.3.3.5\">0.950</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.3.3.6\">0.232</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.3.3.7\">0.308</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.3.3.8\">0.666</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.3.3.9\">0.773</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.3.3.10\">3.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.3.3.11\">23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.3.3.12\">0.548</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.3.3.13\">23.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.3.3.14\">45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.3.3.15\">0.513</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.4.4.1\">U-Net</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.4.4.2\">0.476</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.4.4.3\">0.553</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.4.4.4\">0.867</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.4.4.5\">0.928</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.4.4.6\">0.253</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.4.4.7\">0.349</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.4.4.8\">0.609</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.4.4.9\">0.722</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.4.4.10\">6.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.4.4.11\">32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.4.4.12\">0.563</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.4.4.13\">62.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.4.4.14\">41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.4.4.15\">0.668</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.5.5.1\">DeepLabv3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.5.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.5.5.2.1\">0.515</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.5.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.5.5.3.1\">0.587</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.5.5.4\">0.910</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.5.5.5\">0.952</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.5.5.6\">0.200</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.5.5.7\">0.289</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.5.5.8\">0.506</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.5.5.9\">0.656</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.5.5.10\">4.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.5.5.11\">27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.5.5.12\">0.568</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.5.5.13\">64.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.5.5.14\">35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.5.5.15\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.5.5.15.1\">0.702</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.6.6.1\">TransUNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.6.6.2\">0.500</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.6.6.3\">0.573</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.6.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.6.6.4.1\">0.921</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.6.6.5\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.6.6.5.1\">0.958</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.6.6.6\">0.298</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.6.6.7\">0.388</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.6.6.8\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.6.6.8.1\">0.758</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.6.6.9\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.6.6.9.1\">0.853</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.6.6.10\">4.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.6.6.11\">35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.6.6.12\">0.532</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.6.6.13\">40.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.6.6.14\">34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.6.6.15\">0.609</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.7.7.1\">SwinUNet</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.7.7.2\">0.490</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.7.7.3\">0.562</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.7.7.4\">0.913</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.7.7.5\">0.954</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.7.7.6\">0.309</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.7.7.7\">0.401</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.7.7.8\">0.692</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.7.7.9\">0.783</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.7.7.10\">12.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.7.7.11\">31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.7.7.12\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.7.7.12.1\">0.699</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.7.7.13\">63.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.7.7.14\">34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.7.7.15\">0.690</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.8.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.8.8.1\">SAMed</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.8.8.2\">0.497</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.8.8.3\">0.574</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.8.8.4\">0.908</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.8.8.5\">0.951</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.8.8.6\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.8.8.6.1\">0.347</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.8.8.7\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.8.8.7.1\">0.445</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.8.8.8\">0.616</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.8.8.9\">0.740</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.8.8.10\">5.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.8.8.11\">37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.8.8.12\">0.535</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.8.8.13\">29.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.8.8.14\">35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"Sx4.T5.1.1.8.8.15\">0.563</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 5:  Performance of semantic segmentation models on unseen porcine and human spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "After hyperparameter tuning, the performance of each segmentation model during inference is shown in Table 5, including the Mean Intersection over Union (MIoU) value, speed, and computational load. MIoU is a measure of the overlap between the predicted segmentation and ground truth segmentation. The IoU for a single class (i.e., each anatomical structure) is calculated as the intersection of the predicted and true positive pixels divided by the union of predicted and true positive pixels and the MIoU is the average IoU across all classes. This provides a more comprehensive understanding of how well the model is performing across different classes. The Mean Dice coefficient, which is another metric between 0 and 1 that indicates the level of similarity between the predicted and ground truth masks, is also evaluated. It is calculated as two times the area of the intersection of the predicted and ground truth masks divided by the sum of the areas of the predicted and ground truth mask.",
            "Additionally, the poor zero-shot generalization to human images of these models is influenced by the characteristics and processing of our collected human dataset. While human and porcine spine are morphologically similar, suggesting high generalization accuracy across these species, in the images collected, most patients were undergoing a spinal cord shortening procedure, significantly affecting the curvature of the cord. Even with these drastic changes to spinal cord geometry, SegFormer, TransUNet, SwinUNet, and SAMed were able to segment the spinal cord with high accuracy (Dice >>> 0.74). However, because the CSF space is much larger in humans compared to porcine subjects (that were not undergoing PVCSO), after data preprocessing some anatomical classes were not visible in the frame, such as the dorsal space, dura, and ventral space (Figure 6). This may have a significant impact on the MIoU and Dice scores in Table 5, and should be further explored. Moreover, we had no examples of the hematoma, dura/pia compelex, or dura/ventral space classes in this human inference dataset. Future efforts to improve human generalization can include transfer learning techniques to adapt the deep learning model trained on porcine data for human applications. Employing domain adaptation techniques, such as matching component analysis, cannoncial correlation analysis, or optimal transport, can effectively augment porcine data into \"synthetic\" human data, enhancing the model’s ability to generalize across species [66, 67]. These approaches would rely on a large set of porcine images and a smaller set of human images that it can map onto, and the morphological similarities in these spinal cord anatomies are favorable as transfer learning typically performs best across domains with related underlying distributions [66, 55]. As ultrasound evaluation continues to become the standard-of-care in SCI, there will be increased datasets of human spinal cord images to better inform and train these algorithms [13]."
        ]
    },
    "Sx4.T5.1.1.1.1.2.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T5.1.1.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.1.1.2.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.2.1.1.1.1\">Porcine</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.1.1.2.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.2.1.2.1.1\">anatomy</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 5:  Performance of semantic segmentation models on unseen porcine and human spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "After hyperparameter tuning, the performance of each segmentation model during inference is shown in Table 5, including the Mean Intersection over Union (MIoU) value, speed, and computational load. MIoU is a measure of the overlap between the predicted segmentation and ground truth segmentation. The IoU for a single class (i.e., each anatomical structure) is calculated as the intersection of the predicted and true positive pixels divided by the union of predicted and true positive pixels and the MIoU is the average IoU across all classes. This provides a more comprehensive understanding of how well the model is performing across different classes. The Mean Dice coefficient, which is another metric between 0 and 1 that indicates the level of similarity between the predicted and ground truth masks, is also evaluated. It is calculated as two times the area of the intersection of the predicted and ground truth masks divided by the sum of the areas of the predicted and ground truth mask.",
            "Additionally, the poor zero-shot generalization to human images of these models is influenced by the characteristics and processing of our collected human dataset. While human and porcine spine are morphologically similar, suggesting high generalization accuracy across these species, in the images collected, most patients were undergoing a spinal cord shortening procedure, significantly affecting the curvature of the cord. Even with these drastic changes to spinal cord geometry, SegFormer, TransUNet, SwinUNet, and SAMed were able to segment the spinal cord with high accuracy (Dice >>> 0.74). However, because the CSF space is much larger in humans compared to porcine subjects (that were not undergoing PVCSO), after data preprocessing some anatomical classes were not visible in the frame, such as the dorsal space, dura, and ventral space (Figure 6). This may have a significant impact on the MIoU and Dice scores in Table 5, and should be further explored. Moreover, we had no examples of the hematoma, dura/pia compelex, or dura/ventral space classes in this human inference dataset. Future efforts to improve human generalization can include transfer learning techniques to adapt the deep learning model trained on porcine data for human applications. Employing domain adaptation techniques, such as matching component analysis, cannoncial correlation analysis, or optimal transport, can effectively augment porcine data into \"synthetic\" human data, enhancing the model’s ability to generalize across species [66, 67]. These approaches would rely on a large set of porcine images and a smaller set of human images that it can map onto, and the morphological similarities in these spinal cord anatomies are favorable as transfer learning typically performs best across domains with related underlying distributions [66, 55]. As ultrasound evaluation continues to become the standard-of-care in SCI, there will be increased datasets of human spinal cord images to better inform and train these algorithms [13]."
        ]
    },
    "Sx4.T5.1.1.1.1.3.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T5.1.1.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.1.1.3.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.3.1.1.1.1\">Porcine</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.1.1.3.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.3.1.2.1.1\">spinal cord</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 5:  Performance of semantic segmentation models on unseen porcine and human spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "After hyperparameter tuning, the performance of each segmentation model during inference is shown in Table 5, including the Mean Intersection over Union (MIoU) value, speed, and computational load. MIoU is a measure of the overlap between the predicted segmentation and ground truth segmentation. The IoU for a single class (i.e., each anatomical structure) is calculated as the intersection of the predicted and true positive pixels divided by the union of predicted and true positive pixels and the MIoU is the average IoU across all classes. This provides a more comprehensive understanding of how well the model is performing across different classes. The Mean Dice coefficient, which is another metric between 0 and 1 that indicates the level of similarity between the predicted and ground truth masks, is also evaluated. It is calculated as two times the area of the intersection of the predicted and ground truth masks divided by the sum of the areas of the predicted and ground truth mask.",
            "Additionally, the poor zero-shot generalization to human images of these models is influenced by the characteristics and processing of our collected human dataset. While human and porcine spine are morphologically similar, suggesting high generalization accuracy across these species, in the images collected, most patients were undergoing a spinal cord shortening procedure, significantly affecting the curvature of the cord. Even with these drastic changes to spinal cord geometry, SegFormer, TransUNet, SwinUNet, and SAMed were able to segment the spinal cord with high accuracy (Dice >>> 0.74). However, because the CSF space is much larger in humans compared to porcine subjects (that were not undergoing PVCSO), after data preprocessing some anatomical classes were not visible in the frame, such as the dorsal space, dura, and ventral space (Figure 6). This may have a significant impact on the MIoU and Dice scores in Table 5, and should be further explored. Moreover, we had no examples of the hematoma, dura/pia compelex, or dura/ventral space classes in this human inference dataset. Future efforts to improve human generalization can include transfer learning techniques to adapt the deep learning model trained on porcine data for human applications. Employing domain adaptation techniques, such as matching component analysis, cannoncial correlation analysis, or optimal transport, can effectively augment porcine data into \"synthetic\" human data, enhancing the model’s ability to generalize across species [66, 67]. These approaches would rely on a large set of porcine images and a smaller set of human images that it can map onto, and the morphological similarities in these spinal cord anatomies are favorable as transfer learning typically performs best across domains with related underlying distributions [66, 55]. As ultrasound evaluation continues to become the standard-of-care in SCI, there will be increased datasets of human spinal cord images to better inform and train these algorithms [13]."
        ]
    },
    "Sx4.T5.1.1.1.1.4.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T5.1.1.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.1.1.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.4.1.1.1.1\">Human</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.1.1.4.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.4.1.2.1.1\">anatomy</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 5:  Performance of semantic segmentation models on unseen porcine and human spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "After hyperparameter tuning, the performance of each segmentation model during inference is shown in Table 5, including the Mean Intersection over Union (MIoU) value, speed, and computational load. MIoU is a measure of the overlap between the predicted segmentation and ground truth segmentation. The IoU for a single class (i.e., each anatomical structure) is calculated as the intersection of the predicted and true positive pixels divided by the union of predicted and true positive pixels and the MIoU is the average IoU across all classes. This provides a more comprehensive understanding of how well the model is performing across different classes. The Mean Dice coefficient, which is another metric between 0 and 1 that indicates the level of similarity between the predicted and ground truth masks, is also evaluated. It is calculated as two times the area of the intersection of the predicted and ground truth masks divided by the sum of the areas of the predicted and ground truth mask.",
            "Additionally, the poor zero-shot generalization to human images of these models is influenced by the characteristics and processing of our collected human dataset. While human and porcine spine are morphologically similar, suggesting high generalization accuracy across these species, in the images collected, most patients were undergoing a spinal cord shortening procedure, significantly affecting the curvature of the cord. Even with these drastic changes to spinal cord geometry, SegFormer, TransUNet, SwinUNet, and SAMed were able to segment the spinal cord with high accuracy (Dice >>> 0.74). However, because the CSF space is much larger in humans compared to porcine subjects (that were not undergoing PVCSO), after data preprocessing some anatomical classes were not visible in the frame, such as the dorsal space, dura, and ventral space (Figure 6). This may have a significant impact on the MIoU and Dice scores in Table 5, and should be further explored. Moreover, we had no examples of the hematoma, dura/pia compelex, or dura/ventral space classes in this human inference dataset. Future efforts to improve human generalization can include transfer learning techniques to adapt the deep learning model trained on porcine data for human applications. Employing domain adaptation techniques, such as matching component analysis, cannoncial correlation analysis, or optimal transport, can effectively augment porcine data into \"synthetic\" human data, enhancing the model’s ability to generalize across species [66, 67]. These approaches would rely on a large set of porcine images and a smaller set of human images that it can map onto, and the morphological similarities in these spinal cord anatomies are favorable as transfer learning typically performs best across domains with related underlying distributions [66, 55]. As ultrasound evaluation continues to become the standard-of-care in SCI, there will be increased datasets of human spinal cord images to better inform and train these algorithms [13]."
        ]
    },
    "Sx4.T5.1.1.1.1.5.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T5.1.1.1.1.5.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1.5.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.1.1.5.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.5.1.1.1.1\">Human</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.1.1.5.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.1.1.5.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.1.1.5.1.2.1.1\">spinal cord</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 5:  Performance of semantic segmentation models on unseen porcine and human spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "After hyperparameter tuning, the performance of each segmentation model during inference is shown in Table 5, including the Mean Intersection over Union (MIoU) value, speed, and computational load. MIoU is a measure of the overlap between the predicted segmentation and ground truth segmentation. The IoU for a single class (i.e., each anatomical structure) is calculated as the intersection of the predicted and true positive pixels divided by the union of predicted and true positive pixels and the MIoU is the average IoU across all classes. This provides a more comprehensive understanding of how well the model is performing across different classes. The Mean Dice coefficient, which is another metric between 0 and 1 that indicates the level of similarity between the predicted and ground truth masks, is also evaluated. It is calculated as two times the area of the intersection of the predicted and ground truth masks divided by the sum of the areas of the predicted and ground truth mask.",
            "Additionally, the poor zero-shot generalization to human images of these models is influenced by the characteristics and processing of our collected human dataset. While human and porcine spine are morphologically similar, suggesting high generalization accuracy across these species, in the images collected, most patients were undergoing a spinal cord shortening procedure, significantly affecting the curvature of the cord. Even with these drastic changes to spinal cord geometry, SegFormer, TransUNet, SwinUNet, and SAMed were able to segment the spinal cord with high accuracy (Dice >>> 0.74). However, because the CSF space is much larger in humans compared to porcine subjects (that were not undergoing PVCSO), after data preprocessing some anatomical classes were not visible in the frame, such as the dorsal space, dura, and ventral space (Figure 6). This may have a significant impact on the MIoU and Dice scores in Table 5, and should be further explored. Moreover, we had no examples of the hematoma, dura/pia compelex, or dura/ventral space classes in this human inference dataset. Future efforts to improve human generalization can include transfer learning techniques to adapt the deep learning model trained on porcine data for human applications. Employing domain adaptation techniques, such as matching component analysis, cannoncial correlation analysis, or optimal transport, can effectively augment porcine data into \"synthetic\" human data, enhancing the model’s ability to generalize across species [66, 67]. These approaches would rely on a large set of porcine images and a smaller set of human images that it can map onto, and the morphological similarities in these spinal cord anatomies are favorable as transfer learning typically performs best across domains with related underlying distributions [66, 55]. As ultrasound evaluation continues to become the standard-of-care in SCI, there will be increased datasets of human spinal cord images to better inform and train these algorithms [13]."
        ]
    },
    "Sx4.T5.1.1.2.2.10.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T5.1.1.2.2.10.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2.10.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.2.2.10.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.10.1.1.1.1\">Load</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2.10.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.2.2.10.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.10.1.2.1.1\">(%)</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 5:  Performance of semantic segmentation models on unseen porcine and human spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "After hyperparameter tuning, the performance of each segmentation model during inference is shown in Table 5, including the Mean Intersection over Union (MIoU) value, speed, and computational load. MIoU is a measure of the overlap between the predicted segmentation and ground truth segmentation. The IoU for a single class (i.e., each anatomical structure) is calculated as the intersection of the predicted and true positive pixels divided by the union of predicted and true positive pixels and the MIoU is the average IoU across all classes. This provides a more comprehensive understanding of how well the model is performing across different classes. The Mean Dice coefficient, which is another metric between 0 and 1 that indicates the level of similarity between the predicted and ground truth masks, is also evaluated. It is calculated as two times the area of the intersection of the predicted and ground truth masks divided by the sum of the areas of the predicted and ground truth mask.",
            "Additionally, the poor zero-shot generalization to human images of these models is influenced by the characteristics and processing of our collected human dataset. While human and porcine spine are morphologically similar, suggesting high generalization accuracy across these species, in the images collected, most patients were undergoing a spinal cord shortening procedure, significantly affecting the curvature of the cord. Even with these drastic changes to spinal cord geometry, SegFormer, TransUNet, SwinUNet, and SAMed were able to segment the spinal cord with high accuracy (Dice >>> 0.74). However, because the CSF space is much larger in humans compared to porcine subjects (that were not undergoing PVCSO), after data preprocessing some anatomical classes were not visible in the frame, such as the dorsal space, dura, and ventral space (Figure 6). This may have a significant impact on the MIoU and Dice scores in Table 5, and should be further explored. Moreover, we had no examples of the hematoma, dura/pia compelex, or dura/ventral space classes in this human inference dataset. Future efforts to improve human generalization can include transfer learning techniques to adapt the deep learning model trained on porcine data for human applications. Employing domain adaptation techniques, such as matching component analysis, cannoncial correlation analysis, or optimal transport, can effectively augment porcine data into \"synthetic\" human data, enhancing the model’s ability to generalize across species [66, 67]. These approaches would rely on a large set of porcine images and a smaller set of human images that it can map onto, and the morphological similarities in these spinal cord anatomies are favorable as transfer learning typically performs best across domains with related underlying distributions [66, 55]. As ultrasound evaluation continues to become the standard-of-care in SCI, there will be increased datasets of human spinal cord images to better inform and train these algorithms [13]."
        ]
    },
    "Sx4.T5.1.1.2.2.11.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T5.1.1.2.2.11.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2.11.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.2.2.11.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.11.1.1.1.1\">Implantability</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2.11.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.2.2.11.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.11.1.2.1.1\">Score</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 5:  Performance of semantic segmentation models on unseen porcine and human spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "After hyperparameter tuning, the performance of each segmentation model during inference is shown in Table 5, including the Mean Intersection over Union (MIoU) value, speed, and computational load. MIoU is a measure of the overlap between the predicted segmentation and ground truth segmentation. The IoU for a single class (i.e., each anatomical structure) is calculated as the intersection of the predicted and true positive pixels divided by the union of predicted and true positive pixels and the MIoU is the average IoU across all classes. This provides a more comprehensive understanding of how well the model is performing across different classes. The Mean Dice coefficient, which is another metric between 0 and 1 that indicates the level of similarity between the predicted and ground truth masks, is also evaluated. It is calculated as two times the area of the intersection of the predicted and ground truth masks divided by the sum of the areas of the predicted and ground truth mask.",
            "Additionally, the poor zero-shot generalization to human images of these models is influenced by the characteristics and processing of our collected human dataset. While human and porcine spine are morphologically similar, suggesting high generalization accuracy across these species, in the images collected, most patients were undergoing a spinal cord shortening procedure, significantly affecting the curvature of the cord. Even with these drastic changes to spinal cord geometry, SegFormer, TransUNet, SwinUNet, and SAMed were able to segment the spinal cord with high accuracy (Dice >>> 0.74). However, because the CSF space is much larger in humans compared to porcine subjects (that were not undergoing PVCSO), after data preprocessing some anatomical classes were not visible in the frame, such as the dorsal space, dura, and ventral space (Figure 6). This may have a significant impact on the MIoU and Dice scores in Table 5, and should be further explored. Moreover, we had no examples of the hematoma, dura/pia compelex, or dura/ventral space classes in this human inference dataset. Future efforts to improve human generalization can include transfer learning techniques to adapt the deep learning model trained on porcine data for human applications. Employing domain adaptation techniques, such as matching component analysis, cannoncial correlation analysis, or optimal transport, can effectively augment porcine data into \"synthetic\" human data, enhancing the model’s ability to generalize across species [66, 67]. These approaches would rely on a large set of porcine images and a smaller set of human images that it can map onto, and the morphological similarities in these spinal cord anatomies are favorable as transfer learning typically performs best across domains with related underlying distributions [66, 55]. As ultrasound evaluation continues to become the standard-of-care in SCI, there will be increased datasets of human spinal cord images to better inform and train these algorithms [13]."
        ]
    },
    "Sx4.T5.1.1.2.2.13.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T5.1.1.2.2.13.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2.13.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.2.2.13.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.13.1.1.1.1\">Load</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2.13.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.2.2.13.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.13.1.2.1.1\">(%)</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 5:  Performance of semantic segmentation models on unseen porcine and human spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "After hyperparameter tuning, the performance of each segmentation model during inference is shown in Table 5, including the Mean Intersection over Union (MIoU) value, speed, and computational load. MIoU is a measure of the overlap between the predicted segmentation and ground truth segmentation. The IoU for a single class (i.e., each anatomical structure) is calculated as the intersection of the predicted and true positive pixels divided by the union of predicted and true positive pixels and the MIoU is the average IoU across all classes. This provides a more comprehensive understanding of how well the model is performing across different classes. The Mean Dice coefficient, which is another metric between 0 and 1 that indicates the level of similarity between the predicted and ground truth masks, is also evaluated. It is calculated as two times the area of the intersection of the predicted and ground truth masks divided by the sum of the areas of the predicted and ground truth mask.",
            "Additionally, the poor zero-shot generalization to human images of these models is influenced by the characteristics and processing of our collected human dataset. While human and porcine spine are morphologically similar, suggesting high generalization accuracy across these species, in the images collected, most patients were undergoing a spinal cord shortening procedure, significantly affecting the curvature of the cord. Even with these drastic changes to spinal cord geometry, SegFormer, TransUNet, SwinUNet, and SAMed were able to segment the spinal cord with high accuracy (Dice >>> 0.74). However, because the CSF space is much larger in humans compared to porcine subjects (that were not undergoing PVCSO), after data preprocessing some anatomical classes were not visible in the frame, such as the dorsal space, dura, and ventral space (Figure 6). This may have a significant impact on the MIoU and Dice scores in Table 5, and should be further explored. Moreover, we had no examples of the hematoma, dura/pia compelex, or dura/ventral space classes in this human inference dataset. Future efforts to improve human generalization can include transfer learning techniques to adapt the deep learning model trained on porcine data for human applications. Employing domain adaptation techniques, such as matching component analysis, cannoncial correlation analysis, or optimal transport, can effectively augment porcine data into \"synthetic\" human data, enhancing the model’s ability to generalize across species [66, 67]. These approaches would rely on a large set of porcine images and a smaller set of human images that it can map onto, and the morphological similarities in these spinal cord anatomies are favorable as transfer learning typically performs best across domains with related underlying distributions [66, 55]. As ultrasound evaluation continues to become the standard-of-care in SCI, there will be increased datasets of human spinal cord images to better inform and train these algorithms [13]."
        ]
    },
    "Sx4.T5.1.1.2.2.14.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"Sx4.T5.1.1.2.2.14.1\">\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2.14.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.2.2.14.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.14.1.1.1.1\">Implantability</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx4.T5.1.1.2.2.14.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"Sx4.T5.1.1.2.2.14.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx4.T5.1.1.2.2.14.1.2.1.1\">Score</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 5:  Performance of semantic segmentation models on unseen porcine and human spinal cord ultrasound images.",
        "footnotes": [],
        "references": [
            "After hyperparameter tuning, the performance of each segmentation model during inference is shown in Table 5, including the Mean Intersection over Union (MIoU) value, speed, and computational load. MIoU is a measure of the overlap between the predicted segmentation and ground truth segmentation. The IoU for a single class (i.e., each anatomical structure) is calculated as the intersection of the predicted and true positive pixels divided by the union of predicted and true positive pixels and the MIoU is the average IoU across all classes. This provides a more comprehensive understanding of how well the model is performing across different classes. The Mean Dice coefficient, which is another metric between 0 and 1 that indicates the level of similarity between the predicted and ground truth masks, is also evaluated. It is calculated as two times the area of the intersection of the predicted and ground truth masks divided by the sum of the areas of the predicted and ground truth mask.",
            "Additionally, the poor zero-shot generalization to human images of these models is influenced by the characteristics and processing of our collected human dataset. While human and porcine spine are morphologically similar, suggesting high generalization accuracy across these species, in the images collected, most patients were undergoing a spinal cord shortening procedure, significantly affecting the curvature of the cord. Even with these drastic changes to spinal cord geometry, SegFormer, TransUNet, SwinUNet, and SAMed were able to segment the spinal cord with high accuracy (Dice >>> 0.74). However, because the CSF space is much larger in humans compared to porcine subjects (that were not undergoing PVCSO), after data preprocessing some anatomical classes were not visible in the frame, such as the dorsal space, dura, and ventral space (Figure 6). This may have a significant impact on the MIoU and Dice scores in Table 5, and should be further explored. Moreover, we had no examples of the hematoma, dura/pia compelex, or dura/ventral space classes in this human inference dataset. Future efforts to improve human generalization can include transfer learning techniques to adapt the deep learning model trained on porcine data for human applications. Employing domain adaptation techniques, such as matching component analysis, cannoncial correlation analysis, or optimal transport, can effectively augment porcine data into \"synthetic\" human data, enhancing the model’s ability to generalize across species [66, 67]. These approaches would rely on a large set of porcine images and a smaller set of human images that it can map onto, and the morphological similarities in these spinal cord anatomies are favorable as transfer learning typically performs best across domains with related underlying distributions [66, 55]. As ultrasound evaluation continues to become the standard-of-care in SCI, there will be increased datasets of human spinal cord images to better inform and train these algorithms [13]."
        ]
    },
    "Sx7.T6.1": {
        "table": "<table class=\"ltx_tabular\" id=\"Sx7.T6.1\">\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.1.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.1.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.1.1.1.1\" style=\"width:142.3pt;\">AR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.1.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.1.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.1.2.1.1\" style=\"width:341.4pt;\">Average Recall</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.2.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.2.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.2.1.1.1\" style=\"width:142.3pt;\">ASPP</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.2.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.2.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.2.2.1.1\" style=\"width:341.4pt;\">Atrous Spatial Pyramid Pooling</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.3.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.3.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.3.1.1.1\" style=\"width:142.3pt;\">B-mode</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.3.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.3.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.3.2.1.1\" style=\"width:341.4pt;\">brightness-mode</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.4.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.4.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.4.1.1.1\" style=\"width:142.3pt;\">CNN</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.4.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.4.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.4.2.1.1\" style=\"width:341.4pt;\">convolutional neural network</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.5.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.5.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.5.1.1.1\" style=\"width:142.3pt;\">CPU</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.5.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.5.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.5.2.1.1\" style=\"width:341.4pt;\">central processing unit</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.6.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.6.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.6.1.1.1\" style=\"width:142.3pt;\">CSF</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.6.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.6.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.6.2.1.1\" style=\"width:341.4pt;\">cerebrospinal fluid</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.7.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.7.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.7.1.1.1\" style=\"width:142.3pt;\">CT</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.7.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.7.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.7.2.1.1\" style=\"width:341.4pt;\">computed tomography</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.8\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.8.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.8.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.8.1.1.1\" style=\"width:142.3pt;\">CVAT</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.8.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.8.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.8.2.1.1\" style=\"width:341.4pt;\">Computer Vision Annotation Tool</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.9\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.9.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.9.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.9.1.1.1\" style=\"width:142.3pt;\">DETR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.9.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.9.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.9.2.1.1\" style=\"width:341.4pt;\">Detection Transformer</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.10\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.10.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.10.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.10.1.1.1\" style=\"width:142.3pt;\">DICOM</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.10.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.10.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.10.2.1.1\" style=\"width:341.4pt;\">Digital Imaging and Communications in Medicine</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.11\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.11.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.11.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.11.1.1.1\" style=\"width:142.3pt;\">E-ELAN</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.11.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.11.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.11.2.1.1\" style=\"width:341.4pt;\">Extended Efficient Layer Aggregation Network</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.12\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.12.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.12.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.12.1.1.1\" style=\"width:142.3pt;\">FPS</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.12.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.12.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.12.2.1.1\" style=\"width:341.4pt;\">frames per second</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.13\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.13.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.13.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.13.1.1.1\" style=\"width:142.3pt;\">GPU</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.13.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.13.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.13.2.1.1\" style=\"width:341.4pt;\">graphics processing unit</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.14\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.14.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.14.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.14.1.1.1\" style=\"width:142.3pt;\">IoU</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.14.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.14.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.14.2.1.1\" style=\"width:341.4pt;\">Intersection over Union</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.15\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.15.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.15.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.15.1.1.1\" style=\"width:142.3pt;\">mAP</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.15.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.15.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.15.2.1.1\" style=\"width:341.4pt;\">mean Average Precision</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.16\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.16.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.16.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.16.1.1.1\" style=\"width:142.3pt;\">MIoU</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.16.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.16.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.16.2.1.1\" style=\"width:341.4pt;\">Mean Intersection over Union</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.17\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.17.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.17.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.17.1.1.1\" style=\"width:142.3pt;\">ML</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.17.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.17.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.17.2.1.1\" style=\"width:341.4pt;\">machine learning</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.18\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.18.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.18.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.18.1.1.1\" style=\"width:142.3pt;\">MRI</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.18.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.18.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.18.2.1.1\" style=\"width:341.4pt;\">magnetic resonance imaging</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.19\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.19.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.19.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.19.1.1.1\" style=\"width:142.3pt;\">PNG</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.19.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.19.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.19.2.1.1\" style=\"width:341.4pt;\">Portable Networks Graphics</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.20\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.20.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.20.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.20.1.1.1\" style=\"width:142.3pt;\">PVCSO</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.20.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.20.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.20.2.1.1\" style=\"width:341.4pt;\">posterior vertebral column subtraction osteotomy</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.21\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.21.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.21.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.21.1.1.1\" style=\"width:142.3pt;\">RCNN</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.21.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.21.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.21.2.1.1\" style=\"width:341.4pt;\">Region-based Convolutional Neural Network</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.22\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.22.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.22.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.22.1.1.1\" style=\"width:142.3pt;\">SAMed</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.22.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.22.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.22.2.1.1\" style=\"width:341.4pt;\">Segment Anything for medical images</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.23\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.23.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.23.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.23.1.1.1\" style=\"width:142.3pt;\">SCI</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.23.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.23.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.23.2.1.1\" style=\"width:341.4pt;\">spinal cord injury</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.24\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.24.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.24.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.24.1.1.1\" style=\"width:142.3pt;\">SSD</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.24.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.24.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.24.2.1.1\" style=\"width:341.4pt;\">Single Shot Detector</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx7.T6.1.25\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.25.1\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.25.1.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.25.1.1.1\" style=\"width:142.3pt;\">YOLO</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"Sx7.T6.1.25.2\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"Sx7.T6.1.25.2.1\">\n<span class=\"ltx_p\" id=\"Sx7.T6.1.25.2.1.1\" style=\"width:341.4pt;\">You Only Look Once</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "caption": "",
        "footnotes": [],
        "references": []
    }
}