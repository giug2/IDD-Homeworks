{
    "id_table_1": {
        "caption": "Table 1:  QA performance comparison, where the best results are  boldfaced  and the second-best results are  underlined , in each row. Tok is the average length of extracted evidence fed into generators, where the smaller the value, the lower the computational cost. All improvements are significant with  p p p italic_p -value < 0.01 according  to   t t t italic_t -test.",
        "table": "S3.T1.2",
        "footnotes": [],
        "references": [
            "Given the extracted evidence, a question arises again:  How to evaluate the quality of evidence properly?   In principle, the evidence should be faithful ( i.e.,  avoiding intrinsic hallucination) to the retrieved passages  Rashkin et al. ( 2021 ); Maynez et al. ( 2020 ) , helpful in addressing the user input  Adlakha et al. ( 2023 ) , and concise to facilitate the inference speed  Ko et al. ( 2024 ) . Figure  1  shows three representative scenarios:   (1)  When the evidence only favors faithfulness, LLMs may generate an incorrect answer;   (2)  When the evidence further favors helpfulness but lacks conciseness, LLMs attention may be distracted by noise;   (3)  When the evidence favors all three criteria, LLMs can generate confidently with low computational costs.",
            "As stated in Section  1 , heuristic-based augmentation suffers from several issues,  which severely hinders the optimization of context filtering.  To verify the above claim, we compare the context relevance between heuristic-based and model-based augmentation,  where the  context relevance  is the cosine similarity between the extracted evidence and the user query 2 2 2 We employ the SBERT-NLI-base model  Reimers and Gurevych ( 2019 )  (denoted as SBERT) to encode the extracted evidence and the user query into sentence embedding vectors. .  Here, we use StrInc as the representative heuristic-based augmentation method (abbreviated as  StrInc Heur-based Aug ), as it usually performs best on QA tasks according to  Wang et al. ( 2023 ) .  On the other hand, we perform model-based augmentation by response sampling (More details can be seen in  3.1 ).  We take the best-performing extracted evidence for each QA pair as  Upper Model-based Aug  while the worst-performing one as  Lower Model-based Aug .",
            "Figure  3  depicts the overall framework of  SEER , composing three key stages:  (1) Evidence Extraction  ( 3.1 ), which extracts evidence via response sampling.   (2) Expert Assessment  ( 3.2 ), which assesses the quality of evidence.   (3) Self-Alignment  ( 3.3 ), which aligns the extractor with extraction preference.  The learning algorithm of our proposed method can be seen in Appendix  D  in Algorithm  1 .",
            "As stated in Section  1 , heuristic-based augmentation  Wang et al. ( 2023 )  suffers from several issues.  An empirical study ( 2.2 ) further indicates that model-based augmentation is more beneficial for performance improvement than heuristic-based augmentation.  Hence, we aim to utilize the base extractor  E E \\mathcal{E} caligraphic_E  to improve itself and align it with desired properties.  To this end, we probe into its evidence extraction preference by response sampling for preference data collection.  Specifically, given a query  q q q italic_q  and its retrieved passage  P P {P} italic_P , we prompt the model to generate multiple candidate extracted evidence  { e i } i = 1 M superscript subscript subscript e i i 1 M \\{e_{i}\\}_{i=1}^{M} { italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT  via response sampling  e   E (  | q  P ) e_{*}\\sim{\\mathcal{E}}(\\cdot|q\\oplus P) italic_e start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  caligraphic_E (  | italic_q  italic_P ) , where  M M M italic_M  is the sample size.",
            "To examine the impact of evidence extraction on the final RAG performance, we experimented on three benchmark QA datasets, where we prepended the extracted evidence before the user query and then input it together into the generator.  Besides, we use the tokenizer of Flan-T5 and Llama2 to convert the extracted evidence into a list of subwords and then calculate the length of the list, where the length is adopted as a metric (denoted by  Tok ) measuring the computational burden to a large extend.  Table  1  shows the final RAG performance of different baseline evidence extraction methods and our proposed  SEER . From the experimental results, we mainly have the following observations:",
            "In Figure  6 , we experiment to verify whether the stability of model generation is improved after self-alignment optimization.  Specifically, we generate ten pieces of evidence for each test query by response sampling with the same generation configuration as Section  3.1 .  Subsequently, we measure the oracle scores ( 3.2 ), calculate the standard deviation, and compute the average value.  The experimental results show that:   (1)  The generation stability of the aligned model is much better than that of the base one in most cases. More precisely, the average improvement of the aligned model over the base one on the three datasets is 18.5%.   (2)  The generation stability in terms of helpfulness has seen greater improvements compared to the other two properties ( i.e.,  faithfulness and conciseness), with an average improvement of 32.2%, showing the huge potential to enhance the final RAG performance.  The above observations fully demonstrate that  SEER  is able to endow the backbone with superior generation stability during the inference.",
            "Algorithm  1  demonstrates the learning algorithm of the proposed  SEER  framework. The algorithm can be divided into three stages,  i.e.,    (1) Evidence Extraction  (line 3-6),  (2) Expert Assessment  (line 7-10), as well as  (3) Self-Alignment  (line 11-14)."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Ablation study with key settings of  SEER , where we use FS, HS, and CS to indicate the Faithfulness, Helpfulness, and Conciseness scores, respectively.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "We experiment on three datasets,  i.e.,  NQ, TQA, and HotpotQA.  Figure  2  shows that:  (1)  The context relevance of Upper Model-based Aug is consistently higher than that of StrInc Heur-based Aug.   (2)  The context relevance of StrInc Heur-based Aug generally lies in the middle of Upper and Lower Model-based Aug.  From the above observations, our claim is well-validated, as model-based augmentation shows a larger potential than heuristic-based one.  Therefore, it is valuable to conduct model-based augmentation for better performance.",
            "Figure  3  depicts the overall framework of  SEER , composing three key stages:  (1) Evidence Extraction  ( 3.1 ), which extracts evidence via response sampling.   (2) Expert Assessment  ( 3.2 ), which assesses the quality of evidence.   (3) Self-Alignment  ( 3.3 ), which aligns the extractor with extraction preference.  The learning algorithm of our proposed method can be seen in Appendix  D  in Algorithm  1 .",
            "As stated in Section  1 , heuristic-based augmentation  Wang et al. ( 2023 )  suffers from several issues.  An empirical study ( 2.2 ) further indicates that model-based augmentation is more beneficial for performance improvement than heuristic-based augmentation.  Hence, we aim to utilize the base extractor  E E \\mathcal{E} caligraphic_E  to improve itself and align it with desired properties.  To this end, we probe into its evidence extraction preference by response sampling for preference data collection.  Specifically, given a query  q q q italic_q  and its retrieved passage  P P {P} italic_P , we prompt the model to generate multiple candidate extracted evidence  { e i } i = 1 M superscript subscript subscript e i i 1 M \\{e_{i}\\}_{i=1}^{M} { italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT  via response sampling  e   E (  | q  P ) e_{*}\\sim{\\mathcal{E}}(\\cdot|q\\oplus P) italic_e start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  caligraphic_E (  | italic_q  italic_P ) , where  M M M italic_M  is the sample size.",
            "In Table  2 , we conduct an ablation study to verify the effectiveness of key settings in our method, where w/o denotes without,  (A) represents  SEER ,  (B) removes the deduplication operation,  (C) removes smoothing CoV-weighting by uniformly setting   f superscript  f \\alpha^{f} italic_ start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT ,   h superscript  h \\alpha^{h} italic_ start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT , and   c superscript  c \\alpha^{c} italic_ start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT  to  1 / 3 1 3 1/3 1 / 3  in Eq. ( 7 ),  (D) removes the lambda weight   w , l subscript  w l \\lambda_{w,l} italic_ start_POSTSUBSCRIPT italic_w , italic_l end_POSTSUBSCRIPT  in Eq. ( 8 ).  From the table, we can find that (A) achieves the best or second-best results in all datasets, indicating all key settings are effective and necessary for  SEER .  By comparing (A) and (B), removing duplicates can considerably improve helpfulness, as it effectively avoids preference optimization overwhelmed by head responses.  By comparing (A) and (C), weighting the oracle scores based on their statistical properties is able to match the learning difficulty well.  By comparing (C) and (D), we observe that weighting the preference pairs plays a more key role than weighting the oracle scores.  The main reason might be that equally treating all preference pairs leads to less attention paid to the crucial ones.",
            "Context relevance details.  In Section  2 , we use context relevance as the metric to measure how well the extracted evidence fits the current user query and can be effectively used to augment the quality of generation.  To this end, we naturally define context relevance as the cosine similarity between the extracted evidence and the user query:",
            "In Figure  6 , we experiment to verify whether the stability of model generation is improved after self-alignment optimization.  Specifically, we generate ten pieces of evidence for each test query by response sampling with the same generation configuration as Section  3.1 .  Subsequently, we measure the oracle scores ( 3.2 ), calculate the standard deviation, and compute the average value.  The experimental results show that:   (1)  The generation stability of the aligned model is much better than that of the base one in most cases. More precisely, the average improvement of the aligned model over the base one on the three datasets is 18.5%.   (2)  The generation stability in terms of helpfulness has seen greater improvements compared to the other two properties ( i.e.,  faithfulness and conciseness), with an average improvement of 32.2%, showing the huge potential to enhance the final RAG performance.  The above observations fully demonstrate that  SEER  is able to endow the backbone with superior generation stability during the inference."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Statistics and task metrics for three datasets.",
        "table": "A0.T3.1",
        "footnotes": [],
        "references": [
            "As stated in Section  1 , heuristic-based augmentation suffers from several issues,  which severely hinders the optimization of context filtering.  To verify the above claim, we compare the context relevance between heuristic-based and model-based augmentation,  where the  context relevance  is the cosine similarity between the extracted evidence and the user query 2 2 2 We employ the SBERT-NLI-base model  Reimers and Gurevych ( 2019 )  (denoted as SBERT) to encode the extracted evidence and the user query into sentence embedding vectors. .  Here, we use StrInc as the representative heuristic-based augmentation method (abbreviated as  StrInc Heur-based Aug ), as it usually performs best on QA tasks according to  Wang et al. ( 2023 ) .  On the other hand, we perform model-based augmentation by response sampling (More details can be seen in  3.1 ).  We take the best-performing extracted evidence for each QA pair as  Upper Model-based Aug  while the worst-performing one as  Lower Model-based Aug .",
            "Figure  3  depicts the overall framework of  SEER , composing three key stages:  (1) Evidence Extraction  ( 3.1 ), which extracts evidence via response sampling.   (2) Expert Assessment  ( 3.2 ), which assesses the quality of evidence.   (3) Self-Alignment  ( 3.3 ), which aligns the extractor with extraction preference.  The learning algorithm of our proposed method can be seen in Appendix  D  in Algorithm  1 .",
            "Datasets and Metrics.  We experiment on three benchmark QA datasets, NaturalQuestions (NQ)  Kwiatkowski et al. ( 2019 ) , TriviaQA (TQA)  Joshi et al. ( 2017 ) , and HotpotQA  Yang et al. ( 2018 ) .  Following  Wang et al. ( 2023 ) , we use the processed version  Lee et al. ( 2019 )  of NQ for experiments, discarding answers with more than 5 tokens.  As NQ and TQA belong to the extractive QA task, we use Exact Match (EM) as their evaluation metric, where a score of 1 is assigned if at least one among multiple correct answers appears in the response of the QA model; otherwise, the score is 0.  While HotpotQA belongs to the abstractive QA task, we employ unigram  F 1 subscript F 1 \\mathrm{F}_{1} roman_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  to evaluate answer correctness.  As the test set for HotpotQA is unavailable, we report the dev set results. The detailed statistics of datasets are summarized in Appendix  A  in Table  3 .",
            "To verify the effectiveness of the proposed LPO, we implement  SEER  with different types of PO methods to optimize the three primary criteria:  (1) Base,  i.e.,  the base extractor; (2) PPO  Schulman et al. ( 2017 ) ; (3) DPO  Rafailov et al. ( 2023 ) ; (4) LPO ( 3.3 ).  In Figure  4 , we present the oracle scores made by each method and the percentage of performance improvement over the Base method. From the results, we find that:   (1)  Unsurprisingly, the Base without alignment performs the worst in 11 out of 12 cases, indicating the necessity of alignment for evidence extraction.   (2)  The PPO usually performs worse than the DPO one, as it directly optimizes the reward signal,  i.e.,  the oracle scores in our work, and thus neglects the pairwise signals between the extracted evidence corresponding to the same query. Besides, the relatively poor performance of PPO may be caused by the difficulty of optimizing PPO, making it hard to reach the optimal point.   (3)  Our LPO consistently outperforms the DPO, indicating the superiority of supplementing DPO with a listwise-aware weight.   (4)  After self-alignment, the average improvements of our LPO over the Base on three datasets are 10.2%, 6.16%, and 1.70% regarding the three primary criteria, showing huge potential to enhance the final RAG performance and quicken up the inference.",
            "Statistics of datasets.  We conduct extensive experiments on three benchmark datasets,  i.e.,  NaturalQuestions (NQ)  Kwiatkowski et al. ( 2019 ) , TriviaQA (TQA)  Joshi et al. ( 2017 ) , and HotpotQA  Yang et al. ( 2018 ) , for evaluating our proposed method and the competitive baselines.  We show the detailed statistics  of  these datasets in Table  3 .",
            "In Figure  6 , we experiment to verify whether the stability of model generation is improved after self-alignment optimization.  Specifically, we generate ten pieces of evidence for each test query by response sampling with the same generation configuration as Section  3.1 .  Subsequently, we measure the oracle scores ( 3.2 ), calculate the standard deviation, and compute the average value.  The experimental results show that:   (1)  The generation stability of the aligned model is much better than that of the base one in most cases. More precisely, the average improvement of the aligned model over the base one on the three datasets is 18.5%.   (2)  The generation stability in terms of helpfulness has seen greater improvements compared to the other two properties ( i.e.,  faithfulness and conciseness), with an average improvement of 32.2%, showing the huge potential to enhance the final RAG performance.  The above observations fully demonstrate that  SEER  is able to endow the backbone with superior generation stability during the inference."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Three examples of full-length answers from the NQ, TQA, as well as HotpotQA  datasets,  respectively.",
        "table": "A2.T4.1",
        "footnotes": [],
        "references": [
            "To verify the effectiveness of the proposed LPO, we implement  SEER  with different types of PO methods to optimize the three primary criteria:  (1) Base,  i.e.,  the base extractor; (2) PPO  Schulman et al. ( 2017 ) ; (3) DPO  Rafailov et al. ( 2023 ) ; (4) LPO ( 3.3 ).  In Figure  4 , we present the oracle scores made by each method and the percentage of performance improvement over the Base method. From the results, we find that:   (1)  Unsurprisingly, the Base without alignment performs the worst in 11 out of 12 cases, indicating the necessity of alignment for evidence extraction.   (2)  The PPO usually performs worse than the DPO one, as it directly optimizes the reward signal,  i.e.,  the oracle scores in our work, and thus neglects the pairwise signals between the extracted evidence corresponding to the same query. Besides, the relatively poor performance of PPO may be caused by the difficulty of optimizing PPO, making it hard to reach the optimal point.   (3)  Our LPO consistently outperforms the DPO, indicating the superiority of supplementing DPO with a listwise-aware weight.   (4)  After self-alignment, the average improvements of our LPO over the Base on three datasets are 10.2%, 6.16%, and 1.70% regarding the three primary criteria, showing huge potential to enhance the final RAG performance and quicken up the inference.",
            "Silver faithfulness details.  In Section  4.4 , we devise a metric, silver faithfulness, to measure the robustness of the evidence extractor against data noise issues commonly existing in real-world scenarios.  Specifically, we fed the mixture of the relevant retrieved passage and the randomly sampled irrelevant passages into the extractor.  Then, we treat the relevant retrieved passage and extracted evidence as the premise and hypothesis, respectively, measuring how well the extractor is robust to irrelevant context, which can  be  formulated  as:",
            "To assess the conciseness of the extracted evidence, we propose measuring the information gap between it and the full-length answer.  The full-length answer is generated by transforming the question and its corresponding answer into a declarative statement, as shown in Table  4 .  Towards this end, we prompt GPT-3.5-turbo to transform each question-answer pair into a full-length answer. Additionally, we prepared a few-shot examples to encourage well-organized output.  The prompt for full-   length answer generation can be found in Table  5 ."
        ]
    },
    "global_footnotes": [
        "We employ the SBERT-NLI-base model",
        "(denoted as SBERT) to encode the extracted evidence and the user query into sentence embedding vectors.",
        "We use the term",
        "to denote three primary criteria.",
        "We employ Flan-T5-XL for helpfulness assessment.",
        "The full-length answer is generated by transforming the question and its corresponding answer into a declarative statement",
        ".",
        "In what follows, we use Flan-T5 and Llama2 to represent Flan-T5-XL and Llama2-7B-Chat, respectively, for brevity.",
        "The silver faithfulness measures the entailment degree between the relevant retrieved passage (rather than the mixture of it and the irrelevant passages)  and  the  extracted  evidence.",
        "."
    ]
}