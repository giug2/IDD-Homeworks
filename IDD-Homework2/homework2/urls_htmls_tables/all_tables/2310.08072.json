{
    "S4.T1": {
        "caption": "Table 1: The search range values in LoRA fine-tuning.",
        "table": "<table id=\"S4.T1.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.2.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Batch Size: {4, 8},</td>\n</tr>\n<tr id=\"S4.T1.2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.4.2.1\" class=\"ltx_td ltx_align_left\">Learning Rate: {0.00001, 0.00005, 0.000001},</td>\n</tr>\n<tr id=\"S4.T1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_b\">Epoch: {3, 4, 5,}, <math id=\"S4.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"r\" display=\"inline\"><semantics id=\"S4.T1.1.1.1.m1.1a\"><mi id=\"S4.T1.1.1.1.m1.1.1\" xref=\"S4.T1.1.1.1.m1.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.1.1.1.m1.1b\"><ci id=\"S4.T1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.1.1.1.m1.1.1\">&#119903;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.m1.1c\">r</annotation></semantics></math>: {4, 8, 16, 64, 128},\n<math id=\"S4.T1.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"\\alpha\" display=\"inline\"><semantics id=\"S4.T1.2.2.2.m2.1a\"><mi id=\"S4.T1.2.2.2.m2.1.1\" xref=\"S4.T1.2.2.2.m2.1.1.cmml\">&#945;</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.2.2.2.m2.1b\"><ci id=\"S4.T1.2.2.2.m2.1.1.cmml\" xref=\"S4.T1.2.2.2.m2.1.1\">&#120572;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.2.2.2.m2.1c\">\\alpha</annotation></semantics></math>: {1, 4, 16}</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Parameters\nWe conducted a grid search for tuning parameters: batch size, learning rate, the number of epochs, as well as LoRA\u2019s hyperparameters (specifically \u03b1\ud835\udefc\\alpha and r\ud835\udc5fr).\nThe range of values explored during this search is provided in Table 1.\nSubsequently, the model that attained the highest BERTScore was chosen for evaluation."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Performances on different contexts and numbers of generated QA pairs.",
        "table": "<table id=\"S5.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\">context</th>\n<th id=\"S5.T2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\"><math id=\"S5.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"N\" display=\"inline\"><semantics id=\"S5.T2.1.1.1.m1.1a\"><mi id=\"S5.T2.1.1.1.m1.1.1\" xref=\"S5.T2.1.1.1.m1.1.1.cmml\">N</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.1.1.1.m1.1b\"><ci id=\"S5.T2.1.1.1.m1.1.1.cmml\" xref=\"S5.T2.1.1.1.m1.1.1\">&#119873;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.1.1.1.m1.1c\">N</annotation></semantics></math></th>\n<th id=\"S5.T2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">prompt</th>\n<th id=\"S5.T2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">BERTscore</th>\n<th id=\"S5.T2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">BLEU</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S5.T2.1.2.1.1.1\" class=\"ltx_text ltx_font_typewriter\">Human</span></th>\n<th id=\"S5.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\">-</th>\n<td id=\"S5.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">-</td>\n<td id=\"S5.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">0.899</td>\n<td id=\"S5.T2.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">5.64</td>\n</tr>\n<tr id=\"S5.T2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">GPT</th>\n<th id=\"S5.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">-</th>\n<td id=\"S5.T2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td id=\"S5.T2.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.601</td>\n<td id=\"S5.T2.1.3.2.5\" class=\"ltx_td ltx_align_center\">0.00</td>\n</tr>\n<tr id=\"S5.T2.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\">news</th>\n<th id=\"S5.T2.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\">1</th>\n<td id=\"S5.T2.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">zero</td>\n<td id=\"S5.T2.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">0.697</td>\n<td id=\"S5.T2.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">0.02</td>\n</tr>\n<tr id=\"S5.T2.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">wiki</th>\n<th id=\"S5.T2.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1</th>\n<td id=\"S5.T2.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">zero</td>\n<td id=\"S5.T2.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.713</td>\n<td id=\"S5.T2.1.5.4.5\" class=\"ltx_td ltx_align_center\">0.03</td>\n</tr>\n<tr id=\"S5.T2.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">JSQuAD</th>\n<th id=\"S5.T2.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1</th>\n<td id=\"S5.T2.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">zero</td>\n<td id=\"S5.T2.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.724</td>\n<td id=\"S5.T2.1.6.5.5\" class=\"ltx_td ltx_align_center\">1.55</td>\n</tr>\n<tr id=\"S5.T2.1.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">news</th>\n<th id=\"S5.T2.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">1</th>\n<td id=\"S5.T2.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">one</td>\n<td id=\"S5.T2.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.738</td>\n<td id=\"S5.T2.1.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.11</td>\n</tr>\n<tr id=\"S5.T2.1.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">wiki</th>\n<th id=\"S5.T2.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1</th>\n<td id=\"S5.T2.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">one</td>\n<td id=\"S5.T2.1.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.775</td>\n<td id=\"S5.T2.1.8.7.5\" class=\"ltx_td ltx_align_center\">0.09</td>\n</tr>\n<tr id=\"S5.T2.1.9.8\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">JSQuAD</th>\n<th id=\"S5.T2.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1</th>\n<td id=\"S5.T2.1.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\">one</td>\n<td id=\"S5.T2.1.9.8.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.863</td>\n<td id=\"S5.T2.1.9.8.5\" class=\"ltx_td ltx_align_center\">4.83</td>\n</tr>\n<tr id=\"S5.T2.1.10.9\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">news</th>\n<th id=\"S5.T2.1.10.9.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">3</th>\n<td id=\"S5.T2.1.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">zero</td>\n<td id=\"S5.T2.1.10.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.713</td>\n<td id=\"S5.T2.1.10.9.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.38</td>\n</tr>\n<tr id=\"S5.T2.1.11.10\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.11.10.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">wiki</th>\n<th id=\"S5.T2.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">3</th>\n<td id=\"S5.T2.1.11.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\">zero</td>\n<td id=\"S5.T2.1.11.10.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.706</td>\n<td id=\"S5.T2.1.11.10.5\" class=\"ltx_td ltx_align_center\">0.23</td>\n</tr>\n<tr id=\"S5.T2.1.12.11\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.12.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">JSQuAD</th>\n<th id=\"S5.T2.1.12.11.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">3</th>\n<td id=\"S5.T2.1.12.11.3\" class=\"ltx_td ltx_align_center ltx_border_r\">zero</td>\n<td id=\"S5.T2.1.12.11.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.740</td>\n<td id=\"S5.T2.1.12.11.5\" class=\"ltx_td ltx_align_center\">1.85</td>\n</tr>\n<tr id=\"S5.T2.1.13.12\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.13.12.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">news</th>\n<th id=\"S5.T2.1.13.12.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">3</th>\n<td id=\"S5.T2.1.13.12.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">one</td>\n<td id=\"S5.T2.1.13.12.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.747</td>\n<td id=\"S5.T2.1.13.12.5\" class=\"ltx_td ltx_align_center ltx_border_t\">1.25</td>\n</tr>\n<tr id=\"S5.T2.1.14.13\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.14.13.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">wiki</th>\n<th id=\"S5.T2.1.14.13.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">3</th>\n<td id=\"S5.T2.1.14.13.3\" class=\"ltx_td ltx_align_center ltx_border_r\">one</td>\n<td id=\"S5.T2.1.14.13.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.838</td>\n<td id=\"S5.T2.1.14.13.5\" class=\"ltx_td ltx_align_center\">1.66</td>\n</tr>\n<tr id=\"S5.T2.1.15.14\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.15.14.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">JSQuAD</th>\n<th id=\"S5.T2.1.15.14.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">3</th>\n<td id=\"S5.T2.1.15.14.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">one</td>\n<td id=\"S5.T2.1.15.14.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S5.T2.1.15.14.4.1\" class=\"ltx_text ltx_font_bold\">0.889</span></td>\n<td id=\"S5.T2.1.15.14.5\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T2.1.15.14.5.1\" class=\"ltx_text ltx_font_bold\">6.77</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Table 2 presents the scores of BERTScore and BLEU obtained by varying the contexts while keeping other settings, i.e., N\ud835\udc41N and prompts are fixed.\nThe table is divided into five sections.\nStarting from the top, the first section displays scores for QA models trained on human-authored QA pairs (Human) from the JSQuAD training dataset, along with the plain GPT model (GPT) without fine-tuning.\nThe second and third sections showcase scores obtained when N\ud835\udc41N is fixed to one, but we vary the prompts to zero-shot and one-shot.\nThe fourth and fifth sections represent scores when we use N=3\ud835\udc413N=3.",
            "Impact of Prompts on Performance: \nThe one-shot prompt is more effective.\nAs shown in Table 2, the model fine-tuned on the zero-shot QA pairs (N=1\ud835\udc411N=1) generated from the contexts in JSQuAD training dataset achieves a BERTScore of 0.724.\nHowever, the one-shot prompts with N=1\ud835\udc411N=1 exhibit a significant performance gain, reaching a BERTScore of 0.863."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Accuracy calculated as the number of correct question-context-answer tuples divided by the total 500 evaluation instances.",
        "table": "<table id=\"S5.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">QA Pairs</th>\n<th id=\"S5.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Accuracy (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S5.T3.1.1.1.1\" class=\"ltx_text ltx_font_typewriter\">JSQuAD (<math id=\"S5.T3.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"N=3\" display=\"inline\"><semantics id=\"S5.T3.1.1.1.1.m1.1a\"><mrow id=\"S5.T3.1.1.1.1.m1.1.1\" xref=\"S5.T3.1.1.1.1.m1.1.1.cmml\"><mi id=\"S5.T3.1.1.1.1.m1.1.1.2\" xref=\"S5.T3.1.1.1.1.m1.1.1.2.cmml\">N</mi><mo id=\"S5.T3.1.1.1.1.m1.1.1.1\" xref=\"S5.T3.1.1.1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S5.T3.1.1.1.1.m1.1.1.3\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.cmml\">3</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.1.1.1.1.m1.1b\"><apply id=\"S5.T3.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1\"><eq id=\"S5.T3.1.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.1\"></eq><ci id=\"S5.T3.1.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.2\">&#119873;</ci><cn type=\"integer\" id=\"S5.T3.1.1.1.1.m1.1.1.3.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.3\">3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.1.1.1.1.m1.1c\">N=3</annotation></semantics></math>, one-shot prompt)</span></td>\n<td id=\"S5.T3.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T3.1.1.2.1\" class=\"ltx_text ltx_font_bold\">45.4</span></td>\n</tr>\n<tr id=\"S5.T3.1.3.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.1.3.1.1.1\" class=\"ltx_text ltx_font_typewriter\">Human</span></td>\n<td id=\"S5.T3.1.3.1.2\" class=\"ltx_td ltx_align_center\">38.4</td>\n</tr>\n<tr id=\"S5.T3.1.4.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">Gold</td>\n<td id=\"S5.T3.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">90.4</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We present the results of the manual evaluation.\nTable 3 shows the comparisons between three outputs: answers generated by 1) our best performing model (JSQuAD (N=3\ud835\udc413N=3), and one-shot prompt) and 2) a model that is fine-tuned on human-authored QA pairs from the JSQuAD training dataset, and 3) gold answers in JSQuAD evaluation dataset.\nRemarkably, despite our approach does not use any human-authored QA pairs, the achieved accuracy is 45.4% while the model fine-tuned on human-authored QA pairs achieves only 38.4% in terms of accuracy.\nGilardi et\u00a0al. (2023) mention that automatic annotation with an instructor-tuning model has higher quality than annotations by crowd-workers, and our results are consistent with their claim.\nNote that the performance of both fine-tuned models falls significantly behind the Gold standard (90.4%), indicating ample room for improvement."
        ]
    }
}