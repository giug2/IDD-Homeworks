{
    "S4.T1": {
        "caption": "Table 1: Test accuracy (in %) of our INOLML and previous methods evaluated on various symmetric noise rates. Methods with superscript T represent meta-learning methods that need clean validation sets. The lower block contains meta-learning methods while the upper block shows methods with SOTA results.",
        "table": null,
        "footnotes": [],
        "references": [
            "Table 1 shows the test accuracy of many methods, including meta-learning based approaches that require a clean validation set (indicated with T) and others that automatically build their validation sets, at various level of noise rates ranging from 20% to 80%. In general, the proposed method outperforms most of the previous methods, even though we do not require a clean validation set. The slightly lower performance than Distill on CIFAR100 at 80% noise rate can be explained by Distill’s large manually curated clean validation set with 10 clean samples per class. In addition, as shown in Fig. 2(a), at 80% symmetric noise rate, a significant portion (20% to 45%) of our clean validation set contains noisy samples at the final training stages, which deteriorates the efficacy of our approach.\nWe also carry out additional experiments with different validation set sizes to fairly compare with Distill in Appendix C, in which our method outperforms Distill by 1 to 3% in majority of scenarios. Overall, these results show that a pseudo-clean, balanced, and informative validation set, can outperform a randomly-collected clean validation set in most symmetric noise scenarios. Our results also set new state-of-the-art (SOTA) results on the symmetric label noise benchmarks for methods that do not require clean validation set."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Test accuracy (in %) of our INOLML and previous methods on CIFAR10 with 0.4 asymmetric noise. (table on left): comparison with Distill using a validation set 𝒟(v)superscript𝒟𝑣\\mathcal{D}^{(v)} of sizes 1, 5 and 10 samples per class on Resnet29 and WideResnet28-10, and (table on right): comparison with some leading methods. The superscript T indicates the need for clean validation sets.",
        "table": null,
        "footnotes": [],
        "references": [
            "We compare our algorithm with Distill [80], FSR [79] and other approaches on CIFAR10 at 40% asymmetric noise rate. Similarly to the symmetric noise cases, we also use (pseudo-)clean validation sets of sizes 1, 5 and 10 samples per class and show the results of Distill and our method in Table 2 (table on left).\nAlthough our proposed method does not rely on a manually-labelled validation set, it performs better than Distill, especially with small model architectures (Resnet29) and small validation sets (1 sample per class).\nOur active selection strategy has slightly lower accuracy with larger clean validation set sizes (larger than or equal 5 random clean samples per classes) on larger model architectures (WideResnet28). This might be caused by the confirmation bias of asymmetric noise in our selected pseudo-clean validation subset and the high capacity of larger models, such as WideResnet28-10, which are more prone to overfit label noise, especially when being trained on a small dataset with only 10 classes.\nWe further evaluate the proposed method and show the higher performance of our method compared to other methods, such as FSR and L2R meta-learning methods, in Table 2 (table on right)."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Test accuracy (in %) of our INOLML and other SOTA meta-learning approaches evaluated on the CIFAR imbalanced learning (long-tailed) recognition task. The reported results are from Zhang et al. [79] and Xu et al. [66].",
        "table": null,
        "footnotes": [],
        "references": [
            "We evaluate our INOLML on the imbalanced (long-tailed) CIFAR datasets following the same setting as [79]. The prediction accuracy in Table 3 shows that INOLML considerably surpasses all previous meta-learning approaches."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Test accuracy (in %) of our INOLML and other SOTA methods on CIFAR10 long-tailed recognition mixed with symmetric noise. The reported results are collected from [79] and [63].",
        "table": null,
        "footnotes": [],
        "references": [
            "We evaluate our proposed method in the setting that combines class imbalance and label noise, which has been proposed in [79]. We follow the same experimental configuration by adding 20% and 40% symmetric noise to the CIFAR10 imbalanced datasets with different imbalance ratios (10, 50 and 200).\nThe results in Table 4 show that our proposed method outperforms the benchmarks by a large margin.\nThis result is even more remarkable when studying the results with a noise rate of 40%.\nFor CIFAR100, we show the results for 20% and 40% symmetric noise and imbalance ratio 10.\nWe do not show results for larger imbalance ratios because it was not possible to build validation sets with 10 samples per class for the minority classes.\nNevertheless, for the two CIFAR100 problems, our method shows substantially better results than previous SOTA methods.\nOur method can therefore be considered the new SOTA in this imbalanced noisy-label learning benchmark with Resnet32 model."
        ]
    },
    "S4.T5": {
        "caption": "Table 5: Test accuracy (in %) of our INOLML and previous methods in open-set noise [3] using WideResnet28-10 with 10 samples per class for the validation set.",
        "table": null,
        "footnotes": [],
        "references": [
            "This type of noise refers to training images that belong to classes falling outside the C𝐶C classes in 𝒟𝒟\\mathcal{D}. We follow [31], which forms 3 benchmarks using CIFAR10 contaminated with images from CIFAR100 and ImageNet. We compare with Distill and other meta-learning methods [78, 48, 67, 50, 79] in Table 5, and our method shows significant improvements in all benchmarks. In particular, comparing to Distill, our method is 0.5% to 1% better. One interesting observation is that our method outperforms Distill in the open-set noise even though the selected validation set 𝒟(v)superscript𝒟𝑣\\mathcal{D}^{(v)} is largely contaminated with noisy samples (up to 40%) as shown in Fig. 2(b).\nThis is in contrast to our previous observation in symmetric and asymmetric noise settings where the more noisy samples in 𝒟(v)superscript𝒟𝑣\\mathcal{D}^{(v)}, the worse performance of the models trained with our method compared to Distill.\nSuch difference\nmight be attributed to the out-of-distribution characteristic of open-set noise. As open-set noisy-label datasets contain samples that do not belong to the set of known classes, such samples might help to regularise the learning on mislabelled data, reducing the effect of confirmation bias, resulting in a better performance."
        ]
    },
    "S4.T6": {
        "caption": "Table 6: SOTA prediction accuracy (in %) comparison on real-world datasets. (upper table): WebVision mini dataset (50 classes) using Resnet50 evaluated on Webvision and ImageNet test set; and (lower table): Red Mini-ImageNet. The results of other methods are reported from Zhang et al.[79], PropMix[11] and their own works.",
        "table": null,
        "footnotes": [],
        "references": [
            "Table 6 shows the results of our method and other SOTA approaches on real-world datasets. Except for HAR[4] that uses InceptionResnetV2, Table 6 (upper table) shows the performance on WebVision with Resnet50, while Table 6 (lower table) shows results on four different noise ratios evaluated on Red Mini-ImageNet. In general, our method outperforms many SOTA methods on WebVision and is competitive with the best method [47] on Red Mini-ImageNet. We note that our proposed method is more efficient in terms of memory footprint than most of Co-training based approaches [33, 11, 66] evaluated on Red Mini-ImageNet since we use only a single PreAct Resnet18 model with meta-learning instead of two separate PreAct Resnet18 models."
        ]
    },
    "S5.T7": {
        "caption": "Table 7: Test accuracy (%) on CIFAR10 and CIFAR100 under asymmetric and imbalanced noisy-label problems. The 1s​tsuperscript1𝑠𝑡1^{st} row shows the results of the optimisation of the weight (col. Weight in Eq. 6) instead of Eq. 7. The 2n​dsuperscript2𝑛𝑑2^{nd} row shows the results of optimising the lower part of Eq. 7 (col. 𝖨𝗇𝖿𝗈(.)\\mathsf{Info}(.) Only) without the upper part of Eq. 7 𝖢𝗅𝖾𝖺𝗇(.)\\mathsf{Clean}(.). The last row (Whole Eq. 7) shows our final model result.",
        "table": null,
        "footnotes": [],
        "references": [
            "We first study the optimisation in Eq. 7.\nIn the lower-lever optimisation of Eq. 7, the function 𝖨𝗇𝖿𝗈(.)\\mathsf{Info(.)} not just select samples that maximise the training sample weight from Eq. 6, as that may lead to scenarios where most of selected samples are located in the same region of the feature space. Instead, we also maximise a diversity factor defined by maximising the maximum \\sayinformation content that each training sample can get from any sample in the clean validation set. In Table 7, we show an ablation study about the importance of this factor by redefining 𝖨𝗇𝖿𝗈(.)\\mathsf{Info(.)} in Eq. 7 with Eq. 6 (Weight in Eq. 6). We also study the role of\n𝖢𝗅𝖾𝖺𝗇(.)\\mathsf{Clean}(.)\nin Eq. 7 by optimising only the lower part of Eq. 7 (𝖨𝗇𝖿𝗈(.)\\mathsf{Info}(.) Only). This ablation study is conducted on CIFAR10 and CIFAR100 under 0.4 asymmetric noise and 0.2 symmetric noise with imbalanced data.\nOverall, each component improves the performance compared to just naively optimising the weight in Eq. 6.\nAdding 𝖨𝗇𝖿𝗈(.)\\mathsf{Info}(.) and 𝖢𝗅𝖾𝖺𝗇(.)\\mathsf{Clean}(.) improves the model significantly.\nNaively selecting samples based on Eq. 6 facilitates the overfiting of the noisy-label samples, leading to confirmation bias.\nTo mitigate this problem, 𝖢𝗅𝖾𝖺𝗇(.)\\mathsf{Clean}(.) limits the noise in the clean validation set, while 𝖨𝗇𝖿𝗈(.)\\mathsf{Info}(.) prevents the gradient to go toward a single wrong direction."
        ]
    },
    "A3.T8": {
        "caption": "Table 8: Test accuracy (in %) comparison between our method (’INOLML’) and the Distill noise (’DN’) on symmetric noise using 1, 5 and 10 samples per class in the validation set on two backbone models: Resnet29 (’RN29’) and Wideresnet28-10 (’WRN’). The results of the Distill model with WideResnet28-10 are collected from [80].\nRecall that the Distill needs a clean set, while INOLML works with a pseudo-clean set.",
        "table": null,
        "footnotes": [],
        "references": [
            "We provide additional symmetric noise results of our proposed method and the Distill model [80] in Table 8. Note that our method is markedly better than Distill, particularly for the simpler model (RN29) with few samples per class (1 and 5) in the validation set. For the more complex model (WRN) and large validation set (10 samples per class), our method is still better than Distill, except for CIFAR100 at 0.8 symmetric noise rate."
        ]
    }
}