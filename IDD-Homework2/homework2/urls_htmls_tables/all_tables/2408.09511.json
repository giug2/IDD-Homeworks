{
    "id_table_1": {
        "caption": "Table 1 :  The division to generate specific types (attribute, object, relation, action) of the negative texts. Take an example of generating attribute-type negative texts. In rule-basd approach, we replace the selected word with the words in the pre-defined words lists that contains color, material, state and size. In the LLM-based approach, we use parser to detect the adjective (ADJ), and then replace this word with the LLM predicted word.",
        "table": "S3.T1.2.1",
        "footnotes": [],
        "references": [
            "Recent vision-language models have achieved remarkable performance in aligning visual data with corresponding text for tasks such as image classification and text-image retrieval  [ 37 ,  26 ,  28 ] . Despite their success, it was also found that most of the existing image-text models are limited in understanding the composition between attributes, objects, and their relations  [ 12 ,  54 ] . (e.g. an image with man wearing  white  shoe  can be easily misaligned with the text man wearing  beige  shoe). This limitation becomes more obvious with video data since the compositions presented in the scene vary over time. (e.g. a video with the scenes of a man and a woman are  talking  at a bus stop can be misaligned with the text a man and a woman are  pictured  at a bus stop). Figure  1  illustrates an actual failure case of a state-of-the-art method video-text model  [ 9 ]  in understanding the composition between an action and an object. By making slight modifications to the original text, the model struggles to differentiate changes in the context of the paired video. Although there have been several attempts to address composition understanding issues in image-text models, the problem in the video domain has been largely overlooked.",
            "We provide details on how we use negative text augmentation to build the AARO benchmark and generate diverse training data to boost compositional reasoning in Section  3.1 . Given a source video-text dataset, we construct one negative text for each video-text pair. In Section  3.2  we discuss the architecture of the adopted VidL model. In Section  3.3 , we present the training objectives, including an extension of the negative loss used in image-text compositionality to the video-text domain, and a more effective negative-augmented vision-text matching loss. The illustration of the proposed NAVERO is presented in Figure  2 .",
            "The negative texts are generated offline before training. Examples of negative texts used for training are illustrated in Figure  3 . Section  5.1  presents how  these techniques of diverse augmentation affect the reasoning performance. To create the test data for isolated evaluation, as shown in Table  1 , we construct a series of datasets named AARO, containing 4 types of tasks, Action, Attribution, Relation and Object. In each task, only one type of negative text is assigned to each video-text pair.",
            "As explained in Section  3.1.3 , we use diverse negative text augmentation for the standard training split, and type-specific negative text generation for the testing split to evaluate compositional reasoning on each type in isolation. By using MSRVTT  [ 51 ] , DiDeMo  [ 16 ]  and ActivityNet as the source video-text datasets [ 15 ] , we construct a series of datasets with a prefix name AARO to evaluate the understanding of the action, attribute, relation and objects in each model. To study whether NAVERO can also boost image-text compositionality, we use a large-scale image-text compositional understanding benchmark VL-Checklist  [ 55 ]  and also construct the dataset AARO-COCO with image-text pairs sourced from the COCO captioning dataset  [ 31 ] . Further details for each dataset are available in the supplementary material.",
            "The training procedure is stated in the Section  4.1 . We adopt a two-stage video-text training, including the pre-training and fine-tuning stage. The hyper-parameters used in our experiments are reported in the Table  7  as suggested by the latest empirical study on the video-text pre-training  [ 9 ] . We build a highly performant pre-trained video-text model and then fine-tuning for the video-text and image-text reasoning tasks."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Experimental results to examine the understanding of Action, Attribute, Relation and Object in the video-text compositionality on the datasets AARO-MSRVTT, AARO-DiDeMo and AARO-ActivityNet. The results are reported in the format of  a  c  c a c c acc italic_a italic_c italic_c  /  a  c  c ^ ^ a c c \\hat{acc} over^ start_ARG italic_a italic_c italic_c end_ARG",
        "table": "S4.T2.6.1",
        "footnotes": [],
        "references": [
            "We provide details on how we use negative text augmentation to build the AARO benchmark and generate diverse training data to boost compositional reasoning in Section  3.1 . Given a source video-text dataset, we construct one negative text for each video-text pair. In Section  3.2  we discuss the architecture of the adopted VidL model. In Section  3.3 , we present the training objectives, including an extension of the negative loss used in image-text compositionality to the video-text domain, and a more effective negative-augmented vision-text matching loss. The illustration of the proposed NAVERO is presented in Figure  2 .",
            "In Table  2  we observe, for all datasets, the vanilla fine-tuned models that do not harness any negative augmentation achieve the lowest performance. When comparing with NegVidL which adopts  L n  e  g  v  t  c subscript L n e g v t c \\textit{L}_{neg-vtc} L start_POSTSUBSCRIPT italic_n italic_e italic_g - italic_v italic_t italic_c end_POSTSUBSCRIPT , NAVERO consistently shows higher average score across all datasets, suggesting that the proposed diverse negative augmentation and the negative-augmented matching loss offer a direct and significant improvement over the negative-augmented contrastive loss.",
            "In Section  4.2 , we introduce the datasets used in the experiments. The detailed data specification is listed as below. In practice, we adopt a mixed-type negative text augmentation that use the rule-based and LLM-based augmentation with the equal possibility."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Ablation study on the types of negative text augmentation. Results are reported on AARO-MSRVTT dataset.",
        "table": "S5.T3.4.4",
        "footnotes": [],
        "references": [
            "We provide details on how we use negative text augmentation to build the AARO benchmark and generate diverse training data to boost compositional reasoning in Section  3.1 . Given a source video-text dataset, we construct one negative text for each video-text pair. In Section  3.2  we discuss the architecture of the adopted VidL model. In Section  3.3 , we present the training objectives, including an extension of the negative loss used in image-text compositionality to the video-text domain, and a more effective negative-augmented vision-text matching loss. The illustration of the proposed NAVERO is presented in Figure  2 .",
            "The negative texts are generated offline before training. Examples of negative texts used for training are illustrated in Figure  3 . Section  5.1  presents how  these techniques of diverse augmentation affect the reasoning performance. To create the test data for isolated evaluation, as shown in Table  1 , we construct a series of datasets named AARO, containing 4 types of tasks, Action, Attribution, Relation and Object. In each task, only one type of negative text is assigned to each video-text pair.",
            "As explained in Section  3.1.3 , we use diverse negative text augmentation for the standard training split, and type-specific negative text generation for the testing split to evaluate compositional reasoning on each type in isolation. By using MSRVTT  [ 51 ] , DiDeMo  [ 16 ]  and ActivityNet as the source video-text datasets [ 15 ] , we construct a series of datasets with a prefix name AARO to evaluate the understanding of the action, attribute, relation and objects in each model. To study whether NAVERO can also boost image-text compositionality, we use a large-scale image-text compositional understanding benchmark VL-Checklist  [ 55 ]  and also construct the dataset AARO-COCO with image-text pairs sourced from the COCO captioning dataset  [ 31 ] . Further details for each dataset are available in the supplementary material.",
            "As indicated in Table  3 , our study examines the impact of negative text augmentation during training. The findings show that using a mixed generator and combining both LLM-based and rule-based approaches for generating negative texts outperforms methods that rely on either generator exclusively. This outcome suggests that integrating multiple generators enhances compositional understanding. When the number of rounds is small such as using a single round as in previous work  [ 54 ,  13 ] , the semantics of the original text and negative text is very similar, thereby making the negative text hard. This explains the performance of using 1-round negative text is competitive. We observe however, that using 5-round negative text can also boost the performance with diverse information in the negative text. Additionally, we noted a decline in performance when the action word list is not expanded, particularly in action-type evaluations. This finding emphasizes that augmenting actions is useful for effectively understanding action-related compositional information in videos."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Ablation study on the loss function during the fine-tuning. Results are reported on AARO-MSRVTT dataset.",
        "table": "S5.T4.16.16",
        "footnotes": [],
        "references": [
            "We present the ablation of the loss function during fine-tuning in Table  4 . Compared with the Vanilla model, both the negative-augmented vision-text contrastive loss and negative-augmented vision-text matching loss improve the compositionality. The negative-augmented vision-text matching loss has more significant influence on the results than the contrastive loss. Intuitively, a video-text model with good compositionality is expected to have in-depth understanding for the fine-grained semantics. As discussed in  [ 27 ] , the contrastive loss is computed on the visual features and text features learned in their own spaces, making it hard to understand the semantics from the multimodal interaction. Compared with negative-augmented contrastive loss, the negative-augmented matching loss is added on top of the multimodal fusion module and based on a more grounded vision-language representation learning. Therefore, the proposed negative-augmented matching loss provides more powerful support on the compositional understanding than the negative-augmented contrastive loss.",
            "We study how the number of frames used per video during fine-tuning affect the compositional reasoning capability of the model. In Figure  4 , we experimented with varying the number of frames per video (1, 4, 8, 12, and 16) while fine-tuning the pre-trained video-text model on the AARO-MSRVTT training split, we then evaluate the model on the AARO-MSRVTT and VL-Checklist datasets. For the video-text compositional reasoning, more frames indicate more dense temporal semantics, boosting the reasoning especially for the action composition category. Intriguingly, for image-text compositional reasoning, treating the video as a static image (1 frame) results in the best performance, despite the fact that all fine-tuned models are showing significant improvement over the pre-trained model in Table  5 . This indicates that while high-quality video-text datasets downstream can enhance the image-text compositionality generally, a video-text models image comprehension diminishes as it becomes increasingly focused on the temporal aspects of video data.",
            "In Section  4.2 , we introduce the datasets used in the experiments. The detailed data specification is listed as below. In practice, we adopt a mixed-type negative text augmentation that use the rule-based and LLM-based augmentation with the equal possibility.",
            "The training procedure is stated in the Section  4.1 . We adopt a two-stage video-text training, including the pre-training and fine-tuning stage. The hyper-parameters used in our experiments are reported in the Table  7  as suggested by the latest empirical study on the video-text pre-training  [ 9 ] . We build a highly performant pre-trained video-text model and then fine-tuning for the video-text and image-text reasoning tasks."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Comparison of the recent work in image-text compositional reasoning in accuracy  a  c  c a c c acc italic_a italic_c italic_c , the pre-trained video-text model, and the fine-tuned video-text models and image-text models using the proposed NAVERO on the VL-Checklist dataset.",
        "table": "S5.T5.4.1",
        "footnotes": [],
        "references": [
            "The negative texts are generated offline before training. Examples of negative texts used for training are illustrated in Figure  3 . Section  5.1  presents how  these techniques of diverse augmentation affect the reasoning performance. To create the test data for isolated evaluation, as shown in Table  1 , we construct a series of datasets named AARO, containing 4 types of tasks, Action, Attribution, Relation and Object. In each task, only one type of negative text is assigned to each video-text pair.",
            "Table  5  presents the performance between previous image-text models and our models on the VL-Checklist dataset. Despite CLIPs extensive training on hundreds of millions of vision-language data points, its compositional understanding is relatively weak. BLIP2 outperforms CLIP possibly due to its multimodal fusion module which enhances the interaction between visual and textual features, thereby improving reasoning. NegCLIP and SVLC use the negative augmentation on the COCO and CC3M datasets respectively, based on the pre-trained CLIP. The performance gap between NegCLIP, SVLC and CLIP demonstrate the positive effect of negative augmentation. DAC advances this further by employing an additional image captioner and an LLM to generate high-quality captions for each image.",
            "We study how the number of frames used per video during fine-tuning affect the compositional reasoning capability of the model. In Figure  4 , we experimented with varying the number of frames per video (1, 4, 8, 12, and 16) while fine-tuning the pre-trained video-text model on the AARO-MSRVTT training split, we then evaluate the model on the AARO-MSRVTT and VL-Checklist datasets. For the video-text compositional reasoning, more frames indicate more dense temporal semantics, boosting the reasoning especially for the action composition category. Intriguingly, for image-text compositional reasoning, treating the video as a static image (1 frame) results in the best performance, despite the fact that all fine-tuned models are showing significant improvement over the pre-trained model in Table  5 . This indicates that while high-quality video-text datasets downstream can enhance the image-text compositionality generally, a video-text models image comprehension diminishes as it becomes increasingly focused on the temporal aspects of video data.",
            "We present a case study in Figure  5 . For each evaluation type, we showcase a sample that the Vanilla model misclassified, but is correctly identified by NAVERO. When the text undergoes minor alterations, the Vanilla model struggles to discern differences. Even for the negative text which has an unrealistic description people are singing  made of  the beach, the Vanilla model predicts a higher scores than that of the original text. In contrast, NAVERO effectively differentiates between the original and the negative text, demonstrating its effectiveness in compositional reasoning."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  Performance on text-video retrieval tasks. We compare with the State-of-the-art video-text models, and also study whether the involvement of the proposed negative augmentation bring the problem of catastrophic forgetting that  hurts the retrieval performance.",
        "table": "S5.T6.3.3",
        "footnotes": [],
        "references": [
            "Table  6  presents our findings. Our model consistently demonstrates superior performance across various datasets compared to previous methods. Crucially, the incorporation of negative augmentation does not lead to catastrophic forgetting. When we enhance compositional reasoning through fine-tuning with negative-text augmentation, our models retrieval performance is on par, or even exceeds that of the original model. This result illustrates the effectiveness of our approach in simultaneously improving compositional reasoning and preserving the original model functionalities.",
            "In figure  6 , we present the architecture of the adopted video-language model. After patchifying of the video, we add the absolute temporal positional embedding as  [ 1 ]  and relative spatial positional embeddings  [ 2 ] . In the module of cross-attention, the output video features are projected as the key and value in the cross-attention modules of the text encoder, to enhance the interaction learning between video and text features. The architecture is suggested by the latest empirical study on the video-language learning  [ 9 ] . It achieves a high performance on various video-language downstream tasks, for example, text-video retrieval, video open-ended question answering and video multiple-choice question-answering."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :  Configurations in the pre-training and fine-tuning stage.",
        "table": "S1.T7.2.1",
        "footnotes": [],
        "references": [
            "The training procedure is stated in the Section  4.1 . We adopt a two-stage video-text training, including the pre-training and fine-tuning stage. The hyper-parameters used in our experiments are reported in the Table  7  as suggested by the latest empirical study on the video-text pre-training  [ 9 ] . We build a highly performant pre-trained video-text model and then fine-tuning for the video-text and image-text reasoning tasks."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 :  Word lists for the rule-based negative text augmentation",
        "table": "S4.T8.2.1",
        "footnotes": [],
        "references": [
            "We present the pre-fined word lists used in the rule-based negative text augmentation, which is divided into action, color, size, state, material, noun and relation branches. Besides the word lists adopted from  [ 13 ] , we build a word list of relation that takes the inter-object relations into account. In addition, considering the video contains more dense action information typically compared with the image data, we construct a larger word list for action manually. The detailed words in each branch are listed in Table  8 ."
        ]
    },
    "global_footnotes": []
}