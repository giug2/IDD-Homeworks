{
    "id_table_1": {
        "caption": "Table 1 :  Tokenizations of the string  def foo():\\npass . Tokens are comma separated. The StarCoder tokenization requires fewer tokens due to the merging of whitespace characters into a single token.",
        "table": "S4.T1.6",
        "footnotes": [],
        "references": [
            "The use of retrieval in LLMs is often motivated by the desire to improve models capabilities in open-domain question answering tasks  (Patwardhan et al.,  2023 ) .  DPR  (Karpukhin et al.,  2020 )  and REALM  (Guu et al.,  2020 )  extract the answer from the retrieved data using a BERT encoder  (Devlin et al.,  2019 ) .  DPR fine-tunes the weights of the query encoder to better match its outputs with the frozen document encoder embeddings.  REALM updates the weights of the knowledge retriever encoder during training, which causes computationally demanding re-indexing of data during training.  RAG  (Lewis et al.,  2020 )  and FiD  (Izacard and Grave,  2021 )  differ in the processing of the retrieved snippets with additionally fine-tuned encoder-decoder models.   Ram et al. ( 2023 )  show that retrieval is beneficial even without additional training of models; it is sufficient to append the found snippets to the context of the decoder.   k k k italic_k NN-LM  (Khandelwal et al.,  2020 )  computes a weighted sum of the predictions from an LLM and a  k k k italic_k -nearest neighbors classifier when predicting each token.  RETRO  (Borgeaud et al.,  2022 ) , described in Section  3.1 , performs chunk-based retrieval and includes the found chunks in the decoder via cross-attention, which, in combination with a frozen document encoder, enables retrieval throughout the entire training of the model.",
            "RETRO (Retrieval-Enhanced Transformer) is a transformer based autoregressive language model enhanced with additional snippets retrieved from a database  (Borgeaud et al.,  2022 ) .  The approach splits the input token sequence into contiguous fixed-size chunks and finds snippets similar to the previous chunk when processing the current chunk.  Introducing retrieval into the model allows adapting the model to new data without updating its weights, which is particularly useful for predicting facts not present in the training set (e.g., names in a local programming project). Below, we explain the details of the approach, relevant for our application.  Our RETRO approach is illustrated in Figure  1 .",
            "We denote the token vocabulary by the set  V V \\mathbb{V} blackboard_V  obtained by a text tokenizer (see Section  4.1 ).  The input token sequence  x = ( x 1 , ... , x n )  V n x subscript x 1 ... subscript x n superscript V n x=(x_{1},...,x_{n})\\in\\mathbb{V}^{n} italic_x = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )  blackboard_V start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT  of length  n  N n N n\\in\\mathbb{N} italic_n  blackboard_N  is split into  l  N l N l\\in\\mathbb{N} italic_l  blackboard_N  contiguous chunks  C 1 = ( x 1 , ... , x m ) , ... , C l = ( x n  m + 1 , ... , x n ) formulae-sequence subscript C 1 subscript x 1 ... subscript x m ... subscript C l subscript x n m 1 ... subscript x n C_{1}=(x_{1},...,x_{m}),...,C_{l}=(x_{n-m+1},...,x_{n}) italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) , ... , italic_C start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_n - italic_m + 1 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )  of length  m = 64 m 64 m=64 italic_m = 64 .  The probability of the next token  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  depends only on previous tokens and snippets retrieved by previous chunks:",
            "The main drawback of using the text-based GPT-2 vocabulary for code completion is an inefficient handling of whitespaces.  In most cases, each space in a longer sequence of spaces is represented by its own token, which significantly reduces the number of useful tokens available in the models context during the self-attention computation.  An example is in Table  1 .  In addition to less useful data in the models context, the inefficient tokenization of consecutive spaces also affects the amount of information in each chunk of the retrieval database, as the database is built with a fixed number of tokens per chunk.",
            "We hypothesize that the significant differences in single-token metrics between GPT-2 and StarCoder tokenization are due to the different vocabularies making direct comparison of results less meaningful, so we should instead compare relative differences.  Nevertheless, we surmise that GPT-2 tokenization achieves better single-token metrics in Figure  4  and Table  5  due to the whitespace issue from Table  1 , as the model gets  easy points  by predicting individual spaces.  Despite larger absolute differences in individual metrics between GPT-2 and StarCoder tokenization, the relative differences in Figure  4  and in Table  5  are of comparable magnitude, suggesting a similar contribution of RETRO regardless of the tokenization used.",
            "In Figure  6 , we compare our trained models, RETRO and GPT-2, using both GPT-2 and StarCoder tokenizations.  The metrics confirm the advantage of StarCoder tokenization, which better handles the whitespace issue mentioned in Section  4.1 .  Similar to the single-token metrics, with multi-token predictions, we observe that the 6-layer RETRO achieves significantly better performance than the 6-layer GPT-2.  However, the difference between the 9-layer GPT-2 and the 6-layer RETRO is not as pronounced, especially with the StarCoder tokenization.",
            "In Section  5.1 , we estimate the upper bound of single-token metrics for RETRO by not removing chunks retrieved from the same file as the input context.  Similarly, we want to determine the upper bound of model performance when using In-context RAG if the retrieved snippets can contain the continuation of the current file we are completing.  This means that the context includes the line we want to complete, allowing the model to simply copy it from the context.  In Figure  8 , we see that copying from retrieved snippets is very effective, while copying with RETRO without In-context RAG performs similarly to using RETRO with In-context RAG without copying.  In addition to the explanations in Section  5.1 , we also note that because the first  Retro  layer is the sixth layer (which is also the last and only  Retro  layer for RETRO SC 6 superscript subscript absent 6 SC {}_{6}^{\\textsc{SC}} start_FLOATSUBSCRIPT 6 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT SC end_POSTSUPERSCRIPT ), certain information from the input tokens may get diluted, preventing direct copying of tokens from the encoded chunks with chunked cross-attention.  We hypothesize that adding copying examples to the training would improve the models ability to copy, and  Retro  layers could be included closer to the beginning of processing instead of only in the sixth layer.  Accurate copying is important because when completing code, we want to extract not only the semantic meaning from the found snippets but also exact facts (e.g., the exact name of an existing function and not a hallucinated but semantically similar name).",
            "Now we focus only on the examples highlighted in Table  8 , where we detect worsening or an improvement in metrics.  From Figure  10 , we find that the impact of retrieval is negative mainly at low similarities to the query, while the positive impact is more present at higher similarities.  To mitigate the consequences of poor retrieval, we examine the idea of whether it makes sense to limit retrieval to found snippets with similarities higher than some threshold value.  Unfortunately, the right side of Figure  10  does not show a significant benefit from limiting to sufficiently similar snippets.",
            "Our small models (with or without retrieval) mostly make semantic errors; syntactic errors are rare.  Semantic errors require a higher level of code understanding, which might exceed the capabilities of our small models.  Difficulties often arise when completing natural language strings (e.g., comments and documentation within the code), at the beginning of files in  import  statements, and when using boolean or numerical constants.  Degradation is also present in longer predictions due to greedy decoding and error propagation.  For completing  import  statements, the model would need information about the upcoming lines of the file to infer which modules to import into the program.  In completing documentation, we have a similar problem if we are documenting code that has not yet been written and follows in the subsequent lines.  Additionally, our string matching based evaluation does not distinguish between semantically equivalent ways of expressing messages causing examples like in Figure  11  to achieve low metrics.  Nevertheless, retrieval can offer an advantage in such cases by providing the model with an example of the existing documentation style.",
            "As discussed in Section  5.2.4 , the advantage of retrieval-augmented completion becomes apparent when highly similar examples to the input context are found in the project.  An example of repetitive code occurs when writing tests, which also involves the use of objects defined within the project.  In most cases, the model can focus on the relevant information in the context and successfully skip over unhelpful retrieved snippets.  However, when the model performs worse with retrieval, it is often due to misleading snippets that conflict with the information of the current file.  For example, the model might use a function call as found in a related file instead of how it should be used according to the current file, as seen in Figure  12 .",
            "In this paper, we examine the benefit of including retrieved similar code snippets from the project into the code completion process.  We hypothesize that finding similar examples can be useful when predicting symbols (e.g., function names, variables, classes) defined within the local project, as such names are not present in the training set.  Therefore, we sample  50 000 50000 50\\,000 50 000  random projects with at least three files from the StarCoder dataset and count the frequency of calls to functions defined within the project by traversing the abstract syntax tree.  In Figure  13 , we compare the number of calls within individual files to the number of calls within the project as a whole.  The number of calls increases with the size of the project or files, and the number of calls within the project is greater than the number of calls within individual files.  Consequently, we expect that cross-file code retrieval can improve the quality of code completion, especially for larger projects.",
            "Due to the large number of chunks in the database, exhaustive searching for the exact  k k k italic_k -nearest chunks is not feasible, so we resort to  approximate  searching instead.  As a consequence of approximate search, we seek a compromise between the search accuracy, search speed, index building speed, and index memory usage.  In our implementation, we use the Faiss library  (Johnson et al.,  2021 )  to index the chunk embeddings 4 4 4 In Faiss, the used index can be compactly described by the string  OPQ32_128,IVF1048576_HNSW32,PQ32 . .  The search is sped up using IVF indexing  (Johnson et al.,  2021 )  in combination with the HNSW  (Malkov and Yashunin,  2020 )  algorithm.  Additionally, the vectors are compressed with the PQ  (Jegou et al.,  2008 )  vector codec and the OPQ  (Ge et al.,  2013 )  transformation.  Table  10  summarizes the space requirements for storing the database and retrieval index based on the tokenization method used.",
            "To evaluate the effectiveness of approximate nearest chunk retrieval using the index constructed with Faiss, we compare the  L 2 subscript L 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  distance between the embedding of a chunk and the embedding of the nearest chunk from the retrieval database returned by the index.  For comparison, we observe the distributions of  L 2 subscript L 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  distances between the embeddings of retrieved chunks and chunks from the database, training set, test set, and chunks composed of random tokens.  All chunks contain 64 tokens, and we sample  100 000 100000 100\\,000 100 000  chunks from each set.  The distribution results are shown in Figure  14 .  The right side of the figure shows approximate distances as returned by Faiss, which are not exact due to the compression and transformation of embeddings by the indexing.  As expected, retrieving chunks from the database itself is nearly error-free (98% accuracy), while searching with random chunks returns the largest distances.  Retrieving chunks from the training set results in smaller distances compared to retrieving chunks from the test set, as the database is constructed by partitioning the training set into consecutive non-overlapping chunks."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Size of the training and test sets composed from the Python part of the StarCoder dataset.",
        "table": "S4.T2.6",
        "footnotes": [],
        "references": [
            "This paper is organized into six sections.  In Section  2 , we present related work in retrieval-augmented code completion.  In Section  3 , we present the main methods used, including an overview of the RETRO architecture  (Borgeaud et al.,  2022 ) .  In Section  4 , we describe the setup of our experiments, including the dataset, model training details, and metrics used.  In Section  5 , we present and analyze the results of our trained models evaluated with retrieval from local projects.  We conclude in Section  6 , where we also present ideas for further work.",
            "We solve this problem by finding and removing the longest suffix  t t t italic_t  in the input string  s = s   t s  superscript s  t s=s^{\\prime}\\cdot t italic_s = italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  italic_t  that is a prefix of some token  x i  V subscript x i V x_{i}\\in\\mathbb{V} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  blackboard_V ,  x i = t  x i  subscript x i  t superscript subscript x i  x_{i}=t\\cdot x_{i}^{\\prime} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_t  italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT   (Dagan et al.,  2024 ; Athiwaratkun et al.,  2024 ) .  During decoding, we then constrain the output probability distribution for the next token only to tokens that match the prefix of  t t t italic_t .  When we generate  t t t italic_t  in its entirety, we no longer constrain the output distribution.  We illustrate the token healing process in Figure  2(a)  and show its practical impact in Figure  2(b) .  We observe a significant impact largely due to the choice of the evaluation dataset, which begins code completion at a random position within the line.",
            "For training our models, we use the StarCoder dataset 3 3 3   Available at  https://huggingface.co/datasets/bigcode/starcoderdata .     (Li et al.,  2023 ) .  The dataset contains over 300 million open-source files from more than 80 programming languages, but we use only files in the Python programming language.  We split the dataset into a training and test set in ratio 90%; 10%.  We split the data at the project level and not at the file level, so that all files belonging to a project are placed in the same training or test set.  The validation set is constructed from 1% of the test set and is used to monitor metrics during model training to detect any potential overfitting (which we did not observe).  After training, we evaluate the models performance on both the test set and the evaluation sets as described in Section  4.3 .  Table  2  summarizes the sizes of the training and test sets.",
            "In this section, we present the results of our trained models on the test set from Section  4.2 .  We are interested in the contribution of retrieval with RETRO compared to the baseline GPT-2 model.  We first examine the impact of the retrieval database size, tokenization, and the number of parameters on model performance. We also compare retrieval from the training and test set simulating different numbers of projects in a local database.",
            "In Figure  6 , we also report the metrics for the In-context RAG approach (more in Section  5.2.3 ).  Despite its simplicity, the contribution of the In-context RAG is more significant than the use of RETRO.  When combining In-context RAG with RETRO, we observe smaller improvements compared to enhancing GPT-2 with In-context RAG.  A possible explanation is that including retrieved snippets into the input context  confuses  RETROs chunk based retrieval, which now finds neighbors of the already retrieved snippets for part of the context.  Additionally, snippets included in the context receive the same treatment as other input tokens through processing in the transformer, while RETRO incorporates the retrieved chunks using cross-attention only in the sparsely dispersed  Retro  layers.",
            "In Section  5.2 , we found that retrieval generally benefits model predictions, but there are still cases where retrieval degrades the accuracy of predictions.  In this section, we conduct a qualitative review of successful and less successful line completion examples, using our GPT SC 9 superscript subscript absent 9 SC {}_{9}^{\\textsc{SC}} start_FLOATSUBSCRIPT 9 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT SC end_POSTSUPERSCRIPT  and RETRO SC 6 superscript subscript absent 6 SC {}_{6}^{\\textsc{SC}} start_FLOATSUBSCRIPT 6 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT SC end_POSTSUPERSCRIPT  models.",
            "As discussed in Section  5.2.4 , the advantage of retrieval-augmented completion becomes apparent when highly similar examples to the input context are found in the project.  An example of repetitive code occurs when writing tests, which also involves the use of objects defined within the project.  In most cases, the model can focus on the relevant information in the context and successfully skip over unhelpful retrieved snippets.  However, when the model performs worse with retrieval, it is often due to misleading snippets that conflict with the information of the current file.  For example, the model might use a function call as found in a related file instead of how it should be used according to the current file, as seen in Figure  12 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :    The number of parameters of our GPT-2 and RETRO models when using GPT-2 tokenization.  The relative change in the number of parameters is shown in parentheses.  When using the StarCoder tokenization, the number of parameters for all models decreases by a constant two million parameters due a smaller vocabulary, which affects the number of parameters in the transformers embedding table.",
        "table": "S4.T3.2",
        "footnotes": [],
        "references": [
            "This paper is organized into six sections.  In Section  2 , we present related work in retrieval-augmented code completion.  In Section  3 , we present the main methods used, including an overview of the RETRO architecture  (Borgeaud et al.,  2022 ) .  In Section  4 , we describe the setup of our experiments, including the dataset, model training details, and metrics used.  In Section  5 , we present and analyze the results of our trained models evaluated with retrieval from local projects.  We conclude in Section  6 , where we also present ideas for further work.",
            "The use of retrieval in LLMs is often motivated by the desire to improve models capabilities in open-domain question answering tasks  (Patwardhan et al.,  2023 ) .  DPR  (Karpukhin et al.,  2020 )  and REALM  (Guu et al.,  2020 )  extract the answer from the retrieved data using a BERT encoder  (Devlin et al.,  2019 ) .  DPR fine-tunes the weights of the query encoder to better match its outputs with the frozen document encoder embeddings.  REALM updates the weights of the knowledge retriever encoder during training, which causes computationally demanding re-indexing of data during training.  RAG  (Lewis et al.,  2020 )  and FiD  (Izacard and Grave,  2021 )  differ in the processing of the retrieved snippets with additionally fine-tuned encoder-decoder models.   Ram et al. ( 2023 )  show that retrieval is beneficial even without additional training of models; it is sufficient to append the found snippets to the context of the decoder.   k k k italic_k NN-LM  (Khandelwal et al.,  2020 )  computes a weighted sum of the predictions from an LLM and a  k k k italic_k -nearest neighbors classifier when predicting each token.  RETRO  (Borgeaud et al.,  2022 ) , described in Section  3.1 , performs chunk-based retrieval and includes the found chunks in the decoder via cross-attention, which, in combination with a frozen document encoder, enables retrieval throughout the entire training of the model.",
            "Approaches for retrieval-augmented code completion are similar to those for natural language based open-domain question answering, with differences in the use of retrieval adapted to code and projects as a whole.  ReACC  (Lu et al.,  2022 )  (and similarly REDCODER  (Parvez et al.,  2021 ) ) combines retrieval with a code-adapted encoder, GraphCodeBERT  (Guo et al.,  2021 ) , and classic BM-25 search  (Harman,  1995 ) , incorporating found snippets into the context.   Shrivastava et al. ( 2023 )  use a trained classifier to select the most relevant code snippet from the project to include into the context.  RepoCoder  (Zhang et al.,  2023b )  demonstrates retrieval based on the Jaccard similarity of tokens for retrieving line-based snippets from the project and utilizes the generated code to improve subsequent retrieval.  This approach is the motivation for our approach described in Section  3.3 .  RepoFormer  (Wu et al.,  2024 )  enhances RepoCoder with adaptive retrieval triggered by the model with a special token.  DRAG  (Shapkin et al.,  2024 )  extends the models vocabulary with new tokens from entities in the project that the model can use while generating code.",
            "There are several datasets for evaluating code completion.  HumanEval  (Chen et al.,  2021 )  involves completing functions from given function names and docstrings; CodeXGLUE  (Lu et al.,  2021 )  contains many datasets for various tasks, including line completion.  These datasets only evaluate general code completion abilities within a single file, whereas real projects contain multiple interrelated files.  Consequently, datasets for project-level completion have begun to emerge, often as an addition for evaluating the developed approach  (Shapkin et al.,  2024 ) .  RepoBench  (Liu et al.,  2024 )  and CrossCodeEval  (Ding et al.,  2023 )  compile evaluation sets by pre-selecting useful code snippets from related files that the model should include in the context.  On the other hand, RepoEval  (Zhang et al.,  2023b )   more in Section  4.3   contains entire projects in addition to the marked lines that need to be completed, which allows the use of custom implementations of retrieval from the project and consequently any model.",
            "Let  x = ( x 1 , ... , x n )  V n x subscript x 1 ... subscript x n superscript V n x=(x_{1},...,x_{n})\\in\\mathbb{V}^{n} italic_x = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )  blackboard_V start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT  be the input sequence of tokens.  From  x x x italic_x , we construct a query  q q q italic_q  from the last  m  N m N m\\in\\mathbb{N} italic_m  blackboard_N  tokens,  q = ( x n  m + 1 , ... , x n ) q subscript x n m 1 ... subscript x n q=(x_{n-m+1},...,x_{n}) italic_q = ( italic_x start_POSTSUBSCRIPT italic_n - italic_m + 1 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , and use it to search for the  k k k italic_k  nearest code snippets from the database  D D \\mathcal{D} caligraphic_D .  The tokens of the found snippets  Ret D  ( q ) = ( r 1 , ... , r k ) subscript Ret D q subscript r 1 ... subscript r k \\textsc{Ret}_{\\mathcal{D}}(q)=(r_{1},...,r_{k}) Ret start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ( italic_q ) = ( italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )  are concatenated into a sequence of tokens  r = r 1  ...  r k r  subscript r 1 ... subscript r k r=r_{1}\\cdot...\\cdot r_{k} italic_r = italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  ...  italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT .  The new context for the model is then  x  = r  x superscript x   r x x^{\\prime}=r\\cdot x italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = italic_r  italic_x , which is used to predict the next tokens.  The process is illustrated in Figure  3 .",
            "For training our models, we use the StarCoder dataset 3 3 3   Available at  https://huggingface.co/datasets/bigcode/starcoderdata .     (Li et al.,  2023 ) .  The dataset contains over 300 million open-source files from more than 80 programming languages, but we use only files in the Python programming language.  We split the dataset into a training and test set in ratio 90%; 10%.  We split the data at the project level and not at the file level, so that all files belonging to a project are placed in the same training or test set.  The validation set is constructed from 1% of the test set and is used to monitor metrics during model training to detect any potential overfitting (which we did not observe).  After training, we evaluate the models performance on both the test set and the evaluation sets as described in Section  4.3 .  Table  2  summarizes the sizes of the training and test sets.",
            "In accordance with  Borgeaud et al. ( 2022 )  and  Wang et al. ( 2023a ) , we compare GPT-2 with RETRO, keeping the architecture of both models identical except that RETRO includes both an encoder and decoder compared to GPT-2 which is a decoder only model.  Table  3  summarizes the number of parameters in our models as well as the number of decoder layers.  To compare models with roughly equal number of parameters, we also compare GPT-2 with 9 decoder layers to RETRO with 6 decoder layers; this comparison shall ensure that any potential benefit of RETRO does not arise simply from increasing the number of parameters when adding the chunk encoder.",
            "We set the hidden dimension  d d d italic_d  to 1024, and the number of heads in the multi-headed attention to 16, so that each head processes vectors of dimension 64.  The number of decoder layers is shown in Table  3 .   Retro  layers with the retrieval capability are placed at the sixth layer and then at every subsequent third layer (e.g., layers  6 , 9 , 12 6 9 12 6,9,12 6 , 9 , 12  if there are 12 layers).  The encoder in RETRO has two layers.  The maximum length of the input context is limited to 384 tokens as in  Semenkin et al. ( 2024 ) .  Instead of fixed absolute positional embeddings, we use relative positional embeddings RoPE  (Su et al.,  2024 )  with a rotational factor of  0.5 0.5 0.5 0.5 .  For the nonlinear function within fully connected feed-forward networks, we use the  SwiGLU SwiGLU \\operatorname{SwiGLU} roman_SwiGLU  function, which has been shown to contribute to better learning outcomes in language modeling tasks  (Shazeer,  2020 ; Touvron et al.,  2023 ) .  We use shared weights for the input and output embedding tables to reduce the total number of parameters  (Press and Wolf,  2017 ; Vaswani et al.,  2017 ) .",
            "In this section, we aim to simulate and evaluate completing lines in local projects using the RepoEval dataset, as described in Section  4.3 .  We first present the results of RETRO, GPT-2, and In-context RAG in this setting, followed by an experiment with larger models, and detailed analysis of our most successful approach, namely In-context RAG. We end the section with an analysis of the impact the quality of the retrieved snippets has on the code-completion performance.",
            "In Figure  6 , we also report the metrics for the In-context RAG approach (more in Section  5.2.3 ).  Despite its simplicity, the contribution of the In-context RAG is more significant than the use of RETRO.  When combining In-context RAG with RETRO, we observe smaller improvements compared to enhancing GPT-2 with In-context RAG.  A possible explanation is that including retrieved snippets into the input context  confuses  RETROs chunk based retrieval, which now finds neighbors of the already retrieved snippets for part of the context.  Additionally, snippets included in the context receive the same treatment as other input tokens through processing in the transformer, while RETRO incorporates the retrieved chunks using cross-attention only in the sparsely dispersed  Retro  layers.",
            "Since the found snippets are included directly in the models context, it is important that the model supports a sufficiently large context to append the retrieved data in addition to the input context.  When using our models with a context size of 384 tokens, we often need to truncate the input sequence to leave enough space to include the retrieved snippets at the beginning of the context.  For our models, GPT-2 and RETRO, we include only one retrieved snippet due to the small context size of the model.  For models with larger context sizes, we include two retrieved snippets.  In Table  7  and in Figures  6  and  7 , we report multi-token prediction metrics both with and without retrieval as described in Section  3.3 .  The results indicate that using In-context RAG leads to improvements in multi-token predictions compared to the baseline models in all cases.",
            "In this paper, we examine the benefit of including retrieved similar code snippets from the project into the code completion process.  We hypothesize that finding similar examples can be useful when predicting symbols (e.g., function names, variables, classes) defined within the local project, as such names are not present in the training set.  Therefore, we sample  50 000 50000 50\\,000 50 000  random projects with at least three files from the StarCoder dataset and count the frequency of calls to functions defined within the project by traversing the abstract syntax tree.  In Figure  13 , we compare the number of calls within individual files to the number of calls within the project as a whole.  The number of calls increases with the size of the project or files, and the number of calls within the project is greater than the number of calls within individual files.  Consequently, we expect that cross-file code retrieval can improve the quality of code completion, especially for larger projects."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  The number of training iterations. Training on eight A100 40 GB GPUs took 2 hours for the smaller number of iterations and 38 hours for the larger number of iterations.",
        "table": "S4.T4.8",
        "footnotes": [],
        "references": [
            "This paper is organized into six sections.  In Section  2 , we present related work in retrieval-augmented code completion.  In Section  3 , we present the main methods used, including an overview of the RETRO architecture  (Borgeaud et al.,  2022 ) .  In Section  4 , we describe the setup of our experiments, including the dataset, model training details, and metrics used.  In Section  5 , we present and analyze the results of our trained models evaluated with retrieval from local projects.  We conclude in Section  6 , where we also present ideas for further work.",
            "There are several datasets for evaluating code completion.  HumanEval  (Chen et al.,  2021 )  involves completing functions from given function names and docstrings; CodeXGLUE  (Lu et al.,  2021 )  contains many datasets for various tasks, including line completion.  These datasets only evaluate general code completion abilities within a single file, whereas real projects contain multiple interrelated files.  Consequently, datasets for project-level completion have begun to emerge, often as an addition for evaluating the developed approach  (Shapkin et al.,  2024 ) .  RepoBench  (Liu et al.,  2024 )  and CrossCodeEval  (Ding et al.,  2023 )  compile evaluation sets by pre-selecting useful code snippets from related files that the model should include in the context.  On the other hand, RepoEval  (Zhang et al.,  2023b )   more in Section  4.3   contains entire projects in addition to the marked lines that need to be completed, which allows the use of custom implementations of retrieval from the project and consequently any model.",
            "We denote the token vocabulary by the set  V V \\mathbb{V} blackboard_V  obtained by a text tokenizer (see Section  4.1 ).  The input token sequence  x = ( x 1 , ... , x n )  V n x subscript x 1 ... subscript x n superscript V n x=(x_{1},...,x_{n})\\in\\mathbb{V}^{n} italic_x = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )  blackboard_V start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT  of length  n  N n N n\\in\\mathbb{N} italic_n  blackboard_N  is split into  l  N l N l\\in\\mathbb{N} italic_l  blackboard_N  contiguous chunks  C 1 = ( x 1 , ... , x m ) , ... , C l = ( x n  m + 1 , ... , x n ) formulae-sequence subscript C 1 subscript x 1 ... subscript x m ... subscript C l subscript x n m 1 ... subscript x n C_{1}=(x_{1},...,x_{m}),...,C_{l}=(x_{n-m+1},...,x_{n}) italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) , ... , italic_C start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_n - italic_m + 1 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )  of length  m = 64 m 64 m=64 italic_m = 64 .  The probability of the next token  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  depends only on previous tokens and snippets retrieved by previous chunks:",
            "The retrieval database is built by splitting the set of input documents  D D \\mathcal{D} caligraphic_D  into fixed-size chunks of  m = 64 m 64 m=64 italic_m = 64  tokens.  The database stores key-value pairs, where the key is the embedding of a chunk of tokens  N N N italic_N , and the value is a pair  [ N , F ] N F [N,F] [ italic_N , italic_F ] , which contains both the chunk  N N N italic_N  and the next immediate chunk  F F F italic_F  in the original document.  The retriever  Ret D  ( C ) subscript Ret D C \\textsc{Ret}_{\\mathcal{D}}(C) Ret start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ( italic_C )  finds the  k k k italic_k  most similar snippets by comparing the  L 2 subscript L 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  distance between the embedding of the chunk  M  ( C ) M C \\mathcal{M}(C) caligraphic_M ( italic_C )  and the database key  M  ( N ) M N \\mathcal{M}(N) caligraphic_M ( italic_N ) .   Borgeaud et al. ( 2022 )  use the BERT model  (Devlin et al.,  2019 )  for  M M \\mathcal{M} caligraphic_M , while we use the CodeT5+ encoder  (Wang et al.,  2023b ) , as explained in Section  4.4 .  RETRO thus receives as input the input tokens  x x x italic_x  and the  k k k italic_k  nearest snippets of each input chunk  Ret D  ( C u ) = ( [ N 1 , F 1 ] , ... , [ N k , F k ] ) subscript Ret D subscript C u superscript N 1 superscript F 1 ... superscript N k superscript F k \\textsc{Ret}_{\\mathcal{D}}(C_{u})=([N^{1},F^{1}],...,[N^{k},F^{k}]) Ret start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ( italic_C start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ) = ( [ italic_N start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_F start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ] , ... , [ italic_N start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , italic_F start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ] ) .  When training the model, we ensure that  Ret D  ( C u ) subscript Ret D subscript C u \\textsc{Ret}_{\\mathcal{D}}(C_{u}) Ret start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ( italic_C start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT )  does not contain  C u + 1 subscript C u 1 C_{u+1} italic_C start_POSTSUBSCRIPT italic_u + 1 end_POSTSUBSCRIPT  by filtering out chunks  [ N , F ] N F [N,F] [ italic_N , italic_F ]  that originate from the same training document as  x x x italic_x .",
            "For training our models, we use the StarCoder dataset 3 3 3   Available at  https://huggingface.co/datasets/bigcode/starcoderdata .     (Li et al.,  2023 ) .  The dataset contains over 300 million open-source files from more than 80 programming languages, but we use only files in the Python programming language.  We split the dataset into a training and test set in ratio 90%; 10%.  We split the data at the project level and not at the file level, so that all files belonging to a project are placed in the same training or test set.  The validation set is constructed from 1% of the test set and is used to monitor metrics during model training to detect any potential overfitting (which we did not observe).  After training, we evaluate the models performance on both the test set and the evaluation sets as described in Section  4.3 .  Table  2  summarizes the sizes of the training and test sets.",
            "All training hyperparameters are the same for both models, GPT-2 and RETRO.  For optimization, we use the Adam algorithm  (Kingma and Ba,  2015 )  with   1 = 0.9 subscript  1 0.9 \\beta_{1}=0.9 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9  and   2 = 0.95 subscript  2 0.95 \\beta_{2}=0.95 italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.95 .  The learning rate is set to  6  10  4 6 superscript 10 4 6\\times 10^{-4} 6  10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT  and is reduced using a cosine scheduler by a factor of 10 by the end of training.  For regularization, we use dropout layers with a probability of  0.1 0.1 0.1 0.1 , and the weight decay factor is  0.01 0.01 0.01 0.01 .  Table  4  summarizes the number of training iterations.  For comparing models of all sizes (6, 9, and 12 layers), we conduct training on a smaller number of iterations.  For comparing the final models of sizes 6 and 9 layers, we conduct training on the larger number of iterations.",
            "In this section, we report the results of experiments outlined in Section  4 . We start by comparing the performance of RETRO and GPT-2 on the StarCoder dataset, followed by the evaluation on the RepoEval dataset, which simulates retrieval from local projects. In this setting, we also compare the In-context RAG approach. We finish the section with a qualitative analysis of the results.",
            "In this section, we present the results of our trained models on the test set from Section  4.2 .  We are interested in the contribution of retrieval with RETRO compared to the baseline GPT-2 model.  We first examine the impact of the retrieval database size, tokenization, and the number of parameters on model performance. We also compare retrieval from the training and test set simulating different numbers of projects in a local database.",
            "In Figure  4  we compare our models of all sizes trained on a smaller number of iterations and in Table  5  we report the single-token metrics of our smaller sized models trained over more iterations.  In all cases we observe that the single-token metrics for RETRO are better than those for GPT-2 of comparable size.  However, the difference between the two models decreases when comparing models with approximately equal number of parameters, i.e. RETRO with 6 layers to GPT-2 with 9 layers, as seen in Table  5 .  In Table  5  we observe that the differences between RETRO and GPT-2 are higher when calculating metrics from token 64 onwards, suggesting the usefulness of the retrieval mechanism.  Increasing the number of layers and thus the number of model parameters also contributes to improvements in metrics, as is evident from Figure  4 .  However, by training smaller models over more iterations, we achieve better metrics than using larger models trained for fewer iterations.  The relative differences are larger in bigger models, which may be due to the use of more  Retro  layers, as well as the effect of 21% more parameters in RETRO gpt 12 superscript subscript absent 12 gpt {}_{12}^{\\textsc{gpt}} start_FLOATSUBSCRIPT 12 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT gpt end_POSTSUPERSCRIPT  compared to GPT gpt 12 superscript subscript absent 12 gpt {}_{12}^{\\textsc{gpt}} start_FLOATSUBSCRIPT 12 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT gpt end_POSTSUPERSCRIPT .",
            "We hypothesize that the significant differences in single-token metrics between GPT-2 and StarCoder tokenization are due to the different vocabularies making direct comparison of results less meaningful, so we should instead compare relative differences.  Nevertheless, we surmise that GPT-2 tokenization achieves better single-token metrics in Figure  4  and Table  5  due to the whitespace issue from Table  1 , as the model gets  easy points  by predicting individual spaces.  Despite larger absolute differences in individual metrics between GPT-2 and StarCoder tokenization, the relative differences in Figure  4  and in Table  5  are of comparable magnitude, suggesting a similar contribution of RETRO regardless of the tokenization used.",
            "In this section, we aim to simulate and evaluate completing lines in local projects using the RepoEval dataset, as described in Section  4.3 .  We first present the results of RETRO, GPT-2, and In-context RAG in this setting, followed by an experiment with larger models, and detailed analysis of our most successful approach, namely In-context RAG. We end the section with an analysis of the impact the quality of the retrieved snippets has on the code-completion performance.",
            "In Figure  6 , we compare our trained models, RETRO and GPT-2, using both GPT-2 and StarCoder tokenizations.  The metrics confirm the advantage of StarCoder tokenization, which better handles the whitespace issue mentioned in Section  4.1 .  Similar to the single-token metrics, with multi-token predictions, we observe that the 6-layer RETRO achieves significantly better performance than the 6-layer GPT-2.  However, the difference between the 9-layer GPT-2 and the 6-layer RETRO is not as pronounced, especially with the StarCoder tokenization.",
            "As discussed in Section  5.2.4 , the advantage of retrieval-augmented completion becomes apparent when highly similar examples to the input context are found in the project.  An example of repetitive code occurs when writing tests, which also involves the use of objects defined within the project.  In most cases, the model can focus on the relevant information in the context and successfully skip over unhelpful retrieved snippets.  However, when the model performs worse with retrieval, it is often due to misleading snippets that conflict with the information of the current file.  For example, the model might use a function call as found in a related file instead of how it should be used according to the current file, as seen in Figure  12 .",
            "To evaluate the effectiveness of approximate nearest chunk retrieval using the index constructed with Faiss, we compare the  L 2 subscript L 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  distance between the embedding of a chunk and the embedding of the nearest chunk from the retrieval database returned by the index.  For comparison, we observe the distributions of  L 2 subscript L 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  distances between the embeddings of retrieved chunks and chunks from the database, training set, test set, and chunks composed of random tokens.  All chunks contain 64 tokens, and we sample  100 000 100000 100\\,000 100 000  chunks from each set.  The distribution results are shown in Figure  14 .  The right side of the figure shows approximate distances as returned by Faiss, which are not exact due to the compression and transformation of embeddings by the indexing.  As expected, retrieving chunks from the database itself is nearly error-free (98% accuracy), while searching with random chunks returns the largest distances.  Retrieving chunks from the training set results in smaller distances compared to retrieving chunks from the test set, as the database is constructed by partitioning the training set into consecutive non-overlapping chunks."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :    Single-token metrics on the test set with models trained on a  larger  number of iterations (see Table  4 ).  Reported are also the relative differences of metrics compared to the baseline GPT-2 models.",
        "table": "S5.T5.14",
        "footnotes": [],
        "references": [
            "This paper is organized into six sections.  In Section  2 , we present related work in retrieval-augmented code completion.  In Section  3 , we present the main methods used, including an overview of the RETRO architecture  (Borgeaud et al.,  2022 ) .  In Section  4 , we describe the setup of our experiments, including the dataset, model training details, and metrics used.  In Section  5 , we present and analyze the results of our trained models evaluated with retrieval from local projects.  We conclude in Section  6 , where we also present ideas for further work.",
            "In this section, we present the datasets, evaluation metrics, and experimental settings used to verify if retrieval-augmented code completion can enhance the performance of LLMs when working on local projects.  Since we aim to run models on local hardware, we focus on smaller models and efficient retrieval and text generation methods.  We assess the impact of retrieval by comparing GPT-2, RETRO, and the In-context RAG approach that incorporates found snippets into the context of models without modifying the model architecture or training process. We split the section into seven parts: tokenization, training and evaluation dataset descriptions, retrieval database, model architecture with hyperparameters, training of models, and used comparison metrics. The results are reported in Section  5 .",
            "In Figure  4  we compare our models of all sizes trained on a smaller number of iterations and in Table  5  we report the single-token metrics of our smaller sized models trained over more iterations.  In all cases we observe that the single-token metrics for RETRO are better than those for GPT-2 of comparable size.  However, the difference between the two models decreases when comparing models with approximately equal number of parameters, i.e. RETRO with 6 layers to GPT-2 with 9 layers, as seen in Table  5 .  In Table  5  we observe that the differences between RETRO and GPT-2 are higher when calculating metrics from token 64 onwards, suggesting the usefulness of the retrieval mechanism.  Increasing the number of layers and thus the number of model parameters also contributes to improvements in metrics, as is evident from Figure  4 .  However, by training smaller models over more iterations, we achieve better metrics than using larger models trained for fewer iterations.  The relative differences are larger in bigger models, which may be due to the use of more  Retro  layers, as well as the effect of 21% more parameters in RETRO gpt 12 superscript subscript absent 12 gpt {}_{12}^{\\textsc{gpt}} start_FLOATSUBSCRIPT 12 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT gpt end_POSTSUPERSCRIPT  compared to GPT gpt 12 superscript subscript absent 12 gpt {}_{12}^{\\textsc{gpt}} start_FLOATSUBSCRIPT 12 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT gpt end_POSTSUPERSCRIPT .",
            "We hypothesize that the significant differences in single-token metrics between GPT-2 and StarCoder tokenization are due to the different vocabularies making direct comparison of results less meaningful, so we should instead compare relative differences.  Nevertheless, we surmise that GPT-2 tokenization achieves better single-token metrics in Figure  4  and Table  5  due to the whitespace issue from Table  1 , as the model gets  easy points  by predicting individual spaces.  Despite larger absolute differences in individual metrics between GPT-2 and StarCoder tokenization, the relative differences in Figure  4  and in Table  5  are of comparable magnitude, suggesting a similar contribution of RETRO regardless of the tokenization used.",
            "In Figure  5 , we compare the relative improvement of RETRO gpt 6 superscript subscript absent 6 gpt {}_{6}^{\\textsc{gpt}} start_FLOATSUBSCRIPT 6 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT gpt end_POSTSUPERSCRIPT  against GPT gpt 9 superscript subscript absent 9 gpt {}_{9}^{\\textsc{gpt}} start_FLOATSUBSCRIPT 9 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT gpt end_POSTSUPERSCRIPT  on projects using different retrieval databases.  We limit the comparison to RETRO with 6 layers and GPT-2 with 9 layers to reduce the impact of the difference in the model sizes.  Retrieving from the entire StarCoder training set contributes to a greater improvement compared to retrieving only from individual projects, which is consistent with the findings of  Borgeaud et al. ( 2022 ) , who observe improvements with increasing the retrieval database size. We also test retrieval from the project without removing snippets originating from the same file as the input context, which gives an upper bound estimate for RETRO since the found snippets contain the correct next tokens.  As expected, the difference with GPT-2 is in this case more significant, but it does not reach a perfect success rate.  A lower bound estimate for RETRO is obtained when we retrieve random chunks from the entire training set.  In this case, we observe that the 6-layer RETRO gpt 6 superscript subscript absent 6 gpt {}_{6}^{\\textsc{gpt}} start_FLOATSUBSCRIPT 6 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT gpt end_POSTSUPERSCRIPT  performs worse than the 9-layer GPT gpt 9 superscript subscript absent 9 gpt {}_{9}^{\\textsc{gpt}} start_FLOATSUBSCRIPT 9 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT gpt end_POSTSUPERSCRIPT , which is expected, as it does not benefit from retrieval and consequently behaves as a 6-layer GPT-2 with added noise.",
            "In Figure  6 , we also report the metrics for the In-context RAG approach (more in Section  5.2.3 ).  Despite its simplicity, the contribution of the In-context RAG is more significant than the use of RETRO.  When combining In-context RAG with RETRO, we observe smaller improvements compared to enhancing GPT-2 with In-context RAG.  A possible explanation is that including retrieved snippets into the input context  confuses  RETROs chunk based retrieval, which now finds neighbors of the already retrieved snippets for part of the context.  Additionally, snippets included in the context receive the same treatment as other input tokens through processing in the transformer, while RETRO incorporates the retrieved chunks using cross-attention only in the sparsely dispersed  Retro  layers.",
            "In Section  5.1 , we estimate the upper bound of single-token metrics for RETRO by not removing chunks retrieved from the same file as the input context.  Similarly, we want to determine the upper bound of model performance when using In-context RAG if the retrieved snippets can contain the continuation of the current file we are completing.  This means that the context includes the line we want to complete, allowing the model to simply copy it from the context.  In Figure  8 , we see that copying from retrieved snippets is very effective, while copying with RETRO without In-context RAG performs similarly to using RETRO with In-context RAG without copying.  In addition to the explanations in Section  5.1 , we also note that because the first  Retro  layer is the sixth layer (which is also the last and only  Retro  layer for RETRO SC 6 superscript subscript absent 6 SC {}_{6}^{\\textsc{SC}} start_FLOATSUBSCRIPT 6 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT SC end_POSTSUPERSCRIPT ), certain information from the input tokens may get diluted, preventing direct copying of tokens from the encoded chunks with chunked cross-attention.  We hypothesize that adding copying examples to the training would improve the models ability to copy, and  Retro  layers could be included closer to the beginning of processing instead of only in the sixth layer.  Accurate copying is important because when completing code, we want to extract not only the semantic meaning from the found snippets but also exact facts (e.g., the exact name of an existing function and not a hallucinated but semantically similar name).",
            "In Section  5.2 , we found that retrieval generally benefits model predictions, but there are still cases where retrieval degrades the accuracy of predictions.  In this section, we conduct a qualitative review of successful and less successful line completion examples, using our GPT SC 9 superscript subscript absent 9 SC {}_{9}^{\\textsc{SC}} start_FLOATSUBSCRIPT 9 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT SC end_POSTSUPERSCRIPT  and RETRO SC 6 superscript subscript absent 6 SC {}_{6}^{\\textsc{SC}} start_FLOATSUBSCRIPT 6 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT SC end_POSTSUPERSCRIPT  models.",
            "As discussed in Section  5.2.4 , the advantage of retrieval-augmented completion becomes apparent when highly similar examples to the input context are found in the project.  An example of repetitive code occurs when writing tests, which also involves the use of objects defined within the project.  In most cases, the model can focus on the relevant information in the context and successfully skip over unhelpful retrieved snippets.  However, when the model performs worse with retrieval, it is often due to misleading snippets that conflict with the information of the current file.  For example, the model might use a function call as found in a related file instead of how it should be used according to the current file, as seen in Figure  12 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :    Comparison of the relative improvement of single-token metrics between RETRO gpt 6 superscript subscript absent 6 gpt {}_{6}^{\\textsc{gpt}} start_FLOATSUBSCRIPT 6 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT gpt end_POSTSUPERSCRIPT  in GPT gpt 9 superscript subscript absent 9 gpt {}_{9}^{\\textsc{gpt}} start_FLOATSUBSCRIPT 9 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT gpt end_POSTSUPERSCRIPT  on 500 projects from the test set with respect to the used retrieval database.  The metrics are in percentages, contain 95-percent confidence intervals and are calculated on the offset input  x  64 superscript x absent 64 x^{\\geq 64} italic_x start_POSTSUPERSCRIPT  64 end_POSTSUPERSCRIPT .",
        "table": "S5.T6.24",
        "footnotes": [],
        "references": [
            "This paper is organized into six sections.  In Section  2 , we present related work in retrieval-augmented code completion.  In Section  3 , we present the main methods used, including an overview of the RETRO architecture  (Borgeaud et al.,  2022 ) .  In Section  4 , we describe the setup of our experiments, including the dataset, model training details, and metrics used.  In Section  5 , we present and analyze the results of our trained models evaluated with retrieval from local projects.  We conclude in Section  6 , where we also present ideas for further work.",
            "From Table  6 , we can see that the single-token metrics using only RETRO gpt 6 superscript subscript absent 6 gpt {}_{6}^{\\textsc{gpt}} start_FLOATSUBSCRIPT 6 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT gpt end_POSTSUPERSCRIPT  with the training set (without the test set) for retrieval are comparable to those of GPT gpt 9 superscript subscript absent 9 gpt {}_{9}^{\\textsc{gpt}} start_FLOATSUBSCRIPT 9 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT gpt end_POSTSUPERSCRIPT .  This can be attributed to the fact that retrieving from the training set does not provide new information, as both models have seen the entire training set during training.  We therefore conclude that retrieval is only meaningful on new, previously unseen data for the model.  When retrieving from projects, the data is new, as the projects are sampled from the test set.  An implication of this observation is that it might be beneficial to train RETRO using a retrieval database different from the training set.  The training could also include examples of random chunks to help the model better learn to skip irrelevant information, and examples of same-document snippets to help the model better learn to copy correct answers.",
            "In Figure  6 , we compare our trained models, RETRO and GPT-2, using both GPT-2 and StarCoder tokenizations.  The metrics confirm the advantage of StarCoder tokenization, which better handles the whitespace issue mentioned in Section  4.1 .  Similar to the single-token metrics, with multi-token predictions, we observe that the 6-layer RETRO achieves significantly better performance than the 6-layer GPT-2.  However, the difference between the 9-layer GPT-2 and the 6-layer RETRO is not as pronounced, especially with the StarCoder tokenization.",
            "In Figure  6 , we also report the metrics for the In-context RAG approach (more in Section  5.2.3 ).  Despite its simplicity, the contribution of the In-context RAG is more significant than the use of RETRO.  When combining In-context RAG with RETRO, we observe smaller improvements compared to enhancing GPT-2 with In-context RAG.  A possible explanation is that including retrieved snippets into the input context  confuses  RETROs chunk based retrieval, which now finds neighbors of the already retrieved snippets for part of the context.  Additionally, snippets included in the context receive the same treatment as other input tokens through processing in the transformer, while RETRO incorporates the retrieved chunks using cross-attention only in the sparsely dispersed  Retro  layers.",
            "Since the found snippets are included directly in the models context, it is important that the model supports a sufficiently large context to append the retrieved data in addition to the input context.  When using our models with a context size of 384 tokens, we often need to truncate the input sequence to leave enough space to include the retrieved snippets at the beginning of the context.  For our models, GPT-2 and RETRO, we include only one retrieved snippet due to the small context size of the model.  For models with larger context sizes, we include two retrieved snippets.  In Table  7  and in Figures  6  and  7 , we report multi-token prediction metrics both with and without retrieval as described in Section  3.3 .  The results indicate that using In-context RAG leads to improvements in multi-token predictions compared to the baseline models in all cases."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :    Results of comparing smaller and larger models on RepoEval.  The results for code-davinci-002 are from  Zhang et al. ( 2023b ) .",
        "table": "S5.T7.58",
        "footnotes": [],
        "references": [
            "Using small models is beneficial for local model execution, but one of the advantages of LLMs is their ability to scale by increasing the number of parameters  (Kaplan et al.,  2020 ) .  In Figure  7  and Table  7 , we examine the contribution of larger models compared to our smaller models.  The open-source models StarCoder 164M  (with  164  10 6 164 superscript 10 6 164\\times 10^{6} 164  10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT  parameters) and StarCoder 15.5B  (with  15.5  10 9 15.5 superscript 10 9 15.5\\times 10^{9} 15.5  10 start_POSTSUPERSCRIPT 9 end_POSTSUPERSCRIPT  parameters) are trained on the entire StarCoder dataset and further fine-tuned on Python  (Li et al.,  2023 ) .  StarCoder 164M  serves as a good comparison to our models due to its comparable number of parameters and for assessing the contribution of training on a larger dataset with a larger context size (8192 compared to our size of 384).",
            "Increasing the number of parameters in some cases more than doubles the performance compared to smaller models, as seen in Table  7 .  However, with the use of In-context RAG, even smaller models can achieve results comparable to larger models (without retrieval).  The positive impact of retrieval is not limited to smaller models, as incorporating useful information from the project into the context benefits the completions of larger models as well.",
            "Since the found snippets are included directly in the models context, it is important that the model supports a sufficiently large context to append the retrieved data in addition to the input context.  When using our models with a context size of 384 tokens, we often need to truncate the input sequence to leave enough space to include the retrieved snippets at the beginning of the context.  For our models, GPT-2 and RETRO, we include only one retrieved snippet due to the small context size of the model.  For models with larger context sizes, we include two retrieved snippets.  In Table  7  and in Figures  6  and  7 , we report multi-token prediction metrics both with and without retrieval as described in Section  3.3 .  The results indicate that using In-context RAG leads to improvements in multi-token predictions compared to the baseline models in all cases."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 :    Proportions of improved and worsened examples based on prefix similarity when comparing the baseline model (rows) with the model with retrieval (columns) on the  lineR  dataset.  The first column of the first row tells us that RETRO SC 6 superscript subscript absent 6 SC {}_{6}^{\\textsc{SC}} start_FLOATSUBSCRIPT 6 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT SC end_POSTSUPERSCRIPT  completes the line better than GPT SC 9 superscript subscript absent 9 SC {}_{9}^{\\textsc{SC}} start_FLOATSUBSCRIPT 9 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT SC end_POSTSUPERSCRIPT  in 12.3% of cases, worse in 7.9% of cases, and in the remaining cases, the predictions of both models are the same.",
        "table": "S5.T8.7",
        "footnotes": [],
        "references": [
            "In Section  5.1 , we estimate the upper bound of single-token metrics for RETRO by not removing chunks retrieved from the same file as the input context.  Similarly, we want to determine the upper bound of model performance when using In-context RAG if the retrieved snippets can contain the continuation of the current file we are completing.  This means that the context includes the line we want to complete, allowing the model to simply copy it from the context.  In Figure  8 , we see that copying from retrieved snippets is very effective, while copying with RETRO without In-context RAG performs similarly to using RETRO with In-context RAG without copying.  In addition to the explanations in Section  5.1 , we also note that because the first  Retro  layer is the sixth layer (which is also the last and only  Retro  layer for RETRO SC 6 superscript subscript absent 6 SC {}_{6}^{\\textsc{SC}} start_FLOATSUBSCRIPT 6 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT SC end_POSTSUPERSCRIPT ), certain information from the input tokens may get diluted, preventing direct copying of tokens from the encoded chunks with chunked cross-attention.  We hypothesize that adding copying examples to the training would improve the models ability to copy, and  Retro  layers could be included closer to the beginning of processing instead of only in the sixth layer.  Accurate copying is important because when completing code, we want to extract not only the semantic meaning from the found snippets but also exact facts (e.g., the exact name of an existing function and not a hallucinated but semantically similar name).",
            "Using RETRO or In-context RAG generally improves multi-token predictions.  However, there are cases where the model without retrieval correctly completes a line, while the model with retrieval does not, as summarized in Table  8 .  Most examples remain unchanged regardless of retrieval use, suggesting that retrieval is beneficial only in specific cases.  While positive examples dominate, the proportion of worsened results is not negligible and we want to reduce it.",
            "Now we focus only on the examples highlighted in Table  8 , where we detect worsening or an improvement in metrics.  From Figure  10 , we find that the impact of retrieval is negative mainly at low similarities to the query, while the positive impact is more present at higher similarities.  To mitigate the consequences of poor retrieval, we examine the idea of whether it makes sense to limit retrieval to found snippets with similarities higher than some threshold value.  Unfortunately, the right side of Figure  10  does not show a significant benefit from limiting to sufficiently similar snippets."
        ]
    },
    "id_table_9": {
        "caption": "Table 9 :    Comparison of model training on a smaller training dataset using BERT and CodeT5+ for computing chunk embeddings.  GPT-2 tokenization is used.",
        "table": "A2.T9.7",
        "footnotes": [],
        "references": [
            "Intuitively, we expect the benefit of retrieval-augmented line completion to depend on the quality of the retrieved snippets.  Therefore, we first examine the impact of the retrieved snippets similarities on the exact match metric  EM EM \\operatorname{EM} roman_EM  with the results shown in Figure  9 .  As expected, the metric increases with higher matching according to the Jaccard similarity coefficient.  We also find that most of the metric improvement comes from projects where snippets with high similarity to the query from the input context are found.  The benefits of retrieval are thus project-dependent, with greater benefits in projects with repetitive code, as a high Jaccard similarity indicates token overlap between files.",
            "We test the impact of using the BERT encoder versus CodeT5+ on a smaller dataset (6.7% of the total training set) and find that using the larger BERT encoder for chunk embeddings does not result in significant changes in test metrics  see Table  9 .  The comparison is not exhaustive due to the smaller training set and shorter training duration."
        ]
    },
    "id_table_10": {
        "caption": "Table 10 :  Comparison of the retrieval database sizes using two different vocabularies for tokenization.  The total size of all files before tokenization is 62 GiB.",
        "table": "A3.T10.4",
        "footnotes": [],
        "references": [
            "Now we focus only on the examples highlighted in Table  8 , where we detect worsening or an improvement in metrics.  From Figure  10 , we find that the impact of retrieval is negative mainly at low similarities to the query, while the positive impact is more present at higher similarities.  To mitigate the consequences of poor retrieval, we examine the idea of whether it makes sense to limit retrieval to found snippets with similarities higher than some threshold value.  Unfortunately, the right side of Figure  10  does not show a significant benefit from limiting to sufficiently similar snippets.",
            "Due to the large number of chunks in the database, exhaustive searching for the exact  k k k italic_k -nearest chunks is not feasible, so we resort to  approximate  searching instead.  As a consequence of approximate search, we seek a compromise between the search accuracy, search speed, index building speed, and index memory usage.  In our implementation, we use the Faiss library  (Johnson et al.,  2021 )  to index the chunk embeddings 4 4 4 In Faiss, the used index can be compactly described by the string  OPQ32_128,IVF1048576_HNSW32,PQ32 . .  The search is sped up using IVF indexing  (Johnson et al.,  2021 )  in combination with the HNSW  (Malkov and Yashunin,  2020 )  algorithm.  Additionally, the vectors are compressed with the PQ  (Jegou et al.,  2008 )  vector codec and the OPQ  (Ge et al.,  2013 )  transformation.  Table  10  summarizes the space requirements for storing the database and retrieval index based on the tokenization method used."
        ]
    },
    "global_footnotes": [
        "An example is available at",
        ".",
        "Available at",
        ".",
        "In Faiss, the used index can be compactly described by the string",
        "."
    ]
}