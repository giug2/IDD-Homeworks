{
    "id_table_1": {
        "caption": "Table 1 :  Quantitative comparison with state-of-the-art RefSR methods, GAN-based methods, and Diffusion-based methods on real-world image super-resolution. Our ReFIR achieves consistent performance improvements in both fidelity and perceptual quality.",
        "table": "S5.T1.12.12",
        "footnotes": [],
        "references": [
            "Although scaling up restoration models has achieved remarkable success, existing LRMs may not always produce results that are faithful to the original scene, particularly when faced with heavily degraded images that surpass the LRMs capabilities (see  Fig.   1 ). This issue is similar to the hallucination problem observed in large language models (LLMs)  [ 21 ,  22 ] ,  e.g.  ChatGPT might generate nonsense responses when highly specialized questions exceed its knowledge boundary. Similarly, if one LRM has never seen a specific scene, it will struggle to restore corresponding images faithfully. By analogizing LLM to LRM, we define the phenomenon where LRMs generate textures inconsistent with the original scene when facing hard samples as the hallucination of LRMs.",
            "This work considers using retrieved reference images as an explicit part of the model. In contrast to the existing restoration pipeline, our ReFIR is parameterized by not only the internal knowledge from the network weights but also the external knowledge retrieved from suitable data representations.  Fig.   3  gives an overview of our ReFIR. In the following part, we will first give the technical details of the retriever for reference image retrieval in  Sec.   4.1 , followed by the cross image injection to inject the external data knowledge into the restoration process of LRMs in  Sec.   4.2 .",
            "Restoration with ideal reference.   We first compare on the RefSR dataset with real-world degradation. The compared methods includes state-of-the-art RefSR methods  [ 41 ,  42 ,  43 ] , GAN-based methods  [ 9 ,  8 ] , and recent Diffusion-based methods  [ 16 ,  20 ,  19 ,  17 ,  15 ] .  Tab.   1  gives the results. It can be seen that our method brings significant gains in  all  metrics on both fidelity (PSNR, SSIM) and perceptual quality (LPIPS, NIQE, FID) for the LRMs. Taking SUPIR as an example, our method brings a FID improvement of even 19.57 on the CUFED5 dataset. Moreover, similar performance gains can also be observed in SeeSR. For instance, equipping our ReFIR to SeeSR can lead to 0.38dB PSNR improvement, demonstrating the generalization of our ReFIR. It is noteworthy that the above superiority is obtained without any training or fine-tuning. Moreover, we also give visual comparisons in  Fig.   5 , and it can be seen that our method can generate details that are faithful to the original scene with the help of external knowledge from retrieved reference images.",
            "How does the proposed ReFIR work?   Extensive experiments have shown the state-of-the-art performance of our ReFIR. However, it seems not straightforward to understand how the retrieved reference images influence the image restoration process of the original LRMs. Here, we give an intuitive explanation. As shown in  Fig.   10 , for the latent at the  t t t italic_t -th time step on the latent manifold, there are two forces in different directions pulling it to produce the latent at the next  t  1 t 1 t-1 italic_t - 1 -th time step. One force is from the internal knowledge of frozen weights in LRMs, and the other is the external knowledge from the retrieved reference image through the proposed cross image injection mechanism. These two forces ultimately determine the latent of the next time step. Therefore, a restored image from our ReFIR can utilize both the internal knowledge in the original LRMs as well as the external knowledge in the retrieved image, thus alleviating the hallucination of the LRMs.",
            "In the proposed cross-image injection, the inter-chain attention is used to perform attention between  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  and  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT . Considering the domain gap between  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  and  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  due to the input quality difference, one may ask whether the results of the inter-chain attention are meaningful. Here, We visualize the attention map to validate the effectiveness of the inter-chain attention (see  Fig.   13 ). It can be seen that for a given query pixel query in  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , the inter-chain attention can effectively overcome the spatial misalignment, and find relevant pixel features in  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  for reference.",
            "We also provide quantitative ablation results on the control scale  s s s italic_s  in  Fig.   11 . It can be seen that when  s s s italic_s  is too small, the LRM will mainly use the knowledge contained within its parameters to restore high-quality images, which can lead to performance degradation due to the hallucination problem. On the other hand, when  s s s italic_s  is too large, the LRM will overuse the content in the retrieved reference image, thus producing patterns that are not present in the original LQ image. In practice, we adopt a moderate  s = 0.5 s 0.5 s=0.5 italic_s = 0.5  to trade off the hallucination and the overuse of the reference image.",
            "In  Tab.   1  of the main paper, we give the performance gains of incorporating the proposed ReFIR into the existing LRMs. Considering the randomness of the generative models, we give the performance fluctuations of ReFIR under multiple trials with exactly the same experimental setting and random seed. The results are given in  Tab.   10 . It can be seen that the randomness of the diffusion-based generative model is very small when using a fixed seed, reducing the disturbance from noise errors for evaluation. In addition, we further use hypothesis testing to verify the significance of performance gains, and the test results reject the original hypothesis H0 at 95% confidence level on all metrics and datasets, indicating that the performance gains from the proposed method are statistically significant.",
            "An important application of our method is in scenarios with high fidelity demands, such as scene text images with a specific stylistic structure, or face images with identity preservation, and here we preliminarily explore the application of the proposed ReFIR to real-world face image restoration. The results are given in  Fig.   15 . It can be seen that by using a high quality image of a specific persons identity as a reference, the resulting restoration results can better preserve the persons attributes. However, it should be noted that this experiment is just a preliminary attempt, and we will leave the further improvement of our ReFIR for specific downstream restoration tasks for future work.",
            "In  Fig.   12 , we give more samples of the PCA visualization on the top three principal components of the self-attention layer latent.",
            "In  Fig.   13 , we give a visualization of the attention map from the cross-image injection, to help better understand the feasibility of cross-image attention.",
            "Fig.   14  gives more quantitative comparison results against the state-of-the-art method on real-world degradation without ground truth.",
            "Fig.   15  gives the visualization results of the extension experiments of applying the proposed ReFIR to blind face image restoration."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Quantitative comparison on real-world degradation with RealPhoto datasets.",
        "table": "S5.T2.3.3",
        "footnotes": [],
        "references": [
            "In order to manipulate the LRM so that it can utilize the retrieved reference images as external knowledge, we first delve into the underlying mechanism of existing LRMs to find useful insights. We choose the current popular LRM method SUPIR  [ 20 ]  as a representative. Inspired by previous image editing efforts   [ 44 ,  31 ,  30 ,  29 ] , which show that the self-attention layer of diffusion models contains important spatial correlation of an image, we thus follow this clue and employ the PCA to visualize the principal components of the latent from self-attention layers of SUPIR. We further utilize the Fourier analysis  [ 45 ]  to allow for quantitative results. The results are shown in  Fig.   2 .",
            "This work considers using retrieved reference images as an explicit part of the model. In contrast to the existing restoration pipeline, our ReFIR is parameterized by not only the internal knowledge from the network weights but also the external knowledge retrieved from suitable data representations.  Fig.   3  gives an overview of our ReFIR. In the following part, we will first give the technical details of the retriever for reference image retrieval in  Sec.   4.1 , followed by the cross image injection to inject the external data knowledge into the restoration process of LRMs in  Sec.   4.2 .",
            "Restoration in the wild.   The above experiments on RefSR datasets focus on utilizing the already provided reference images from the dataset, which applies when the user has relevant HQ images. In this section, we turn to more challenging scenarios in which the reference image has to be obtained by retrieval. Since the ground truth of RealPhoto datasets is unavailable, we use non-reference image quality assessment metrics,  i.e.  NIQE, MUSIQ, and CLIPIQA for evaluation. As shown in  Tab.   2 , our approach continues to produce significant gains over its non-ReFIR counterparts. For instance, our SeeSR+ReFIR surpasses the original SeeSR by 0.2866 NIQE and 1.59 MUSIQ. Since the retrieved image can not serve as an ideal reference, the above favorable results demonstrate the robustness of our ReFIR in the face of real-world retrieved images. We also give quantitative results in  Fig.   6 . Even under severe real-world degradation, our method maintains good perceptual quality.",
            "What is the impact of the control scale?   The scale  s s s italic_s  in  Eq.   2  can control the extent to which the LRMs use external knowledge from the retrieved reference image for restoration. Here, we conduct an ablation study to explore the effect of  s s s italic_s . The results are shown in  Fig.   9 . It can be seen that when  s s s italic_s  takes smaller values, the model mainly uses the internal knowledge embedded in its own parameters, which can make the model hallucinate when the degradation is severe. For example, the model produces incorrect textures when  s = 0 s 0 s=0 italic_s = 0 . As  s s s italic_s  increases, the model starts to use external knowledge from the retrieved reference image, from which the models hallucination problem can be alleviated. We also provide quantitative ablation experiments on  s s s italic_s  in  Appendix   C .",
            "In the main paper, we mainly focus on the case of one single retrieved image. However, in practice, there may be multiple available reference images at hand, and using multiple reference images for resemblance could intuitively gain better performance. To this end, we extend the original cross-image injection to allow to incorporation of multiple reference images for reconstruction.  Our key idea is to modify the scale factor  s s s italic_s  in  Eq.   2  from a scalar into a vector:  s = { s 1 , s 2 ,  , s k } s subscript s 1 subscript s 2  subscript s k \\mathbf{s}=\\{s_{1},s_{2},\\cdots,s_{k}\\} bold_s = { italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ,  , italic_s start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } , where   s n = 1 subscript s n 1 \\sum s_{n}=1  italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = 1 . Each  s n subscript s n s_{n} italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  can be obtained by computing the cosine similarity between  I L  Q subscript I L Q I_{LQ} italic_I start_POSTSUBSCRIPT italic_L italic_Q end_POSTSUBSCRIPT  and the corresponding  n n n italic_n -th retrieved image in  I R subscript I R \\mathbf{I_{R}} bold_I start_POSTSUBSCRIPT bold_R end_POSTSUBSCRIPT  followed by Softmax normalization. Then we can modify the original single-reference  cross-image injection of  Eq.   2  to the following multi-reference version:",
            "In  Fig.   12 , we give more samples of the PCA visualization on the top three principal components of the self-attention layer latent."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Comparison of model complexity before and after incorporating our ReFIR. We use an input image with the resolution of  2048  2048 2048 2048 2048\\times 2048 2048  2048  to evaluate the GPU memory and the inference time on one single 80G NVIDIA A100 GPU.",
        "table": "S5.T3.9.7",
        "footnotes": [],
        "references": [
            "To this end, we delve deep into the working mechanisms of LRMs for insightful observations. Details of the experimental setup are described in  Sec.   3 . Our key findings indicate that the workflow of LRMs can be divided into two distinct stages: the  Denoising Structure Reconstruction  stage, during which the self-attention in the ControlNet  [ 26 ]  reconstructs a clear overall structure from the noised representation. After that, in the  Detail Texture Restoration  stage, the self-attention in the UNet  [ 27 ]  decoder fills scene-specific textures based on the denoised structure map. Based on these findings, a natural solution emerged: we can transfer high-quality, scene-specific textures from the retrieved images to the low-quality images during the detail texture restoration stage. In this way, the restored image is allowed a consistent texture with the retrieved image, thus mitigating the hallucination.",
            "This work considers using retrieved reference images as an explicit part of the model. In contrast to the existing restoration pipeline, our ReFIR is parameterized by not only the internal knowledge from the network weights but also the external knowledge retrieved from suitable data representations.  Fig.   3  gives an overview of our ReFIR. In the following part, we will first give the technical details of the retriever for reference image retrieval in  Sec.   4.1 , followed by the cross image injection to inject the external data knowledge into the restoration process of LRMs in  Sec.   4.2 .",
            "In this work, we implement a conceptually simple solution of  R R \\mathcal{R} caligraphic_R , which uses the query image  I L  Q subscript I L Q I_{LQ} italic_I start_POSTSUBSCRIPT italic_L italic_Q end_POSTSUBSCRIPT  to retrieve its  k k k italic_k  nearest neighbor in  D D \\mathcal{D} caligraphic_D  using cosine similarity in the compact feature space derived from any feature extractors, such as VGG  [ 46 ] , ResNet  [ 47 ]  or CLIP  [ 48 ] . Since the  D D \\mathcal{D} caligraphic_D  is fixed, in practice, we can pre-extract and store the compact feature before training. Given a sufficiently large database  D D \\mathcal{D} caligraphic_D , this strategy ensures that the set of neighbors  I R subscript I R \\mathbf{I_{R}} bold_I start_POSTSUBSCRIPT bold_R end_POSTSUBSCRIPT  shares sufficient semantic consistency with  I L  Q subscript I L Q I_{LQ} italic_I start_POSTSUBSCRIPT italic_L italic_Q end_POSTSUBSCRIPT  and thus provides useful visual information for the restoration. Although this scheme seems simple, we show that it is efficient and effective, please see  Sec.   5.3  for discussion.",
            "Separate attention.   To allow the  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to learn the knowledge from the  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , an effective interaction between the latents is crucial.  Inspired by the observation in  Sec.   3 , we aim to transfer the knowledge embedded in the self-attention layer of  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT s decoder to the counterpart of  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT .  To this end, we modify the original self-attention in  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to our proposed separate attention.  The core idea of our separate attention is to add inter-chain cross-attention to the original intra-chain self-attention so that  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  can attend high-quality texture knowledge from  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  while preserving its original features.  As shown in  Fig.   4 (a), formally, denote  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ,  K T subscript K T K_{T} italic_K start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ,  V T subscript V T V_{T} italic_V start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  as the query, key and value from the  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , and  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ,  V S subscript V S V_{S} italic_V start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  as the key and value from the  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , the intra-chain self-attention preserves the original attention of  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to obtain the output  O i  n  t  r  a subscript O i n t r a O_{intra} italic_O start_POSTSUBSCRIPT italic_i italic_n italic_t italic_r italic_a end_POSTSUBSCRIPT , and the inter-chain cross-attention uses the  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to query the  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  and  V S subscript V S V_{S} italic_V start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  to facilitate  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  utilizing the knowledge from  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  to get the result  O i  n  t  e  r subscript O i n t e r O_{inter} italic_O start_POSTSUBSCRIPT italic_i italic_n italic_t italic_e italic_r end_POSTSUBSCRIPT . In short, the proposed separate attention can be formalized as follows:",
            "It is worth mentioning that directly using  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to query the concatenate results of  K T subscript K T K_{T} italic_K start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  and  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT   can only yield sub-optimal results due to the domain preference issue,  i.e. ,  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  will prefer latent from the same domain  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  even though  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  is more helpful for reconstruction. By using the proposed separate attention, the  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  is separated to attend  K T subscript K T K_{T} italic_K start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  and  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , thus effectively mitigating this problem. We give more discussion in  Sec.   5.3 .",
            "Complexity analysis.    Tab.   3  gives the comparison of the computational complexity, including the number of parameters, GPU cost, and the inference latency. We also give the restoration performance for a more comprehensive comparison. As for the parameters, our ReFIR can facilitate both fidelity and realistic image restoration using the same #param as the original base LRMs. For the GPU memory, since our ReFIR uses two images as input,  i.e. , one LQ image, and one reference image, the GPU cost will become larger than the original one. For instance, it rises 1.38 times the increase of SUPIR+ReFIR than the original SUPIR model. Moreover, the inference time also increases due to more inputs as well as the additional interaction between two chains. In the future, we will delve deep into the effective utilization of retrieved images while maintaining efficiency.",
            "Other choices on injection position.   In  Sec.   3 , we find the diffusion decoder is responsible for restoring textures. Based on this observation we propose to apply cross-image injection on the UNet decoder. Here, we ablate to analyze the impact of different cross-image injection positions. The results are shown in  Tab.   5 . It can be seen that performing cross-image injection only on the encoder will cause 19.57 FID drops. This is because the encoder focuses on the structure reconstruction, thus transferring the structure of  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  will destroy the layout of the  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT . Moreover, performing injection only in the decoder achieves the best results since it can transfer the high-quality textures from the  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT . Due to the page limit, more ablation experiments can be seen in  Appendix   C .",
            "In the proposed cross-image injection, the inter-chain attention is used to perform attention between  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  and  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT . Considering the domain gap between  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  and  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  due to the input quality difference, one may ask whether the results of the inter-chain attention are meaningful. Here, We visualize the attention map to validate the effectiveness of the inter-chain attention (see  Fig.   13 ). It can be seen that for a given query pixel query in  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , the inter-chain attention can effectively overcome the spatial misalignment, and find relevant pixel features in  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  for reference.",
            "In  Fig.   13 , we give a visualization of the attention map from the cross-image injection, to help better understand the feasibility of cross-image attention."
        ]
    },
    "id_table_4": {
        "caption": "Table 8 :  Experiments on extending to use multiple retrieved images for restoration. The inference time is evaluated on A100 GPU.",
        "table": "S5.SS3.4.4.4.4",
        "footnotes": [],
        "references": [
            "This work considers using retrieved reference images as an explicit part of the model. In contrast to the existing restoration pipeline, our ReFIR is parameterized by not only the internal knowledge from the network weights but also the external knowledge retrieved from suitable data representations.  Fig.   3  gives an overview of our ReFIR. In the following part, we will first give the technical details of the retriever for reference image retrieval in  Sec.   4.1 , followed by the cross image injection to inject the external data knowledge into the restoration process of LRMs in  Sec.   4.2 .",
            "Given the retrieved reference images  I R = R  ( I L  Q , D ) subscript I R R subscript I L Q D \\mathbf{I_{R}}=\\mathcal{R}(I_{LQ},\\mathcal{D}) bold_I start_POSTSUBSCRIPT bold_R end_POSTSUBSCRIPT = caligraphic_R ( italic_I start_POSTSUBSCRIPT italic_L italic_Q end_POSTSUBSCRIPT , caligraphic_D ) , we further propose the cross image injection to allow the original LRMs to use the external knowledge from  I R subscript I R \\mathbf{I_{R}} bold_I start_POSTSUBSCRIPT bold_R end_POSTSUBSCRIPT .  As shown in  Fig.   4 , we first construct two parallel denoising chains: the target restoration chain  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  which is used to restore  I L  Q subscript I L Q I_{LQ} italic_I start_POSTSUBSCRIPT italic_L italic_Q end_POSTSUBSCRIPT , and the source reference chain  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  which unfolds  I R subscript I R \\mathbf{I_{R}} bold_I start_POSTSUBSCRIPT bold_R end_POSTSUBSCRIPT  into denoising time steps. After that, we introduce separate attention to separately perform attention within and between chains, followed by spatial adaptive gating to filter out irrelevant pixels. At last, we use the distribution alignment to mitigate the domain gap between chains. More details are given below.",
            "Separate attention.   To allow the  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to learn the knowledge from the  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , an effective interaction between the latents is crucial.  Inspired by the observation in  Sec.   3 , we aim to transfer the knowledge embedded in the self-attention layer of  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT s decoder to the counterpart of  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT .  To this end, we modify the original self-attention in  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to our proposed separate attention.  The core idea of our separate attention is to add inter-chain cross-attention to the original intra-chain self-attention so that  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  can attend high-quality texture knowledge from  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  while preserving its original features.  As shown in  Fig.   4 (a), formally, denote  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ,  K T subscript K T K_{T} italic_K start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ,  V T subscript V T V_{T} italic_V start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  as the query, key and value from the  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , and  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ,  V S subscript V S V_{S} italic_V start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  as the key and value from the  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , the intra-chain self-attention preserves the original attention of  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to obtain the output  O i  n  t  r  a subscript O i n t r a O_{intra} italic_O start_POSTSUBSCRIPT italic_i italic_n italic_t italic_r italic_a end_POSTSUBSCRIPT , and the inter-chain cross-attention uses the  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to query the  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  and  V S subscript V S V_{S} italic_V start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  to facilitate  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  utilizing the knowledge from  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  to get the result  O i  n  t  e  r subscript O i n t e r O_{inter} italic_O start_POSTSUBSCRIPT italic_i italic_n italic_t italic_e italic_r end_POSTSUBSCRIPT . In short, the proposed separate attention can be formalized as follows:",
            "To address this spatial misalignment, we propose the spatial adaptive gating to selectively fuse  O i  n  t  r  a subscript O i n t r a O_{intra} italic_O start_POSTSUBSCRIPT italic_i italic_n italic_t italic_r italic_a end_POSTSUBSCRIPT  and  O i  n  t  e  r subscript O i n t e r O_{inter} italic_O start_POSTSUBSCRIPT italic_i italic_n italic_t italic_e italic_r end_POSTSUBSCRIPT  without introducing additional parameters ( Fig.   4 (b)). Specifically, given latents at specific denoising blocks from  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  and  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , respectively, we first flatten them along the spatial dimension to obtain  h T , h S  R C  H  W subscript h T subscript h S superscript R C H W \\mathbf{h_{T}},\\mathbf{h_{S}}\\in\\mathbb{R}^{C\\times HW} bold_h start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT , bold_h start_POSTSUBSCRIPT bold_S end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_C  italic_H italic_W end_POSTSUPERSCRIPT . Next, we compute their pixel-wise cosine similarity to obtain the similarity matrix  sim  R H  W  H  W sim superscript R H W H W \\mathrm{sim}\\in\\mathbb{R}^{HW\\times HW} roman_sim  blackboard_R start_POSTSUPERSCRIPT italic_H italic_W  italic_H italic_W end_POSTSUPERSCRIPT . Since the  i i i italic_i -th row of  sim sim \\mathrm{sim} roman_sim  represents the similarity of the  i i i italic_i -th pixel in  h T subscript h T \\mathbf{h_{T}} bold_h start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT  to all the pixels in  h S subscript h S \\mathbf{h_{S}} bold_h start_POSTSUBSCRIPT bold_S end_POSTSUBSCRIPT , therefore, a large sum of the  i i i italic_i -th row indicates a large impact of  h S subscript h S \\mathbf{h_{S}} bold_h start_POSTSUBSCRIPT bold_S end_POSTSUBSCRIPT  in restoring the  i i i italic_i -th pixel of  h T subscript h T \\mathbf{h_{T}} bold_h start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT . Following this idea, we summation over the  i i i italic_i -th row of the  sim sim \\mathrm{sim} roman_sim  to approximate the utility of  h S subscript h S \\mathbf{h_{S}} bold_h start_POSTSUBSCRIPT bold_S end_POSTSUBSCRIPT  to the  i i i italic_i -th pixel of  h T subscript h T \\mathbf{h_{T}} bold_h start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT . Finally, we reshape this summation results back to 2D shape and use min-max normalization to restrict the range to  [ 0 , 1 ] 0 1 [0,1] [ 0 , 1 ] , to get the pixel-wise mask  M M \\mathcal{M} caligraphic_M  for adaptive gated fusion:",
            "Ablation on cross image injection.   In the proposed cross image injection, we use separate attention (SA), spatial adaptive gating (SG), and distribution alignment (DA) for effective external knowledge injection. Here, we ablate to validate the effectiveness of different components. We use SUPIR+ReFIR as a representative on the CUFED5 dataset and use the scalar weighted sum when SG is removed. The results are shown in  Tab.   4 . One can see that using fixed scalar weights instead of spatial adaptive gating results in a 0.18 NIQE drop. This is because not all pixels of the reference image are useful, and thus fine-grain gated mask is needed. Moreover, removing the distribution alignment also impairs performance,  e.g. , 4.36 FID drop, since the distribution of raw fusion results  O f  u  s  e subscript O f u s e O_{fuse} italic_O start_POSTSUBSCRIPT italic_f italic_u italic_s italic_e end_POSTSUBSCRIPT  does not match  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , and directly inject  O f  u  s  e subscript O f u s e O_{fuse} italic_O start_POSTSUBSCRIPT italic_f italic_u italic_s italic_e end_POSTSUBSCRIPT  to the denoising chain of  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  can cause sub-optimal results.",
            "Fig.   14  gives more quantitative comparison results against the state-of-the-art method on real-world degradation without ground truth."
        ]
    },
    "id_table_5": {
        "caption": "Table 10 :  Performance fluctuations under different experiment trials. We use ten trails to obtain a stable fluctuation range.",
        "table": "S5.SS3.8.8.4.4",
        "footnotes": [],
        "references": [
            "In this work, we implement a conceptually simple solution of  R R \\mathcal{R} caligraphic_R , which uses the query image  I L  Q subscript I L Q I_{LQ} italic_I start_POSTSUBSCRIPT italic_L italic_Q end_POSTSUBSCRIPT  to retrieve its  k k k italic_k  nearest neighbor in  D D \\mathcal{D} caligraphic_D  using cosine similarity in the compact feature space derived from any feature extractors, such as VGG  [ 46 ] , ResNet  [ 47 ]  or CLIP  [ 48 ] . Since the  D D \\mathcal{D} caligraphic_D  is fixed, in practice, we can pre-extract and store the compact feature before training. Given a sufficiently large database  D D \\mathcal{D} caligraphic_D , this strategy ensures that the set of neighbors  I R subscript I R \\mathbf{I_{R}} bold_I start_POSTSUBSCRIPT bold_R end_POSTSUBSCRIPT  shares sufficient semantic consistency with  I L  Q subscript I L Q I_{LQ} italic_I start_POSTSUBSCRIPT italic_L italic_Q end_POSTSUBSCRIPT  and thus provides useful visual information for the restoration. Although this scheme seems simple, we show that it is efficient and effective, please see  Sec.   5.3  for discussion.",
            "It is worth mentioning that directly using  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to query the concatenate results of  K T subscript K T K_{T} italic_K start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  and  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT   can only yield sub-optimal results due to the domain preference issue,  i.e. ,  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  will prefer latent from the same domain  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  even though  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  is more helpful for reconstruction. By using the proposed separate attention, the  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  is separated to attend  K T subscript K T K_{T} italic_K start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  and  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , thus effectively mitigating this problem. We give more discussion in  Sec.   5.3 .",
            "Restoration with ideal reference.   We first compare on the RefSR dataset with real-world degradation. The compared methods includes state-of-the-art RefSR methods  [ 41 ,  42 ,  43 ] , GAN-based methods  [ 9 ,  8 ] , and recent Diffusion-based methods  [ 16 ,  20 ,  19 ,  17 ,  15 ] .  Tab.   1  gives the results. It can be seen that our method brings significant gains in  all  metrics on both fidelity (PSNR, SSIM) and perceptual quality (LPIPS, NIQE, FID) for the LRMs. Taking SUPIR as an example, our method brings a FID improvement of even 19.57 on the CUFED5 dataset. Moreover, similar performance gains can also be observed in SeeSR. For instance, equipping our ReFIR to SeeSR can lead to 0.38dB PSNR improvement, demonstrating the generalization of our ReFIR. It is noteworthy that the above superiority is obtained without any training or fine-tuning. Moreover, we also give visual comparisons in  Fig.   5 , and it can be seen that our method can generate details that are faithful to the original scene with the help of external knowledge from retrieved reference images.",
            "Other choices on injection position.   In  Sec.   3 , we find the diffusion decoder is responsible for restoring textures. Based on this observation we propose to apply cross-image injection on the UNet decoder. Here, we ablate to analyze the impact of different cross-image injection positions. The results are shown in  Tab.   5 . It can be seen that performing cross-image injection only on the encoder will cause 19.57 FID drops. This is because the encoder focuses on the structure reconstruction, thus transferring the structure of  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  will destroy the layout of the  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT . Moreover, performing injection only in the decoder achieves the best results since it can transfer the high-quality textures from the  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT . Due to the page limit, more ablation experiments can be seen in  Appendix   C .",
            "An important application of our method is in scenarios with high fidelity demands, such as scene text images with a specific stylistic structure, or face images with identity preservation, and here we preliminarily explore the application of the proposed ReFIR to real-world face image restoration. The results are given in  Fig.   15 . It can be seen that by using a high quality image of a specific persons identity as a reference, the resulting restoration results can better preserve the persons attributes. However, it should be noted that this experiment is just a preliminary attempt, and we will leave the further improvement of our ReFIR for specific downstream restoration tasks for future work.",
            "Fig.   15  gives the visualization results of the extension experiments of applying the proposed ReFIR to blind face image restoration."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "S5.F10.5.5.5",
        "footnotes": [],
        "references": [
            "Restoration in the wild.   The above experiments on RefSR datasets focus on utilizing the already provided reference images from the dataset, which applies when the user has relevant HQ images. In this section, we turn to more challenging scenarios in which the reference image has to be obtained by retrieval. Since the ground truth of RealPhoto datasets is unavailable, we use non-reference image quality assessment metrics,  i.e.  NIQE, MUSIQ, and CLIPIQA for evaluation. As shown in  Tab.   2 , our approach continues to produce significant gains over its non-ReFIR counterparts. For instance, our SeeSR+ReFIR surpasses the original SeeSR by 0.2866 NIQE and 1.59 MUSIQ. Since the retrieved image can not serve as an ideal reference, the above favorable results demonstrate the robustness of our ReFIR in the face of real-world retrieved images. We also give quantitative results in  Fig.   6 . Even under severe real-world degradation, our method maintains good perceptual quality.",
            "How much do the reference images affect performance?   In the proposed framework, the retrieved images  I R subscript I R \\mathbf{I_{R}} bold_I start_POSTSUBSCRIPT bold_R end_POSTSUBSCRIPT  is crucial in alleviating hallucinations. Here, we try to answer the role of  I R subscript I R \\mathbf{I_{R}} bold_I start_POSTSUBSCRIPT bold_R end_POSTSUBSCRIPT  during restoration process, by manually controlling different types of retrieved images.  As shown in  Tab.   6 , we find that using the exact ground truth  I H  Q subscript I H Q I_{HQ} italic_I start_POSTSUBSCRIPT italic_H italic_Q end_POSTSUBSCRIPT  as the  I R subscript I R \\mathbf{I_{R}} bold_I start_POSTSUBSCRIPT bold_R end_POSTSUBSCRIPT  can further improve the performance, which can be seen as an ideal up-bound. Interestingly, using  I L  Q subscript I L Q I_{LQ} italic_I start_POSTSUBSCRIPT italic_L italic_Q end_POSTSUBSCRIPT  itself as its own retrieved image instead brings a slight improvement compared with no retrieval, which we attribute to the regularization effect from the distribution alignment strategy. Finally, randomly selecting a high-quality reference image even resulted in a huge performance degradation, suggesting that the content correlation is more important than the image quality for a favorable retrieved reference image."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A2.SS0.SSS0.Px1.3.3.3",
        "footnotes": [],
        "references": [
            "Effectiveness of the reference retriever.     In order to obtain content-relevant retrieved images, we present a simple but inference-efficient retriever  R R \\mathcal{R} caligraphic_R  that uses the high-level semantic vectors from the pre-trained deep models for similarity matching in the high-quality image dataset  D D \\mathcal{D} caligraphic_D . Despite the simple design, we here demonstrate its effectiveness in  Fig.   7 . Since semantically consistent images usually contain similar textures,  e.g. , the texture in the first elephant image can help in the restoration of the LQ elephant image, and thus the proposed retriever can yield satisfactory retrieval results. Although texture-based retrieval may be a better choice for image restoration, it usually necessitates additional training of new retrieval models. For simplicity, we adopt semantic-based retrieval and leave the exploration of more advanced reference retrievers for future work.",
            "Since our ReFIR relies on the retrieved images, it is interesting to explore extreme situations when highly relevant and high-quality reference images are scarce or even unavailable. To this end, we introduce the fallback strategies to handle this situation. Specifically, since our method does not modify the parameters of LRMs, we can directly use the original inference pipeline of the LRM without using reference images. We denote this as  origin  _  lrm origin _ lrm \\mathrm{origin\\_lrm} roman_origin _ roman_lrm . In addition, we also use the BLIP model to caption the LR image to obtain the text prompt, which will then be fed into the StableDiffusion2.0 model to generate semantic-similar high-quality images as the reference. We denote this as  gen  _  ref gen _ ref \\mathrm{gen\\_ref} roman_gen _ roman_ref . We use SeeSR  [ 19 ]  as a representative, on the real-world degradation dataset RealPhoto60  [ 20 ] . We first give the results in which all LR images adopt the fallback strategies in  Tab.   7 . It can be seen that using the SD2.0 generated images as the fallback image can bring slightly improvement compared with noReference. After that, we further develop task-oriented adaptive strategies to enhance the performance of ReFIR in real-world scenarios. In detial, we respectively use the retrieved images and the  gen  _  ref gen _ ref \\mathrm{gen\\_ref} roman_gen _ roman_ref  to generate the results. And then we select the one with a larger task score as the final result. We denote it as  ada  _  gen  _  ref ada _ gen _ ref \\mathrm{ada\\_gen\\_ref} roman_ada _ roman_gen _ roman_ref . From  Tab.   7 , it can be seen that the task-oriented strategy achieves a significant performance improvement against previous ReFIR baselines,  e.g. , 0.0183 CLIPIQA improvements, due to the fact that it works in the output end. However, this setup is accompanied by a larger inference time, and further acceleration on this fallback strategies can be an promising future work."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A2.T8.7.7",
        "footnotes": [],
        "references": [
            "The motivation behind the proposed separate attention is to address the domain preference problem,  i.e. , the attention in  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  will prefer to use latent from the same chain even though the latent from  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  is more helpful for reconstruction. To verify the existence of the domain preference, we use the ground truth  I H  R subscript I H R I_{HR} italic_I start_POSTSUBSCRIPT italic_H italic_R end_POSTSUBSCRIPT  as the input of  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  and compute the normalized attention scores between  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  and  K T subscript K T K_{T} italic_K start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ,  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  and  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT . It can be seen in  Fig.   8  that even using the spatially strictly aligned  I H  Q subscript I H Q I_{HQ} italic_I start_POSTSUBSCRIPT italic_H italic_Q end_POSTSUBSCRIPT  as the reference,  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  still has significantly high attention for the latent from the same chain, indicating that the domain preference problem interferes with the  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT s utilization of external knowledge in  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT . By contrast, the proposed separate attention can effectively mitigate this problem by forcing the  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to separately attend  K T subscript K T K_{T} italic_K start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  and  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT .",
            "For experiments with multiple reference images, we use SUPIR+ReFIR as a representative. Since the CUFED5 dataset contains multiple reference images, we directly use the provided images as the retrieved reference for reproducibility.  Tab.   8  gives the results. It can be seen that using multiple reference images produces better results than one single reference image,  e.g.  the 2.08 improvement in FID. However, it is worth noting that the marginal gain from adding reference images is diminishing, accompanied by a notable increase in computational cost. Therefore, in practice, we use one single reference image to balance the model performance and inference efficiency."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A3.SS0.SSS0.Px1.4.4.4",
        "footnotes": [],
        "references": [
            "What is the impact of the control scale?   The scale  s s s italic_s  in  Eq.   2  can control the extent to which the LRMs use external knowledge from the retrieved reference image for restoration. Here, we conduct an ablation study to explore the effect of  s s s italic_s . The results are shown in  Fig.   9 . It can be seen that when  s s s italic_s  takes smaller values, the model mainly uses the internal knowledge embedded in its own parameters, which can make the model hallucinate when the degradation is severe. For example, the model produces incorrect textures when  s = 0 s 0 s=0 italic_s = 0 . As  s s s italic_s  increases, the model starts to use external knowledge from the retrieved reference image, from which the models hallucination problem can be alleviated. We also provide quantitative ablation experiments on  s s s italic_s  in  Appendix   C .",
            "The proposed cross image injection mitigates the domain preference problem by using separate attention to promote latent in  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to attend  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT . Here, we conduct ablation to study the impact of different design choices of cross image injection. As shown in  Tab.   9 , directly replacing the original self-attention results from  O i  n  t  r  a subscript O i n t r a O_{intra} italic_O start_POSTSUBSCRIPT italic_i italic_n italic_t italic_r italic_a end_POSTSUBSCRIPT  in  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  with corresponding latent in  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  causes severe performance degradation, due to the significant loss of original knowledge in  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT .  In addition, using  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to query the concatenation results of  K T subscript K T K_{T} italic_K start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  and  K S subscript K S K_{S} italic_K start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  also causes a performance drop, which further confirms that the domain preference problem,  i.e. ,  Q T subscript Q T Q_{T} italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  prefers to use latent from the same chain  C T subscript C T \\mathcal{C}_{T} caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , even though  C S subscript C S \\mathcal{C}_{S} caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  is more helpful for reconstruction."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A6.T10.30.30",
        "footnotes": [],
        "references": [
            "How does the proposed ReFIR work?   Extensive experiments have shown the state-of-the-art performance of our ReFIR. However, it seems not straightforward to understand how the retrieved reference images influence the image restoration process of the original LRMs. Here, we give an intuitive explanation. As shown in  Fig.   10 , for the latent at the  t t t italic_t -th time step on the latent manifold, there are two forces in different directions pulling it to produce the latent at the next  t  1 t 1 t-1 italic_t - 1 -th time step. One force is from the internal knowledge of frozen weights in LRMs, and the other is the external knowledge from the retrieved reference image through the proposed cross image injection mechanism. These two forces ultimately determine the latent of the next time step. Therefore, a restored image from our ReFIR can utilize both the internal knowledge in the original LRMs as well as the external knowledge in the retrieved image, thus alleviating the hallucination of the LRMs.",
            "In  Tab.   1  of the main paper, we give the performance gains of incorporating the proposed ReFIR into the existing LRMs. Considering the randomness of the generative models, we give the performance fluctuations of ReFIR under multiple trials with exactly the same experimental setting and random seed. The results are given in  Tab.   10 . It can be seen that the randomness of the diffusion-based generative model is very small when using a fixed seed, reducing the disturbance from noise errors for evaluation. In addition, we further use hypothesis testing to verify the significance of performance gains, and the test results reject the original hypothesis H0 at 95% confidence level on all metrics and datasets, indicating that the performance gains from the proposed method are statistically significant."
        ]
    },
    "global_footnotes": [
        "Corresponding author: Tao Dai"
    ]
}