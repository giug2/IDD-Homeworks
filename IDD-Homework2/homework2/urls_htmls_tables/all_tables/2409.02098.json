{
    "id_table_1": {
        "caption": "Table 1:  OOD evaluation of baseline and CRAFT XL .",
        "table": "S5.T1.1.1",
        "footnotes": [],
        "references": [
            "CRAFT is used to fine-tune language models by generating task-specific synthetic datasets, given a few human-curated examples of the task.  During CRAFT (see Figure  1 ), we retrieve  human-written, free-text documents from a large collection  of corpora by calculating their similarity to the provided  few-shots and transforming them into  the task-specific format  through augmentation.  The only human effort required is in writing a small number of high-quality examples of the target task.  CRAFT has two phases: In the initial phase, an embedding database is created from large corpora.  While this phase can be resource-intensive, its cost is incurred only once for all subsequent tasks, and it can be easily expanded with new corpora.  In the second phase, the user-generated, task-specific few-shot examples are embedded,  enabling the retrieval of relevant documents by calculating similarity measures between few-shots and corpus documents.  Once relevant documents are retrieved,  an instruction-tuned LLM is used to  augment the retrieved free-text documents into a  task-specific design, generating synthetic task samples in  the layout that is needed for instruction-tuning (illustrated in Figure  1 ).  Finally, the synthetic dataset is used to fine-tune a task-specific language model.  We report implementation details for the whole CRAFT framework in Appendix  A .",
            "In Table  1 , we compare the human-curated baseline with CRAFT on the in-domain dataset for the baseline and four out-of-domain (OOD) datasets selected from MMLU  Hendrycks et al. ( 2021 ) . Although the baseline outperforms CRAFT by more than 11% in the biology subset of the ScienceQA test set (in-domain for the baseline), CRAFT outperforms the baseline on 3 out of 4 OOD datasets. On average, CRAFT outperforms the baseline by 4%."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Performance comparison of Llama and Mistral across different tasks and few-shot settings. Values show accuracy  standard deviation, with relative improvement in parentheses compared to the CRAFT XS  model.",
        "table": "S5.T2.1.1",
        "footnotes": [],
        "references": [
            "To address this, we utilize instruction-tuning prompt templates  (Sanh et al.,  2022 ; Maini et al.,  2024 )  to augment the free-text documents into task-specific training data while simultaneously eliminating noise.  A few-shot task template consists of three elements:  (i) one or more few-shots,  (ii) a corpus sample,  (iii) and a brief instruction for the model to generate instruction-output pairs from the content of the corpus sample.  Aside from the brief instruction,  it is easy to assemble  these templates  from material we already have.  The template only structures all information from the instruction, the few-shots, and the retrieved corpus samples to generate one continuous string that serves as input for the model generating the synthetic task samples.  In this setup, the contents of the few-shots serve as in-context examples for the completion of the instruction.  Figure  2 , step 3, shows an example of how these templates guide the model in augmenting the corpus samples into synthetic task samples.  Any instruction-tuned language model can be used for this purpose.  This augmentation step not only rephrases the text but also condenses the retrieved document down to the essential information required for the task.  The result of this step produces the final synthetic instruction-output pairs that can be used to fine-tune a language model.  Figure  2 , step 4, shows an actual example output from the generated pool of synthetic training samples, and Appendix  C  provides an overview of length statistics from the stages of corpus retrieval up to the synthesized input-output pairs.",
            "In previous sections, we fine-tuned CRAFT models using the pretrained Mistral 7B model. Now, we repeat the experiments using the pretrained Llama 3 8B model. We observe similar trends across all tasks, and the relative improvement is comparable when scaling up from few-shots to 25,000 examples, as illustrated in Table  2 .",
            "The BioQA few-shot text samples were compiled from a diverse range of sources, including textbooks  (Alberts,  2017 ; Malmquist and Prescott,  2022 ; Wilkin and Brainard,  2016 ; Rye et al.,  2016 ) , openly accessible materials, and Encyclopedia Britannica.  For MedQA, we primarily drew upon openly accessible websites, such as the National Institutes of Health, National Health Service, Food and Drug Administration, Mayo and Cleveland Clinics, and other medicine-related websites, to generate our few-shot text samples.  The CSQA few-shot text samples were sourced from a variety of online resources, including blogs, articles, and other websites tailored to the specific topic at hand.  From sources that are not openly accessible through websites, continuous text snippets were directly extracted and used as texts, while all other material was shortened, rephrased, or restructured by the authors.  This process ensures that articles which may have been crawled through C4  (Raffel et al.,  2020 )  do not produce exact matches at retrieval time.  Since none of these texts have direct question-answer pairs available, the question, answer options, and the answer were generated by the authors for each sample.  Table  2 , step 1, shows an example.",
            "In our experiments, all task samples are created using Mistral 7B Instruct v0.2  (Jiang et al.,  2023 )  along with the few-shot task template shown in Table  2 , step 3.  During the generation process, vLLM  (Kwon et al.,  2023 )  is used with temperature 0.7, top-k  (Fan et al.,  2018 )  and top-p  (Holtzman et al.,  2020 )  sampling at 40 and 0.9, respectively.  The maximum generation length was determined based on preliminary experiments with the goal of accommodating full-length sample generation.  For all QA datasets we limited samples to 256 tokens, whereas recipe generation and summarization were limited at 1280 and 1536 tokens, respectively."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  BioQA corpus and task sample filtering process.  Corpus samples are turned into task samples after filtering for excessive length.",
        "table": "A2.T3.1",
        "footnotes": [],
        "references": [
            "We report the performance gain when scaling up our training data in Figure  3 .  We report the mean  and standard deviation  across three seeds.  We observe consistent improvements across four tasks as we  increase the data size. Relative to the baseline models  trained with only few-shot examples, we see improvements of  17% (from 66.9 to 78.1), 12% (from 55.3 to 62.1), 23%  (from 39.1 to 48.0), and 124% (from 43.7 to 97.9) for  BioQA, CSQA, MedQA, and Summarization, respectively. This  shows that CRAFT can be used for diverse tasks, starting  with just a few curated examples. We also find  appropriate scaling for each set of examples, ranging from  100 to 25,000 across all tasks. Additionally,  that models trained with fewer examples (32, 100) exhibit  much more variance than those trained with 5,000 and 25,000  examples, as indicated by the gray regions in the plots that visualize the standard deviation.",
            "We now report experiments to understand the level of data contamination or similarity between test and training examples in the experiments introduced in Figure  3 . We conduct 5-gram weighted Jaccard similarity analyses between CRAFT datasets and the test data.  For each sample, we combine the instruction and output and gather 5-gram frequencies for the whole dataset.  We then calculate the Jaccard similarity between the 5-gram  frequency distributions of the respective CRAFT and test  dataset, where 5-grams receive weight proportional to their frequency.",
            "In all experiments, we manually curated 32-shot examples and expanded our synthetic data examples from that point. However, even curating 32 examples can be time-consuming. We can limit the number of few-shots to just eight examples to bootstrap the CRAFT process. Figure  3  shows the results compared to the CRAFT pipeline with 32-shots. While the final results with 25,000 examples are slightly lower for 8-shots, the trend is similar for both 8- and 32-shot examples. We observe that the model trained with 25,000 samples, based on running CRAFT with 8 few-shots, significantly outperforms the model trained with only 32 few-shots (i.e., no extra synthetic data). This suggests that if there are time and resource limitations, using CRAFT with fewer initial examples leads to better models than trying to curate and train only on more human-curated few-shot examples."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  CSQA corpus and task sample filtering process.  Corpus samples are turned into task samples after filtering for excessive length.",
        "table": "A2.T4.1",
        "footnotes": [],
        "references": [
            "Out of the five tasks we selected, we observe non-scaling behavior in one task: Recipe Generation. While our manually curated few-shots are of high quality, we see a drop when scaling from 32 to 25,000 examples, as illustrated in Figure  4 . CRAFTs performance is still better than the baseline with official human data, which means that the final dataset is usable. However, we explore why this reverse scaling occurs and examine the drop in performance."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  MedQA corpus and task sample filtering process.  Corpus samples are turned into task samples after filtering for excessive length.",
        "table": "A2.T5.1",
        "footnotes": [],
        "references": []
    },
    "id_table_6": {
        "caption": "Table 6:  RecipeGen corpus and task sample filtering process.  Corpus samples are turned into task samples after filtering for excessive length.",
        "table": "A2.T6.1",
        "footnotes": [],
        "references": [
            "We generate datasets for all tasks in sizes 100, 500, 5000, and 25,000.  We refer to the few-shots as a dataset of size XS, and to the sizes ranging from 100 to 25,000 as S, M, L, and XL, respectively.  Implementation details related to fine-tuning can be found in Appendix  A.6 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Summarization corpus and task sample filtering process.  Corpus samples are turned into task samples after filtering for excessive length.",
        "table": "A2.T7.1",
        "footnotes": [],
        "references": []
    },
    "id_table_8": {
        "caption": "Table 8:  Dataset Statistics.  TS is short for task sample.  For summarization, the instruction includes the model-generated long but cleaned text augmentation from a corpus sample that will subsequently be summarized.",
        "table": "A3.T8.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}