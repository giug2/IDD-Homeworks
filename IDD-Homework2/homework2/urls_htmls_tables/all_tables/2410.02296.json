{
    "id_table_1": {
        "caption": "Table 1:  Dataset statistics.",
        "table": "S5.T1.1",
        "footnotes": [],
        "references": [
            "As shown in Figure  1 , our approach differs from InstructGLM  (Ye et al.,  2023 ) , the current state-of-the-art LM for TAGs, in its underlying design. While both methods utilize prompt templates to transform input graphs into text, InstructGLM relies on explicit encoding of node embeddings into the LMs token embeddings as a form of soft prompting  (Lester et al.,  2021 ) . In contrast, our approach provides a more general framework, leveraging gradient descent through the LM without modifying its underlying text-to-text architecture. This design choice enables our model to retain the versatility of the original LM while adapting it to graph-based tasks. The following sections provide a detailed description of the specific techniques developed in the paper to achieve this goal.",
            "Topological Retrieval.  We leverage PPR  (Page,  1999 ; Jeh & Widom,  2003 )  to perform topological retrieval, which has shown great effectiveness in conjunction with GNNs  (Klicpera et al.,  2019 ) . The success of PPR suggests that the neighbors it identifies may provide more informative context than generic 1-hop or multi-hop neighbors. Specifically, for a target node  v i subscript v i v_{i} italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , we select its top- K K K italic_K  neighbors  PPR  neigh  ( v i ) PPR neigh subscript v i \\mathtt{PPR\\ neigh}(v_{i}) typewriter_PPR typewriter_neigh ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  based on their PPR scores, computed using Eqs. ( 1 ) and ( 2 ). The details of the PPR algorithm are introduced in Section  3 . We then concatenate the text features from the PPR neighbors to form the PPR-retrieved text  t PPR retri =  j ; v j  PPR  neigh  ( v i ) t j superscript t PPR retri subscript direct-sum j subscript v j PPR neigh subscript v i subscript t j t^{\\textrm{PPR retri}}=\\mathop{\\oplus}_{j;v_{j}\\in\\mathtt{PPR\\ neigh}(v_{i})}t%  _{j} italic_t start_POSTSUPERSCRIPT PPR retri end_POSTSUPERSCRIPT =  start_POSTSUBSCRIPT italic_j ; italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  typewriter_PPR typewriter_neigh ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , where   direct-sum \\oplus   denotes text concatenation.",
            "It is worth noting that the classic PPR algorithm is computationally expensive for large graphs due to the matrix multiplication (Eq. ( 1 )). However, efficient approximate solutions such as ApproximatePR  (Andersen et al.,  2006 ) , can be applied to mitigate this issue. Nevertheless, PPR is a topology-based heuristic that does not inherently leverage textual features nor adapt to feedback from downstream node classification tasks. To address these limitations and enhance our frameworks semantic awareness, we propose a complementary semantic retrieval strategy, which is discussed in the following section.",
            "Our framework includes three parameterized modules that require training or fine-tuning: (1) GNNs for generating prototypes and candidate label pruning, as described in Sections  4.1  and  4.2 , (2) the encoder   italic- \\phi italic_  from the semantic retriever, defined in Eq.  5 , and (3) the backbone LM, utilized in Eq.  10 . The GNNs from Sections  4.1  and  4.2  can be shared and their training is independent of the other modules which is supervised by ground truth labels. We provide more details on this process in Appendix  A .",
            "For the backbone LM, we adopt a standard training objective: minimizing the average token-wise negative log-likelihood (NLL) between the ground truth target sequence and the models estimated output probability (Eq.  10 ). Specifically, for the target node, the NLL loss is computed as:",
            "This distribution represents the normalized importance of each prototype text  t  D t D t\\in\\mathcal{D} italic_t  caligraphic_D  based on the LMs likelihood of generating the target ground truth label text  y target superscript y target y^{\\mathrm{target}} italic_y start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT . We use  p ~ ~ p \\tilde{p} over~ start_ARG italic_p end_ARG  to distinguish this distribution from the generation probability defined in Eqs. ( 3 ) and ( 10 ). For simplicity, we omit the pruned candidate classes  t candidates superscript t candidates t^{\\mathrm{candidates}} italic_t start_POSTSUPERSCRIPT roman_candidates end_POSTSUPERSCRIPT  from the LM input in this equation, although they are indeed included in practice, as shown in Eq. ( 10 ).",
            "Notably, computing Eq. ( 13 ) requires  | D | D |\\mathcal{D}| | caligraphic_D |  inferences of the backbone LM due to the denominator. However, the LM is fine-tuned only on the NLL loss for the most relevant prototype,  arg  max d  D s   ( d , t target ) subscript d D subscript s italic- d superscript t target \\mathop{\\arg\\max}_{d\\in\\mathcal{D}}s_{\\phi}(d,t^{\\textrm{target}}) start_BIGOP roman_arg roman_max end_BIGOP start_POSTSUBSCRIPT italic_d  caligraphic_D end_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_d , italic_t start_POSTSUPERSCRIPT target end_POSTSUPERSCRIPT )  via Eq. ( 11 ). Consequently, each update step involves  | D | D |\\mathcal{D}| | caligraphic_D |  forward passes but only one backward pass. To further reduce the computational overhead associated with  | D | D |\\mathcal{D}| | caligraphic_D |  inferences, we can employ a typical sampling strategy: selecting the top- M M M italic_M  batch  D M = { t : t  top-M t   D  s   ( t  , t target ) } subscript D M conditional-set t t subscript top-M superscript t  D subscript s italic- superscript t  superscript t target \\mathcal{D}_{M}=\\{t:t\\in\\text{top-M}_{t^{\\prime}\\in\\mathcal{D}}s_{\\phi}(t^{%  \\prime},t^{\\mathrm{target}})\\} caligraphic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT = { italic_t : italic_t  top-M start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  caligraphic_D end_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_t start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT ) } . By replacing  D D \\mathcal{D} caligraphic_D  with  D M subscript D M \\mathcal{D}_{M} caligraphic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT  in Eqs. ( 12 ) and ( 13 ), we can compute the retrieval probability distribution and the LM-supervised distribution in-batch, effectively reducing the total number of inferences from  | D | D |\\mathcal{D}| | caligraphic_D |  to  M M M italic_M .",
            "Algorithm  1  outlines a step-by-step process for fine-tuning our entire framework, processing one training node per step. This procedure can be readily extended to mini-batch settings.",
            "Our model consists of three parameterized modules: (1) a GNN    \\psi italic_  for generating prototypes and pruned label candidates, (2) the semantic retriever   italic- \\phi italic_ , and (3) the backbone LM    \\theta italic_ . Notably,    \\psi italic_  and   italic- \\phi italic_  are lightweight, with a number of parameters that is only  1 / 30 1 30 1/30 1 / 30  to  1 / 3 1 3 1/3 1 / 3  of the number of parameters of LM    \\theta italic_ . Compared to the state-of-the-art InstructGLM, our model has an additional module   italic- \\phi italic_ , resulting in slightly more parameters which is relatively minor. During training, the GNN    \\psi italic_  can be trained independently, and the PPR scores can be precomputed. The training of    \\theta italic_  relies on the retrieved text from   italic- \\phi italic_ , while the training of   italic- \\phi italic_  requires  p ~ LM (  | t target , y target ) \\tilde{p}_{\\mathrm{LM}}(\\cdot|t^{\\mathrm{target}},y^{\\mathrm{target}}) over~ start_ARG italic_p end_ARG start_POSTSUBSCRIPT roman_LM end_POSTSUBSCRIPT (  | italic_t start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT ) , which is obtained through forward inference of    \\theta italic_ . Importantly, their computational graphs (used for gradient computation) are independent. This is because the LM    \\theta italic_  concatenates the retrieved text from   italic- \\phi italic_  into its input, which is not differentiable with respect to   italic- \\phi italic_ . Furthermore, when training   italic- \\phi italic_ , the loss in Eq. ( 14 ) involves  p ~ LM (  | t target , y target ) \\tilde{p}_{\\mathrm{LM}}(\\cdot|t^{\\mathrm{target}},y^{\\mathrm{target}}) over~ start_ARG italic_p end_ARG start_POSTSUBSCRIPT roman_LM end_POSTSUBSCRIPT (  | italic_t start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT )  wrapped with the  StopGradient StopGradient \\mathrm{StopGradient} roman_StopGradient  operator, ensuring that the gradient computation does not update    \\theta italic_ . As a result, the cost of back-propagation is similar to updating the LM    \\theta italic_  and the semantic encoder   italic- \\phi italic_  separately.",
            "Following  (He et al.,  2024 ; Ye et al.,  2023 ) , we evaluate our approach on four benchmark datasets: Cora  (Sen et al.,  2008 ) , Pubmed  (Sen et al.,  2008 ) , ogbn-arxiv  (Hu et al.,  2020 ) , and a subset of ogbn-products  (Hu et al.,  2020 ; He et al.,  2024 ) . The statistics of the dataset are summarized in Table  1 .",
            "Our implementation employs two pretrained all-MiniLM-L6-v2 models  1 1 1 https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  as the dual encoder for the semantic retriever   italic- \\phi italic_  (Eq. ( 5 )) and the text encoder for GNN    \\psi italic_  (Eq. ( 15 )), respectively. We set the teleport probability of the PPR to   = 0.1  0.1 \\alpha=0.1 italic_ = 0.1 . For the GNN, we employ a  3 3 3 3 -layer GraphSAGE  (Hamilton et al.,  2017 )  architecture with a hidden dimension of  256 256 256 256  as    \\psi italic_ . Our hyperparameter settings include  K = 5 K 5 K=5 italic_K = 5  PPR neighbors,  N = 10 N 10 N=10 italic_N = 10  prototypes, and  M = 8 M 8 M=8 italic_M = 8  samples for LM inference. We choose the number of label candidates  I I I italic_I , from  { 2 , 3 } 2 3 \\{2,3\\} { 2 , 3 } . The backbone LM    \\theta italic_  is implemented using Flan-T5-small/base/large  (Chung et al.,  2022 ) 2 2 2 https://huggingface.co/docs/transformers/en/model_doc/flan-t5 , whose parameters are instruction-fine-tuned using the templates shown in Figure  3  and Section  C ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Performance comparison (accuracy) between  AugGLM  and state-of-the-art models. The best-performing vector-output and text-output models are highlighted in  blue  and  red , respectively.",
        "table": "S5.T2.56",
        "footnotes": [],
        "references": [
            "General LMs are not designed to directly process graph-structured data. To overcome this limitation, a common approach is to employ prompt templates that transform graph data and associated tasks into a textual format that LMs can understand. For instance, consider the Cora  (Sen et al.,  2008 )  literature citation graph. A typical template  (Huang et al.,  2023 ; Ye et al.,  2023 )  for node classification, as shown in Figure  2  consists of three main components: (1) a short description of the classification task, (2) the target nodes textual features, such as its title and abstract, and (3) textual features from relevant neighboring nodes within the graph.",
            "Topological Retrieval.  We leverage PPR  (Page,  1999 ; Jeh & Widom,  2003 )  to perform topological retrieval, which has shown great effectiveness in conjunction with GNNs  (Klicpera et al.,  2019 ) . The success of PPR suggests that the neighbors it identifies may provide more informative context than generic 1-hop or multi-hop neighbors. Specifically, for a target node  v i subscript v i v_{i} italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , we select its top- K K K italic_K  neighbors  PPR  neigh  ( v i ) PPR neigh subscript v i \\mathtt{PPR\\ neigh}(v_{i}) typewriter_PPR typewriter_neigh ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  based on their PPR scores, computed using Eqs. ( 1 ) and ( 2 ). The details of the PPR algorithm are introduced in Section  3 . We then concatenate the text features from the PPR neighbors to form the PPR-retrieved text  t PPR retri =  j ; v j  PPR  neigh  ( v i ) t j superscript t PPR retri subscript direct-sum j subscript v j PPR neigh subscript v i subscript t j t^{\\textrm{PPR retri}}=\\mathop{\\oplus}_{j;v_{j}\\in\\mathtt{PPR\\ neigh}(v_{i})}t%  _{j} italic_t start_POSTSUPERSCRIPT PPR retri end_POSTSUPERSCRIPT =  start_POSTSUBSCRIPT italic_j ; italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  typewriter_PPR typewriter_neigh ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , where   direct-sum \\oplus   denotes text concatenation.",
            "Our framework includes three parameterized modules that require training or fine-tuning: (1) GNNs for generating prototypes and candidate label pruning, as described in Sections  4.1  and  4.2 , (2) the encoder   italic- \\phi italic_  from the semantic retriever, defined in Eq.  5 , and (3) the backbone LM, utilized in Eq.  10 . The GNNs from Sections  4.1  and  4.2  can be shared and their training is independent of the other modules which is supervised by ground truth labels. We provide more details on this process in Appendix  A .",
            "Notably, computing Eq. ( 13 ) requires  | D | D |\\mathcal{D}| | caligraphic_D |  inferences of the backbone LM due to the denominator. However, the LM is fine-tuned only on the NLL loss for the most relevant prototype,  arg  max d  D s   ( d , t target ) subscript d D subscript s italic- d superscript t target \\mathop{\\arg\\max}_{d\\in\\mathcal{D}}s_{\\phi}(d,t^{\\textrm{target}}) start_BIGOP roman_arg roman_max end_BIGOP start_POSTSUBSCRIPT italic_d  caligraphic_D end_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_d , italic_t start_POSTSUPERSCRIPT target end_POSTSUPERSCRIPT )  via Eq. ( 11 ). Consequently, each update step involves  | D | D |\\mathcal{D}| | caligraphic_D |  forward passes but only one backward pass. To further reduce the computational overhead associated with  | D | D |\\mathcal{D}| | caligraphic_D |  inferences, we can employ a typical sampling strategy: selecting the top- M M M italic_M  batch  D M = { t : t  top-M t   D  s   ( t  , t target ) } subscript D M conditional-set t t subscript top-M superscript t  D subscript s italic- superscript t  superscript t target \\mathcal{D}_{M}=\\{t:t\\in\\text{top-M}_{t^{\\prime}\\in\\mathcal{D}}s_{\\phi}(t^{%  \\prime},t^{\\mathrm{target}})\\} caligraphic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT = { italic_t : italic_t  top-M start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  caligraphic_D end_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_t start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT ) } . By replacing  D D \\mathcal{D} caligraphic_D  with  D M subscript D M \\mathcal{D}_{M} caligraphic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT  in Eqs. ( 12 ) and ( 13 ), we can compute the retrieval probability distribution and the LM-supervised distribution in-batch, effectively reducing the total number of inferences from  | D | D |\\mathcal{D}| | caligraphic_D |  to  M M M italic_M .",
            "Table  2  presents a comprehensive comparison of our models performance against existing approaches. The results demonstrate that our proposed method consistently outperforms InstructGLM, achieving new state-of-the-art performance among LMs with text-space outputs for node classification tasks on TAGs. Notably, this superior performance is achieved without modifying the underlying architecture of the LMs, demonstrating the effectiveness of our approach. Furthermore, our models exhibit competitive performance compared to the best vector-output models. Specifically, on Cora, Pubmed, and ogbn-arxiv datasets, our models performance closely approaches that of the state-of-the-art vector-output models. Furthermore, on the ogbn-products dataset, our approach surpasses the performance of the best vector-output model, TAPE."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Ablation study results. T, S, L, denotes the topological retrieval, semantic retrieval, and candidate label pruning. The    \\downarrow   symbol denotes the decrease in accuracy of the ablated version compared to the full model.",
        "table": "S5.T3.14",
        "footnotes": [],
        "references": [
            "Topological Retrieval.  We leverage PPR  (Page,  1999 ; Jeh & Widom,  2003 )  to perform topological retrieval, which has shown great effectiveness in conjunction with GNNs  (Klicpera et al.,  2019 ) . The success of PPR suggests that the neighbors it identifies may provide more informative context than generic 1-hop or multi-hop neighbors. Specifically, for a target node  v i subscript v i v_{i} italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , we select its top- K K K italic_K  neighbors  PPR  neigh  ( v i ) PPR neigh subscript v i \\mathtt{PPR\\ neigh}(v_{i}) typewriter_PPR typewriter_neigh ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  based on their PPR scores, computed using Eqs. ( 1 ) and ( 2 ). The details of the PPR algorithm are introduced in Section  3 . We then concatenate the text features from the PPR neighbors to form the PPR-retrieved text  t PPR retri =  j ; v j  PPR  neigh  ( v i ) t j superscript t PPR retri subscript direct-sum j subscript v j PPR neigh subscript v i subscript t j t^{\\textrm{PPR retri}}=\\mathop{\\oplus}_{j;v_{j}\\in\\mathtt{PPR\\ neigh}(v_{i})}t%  _{j} italic_t start_POSTSUPERSCRIPT PPR retri end_POSTSUPERSCRIPT =  start_POSTSUBSCRIPT italic_j ; italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  typewriter_PPR typewriter_neigh ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , where   direct-sum \\oplus   denotes text concatenation.",
            "where  I < C I C I<C italic_I < italic_C . Given that  IndexToLabel IndexToLabel \\mathrm{IndexToLabel} roman_IndexToLabel  maps are available for our target datasets, which associate numerical labels with their corresponding text representations, we can leverage this mapping to present the pruned label candidates for node  v i subscript v i v_{i} italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  as a concatenated text:  t candidates =  i  L i IndexToLabel  ( i ) superscript t candidates subscript direct-sum i subscript L i IndexToLabel i t^{\\text{candidates}}=\\oplus_{i\\in\\mathcal{L}_{i}}\\mathrm{IndexToLabel}(i) italic_t start_POSTSUPERSCRIPT candidates end_POSTSUPERSCRIPT =  start_POSTSUBSCRIPT italic_i  caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_IndexToLabel ( italic_i ) . The integration of this pruned candidate set into the input template is detailed in Section  4.3 , where we elaborate on our overall template design.",
            "Our augmented training samples are presented in Figure  3 , which includes three key elements: (1) the target nodes text  t target superscript t target t^{\\mathrm{target}} italic_t start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT , (2) the retrieved nodes text  t retri superscript t retri t^{\\mathrm{retri}} italic_t start_POSTSUPERSCRIPT roman_retri end_POSTSUPERSCRIPT , and (3) the pruned label candidates  t candidates superscript t candidates t^{\\mathrm{candidates}} italic_t start_POSTSUPERSCRIPT roman_candidates end_POSTSUPERSCRIPT . We collectively denote these elements as  t input = ( t target , t retri , t candidates ) superscript t input superscript t target superscript t retri superscript t candidates t^{\\mathrm{input}}=(t^{\\mathrm{target}},t^{\\mathrm{retri}},t^{\\mathrm{%  candidates}}) italic_t start_POSTSUPERSCRIPT roman_input end_POSTSUPERSCRIPT = ( italic_t start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT , italic_t start_POSTSUPERSCRIPT roman_retri end_POSTSUPERSCRIPT , italic_t start_POSTSUPERSCRIPT roman_candidates end_POSTSUPERSCRIPT ) . The backbone LM generates a prediction probability for the label sequence  y target superscript y target y^{\\mathrm{target}} italic_y start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT  according to the following equation:",
            "where  |  | |\\cdot| |  |  represents the sequence length; in this equation,  i i i italic_i  and  1 : i  1 : 1 i 1 1:i-1 1 : italic_i - 1  are token-level indices. We will introduce the detailed selection of the backbone LM in Section  5 . Figure  3  presents an exemplar template for the Cora dataset, showcasing the integration of  t target superscript t target t^{\\mathrm{target}} italic_t start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT ,  t retri superscript t retri t^{\\mathrm{retri}} italic_t start_POSTSUPERSCRIPT roman_retri end_POSTSUPERSCRIPT , and  t candidates superscript t candidates t^{\\mathrm{candidates}} italic_t start_POSTSUPERSCRIPT roman_candidates end_POSTSUPERSCRIPT . A full list of templates used on all datasets in our experiments is detailed in Appendix  C . Note that we exclude the abstracts of the retrieved nodes to prevent exceeding the maximum input length constraints of most LMs. During evaluation, we utilize only the model input portion of this template.",
            "This distribution represents the normalized importance of each prototype text  t  D t D t\\in\\mathcal{D} italic_t  caligraphic_D  based on the LMs likelihood of generating the target ground truth label text  y target superscript y target y^{\\mathrm{target}} italic_y start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT . We use  p ~ ~ p \\tilde{p} over~ start_ARG italic_p end_ARG  to distinguish this distribution from the generation probability defined in Eqs. ( 3 ) and ( 10 ). For simplicity, we omit the pruned candidate classes  t candidates superscript t candidates t^{\\mathrm{candidates}} italic_t start_POSTSUPERSCRIPT roman_candidates end_POSTSUPERSCRIPT  from the LM input in this equation, although they are indeed included in practice, as shown in Eq. ( 10 ).",
            "Notably, computing Eq. ( 13 ) requires  | D | D |\\mathcal{D}| | caligraphic_D |  inferences of the backbone LM due to the denominator. However, the LM is fine-tuned only on the NLL loss for the most relevant prototype,  arg  max d  D s   ( d , t target ) subscript d D subscript s italic- d superscript t target \\mathop{\\arg\\max}_{d\\in\\mathcal{D}}s_{\\phi}(d,t^{\\textrm{target}}) start_BIGOP roman_arg roman_max end_BIGOP start_POSTSUBSCRIPT italic_d  caligraphic_D end_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_d , italic_t start_POSTSUPERSCRIPT target end_POSTSUPERSCRIPT )  via Eq. ( 11 ). Consequently, each update step involves  | D | D |\\mathcal{D}| | caligraphic_D |  forward passes but only one backward pass. To further reduce the computational overhead associated with  | D | D |\\mathcal{D}| | caligraphic_D |  inferences, we can employ a typical sampling strategy: selecting the top- M M M italic_M  batch  D M = { t : t  top-M t   D  s   ( t  , t target ) } subscript D M conditional-set t t subscript top-M superscript t  D subscript s italic- superscript t  superscript t target \\mathcal{D}_{M}=\\{t:t\\in\\text{top-M}_{t^{\\prime}\\in\\mathcal{D}}s_{\\phi}(t^{%  \\prime},t^{\\mathrm{target}})\\} caligraphic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT = { italic_t : italic_t  top-M start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  caligraphic_D end_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_t start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT ) } . By replacing  D D \\mathcal{D} caligraphic_D  with  D M subscript D M \\mathcal{D}_{M} caligraphic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT  in Eqs. ( 12 ) and ( 13 ), we can compute the retrieval probability distribution and the LM-supervised distribution in-batch, effectively reducing the total number of inferences from  | D | D |\\mathcal{D}| | caligraphic_D |  to  M M M italic_M .",
            "Our implementation employs two pretrained all-MiniLM-L6-v2 models  1 1 1 https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  as the dual encoder for the semantic retriever   italic- \\phi italic_  (Eq. ( 5 )) and the text encoder for GNN    \\psi italic_  (Eq. ( 15 )), respectively. We set the teleport probability of the PPR to   = 0.1  0.1 \\alpha=0.1 italic_ = 0.1 . For the GNN, we employ a  3 3 3 3 -layer GraphSAGE  (Hamilton et al.,  2017 )  architecture with a hidden dimension of  256 256 256 256  as    \\psi italic_ . Our hyperparameter settings include  K = 5 K 5 K=5 italic_K = 5  PPR neighbors,  N = 10 N 10 N=10 italic_N = 10  prototypes, and  M = 8 M 8 M=8 italic_M = 8  samples for LM inference. We choose the number of label candidates  I I I italic_I , from  { 2 , 3 } 2 3 \\{2,3\\} { 2 , 3 } . The backbone LM    \\theta italic_  is implemented using Flan-T5-small/base/large  (Chung et al.,  2022 ) 2 2 2 https://huggingface.co/docs/transformers/en/model_doc/flan-t5 , whose parameters are instruction-fine-tuned using the templates shown in Figure  3  and Section  C .",
            "To evaluate the contribution of each key component in  AugGLM  , we conducted an ablation study on three crucial modules: (1) topological retrieval, (2) semantic retrieval, and (3) candidate label pruning. We use the Flan-T5-small as the backbone LM for this analysis. The results, presented in Table  3 , demonstrate that each module consistently improves performance across all datasets. Notably, our analysis reveals that the relative importance of each component varies across different datasets. For instance, candidate label pruning has a significant impact on performance for the Cora dataset, whereas its effect is less pronounced for the ogbn-products dataset. This variation in component importance underscores adaptability of our approach, which can effectively accommodate diverse datasets with different characteristics."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Performance of the jointly trained model (accuracy).",
        "table": "S5.T4.1",
        "footnotes": [],
        "references": [
            "Next, for each target node with its associated text features  t target superscript t target t^{\\textrm{target}} italic_t start_POSTSUPERSCRIPT target end_POSTSUPERSCRIPT , we compute the semantically retrieved text using Eq. ( 4 ):  t semantic retri = arg  max d  D s   ( d , t target ) superscript t semantic retri subscript d D subscript s italic- d superscript t target t^{\\textrm{semantic retri}}=\\mathop{\\arg\\max}_{d\\in\\mathcal{D}}s_{\\phi}(d,t^{%  \\textrm{target}}) italic_t start_POSTSUPERSCRIPT semantic retri end_POSTSUPERSCRIPT = start_BIGOP roman_arg roman_max end_BIGOP start_POSTSUBSCRIPT italic_d  caligraphic_D end_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_d , italic_t start_POSTSUPERSCRIPT target end_POSTSUPERSCRIPT ) . In our experiments, we may use topological retrieval, prototypical semantic retrieval, or a hybrid approach that combines both by concatenating their retrieved texts. For simplicity, we denote the retrieved text as  t retri superscript t retri t^{\\mathrm{retri}} italic_t start_POSTSUPERSCRIPT roman_retri end_POSTSUPERSCRIPT .",
            "We defer the discussion of training   italic- \\phi italic_  and  GNN  subscript GNN  \\mathtt{GNN}_{\\psi} typewriter_GNN start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  to Section  4.4  and their specific architectures in Section  5 .",
            "where  I < C I C I<C italic_I < italic_C . Given that  IndexToLabel IndexToLabel \\mathrm{IndexToLabel} roman_IndexToLabel  maps are available for our target datasets, which associate numerical labels with their corresponding text representations, we can leverage this mapping to present the pruned label candidates for node  v i subscript v i v_{i} italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  as a concatenated text:  t candidates =  i  L i IndexToLabel  ( i ) superscript t candidates subscript direct-sum i subscript L i IndexToLabel i t^{\\text{candidates}}=\\oplus_{i\\in\\mathcal{L}_{i}}\\mathrm{IndexToLabel}(i) italic_t start_POSTSUPERSCRIPT candidates end_POSTSUPERSCRIPT =  start_POSTSUBSCRIPT italic_i  caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_IndexToLabel ( italic_i ) . The integration of this pruned candidate set into the input template is detailed in Section  4.3 , where we elaborate on our overall template design.",
            "Our framework includes three parameterized modules that require training or fine-tuning: (1) GNNs for generating prototypes and candidate label pruning, as described in Sections  4.1  and  4.2 , (2) the encoder   italic- \\phi italic_  from the semantic retriever, defined in Eq.  5 , and (3) the backbone LM, utilized in Eq.  10 . The GNNs from Sections  4.1  and  4.2  can be shared and their training is independent of the other modules which is supervised by ground truth labels. We provide more details on this process in Appendix  A .",
            "Our model consists of three parameterized modules: (1) a GNN    \\psi italic_  for generating prototypes and pruned label candidates, (2) the semantic retriever   italic- \\phi italic_ , and (3) the backbone LM    \\theta italic_ . Notably,    \\psi italic_  and   italic- \\phi italic_  are lightweight, with a number of parameters that is only  1 / 30 1 30 1/30 1 / 30  to  1 / 3 1 3 1/3 1 / 3  of the number of parameters of LM    \\theta italic_ . Compared to the state-of-the-art InstructGLM, our model has an additional module   italic- \\phi italic_ , resulting in slightly more parameters which is relatively minor. During training, the GNN    \\psi italic_  can be trained independently, and the PPR scores can be precomputed. The training of    \\theta italic_  relies on the retrieved text from   italic- \\phi italic_ , while the training of   italic- \\phi italic_  requires  p ~ LM (  | t target , y target ) \\tilde{p}_{\\mathrm{LM}}(\\cdot|t^{\\mathrm{target}},y^{\\mathrm{target}}) over~ start_ARG italic_p end_ARG start_POSTSUBSCRIPT roman_LM end_POSTSUBSCRIPT (  | italic_t start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT ) , which is obtained through forward inference of    \\theta italic_ . Importantly, their computational graphs (used for gradient computation) are independent. This is because the LM    \\theta italic_  concatenates the retrieved text from   italic- \\phi italic_  into its input, which is not differentiable with respect to   italic- \\phi italic_ . Furthermore, when training   italic- \\phi italic_ , the loss in Eq. ( 14 ) involves  p ~ LM (  | t target , y target ) \\tilde{p}_{\\mathrm{LM}}(\\cdot|t^{\\mathrm{target}},y^{\\mathrm{target}}) over~ start_ARG italic_p end_ARG start_POSTSUBSCRIPT roman_LM end_POSTSUBSCRIPT (  | italic_t start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT )  wrapped with the  StopGradient StopGradient \\mathrm{StopGradient} roman_StopGradient  operator, ensuring that the gradient computation does not update    \\theta italic_ . As a result, the cost of back-propagation is similar to updating the LM    \\theta italic_  and the semantic encoder   italic- \\phi italic_  separately.",
            "One of the key advantage of pure text-to-text instruction tuning is that a single model can be trained on multiple tasks with the same input-output format. To verify this, we conducted an experiment using a Flan-T5-small model, applying our proposed strategies to jointly train it on four diverse datasets: Cora, Pubmed, ogbn-arxiv, and ogbn-products. The results, presented in Table  4  show that the jointly trained model achieve performance comparable to models trained separately on each individual dataset. We observe that on some datasets, such as Cora and ogbn-products, the jointly trained model even outperforms its dataset-specific counterparts."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Templates used for all datasets.",
        "table": "A3.T5.1",
        "footnotes": [],
        "references": [
            "We defer the discussion of training   italic- \\phi italic_  and  GNN  subscript GNN  \\mathtt{GNN}_{\\psi} typewriter_GNN start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  to Section  4.4  and their specific architectures in Section  5 .",
            "where  |  | |\\cdot| |  |  represents the sequence length; in this equation,  i i i italic_i  and  1 : i  1 : 1 i 1 1:i-1 1 : italic_i - 1  are token-level indices. We will introduce the detailed selection of the backbone LM in Section  5 . Figure  3  presents an exemplar template for the Cora dataset, showcasing the integration of  t target superscript t target t^{\\mathrm{target}} italic_t start_POSTSUPERSCRIPT roman_target end_POSTSUPERSCRIPT ,  t retri superscript t retri t^{\\mathrm{retri}} italic_t start_POSTSUPERSCRIPT roman_retri end_POSTSUPERSCRIPT , and  t candidates superscript t candidates t^{\\mathrm{candidates}} italic_t start_POSTSUPERSCRIPT roman_candidates end_POSTSUPERSCRIPT . A full list of templates used on all datasets in our experiments is detailed in Appendix  C . Note that we exclude the abstracts of the retrieved nodes to prevent exceeding the maximum input length constraints of most LMs. During evaluation, we utilize only the model input portion of this template.",
            "Our framework includes three parameterized modules that require training or fine-tuning: (1) GNNs for generating prototypes and candidate label pruning, as described in Sections  4.1  and  4.2 , (2) the encoder   italic- \\phi italic_  from the semantic retriever, defined in Eq.  5 , and (3) the backbone LM, utilized in Eq.  10 . The GNNs from Sections  4.1  and  4.2  can be shared and their training is independent of the other modules which is supervised by ground truth labels. We provide more details on this process in Appendix  A .",
            "Our implementation employs two pretrained all-MiniLM-L6-v2 models  1 1 1 https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  as the dual encoder for the semantic retriever   italic- \\phi italic_  (Eq. ( 5 )) and the text encoder for GNN    \\psi italic_  (Eq. ( 15 )), respectively. We set the teleport probability of the PPR to   = 0.1  0.1 \\alpha=0.1 italic_ = 0.1 . For the GNN, we employ a  3 3 3 3 -layer GraphSAGE  (Hamilton et al.,  2017 )  architecture with a hidden dimension of  256 256 256 256  as    \\psi italic_ . Our hyperparameter settings include  K = 5 K 5 K=5 italic_K = 5  PPR neighbors,  N = 10 N 10 N=10 italic_N = 10  prototypes, and  M = 8 M 8 M=8 italic_M = 8  samples for LM inference. We choose the number of label candidates  I I I italic_I , from  { 2 , 3 } 2 3 \\{2,3\\} { 2 , 3 } . The backbone LM    \\theta italic_  is implemented using Flan-T5-small/base/large  (Chung et al.,  2022 ) 2 2 2 https://huggingface.co/docs/transformers/en/model_doc/flan-t5 , whose parameters are instruction-fine-tuned using the templates shown in Figure  3  and Section  C .",
            "Limitation and Future Work.  One limitation of this work is the need for manual definition of prompt templates in Table  5 . A promising direction for future research is to develop methods for automatically searching for optimal templates in a data-driven manner. Another limitation is the requirement for pretraining a GNN    \\psi italic_  on each dataset, which stems from the inherent challenges of language models in understanding graph data. Addressing this limitation by developing more powerful language models capable of handling graph data is a challenging yet impactful area of future work.",
            "Table  5  presents templates used in this paper. We design the Citation template for the Cora, Pubmed, and ogbn-arxiv datasets and the Amazon template for the ogbn-products dataset."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A3.T5.1.2.1.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_7": {
        "caption": "",
        "table": "A3.T5.1.3.2.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_8": {
        "caption": "",
        "table": "A3.T5.1.4.3.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "",
        "table": "A3.T5.1.5.4.1.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}