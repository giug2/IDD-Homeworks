{
    "id_table_1": {
        "caption": "Table 1.  Hair diversity support in existing work: While 3D hairstyle regression techniques are making rapid progress in extending their support to more complex hairstyles, support for coily hair, strands (such as braids and dreadlocks) and gathering (such as ponytails and buns) remains an open problem.",
        "table": "S2.T1.7.7",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "We address the problem of hairstyle prediction from a single image. This is particularly relevant for enrollment of user appearance for virtual experiences. It is critical for enrollment systems in this context to work well for diverse users in order to provide an inclusive experience. Hairstyles play a significant role in human appearance and identity, allowing for self-expression, artistic expression, and cultural representation. Inclusive hair representation is also protected by law in many places, with legal guidelines against race discrimination based on hair texture and style  (nyc,  2019 ; cro,  2019 ; ehr,  2022 ) . Example results of our method are shown in  Figure 1 .",
            "Table 1  provides a summary of recent work in the space and its coverage of different hair types and features critical for inclusive hair description/prediction. The rest of this section gives an overview of these works and others, and how our method compares to them.",
            "As we cannot compare to recent parametric approaches quantitatively without manual labeling of generated results with our taxonomy, we provide a qualitative comparison of our method with the state-of-the-art method for hairstyle reconstruction, shown in  Figure 5 . While our method does not enable direct strand-wise representation, it is far more robust for diverse input hairstyles. Existing methods show a strong bias to straight, long hair while our method is able to provide appropriate hairstyle predictions for short, frizzy, coily and gathered styles, as well as long, straight hair. Further qualitative results of our method are shown in  Figure 1  and  Figure 6 , face shapes are reconstructed using method of  Wood et al .  ( 2022 ) . The remainder of this section describes the experimental setup, evaluation metrics and quantitative results.",
            "We assess the quality of our trained model on a labeled subset of the FairFace dataset  (Karkkainen and Joo,  2019 )  described in  Section 3.2 . We compute both accuracy  and  fairness for a number of taxonomy attributes. We focus on the following set of attributes: bald, bang styling, hair gathered, hair length, hair type, and strand styling. Bald is a subset of the hair length attribute. This set of metrics was chosen based on correlation with protected demographic groups, and consequent significance for fairness. This is in-line with previous attribute and hairstyle classification work ( Section 2.1 ) with addition of the hair gathered attribute, a common failure case in existing methods. Many attributes in our taxonomy include regional annotations so reporting individually in a concise manner is challenging, we therefore collate labels from all scalp regions. These taxonomy-based accuracy metrics serve as a  perceptual  measure of hairstyle similarity which are agnostic to the exact hair classes being predicted, providing a good indication of acceptability of the predicted style to a user. To calculate fairness for three most common demographics categories (gender, age, ancestry), we first calculate attribute accuracy across each group within a category (e.g. female/male for gender). Then we calculate attribute fairness as mean accuracy across different demographics groups divided by the maximum accuracy of any individual group and we represent it as percentage. This provides metric that converges to 100% as disparity between groups becomes smaller."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.   Results of our ablation experiments. We report hairstyle attribute classification accuracy for  (i)  Baseline ResNet34  (He et al . ,  2016b )  architecture trained with  L s  t  y  l  e subscript L s t y l e \\mathcal{L}_{style} caligraphic_L start_POSTSUBSCRIPT italic_s italic_t italic_y italic_l italic_e end_POSTSUBSCRIPT  loss in row 1;  (ii)  Model with frozen DINOv2  (Oquab et al . ,  2023 )  backbone trained with  L s  t  y  l  e subscript L s t y l e \\mathcal{L}_{style} caligraphic_L start_POSTSUBSCRIPT italic_s italic_t italic_y italic_l italic_e end_POSTSUBSCRIPT  in row 2;  (iii)  Model with frozen DINOv2 backbone trained with  L s  t  y  l  e subscript L s t y l e \\mathcal{L}_{style} caligraphic_L start_POSTSUBSCRIPT italic_s italic_t italic_y italic_l italic_e end_POSTSUBSCRIPT , and  L a  t  t  r subscript L a t t r \\mathcal{L}_{attr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_t italic_t italic_r end_POSTSUBSCRIPT  losses in row 3;  (iv)  Our final model with frozen DINOv2 backbone trained with  L s  t  y  l  e subscript L s t y l e \\mathcal{L}_{style} caligraphic_L start_POSTSUBSCRIPT italic_s italic_t italic_y italic_l italic_e end_POSTSUBSCRIPT , and  L a  t  t  r subscript L a t t r \\mathcal{L}_{attr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_t italic_t italic_r end_POSTSUBSCRIPT  losses on attribute balanced training set.",
        "table": "A4.EGx1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "Our hairstyle taxonomy consists of 18 attributes. There are ten global attributes which are based on the whole hairstyle, for example the shape of the hairline or surface appearance of the hair. The scalp is divided into eight regions and each region is annotated with eight local attributes, such as length and strand styling. So in total each hairstyle has  A = 74 A 74 A=74 italic_A = 74  taxonomic labels. A graphical representation of the full taxonomy is shown in  Figure 2 , and a detailed textual description is given in the supplementary material. While we hope that the taxonomy presented is sufficiently fair, objective and complete, we recognize that it is likely impossible for it to be  truly  complete. We therefore encourage future work to extend the taxonomy as required and publish any modifications.",
            "We assess the quality of our trained model on a labeled subset of the FairFace dataset  (Karkkainen and Joo,  2019 )  described in  Section 3.2 . We compute both accuracy  and  fairness for a number of taxonomy attributes. We focus on the following set of attributes: bald, bang styling, hair gathered, hair length, hair type, and strand styling. Bald is a subset of the hair length attribute. This set of metrics was chosen based on correlation with protected demographic groups, and consequent significance for fairness. This is in-line with previous attribute and hairstyle classification work ( Section 2.1 ) with addition of the hair gathered attribute, a common failure case in existing methods. Many attributes in our taxonomy include regional annotations so reporting individually in a concise manner is challenging, we therefore collate labels from all scalp regions. These taxonomy-based accuracy metrics serve as a  perceptual  measure of hairstyle similarity which are agnostic to the exact hair classes being predicted, providing a good indication of acceptability of the predicted style to a user. To calculate fairness for three most common demographics categories (gender, age, ancestry), we first calculate attribute accuracy across each group within a category (e.g. female/male for gender). Then we calculate attribute fairness as mean accuracy across different demographics groups divided by the maximum accuracy of any individual group and we represent it as percentage. This provides metric that converges to 100% as disparity between groups becomes smaller.",
            "We investigate the influence of auxiliary hairstyle prediction loss term,  L a  t  t  r subscript L a t t r \\mathcal{L}_{attr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_t italic_t italic_r end_POSTSUBSCRIPT , defined in  Equation 2 . In the third row of  Table 6  we report the results of the model trained to minimize the  L s  t  y  l  e subscript L s t y l e \\mathcal{L}_{style} caligraphic_L start_POSTSUBSCRIPT italic_s italic_t italic_y italic_l italic_e end_POSTSUBSCRIPT  and  L a  t  t  r subscript L a t t r \\mathcal{L}_{attr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_t italic_t italic_r end_POSTSUBSCRIPT  losses simultaneously. One can observe that hairstyle attribute prediction overall improves the accuracy for most metrics."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Popular hair typing systems.",
        "table": "A4.EGx2",
        "footnotes": [],
        "references": [
            "We annotate the synthetic hairstyle library of  Wood et al .  ( 2021 )  using our taxonomy, such that each style,  s s s italic_s , has associated taxonomy attributes,  [ a 1 s , a 2 s , ... , a A s ] superscript subscript a 1 s superscript subscript a 2 s ... superscript subscript a A s \\left[a_{1}^{s},a_{2}^{s},\\ldots,a_{A}^{s}\\right] [ italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT , italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT , ... , italic_a start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ] . We expand the library to address any common taxonomic combinations that were missing or under-represented. In total, the expanded library contains 480 distinct hairstyles, represented as 3D strands. We use the rendering pipeline of  Wood et al .  ( 2021 )  to render a dataset containing images of the face with our expanded hair library. The dataset contains 100,000  512  512 512 512 512\\times 512 512  512  pixel images rendered under different lighting conditions with a different pose and facial expression and randomly sampled camera positions. Each synthetic image therefore has a hairstyle class annotation and the taxonomic annotations inherited from that hairstyle. Hairstyles are sampled non-uniformly to promote fairness across various taxonomic attributes, as detailed below. Some example training images are shown in  Figure 3 .",
            "We instead choose to balance our synthetic dataset by weighting the sampling of hairstyles in a non-uniform way. In so doing we promote fairness of the dataset along important, semantically meaningful attributes of hair, such as length and hair type. Our balanced hairstyle attribute distribution is: 50% with fringes; 75% with gathering (in the back, in cornrows, behind the ear for long hairstyle); Short/Medium/Long hair: 40%/30%/30%; Straight/Wavy/Curly/Coily: 50%/15%/15%/20%  (Daniels et al . ,  2023 ) . The influence of the training set on different aspects of the trained model will be covered in  Section 4.3 .",
            "We assess the quality of our trained model on a labeled subset of the FairFace dataset  (Karkkainen and Joo,  2019 )  described in  Section 3.2 . We compute both accuracy  and  fairness for a number of taxonomy attributes. We focus on the following set of attributes: bald, bang styling, hair gathered, hair length, hair type, and strand styling. Bald is a subset of the hair length attribute. This set of metrics was chosen based on correlation with protected demographic groups, and consequent significance for fairness. This is in-line with previous attribute and hairstyle classification work ( Section 2.1 ) with addition of the hair gathered attribute, a common failure case in existing methods. Many attributes in our taxonomy include regional annotations so reporting individually in a concise manner is challenging, we therefore collate labels from all scalp regions. These taxonomy-based accuracy metrics serve as a  perceptual  measure of hairstyle similarity which are agnostic to the exact hair classes being predicted, providing a good indication of acceptability of the predicted style to a user. To calculate fairness for three most common demographics categories (gender, age, ancestry), we first calculate attribute accuracy across each group within a category (e.g. female/male for gender). Then we calculate attribute fairness as mean accuracy across different demographics groups divided by the maximum accuracy of any individual group and we represent it as percentage. This provides metric that converges to 100% as disparity between groups becomes smaller.",
            "Table 3  provides an overview of existing taxonomy for hair type, note that these do not cover hair  styling .  Table 4  gives a summary of existing datasets for hairstyle prediction and their annotation schemes. These are often limited in terms of diversity of style, and richness of annotation."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  Hairstyle datasets: Name, Number of hairstyles (if any), Discrete parameters (if any) excluding N/A, Other relevant annotations, Number of images if applicable and number of subjects in the dataset, if provided.",
        "table": "A4.EGx3",
        "footnotes": [],
        "references": [
            "We instead choose to balance our synthetic dataset by weighting the sampling of hairstyles in a non-uniform way. In so doing we promote fairness of the dataset along important, semantically meaningful attributes of hair, such as length and hair type. Our balanced hairstyle attribute distribution is: 50% with fringes; 75% with gathering (in the back, in cornrows, behind the ear for long hairstyle); Short/Medium/Long hair: 40%/30%/30%; Straight/Wavy/Curly/Coily: 50%/15%/15%/20%  (Daniels et al . ,  2023 ) . The influence of the training set on different aspects of the trained model will be covered in  Section 4.3 .",
            "We use a convolutional neural network (CNN) trained entirely on synthetic data. The network is comprised of a frozen backbone that has been pre-trained through self-supervision on a large corpus of real images to provide general-purpose visual features, DINOv2 followed by a number of fully-connected layers optimized during training, visualized in  Figure 4 .",
            "As mentioned in  Figure 4 , in addition to predicting synthetic hairstyles from our library, we also predict semantic hairstyle attributes,  [ a ^ 1 s , ... , a ^ A s ] superscript subscript ^ a 1 s ... superscript subscript ^ a A s \\left[\\hat{a}_{1}^{s},\\ldots,\\hat{a}_{A}^{s}\\right] [ over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT , ... , over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ] , as defined by our taxonomy. Hence, we define an additional attribute related loss term  L a  t  t  r subscript L a t t r \\mathcal{L}_{attr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_t italic_t italic_r end_POSTSUBSCRIPT  as follows:",
            "Table 3  provides an overview of existing taxonomy for hair type, note that these do not cover hair  styling .  Table 4  gives a summary of existing datasets for hairstyle prediction and their annotation schemes. These are often limited in terms of diversity of style, and richness of annotation."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.  Attribute counts on the labelled subset of FairFace  (Karkkainen and Joo,  2019 ) , real images used for evaluations.",
        "table": "S4.F5.1",
        "footnotes": [
            ""
        ],
        "references": [
            "As we cannot compare to recent parametric approaches quantitatively without manual labeling of generated results with our taxonomy, we provide a qualitative comparison of our method with the state-of-the-art method for hairstyle reconstruction, shown in  Figure 5 . While our method does not enable direct strand-wise representation, it is far more robust for diverse input hairstyles. Existing methods show a strong bias to straight, long hair while our method is able to provide appropriate hairstyle predictions for short, frizzy, coily and gathered styles, as well as long, straight hair. Further qualitative results of our method are shown in  Figure 1  and  Figure 6 , face shapes are reconstructed using method of  Wood et al .  ( 2022 ) . The remainder of this section describes the experimental setup, evaluation metrics and quantitative results.",
            "Table 5  provides details of the statistics of the subset of the FairFace  (Karkkainen and Joo,  2019 )  dataset we annotate with our taxonomy and use for evaluation."
        ]
    },
    "id_table_6": {
        "caption": "Table 6.   Results of our ablation experiments. We report hairstyle attribute classification accuracy for  (i)  ResNet baseline  (He et al . ,  2016b )  architectures trained with  L s  t  y  l  e subscript L s t y l e \\mathcal{L}_{style} caligraphic_L start_POSTSUBSCRIPT italic_s italic_t italic_y italic_l italic_e end_POSTSUBSCRIPT  loss in rows 1-3;  (ii)  Model with frozen DINOv2  (Oquab et al . ,  2023 )  backbone trained with  L s  t  y  l  e subscript L s t y l e \\mathcal{L}_{style} caligraphic_L start_POSTSUBSCRIPT italic_s italic_t italic_y italic_l italic_e end_POSTSUBSCRIPT  in row 4;  (iii)  Model with frozen DINOv2 backbone trained with  L s  t  y  l  e subscript L s t y l e \\mathcal{L}_{style} caligraphic_L start_POSTSUBSCRIPT italic_s italic_t italic_y italic_l italic_e end_POSTSUBSCRIPT , and  L a  t  t  r subscript L a t t r \\mathcal{L}_{attr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_t italic_t italic_r end_POSTSUBSCRIPT  losses in row 5;  (iv)  Our final model with frozen DINOv2 backbone trained with  L s  t  y  l  e subscript L s t y l e \\mathcal{L}_{style} caligraphic_L start_POSTSUBSCRIPT italic_s italic_t italic_y italic_l italic_e end_POSTSUBSCRIPT , and  L a  t  t  r subscript L a t t r \\mathcal{L}_{attr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_t italic_t italic_r end_POSTSUBSCRIPT  losses on attribute balanced training set.",
        "table": "S4.F5.2",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "As we cannot compare to recent parametric approaches quantitatively without manual labeling of generated results with our taxonomy, we provide a qualitative comparison of our method with the state-of-the-art method for hairstyle reconstruction, shown in  Figure 5 . While our method does not enable direct strand-wise representation, it is far more robust for diverse input hairstyles. Existing methods show a strong bias to straight, long hair while our method is able to provide appropriate hairstyle predictions for short, frizzy, coily and gathered styles, as well as long, straight hair. Further qualitative results of our method are shown in  Figure 1  and  Figure 6 , face shapes are reconstructed using method of  Wood et al .  ( 2022 ) . The remainder of this section describes the experimental setup, evaluation metrics and quantitative results.",
            "Table 6  reports classification accuracy for different hairstyle attributes and mean fairness over different models. Prior work performing hairstyle classification or attribute prediction often follows conventional machine learning methods, varying in specifics based on the data formats used  (Yin et al . ,  2017 ; Hu et al . ,  2017 ; Svanera et al . ,  2016 ; Chen et al . ,  2021 ; Kim et al . ,  2021 ) . To approximate these we choose a ResNet34  (He et al . ,  2016a )  baseline trained using  L style subscript L style \\mathcal{L}_{\\text{style}} caligraphic_L start_POSTSUBSCRIPT style end_POSTSUBSCRIPT , i.e., conventional cross-entropy loss (Row 1). In rows 2-4, we report results of our DINOv2-based models. All models were trained to classify hairstyles into the 480 hairstyles from our catalog. An extended version of  Table 6  with additional results for ResNet50 and ResNet101 models can be found in supplementary material.",
            "For comparison, the DINOv2-based model achieves a training accuracy of  74.8 % percent 74.8 74.8\\% 74.8 %  and  78.0 % percent 78.0 78.0\\% 78.0 %  accuracy on the additional synthetic test set. As shown in rows 1-2 of the  Table 6 , the DINOv2-based model significantly outperforms the baseline model on the real-image test set. This demonstrates that by preventing over-fitting to the synthetic data, the model is better able to generalize to real images.",
            "We investigate the influence of auxiliary hairstyle prediction loss term,  L a  t  t  r subscript L a t t r \\mathcal{L}_{attr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_t italic_t italic_r end_POSTSUBSCRIPT , defined in  Equation 2 . In the third row of  Table 6  we report the results of the model trained to minimize the  L s  t  y  l  e subscript L s t y l e \\mathcal{L}_{style} caligraphic_L start_POSTSUBSCRIPT italic_s italic_t italic_y italic_l italic_e end_POSTSUBSCRIPT  and  L a  t  t  r subscript L a t t r \\mathcal{L}_{attr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_t italic_t italic_r end_POSTSUBSCRIPT  losses simultaneously. One can observe that hairstyle attribute prediction overall improves the accuracy for most metrics.",
            "We investigate how the distribution of different hairstyle attributes in training data influences the fairness of the trained model.  Table 6  shows the impact on accuracy of training with the balanced dataset compared with a dataset using uniform sampling (bottom two rows). Balancing the dataset results in a small degradation in accuracy, however, as shown in  Figure 7 , results in a significant improvement in fairness across the majority of groups and attributes, while retaining similar accuracy. In particular we observe a significant fairness improvement for the hair gathered attribute across gender from  85.3 % percent 85.3 85.3\\% 85.3 %  to  91.7 % percent 91.7 91.7\\% 91.7 % , a particularly challenging attribute due to visual ambiguity between gathered and short hairstyles in frontal images. A table with fairness results for different models can be found in the supplementary material.",
            "Example failure cases are shown in the bottom row of  Figure 6 . We observe that in very dark or bright illumination conditions our model confuses coily hair with other hair types, which reduces fairness and accuracy for certain ancestry groups dominated by coily hair. Sometimes our model confuses long, gathered hair with short and medium-length, non-gathered hair due to the absence of visual cues in the image. These issues can be partially addressed via further resampling training data and incorporating training samples that better cover the aforementioned scenarios. Attributes such as hair gathering, however, are challenging due to the lack of visual information for the back of the head. Multiple input images could provide a solution to this problem. In the long term, parametric modeling of hairstyles is likely necessary and should remain an active area of research. To enable inclusive products in the mean-time, classification approaches are likely to lead to better outcomes for a broader range of users.",
            "Additional results of attribute prediction accuracy and fairness for different baselines and our models are reported in  Table 6 , and  Table 7  respectively."
        ]
    },
    "id_table_7": {
        "caption": "Table 7.  Results of our ablation experiments. We report fairness for different attributes across different genders, age groups and ancestries for  (i)  Baseline architectures trained with  L s  t  y  l  e subscript L s t y l e \\mathcal{L}_{style} caligraphic_L start_POSTSUBSCRIPT italic_s italic_t italic_y italic_l italic_e end_POSTSUBSCRIPT  loss in rows 1-3;  (ii)  Model with frozen DINOv2 backbone trained with  L s  t  y  l  e subscript L s t y l e \\mathcal{L}_{style} caligraphic_L start_POSTSUBSCRIPT italic_s italic_t italic_y italic_l italic_e end_POSTSUBSCRIPT  loss in row 4;  (iii) ; Model with frozen DINOv2 backbone trained with  L s  t  y  l  e subscript L s t y l e \\mathcal{L}_{style} caligraphic_L start_POSTSUBSCRIPT italic_s italic_t italic_y italic_l italic_e end_POSTSUBSCRIPT  and  L a  t  t  r subscript L a t t r \\mathcal{L}_{attr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_t italic_t italic_r end_POSTSUBSCRIPT  losses in row 5;  (iv)  Our final model with frozen DINOv2 backbone trained with  L s  t  y  l  e subscript L s t y l e \\mathcal{L}_{style} caligraphic_L start_POSTSUBSCRIPT italic_s italic_t italic_y italic_l italic_e end_POSTSUBSCRIPT , and  L a  t  t  r subscript L a t t r \\mathcal{L}_{attr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_t italic_t italic_r end_POSTSUBSCRIPT  losses on attribute balanced training set.",
        "table": "S4.T2.18",
        "footnotes": [],
        "references": [
            "We investigate how the distribution of different hairstyle attributes in training data influences the fairness of the trained model.  Table 6  shows the impact on accuracy of training with the balanced dataset compared with a dataset using uniform sampling (bottom two rows). Balancing the dataset results in a small degradation in accuracy, however, as shown in  Figure 7 , results in a significant improvement in fairness across the majority of groups and attributes, while retaining similar accuracy. In particular we observe a significant fairness improvement for the hair gathered attribute across gender from  85.3 % percent 85.3 85.3\\% 85.3 %  to  91.7 % percent 91.7 91.7\\% 91.7 % , a particularly challenging attribute due to visual ambiguity between gathered and short hairstyles in frontal images. A table with fairness results for different models can be found in the supplementary material.",
            "Additional results of attribute prediction accuracy and fairness for different baselines and our models are reported in  Table 6 , and  Table 7  respectively."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A1.T3.1",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": []
    },
    "id_table_9": {
        "caption": "",
        "table": "A1.T4.5.5",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": []
    },
    "id_table_10": {
        "caption": "",
        "table": "A2.T5.7.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "A2.T6.20.8",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "A2.T7.20.8",
        "footnotes": [],
        "references": []
    }
}