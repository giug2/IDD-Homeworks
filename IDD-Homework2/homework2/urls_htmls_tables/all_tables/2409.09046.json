{
    "id_table_1": {
        "caption": "Table 1:  3-Class Classification Results",
        "table": "S7.T1.1.1",
        "footnotes": [],
        "references": [
            "While Large Language Models (LLMs) excel in text generation and question-answering, their effectiveness in AI legal and policy is limited by outdated knowledge, hallucinations, and inadequate reasoning in complex contexts. Retrieval-Augmented Generation (RAG) systems improve response accuracy by integrating external knowledge but struggle with retrieval errors, poor context integration, and high costs, particularly in interpreting qualitative and quantitative AI legal texts. This paper introduces a Hybrid Parameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy, exemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity classifier for adaptive parameter tuning, a hybrid retrieval strategy combining dense, sparse, and knowledge graph methods, and an evaluation framework with specific question types and metrics. By dynamically adjusting parameters, HyPA-RAG significantly improves retrieval accuracy and response fidelity. Testing on LL144 shows enhanced correctness, faithfulness, and contextual precision, addressing the need for adaptable NLP systems in complex, high-stakes AI legal and policy applications.  1 1 1 The demo (Preview in Appendix  A.1 ), dataset and code will be made publicly available upon acceptance of this paper.",
            "To address these challenges, this research integrates three key components (see Figure  6  in Appendix  A.2  for a flow overview and Figure  1  for the system design):",
            "The hybrid parameter-adaptive RAG system, depicted in Figure  1 , integrates vector-based text chunks and a knowledge graph of entities and their relationships to enhance retrieval accuracy. The system employs a hybrid retrieval process, combining sparse (BM25) and dense (vector) methods to retrieve an initial top- k k k italic_k  set of results. These results are refined using reciprocal rank fusion based on predefined parameter mappings.",
            "Tables  1  and  8  in Appendix  A.10  summarise the classification results. We compare performance using macro precision, recall and F1 score. The fine-tuned DistilBERT model achieved the highest F1 scores, 0.90 for the 3-class task and 0.92 for the 2-class task, highlighting the benefits of transfer learning and fine-tuning. The SVM (TF-IDF) and Logistic Regression models also performed well, particularly in binary classification, indicating their effectiveness in handling sparse data. Zero-shot classifiers performed lower, likely due to the lack of task-specific fine-tuning.",
            "The Parameter-Adaptive RAG system integrates our fine-tuned DistilBERT model to classify query complexity and dynamically adjusts retrieval parameters accordingly, as illustrated in Figure  1 , but excluding the knowledge graph component. The PA-RAG system adaptively selects the number of query rewrites ( Q Q Q italic_Q ) and the top- k k k italic_k  value based on the complexity classification, with specific parameter mappings provided in Table  6  in Appendix  A.6.1 . In the 2-class model, simpler queries (label 0) use a top- k k k italic_k  of 5 and 3 query rewrites, while more complex queries (label 1) use a top- k k k italic_k  of 10 and 5 rewrites. The 3-class model uses a top- k k k italic_k  of 7 and 7 rewrites for the most complex queries (label 2).",
            "The HyPA-RAG system uses the architecture outlined in Figure  1 . The knowledge graph is constructed by extracting triplets (subject, predicate, object) from raw text using GPT-4o. Parameter mappings specific to this implementation, such as the maximum number of keywords per query ( K K K italic_K ) and maximum knowledge sequence length ( S S S italic_S ), are detailed in Table  7 , extending those provided in Table  6 .",
            "We evaluate the impact of adaptive parameters, a reranker (bge-reranker-large), and a query rewriter on model performance using PA and HyPA RAG methods with 2-class (Table  9  in Appendix  A.11 ) and 3-class classifiers (Table  3 )."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Performance metrics for LLM Only, Fixed  k k k italic_k , Parameter-Adaptive (PA), and Hybrid Parameter Adaptive (HyPA) RAG implementations for the 2 and 3-class classifier configurations.  k k k italic_k  is the top- k k k italic_k  value,  Q Q Q italic_Q  the number of query rewrites,  S S S italic_S  the maximum knowledge graph depth, and  K K K italic_K  the maximum keywords for knowledge graph retrieval.",
        "table": "S7.T2.9.9",
        "footnotes": [],
        "references": [
            "To address these challenges, this research integrates three key components (see Figure  6  in Appendix  A.2  for a flow overview and Figure  1  for the system design):",
            "The evaluation process starts by generating custom questions tailored to AI policy and legal question-answering, then introduces and verifies evaluation metrics (see evaluation section of Figure  6  in Appendix  A.2 ).  For reproducibility, the LLM temperature is set to zero for consistent responses and all other parameters are set to defaults.",
            "The Spearman coefficient (Figure  2 ) illustrates how our prompt-based LLM-as-a-judge correctness evaluation aligns with human judgment. Prompts 1 and 2 (Appendix  A.7 ) employ different methods: the baseline prompt provides general scoring guidelines, Prompt 1 offers detailed refinements, and Prompt 2 includes one-shot examples and guidance for edge cases.",
            "The adaptive methods generally outperform the fixed  k k k italic_k  baseline across most metrics (Table  2 ). PA-RAG with  k , Q k Q k,Q italic_k , italic_Q  (2 class) achieves the highest faithfulness score of 0.9044, which is an improvement of 0.0564 over the best fixed  k = 10 k 10 k=10 italic_k = 10  method (0.8480). Similarly, the PA  k , Q k Q k,Q italic_k , italic_Q  (3 class) configuration also performs strongly with a faithfulness score of 0.8971, surpassing all fixed  k k k italic_k  methods."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:   Ablation study results for different configurations of adaptive  k k k italic_k  in a 3-class setting. For descriptions of parameters, refer to Table  2 . The highest value in each column is highlighted in bold, and the second highest value is underlined. The * indicates parameters held fixed, rather than adaptive.",
        "table": "S8.T3.22",
        "footnotes": [],
        "references": [
            "Creating a \"gold standard\" evaluation set usually requires extensive human expertise and time, but LLMs like GPT-3.5-Turbo can efficiently handle such tasks, if sufficiently prompted. For this purpose, Giskard  (AI,  2023 )  provides a library for synthetic data generation, using LLMs to create various question types from text chunks, such as simple, complex, and situational. We introduce additional types and question generators: comparative, complex situational, vague, and rule-conclusion. Comparative questions require multi-context retrieval to compare concepts. Complex situational questions involve user-specific contexts and follow-ups. Vague questions obscure parts of the query to test interpretation, while rule-conclusion questions, adapted from LegalBench  (Guha et al.,  2023 ) , require conclusions based on legislative content. Table  4  in Appendix  A.3  summarises these types with examples.",
            "Figure  3  shows that pattern-based chunking achieves the highest context recall (0.9046), faithfulness (0.8430), answer similarity (0.8621), and correctness (0.7918) scores. Sentence-level chunking, however, yields the highest context precision and F1 scores. Semantic chunking performs reasonably well with increased buffer size but generally underperforms compared to the simpler methods. Further hyperparameter tuning may improve its effectiveness. These findings suggest that a corpus-specific delimiter can enhance performance over standard chunking methods.",
            "We evaluate the impact of adaptive parameters, a reranker (bge-reranker-large), and a query rewriter on model performance using PA and HyPA RAG methods with 2-class (Table  9  in Appendix  A.11 ) and 3-class classifiers (Table  3 )."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:   Question types and their descriptions with targeted RAG components.",
        "table": "A1.T4.1",
        "footnotes": [],
        "references": [
            "Creating a \"gold standard\" evaluation set usually requires extensive human expertise and time, but LLMs like GPT-3.5-Turbo can efficiently handle such tasks, if sufficiently prompted. For this purpose, Giskard  (AI,  2023 )  provides a library for synthetic data generation, using LLMs to create various question types from text chunks, such as simple, complex, and situational. We introduce additional types and question generators: comparative, complex situational, vague, and rule-conclusion. Comparative questions require multi-context retrieval to compare concepts. Complex situational questions involve user-specific contexts and follow-ups. Vague questions obscure parts of the query to test interpretation, while rule-conclusion questions, adapted from LegalBench  (Guha et al.,  2023 ) , require conclusions based on legislative content. Table  4  in Appendix  A.3  summarises these types with examples.",
            "To enable adaptive parameter selection, we developed a domain-specific query complexity classifier that categorises user queries, each corresponding to specific hyper-parameter mappings. Our analysis of top- k k k italic_k  selection indicated different optimal top- k k k italic_k  values for various question types, as shown in Figure  7  (Appendix  A.4 ).",
            "Integrating human feedback into the evaluation loop (see Figure  4 ) could better align metrics with user preferences and validate performance metrics in real-world settings. Future work should also consider fine-tuning the LLM using techniques like RLHF  (Bai et al.,  2022 ) , RLAIF  (Lee et al.,  2023 ) , or other preference optimisation methods  (Song et al.,  2023 ) . Further, refining the query rewriter  (Ma et al.,  2023 ; Mao et al.,  2024 )  and exploring iterative answer refinement  (Asai et al.,  2023 )  could enhance metrics like relevancy and correctness."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:   Criteria for evaluating the quality of QA pairs.",
        "table": "A1.T5.1",
        "footnotes": [],
        "references": [
            "These question generators produce a set of questions, which are then deduplicated. Inaccurate or incomplete questions are identified through a human expert review process, using the criteria outlined in Table  5  in Appendix  A.5 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Parameter Symbols, Descriptions, and Mappings",
        "table": "A1.T6.13.13",
        "footnotes": [],
        "references": [
            "To address these challenges, this research integrates three key components (see Figure  6  in Appendix  A.2  for a flow overview and Figure  1  for the system design):",
            "The evaluation process starts by generating custom questions tailored to AI policy and legal question-answering, then introduces and verifies evaluation metrics (see evaluation section of Figure  6  in Appendix  A.2 ).  For reproducibility, the LLM temperature is set to zero for consistent responses and all other parameters are set to defaults.",
            "The Parameter-Adaptive RAG system integrates our fine-tuned DistilBERT model to classify query complexity and dynamically adjusts retrieval parameters accordingly, as illustrated in Figure  1 , but excluding the knowledge graph component. The PA-RAG system adaptively selects the number of query rewrites ( Q Q Q italic_Q ) and the top- k k k italic_k  value based on the complexity classification, with specific parameter mappings provided in Table  6  in Appendix  A.6.1 . In the 2-class model, simpler queries (label 0) use a top- k k k italic_k  of 5 and 3 query rewrites, while more complex queries (label 1) use a top- k k k italic_k  of 10 and 5 rewrites. The 3-class model uses a top- k k k italic_k  of 7 and 7 rewrites for the most complex queries (label 2).",
            "The HyPA-RAG system uses the architecture outlined in Figure  1 . The knowledge graph is constructed by extracting triplets (subject, predicate, object) from raw text using GPT-4o. Parameter mappings specific to this implementation, such as the maximum number of keywords per query ( K K K italic_K ) and maximum knowledge sequence length ( S S S italic_S ), are detailed in Table  7 , extending those provided in Table  6 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Parameter Symbols, Descriptions, and Mappings (Part 2)",
        "table": "A1.T7.12.12",
        "footnotes": [],
        "references": [
            "The Spearman coefficient (Figure  2 ) illustrates how our prompt-based LLM-as-a-judge correctness evaluation aligns with human judgment. Prompts 1 and 2 (Appendix  A.7 ) employ different methods: the baseline prompt provides general scoring guidelines, Prompt 1 offers detailed refinements, and Prompt 2 includes one-shot examples and guidance for edge cases.",
            "To enable adaptive parameter selection, we developed a domain-specific query complexity classifier that categorises user queries, each corresponding to specific hyper-parameter mappings. Our analysis of top- k k k italic_k  selection indicated different optimal top- k k k italic_k  values for various question types, as shown in Figure  7  (Appendix  A.4 ).",
            "The HyPA-RAG system uses the architecture outlined in Figure  1 . The knowledge graph is constructed by extracting triplets (subject, predicate, object) from raw text using GPT-4o. Parameter mappings specific to this implementation, such as the maximum number of keywords per query ( K K K italic_K ) and maximum knowledge sequence length ( S S S italic_S ), are detailed in Table  7 , extending those provided in Table  6 ."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  2-Class Classification Results",
        "table": "A1.T8.1.1",
        "footnotes": [],
        "references": [
            "Additional metrics, including macro precision, recall, F1 score, and percentage agreement with human labels, are shown in Figure  8  (Appendix  A.8 ). A detailed breakdown of the Spearman coefficient metrics is provided in Figure  9  (Appendix  A.8 ).",
            "Tables  1  and  8  in Appendix  A.10  summarise the classification results. We compare performance using macro precision, recall and F1 score. The fine-tuned DistilBERT model achieved the highest F1 scores, 0.90 for the 3-class task and 0.92 for the 2-class task, highlighting the benefits of transfer learning and fine-tuning. The SVM (TF-IDF) and Logistic Regression models also performed well, particularly in binary classification, indicating their effectiveness in handling sparse data. Zero-shot classifiers performed lower, likely due to the lack of task-specific fine-tuning."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:   Ablation study results for different configurations starting from adaptive  k k k italic_k . The highest value in each column is highlighted in bold, and the second highest value is underlined.",
        "table": "A1.T9.18",
        "footnotes": [],
        "references": [
            "Additional metrics, including macro precision, recall, F1 score, and percentage agreement with human labels, are shown in Figure  8  (Appendix  A.8 ). A detailed breakdown of the Spearman coefficient metrics is provided in Figure  9  (Appendix  A.8 ).",
            "To train a domain-specific query complexity classifier, we generated a dataset using a GPT-4o model on legal documents. Queries were categorised into three classes based on the number of contexts required: one context (0), two contexts (1), and three or more contexts (2). This classification resulted in varying token counts, keywords, and clauses across classes, which could bias models toward associating these features with complexity. To mitigate this, we applied data augmentation techniques to diversify the dataset.  To enhance robustness, 67% of the queries were modified. We increased vagueness in 10% of the questions while preserving their informational content, added random noise words or punctuation to another 10%, and applied both word and punctuation noise to a further 10%. Additionally, 5% of questions had phrases reordered, and another 5% contained random spelling errors. For label-specific augmentation, 25% of label 0 queries were made more verbose, and 25% of label 2 queries were shortened, ensuring they retained the necessary informational content. The augmentation prompts are in Appendix  A.9 .",
            "We evaluate the impact of adaptive parameters, a reranker (bge-reranker-large), and a query rewriter on model performance using PA and HyPA RAG methods with 2-class (Table  9  in Appendix  A.11 ) and 3-class classifiers (Table  3 )."
        ]
    },
    "global_footnotes": [
        "The demo (Preview in Appendix",
        "), dataset and code will be made publicly available upon acceptance of this paper."
    ]
}