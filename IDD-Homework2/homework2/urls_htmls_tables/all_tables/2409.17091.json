{
    "id_table_1": {
        "caption": "TABLE I:  Experimental datasets and settings.",
        "table": "S4.T1.1",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "Intuitively, building customized, high-fidelity, and sequentially coherent synthetic medical databases, and effectively utilizing them, is essential for enhancing sequence recognition. However, this task presents several challenges. First, lesions (Fig.  1 (a-b)) or structures (Fig.  1 (c)) of the same disease category exhibit large visual variances (e.g., shapes, intensities, positions, etc). This may confuse diffusion learning seriously, thus causing uncontrollable and unreliable generation. Second, artifacts and noises (e.g., US speckle noise) may prevent models from accurately perceiving vital anatomical targets, hindering the synthesis of high-fidelity sequences. Third, the complex dynamic changes (Fig.  1 (a-b)) and the varying sizes of anatomical regions (Fig.  1 (c)) challenge the coherence of synthesized sequences. The fourth challenge lies in the domain gap between synthetic and real samples, where achieving real-domain customization is crucial for effectively utilizing synthetic samples in downstream learning  [ 26 ] . Lastly, even a well-designed sequence generator cannot always guarantee high-quality synthesis due to random sampling, and unsatisfactory synthetic samples may negatively impact subsequent classifier learning. In summary, from model- and data-centric perspectives, the challenges of the task can be concluded as: 1) How can we design the generator architecture to achieve satisfactory sequence synthesis? 2) How can we discriminatively filter out potentially harmful synthetic samples?",
            "Fig.  3  shows the pipeline of our proposed sequence generator. It supports customized and high-quality medical sequence generation via multimodal conditions. The generator is implemented with a two-stage training scheme. In the pretraining stage, it attends to high-fidelity  visual features learning  for controllable image synthesis. While in the finetuning stage, the domain-specific visual knowledge acquired from the previous stage is reused and focuses on  sequential patterns modeling  for customized sequence synthesis. During inference, given a single or combination of multimodal conditions as control signal inputs, high-quality and steerable sequence generation from Gaussian noise can work. We then provide a short background of video diffusion models in Sec.  III-B 1 , followed by a detailed description of the basic architecture of our generator in Secs.  III-B 2  and  III-B 3 .",
            "We use the proposed sequence generator to constitute the synthetic samples set. Concretely, assuming there are  n n n italic_n  training clips in the target dataset, with a bank of conditions derived from each clip, we synthesize a group of clips guided by each conditions bank. Eventually, we obtain  n n n italic_n  groups of clips to form our synthetic sample set, with a total of  N N N italic_N  clips. Although visually realistic and smooth (good cases, Fig.  5 ), sequence synthesis may still suffer from class semantics misalignment, cross-frame/slice inconsistency or over-consistency (i.e., almost static clip), and inter-clip similarity. For instance, in Fig.  5 (e), the synthesized carotid clip category is wrong, which should be moderate rather than mild. In Fig.  5 (f), the generated clip includes abrupt changes in anatomical structures. Hence, blindly using all synthetic clips for classifier learning will significantly cause a performance drop due to noisy samples. Our work proposes a noisy synthetic data filter to adaptively remove harmful generated clips at class semantics and sequential levels, as illustrated in Algorithm  1 . This can also link the synthetic data to downstream tasks, thus potentially achieving a higher performance upper bound.",
            "This strategy aims to filter noisy samples at a) inner-sequence and b) inter-sequence levels.  For a) , we screen out synthetic clips with gentle dynamic, avoiding those that are either inconsistent or over-consistent affecting downstream learning. Specifically, we retain a synthetic clip whose cross-frame/slice consistency falls in a pre-computed range. It is determined by K-means clustering  [ 64 ]  based on the cross-frame/slice consistency of all synthetic clips. To assess the video sequence consistency,  [ 33 ]  tends to use CLIP  [ 59 ]  image embeddings to compute the average cosine similarity across consecutive frames. However, this method leads to inadequate evaluation due to the usage of limited informative CLIP embeddings. To resolve the problem, we propose a metric termed VAE-Seq that utilizes latent embeddings from pretrained VAE, instead of deriving embeddings from CLIP space, to assess cross-frame/slice consistency. Therefore, with the higher dimension of latents (e.g., 4  \\times  32  \\times  32 for input size 256  \\times  256 compared to 768 in CLIP embeddings), VAE-Seq reflects an accurate evaluation using more fine-grained and informative features.  For b) , we seek to diversify the samples in the synthetic set to prevent overfitting and avoid wasting computational resources in downstream training. To this end, we perform the inter-sequence filtering based on inter-clip similarity in each group, as shown in Algorithm  1 . We quantify the inter-clip similarity by calculating the cosine distance of VAE latent embeddings between frame/slice pairs in two clips and averaging the obtained distance values."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  Diagnostic performance comparison using eleven classifiers trained on  Baseline  (A),  Real-finetune  (B), and  Joint-train  (C) paradigms in multi-organ and multi-modal datasets. Hybrid, the classifier with CNN-Transformer design. Acc., accuracy ( % percent \\% % ).",
        "table": "S4.T2.140",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Fig.  2  shows the pipeline of using our proposed controllable generative augmentation framework to aid medical sequence classification. First, we propose a sequence generator that enables perceiving multiple semantic and sequential conditions to guide controllable and diverse generation. Then, an efficient noisy data filter is introduced to suppress unsatisfactory synthetic sequences. Last, the quality-controlled synthetic sequences and real ones will work together to improve the performance of arbitrary classifiers. To formulate customized, high-fidelity, and coherent synthetic databases to boost classification, we design the whole framework from model-centric, i.e., sequence generator (Secs.  III-B ,  III-C , and  III-D ) and data-centric, i.e., data filter (Sec.  III-E ) perspectives.",
            "Fig.  3  shows the pipeline of our proposed sequence generator. It supports customized and high-quality medical sequence generation via multimodal conditions. The generator is implemented with a two-stage training scheme. In the pretraining stage, it attends to high-fidelity  visual features learning  for controllable image synthesis. While in the finetuning stage, the domain-specific visual knowledge acquired from the previous stage is reused and focuses on  sequential patterns modeling  for customized sequence synthesis. During inference, given a single or combination of multimodal conditions as control signal inputs, high-quality and steerable sequence generation from Gaussian noise can work. We then provide a short background of video diffusion models in Sec.  III-B 1 , followed by a detailed description of the basic architecture of our generator in Secs.  III-B 2  and  III-B 3 .",
            "We consider four multimodal conditions to ensure comprehensive and accurate control over the sequence synthesis procedure (see Figs.  2 - 3 ). Specifically, these conditions are divided into semantic and sequential ones for visual appearance control and serial guidance, respectively. We highlight that our generator supports composable synthesis by allowing users to flexibly choose any single condition or composition during inference. This flexibility makes our generator particularly user-friendly, as it enables high-quality synthesis even when certain conditions are missing. Details are described below.",
            "Apart from perceiving visual concepts, modeling sequential knowledge is also important in sequence synthesis, with sequential cues playing a vital role in this process. Most existing methods  [ 35 ,  36 ]  adopted dense optical flow  [ 34 ]  to promote sequential dynamics modeling. However, optical flow extraction requires high computational demands  [ 33 ] . Thus, we instead introduce computation-efficient  motion field  as a sequential condition in sequence LDM. It explicitly showcases the pixel-wise motions between adjacent frames/slices (see Figs.  2 - 3 ). In our approach, we first extract the motion fields of real sequences using a Python package  [ 61 ] . Then, a motion encoder (see Fig.  3 ) receives the motion fields and produces motion features. Last, the features are concatenated with latent representations  z z \\mathit{z} italic_z  from VAE along the channel dimension for serial guidance.",
            "To further boost the  sequential coherence  of the generated sequences, we propose to reuse the motion field condition (refer to Sec.  III-C 2 ) by introducing a motion field attention (MFA) mechanism after KA. As shown in Fig.  4 (c), MFA requires the patches to communicate with those in the same motion field-based pathway including itself, which eliminates flickers of the generated sequences to make the contents visually smooth. Inspired by  [ 62 ] , MFA is implemented in two steps: a) motion field-based patch pathway sampling and b) attention calculation. In  step 1 , we sample the patch pathways based on the  m -scaled downsampled motion fields. Take a video sequence as an example, for a patch  p l subscript p l p_{l} italic_p start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT  on the  l -th frame of a  f -frame video, the  p  a  t  h p a t h path italic_p italic_a italic_t italic_h  can be derived from the motion field. Since the sampling procedure inevitably generates multiple pathways for the same patch, we randomly sample a pathway to ensure its uniqueness to which each patch belongs. In this setting, let  H , W H W H,W italic_H , italic_W  the height and width of the input video frame, then the size of the pathway set after sampling equals  H  W m 2  f H W superscript m 2 f \\frac{H\\times W}{m^{2}}\\times f divide start_ARG italic_H  italic_W end_ARG start_ARG italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG  italic_f . In  step 2 , we calculate  A  t  t  e  n  t  i  o  n  ( Q , K , V ) A t t e n t i o n Q K V Attention(\\mathbf{Q},\\mathbf{K},\\mathbf{V}) italic_A italic_t italic_t italic_e italic_n italic_t italic_i italic_o italic_n ( bold_Q , bold_K , bold_V )  with:",
            "As a common practice, existing sequence synthesis works evaluate the synthetic quality by mostly using Frechet Inception Distance (FID)  [ 65 ]  and Frechet Video Distance (FVD)  [ 66 ] . However, researchers have verified that these metrics do not consistently correlate with performance metrics on downstream tasks  [ 46 ,  7 ] . To exhaustively evaluate the values of synthetic samples to downstream tasks, eleven popular classifiers are deployed for comparison with respect to diagnostic accuracy and area under the receiver operating characteristic curve (AUROC). Besides, we adopt the proposed VAE-Seq to assess the cross-frame/slice consistency (refer to Sec.  III-E 2 ). Moreover, we consider it crucial to evaluate the smoothness of generated clips. Motivated by  [ 67 ] , we quantify it using the proposed metric (termed Dynamic Smoothness), which is the mean absolute error between the reconstructed frames/slices from the interpolation model  [ 68 ]  and the original real ones.",
            "Carotid dataset  was collected by three medical centers with approval from local institutional review boards, including a) The Third Affiliated Hospital of Sun Yat-sen University, b) Shenzhen Longgang District Peoples Hospital, and c) The Eighth Affiliated Hospital of Sun Yat-sen University. It consists of 273 US video sequences from 231 patients with carotid plaque-induced stenosis. The severity of carotid stenosis is graded into three stages, i.e., mild, moderate, and severe. We uniformly sampled several non-overlapping 8-frame clips from each sequence. The final dataset involved an in-domain (ID) subset (407 clips from 193 patients collected by Center (a)) and an out-domain (OD) subset (79 clips from 38 patients acquired from Center (b)-(c)). The ID/OD subset was randomly split into 309/39 and 98/40 clips for training and testing at the patient level. Each clip has up to two plaques and was labeled with stenosis grading and descriptive text by experienced sonographers using Pair annotation software package  [ 72 ] . The text annotations indicated the plaque characteristics of echogenicity and location (see Fig.  2 ). We cropped the region of interest based on the pretrained carotid vessel detector  [ 73 ]  for easing the model learning."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III:  Out-domain performance comparison using three nets trained on  Baseline  (A),  Real-finetune  (B), and  Joint-train  (C) paradigms in carotid dataset. Hybrid, the net with CNN-Transformer design. Acc., accuracy ( % percent \\% % ).",
        "table": "S4.T3.16",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "Fig.  3  shows the pipeline of our proposed sequence generator. It supports customized and high-quality medical sequence generation via multimodal conditions. The generator is implemented with a two-stage training scheme. In the pretraining stage, it attends to high-fidelity  visual features learning  for controllable image synthesis. While in the finetuning stage, the domain-specific visual knowledge acquired from the previous stage is reused and focuses on  sequential patterns modeling  for customized sequence synthesis. During inference, given a single or combination of multimodal conditions as control signal inputs, high-quality and steerable sequence generation from Gaussian noise can work. We then provide a short background of video diffusion models in Sec.  III-B 1 , followed by a detailed description of the basic architecture of our generator in Secs.  III-B 2  and  III-B 3 .",
            "In the pretraining phase, we develop a 2D UNet  [ 57 ]  in LDM to predict noises for image synthesis. Referring to  [ 13 ] , we extend the 2D UNet to a 3D version via an inflation scheme to build the sequence LDM. Specifically, all the spatial convolution layers are inflated to pseudo-3D counterparts by expanding the kernels at the sequential dimension (e.g.,  3  3  1  3  3  3 3 1 3 3 3\\times 3\\rightarrow 1\\times 3\\times 3 3  3  1  3  3  kernel). Besides, we perform sequential insertion by adding sequential attention (SA) layers (see Fig.  3 ). Fig.  4 (a) shows the principle of the SA mechanism, where each patch queries to those at the same spatial position and across frames/slices. This design encourages the generator to model the sequential patterns, while not significantly altering visual feature distribution baked in the LDM  [ 31 ] . Overall, sequence LDM enables inheriting the rich visual concepts preserved in LDM and focusing on sequential pattern aggregation, making the model learning efficient.",
            "We consider four multimodal conditions to ensure comprehensive and accurate control over the sequence synthesis procedure (see Figs.  2 - 3 ). Specifically, these conditions are divided into semantic and sequential ones for visual appearance control and serial guidance, respectively. We highlight that our generator supports composable synthesis by allowing users to flexibly choose any single condition or composition during inference. This flexibility makes our generator particularly user-friendly, as it enables high-quality synthesis even when certain conditions are missing. Details are described below.",
            "As shown in Fig.  3 , the sequence generator exploits three semantic conditions to perform visual appearance control, thus achieving controllable and high-fidelity synthesis.",
            "-  Image Prior:  Merely using the above conditions faces challenges of insufficient semantic control and an inevitable domain gap between real and synthetic sequences, constraining the capability of generative augmentation. Hence, we introduce the first frame/slice of real-domain sequences as image priors to provide rich semantic guidance and yield real-domain style sequences. As image prior offers global semantics like texts, we align image prior features  f i subscript f i f_{i} italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  extracted from image prior encoder (see Fig.  3 ) with text embeddings  f t subscript f t f_{t} italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  generated by our medical data-specific CLIP text encoder for joint guidance. Specifically, motivated by  [ 60 ] , we replace the text-guided cross-attention layers in original UNet blocks with decoupled counterparts that handle texts and image priors in parallel and then merge the results by addition. The decoupled attention can be formulated as:",
            "Apart from perceiving visual concepts, modeling sequential knowledge is also important in sequence synthesis, with sequential cues playing a vital role in this process. Most existing methods  [ 35 ,  36 ]  adopted dense optical flow  [ 34 ]  to promote sequential dynamics modeling. However, optical flow extraction requires high computational demands  [ 33 ] . Thus, we instead introduce computation-efficient  motion field  as a sequential condition in sequence LDM. It explicitly showcases the pixel-wise motions between adjacent frames/slices (see Figs.  2 - 3 ). In our approach, we first extract the motion fields of real sequences using a Python package  [ 61 ] . Then, a motion encoder (see Fig.  3 ) receives the motion fields and produces motion features. Last, the features are concatenated with latent representations  z z \\mathit{z} italic_z  from VAE along the channel dimension for serial guidance."
        ]
    },
    "id_table_4": {
        "caption": "TABLE IV:  Ablation study for different conditional controls in TUSC  [ 69 ] , including class label (C), text (T), image prior (I), and motion field (MF). SlowFast  [ 81 ]  (C1), CSN  [ 82 ]  (C2), and FTC  [ 78 ]  (C3) trained on  Joint-train  paradigm were used for downstream diagnosis. Hybrid, the classifier with CNN-Transformer design.",
        "table": "S4.T4.29",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "In the pretraining phase, we develop a 2D UNet  [ 57 ]  in LDM to predict noises for image synthesis. Referring to  [ 13 ] , we extend the 2D UNet to a 3D version via an inflation scheme to build the sequence LDM. Specifically, all the spatial convolution layers are inflated to pseudo-3D counterparts by expanding the kernels at the sequential dimension (e.g.,  3  3  1  3  3  3 3 1 3 3 3\\times 3\\rightarrow 1\\times 3\\times 3 3  3  1  3  3  kernel). Besides, we perform sequential insertion by adding sequential attention (SA) layers (see Fig.  3 ). Fig.  4 (a) shows the principle of the SA mechanism, where each patch queries to those at the same spatial position and across frames/slices. This design encourages the generator to model the sequential patterns, while not significantly altering visual feature distribution baked in the LDM  [ 31 ] . Overall, sequence LDM enables inheriting the rich visual concepts preserved in LDM and focusing on sequential pattern aggregation, making the model learning efficient.",
            "Solely equipping with SA layers and sequential cues in sequence LDM may present inadequate consistency and coherence across synthetic frames/slices. This issue may arise from insufficient sequential modeling of input noisy latents and motion fields, constrained by a small parameter space, overly burdening the SA layers. To solve the issue, we propose a sequential augmentation module (SAM) that enables the generator to more effectively model sequential dependencies. As shown in Fig.  4 (d), SAM integrates two attention mechanisms in cascade for sequential augmentation.",
            "where  W Q , W K , W V superscript W Q superscript W K superscript W V W^{Q},W^{K},W^{V} italic_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT , italic_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT , italic_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT  are initialized on the original spatial self-attention weights for inheriting the semantic perception capability of LDM in the finetuning stage.  {  }  \\left\\{\\cdot\\right\\} {  }  represents concatenation operation. It is highlighted that KA retains low computational complexity compared with full attention  [ 19 ] . Please refer to Fig.  4 (b) for a visual illustration.",
            "To further boost the  sequential coherence  of the generated sequences, we propose to reuse the motion field condition (refer to Sec.  III-C 2 ) by introducing a motion field attention (MFA) mechanism after KA. As shown in Fig.  4 (c), MFA requires the patches to communicate with those in the same motion field-based pathway including itself, which eliminates flickers of the generated sequences to make the contents visually smooth. Inspired by  [ 62 ] , MFA is implemented in two steps: a) motion field-based patch pathway sampling and b) attention calculation. In  step 1 , we sample the patch pathways based on the  m -scaled downsampled motion fields. Take a video sequence as an example, for a patch  p l subscript p l p_{l} italic_p start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT  on the  l -th frame of a  f -frame video, the  p  a  t  h p a t h path italic_p italic_a italic_t italic_h  can be derived from the motion field. Since the sampling procedure inevitably generates multiple pathways for the same patch, we randomly sample a pathway to ensure its uniqueness to which each patch belongs. In this setting, let  H , W H W H,W italic_H , italic_W  the height and width of the input video frame, then the size of the pathway set after sampling equals  H  W m 2  f H W superscript m 2 f \\frac{H\\times W}{m^{2}}\\times f divide start_ARG italic_H  italic_W end_ARG start_ARG italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG  italic_f . In  step 2 , we calculate  A  t  t  e  n  t  i  o  n  ( Q , K , V ) A t t e n t i o n Q K V Attention(\\mathbf{Q},\\mathbf{K},\\mathbf{V}) italic_A italic_t italic_t italic_e italic_n italic_t italic_i italic_o italic_n ( bold_Q , bold_K , bold_V )  with:"
        ]
    },
    "id_table_5": {
        "caption": "TABLE V:  Ablation study for the sequential augmentation module of the generator in TUSC  [ 69 ]  and ACDC  [ 70 ] . SlowFast  [ 81 ]  (C1), CSN  [ 82 ]  (C2), and FTC  [ 78 ]  (C3) trained on  Joint-train  paradigm were used for downstream diagnosis.   Hybrid, the classifier with CNN-Transformer design.",
        "table": "S4.T5.41",
        "footnotes": [
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "We use the proposed sequence generator to constitute the synthetic samples set. Concretely, assuming there are  n n n italic_n  training clips in the target dataset, with a bank of conditions derived from each clip, we synthesize a group of clips guided by each conditions bank. Eventually, we obtain  n n n italic_n  groups of clips to form our synthetic sample set, with a total of  N N N italic_N  clips. Although visually realistic and smooth (good cases, Fig.  5 ), sequence synthesis may still suffer from class semantics misalignment, cross-frame/slice inconsistency or over-consistency (i.e., almost static clip), and inter-clip similarity. For instance, in Fig.  5 (e), the synthesized carotid clip category is wrong, which should be moderate rather than mild. In Fig.  5 (f), the generated clip includes abrupt changes in anatomical structures. Hence, blindly using all synthetic clips for classifier learning will significantly cause a performance drop due to noisy samples. Our work proposes a noisy synthetic data filter to adaptively remove harmful generated clips at class semantics and sequential levels, as illustrated in Algorithm  1 . This can also link the synthetic data to downstream tasks, thus potentially achieving a higher performance upper bound.",
            "To illustrate the role of different conditional guidance, we conduct comparative experiments with generators that resort to various banks of conditions for training and sampling. Table  IV  validates that all proposed conditions are effective for enhancing synthesis and diagnosis tasks. Specifically, without text control, the downstream diagnostic performance is comparatively poor. Fig.  5 (b-c) compares typical synthetic thyroid nodule sequences generated under condition banks with and without text guidance. The former is visibly high-fidelity and faithful to the given text prompt (e.g., smooth margin), while the latter exhibits less distinguishable features for diagnosis (e.g., blurry margin). That is to say, the proposed  Ctrl-GenAug  can create more diagnosis-reliable samples by enhancing semantic steerability in the generation process. Besides, by incorporating image prior knowledge in the sampling process, the average accuracy and AUROC of three classifiers are improved by 4.20% and 0.040, respectively. This proves the domain gap between synthetic and real samples is mitigated by introducing the image prior. Moreover, as shown in Table  IV , conditioning the generator on the motion field produces more content-consistent and smoother samples, leading to continuous improvements in diagnostic performance. It can also be observed that the FVD results show a limited correlation with downstream evaluation metrics, confirming the finding in  [ 7 ] ."
        ]
    },
    "id_table_6": {
        "caption": "TABLE VI:  Ablation study for augmentation-free (None), traditional augmentation (TraAug), and generative augmentation (GenAug) using I3D  [ 79 ]  in three diagnostic tasks. For GenAug, both  Real-finetune  and  Joint-train  paradigms were investigated. Acc., accuracy ( % percent \\% % ).",
        "table": "S4.T6.32",
        "footnotes": [
            ""
        ],
        "references": [
            "Fig.  6  quantitatively compares the carotid stenosis diagnostic performance of underrepresented high-risk sets on  Baseline  and  Joint-train  paradigms. It shows that  Baseline  performs poorly on underrepresented high-risk sets, especially in terms of sensitivity (average 18.52% in moderate and 36.25% in severe sets). In comparison, the classifiers can be greatly enhanced by jointly training with synthetic samples (e.g.,  SlowFast : 11.11%   \\rightarrow  44.44% sensitivity on the moderate set)."
        ]
    },
    "id_table_7": {
        "caption": "TABLE S1:  A brief review of studies in the medical field utilizing diffusion-based generative models to promote downstream tasks. We consider six aspects: the conditional control, the downstream task, the underlying architecture of the downstream network, the number of the downstream network, and the modality and type of the medical dataset on which the surveyed approaches were applied. Arch., architecture. Num., number. OCT, optical coherence tomography.",
        "table": "S5.T1.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "As shown in Fig.  7 , we validated the indispensable role of our proposed noisy synthetic data filter by assessing its impact on downstream diagnostic accuracy, using two classifiers trained on  Real-finetune  and  Joint-train  paradigms across the carotid and ACDC datasets. For instance, on the carotid dataset, the  Joint-train  CSN resorting to non-quality controlled synthetic clips (i.e., None), displays no significant difference with its  Baseline  (80.95%  vs . 80.27%), even an accuracy degradation for its VideoSwin counterpart (78.91%  vs . 79.59%). Then, by implementing our CF and SF strategy, diagnostic accuracy is significantly improved by 2.04% and 3.40%, respectively. Equipped with both, it achieves an accuracy of 87.07%, outperforming the None by 6.12%."
        ]
    }
}