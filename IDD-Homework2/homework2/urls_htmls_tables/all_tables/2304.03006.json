{
    "PAPER'S NUMBER OF TABLES": 6,
    "S3.T1": {
        "caption": "TABLE I: Accuracy of convolutional model on CIFAR-10 after 150 epochs. The top accuracy in each case is highlighted.",
        "table": "<table id=\"S3.T1.st1.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.st1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Update</td>\n<td id=\"S3.T1.st1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">#Models</td>\n<td id=\"S3.T1.st1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Accuracy</td>\n</tr>\n<tr id=\"S3.T1.st1.2.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">25</td>\n<td id=\"S3.T1.st1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st1.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">50.57%</td>\n</tr>\n<tr id=\"S3.T1.st1.2.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">25</td>\n<td id=\"S3.T1.st1.2.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st1.2.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">51.04%</td>\n</tr>\n<tr id=\"S3.T1.st1.2.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">25</td>\n<td id=\"S3.T1.st1.2.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st1.2.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">51.33%</td>\n</tr>\n<tr id=\"S3.T1.st1.2.5.5\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">50</td>\n<td id=\"S3.T1.st1.2.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st1.2.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">50.37%</td>\n</tr>\n<tr id=\"S3.T1.st1.2.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">50</td>\n<td id=\"S3.T1.st1.2.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st1.2.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">51.3%</td>\n</tr>\n<tr id=\"S3.T1.st1.2.7.7\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">50</td>\n<td id=\"S3.T1.st1.2.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st1.2.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">52.33%</td>\n</tr>\n<tr id=\"S3.T1.st1.2.8.8\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">75</td>\n<td id=\"S3.T1.st1.2.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st1.2.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">50.95%</td>\n</tr>\n<tr id=\"S3.T1.st1.2.9.9\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">75</td>\n<td id=\"S3.T1.st1.2.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st1.2.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\">49.03%</td>\n</tr>\n<tr id=\"S3.T1.st1.2.10.10\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">75</td>\n<td id=\"S3.T1.st1.2.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st1.2.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\">* 52.90% *</td>\n</tr>\n<tr id=\"S3.T1.st1.2.11.11\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" colspan=\"2\">Non-Federated</td>\n<td id=\"S3.T1.st1.2.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">52.04%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "To test our system111We will release the code used to generate these results in a python notebook on GitHub once this paper is accepted for publication. we used TensorFlow to build a simple model, comprising two convolutional and max pooling layers, a final convolutional layer and two dense layers, to classify the CIFAR-10 dataset. Using both standard and federated training paradigms we trained the model five times using 10%, 25%, 50%, 75% and 100% of the training data; in the federated case the data was shared equally amongst all participating models, such that no two models saw the same data points, as would be the case in a live system (especially when using image data as the input). For each subset of the data, the model was trained for 150 epochs whilst additionally measuring the affect of altering the number of epochs each participating member trained for before the federated update and the number of participates to the federated scheme as shown in Table I. The resulting accuracies in each sub-table are all within a small range showing that federated learning produces similar results to the standard method but with the benefit of being applicable to distribution and working on different (albeit similar) datasets with no shared datapoints. Furthermore, when training using federated updates, with each “‘local training”’ round being performed sequentially, the training process runs quicker compared with the standard method due to each participant operating on smaller subsets of the data, allowing for optimisations such as better caching; this is especially apparent on smaller devices."
        ]
    },
    "S3.T1.st1": {
        "caption": "(a) Trained with 10% of the training data",
        "table": "<table id=\"S3.T1.st1.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.st1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Update</td>\n<td id=\"S3.T1.st1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">#Models</td>\n<td id=\"S3.T1.st1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Accuracy</td>\n</tr>\n<tr id=\"S3.T1.st1.2.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">25</td>\n<td id=\"S3.T1.st1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st1.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">50.57%</td>\n</tr>\n<tr id=\"S3.T1.st1.2.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">25</td>\n<td id=\"S3.T1.st1.2.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st1.2.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">51.04%</td>\n</tr>\n<tr id=\"S3.T1.st1.2.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">25</td>\n<td id=\"S3.T1.st1.2.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st1.2.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">51.33%</td>\n</tr>\n<tr id=\"S3.T1.st1.2.5.5\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">50</td>\n<td id=\"S3.T1.st1.2.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st1.2.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">50.37%</td>\n</tr>\n<tr id=\"S3.T1.st1.2.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">50</td>\n<td id=\"S3.T1.st1.2.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st1.2.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">51.3%</td>\n</tr>\n<tr id=\"S3.T1.st1.2.7.7\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">50</td>\n<td id=\"S3.T1.st1.2.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st1.2.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">52.33%</td>\n</tr>\n<tr id=\"S3.T1.st1.2.8.8\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">75</td>\n<td id=\"S3.T1.st1.2.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st1.2.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">50.95%</td>\n</tr>\n<tr id=\"S3.T1.st1.2.9.9\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">75</td>\n<td id=\"S3.T1.st1.2.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st1.2.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\">49.03%</td>\n</tr>\n<tr id=\"S3.T1.st1.2.10.10\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">75</td>\n<td id=\"S3.T1.st1.2.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st1.2.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\">* 52.90% *</td>\n</tr>\n<tr id=\"S3.T1.st1.2.11.11\" class=\"ltx_tr\">\n<td id=\"S3.T1.st1.2.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" colspan=\"2\">Non-Federated</td>\n<td id=\"S3.T1.st1.2.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">52.04%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We propose a novel method for distributing the training step of each individual device (prior to the federated step) to another device (node) without losing the privacy guarantee of federated learning; a reconstructable form of the training data never leaves the device. This would allow hospitals to harvest spare computing power, e.g. from a receptionist’s PC.",
            "The criticism stems from the high computational cost that provide no actual benefit, other than to make it infeasible for a malicious node to pervert the system. However, in an IoT system this is actually beneficial over PoS; for example, with hundreds of mining devices, the problem can be split across them, much like mining pools.\nMoreover, since the blockchain is being utilized as a trust mechanism for federated learning, the mining target difficulty can remain lower, reducing the computational cost and increasing the rate at which blocks are added to the chain; this results in lower powered devices having enough computing resources to generate hashes competitively whilst still providing the same protection. We therefore decided on adding a block approximately every 1.5 minutes; this is long enough for multiple local updates, from different sources, to be added to the block, prior to the block being added to the chain, without being so long that either the global update is outdated or a local device that misses the update will grow stale. PoS would not be as suitable since it relies too heavily on transactions, doesn’t include mining, would give too much power to larger institutions and it promotes coin hording which negates the bonus benefit of blockchain, rewards: This is what incentivizes hospitals to utilise their spare computing power.",
            "To test our system111We will release the code used to generate these results in a python notebook on GitHub once this paper is accepted for publication. we used TensorFlow to build a simple model, comprising two convolutional and max pooling layers, a final convolutional layer and two dense layers, to classify the CIFAR-10 dataset. Using both standard and federated training paradigms we trained the model five times using 10%, 25%, 50%, 75% and 100% of the training data; in the federated case the data was shared equally amongst all participating models, such that no two models saw the same data points, as would be the case in a live system (especially when using image data as the input). For each subset of the data, the model was trained for 150 epochs whilst additionally measuring the affect of altering the number of epochs each participating member trained for before the federated update and the number of participates to the federated scheme as shown in Table I. The resulting accuracies in each sub-table are all within a small range showing that federated learning produces similar results to the standard method but with the benefit of being applicable to distribution and working on different (albeit similar) datasets with no shared datapoints. Furthermore, when training using federated updates, with each “‘local training”’ round being performed sequentially, the training process runs quicker compared with the standard method due to each participant operating on smaller subsets of the data, allowing for optimisations such as better caching; this is especially apparent on smaller devices."
        ]
    },
    "S3.T1.st2": {
        "caption": "(b) Trained with 25% of the training data",
        "table": "<table id=\"S3.T1.st2.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.st2.2.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.st2.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Update</td>\n<td id=\"S3.T1.st2.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">#Models</td>\n<td id=\"S3.T1.st2.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Accuracy</td>\n</tr>\n<tr id=\"S3.T1.st2.2.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.st2.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">25</td>\n<td id=\"S3.T1.st2.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st2.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">57.69%</td>\n</tr>\n<tr id=\"S3.T1.st2.2.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.st2.2.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">25</td>\n<td id=\"S3.T1.st2.2.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st2.2.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">57.01%</td>\n</tr>\n<tr id=\"S3.T1.st2.2.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.st2.2.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">25</td>\n<td id=\"S3.T1.st2.2.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st2.2.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">57.87%</td>\n</tr>\n<tr id=\"S3.T1.st2.2.5.5\" class=\"ltx_tr\">\n<td id=\"S3.T1.st2.2.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">50</td>\n<td id=\"S3.T1.st2.2.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st2.2.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">57.81%</td>\n</tr>\n<tr id=\"S3.T1.st2.2.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.st2.2.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">50</td>\n<td id=\"S3.T1.st2.2.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st2.2.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">57.95%</td>\n</tr>\n<tr id=\"S3.T1.st2.2.7.7\" class=\"ltx_tr\">\n<td id=\"S3.T1.st2.2.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">50</td>\n<td id=\"S3.T1.st2.2.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st2.2.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">58.16%</td>\n</tr>\n<tr id=\"S3.T1.st2.2.8.8\" class=\"ltx_tr\">\n<td id=\"S3.T1.st2.2.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">75</td>\n<td id=\"S3.T1.st2.2.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st2.2.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">58.54%</td>\n</tr>\n<tr id=\"S3.T1.st2.2.9.9\" class=\"ltx_tr\">\n<td id=\"S3.T1.st2.2.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">75</td>\n<td id=\"S3.T1.st2.2.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st2.2.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\">* 59.51% *</td>\n</tr>\n<tr id=\"S3.T1.st2.2.10.10\" class=\"ltx_tr\">\n<td id=\"S3.T1.st2.2.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">75</td>\n<td id=\"S3.T1.st2.2.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st2.2.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\">59.43%</td>\n</tr>\n<tr id=\"S3.T1.st2.2.11.11\" class=\"ltx_tr\">\n<td id=\"S3.T1.st2.2.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" colspan=\"2\">Non-Federated</td>\n<td id=\"S3.T1.st2.2.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">56.94%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We propose a novel method for distributing the training step of each individual device (prior to the federated step) to another device (node) without losing the privacy guarantee of federated learning; a reconstructable form of the training data never leaves the device. This would allow hospitals to harvest spare computing power, e.g. from a receptionist’s PC.",
            "The criticism stems from the high computational cost that provide no actual benefit, other than to make it infeasible for a malicious node to pervert the system. However, in an IoT system this is actually beneficial over PoS; for example, with hundreds of mining devices, the problem can be split across them, much like mining pools.\nMoreover, since the blockchain is being utilized as a trust mechanism for federated learning, the mining target difficulty can remain lower, reducing the computational cost and increasing the rate at which blocks are added to the chain; this results in lower powered devices having enough computing resources to generate hashes competitively whilst still providing the same protection. We therefore decided on adding a block approximately every 1.5 minutes; this is long enough for multiple local updates, from different sources, to be added to the block, prior to the block being added to the chain, without being so long that either the global update is outdated or a local device that misses the update will grow stale. PoS would not be as suitable since it relies too heavily on transactions, doesn’t include mining, would give too much power to larger institutions and it promotes coin hording which negates the bonus benefit of blockchain, rewards: This is what incentivizes hospitals to utilise their spare computing power.",
            "To test our system111We will release the code used to generate these results in a python notebook on GitHub once this paper is accepted for publication. we used TensorFlow to build a simple model, comprising two convolutional and max pooling layers, a final convolutional layer and two dense layers, to classify the CIFAR-10 dataset. Using both standard and federated training paradigms we trained the model five times using 10%, 25%, 50%, 75% and 100% of the training data; in the federated case the data was shared equally amongst all participating models, such that no two models saw the same data points, as would be the case in a live system (especially when using image data as the input). For each subset of the data, the model was trained for 150 epochs whilst additionally measuring the affect of altering the number of epochs each participating member trained for before the federated update and the number of participates to the federated scheme as shown in Table I. The resulting accuracies in each sub-table are all within a small range showing that federated learning produces similar results to the standard method but with the benefit of being applicable to distribution and working on different (albeit similar) datasets with no shared datapoints. Furthermore, when training using federated updates, with each “‘local training”’ round being performed sequentially, the training process runs quicker compared with the standard method due to each participant operating on smaller subsets of the data, allowing for optimisations such as better caching; this is especially apparent on smaller devices."
        ]
    },
    "S3.T1.st3": {
        "caption": "(c) Trained with 50% of the training data",
        "table": "<table id=\"S3.T1.st3.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.st3.2.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.st3.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Update</td>\n<td id=\"S3.T1.st3.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">#Models</td>\n<td id=\"S3.T1.st3.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Accuracy</td>\n</tr>\n<tr id=\"S3.T1.st3.2.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.st3.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">25</td>\n<td id=\"S3.T1.st3.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st3.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">63.16%</td>\n</tr>\n<tr id=\"S3.T1.st3.2.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.st3.2.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">25</td>\n<td id=\"S3.T1.st3.2.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st3.2.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">* 64.29% *</td>\n</tr>\n<tr id=\"S3.T1.st3.2.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.st3.2.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">25</td>\n<td id=\"S3.T1.st3.2.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st3.2.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">63.53%</td>\n</tr>\n<tr id=\"S3.T1.st3.2.5.5\" class=\"ltx_tr\">\n<td id=\"S3.T1.st3.2.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">50</td>\n<td id=\"S3.T1.st3.2.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st3.2.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">63.06%</td>\n</tr>\n<tr id=\"S3.T1.st3.2.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.st3.2.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">50</td>\n<td id=\"S3.T1.st3.2.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st3.2.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">64.24%</td>\n</tr>\n<tr id=\"S3.T1.st3.2.7.7\" class=\"ltx_tr\">\n<td id=\"S3.T1.st3.2.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">50</td>\n<td id=\"S3.T1.st3.2.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st3.2.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">62.33%</td>\n</tr>\n<tr id=\"S3.T1.st3.2.8.8\" class=\"ltx_tr\">\n<td id=\"S3.T1.st3.2.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">75</td>\n<td id=\"S3.T1.st3.2.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st3.2.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">63.93%</td>\n</tr>\n<tr id=\"S3.T1.st3.2.9.9\" class=\"ltx_tr\">\n<td id=\"S3.T1.st3.2.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">75</td>\n<td id=\"S3.T1.st3.2.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st3.2.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\">63.64%</td>\n</tr>\n<tr id=\"S3.T1.st3.2.10.10\" class=\"ltx_tr\">\n<td id=\"S3.T1.st3.2.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">75</td>\n<td id=\"S3.T1.st3.2.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st3.2.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\">63.89%</td>\n</tr>\n<tr id=\"S3.T1.st3.2.11.11\" class=\"ltx_tr\">\n<td id=\"S3.T1.st3.2.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" colspan=\"2\">Non-Federated</td>\n<td id=\"S3.T1.st3.2.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">62.62%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We propose a novel method for distributing the training step of each individual device (prior to the federated step) to another device (node) without losing the privacy guarantee of federated learning; a reconstructable form of the training data never leaves the device. This would allow hospitals to harvest spare computing power, e.g. from a receptionist’s PC.",
            "The criticism stems from the high computational cost that provide no actual benefit, other than to make it infeasible for a malicious node to pervert the system. However, in an IoT system this is actually beneficial over PoS; for example, with hundreds of mining devices, the problem can be split across them, much like mining pools.\nMoreover, since the blockchain is being utilized as a trust mechanism for federated learning, the mining target difficulty can remain lower, reducing the computational cost and increasing the rate at which blocks are added to the chain; this results in lower powered devices having enough computing resources to generate hashes competitively whilst still providing the same protection. We therefore decided on adding a block approximately every 1.5 minutes; this is long enough for multiple local updates, from different sources, to be added to the block, prior to the block being added to the chain, without being so long that either the global update is outdated or a local device that misses the update will grow stale. PoS would not be as suitable since it relies too heavily on transactions, doesn’t include mining, would give too much power to larger institutions and it promotes coin hording which negates the bonus benefit of blockchain, rewards: This is what incentivizes hospitals to utilise their spare computing power.",
            "To test our system111We will release the code used to generate these results in a python notebook on GitHub once this paper is accepted for publication. we used TensorFlow to build a simple model, comprising two convolutional and max pooling layers, a final convolutional layer and two dense layers, to classify the CIFAR-10 dataset. Using both standard and federated training paradigms we trained the model five times using 10%, 25%, 50%, 75% and 100% of the training data; in the federated case the data was shared equally amongst all participating models, such that no two models saw the same data points, as would be the case in a live system (especially when using image data as the input). For each subset of the data, the model was trained for 150 epochs whilst additionally measuring the affect of altering the number of epochs each participating member trained for before the federated update and the number of participates to the federated scheme as shown in Table I. The resulting accuracies in each sub-table are all within a small range showing that federated learning produces similar results to the standard method but with the benefit of being applicable to distribution and working on different (albeit similar) datasets with no shared datapoints. Furthermore, when training using federated updates, with each “‘local training”’ round being performed sequentially, the training process runs quicker compared with the standard method due to each participant operating on smaller subsets of the data, allowing for optimisations such as better caching; this is especially apparent on smaller devices."
        ]
    },
    "S3.T1.st4": {
        "caption": "(d) Trained with 75% of the training data",
        "table": "<table id=\"S3.T1.st4.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.st4.2.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.st4.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Update</td>\n<td id=\"S3.T1.st4.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">#Models</td>\n<td id=\"S3.T1.st4.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Accuracy</td>\n</tr>\n<tr id=\"S3.T1.st4.2.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.st4.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">25</td>\n<td id=\"S3.T1.st4.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st4.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">65.91%</td>\n</tr>\n<tr id=\"S3.T1.st4.2.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.st4.2.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">25</td>\n<td id=\"S3.T1.st4.2.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st4.2.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">66.06%</td>\n</tr>\n<tr id=\"S3.T1.st4.2.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.st4.2.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">25</td>\n<td id=\"S3.T1.st4.2.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st4.2.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">65.83%</td>\n</tr>\n<tr id=\"S3.T1.st4.2.5.5\" class=\"ltx_tr\">\n<td id=\"S3.T1.st4.2.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">50</td>\n<td id=\"S3.T1.st4.2.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st4.2.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">66.46%</td>\n</tr>\n<tr id=\"S3.T1.st4.2.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.st4.2.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">50</td>\n<td id=\"S3.T1.st4.2.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st4.2.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">65.65%</td>\n</tr>\n<tr id=\"S3.T1.st4.2.7.7\" class=\"ltx_tr\">\n<td id=\"S3.T1.st4.2.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">50</td>\n<td id=\"S3.T1.st4.2.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st4.2.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">65.76%</td>\n</tr>\n<tr id=\"S3.T1.st4.2.8.8\" class=\"ltx_tr\">\n<td id=\"S3.T1.st4.2.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">75</td>\n<td id=\"S3.T1.st4.2.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st4.2.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">65.61%</td>\n</tr>\n<tr id=\"S3.T1.st4.2.9.9\" class=\"ltx_tr\">\n<td id=\"S3.T1.st4.2.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">75</td>\n<td id=\"S3.T1.st4.2.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st4.2.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\">* 66.86% *</td>\n</tr>\n<tr id=\"S3.T1.st4.2.10.10\" class=\"ltx_tr\">\n<td id=\"S3.T1.st4.2.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">75</td>\n<td id=\"S3.T1.st4.2.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st4.2.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\">65.51%</td>\n</tr>\n<tr id=\"S3.T1.st4.2.11.11\" class=\"ltx_tr\">\n<td id=\"S3.T1.st4.2.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" colspan=\"2\">Non-Federated</td>\n<td id=\"S3.T1.st4.2.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">65.88%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We propose a novel method for distributing the training step of each individual device (prior to the federated step) to another device (node) without losing the privacy guarantee of federated learning; a reconstructable form of the training data never leaves the device. This would allow hospitals to harvest spare computing power, e.g. from a receptionist’s PC.",
            "The criticism stems from the high computational cost that provide no actual benefit, other than to make it infeasible for a malicious node to pervert the system. However, in an IoT system this is actually beneficial over PoS; for example, with hundreds of mining devices, the problem can be split across them, much like mining pools.\nMoreover, since the blockchain is being utilized as a trust mechanism for federated learning, the mining target difficulty can remain lower, reducing the computational cost and increasing the rate at which blocks are added to the chain; this results in lower powered devices having enough computing resources to generate hashes competitively whilst still providing the same protection. We therefore decided on adding a block approximately every 1.5 minutes; this is long enough for multiple local updates, from different sources, to be added to the block, prior to the block being added to the chain, without being so long that either the global update is outdated or a local device that misses the update will grow stale. PoS would not be as suitable since it relies too heavily on transactions, doesn’t include mining, would give too much power to larger institutions and it promotes coin hording which negates the bonus benefit of blockchain, rewards: This is what incentivizes hospitals to utilise their spare computing power.",
            "To test our system111We will release the code used to generate these results in a python notebook on GitHub once this paper is accepted for publication. we used TensorFlow to build a simple model, comprising two convolutional and max pooling layers, a final convolutional layer and two dense layers, to classify the CIFAR-10 dataset. Using both standard and federated training paradigms we trained the model five times using 10%, 25%, 50%, 75% and 100% of the training data; in the federated case the data was shared equally amongst all participating models, such that no two models saw the same data points, as would be the case in a live system (especially when using image data as the input). For each subset of the data, the model was trained for 150 epochs whilst additionally measuring the affect of altering the number of epochs each participating member trained for before the federated update and the number of participates to the federated scheme as shown in Table I. The resulting accuracies in each sub-table are all within a small range showing that federated learning produces similar results to the standard method but with the benefit of being applicable to distribution and working on different (albeit similar) datasets with no shared datapoints. Furthermore, when training using federated updates, with each “‘local training”’ round being performed sequentially, the training process runs quicker compared with the standard method due to each participant operating on smaller subsets of the data, allowing for optimisations such as better caching; this is especially apparent on smaller devices."
        ]
    },
    "S3.T1.st5": {
        "caption": "(e) Trained with 100% of the training data",
        "table": "<table id=\"S3.T1.st5.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.st5.2.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.st5.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Update</td>\n<td id=\"S3.T1.st5.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">#Models</td>\n<td id=\"S3.T1.st5.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Accuracy</td>\n</tr>\n<tr id=\"S3.T1.st5.2.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.st5.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">25</td>\n<td id=\"S3.T1.st5.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st5.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">68.09%</td>\n</tr>\n<tr id=\"S3.T1.st5.2.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.st5.2.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">25</td>\n<td id=\"S3.T1.st5.2.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st5.2.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">68.75%</td>\n</tr>\n<tr id=\"S3.T1.st5.2.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.st5.2.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">25</td>\n<td id=\"S3.T1.st5.2.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st5.2.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">68.85%</td>\n</tr>\n<tr id=\"S3.T1.st5.2.5.5\" class=\"ltx_tr\">\n<td id=\"S3.T1.st5.2.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">50</td>\n<td id=\"S3.T1.st5.2.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st5.2.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">* 69.23% *</td>\n</tr>\n<tr id=\"S3.T1.st5.2.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.st5.2.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">50</td>\n<td id=\"S3.T1.st5.2.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st5.2.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">68.12%</td>\n</tr>\n<tr id=\"S3.T1.st5.2.7.7\" class=\"ltx_tr\">\n<td id=\"S3.T1.st5.2.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">50</td>\n<td id=\"S3.T1.st5.2.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st5.2.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">68.11%</td>\n</tr>\n<tr id=\"S3.T1.st5.2.8.8\" class=\"ltx_tr\">\n<td id=\"S3.T1.st5.2.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">75</td>\n<td id=\"S3.T1.st5.2.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S3.T1.st5.2.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">68.56%</td>\n</tr>\n<tr id=\"S3.T1.st5.2.9.9\" class=\"ltx_tr\">\n<td id=\"S3.T1.st5.2.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">75</td>\n<td id=\"S3.T1.st5.2.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"S3.T1.st5.2.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\">68.09%</td>\n</tr>\n<tr id=\"S3.T1.st5.2.10.10\" class=\"ltx_tr\">\n<td id=\"S3.T1.st5.2.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">75</td>\n<td id=\"S3.T1.st5.2.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n<td id=\"S3.T1.st5.2.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\">68.07%</td>\n</tr>\n<tr id=\"S3.T1.st5.2.11.11\" class=\"ltx_tr\">\n<td id=\"S3.T1.st5.2.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" colspan=\"2\">Non-Federated</td>\n<td id=\"S3.T1.st5.2.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">67.64%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We propose a novel method for distributing the training step of each individual device (prior to the federated step) to another device (node) without losing the privacy guarantee of federated learning; a reconstructable form of the training data never leaves the device. This would allow hospitals to harvest spare computing power, e.g. from a receptionist’s PC.",
            "The criticism stems from the high computational cost that provide no actual benefit, other than to make it infeasible for a malicious node to pervert the system. However, in an IoT system this is actually beneficial over PoS; for example, with hundreds of mining devices, the problem can be split across them, much like mining pools.\nMoreover, since the blockchain is being utilized as a trust mechanism for federated learning, the mining target difficulty can remain lower, reducing the computational cost and increasing the rate at which blocks are added to the chain; this results in lower powered devices having enough computing resources to generate hashes competitively whilst still providing the same protection. We therefore decided on adding a block approximately every 1.5 minutes; this is long enough for multiple local updates, from different sources, to be added to the block, prior to the block being added to the chain, without being so long that either the global update is outdated or a local device that misses the update will grow stale. PoS would not be as suitable since it relies too heavily on transactions, doesn’t include mining, would give too much power to larger institutions and it promotes coin hording which negates the bonus benefit of blockchain, rewards: This is what incentivizes hospitals to utilise their spare computing power.",
            "To test our system111We will release the code used to generate these results in a python notebook on GitHub once this paper is accepted for publication. we used TensorFlow to build a simple model, comprising two convolutional and max pooling layers, a final convolutional layer and two dense layers, to classify the CIFAR-10 dataset. Using both standard and federated training paradigms we trained the model five times using 10%, 25%, 50%, 75% and 100% of the training data; in the federated case the data was shared equally amongst all participating models, such that no two models saw the same data points, as would be the case in a live system (especially when using image data as the input). For each subset of the data, the model was trained for 150 epochs whilst additionally measuring the affect of altering the number of epochs each participating member trained for before the federated update and the number of participates to the federated scheme as shown in Table I. The resulting accuracies in each sub-table are all within a small range showing that federated learning produces similar results to the standard method but with the benefit of being applicable to distribution and working on different (albeit similar) datasets with no shared datapoints. Furthermore, when training using federated updates, with each “‘local training”’ round being performed sequentially, the training process runs quicker compared with the standard method due to each participant operating on smaller subsets of the data, allowing for optimisations such as better caching; this is especially apparent on smaller devices."
        ]
    }
}