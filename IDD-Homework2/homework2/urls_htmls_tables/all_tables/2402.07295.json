{
    "PAPER'S NUMBER OF TABLES": 8,
    "S6.T1": {
        "caption": "Table 1. Evaluation tasks and datasets for serverless FedMD and FedDF.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nNo.\nTask\nType\nFedMD\nFedDF\n\nPrivate\nPublic\nPrivate\nPublic (Unlabeled)\n\n1\nCV\nClassification\nMNIST\nEMNIST Letters\nMNIST\nEMNIST Letters\n\n2\nCV\nClassification\nCIFAR100 (Subset)\nCIFAR10\nCIFAR10\nCIFAR100\n\n3\nNLP\nCharacter Prediction\nShakespeare\nNietzsche\nShakespeare\nNietzsche\n\n\n",
        "references": [
            "In our experiments, we evaluate the implemented serverless KD algorithms (¬ß5) for a variety of tasks. Table¬†1 shows the different public/private datasets for the two algorithms.",
            "Figure¬†8(a) visualizes the private client training data distributions for the MNIST dataset for 100100100 clients and different values of Œ±ùõº\\alpha. A value of Œ±=100ùõº100\\alpha=100 represents uniform data distribution, while a value of Œ±=0.1ùõº0.1\\alpha=0.1 represents an extreme non-IID scenario. For task one (Table¬†1), we use the same public and private data distributions for FedMD and FedDF. However, in FedDF, only the image features and no labels are utilized for the distillation process. Each client model gets evaluated on the complete MNIST test dataset comprising 100001000010000 testing images (¬ß6.1). In task two for FedMD, we use a subset of six classes from the CIFAR-100 dataset as in¬†(Li and Wang, 2019) for the main learning task. Figure¬†8(b) shows the private client data distribution in this scenario for various values of Œ±ùõº\\alpha. The complete CIFAR-10 dataset is employed as the public distillation dataset for this task (¬ß2.2). During testing, each client‚Äôs performance is evaluated on the complete global CIFAR-100 test dataset subsetted for the six classes. On the other hand, for FedDF in task two, we use CIFAR-10 as the private dataset and CIFAR-100 as the public dataset. The private training data distribution for FedDF with CIFAR-10 is shown in Figure¬†8(c). For evaluating individual client models, we use the complete CIFAR-10 test dataset comprising 100001000010000 images. Similar to task one, we use the same public and private dataset distributions for the two algorithms in task three, as shown in Table¬†1. For the non-IID scenario, we use the pre-provided non-IID partitions for the Shakespeare dataset from the LEAF FL benchmark suite¬†(Caldas et¬†al., 2018). We don‚Äôt use the Dirichlet distribution for creating non-IID partitions for Shakespeare since that is only suitable for classification tasks. Note that for all tasks, the public dataset is uniformly distributed.",
            "In our experiments, we use multiple model architectures distributed among 100 participating clients based on the machine learning task (Table¬†1). For tasks one and two, we utilize 2-layer and 3-layer convolutional neural networks (CNNs). These model architectures are unevenly distributed among the clients to simulate real-world scenarios, as shown in Tables¬†2 and¬†3. Each convolution filter layer is followed by batch normalization, ReLU activation, and a dropout of 0.20.20.2. Dropout enables regularization and prevents client models from overfitting on small amounts of private local data. To ensure a fair comparison between the performance of FedMD and FedDF in the serverless paradigm, we use the same client model distributions for both algorithms. For task three, we utilize LSTM Recurrent Neural Networks with a single layer and a varying number of units, as shown in Table¬†4. Every network takes an input sequence length of 80, followed by an initial embedding size of 8 and then a single LSTM layer. For all tasks, we chose the model architectures that have been used previously in this domain¬†(Li and Wang, 2019; Elzohairy et¬†al., 2022; Lin et¬†al., 2020; Chadha et¬†al., 2020). Due to space limitations, we omit the specific hyperparameters used for the two algorithms but will describe them in detail in our code repository."
        ]
    },
    "S6.T2": {
        "caption": "Table 2. Different client CNN models for FedMD/FedDF in task one (Table¬†1).",
        "table": "",
        "footnotes": "\n\n\n\n\n\nModel ID\n#Clients\n#Layer 1 Conv. Filters\n#Layer 2 Conv. Filters\n#Layer 3 Conv. Filters\n#Trainable Parameters\n\n0\n10\n128\n256\n-\n729,856\n\n1\n30\n128\n512\n-\n1,458,176\n\n2\n20\n64\n128\n128\n193,280\n\n3\n20\n64\n128\n256\n352,640\n\n4\n10\n128\n128\n128\n226,816\n\n5\n10\n128\n128\n256\n386,176\n\n\n",
        "references": [
            [
                "One of the major challenges in FL is the non-independent and identical (non-IID) data distribution among the participating clients¬†",
                "(Hsieh et¬†al",
                ".",
                ", ",
                "2020",
                ")",
                " (¬ß",
                "1",
                "). To analyze the behavior and robustness of the two serverless KD strategies toward different degrees of data heterogeneity, we use the Dirichlet distribution as in¬†",
                "(Hsu et¬†al",
                ".",
                ", ",
                "2019",
                ")",
                " to create disjoint non-IID client training data partitions. A parameter ",
                "Œ±",
                "ùõº",
                "\\alpha",
                " controls the degree of non-IID data distribution, where a smaller ",
                "Œ±",
                "ùõº",
                "\\alpha",
                " value increases the probability of clients holding training samples from only one class and vice-versa.",
                "Figure¬†",
                "8(a)",
                " visualizes the private client training data distributions for the MNIST dataset for ",
                "100",
                "100",
                "100",
                " clients and different values of ",
                "Œ±",
                "ùõº",
                "\\alpha",
                ". A value of ",
                "Œ±",
                "=",
                "100",
                "ùõº",
                "100",
                "\\alpha=100",
                " represents uniform data distribution, while a value of ",
                "Œ±",
                "=",
                "0.1",
                "ùõº",
                "0.1",
                "\\alpha=0.1",
                " represents an extreme non-IID scenario. For task one (Table¬†",
                "1",
                "), we use the same public and private data distributions for ",
                "FedMD",
                " and ",
                "FedDF",
                ". However, in ",
                "FedDF",
                ", only the image features and no labels are utilized for the distillation process. Each client model gets evaluated on the complete MNIST test dataset comprising ",
                "10000",
                "10000",
                "10000",
                " testing images (¬ß",
                "6.1",
                "). In task two for ",
                "FedMD",
                ", we use a subset of six classes from the CIFAR-100 dataset as in¬†",
                "(Li and Wang, ",
                "2019",
                ")",
                " for the main learning task. Figure¬†",
                "8(b)",
                " shows the private client data distribution in this scenario for various values of ",
                "Œ±",
                "ùõº",
                "\\alpha",
                ". The complete CIFAR-10 dataset is employed as the public distillation dataset for this task (¬ß",
                "2.2",
                "). During testing, each client‚Äôs performance is evaluated on the complete global CIFAR-100 test dataset subsetted for the six classes. On the other hand, for ",
                "FedDF",
                " in task two, we use CIFAR-10 as the private dataset and CIFAR-100 as the public dataset. The private training data distribution for ",
                "FedDF",
                " with CIFAR-10 is shown in Figure¬†",
                "8(c)",
                ". For evaluating individual client models, we use the complete CIFAR-10 test dataset comprising ",
                "10000",
                "10000",
                "10000",
                " images. Similar to task one, we use the same public and private dataset distributions for the two algorithms in task three, as shown in Table¬†",
                "1",
                ". For the non-IID scenario, we use the pre-provided non-IID partitions for the Shakespeare dataset from the ",
                "LEAF",
                " FL benchmark suite¬†",
                "(Caldas et¬†al",
                ".",
                ", ",
                "2018",
                ")",
                ". We don‚Äôt use the Dirichlet distribution for creating non-IID partitions for Shakespeare since that is only suitable for classification tasks. Note that for all tasks, the public dataset is uniformly distributed."
            ]
        ]
    },
    "S6.T3": {
        "caption": "Table 3. Different client CNN models for FedMD/FedDF in task two (Table¬†1).",
        "table": "",
        "footnotes": "\n\n\n\n\n\nModel ID\n#Clients\n#Layer 1 Conv. Filters\n#Layer 2 Conv. Filters\n#Layer 3 Conv. Filters\n#Trainable Parameters\n\n0\n10\n128\n256\n-\n729,856\n\n1\n10\n64\n128\n192\n274,112\n\n2\n30\n64\n64\n128\n104,128\n\n3\n30\n64\n128\n256\n352,640\n\n4\n20\n128\n128\n128\n226,816\n\n\n",
        "references": [
            [
                "One of the major challenges in FL is the non-independent and identical (non-IID) data distribution among the participating clients¬†",
                "(Hsieh et¬†al",
                ".",
                ", ",
                "2020",
                ")",
                " (¬ß",
                "1",
                "). To analyze the behavior and robustness of the two serverless KD strategies toward different degrees of data heterogeneity, we use the Dirichlet distribution as in¬†",
                "(Hsu et¬†al",
                ".",
                ", ",
                "2019",
                ")",
                " to create disjoint non-IID client training data partitions. A parameter ",
                "Œ±",
                "ùõº",
                "\\alpha",
                " controls the degree of non-IID data distribution, where a smaller ",
                "Œ±",
                "ùõº",
                "\\alpha",
                " value increases the probability of clients holding training samples from only one class and vice-versa.",
                "Figure¬†",
                "8(a)",
                " visualizes the private client training data distributions for the MNIST dataset for ",
                "100",
                "100",
                "100",
                " clients and different values of ",
                "Œ±",
                "ùõº",
                "\\alpha",
                ". A value of ",
                "Œ±",
                "=",
                "100",
                "ùõº",
                "100",
                "\\alpha=100",
                " represents uniform data distribution, while a value of ",
                "Œ±",
                "=",
                "0.1",
                "ùõº",
                "0.1",
                "\\alpha=0.1",
                " represents an extreme non-IID scenario. For task one (Table¬†",
                "1",
                "), we use the same public and private data distributions for ",
                "FedMD",
                " and ",
                "FedDF",
                ". However, in ",
                "FedDF",
                ", only the image features and no labels are utilized for the distillation process. Each client model gets evaluated on the complete MNIST test dataset comprising ",
                "10000",
                "10000",
                "10000",
                " testing images (¬ß",
                "6.1",
                "). In task two for ",
                "FedMD",
                ", we use a subset of six classes from the CIFAR-100 dataset as in¬†",
                "(Li and Wang, ",
                "2019",
                ")",
                " for the main learning task. Figure¬†",
                "8(b)",
                " shows the private client data distribution in this scenario for various values of ",
                "Œ±",
                "ùõº",
                "\\alpha",
                ". The complete CIFAR-10 dataset is employed as the public distillation dataset for this task (¬ß",
                "2.2",
                "). During testing, each client‚Äôs performance is evaluated on the complete global CIFAR-100 test dataset subsetted for the six classes. On the other hand, for ",
                "FedDF",
                " in task two, we use CIFAR-10 as the private dataset and CIFAR-100 as the public dataset. The private training data distribution for ",
                "FedDF",
                " with CIFAR-10 is shown in Figure¬†",
                "8(c)",
                ". For evaluating individual client models, we use the complete CIFAR-10 test dataset comprising ",
                "10000",
                "10000",
                "10000",
                " images. Similar to task one, we use the same public and private dataset distributions for the two algorithms in task three, as shown in Table¬†",
                "1",
                ". For the non-IID scenario, we use the pre-provided non-IID partitions for the Shakespeare dataset from the ",
                "LEAF",
                " FL benchmark suite¬†",
                "(Caldas et¬†al",
                ".",
                ", ",
                "2018",
                ")",
                ". We don‚Äôt use the Dirichlet distribution for creating non-IID partitions for Shakespeare since that is only suitable for classification tasks. Note that for all tasks, the public dataset is uniformly distributed."
            ]
        ]
    },
    "S6.T4": {
        "caption": "Table 4. Different client LSTM models for FedMD/FedDF in task three (Table¬†1).",
        "table": "",
        "footnotes": "\n\n\n\n\n\nModel ID\n#Clients\n#Units\nEmbedding Dim\n#Trainable Parameters\n\n\n\n0\n60\n128\n8\n81,378\n\n1\n10\n64\n8\n24,674\n\n2\n30\n256\n8\n293,090\n\n\n",
        "references": [
            "In our experiments, we use multiple model architectures distributed among 100 participating clients based on the machine learning task (Table¬†1). For tasks one and two, we utilize 2-layer and 3-layer convolutional neural networks (CNNs). These model architectures are unevenly distributed among the clients to simulate real-world scenarios, as shown in Tables¬†2 and¬†3. Each convolution filter layer is followed by batch normalization, ReLU activation, and a dropout of 0.20.20.2. Dropout enables regularization and prevents client models from overfitting on small amounts of private local data. To ensure a fair comparison between the performance of FedMD and FedDF in the serverless paradigm, we use the same client model distributions for both algorithms. For task three, we utilize LSTM Recurrent Neural Networks with a single layer and a varying number of units, as shown in Table¬†4. Every network takes an input sequence length of 80, followed by an initial embedding size of 8 and then a single LSTM layer. For all tasks, we chose the model architectures that have been used previously in this domain¬†(Li and Wang, 2019; Elzohairy et¬†al., 2022; Lin et¬†al., 2020; Chadha et¬†al., 2020). Due to space limitations, we omit the specific hyperparameters used for the two algorithms but will describe them in detail in our code repository."
        ]
    },
    "S7.T5": {
        "caption": "Table 5. Comparing total execution time and cost for serverless FedMD across the different datasets.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nDataset\nMetric\nAggregate\nCommunicate\nRevisit\n\n\n\nTransfer Learning\n\n(Private)\n\nDigest\nOverall\n\nMNIST\nDuration (min)\n55.2\n378.4\n867\n42.5\n1329.8\n2672.9\n\nCost (USD)\n0.37\n1.32\n3.02\n0.15\n4.63\n9.49\n\nCIFAR\nDuration (min)\n47.7\n579.4\n111.6\n6.5\n746.4\n1491.6\n\nCost (USD)\n0.32\n2.02\n0.39\n0.02\n2.6\n5.35\n\nShakespeare\nDuration (min)\n64.5\n300.8\n5945.2\n122.7\n656.3\n7089.5\n\nCost (USD)\n0.43\n1.05\n20.69\n0.43\n2.28\n24.88\n\n\n",
        "references": [
            [
                "Analyzing FL systems involves considering important factors such as the time and the cost required to complete different training tasks. This is particularly relevant in our case, as we utilize FaaS, in which users are only billed for the execution time of functions¬†",
                "(Castro et¬†al",
                ".",
                ", ",
                "2019",
                ")",
                ". To present summarized results, we aggregate the timings and costs across all clients for the different levels of data heterogeneity. For computing training costs, we use the cost computation model¬†",
                "(Cloud, ",
                "2022",
                ")",
                " used by Google to estimate the cost for each function based on the number of invocations, allocated memory, and execution duration (¬ß",
                "6.4",
                ").",
                "To offer detailed insights into the serverless KD training process, Figure¬†",
                "10",
                " shows the timings of the individual training steps and the collaborative round durations for the two algorithms. For relevance, we omit the timings for the initial one-time pre-training process in ",
                "FedMD",
                " (¬ß",
                "5.1",
                "). For the MNIST dataset with ",
                "FedMD",
                ", we observe that each collaborative training round takes between ",
                "360",
                "360",
                "360",
                " to ",
                "390",
                "390",
                "390",
                " seconds, as shown in Figure¬†",
                "10(a)",
                ". A significant portion of this time (",
                "40",
                "40",
                "40",
                "%) is spent within the single aggregator function for aggregating prediction logits from all ",
                "100",
                "100",
                "100",
                " participating clients. The digest and revisit steps take a comparable amount of time, while the communicate step is the fastest as it involves only a forward pass inference on the public dataset by all clients (¬ß",
                "5.1",
                "). We observe relatively shorter round durations for the CIFAR dataset with ",
                "FedMD",
                " compared to MNIST, as shown in Figure¬†",
                "10(b)",
                ". This can be attributed to the shorter revisit step due to the smaller private client dataset as described in ¬ß",
                "6.2",
                ". For the Shakespeare dataset with ",
                "FedMD",
                ", a majority of the time (",
                "90",
                "90",
                "90",
                "%) is spent in the revisit step, as shown in Figure¬†",
                "10(c)",
                ". This can be attributed to the significantly large number of epochs required by the LSTM text models for this training step as compared to the CNN models for image tasks (¬ß",
                "6.3",
                "). Figures¬†",
                "10(d)",
                ",¬†",
                "10(e)",
                ", and¬†",
                "10(f)",
                " present the total collaborative round duration along with the timings for private client training and ensemble distillation for the different datasets with ",
                "FedDF",
                ". The highlighted area in these plots represents the variability observed for multiple runs across different data heterogeneity levels (¬ß",
                "6.2",
                "). For the MNIST and CIFAR datasets, we observe that the majority of the round duration (",
                ">",
                ">",
                "90",
                "90",
                "90",
                "%) is spent in the ensemble distillation process that occurs in the aggregator functions for each unique model architecture (¬ß",
                "5.2",
                "). On the other hand, for the Shakespeare dataset, we observe similar durations for the ensemble distillation and private client training process. This can again be attributed to the higher number of local epochs required in private client training for LSTM networks.",
                "Tables¬†",
                "5",
                " and¬†",
                "6",
                " present the total execution times and costs for the serverless implementations of the two algorithms. For the MNIST and CIFAR datasets with ",
                "FedMD",
                ", we observe that most of the total costs are due to the digest step, which is executed on each client for every round. For the Shakespeare dataset, we observe a significant increase in costs primarily driven by the longer training duration in the revisit step. In contrast to ",
                "FedMD",
                ", we observe comparatively lower costs for ",
                "FedDF",
                ". This is because we only select a fraction of clients, i.e., ten, to participate in each training round using our intelligent selection algorithm (¬ß",
                "6.4",
                ",¬ß",
                "5.2",
                "). To summarize, ",
                "FedDF",
                " demonstrates cost savings of ",
                "34",
                "34",
                "34",
                "% and ",
                "82.7",
                "82.7",
                "82.7",
                "% compared to ",
                "FedMD",
                " for the MNIST and Shakespeare datasets, respectively, while incurring approximately 16% higher costs for the CIFAR dataset.\nThe higher costs associated with the CIFAR dataset in ",
                "FedDF",
                " can be attributed to the utilization of the entire CIFAR-100 dataset for the ensemble distillation process leading to a higher number of local knowledge distillation steps in every communication round, in contrast to the use of a subset in ",
                "FedMD",
                " (¬ß",
                "6.2",
                ")."
            ]
        ]
    },
    "S7.T6": {
        "caption": "Table 6. Comparing total execution time and cost for serverless FedDF across the different datasets.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nDataset\nMetric\nAggregators\nClients\nOverall\n\n\n\nMNIST\nDuration (min)\n152\n56.4\n208.4\n\nCost (USD)\n6.07\n0.2\n6.27\n\nCIFAR\nDuration (min)\n163.87\n213.55\n377.42\n\nCost (USD)\n5.46\n0.74\n6.2\n\nShakespeare\nDuration (min)\n134.4\n466.9\n601.3\n\nCost (USD)\n2.68\n1.62\n4.3\n\n\n",
        "references": [
            [
                "Analyzing FL systems involves considering important factors such as the time and the cost required to complete different training tasks. This is particularly relevant in our case, as we utilize FaaS, in which users are only billed for the execution time of functions¬†",
                "(Castro et¬†al",
                ".",
                ", ",
                "2019",
                ")",
                ". To present summarized results, we aggregate the timings and costs across all clients for the different levels of data heterogeneity. For computing training costs, we use the cost computation model¬†",
                "(Cloud, ",
                "2022",
                ")",
                " used by Google to estimate the cost for each function based on the number of invocations, allocated memory, and execution duration (¬ß",
                "6.4",
                ").",
                "To offer detailed insights into the serverless KD training process, Figure¬†",
                "10",
                " shows the timings of the individual training steps and the collaborative round durations for the two algorithms. For relevance, we omit the timings for the initial one-time pre-training process in ",
                "FedMD",
                " (¬ß",
                "5.1",
                "). For the MNIST dataset with ",
                "FedMD",
                ", we observe that each collaborative training round takes between ",
                "360",
                "360",
                "360",
                " to ",
                "390",
                "390",
                "390",
                " seconds, as shown in Figure¬†",
                "10(a)",
                ". A significant portion of this time (",
                "40",
                "40",
                "40",
                "%) is spent within the single aggregator function for aggregating prediction logits from all ",
                "100",
                "100",
                "100",
                " participating clients. The digest and revisit steps take a comparable amount of time, while the communicate step is the fastest as it involves only a forward pass inference on the public dataset by all clients (¬ß",
                "5.1",
                "). We observe relatively shorter round durations for the CIFAR dataset with ",
                "FedMD",
                " compared to MNIST, as shown in Figure¬†",
                "10(b)",
                ". This can be attributed to the shorter revisit step due to the smaller private client dataset as described in ¬ß",
                "6.2",
                ". For the Shakespeare dataset with ",
                "FedMD",
                ", a majority of the time (",
                "90",
                "90",
                "90",
                "%) is spent in the revisit step, as shown in Figure¬†",
                "10(c)",
                ". This can be attributed to the significantly large number of epochs required by the LSTM text models for this training step as compared to the CNN models for image tasks (¬ß",
                "6.3",
                "). Figures¬†",
                "10(d)",
                ",¬†",
                "10(e)",
                ", and¬†",
                "10(f)",
                " present the total collaborative round duration along with the timings for private client training and ensemble distillation for the different datasets with ",
                "FedDF",
                ". The highlighted area in these plots represents the variability observed for multiple runs across different data heterogeneity levels (¬ß",
                "6.2",
                "). For the MNIST and CIFAR datasets, we observe that the majority of the round duration (",
                ">",
                ">",
                "90",
                "90",
                "90",
                "%) is spent in the ensemble distillation process that occurs in the aggregator functions for each unique model architecture (¬ß",
                "5.2",
                "). On the other hand, for the Shakespeare dataset, we observe similar durations for the ensemble distillation and private client training process. This can again be attributed to the higher number of local epochs required in private client training for LSTM networks.",
                "Tables¬†",
                "5",
                " and¬†",
                "6",
                " present the total execution times and costs for the serverless implementations of the two algorithms. For the MNIST and CIFAR datasets with ",
                "FedMD",
                ", we observe that most of the total costs are due to the digest step, which is executed on each client for every round. For the Shakespeare dataset, we observe a significant increase in costs primarily driven by the longer training duration in the revisit step. In contrast to ",
                "FedMD",
                ", we observe comparatively lower costs for ",
                "FedDF",
                ". This is because we only select a fraction of clients, i.e., ten, to participate in each training round using our intelligent selection algorithm (¬ß",
                "6.4",
                ",¬ß",
                "5.2",
                "). To summarize, ",
                "FedDF",
                " demonstrates cost savings of ",
                "34",
                "34",
                "34",
                "% and ",
                "82.7",
                "82.7",
                "82.7",
                "% compared to ",
                "FedMD",
                " for the MNIST and Shakespeare datasets, respectively, while incurring approximately 16% higher costs for the CIFAR dataset.\nThe higher costs associated with the CIFAR dataset in ",
                "FedDF",
                " can be attributed to the utilization of the entire CIFAR-100 dataset for the ensemble distillation process leading to a higher number of local knowledge distillation steps in every communication round, in contrast to the use of a subset in ",
                "FedMD",
                " (¬ß",
                "6.2",
                ")."
            ]
        ]
    },
    "S7.T7": {
        "caption": "Table 7. Comparing maximum Top-1 accuracy for FedMD and FedDF across all datasets and heterogeneous client model architectures.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nModel ID\nMaximum Top-1 accuracy (%)\n\nMNIST Œ±=1ùõº1\\alpha=1\nCIFAR Œ±=1ùõº1\\alpha=1\nShakespeare (non-IID)\n\nFedMD\nFedDF\nFedMD\nFedDF\nFedMD\nFedDF\n\n0\n0.86\n0.90\n0.40\n0.52\n0.33\n0.43\n\n1\n0.86\n0.93\n0.58\n0.52\n0.38\n0.37\n\n2\n0.90\n0.96\n0.54\n0.55\n0.34\n0.40\n\n3\n0.89\n0.96\n0.51\n0.55\n-\n-\n\n4\n0.92\n0.93\n0.51\n0.56\n-\n-\n\n5\n0.91\n0.96\n-\n-\n-\n-\n\n\n",
        "references": [
            "Table¬†7 shows the maximum Top-1 model accuracy for the two algorithms across all datasets and heterogeneous client model architectures. For the MNIST and CIFAR datasets, we present results with Œ±=1ùõº1\\alpha=1, while for the Shakespeare dataset, we present results for the non-IID data partition. We chose Œ±=1ùõº1\\alpha=1 since it represents a standard non-IID scenario and enables us to compare the robustness of the algorithms toward data heterogeneity. The model architecture level comparison is fair since we use the same architectures for both algorithms. For MNIST, FedDF exhibits higher accuracy levels across all model types compared to FedMD, with an average performance improvement of 555% across the six unique model architectures. Similarly, for the CIFAR dataset, FedDF generally outperforms FedMD, except for model 1, where FedMD exhibits better performance. However, on average, across the five unique model architectures, FedDF leads to 3.23.23.2% better accuracy. Finally, for Shakespeare, FedDF consistently outperforms FedMD by an average of 555% across the three unique model architectures."
        ]
    },
    "S7.T8": {
        "caption": "Table 8. Summary of speedups obtained with our extensions to FedLess (¬ß4.2) for the different datasets.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nOptimizations\nMNIST\nCIFAR\nShakespeare\n\n\n\nTransfer Learning with Ray\n3.2x\n3.7x\n3.6x\n\nParallel Ensemble Distillation\n1.83x\n1.8x\n1.67x\n\n\n",
        "references": [
            "Table¬†8 presents summarized results for speedups obtained with our optimizations compared to the original sequential implementation of the two algorithms (¬ß4.2,¬ß2.2,¬ß2.3). To ensure a fair comparison, we execute the original algorithms after migrating them to the serverless paradigm. For the initial transfer learning process in FedMD (¬ß5.1), we obtain an average speedup of 3.53.53.5x across all datasets with our implementation using Ray. On the other hand, for the ensemble distillation process using multiple aggregators in FedDF (¬ß5.2), we observe an average speedup of 1.761.761.76x across all datasets."
        ]
    }
}