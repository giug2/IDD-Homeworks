{
    "id_table_1": {
        "caption": "TABLE I:  Azerbaijani laws in our dataset.",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "Topic embeddings are supposed to be a vector of the same size as document embeddings. If we are using a neural network like BERT to embed the text, we usually cannot feed the entire topic into the model. We can bypass this by taking an element-wise average of all document embeddings for that topic. If we are using a statistical method like TF-IDF, we can run it on the entire text regardless of the text length. However, we expect both the original document embeddings and the topic embeddings to use the same embedding method. Figure  1  visualizes the process of obtaining topic embeddings from documents."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  Performance of original and topic-based clustering. DBI: DaviesBouldin index, CHI: CalinskiHarabasz index.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "To visualize the embeddings of different methods, the tSNE algorithm has been used to reduce the number of dimensions to 2  [ 14 ] . You can see the results in Figure  2a  and  2b ."
        ]
    },
    "global_footnotes": []
}