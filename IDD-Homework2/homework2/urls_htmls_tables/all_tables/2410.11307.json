{
    "id_table_1": {
        "caption": "Table 1 :  Image AUROC for Few Shot Brain Tumor Detection",
        "table": "S4.T1.4",
        "footnotes": [],
        "references": [
            "It is worth noting that previous studies typically employ over 15 samples for training. In contrast, CONSULT only leverages 2 to 8 shots, the experimental results are detailed in Section  4.1 .",
            "Because most brain tumors are convex, circular, and irregularly shaped, it is essential to consider all of these three criteria when attempting to simulate pseudo-defective images. Therefore, we randomly select an image from  D G subscript D G \\mathcal{D}_{G} caligraphic_D start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT  to generate the pseudo-defective images by inpainting an elliptic discrepancy in the brain image. We denote the pseudo anomalous images set as  D B subscript D B \\mathcal{D}_{B} caligraphic_D start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT . Inspired by the Bezier Curve properties, we use the Bezier Curve formula to generate these random elliptic defects. First, the rough location of the brain is located by using simple image morphology and thresholding. This is to refine the position of the elliptic defects such that it is not generated outside the brain. Next, we randomly pick several points within the brain and used them as the fixed points in the Bezier Curve. These fixed points are certain to be convex within the convex hull. Finally, we just fill in the convex hull with random Gaussian Noise and inpaint it into the normal image. Our algorithm allows the generation of elliptic defects with different shapes and complexity. By increasing the number of fixed points  N C subscript N C N_{C} italic_N start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT  in the Bezier Curve, we increase the complexity of the generated spot. To introduce some degree of randomness in the Bezier Curve, we can adjust the smoothness of the curve about the fixed points with  E E E italic_E . In this paper, we set  N C = 5 subscript N C 5 N_{C}=5 italic_N start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT = 5  and  E = 0.05 E 0.05 E=0.05 italic_E = 0.05 . Algorithm  1  shows the algorithm to generate these random spots.",
            "CONSULT is able to outperform current state-of-the-art anomaly detection algorithms in a low shot scenario. CONSULT shows consistent improvement when more training images are used. PaDIM and PatchCore are two most common downstream anomaly detection algorithms. We also compare CONSULT with reconstruction based anomaly detection such as f-AnoGAN, GANormaly, and EfficientAD. To train a robust model for f-AnoGAN and GANormaly, we increase the training epoch to  100 100 100 100 . The settings for PaDIM and EfficientAD follow the original implementation  [ defard2021padim ,  batzner2024efficientad ] . The results are shown in Table  1 .",
            "This discrepancy between downstream-based anomaly detection and reconstruction-based approaches catalyzes the development of CONSULT. CONSULT aims to amalgamate the strengths of both downstream and reconstruction approaches. By leveraging unsupervised contrastive learning for backbone fine-tuning, we have demonstrated significant improvements in PatchCore. Specifically, 9.4%, 12.9%, 10.2%, and 6.0% for 2, 4, 6, and 8 shots respectively. The improvement for our solution primarily comes from the pretraining stage. The objective of the pretraining stage respect the way anomaly detection is being done. Therefore, it can extract more compact features compared to ImageNet, improving the robustness of the feature extractor. In section  4.3.1 , we show our pretraining stage proves to be more effective in extracting features for anomaly detection compared to a supervised training method such as classification and segmentation."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Ablation Study: Domain Adaptation",
        "table": "S4.T2.4",
        "footnotes": [],
        "references": [
            "We introduce CONtrastive Self-sUpervised Learning for few-shot Tumor detection (CONSULT), a 2-stage downstream anomaly detection methodology that achieves extreme-few-shot anomaly detection with high precision. A comprehensive overview of CONSULT is in Figure  2 .",
            "Next, the attention module learns to pay more attention to anomalous regions and facilitates normal feature generalizability through contrastive learning. Our first approach uses anchor loss, a popular contrastive loss. Anchor loss perfectly fits with anomaly detection problems by minimizing the distance between normal features and maximizing interclass distance. Equation  2  shows the formula for anchor loss.",
            "Traditional anchor loss, a typical loss function for contrastive learning is unbounded and can result in multiple solutions. Furthermore, choosing the optimal parameters for anchor loss is extremely hard as it will directly affect the training objective. We proposed a novel loss function that is bounded and more robust compared to anchor loss.  Tritanh  loss only has a single solution, and the parameters for  Tritanh  loss have less effect on the training objectives. Besides, the gradient for  Tritanh  loss is steeper compared to anchor loss. The implementation of  Tritanh  loss significantly improves upon anchor loss both in robustness and training stability. Next, we introduce other loss functions and regularizers to aid and stabilize the training process. Self-similarity loss maximizes the similarity of healthy features, effectively reducing the need for large training data. It is also coherent with the way PatchCore evaluates anomalies. However, self-similarity loss is very unstable and will result in catastrophic collapse to a singular point as the training epoch progresses. To counter this problem, we introduce a KoLeo Loss as a regularizer for self-similarity loss. We modify the input for KoLeo Loss to be the final feature output from the backbone. Doing so allows the final feature to be more well spread, opposing the self-similarity loss. In section  4.3.2 , we compare the effectiveness of the proposed loss functions on CONSULT."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Ablation Study: Loss Function",
        "table": "S4.T3.4",
        "footnotes": [],
        "references": [
            "However, anchor loss suffers from slow convergence and has multiple solutions, destabilizing training. Inspired by the hyperbolic tangent function, we propose a novel loss function, termed  Tritanh  loss, that is more stable and has a more robust gradient flow. Equation  3  show the proposed  Tritanh  loss.",
            "As the contrastive loss function minimizes, the attention module iteratively highlights the anomalous regions. Figure  3  shows the feature maps generated in different layers of the feature extractor. We also change the default activation function from ReLU to Leaky ReLU. This is because Leaky ReLu allows small negative value, rather than zero gradient flow when a negative value is computed.",
            "This discrepancy between downstream-based anomaly detection and reconstruction-based approaches catalyzes the development of CONSULT. CONSULT aims to amalgamate the strengths of both downstream and reconstruction approaches. By leveraging unsupervised contrastive learning for backbone fine-tuning, we have demonstrated significant improvements in PatchCore. Specifically, 9.4%, 12.9%, 10.2%, and 6.0% for 2, 4, 6, and 8 shots respectively. The improvement for our solution primarily comes from the pretraining stage. The objective of the pretraining stage respect the way anomaly detection is being done. Therefore, it can extract more compact features compared to ImageNet, improving the robustness of the feature extractor. In section  4.3.1 , we show our pretraining stage proves to be more effective in extracting features for anomaly detection compared to a supervised training method such as classification and segmentation.",
            "Traditional anchor loss, a typical loss function for contrastive learning is unbounded and can result in multiple solutions. Furthermore, choosing the optimal parameters for anchor loss is extremely hard as it will directly affect the training objective. We proposed a novel loss function that is bounded and more robust compared to anchor loss.  Tritanh  loss only has a single solution, and the parameters for  Tritanh  loss have less effect on the training objectives. Besides, the gradient for  Tritanh  loss is steeper compared to anchor loss. The implementation of  Tritanh  loss significantly improves upon anchor loss both in robustness and training stability. Next, we introduce other loss functions and regularizers to aid and stabilize the training process. Self-similarity loss maximizes the similarity of healthy features, effectively reducing the need for large training data. It is also coherent with the way PatchCore evaluates anomalies. However, self-similarity loss is very unstable and will result in catastrophic collapse to a singular point as the training epoch progresses. To counter this problem, we introduce a KoLeo Loss as a regularizer for self-similarity loss. We modify the input for KoLeo Loss to be the final feature output from the backbone. Doing so allows the final feature to be more well spread, opposing the self-similarity loss. In section  4.3.2 , we compare the effectiveness of the proposed loss functions on CONSULT.",
            "The attention module iteratively helps the model \"focus\" on regions that are considered anomalous. As shown in Figure  3 , the feature maps predicted from each layer focus on the brain tumor region of the query image. Next, we change the activation function of the feature extractor from ReLu to Leaky ReLu for better gradient flow. These improvements help the model to extract a more robust feature compared to a standard ResNet backbone. In section  4.3.3 , we compare different feature extractors and the modifications we did to them.",
            "Tritanh  loss is bounded from -1 to 2 with optimal solution reached will reach when  d p  u  l  l  0  subscript d p u l l 0 d_{pull}\\longrightarrow 0 italic_d start_POSTSUBSCRIPT italic_p italic_u italic_l italic_l end_POSTSUBSCRIPT  0  and  d p  u  s  h    subscript d p u s h d_{push}\\longrightarrow\\infty italic_d start_POSTSUBSCRIPT italic_p italic_u italic_s italic_h end_POSTSUBSCRIPT   . This ensures that the computed loss does not destabilize the training process. In contrast to anchor loss which will result in multiple solutions as long as the pushing force is greater than the pulling force by a margin,  Tritanh  loss only has a single unique solution. Multiple minima can make the training process more challenging because the optimization algorithm may get stuck in local minima rather than finding the global minimum. Additionally, due to the exponential gradient flow,  Tritanh  loss gradually slows down as it approaches the minimum, making a near-optimal solution ( d p  u  l  l  0 subscript d p u l l 0 d_{pull}\\approx 0 italic_d start_POSTSUBSCRIPT italic_p italic_u italic_l italic_l end_POSTSUBSCRIPT  0  and  d p  u  s  h   subscript d p u s h d_{push}\\approx\\infty italic_d start_POSTSUBSCRIPT italic_p italic_u italic_s italic_h end_POSTSUBSCRIPT   ) almost equal to the true optimal solution. In practice, achieving the true optimal solution is not necessary; an approximation of it is sufficient for effective performance. Furthermore, the gradient flow when  d p  u  l  l  d p  u  s  h subscript d p u l l subscript d p u s h d_{pull}\\approx d_{push} italic_d start_POSTSUBSCRIPT italic_p italic_u italic_l italic_l end_POSTSUBSCRIPT  italic_d start_POSTSUBSCRIPT italic_p italic_u italic_s italic_h end_POSTSUBSCRIPT  is stepper compared to anchor loss, which only has a constant gradient flow helping the model to converge faster. Table  3  compares the result obtained from different loss functions."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Ablation Study: Model Selection",
        "table": "S4.T4.4",
        "footnotes": [],
        "references": [
            "It is worth noting that previous studies typically employ over 15 samples for training. In contrast, CONSULT only leverages 2 to 8 shots, the experimental results are detailed in Section  4.1 .",
            "Tritanh  loss only has a unique solution where the pulling force,  d pull subscript d pull d_{\\text{pull}} italic_d start_POSTSUBSCRIPT pull end_POSTSUBSCRIPT  approaches zero, and the pushing force,  d push subscript d push d_{\\text{push}} italic_d start_POSTSUBSCRIPT push end_POSTSUBSCRIPT  approaches infinity. Furthermore, we can adjust the gradient of the loss function by changing the scaling factors,   0 subscript  0 \\lambda_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , and   1 subscript  1 \\lambda_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  for  d pull subscript d pull d_{\\text{pull}} italic_d start_POSTSUBSCRIPT pull end_POSTSUBSCRIPT  and  d push subscript d push d_{\\text{push}} italic_d start_POSTSUBSCRIPT push end_POSTSUBSCRIPT  respectively. As   0 >  1 subscript  0 subscript  1 \\lambda_{0}>\\lambda_{1} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT > italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  Tritanh  loss prioritizes the pulling force more than the pushing force. We also add two margins,  m 0 subscript m 0 m_{0} italic_m start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  and  m 1 subscript m 1 m_{1} italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  to regularize the  Tritanh  loss. By changing these two margins, the gradient of the loss function can be scaled more robustly while being bounded within a controlled value. Figure  4  compares different contrastive loss functions and the proposed contrastive loss function.",
            "We use self-similarity loss to maximize the similarity between healthy image features to further enhance the model generalizability on normal features. We compute the self-similarity loss from the anchor image alone using a simple  L  2 L 2 L2 italic_L 2  loss. Self-similarity loss will make the convex hull of the healthy image features to be more compact. The proposed self-similarity loss is shown in Equation  4 .",
            "This discrepancy between downstream-based anomaly detection and reconstruction-based approaches catalyzes the development of CONSULT. CONSULT aims to amalgamate the strengths of both downstream and reconstruction approaches. By leveraging unsupervised contrastive learning for backbone fine-tuning, we have demonstrated significant improvements in PatchCore. Specifically, 9.4%, 12.9%, 10.2%, and 6.0% for 2, 4, 6, and 8 shots respectively. The improvement for our solution primarily comes from the pretraining stage. The objective of the pretraining stage respect the way anomaly detection is being done. Therefore, it can extract more compact features compared to ImageNet, improving the robustness of the feature extractor. In section  4.3.1 , we show our pretraining stage proves to be more effective in extracting features for anomaly detection compared to a supervised training method such as classification and segmentation.",
            "Traditional anchor loss, a typical loss function for contrastive learning is unbounded and can result in multiple solutions. Furthermore, choosing the optimal parameters for anchor loss is extremely hard as it will directly affect the training objective. We proposed a novel loss function that is bounded and more robust compared to anchor loss.  Tritanh  loss only has a single solution, and the parameters for  Tritanh  loss have less effect on the training objectives. Besides, the gradient for  Tritanh  loss is steeper compared to anchor loss. The implementation of  Tritanh  loss significantly improves upon anchor loss both in robustness and training stability. Next, we introduce other loss functions and regularizers to aid and stabilize the training process. Self-similarity loss maximizes the similarity of healthy features, effectively reducing the need for large training data. It is also coherent with the way PatchCore evaluates anomalies. However, self-similarity loss is very unstable and will result in catastrophic collapse to a singular point as the training epoch progresses. To counter this problem, we introduce a KoLeo Loss as a regularizer for self-similarity loss. We modify the input for KoLeo Loss to be the final feature output from the backbone. Doing so allows the final feature to be more well spread, opposing the self-similarity loss. In section  4.3.2 , we compare the effectiveness of the proposed loss functions on CONSULT.",
            "The attention module iteratively helps the model \"focus\" on regions that are considered anomalous. As shown in Figure  3 , the feature maps predicted from each layer focus on the brain tumor region of the query image. Next, we change the activation function of the feature extractor from ReLu to Leaky ReLu for better gradient flow. These improvements help the model to extract a more robust feature compared to a standard ResNet backbone. In section  4.3.3 , we compare different feature extractors and the modifications we did to them."
        ]
    }
}