{
    "id_table_1": {
        "caption": "Table 1:  Performance metrics of our pipeline for various papers.",
        "table": "S5.T1.1",
        "footnotes": [],
        "references": [
            "Figure  1  illustrates the comprehensive flow of the CodeRefine pipeline. 1 1 1 The code is available via https://github.com/AbhijitJowhari/CodeRefine.  The process begins with a user-provided research paper, which is segmented into multiple text chunks ( Text_1, Text_2, ..., Text_n ), each accompanied by a summary. These text chunks are trivially decided as the paragraphs of the paper. That is two pieces of text separated by more than one newline character. The summary of each of the chunks is generated using the Llama3-70B model. The segmentation ensures the content is granular enough for effective analysis.",
            "We test our pipeline on five scientific papers (see Appendix  0.B ) using the TSED similarity score, and the results are tabulated in Table  1  above. Each paper is assigned a unique paper ID for identification. The bibliographic information and links to GitHub repositories providing the ground truth code implementations for each paper ID are provided in the appendix section."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  When the research paper is not included in the dynamic database.",
        "table": "S5.T2.1",
        "footnotes": [],
        "references": [
            "The paper is organized in the following sections. Section  2  reviews the related work in the field. Section  3  describes the metric used for the comparison of code variants. Section  4  introduces the methodology under the CodeRefine framework. Section  5  describes the experiments and results. Section  6  discuss the limitations of the proposed framework, while Section  7  shows how the presented ideas can be useful in another application. Section  8  concludes the work.",
            "The core of the pipeline is the Retrospective Retrieval-Augmented Generation (RRAG) with vector search, detailed in Figure  2 . Queries are formulated by prompting the LLM to seek detailed explanations, implementation specifics, and steps for complex procedures. These queries are converted into task-aware vector embeddings using a transformer-based model.",
            "We use the MixedBreadAIs embed-2d-large-v1 model  [ 11 ]   [ 13 ]  as task-aware vectorizer (Figure  2 ). This model is an advanced embedding model designed to process high-dimensional data and create rich semantic representations. This model leverages a bi-encoder architecture where queries and documents are processed separately and then compared in a shared embedding space. The embed-2d-large-v1 is particularly suited for tasks requiring high semantic accuracy and relevance, such as document retrieval, semantic search, and knowledge-based tasks. Instruction-tuned embeddings enhance the traditional embedding approach by incorporating task-specific instructions directly into the embedding process.",
            "Our findings indicate that GPT-4o is substantially aided by the Retrospective Retrieval-Augmented Generation (RRAG) only when we include the research paper itself along with its references in the dynamic database. For example, for paper 4, the results without including the research paper in the dynamic database are shown in Table  2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Table of Test Papers with BibTeX Citations and Code Links.",
        "table": "Pt0.A2.T3.1",
        "footnotes": [],
        "references": [
            "The paper is organized in the following sections. Section  2  reviews the related work in the field. Section  3  describes the metric used for the comparison of code variants. Section  4  introduces the methodology under the CodeRefine framework. Section  5  describes the experiments and results. Section  6  discuss the limitations of the proposed framework, while Section  7  shows how the presented ideas can be useful in another application. Section  8  concludes the work.",
            "The final code output from the pipeline is expected to be significantly more accurate and relevant compared to the initial intermediate code. This final code is pushed into the TSED metric (see Section  3 ) for comparison with the available ground truth.",
            "As described in Section  3 , the tree distance represents the cumulative cost function, which pertains to the minimum number of deletion, insertion, and renaming operations required to convert the origin tree to the target tree. The weight or penalty associated with each operation is summed to form the cost function. Below, we provide a step-by-step breakdown of penalty weight optimization for TSED.",
            "Table  3  provides the main information about the scientific papers used for test CodeRefine pipeline. Each paper has a corresponding programming realization."
        ]
    },
    "global_footnotes": [
        "The code is available via https://github.com/AbhijitJowhari/CodeRefine.",
        "The code is available via https://github.com/AbhijitJowhari/ANVESHAK."
    ]
}