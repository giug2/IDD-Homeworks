{
    "PAPER'S NUMBER OF TABLES": 4,
    "S3.T2": {
        "caption": "",
        "table": "<table id=\"S3.T2.fig1.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T2.fig1.3.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.fig1.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">Device</th>\n<th id=\"S3.T2.fig1.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">CPU</th>\n<th id=\"S3.T2.fig1.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Memory</th>\n<th id=\"S3.T2.fig1.3.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">GPU</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.fig1.3.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.fig1.3.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Rasp Pi</th>\n<td id=\"S3.T2.fig1.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">A72, 1.8GHz</td>\n<td id=\"S3.T2.fig1.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">8GB</td>\n<td id=\"S3.T2.fig1.3.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">N/A</td>\n</tr>\n<tr id=\"S3.T2.fig1.3.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T2.fig1.3.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">LattePanda</th>\n<td id=\"S3.T2.fig1.3.3.2.2\" class=\"ltx_td ltx_align_center\">M3, 3.4GHz</td>\n<td id=\"S3.T2.fig1.3.3.2.3\" class=\"ltx_td ltx_align_center\">8GB</td>\n<td id=\"S3.T2.fig1.3.3.2.4\" class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr id=\"S3.T2.fig1.3.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T2.fig1.3.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">PC</th>\n<td id=\"S3.T2.fig1.3.4.3.2\" class=\"ltx_td ltx_align_center\">i7, 2.2GHz</td>\n<td id=\"S3.T2.fig1.3.4.3.3\" class=\"ltx_td ltx_align_center\">16GB</td>\n<td id=\"S3.T2.fig1.3.4.3.4\" class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr id=\"S3.T2.fig1.3.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T2.fig1.3.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">PC with GPU</th>\n<td id=\"S3.T2.fig1.3.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">i7, 2.4GHz</td>\n<td id=\"S3.T2.fig1.3.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">32GB</td>\n<td id=\"S3.T2.fig1.3.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">3080Ti</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Diffusion Models. With large-scale training data and model parameters, various text-to-image generative models are now capable of creating high-quality images [36, 9, 40]. Among them, diffusion models [17] have achieved significant success in the image synthesis domain and AI-generated content (AIGC) fields, such as the popular Stable Diffusion [38] and DALL-E2 [37]. A typical diffusion model includes a forward diffusion stage and a reverse diffusion stage [8]. Given a textual prompt, the diffusion model is able to convert Gaussian noise into a text-compliant image through an iterative denoising step. Despite diffusion models showcasing excellent performance, only a few works have leveraged them in FL. [51] and [57] combine diffusion models with FL to mitigate non-IID problems, while [45] applies FL to the training process to address privacy issues.",
            "We motivate this study by presenting the impact of system heterogeneity through a small-scale experiment. We establish a heterogeneous system comprising four types of devices, including three Raspberry Pi 4B units, three LattePanda 2 Alpha 864s [23], two PCs without GPU, and one PC equipped with an NVIDIA 3080Ti GPU. These devices range from embedded systems to PCs with GPUs, effectively presenting the system heterogeneity prevalent in real-world scenarios. The core specifications of each device type are detailed in Table 2. Given the limited resources of the Raspberry Pi, we select a FL training task using the MNIST dataset and a CNN model. This model includes two convolutional layers with max pooling, followed by two fully connected layers. Each device serves as a FL client, with an additional machine serving as the central server for aggregation. We employ FedAvg [32] in this study.",
            "We design four experiments to assess the impact of system heterogeneity. The settings and the wall-clock convergence times are detailed in Table 2. We can observe that when all devices participate in training, the convergence speed is extremely slow, exceeding 2700 seconds. This is attributed to tail latency [25] caused by the existence of Raspberry Pi devices, which delay the process even after other devices have completed their training. The convergence times for the remaining three experiments are reduced to 1428, 210, and 143 seconds, respectively, showing decreases of 47%, 92% and 95% compared to the initial scenario. These results show the idle times of faster devices, highlighting the significant impact of system heterogeneity on FL training efficiency.",
            "To enhance efficiency, FedTSA avoids the long training process that is usually required for diffusion models. We simply develop a prompt pool denoted as 𝒴𝒴\\mathcal{Y} and leverage a pre-trained Stable Diffusion v1-4 Model [38] for inference. By allowing clients to upload prompts to the server’s prompt pool, we have effectively mitigated concerns about data distribution and potential privacy issues. Moreover, before uploading textual prompts, the implementation of differential privacy techniques [42] can be employed, whereby the inclusion of irrelevant text further safeguards the privacy of the system.",
            "Performance Comparison. From the results presented in Table 3, we can observe that FedTSA outperforms other FL baselines across both IID and non-IID settings, highlighting its promising performance and robustness to heterogeneous data. An exception is noted in the IID setting of CIFAR-10 dataset, where FedTSA slightly lags behind the upper bound performance of FedAvg. However, we should notice that the upper bounds for FedAvg and FedProx are based on an idealized scenario where all clients have unpruned models. Given this context, FedTSA still maintains a dominant performance over most upper bounds across both IID and non-IID settings. This superior performance is attributed to FedTSA’s innovative strategy. Contrasting with algorithms that conventionally replace the global model with a local model for round updates, FedTSA fosters inter-client learning through DML. Specifically, FedTSA utilizes the KL divergence loss to learn the average logits from other client models, thereby effectively obviating the need to rely on a singular global model. Consequently, each model not only gains knowledge from its counterparts but also sidesteps potential information loss that may arise from direct replacement. Besides, we can notice that FedProto presents the lowest accuracy, revealing that prototype-based FL methods do not perform as well as methods based on knowledge distillation and local training in the presence of model heterogeneity. This is because simpler models produce lower-quality prototypes, which in turn degrades the quality of the aggregated global prototypes, ultimately leading to poor model training outcomes.",
            "Effect of Different Degrees of Model Heterogeneity. We evaluate the performance of FedTSA across varying degrees of model heterogeneity, comparing it with other model-heterogeneous FL baselines. To introduce model heterogeneity, we continue to use the pruning rate and keep the number of clients to 20. We select CIFAR-10 as the dataset and configure the data distribution to be IID. The specific settings for heterogeneity and the corresponding experimental results are detailed in Table 4.",
            "where Lk​lsubscript𝐿𝑘𝑙L_{kl} is KL divergence loss, Lc​esubscript𝐿𝑐𝑒L_{ce} is cross-entropy (CE) loss, and α𝛼\\alpha is a weighting factor. Figure 5 shows the accuracy differences among these approaches. KL-only converges fastest initially but ultimately underperforms KL+CE in accuracy. While KL+CE is slower and less stable initially, it achieves the highest final accuracy. When the weight of KL divergence loss decreases, for instance, from 1.0 to 0.8, as the figure shows, the performance lies between the KL-only and KL+CE with equal weights. This suggests that combining KL and CE losses enhances global aggregation, but the slight accuracy gain from adding CE loss may not justify the extra computational cost.",
            "Effect of Temperature. The temperature T𝑇T is a hyperparameter in the softmax function to control the sharpness of the predicted probabilities, as Eq. 6 indicates. Higher T𝑇T values create a uniform distribution, while lower values lead to a peaked one. Properly tuning T𝑇T enhances knowledge exchange in DML, as depicted in Table 6. Optimal performance is achieved at T=5𝑇5T=5 for both IID and non-IID settings, with accuracy declining at higher T𝑇T values. Notably, the decline in performance exhibits a more gradual trend in non-IID scenarios compared to IID. This can be explained by the smoothing effect of a higher T𝑇T on the logits output: even if the model lacks confidence for certain classes due to an imbalanced data distribution, the student model still benefits from the teacher’s guidance. Furthermore, the smoothed logits help to prevent the student model from overfitting to classes that have more training data.",
            "Effect of Prompts. We verify the impact of different prompts on the model performance and test a range of prompts from simple to descriptive, as detailed in Table 6. Taking ’bird’ as an example, we observe that prompt quality significantly impacts performance. A single-word prompt results in 81.31%, while more descriptive prompts improve the accuracy. We explore the reason behind it by examining the generated data and find that simple prompts like a single word can produce abstract images, such as hand-drawn or stitching-style images. Since the model is trained with real-world photos, it cannot generate high-quality logits on these images, leading to degraded Stage 2 aggregation and thus impacting the model performance. We also notice that the model accuracy is nearly identical when using the last two prompts, which shows that FedTSA does not require detailed information about the original dataset, offering better privacy than constructing public datasets by studying raw data.",
            "We conduct experiments under Python 3.8.0 and PyTorch 1.31.1. We use a NVIDIA A100 provided by RunPod111https://www.runpod.io/ for computation. Weights&Biases222https://wandb.ai/site is leveraged to track and log the experimental results. Regarding the diffusion model, we choose the Stable Diffusion model v1-4 from Hugging Face333https://huggingface.co/CompVis/stable-diffusion-v1-4."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Average test accuracy (%) on three datasets under different degrees of data heterogeneity.",
        "table": "<table id=\"S5.T3.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.4.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.4.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Dataset</th>\n<th id=\"S5.T3.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">IID/non-IID</th>\n<th id=\"S5.T3.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedAvg</th>\n<th id=\"S5.T3.4.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedProx</th>\n<th id=\"S5.T3.4.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedProto</th>\n<th id=\"S5.T3.4.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedMD</th>\n<th id=\"S5.T3.4.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedDF</th>\n<th id=\"S5.T3.4.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">HeteroFL</th>\n<th id=\"S5.T3.4.1.1.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedTSA</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.4.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.4.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\" rowspan=\"3\"><span id=\"S5.T3.4.2.1.1.1\" class=\"ltx_text\">CIFAR-10</span></th>\n<th id=\"S5.T3.4.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">IID</th>\n<td id=\"S5.T3.4.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">84.23-<span id=\"S5.T3.4.2.1.3.1\" class=\"ltx_text ltx_font_bold\">90.86</span>\n</td>\n<td id=\"S5.T3.4.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.85-86.47</td>\n<td id=\"S5.T3.4.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">59.6</td>\n<td id=\"S5.T3.4.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.26</td>\n<td id=\"S5.T3.4.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.01</td>\n<td id=\"S5.T3.4.2.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.61</td>\n<td id=\"S5.T3.4.2.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">89.28</td>\n</tr>\n<tr id=\"S5.T3.4.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T3.4.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Dir(0.6)</th>\n<td id=\"S5.T3.4.3.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">81.54-83.55</td>\n<td id=\"S5.T3.4.3.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.31-84.26</td>\n<td id=\"S5.T3.4.3.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">49.51</td>\n<td id=\"S5.T3.4.3.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">71.33</td>\n<td id=\"S5.T3.4.3.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">76.98</td>\n<td id=\"S5.T3.4.3.2.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.09</td>\n<td id=\"S5.T3.4.3.2.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T3.4.3.2.8.1\" class=\"ltx_text ltx_font_bold\">86.72</span></td>\n</tr>\n<tr id=\"S5.T3.4.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T3.4.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Dir(0.3)</th>\n<td id=\"S5.T3.4.4.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">77.25-80.91</td>\n<td id=\"S5.T3.4.4.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">80.71-82.39</td>\n<td id=\"S5.T3.4.4.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">41.33</td>\n<td id=\"S5.T3.4.4.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">70.03</td>\n<td id=\"S5.T3.4.4.3.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">75.12</td>\n<td id=\"S5.T3.4.4.3.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.77</td>\n<td id=\"S5.T3.4.4.3.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T3.4.4.3.8.1\" class=\"ltx_text ltx_font_bold\">84.61</span></td>\n</tr>\n<tr id=\"S5.T3.4.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T3.4.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\" rowspan=\"3\"><span id=\"S5.T3.4.5.4.1.1\" class=\"ltx_text\">CIFAR-100</span></th>\n<th id=\"S5.T3.4.5.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">IID</th>\n<td id=\"S5.T3.4.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">47.69-50.73</td>\n<td id=\"S5.T3.4.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">48.27-51.96</td>\n<td id=\"S5.T3.4.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">36.41</td>\n<td id=\"S5.T3.4.5.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">35.24</td>\n<td id=\"S5.T3.4.5.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">39.71</td>\n<td id=\"S5.T3.4.5.4.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">58.92</td>\n<td id=\"S5.T3.4.5.4.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T3.4.5.4.9.1\" class=\"ltx_text ltx_font_bold\">60.56</span></td>\n</tr>\n<tr id=\"S5.T3.4.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T3.4.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Dir(0.6)</th>\n<td id=\"S5.T3.4.6.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">46.94-50.12</td>\n<td id=\"S5.T3.4.6.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">47.95-51.23</td>\n<td id=\"S5.T3.4.6.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">31.07</td>\n<td id=\"S5.T3.4.6.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">30.33</td>\n<td id=\"S5.T3.4.6.5.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">35.98</td>\n<td id=\"S5.T3.4.6.5.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">56.04</td>\n<td id=\"S5.T3.4.6.5.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T3.4.6.5.8.1\" class=\"ltx_text ltx_font_bold\">60.02</span></td>\n</tr>\n<tr id=\"S5.T3.4.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T3.4.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Dir(0.3)</th>\n<td id=\"S5.T3.4.7.6.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">45.28-48.31</td>\n<td id=\"S5.T3.4.7.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">46.24-49.02</td>\n<td id=\"S5.T3.4.7.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">28.46</td>\n<td id=\"S5.T3.4.7.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">29.78</td>\n<td id=\"S5.T3.4.7.6.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">33.25</td>\n<td id=\"S5.T3.4.7.6.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">53.37</td>\n<td id=\"S5.T3.4.7.6.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T3.4.7.6.8.1\" class=\"ltx_text ltx_font_bold\">59.57</span></td>\n</tr>\n<tr id=\"S5.T3.4.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T3.4.8.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\" rowspan=\"3\"><span id=\"S5.T3.4.8.7.1.1\" class=\"ltx_text\">Tiny-ImageNet</span></th>\n<th id=\"S5.T3.4.8.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">IID</th>\n<td id=\"S5.T3.4.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">29.88-31.04</td>\n<td id=\"S5.T3.4.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">30.12-31.87</td>\n<td id=\"S5.T3.4.8.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">18.55</td>\n<td id=\"S5.T3.4.8.7.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">19.23</td>\n<td id=\"S5.T3.4.8.7.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">22.76</td>\n<td id=\"S5.T3.4.8.7.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">34.57</td>\n<td id=\"S5.T3.4.8.7.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T3.4.8.7.9.1\" class=\"ltx_text ltx_font_bold\">36.28</span></td>\n</tr>\n<tr id=\"S5.T3.4.9.8\" class=\"ltx_tr\">\n<th id=\"S5.T3.4.9.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Dir(0.6)</th>\n<td id=\"S5.T3.4.9.8.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">27.31-29.53</td>\n<td id=\"S5.T3.4.9.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">28.05-30.07</td>\n<td id=\"S5.T3.4.9.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">17.84</td>\n<td id=\"S5.T3.4.9.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">18.65</td>\n<td id=\"S5.T3.4.9.8.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">21.14</td>\n<td id=\"S5.T3.4.9.8.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">33.82</td>\n<td id=\"S5.T3.4.9.8.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T3.4.9.8.8.1\" class=\"ltx_text ltx_font_bold\">35.62</span></td>\n</tr>\n<tr id=\"S5.T3.4.10.9\" class=\"ltx_tr\">\n<th id=\"S5.T3.4.10.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Dir(0.3)</th>\n<td id=\"S5.T3.4.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">26.59-27.49</td>\n<td id=\"S5.T3.4.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">27.23-29.38</td>\n<td id=\"S5.T3.4.10.9.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">17.03</td>\n<td id=\"S5.T3.4.10.9.5\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">17.49</td>\n<td id=\"S5.T3.4.10.9.6\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">20.68</td>\n<td id=\"S5.T3.4.10.9.7\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">32.73</td>\n<td id=\"S5.T3.4.10.9.8\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T3.4.10.9.8.1\" class=\"ltx_text ltx_font_bold\">35.07</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Performance Comparison. From the results presented in Table 3, we can observe that FedTSA outperforms other FL baselines across both IID and non-IID settings, highlighting its promising performance and robustness to heterogeneous data. An exception is noted in the IID setting of CIFAR-10 dataset, where FedTSA slightly lags behind the upper bound performance of FedAvg. However, we should notice that the upper bounds for FedAvg and FedProx are based on an idealized scenario where all clients have unpruned models. Given this context, FedTSA still maintains a dominant performance over most upper bounds across both IID and non-IID settings. This superior performance is attributed to FedTSA’s innovative strategy. Contrasting with algorithms that conventionally replace the global model with a local model for round updates, FedTSA fosters inter-client learning through DML. Specifically, FedTSA utilizes the KL divergence loss to learn the average logits from other client models, thereby effectively obviating the need to rely on a singular global model. Consequently, each model not only gains knowledge from its counterparts but also sidesteps potential information loss that may arise from direct replacement. Besides, we can notice that FedProto presents the lowest accuracy, revealing that prototype-based FL methods do not perform as well as methods based on knowledge distillation and local training in the presence of model heterogeneity. This is because simpler models produce lower-quality prototypes, which in turn degrades the quality of the aggregated global prototypes, ultimately leading to poor model training outcomes."
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Average test accuracy (%) on CIFAR-10 under different degrees of system heterogeneity.",
        "table": "<table id=\"S5.T4.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.4.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.4.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Heterogeneity degrees</th>\n<th id=\"S5.T4.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedProto</th>\n<th id=\"S5.T4.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedMD</th>\n<th id=\"S5.T4.4.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedDF</th>\n<th id=\"S5.T4.4.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">HeteroFL</th>\n<th id=\"S5.T4.4.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedTSA</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.4.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.4.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">[1.0]</th>\n<td id=\"S5.T4.4.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.31</td>\n<td id=\"S5.T4.4.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">85.23</td>\n<td id=\"S5.T4.4.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.62</td>\n<td id=\"S5.T4.4.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">91.13</td>\n<td id=\"S5.T4.4.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T4.4.2.1.6.1\" class=\"ltx_text ltx_font_bold\">91.56</span></td>\n</tr>\n<tr id=\"S5.T4.4.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.4.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">[0.5, 1.0]</th>\n<td id=\"S5.T4.4.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">62.13</td>\n<td id=\"S5.T4.4.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">78.11</td>\n<td id=\"S5.T4.4.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.43</td>\n<td id=\"S5.T4.4.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">83.30</td>\n<td id=\"S5.T4.4.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T4.4.3.2.6.1\" class=\"ltx_text ltx_font_bold\">89.61</span></td>\n</tr>\n<tr id=\"S5.T4.4.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T4.4.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">[0.6, 0.8, 1.0]</th>\n<td id=\"S5.T4.4.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">59.60</td>\n<td id=\"S5.T4.4.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.26</td>\n<td id=\"S5.T4.4.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">79.01</td>\n<td id=\"S5.T4.4.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">82.61</td>\n<td id=\"S5.T4.4.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T4.4.4.3.6.1\" class=\"ltx_text ltx_font_bold\">89.28</span></td>\n</tr>\n<tr id=\"S5.T4.4.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T4.4.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">[0.1, 0.4, 0.7, 1.0]</th>\n<td id=\"S5.T4.4.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">50.03</td>\n<td id=\"S5.T4.4.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">64.31</td>\n<td id=\"S5.T4.4.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">70.55</td>\n<td id=\"S5.T4.4.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">68.21</td>\n<td id=\"S5.T4.4.5.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T4.4.5.4.6.1\" class=\"ltx_text ltx_font_bold\">82.37</span></td>\n</tr>\n<tr id=\"S5.T4.4.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T4.4.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">[0.1, 0.3, 0.5, 0.7, 1.0]</th>\n<td id=\"S5.T4.4.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">48.87</td>\n<td id=\"S5.T4.4.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">63.16</td>\n<td id=\"S5.T4.4.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">68.83</td>\n<td id=\"S5.T4.4.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">67.79</td>\n<td id=\"S5.T4.4.6.5.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T4.4.6.5.6.1\" class=\"ltx_text ltx_font_bold\">80.09</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Effect of Different Degrees of Model Heterogeneity. We evaluate the performance of FedTSA across varying degrees of model heterogeneity, comparing it with other model-heterogeneous FL baselines. To introduce model heterogeneity, we continue to use the pruning rate and keep the number of clients to 20. We select CIFAR-10 as the dataset and configure the data distribution to be IID. The specific settings for heterogeneity and the corresponding experimental results are detailed in Table 4."
        ]
    },
    "S5.T6": {
        "caption": "",
        "table": "<table id=\"S5.T6.fig1.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T6.fig1.3.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T6.fig1.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">T</th>\n<th id=\"S5.T6.fig1.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">IID</th>\n<th id=\"S5.T6.fig1.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Dir(0.6)</th>\n<th id=\"S5.T6.fig1.3.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Dir(0.3)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T6.fig1.3.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T6.fig1.3.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">1</th>\n<td id=\"S5.T6.fig1.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.59</td>\n<td id=\"S5.T6.fig1.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.58</td>\n<td id=\"S5.T6.fig1.3.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.96</td>\n</tr>\n<tr id=\"S5.T6.fig1.3.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T6.fig1.3.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">5</th>\n<td id=\"S5.T6.fig1.3.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T6.fig1.3.3.2.2.1\" class=\"ltx_text ltx_font_bold\">89.28</span></td>\n<td id=\"S5.T6.fig1.3.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T6.fig1.3.3.2.3.1\" class=\"ltx_text ltx_font_bold\">87.80</span></td>\n<td id=\"S5.T6.fig1.3.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T6.fig1.3.3.2.4.1\" class=\"ltx_text ltx_font_bold\">87.10</span></td>\n</tr>\n<tr id=\"S5.T6.fig1.3.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T6.fig1.3.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">10</th>\n<td id=\"S5.T6.fig1.3.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">88.97</td>\n<td id=\"S5.T6.fig1.3.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.57</td>\n<td id=\"S5.T6.fig1.3.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.84</td>\n</tr>\n<tr id=\"S5.T6.fig1.3.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T6.fig1.3.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">20</th>\n<td id=\"S5.T6.fig1.3.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">87.30</td>\n<td id=\"S5.T6.fig1.3.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.43</td>\n<td id=\"S5.T6.fig1.3.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">86.12</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Diffusion Models. With large-scale training data and model parameters, various text-to-image generative models are now capable of creating high-quality images [36, 9, 40]. Among them, diffusion models [17] have achieved significant success in the image synthesis domain and AI-generated content (AIGC) fields, such as the popular Stable Diffusion [38] and DALL-E2 [37]. A typical diffusion model includes a forward diffusion stage and a reverse diffusion stage [8]. Given a textual prompt, the diffusion model is able to convert Gaussian noise into a text-compliant image through an iterative denoising step. Despite diffusion models showcasing excellent performance, only a few works have leveraged them in FL. [51] and [57] combine diffusion models with FL to mitigate non-IID problems, while [45] applies FL to the training process to address privacy issues.",
            "We motivate this study by presenting the impact of system heterogeneity through a small-scale experiment. We establish a heterogeneous system comprising four types of devices, including three Raspberry Pi 4B units, three LattePanda 2 Alpha 864s [23], two PCs without GPU, and one PC equipped with an NVIDIA 3080Ti GPU. These devices range from embedded systems to PCs with GPUs, effectively presenting the system heterogeneity prevalent in real-world scenarios. The core specifications of each device type are detailed in Table 2. Given the limited resources of the Raspberry Pi, we select a FL training task using the MNIST dataset and a CNN model. This model includes two convolutional layers with max pooling, followed by two fully connected layers. Each device serves as a FL client, with an additional machine serving as the central server for aggregation. We employ FedAvg [32] in this study.",
            "We design four experiments to assess the impact of system heterogeneity. The settings and the wall-clock convergence times are detailed in Table 2. We can observe that when all devices participate in training, the convergence speed is extremely slow, exceeding 2700 seconds. This is attributed to tail latency [25] caused by the existence of Raspberry Pi devices, which delay the process even after other devices have completed their training. The convergence times for the remaining three experiments are reduced to 1428, 210, and 143 seconds, respectively, showing decreases of 47%, 92% and 95% compared to the initial scenario. These results show the idle times of faster devices, highlighting the significant impact of system heterogeneity on FL training efficiency.",
            "To enhance efficiency, FedTSA avoids the long training process that is usually required for diffusion models. We simply develop a prompt pool denoted as 𝒴𝒴\\mathcal{Y} and leverage a pre-trained Stable Diffusion v1-4 Model [38] for inference. By allowing clients to upload prompts to the server’s prompt pool, we have effectively mitigated concerns about data distribution and potential privacy issues. Moreover, before uploading textual prompts, the implementation of differential privacy techniques [42] can be employed, whereby the inclusion of irrelevant text further safeguards the privacy of the system.",
            "Performance Comparison. From the results presented in Table 3, we can observe that FedTSA outperforms other FL baselines across both IID and non-IID settings, highlighting its promising performance and robustness to heterogeneous data. An exception is noted in the IID setting of CIFAR-10 dataset, where FedTSA slightly lags behind the upper bound performance of FedAvg. However, we should notice that the upper bounds for FedAvg and FedProx are based on an idealized scenario where all clients have unpruned models. Given this context, FedTSA still maintains a dominant performance over most upper bounds across both IID and non-IID settings. This superior performance is attributed to FedTSA’s innovative strategy. Contrasting with algorithms that conventionally replace the global model with a local model for round updates, FedTSA fosters inter-client learning through DML. Specifically, FedTSA utilizes the KL divergence loss to learn the average logits from other client models, thereby effectively obviating the need to rely on a singular global model. Consequently, each model not only gains knowledge from its counterparts but also sidesteps potential information loss that may arise from direct replacement. Besides, we can notice that FedProto presents the lowest accuracy, revealing that prototype-based FL methods do not perform as well as methods based on knowledge distillation and local training in the presence of model heterogeneity. This is because simpler models produce lower-quality prototypes, which in turn degrades the quality of the aggregated global prototypes, ultimately leading to poor model training outcomes.",
            "Effect of Different Degrees of Model Heterogeneity. We evaluate the performance of FedTSA across varying degrees of model heterogeneity, comparing it with other model-heterogeneous FL baselines. To introduce model heterogeneity, we continue to use the pruning rate and keep the number of clients to 20. We select CIFAR-10 as the dataset and configure the data distribution to be IID. The specific settings for heterogeneity and the corresponding experimental results are detailed in Table 4.",
            "where Lk​lsubscript𝐿𝑘𝑙L_{kl} is KL divergence loss, Lc​esubscript𝐿𝑐𝑒L_{ce} is cross-entropy (CE) loss, and α𝛼\\alpha is a weighting factor. Figure 5 shows the accuracy differences among these approaches. KL-only converges fastest initially but ultimately underperforms KL+CE in accuracy. While KL+CE is slower and less stable initially, it achieves the highest final accuracy. When the weight of KL divergence loss decreases, for instance, from 1.0 to 0.8, as the figure shows, the performance lies between the KL-only and KL+CE with equal weights. This suggests that combining KL and CE losses enhances global aggregation, but the slight accuracy gain from adding CE loss may not justify the extra computational cost.",
            "Effect of Temperature. The temperature T𝑇T is a hyperparameter in the softmax function to control the sharpness of the predicted probabilities, as Eq. 6 indicates. Higher T𝑇T values create a uniform distribution, while lower values lead to a peaked one. Properly tuning T𝑇T enhances knowledge exchange in DML, as depicted in Table 6. Optimal performance is achieved at T=5𝑇5T=5 for both IID and non-IID settings, with accuracy declining at higher T𝑇T values. Notably, the decline in performance exhibits a more gradual trend in non-IID scenarios compared to IID. This can be explained by the smoothing effect of a higher T𝑇T on the logits output: even if the model lacks confidence for certain classes due to an imbalanced data distribution, the student model still benefits from the teacher’s guidance. Furthermore, the smoothed logits help to prevent the student model from overfitting to classes that have more training data.",
            "Effect of Prompts. We verify the impact of different prompts on the model performance and test a range of prompts from simple to descriptive, as detailed in Table 6. Taking ’bird’ as an example, we observe that prompt quality significantly impacts performance. A single-word prompt results in 81.31%, while more descriptive prompts improve the accuracy. We explore the reason behind it by examining the generated data and find that simple prompts like a single word can produce abstract images, such as hand-drawn or stitching-style images. Since the model is trained with real-world photos, it cannot generate high-quality logits on these images, leading to degraded Stage 2 aggregation and thus impacting the model performance. We also notice that the model accuracy is nearly identical when using the last two prompts, which shows that FedTSA does not require detailed information about the original dataset, offering better privacy than constructing public datasets by studying raw data.",
            "We conduct experiments under Python 3.8.0 and PyTorch 1.31.1. We use a NVIDIA A100 provided by RunPod111https://www.runpod.io/ for computation. Weights&Biases222https://wandb.ai/site is leveraged to track and log the experimental results. Regarding the diffusion model, we choose the Stable Diffusion model v1-4 from Hugging Face333https://huggingface.co/CompVis/stable-diffusion-v1-4."
        ]
    }
}