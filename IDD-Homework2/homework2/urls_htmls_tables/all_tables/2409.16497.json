{
    "id_table_1": {
        "caption": "Table 1.  Details of datasets used. The size of corpus is downsampled to 15K in SCIDOCS and 10K in GermanQuAD. Filtered Queries: Generated synthetic queries from FLAN-T5-Large with filtering.",
        "table": "S4.T1.1.1",
        "footnotes": [],
        "references": [
            "Different from the previous work, we demonstrate directly on the embedding level instead of the text level, that the synthetically generated queries embeddings can effectively augment the corpus representation (Figure  1 ). Here, we propose an unsupervised representation learning approach through self-instructed-tuning leveraging the embedding generation and sequence generation capability of an encoder-decoder LLM. This approach consists of two steps, i.e. self-instructed-learning and Rao-Blackwellization. In the first step, we design two instruction tasks, namely question generation and keyword summarization, to generate synthetic questions and keywords for each given corpus via prompting a pre-trained LLM. Next, we apply filters to gate the synthetic data quality and instruction-tune the pre-trained LLM (and its variant versions) on the filtered output (Step one in Figure  2 ). In the second step, we use the instruct-tuned LLM to generate better synthetic questions and keywords following the same instruction prompts as in training. We then obtain the embeddings of the newly generated synthetic questions and keywords and that of the corpus from the instruct-tuned LLM encoder, and take the weighted average as our augmented corpus representation (Step two in Figure  2 ).",
            "We assume that an effective encoder maps each group of queries  Q i subscript Q i Q_{i} italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  near a group center in the high-dimensional embedding space and also maps the corresponding  C i subscript C i C_{i} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  to the surrounding area so that  Q i subscript Q i Q_{i} italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  C i subscript C i C_{i} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  are well associated. For example, when we have  Q 21  Q 2 subscript Q 21 subscript Q 2 Q_{21}\\in Q_{2} italic_Q start_POSTSUBSCRIPT 21 end_POSTSUBSCRIPT  italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  query, the retrieval system will retrieve the  C 2 subscript C 2 C_{2} italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  corpora which is the closest to the query (Figure  1 ).",
            "We expect to achieve better performance with this formula for corpus representation. An intuitive understanding is that it gets closer to the relevant queries embedding in the vector space (Figure  1 ).",
            "We use the same instructions across the entire framework, including generation and training. Figure  1  shows a schematic diagram that although the generated queries from an open-box pre-trained LLM may not effectively enrich the corpora, after instruction-tuning, the generated synthetic queries become more relevant and the corpus representation can be improved consequently.",
            "In this work, we tested four IR datasets where the summary of the database is shown in Table  1 .  English:  (1) NFCorpus  (Boteva et al . ,  2016 )  has automatically extracted relevance judgments for medical documents. (2) SciFact  (Wadden et al . ,  2020 )  consists of expert-annotated scientific claims with abstracts and rationales. (3) SCIDOCS  (Cohan et al . ,  2020 )  has seven document-level tasks from citation prediction, document classification, and recommendation.  German:  (4) GermanQuAD  (Moller et al . ,  2021 )  has the relevant information for high complex German QA with a large size of corpora. Due to computation resource limits, we downsample the corpus in SCIDOCS and GermanQuAD datasets, where we ensure the downsampled corpus include all relevant corpus for test queries. Note that such downsampling does not prevent us from fairly comparing the zero-shot retrieval efficacy of our approach with open-box LLMs because all experiments are performed under the same data setting. To help the encoder capture the fine-grained semantic interaction between queries and corpus, we divide each corpora into multiple sentences using the PunktSentenceTokenizer  1 1 1 https://www.nltk.org/api/nltk.tokenize.PunktSentenceTokenizer.html  from nltk package and use the sentence level multi representation for the corpora, meaning that if any of the sentence is retrieved, the passage is retrieved.",
            "For instruction query generation and instruction-tuning, we consider two types of instructions (i.e. keyword summarization and question generation) as shown in Figure  2 . We also develop a filter to improve the quality of generated instructions. If the task is keyword summarization, the number of keywords should be smaller than the half number of sentences in corpus. If its question generation, the generated sequence should end with a question mark. The filter is simple, leaving room for further improvement. The numbers of the filtered synthetic queries are shown in Table  1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Open-box encoder-only average performance for passage level and sentence level indexing. Model is FLAN-T5.    \\spadesuit  : NDCG@10.    \\clubsuit  : MRR@100    \\heartsuit  : Recall@100.",
        "table": "S4.T1.1.1.1.1.3.1",
        "footnotes": [],
        "references": [
            "Different from the previous work, we demonstrate directly on the embedding level instead of the text level, that the synthetically generated queries embeddings can effectively augment the corpus representation (Figure  1 ). Here, we propose an unsupervised representation learning approach through self-instructed-tuning leveraging the embedding generation and sequence generation capability of an encoder-decoder LLM. This approach consists of two steps, i.e. self-instructed-learning and Rao-Blackwellization. In the first step, we design two instruction tasks, namely question generation and keyword summarization, to generate synthetic questions and keywords for each given corpus via prompting a pre-trained LLM. Next, we apply filters to gate the synthetic data quality and instruction-tune the pre-trained LLM (and its variant versions) on the filtered output (Step one in Figure  2 ). In the second step, we use the instruct-tuned LLM to generate better synthetic questions and keywords following the same instruction prompts as in training. We then obtain the embeddings of the newly generated synthetic questions and keywords and that of the corpus from the instruct-tuned LLM encoder, and take the weighted average as our augmented corpus representation (Step two in Figure  2 ).",
            "We propose an unsupervised text representation learning approach through self-instructed-tuning a pre-trained encoder-decoder LLM. The first step is to generate instruction following responses from an LLM and instruction-tune the LLM itself with filtered quality  (natural language instruction, response)  pairs. The second step is to compute the augmented corpus embedding weighing in synthetic queries (e.g. questions, keywords) embeddings. Figure  2  presents the overall flow of our approach.",
            "According to RaoBlackwell Theorem: If  g  ( X ) g X g(\\mathbf{X}) italic_g ( bold_X )  is any kind of estimator of a parameter    \\theta italic_ , then the conditional expectation of  g  ( X ) g X g(\\mathbf{X}) italic_g ( bold_X )  given  T  ( X ) T X T(\\mathbf{X}) italic_T ( bold_X ) , namely  E  ( g  ( x ) | T  ( x ) ) E conditional g x T x \\mathbb{E}(g(x)|T(x)) blackboard_E ( italic_g ( italic_x ) | italic_T ( italic_x ) ) , where  T T T italic_T  is a sufficient statistic, is typically a better estimator of    \\theta italic_ , and is never worse. Plug in Equation ( 2 ), we get an improved estimator for  E  ( Encoder  ( Q i ) ) E Encoder subscript Q i \\mathbb{E}(\\texttt{Encoder}(Q_{i})) blackboard_E ( Encoder ( italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) , which is  E  ( Encoder  ( C i ) | 1 m   j = 1 m Encoder  ( Q i  j ) ) E conditional Encoder subscript C i 1 m superscript subscript j 1 m Encoder subscript Q i j \\mathbb{E}(\\texttt{Encoder}(C_{i})|\\frac{1}{m}\\sum_{j=1}^{m}\\texttt{Encoder}(Q%  _{ij})) blackboard_E ( Encoder ( italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) | divide start_ARG 1 end_ARG start_ARG italic_m end_ARG  start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT Encoder ( italic_Q start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) ) .",
            "For instruction query generation and instruction-tuning, we consider two types of instructions (i.e. keyword summarization and question generation) as shown in Figure  2 . We also develop a filter to improve the quality of generated instructions. If the task is keyword summarization, the number of keywords should be smaller than the half number of sentences in corpus. If its question generation, the generated sequence should end with a question mark. The filter is simple, leaving room for further improvement. The numbers of the filtered synthetic queries are shown in Table  1 .",
            "We evaluate whether the sentence level multi-representation can capture the semantic interaction between the corpora and the query. Results for FLAN-T5 models using encoder-only representation are shown in Table  2 . The sentence level multi-representation embedding technique outperforms the corpora level single representation by a large margin across all datasets. As the model size increases, the performance also gets better. Note that our approach uses no labeled data to achieve on par performance as SOTA models, and sentence level indexing is a way we do for chunking. According to the promising empirical results, we will apply the sentence level multi-representation technique to all the following experiments."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Comparison of performances according to instruction-tuning.",
        "table": "S4.T1.1.1.1.1.5.1",
        "footnotes": [],
        "references": [
            "Plug in the generated synthetic queries, let  R  ( C i ) R subscript C i R(C_{i}) italic_R ( italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  denote the final representation of corpora  C i subscript C i C_{i} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , the Equation ( 3 ) becomes a weighted average of original corpora embedding and its synthetic query embedding,",
            "Table  3  describes the performance of FLAN-T5 models regarding instruction-tuning. Overall, we can mostly find the improvements of performances in all metrics after instruction-tuning, except for SCIDOCS. This is mainly because the quality of generated queries after instruction-tuning are proper and detailed (Table  5 ), and also each synthetic query is less overlapped which makes the corpora distinguishable. The influence of instruction-tuning is greater in larger model since it can have better generation capability and be more affected by fine-tuning with instructions.",
            "Figure  3  shows the distributions of embeddings of corpora and test queries based on FLAN-T5-Large. Overall, the weighted corpus representation and instruction-tuning spread out the corpora embeddings to make them distinguishable. Also, it helps to locate the test queries closer to the corpora. Thus, our approach helps to integrate the crucial and detailed synthetic queries for corpus representation which helps to generate the unique corpora to achieve the enhanced retrieval performances."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  Comparison with SOTA. Tuned: Model with instruction-tuning.",
        "table": "S4.T2.10.4",
        "footnotes": [],
        "references": [
            "We verify the effectiveness of the proposed methods on three English and one German IR datasets measured by NDCG@10, MRR@100, Recall@100. We significantly improve the zero-shot average retrieval performance on all metrics with our unsupervised approach and exceed three competitive supervised dense retrievers, with model of size at least  38 % percent 38 38\\% 38 %  smaller, by  1.96 % percent 1.96 1.96\\% 1.96 % ,  4.62 % percent 4.62 4.62\\% 4.62 % ,  9.52 % percent 9.52 9.52\\% 9.52 %  absolute on NDCG@10 (Table  4 ).",
            "where  w 0 subscript w 0 w_{0} italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  is a hyper-parameter that is tuned on a subset of test queries. Equation ( 5 ) is our proposed corpus representation for the dual-encoder retrieval system. Note that we can generate different types of synthetic queries in Equation ( 4 ) using various instructions, and we can generate multiple sequences for each instruction by adopting decoding strategies such as beam search. We can also improve the quality of the generated queries through instruction-tuning as follows.",
            "Table  4  compares our approach and SOTA models in zero-shot scenarios. To clarify, FLAN-T5-Base has similar size as other SOTA models which can be considered as a fair comparison. First of all, instruct-tuned FLAN-T5-Base shows the boosted averaged results than other SOTA models which reveals the prowess of our approach. Considering a larger model (i.e. Tuned-FLAN-T5-Large) enhances the performance further. Thus, our suggested approach is consistently applicable in different size of models where the larger one promises the better performances."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.  Example of synthetic queries according to the instruction-tuning. FLAN-T5-Large is used for generating the examples.",
        "table": "S4.T3.16.16",
        "footnotes": [],
        "references": [
            "where  w 0 subscript w 0 w_{0} italic_w start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  is a hyper-parameter that is tuned on a subset of test queries. Equation ( 5 ) is our proposed corpus representation for the dual-encoder retrieval system. Note that we can generate different types of synthetic queries in Equation ( 4 ) using various instructions, and we can generate multiple sequences for each instruction by adopting decoding strategies such as beam search. We can also improve the quality of the generated queries through instruction-tuning as follows.",
            "Table  3  describes the performance of FLAN-T5 models regarding instruction-tuning. Overall, we can mostly find the improvements of performances in all metrics after instruction-tuning, except for SCIDOCS. This is mainly because the quality of generated queries after instruction-tuning are proper and detailed (Table  5 ), and also each synthetic query is less overlapped which makes the corpora distinguishable. The influence of instruction-tuning is greater in larger model since it can have better generation capability and be more affected by fine-tuning with instructions.",
            "Effectiveness of Instruction-tuning     Table  5  gives the examples of generated synthetic queries. In keyword summarization, open-box extracts the ambiguous and meaningless words (first example) or a simple copy of sentence (second example) as keywords while instruction-tuning helps to observe the whole corpus to extract the core keywords. For question generation, open-box generates the general (third example) or unanswerable questions (fourth example) while instruction-tuning gives the detailed and suitable questions which can be accountable by the specific corpus."
        ]
    },
    "id_table_6": {
        "caption": "Table 6.  Different weight methods for corpus representation. Model is based on FLAN-T5.",
        "table": "S4.T4.16.16",
        "footnotes": [],
        "references": [
            "As we dont have the query-corpora labeled data, we propose to self-instructed-tuning the LLM on its self-generated quality (i.e. gated) responses following given instructions to enhance synthetic queries generation relevance. This approach has demonstrated its effectiveness  (Wang et al . ,  2023 ) . The instruct-tuned LLM is then used to prepare the synthetic queries for the corpus representation augmentation as in Equation ( 6 ).",
            "Table  6  shows the overall performances according to the different weight approaches in corpus representation. First of all, the equal weight approach shows the worst performance which confirms that the corpus basically contains the most relevant information for queries which should be weighted more. Also, extracted keywords and questions mostly have the essential contexts but partial information of corpus which is not enough to include the semantic meaning of corpus. Thus, manual weighting with emphasis on corpus promises the better result than BERTScore approaches."
        ]
    },
    "id_table_7": {
        "caption": "Table 7.  Different corpus representation augmentation. Model is based on FLAN-T5. Note that we evaluated on English datasets.",
        "table": "S5.T5.1.1",
        "footnotes": [],
        "references": [
            "Optimal Corpus Representation     From our findings, new corpus representation based on synthetic queries from instructions is useful to improve the retrieval performances. To define the optimal weights in corpus representation, we investigate the four different weight methods: (1) Equal: Giving the equal weights for corpus and generated synthetic queries (i.e. keyword, question). (2) Manual: It is same as Equation ( 7 ). (3) BERTScore: Giving the weights based on BERTScore (F1) with BERT-Base-Multilingual-Cased model  (Devlin et al . ,  2018 ) . Equation ( 8 ) shows the details of it. (4) BERTScore Softmax : Similar as BERTScore but including the Softmax.",
            "Effectiveness of Corpus Representation Augmentation     We compare with other corpus representation augmentation, docTTTTTquery  (Nogueira et al . ,  2019a ) , to validate our corpus augmentation. Here, we follow the default strategy of docTTTTTquery: top-10 with 40 predictions appending on corpus. According to Table  7 , we demonstrate significant improvement via our approach - embedding level augmentation with representations from self-instructed-tuned model. Based on this finding, we can confirm that augmenting representation on embedding level is more effective than on input text level with concatenation as docTTTTTquery, and our self-instructed-tuned model performs better than their supervised representation generation model."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "S5.T5.1.1.2.1.1.1",
        "footnotes": [],
        "references": [
            "Optimal Corpus Representation     From our findings, new corpus representation based on synthetic queries from instructions is useful to improve the retrieval performances. To define the optimal weights in corpus representation, we investigate the four different weight methods: (1) Equal: Giving the equal weights for corpus and generated synthetic queries (i.e. keyword, question). (2) Manual: It is same as Equation ( 7 ). (3) BERTScore: Giving the weights based on BERTScore (F1) with BERT-Base-Multilingual-Cased model  (Devlin et al . ,  2018 ) . Equation ( 8 ) shows the details of it. (4) BERTScore Softmax : Similar as BERTScore but including the Softmax."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "S5.T5.1.1.2.1.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "",
        "table": "S5.T5.1.1.3.2.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "S5.T5.1.1.3.2.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "S5.T5.1.1.4.3.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "S5.T5.1.1.4.3.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "S5.T5.1.1.4.3.4.1",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "S5.T5.1.1.5.4.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "S5.T5.1.1.5.4.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "S5.T5.1.1.5.4.4.1",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "S5.T6.5.5",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "S5.T6.4.4.4.5.1",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "S5.T7.4.4",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}