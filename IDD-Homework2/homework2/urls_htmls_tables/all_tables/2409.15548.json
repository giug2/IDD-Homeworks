{
    "id_table_1": {
        "caption": "Table 1:  Numerical simulation for synthetic data showing the average error rate, the fraction of infinite prediction intervals, and the average interval score for the finite intervals, averaged over 1000 independent trials. Inf denotes fraction of infinite prediction intervals, and IS is the mean Winkler interval score.",
        "table": "S2.E12",
        "footnotes": [],
        "references": [
            "called examples. For notational convenience, we write  z i := ( x i , y i )  X  Y := Z assign subscript z i subscript x i subscript y i X Y assign Z z_{i}:=(x_{i},y_{i})\\in\\boldsymbol{X}\\times\\boldsymbol{Y}:=\\boldsymbol{Z} italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT := ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  bold_italic_X  bold_italic_Y := bold_italic_Z . The measurable space  Z Z \\boldsymbol{Z} bold_italic_Z  is called the example space. Thus, the infinite sequence ( 1 ) is an element of the measurable space  Z  superscript Z \\boldsymbol{Z}^{\\infty} bold_italic_Z start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , and we assume that it is drawn from some probability distribution  P P P italic_P  on  Z  superscript Z \\boldsymbol{Z}^{\\infty} bold_italic_Z start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . The standard assumption in CP is that  P P P italic_P  is exchangeable, but in this paper we will make no such assumption, and  P P P italic_P  can be any distribution.",
            "The main disadvantage of conformal predictors is their, often, intractable computational cost. In a regression setting, where  Y = R Y R \\\\ Y=\\mathbb{R} italic_Y = blackboard_R , we would theoretically have to compute the fraction in ( 11 ) for each real number, which is clearly impossible. For some special cases, such as ridge regression, efficient implementations exist, but generally the computational cost is too high. For this reason, inductive conformal predictors (ICP) were introduced. We give a brief description here, and refer the interested reader to  [ 2 ]  for more details. Given a training set of size  l l l italic_l , we split it into two parts: the proper training set  z 1 , ... , z m subscript z 1 ... subscript z m z_{1},\\dots,z_{m} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_z start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT  of size  m m m italic_m , and the calibration set  z m + 1 , ... , z l subscript z m 1 ... subscript z l z_{m+1},\\dots,z_{l} italic_z start_POSTSUBSCRIPT italic_m + 1 end_POSTSUBSCRIPT , ... , italic_z start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT  of size  l  m l m l-m italic_l - italic_m . For every test object  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , compute the prediction set",
            "The iteration ( 15 ) could cause   t  0 subscript  t 0 \\varepsilon_{t}\\leq 0 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  0  or    1  1 \\varepsilon\\geq 1 italic_  1 . Technically, prediction sets are undefined for confidence predictors, and in particular conformal predictors for    ( 0 , 1 )  0 1 \\varepsilon\\notin(0,1) italic_  ( 0 , 1 ) . Thus, we introduce a trivial extension that is compatible with ACI.",
            "An extended confidence predictor is a confidence predictor as defined in definition  1 , with the additional property that",
            "Interestingly, the proof of ( 16 ) does not use any property of a conformal, or even confidence predictor. In fact, all that is required of a predictor    \\Gamma roman_  is that   \\emptyset   is predicted for    1  1 \\varepsilon\\geq 1 italic_  1  and  Y Y \\boldsymbol{Y} bold_italic_Y  for    0  0 \\varepsilon\\leq 0 italic_  0 . For all other significance levels, even completely random prediction sets ensure ( 16 ). The result is proved by Lemma 4.1 in  [ 5 ] , which we reproduce here with notation slightly modified to align with ours.",
            "For all  t  N t N t\\in\\mathbb{N} italic_t  blackboard_N ,   t  [   , 1 +  ] subscript  t  1  \\varepsilon_{t}\\in[-\\delta,1+\\delta] italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  [ - italic_ , 1 + italic_ ] , almost surely, if ACI ( 15 ) is used together with a set predictor that outputs   \\emptyset   is predicted for    1  1 \\varepsilon\\geq 1 italic_  1  and  Y Y \\boldsymbol{Y} bold_italic_Y  for    0  0 \\varepsilon\\leq 0 italic_  0 .",
            "Assume that the sequence  {  t } t  N subscript subscript  t t N \\{\\varepsilon_{t}\\}_{t\\in\\mathbb{N}} { italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t  blackboard_N end_POSTSUBSCRIPT  is such that  inf t  t <   subscript infimum t subscript  t  \\inf_{t}\\varepsilon_{t}<-\\gamma roman_inf start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT < - italic_  (the case when  sup t  t > 1 +  subscript supremum t subscript  t 1  \\sup_{t}\\varepsilon_{t}>1+\\gamma roman_sup start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT > 1 + italic_  is identical). By ( 15 ),  sup t |  t + 1   t | = sup t   |   err t  t | <  subscript supremum t subscript  t 1 subscript  t subscript supremum t   superscript subscript err t subscript  t  \\sup_{t}|\\varepsilon_{t+1}-\\varepsilon_{t}|=\\sup_{t}\\gamma|\\varepsilon-\\text{% err}_{t}^{\\varepsilon_{t}}|<\\gamma roman_sup start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_ start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | = roman_sup start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_ | italic_ - err start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT | < italic_ . Thus, with positive probability, we may find  t  N t N t\\in\\mathbb{N} italic_t  blackboard_N  such that   t < 0 subscript  t 0 \\varepsilon_{t}<0 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT < 0  and   t + 1 <  t subscript  t 1 subscript  t \\varepsilon_{t+1}<\\varepsilon_{t} italic_ start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT < italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . However, by assumption, and ( 15 ),",
            "It is clear that no property of conformal, or even confidence predictors, is needed to achieve ( 16 ).",
            "This section argues that, while not strictly necessary for achieving the finite sample guarantee ( 16 ) of ACI, we should restrict ourselves to confidence predictors, as this is the most general type of predictor that satisfy natural assumptions on prediction sets at a confidence level.",
            "We have shown that ACI achieves the finite sample guarantee ( 16 ) for non-exchangeable data even for non confidence predictors, but we have argued that it would be absurd to drop the property of nested prediction sets. A natural question is whether using ACI with a conformal predictor has any advantages over using it together with a general confidence predictor. For non-exchangeable data, conformal predictors are not necessarily valid, which is also true for confidence predictors.",
            "In our evaluation, we have to modify the definition of OE slightly to account for the ACI update ( 15 ). We want to achieve the finite sample guarantee ( 16 ) with a target error rate    \\varepsilon italic_ . Then",
            "Conformalized least squares (CP).  We use the conformalized ridge regression algorithm  [ 2 ]  with ridge parameter set to 0 (which corresponds to least squares). The conformalized ridge regresison algorithm is described in  A.1 .",
            "For all three experiments on synthetic data, the first 100 examples are used as initial training set, and we run them 1000 times using different random seeds (ranging from 0 to 999). In all our experiments, we ask for a target error rate   = 0.1  0.1 \\varepsilon=0.1 italic_ = 0.1 , and set   1 =  subscript  1  \\varepsilon_{1}=\\varepsilon italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_ . Choosing the step size    \\gamma italic_  according to ( 17 ) with   = 0.05  0.05 \\delta=0.05 italic_ = 0.05  would suggest    0.0096  0.0096 \\gamma\\approx 0.0096 italic_  0.0096 .",
            "The results are shown in Figure  1  and Table  1 . The first thing to note is that both methods achieve ( 16 ) as expected. The gaps in the right-hand side plots in  1  indicate an infinite prediction interval, and the fraction of such infinite intervals is presented in Table  1 . We see that although the error rates are identical for both methods, CP outputs fewer infinite intervals in the second and third settings, with change points and drift. As can be seen in Table  1 , the difference in terms of average interval score (IS) is not large, but CP outputs fewer infinite intervals on average.",
            "Predicting the label can be seen either as regression or classification, and we choose to perform the regression task. For confidence predictor, we use the  RandomForestQuantileRegressor  from the  quantile-forest  python package  [ 17 ] . We turn it into an inductive conformal predictor (ICP) by using the  WrapRegressor  class from the  crepes  python package  [ 18 ] . We run our experiment 1000 times using different random seeds for each run. For the ICP, we must set aside a calibration set that is not used in training. Thus, we use 3/4 of the training set as proper training set, and the rest as calibration set. A different random split is used for each run. As a result, the training set for our confidence predictor (QRF) has size 4898 while the ICP has a proper training set size of just 3673 because the remaining data is used for calibration. Again, we choose step size according to ( 17 ) with   = 0.05  0.05 \\delta=0.05 italic_ = 0.05 . With  N = 1599 N 1599 N=1599 italic_N = 1599  we should choose    0.011  0.011 \\gamma\\approx 0.011 italic_  0.011 .",
            "Our results are summarised in Figure  2  and Table  2 . Again, both methods achieve the theoretical guarantee ( 16 ) as expected. Figure  2  summarises the mean running error rate, and the mean interval score (IS), averaged over the 1000 independent trials. It can be noted that CP is overly confident on average early on, while QRF is conservative. In terms of computational efficiency there is not much difference, ICP was introduced to overcome the often intractable computational cost of full CP, and the calibration step is quite fast. The mean error rate together with mean interval scores and the mean fraction of infinite intervals is presented in Table  2 . It is worth pointing out that QRF produced infinite intervals at all in only eight out of 1000 trials, while CP did so in 696 trials.",
            "The US Postal Service (USPS) dataset consists of 7291 training images and 2007 test images of handwritten digits from 0 to 9. The images are  16  16 16 16 16\\times 16 16  16  greyscale pixels. Exchangeability of the USPS dataset has been studied in  [ 19 ,  20 ] , and it is well known that the examples are not perfectly exchangeable. We use a  RandomForestClassifier  from the  scikit-learn  python package  [ 21 ]  with the default parameters, e.g. the forest consists of 100 trees. Again, we use the  crepes  python package  [ 18 ] , sacrificing 1/4 of the training data for calibration to train an ICP. We also define a confidence predictor by using the  predict_proba  method, which outputs estimated probabilities for every possible label (details in Appendix  B ). Choosing step size according to ( 17 ) with   = 0.05  0.05 \\delta=0.05 italic_ = 0.05  and  N = 2007 N 2007 N=2007 italic_N = 2007  leads to    0.009  0.009 \\gamma\\approx 0.009 italic_  0.009 .",
            "The results of the USPS experiment is summarised in Figure  3  and Table  3 . As expected, both methods achieve the theoretical guarantee ( 16 ), but RF is quite a bit more conservative than CP, in particular early on, as can be seen in Figure  3 . In terms of mean prediction set size, there is not much difference between the methods, but the OE of RF is relatively high, which is likely due to the large prediction sets output by RF in the beginning.",
            "In this paper, we have demonstrated that adaptive conformal inference (ACI) does not rely on any specific properties of conformal predictors to achieve its finite sample guarantee ( 16 ). In fact, it does not even need the more general concept of a confidence predictor, where the prediction sets are required to be nested (see Definition  1 ). However, we have argued that the property of nested prediction sets is the very least one should require when predicting with a confidence level. Without it, the coin flip predictor (Definition  4 ) is exactly valid but rather unhelpful. Since the validity guarantees of conformal predictors are lost if the exchangeability assumption is violated, and ACI provides finite sample coverage guarantees of another kind, we asked if anything is gained by using ACI with a conformal predictor, over using it with a confidence predictor in situations when exchangeability can not be expected.",
            "For the classification task in the USPS experiment, the picture is less clear. While the ICP based on random forest outperformed the confidence predictor based on the same algorithm in terms of observed excess, it is possible that better results could have been obtained by choosing a larger   1 subscript  1 \\varepsilon_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  to start the ACI iteration ( 15 ). It is clear from Figure  3  that the confidence predictor was way too conservative early on, and took some time to adapt. Much of the observed excess, i.e. the number of false labels included in the prediction sets, comes from the first 200 predictions, illustrating that there may be some utility in careful tuning of the initial control input   1 subscript  1 \\varepsilon_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT .",
            "The conformalised ridge regression algorithm (CRR) is a combination of two algorithms; lower CRR and upper CRR, that produce the lower and upper bound respectively. For the upper CRR we use the nonconformity measure  y  y ^ y ^ y y-\\hat{y} italic_y - over^ start_ARG italic_y end_ARG , and the lower uses  y ^  y ^ y y \\hat{y}-y over^ start_ARG italic_y end_ARG - italic_y , where  y ^ ^ y \\hat{y} over^ start_ARG italic_y end_ARG  is the ridge regression prediction of  y y y italic_y . The complete algorithm is presented in Algorithm  1 .",
            "In our experiments on synthetic data, we trivially extend Algorithm  1  by requiring the output to be  (   ,  ) (-\\infty,\\infty) ( -  ,  )  for    0  0 \\varepsilon\\leq 0 italic_  0  and   \\emptyset   for    1  1 \\varepsilon\\geq 1 italic_  1 , which results in an extended conformal predictor. It is shown in  [ 22 ]  that the prediction sets can be computed in  O  ( n  ln  n ) O n n \\mathcal{O}(n\\ln n) caligraphic_O ( italic_n roman_ln italic_n )  in the online mode. Computing  A A A italic_A  and  B B B italic_B  can be done in time  O  ( n ) O n \\mathcal{O}(n) caligraphic_O ( italic_n ) , and sorting can be done in time  O  ( n  ln  n ) O n n \\mathcal{O}(n\\ln n) caligraphic_O ( italic_n roman_ln italic_n ) .",
            "Since Algorithm  2  avoids the sorting that is needed in Algorithm  1 , it can be computed in  O  ( n ) . O n \\mathcal{O}(n). caligraphic_O ( italic_n ) ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Numerical experiment on the Wine Quality dataset showing the mean error rate, interval scores for the finite prediction intervals, and the fraction of infinite intervals output. The results are averaged over 1000 independent trials. Inf denotes fraction of infinite prediction intervals, and IS is the mean Winkler interval score.",
        "table": "S2.E14",
        "footnotes": [],
        "references": [
            "The rest of this paper is organised as follows. Section  2  introduces conformal and confidence predictors, as well as the relevant notation. Readers familiar with the terminology used in  [ 2 ]  may find it convenient to skip ahead to Section  3 , where ACI and its finite sample coverage guarantee is introduced, as well as a trivial but technically necessary extension of conformal and confidence predictors. Our main result is given in Section  4 , where we restate Lemma 4.1 in  [ 5 ] , which is the key result that is used to prove the finite sample guarantee of ACI. We indicate that this lemma does not rely on any particular property of conformal predictors, or even confidence predictors. We then argue in Section  5  that, while not strictly necessary for the finite sample guarantee of ACI, confidence predictors represent the natural way of predicting at a confidence level. Our numerical experiments on synthetic and real data are described in Section  6 , and Section  7  concludes. Additional details on some algorithms used in the experiments, and a summary of the results without using ACI (i.e. at a fixed significance level) are given in the appendices.",
            "Prediction intervals from ordinary least squares (OLS).  The ordinary least squares algorithm can output prediction intervals natively. We implement an online ordinary least squares algorithm, and output the least squares prediction intervals. The resulting confidence predictor is described in  A.2 .",
            "Our results are summarised in Figure  2  and Table  2 . Again, both methods achieve the theoretical guarantee ( 16 ) as expected. Figure  2  summarises the mean running error rate, and the mean interval score (IS), averaged over the 1000 independent trials. It can be noted that CP is overly confident on average early on, while QRF is conservative. In terms of computational efficiency there is not much difference, ICP was introduced to overcome the often intractable computational cost of full CP, and the calibration step is quite fast. The mean error rate together with mean interval scores and the mean fraction of infinite intervals is presented in Table  2 . It is worth pointing out that QRF produced infinite intervals at all in only eight out of 1000 trials, while CP did so in 696 trials.",
            "From Eq. ( 24 ) we see that the predictions  y ^ ^ y \\hat{y} over^ start_ARG italic_y end_ARG  for all objects  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  are given by",
            "The confidence predictor based on ridge regression is summarised in Algorithm  2 , where  t  / 2 , n  1  p subscript t  2 n 1 p t_{\\varepsilon/2,n-1-p} italic_t start_POSTSUBSCRIPT italic_ / 2 , italic_n - 1 - italic_p end_POSTSUBSCRIPT  is the critical value from the Students  t t t italic_t -distribution with  n  1  p n 1 p n-1-p italic_n - 1 - italic_p  degrees of freedom.",
            "Since Algorithm  2  avoids the sorting that is needed in Algorithm  1 , it can be computed in  O  ( n ) . O n \\mathcal{O}(n). caligraphic_O ( italic_n ) ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Numerical experiment on the Wine Quality dataset showing the mean error rate, prediction set size, and observed excess (OE). The results are averaged over 1000 independent trials.",
        "table": "S3.E18",
        "footnotes": [],
        "references": [
            "The rest of this paper is organised as follows. Section  2  introduces conformal and confidence predictors, as well as the relevant notation. Readers familiar with the terminology used in  [ 2 ]  may find it convenient to skip ahead to Section  3 , where ACI and its finite sample coverage guarantee is introduced, as well as a trivial but technically necessary extension of conformal and confidence predictors. Our main result is given in Section  4 , where we restate Lemma 4.1 in  [ 5 ] , which is the key result that is used to prove the finite sample guarantee of ACI. We indicate that this lemma does not rely on any particular property of conformal predictors, or even confidence predictors. We then argue in Section  5  that, while not strictly necessary for the finite sample guarantee of ACI, confidence predictors represent the natural way of predicting at a confidence level. Our numerical experiments on synthetic and real data are described in Section  6 , and Section  7  concludes. Additional details on some algorithms used in the experiments, and a summary of the results without using ACI (i.e. at a fixed significance level) are given in the appendices.",
            "Since the difference between confidence predictors and extended confidence predictors is minor, and since any confidence predictor can be trivially modified to be an extended confidence predictor, we will use the terms interchangeably. In analogy with definition  3 , we can also define an extended conformal predictor by requiring that the output is   \\emptyset   if    1  1 \\varepsilon\\geq 1 italic_  1  and  Y Y \\boldsymbol{Y} bold_italic_Y  if    0  0 \\varepsilon\\leq 0 italic_  0 . Similar to confidence predictors and extended confidence predictors, we will use the terms conformal predictor and extended conformal predictor interchangeably.",
            "We argue that if we want to predict with confidence, the very least we can ask for is a confidence predictor. Imagine that we play the following game: Before throwing and  n n n italic_n  sided die, a player is allowed to bet that the outcome will be contained in a subset of  1 , ... , n 1 ... n {1,\\dots,n} 1 , ... , italic_n . Surely, we would not be willing to bet a larger amount on a smaller subset. Dropping the property of nested prediction sets is analogous to this game in the sense that the predictor could state a higher confidence in a smaller prediction set, which is absurd. If we are to mean anything reasonable by stating a confidence level, we simply must require that the prediction sets are nested. For this reason, we argue that, while ACI does not strictly require it, we should restrict its usage to extended confidence predictors in the sense of Definition  3  (and the analogous extended conformal predictors). But that raises a natural empirical question.",
            "For each setting, we implement the following two methods with the desired error rate set to   = 0.1  0.1 \\varepsilon=0.1 italic_ = 0.1 . For both methods, the significance level for the next prediction is computed using ACI. We ensure that both methods are extended confidence predictors in the sense of Definition  3  by simply outputting  R R \\mathbb{R} blackboard_R  for    0  0 \\varepsilon\\leq 0 italic_  0  and   \\emptyset   for    1  1 \\varepsilon\\geq 1 italic_  1 .",
            "The results of the USPS experiment is summarised in Figure  3  and Table  3 . As expected, both methods achieve the theoretical guarantee ( 16 ), but RF is quite a bit more conservative than CP, in particular early on, as can be seen in Figure  3 . In terms of mean prediction set size, there is not much difference between the methods, but the OE of RF is relatively high, which is likely due to the large prediction sets output by RF in the beginning.",
            "For the classification task in the USPS experiment, the picture is less clear. While the ICP based on random forest outperformed the confidence predictor based on the same algorithm in terms of observed excess, it is possible that better results could have been obtained by choosing a larger   1 subscript  1 \\varepsilon_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  to start the ACI iteration ( 15 ). It is clear from Figure  3  that the confidence predictor was way too conservative early on, and took some time to adapt. Much of the observed excess, i.e. the number of false labels included in the prediction sets, comes from the first 200 predictions, illustrating that there may be some utility in careful tuning of the initial control input   1 subscript  1 \\varepsilon_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Numerical simulation for synthetic data showing the average error rate, the fraction of infinite prediction intervals, and the average interval score for the finite intervals, averaged over 1000 independent trials.",
        "table": "S6.T1.1",
        "footnotes": [],
        "references": [
            "The rest of this paper is organised as follows. Section  2  introduces conformal and confidence predictors, as well as the relevant notation. Readers familiar with the terminology used in  [ 2 ]  may find it convenient to skip ahead to Section  3 , where ACI and its finite sample coverage guarantee is introduced, as well as a trivial but technically necessary extension of conformal and confidence predictors. Our main result is given in Section  4 , where we restate Lemma 4.1 in  [ 5 ] , which is the key result that is used to prove the finite sample guarantee of ACI. We indicate that this lemma does not rely on any particular property of conformal predictors, or even confidence predictors. We then argue in Section  5  that, while not strictly necessary for the finite sample guarantee of ACI, confidence predictors represent the natural way of predicting at a confidence level. Our numerical experiments on synthetic and real data are described in Section  6 , and Section  7  concludes. Additional details on some algorithms used in the experiments, and a summary of the results without using ACI (i.e. at a fixed significance level) are given in the appendices.",
            "In this paper, we have demonstrated that adaptive conformal inference (ACI) does not rely on any specific properties of conformal predictors to achieve its finite sample guarantee ( 16 ). In fact, it does not even need the more general concept of a confidence predictor, where the prediction sets are required to be nested (see Definition  1 ). However, we have argued that the property of nested prediction sets is the very least one should require when predicting with a confidence level. Without it, the coin flip predictor (Definition  4 ) is exactly valid but rather unhelpful. Since the validity guarantees of conformal predictors are lost if the exchangeability assumption is violated, and ACI provides finite sample coverage guarantees of another kind, we asked if anything is gained by using ACI with a conformal predictor, over using it with a confidence predictor in situations when exchangeability can not be expected.",
            "From Eq. ( 24 ) we see that the predictions  y ^ ^ y \\hat{y} over^ start_ARG italic_y end_ARG  for all objects  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  are given by",
            "For reference, we present the results attained by setting   = 0  0 \\gamma=0 italic_ = 0 , which corresponds to no using ACI at all. The results are summarised in Tables  4 - 6 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Numerical experiment on the Wine Quality dataset showing the mean error rate, interval scores for the finite prediction intervals, and the fraction of infinite intervals output. The results are averaged over 1000 independent trials.",
        "table": "S6.T2.1",
        "footnotes": [],
        "references": [
            "The rest of this paper is organised as follows. Section  2  introduces conformal and confidence predictors, as well as the relevant notation. Readers familiar with the terminology used in  [ 2 ]  may find it convenient to skip ahead to Section  3 , where ACI and its finite sample coverage guarantee is introduced, as well as a trivial but technically necessary extension of conformal and confidence predictors. Our main result is given in Section  4 , where we restate Lemma 4.1 in  [ 5 ] , which is the key result that is used to prove the finite sample guarantee of ACI. We indicate that this lemma does not rely on any particular property of conformal predictors, or even confidence predictors. We then argue in Section  5  that, while not strictly necessary for the finite sample guarantee of ACI, confidence predictors represent the natural way of predicting at a confidence level. Our numerical experiments on synthetic and real data are described in Section  6 , and Section  7  concludes. Additional details on some algorithms used in the experiments, and a summary of the results without using ACI (i.e. at a fixed significance level) are given in the appendices.",
            "The iteration ( 15 ) could cause   t  0 subscript  t 0 \\varepsilon_{t}\\leq 0 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  0  or    1  1 \\varepsilon\\geq 1 italic_  1 . Technically, prediction sets are undefined for confidence predictors, and in particular conformal predictors for    ( 0 , 1 )  0 1 \\varepsilon\\notin(0,1) italic_  ( 0 , 1 ) . Thus, we introduce a trivial extension that is compatible with ACI.",
            "For all  t  N t N t\\in\\mathbb{N} italic_t  blackboard_N ,   t  [   , 1 +  ] subscript  t  1  \\varepsilon_{t}\\in[-\\delta,1+\\delta] italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  [ - italic_ , 1 + italic_ ] , almost surely, if ACI ( 15 ) is used together with a set predictor that outputs   \\emptyset   is predicted for    1  1 \\varepsilon\\geq 1 italic_  1  and  Y Y \\boldsymbol{Y} bold_italic_Y  for    0  0 \\varepsilon\\leq 0 italic_  0 .",
            "Assume that the sequence  {  t } t  N subscript subscript  t t N \\{\\varepsilon_{t}\\}_{t\\in\\mathbb{N}} { italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t  blackboard_N end_POSTSUBSCRIPT  is such that  inf t  t <   subscript infimum t subscript  t  \\inf_{t}\\varepsilon_{t}<-\\gamma roman_inf start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT < - italic_  (the case when  sup t  t > 1 +  subscript supremum t subscript  t 1  \\sup_{t}\\varepsilon_{t}>1+\\gamma roman_sup start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT > 1 + italic_  is identical). By ( 15 ),  sup t |  t + 1   t | = sup t   |   err t  t | <  subscript supremum t subscript  t 1 subscript  t subscript supremum t   superscript subscript err t subscript  t  \\sup_{t}|\\varepsilon_{t+1}-\\varepsilon_{t}|=\\sup_{t}\\gamma|\\varepsilon-\\text{% err}_{t}^{\\varepsilon_{t}}|<\\gamma roman_sup start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_ start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | = roman_sup start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_ | italic_ - err start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT | < italic_ . Thus, with positive probability, we may find  t  N t N t\\in\\mathbb{N} italic_t  blackboard_N  such that   t < 0 subscript  t 0 \\varepsilon_{t}<0 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT < 0  and   t + 1 <  t subscript  t 1 subscript  t \\varepsilon_{t+1}<\\varepsilon_{t} italic_ start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT < italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . However, by assumption, and ( 15 ),",
            "In our evaluation, we have to modify the definition of OE slightly to account for the ACI update ( 15 ). We want to achieve the finite sample guarantee ( 16 ) with a target error rate    \\varepsilon italic_ . Then",
            "For the classification task in the USPS experiment, the picture is less clear. While the ICP based on random forest outperformed the confidence predictor based on the same algorithm in terms of observed excess, it is possible that better results could have been obtained by choosing a larger   1 subscript  1 \\varepsilon_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  to start the ACI iteration ( 15 ). It is clear from Figure  3  that the confidence predictor was way too conservative early on, and took some time to adapt. Much of the observed excess, i.e. the number of false labels included in the prediction sets, comes from the first 200 predictions, illustrating that there may be some utility in careful tuning of the initial control input   1 subscript  1 \\varepsilon_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Numerical experiment on the Wine Quality dataset showing the mean error rate, prediction set size, and observed excess (lower values are better for observed excess). The results are averaged over 1000 independent trials.",
        "table": "S6.T3.1",
        "footnotes": [],
        "references": [
            "The rest of this paper is organised as follows. Section  2  introduces conformal and confidence predictors, as well as the relevant notation. Readers familiar with the terminology used in  [ 2 ]  may find it convenient to skip ahead to Section  3 , where ACI and its finite sample coverage guarantee is introduced, as well as a trivial but technically necessary extension of conformal and confidence predictors. Our main result is given in Section  4 , where we restate Lemma 4.1 in  [ 5 ] , which is the key result that is used to prove the finite sample guarantee of ACI. We indicate that this lemma does not rely on any particular property of conformal predictors, or even confidence predictors. We then argue in Section  5  that, while not strictly necessary for the finite sample guarantee of ACI, confidence predictors represent the natural way of predicting at a confidence level. Our numerical experiments on synthetic and real data are described in Section  6 , and Section  7  concludes. Additional details on some algorithms used in the experiments, and a summary of the results without using ACI (i.e. at a fixed significance level) are given in the appendices.",
            "Interestingly, the proof of ( 16 ) does not use any property of a conformal, or even confidence predictor. In fact, all that is required of a predictor    \\Gamma roman_  is that   \\emptyset   is predicted for    1  1 \\varepsilon\\geq 1 italic_  1  and  Y Y \\boldsymbol{Y} bold_italic_Y  for    0  0 \\varepsilon\\leq 0 italic_  0 . For all other significance levels, even completely random prediction sets ensure ( 16 ). The result is proved by Lemma 4.1 in  [ 5 ] , which we reproduce here with notation slightly modified to align with ours.",
            "It is clear that no property of conformal, or even confidence predictors, is needed to achieve ( 16 ).",
            "This section argues that, while not strictly necessary for achieving the finite sample guarantee ( 16 ) of ACI, we should restrict ourselves to confidence predictors, as this is the most general type of predictor that satisfy natural assumptions on prediction sets at a confidence level.",
            "We have shown that ACI achieves the finite sample guarantee ( 16 ) for non-exchangeable data even for non confidence predictors, but we have argued that it would be absurd to drop the property of nested prediction sets. A natural question is whether using ACI with a conformal predictor has any advantages over using it together with a general confidence predictor. For non-exchangeable data, conformal predictors are not necessarily valid, which is also true for confidence predictors.",
            "In our evaluation, we have to modify the definition of OE slightly to account for the ACI update ( 15 ). We want to achieve the finite sample guarantee ( 16 ) with a target error rate    \\varepsilon italic_ . Then",
            "The results are shown in Figure  1  and Table  1 . The first thing to note is that both methods achieve ( 16 ) as expected. The gaps in the right-hand side plots in  1  indicate an infinite prediction interval, and the fraction of such infinite intervals is presented in Table  1 . We see that although the error rates are identical for both methods, CP outputs fewer infinite intervals in the second and third settings, with change points and drift. As can be seen in Table  1 , the difference in terms of average interval score (IS) is not large, but CP outputs fewer infinite intervals on average.",
            "Our results are summarised in Figure  2  and Table  2 . Again, both methods achieve the theoretical guarantee ( 16 ) as expected. Figure  2  summarises the mean running error rate, and the mean interval score (IS), averaged over the 1000 independent trials. It can be noted that CP is overly confident on average early on, while QRF is conservative. In terms of computational efficiency there is not much difference, ICP was introduced to overcome the often intractable computational cost of full CP, and the calibration step is quite fast. The mean error rate together with mean interval scores and the mean fraction of infinite intervals is presented in Table  2 . It is worth pointing out that QRF produced infinite intervals at all in only eight out of 1000 trials, while CP did so in 696 trials.",
            "The results of the USPS experiment is summarised in Figure  3  and Table  3 . As expected, both methods achieve the theoretical guarantee ( 16 ), but RF is quite a bit more conservative than CP, in particular early on, as can be seen in Figure  3 . In terms of mean prediction set size, there is not much difference between the methods, but the OE of RF is relatively high, which is likely due to the large prediction sets output by RF in the beginning.",
            "In this paper, we have demonstrated that adaptive conformal inference (ACI) does not rely on any specific properties of conformal predictors to achieve its finite sample guarantee ( 16 ). In fact, it does not even need the more general concept of a confidence predictor, where the prediction sets are required to be nested (see Definition  1 ). However, we have argued that the property of nested prediction sets is the very least one should require when predicting with a confidence level. Without it, the coin flip predictor (Definition  4 ) is exactly valid but rather unhelpful. Since the validity guarantees of conformal predictors are lost if the exchangeability assumption is violated, and ACI provides finite sample coverage guarantees of another kind, we asked if anything is gained by using ACI with a conformal predictor, over using it with a confidence predictor in situations when exchangeability can not be expected.",
            "For reference, we present the results attained by setting   = 0  0 \\gamma=0 italic_ = 0 , which corresponds to no using ACI at all. The results are summarised in Tables  4 - 6 ."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A3.T4.1",
        "footnotes": [],
        "references": [
            "The rest of this paper is organised as follows. Section  2  introduces conformal and confidence predictors, as well as the relevant notation. Readers familiar with the terminology used in  [ 2 ]  may find it convenient to skip ahead to Section  3 , where ACI and its finite sample coverage guarantee is introduced, as well as a trivial but technically necessary extension of conformal and confidence predictors. Our main result is given in Section  4 , where we restate Lemma 4.1 in  [ 5 ] , which is the key result that is used to prove the finite sample guarantee of ACI. We indicate that this lemma does not rely on any particular property of conformal predictors, or even confidence predictors. We then argue in Section  5  that, while not strictly necessary for the finite sample guarantee of ACI, confidence predictors represent the natural way of predicting at a confidence level. Our numerical experiments on synthetic and real data are described in Section  6 , and Section  7  concludes. Additional details on some algorithms used in the experiments, and a summary of the results without using ACI (i.e. at a fixed significance level) are given in the appendices.",
            "For all three experiments on synthetic data, the first 100 examples are used as initial training set, and we run them 1000 times using different random seeds (ranging from 0 to 999). In all our experiments, we ask for a target error rate   = 0.1  0.1 \\varepsilon=0.1 italic_ = 0.1 , and set   1 =  subscript  1  \\varepsilon_{1}=\\varepsilon italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_ . Choosing the step size    \\gamma italic_  according to ( 17 ) with   = 0.05  0.05 \\delta=0.05 italic_ = 0.05  would suggest    0.0096  0.0096 \\gamma\\approx 0.0096 italic_  0.0096 .",
            "Predicting the label can be seen either as regression or classification, and we choose to perform the regression task. For confidence predictor, we use the  RandomForestQuantileRegressor  from the  quantile-forest  python package  [ 17 ] . We turn it into an inductive conformal predictor (ICP) by using the  WrapRegressor  class from the  crepes  python package  [ 18 ] . We run our experiment 1000 times using different random seeds for each run. For the ICP, we must set aside a calibration set that is not used in training. Thus, we use 3/4 of the training set as proper training set, and the rest as calibration set. A different random split is used for each run. As a result, the training set for our confidence predictor (QRF) has size 4898 while the ICP has a proper training set size of just 3673 because the remaining data is used for calibration. Again, we choose step size according to ( 17 ) with   = 0.05  0.05 \\delta=0.05 italic_ = 0.05 . With  N = 1599 N 1599 N=1599 italic_N = 1599  we should choose    0.011  0.011 \\gamma\\approx 0.011 italic_  0.011 .",
            "The US Postal Service (USPS) dataset consists of 7291 training images and 2007 test images of handwritten digits from 0 to 9. The images are  16  16 16 16 16\\times 16 16  16  greyscale pixels. Exchangeability of the USPS dataset has been studied in  [ 19 ,  20 ] , and it is well known that the examples are not perfectly exchangeable. We use a  RandomForestClassifier  from the  scikit-learn  python package  [ 21 ]  with the default parameters, e.g. the forest consists of 100 trees. Again, we use the  crepes  python package  [ 18 ] , sacrificing 1/4 of the training data for calibration to train an ICP. We also define a confidence predictor by using the  predict_proba  method, which outputs estimated probabilities for every possible label (details in Appendix  B ). Choosing step size according to ( 17 ) with   = 0.05  0.05 \\delta=0.05 italic_ = 0.05  and  N = 2007 N 2007 N=2007 italic_N = 2007  leads to    0.009  0.009 \\gamma\\approx 0.009 italic_  0.009 ."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A3.T5.1",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "",
        "table": "A3.T6.1",
        "footnotes": [],
        "references": []
    }
}