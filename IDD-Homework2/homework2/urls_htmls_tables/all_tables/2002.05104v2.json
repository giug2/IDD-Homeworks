{
    "S5.T1": {
        "caption": "TABLE I: Impact of the network used for visual feature extraction.",
        "table": "<table id=\"S5.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.1.1.1.1\" class=\"ltx_text ltx_font_italic\">Embedding</span></th>\n<th id=\"S5.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">RNN</th>\n<th id=\"S5.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Network</th>\n<th id=\"S5.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Training</th>\n<th id=\"S5.T1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Validation</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S5.T1.1.2.1.1.1\" class=\"ltx_text\">BERT</span></td>\n<td id=\"S5.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S5.T1.1.2.1.2.1\" class=\"ltx_text\">GRU</span></td>\n<td id=\"S5.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Faster</td>\n<td id=\"S5.T1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">79.34</td>\n<td id=\"S5.T1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.2.1.5.1\" class=\"ltx_text ltx_font_bold\">58.88</span></td>\n</tr>\n<tr id=\"S5.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">ResNet-101</td>\n<td id=\"S5.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">76.14</td>\n<td id=\"S5.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">56.09</td>\n</tr>\n<tr id=\"S5.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">VGG-16</td>\n<td id=\"S5.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">65.59</td>\n<td id=\"S5.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">53.49</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Table I illustrates the result of this experiment. Intuitively, visual features provide a larger impact on model’s performance. The accuracy difference between the best and the worst performing approaches is ≈5%absentpercent5\\approx 5\\%. That difference accounts for roughly 10,0001000010,000 validation set instances.\nVGG-16 visual features presented the worst accuracy, but that was expected since it is the oldest network used in this study. In addition, it is only sixteen layers deep, and it has been shown that the depth of the network is quite important to hierarchically encode complex structures. Moreover, VGG-16 architecture encodes all the information in a 409640964096 dimensional vector that is extracted after the second fully-connected layer at the end. That vector encodes little to none spatial information, which makes it almost impossible for the network to answer questions on the spatial positioning of objects."
        ]
    },
    "S5.T2": {
        "caption": "TABLE II: Experiment using different fusion strategies.",
        "table": "<table id=\"S5.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T2.1.1.1.1.1\" class=\"ltx_text ltx_font_italic\">Embedding</span></th>\n<th id=\"S5.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">RNN</th>\n<th id=\"S5.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Fusion</th>\n<th id=\"S5.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Training</th>\n<th id=\"S5.T2.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Validation</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S5.T2.1.2.1.1.1\" class=\"ltx_text\">BERT</span></td>\n<td id=\"S5.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S5.T2.1.2.1.2.1\" class=\"ltx_text\">GRU</span></td>\n<td id=\"S5.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Mult</td>\n<td id=\"S5.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">78.28</td>\n<td id=\"S5.T2.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T2.1.2.1.5.1\" class=\"ltx_text ltx_font_bold\">58.75</span></td>\n</tr>\n<tr id=\"S5.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Concat</td>\n<td id=\"S5.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">67.85</td>\n<td id=\"S5.T2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">55.07</td>\n</tr>\n<tr id=\"S5.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">Sum</td>\n<td id=\"S5.T2.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">68.21</td>\n<td id=\"S5.T2.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">54.93</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Table II presents the experimental results with the fusion strategies. The best result is obtained using the element-wise multiplication. Such an approach functions as a filtering strategy that is able to scale down the importance of irrelevant dimensions from the visual-question feature vectors. In other words, vector dimensions with high cross-modal affinity will have their magnitudes increased, differently from the uncorrelated ones that will have their values reduced. Summation does provide the worst results overall, closely followed by the concatenation operator. Moreover, among all the fusion strategies used in this study, multiplication seems to ease the training process as it presents a much higher training set accuracy (≈11%absentpercent11\\approx 11\\% improvement) as well."
        ]
    },
    "S5.T3": {
        "caption": "TABLE III: Experiment using different attention mechanisms.",
        "table": "<table id=\"S5.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T3.1.2.1.1.1\" class=\"ltx_text ltx_font_italic\">Embedding</span></th>\n<th id=\"S5.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">RNN</th>\n<th id=\"S5.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Attention</th>\n<th id=\"S5.T3.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Training</th>\n<th id=\"S5.T3.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Validation</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.1.3.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S5.T3.1.3.1.1.1\" class=\"ltx_text\">BERT</span></td>\n<td id=\"S5.T3.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S5.T3.1.3.1.2.1\" class=\"ltx_text\">GRU</span></td>\n<td id=\"S5.T3.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-</td>\n<td id=\"S5.T3.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">78.20</td>\n<td id=\"S5.T3.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">58.75</td>\n</tr>\n<tr id=\"S5.T3.1.4.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Co-Attention</td>\n<td id=\"S5.T3.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">71.10</td>\n<td id=\"S5.T3.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">58.54</td>\n</tr>\n<tr id=\"S5.T3.1.5.3\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Co-Attention (L2 norm)</td>\n<td id=\"S5.T3.1.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">86.03</td>\n<td id=\"S5.T3.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">64.03</td>\n</tr>\n<tr id=\"S5.T3.1.6.4\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Top Down</td>\n<td id=\"S5.T3.1.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">82.64</td>\n<td id=\"S5.T3.1.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">62.37</td>\n</tr>\n<tr id=\"S5.T3.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">Top Down (<math id=\"S5.T3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma=\" display=\"inline\"><semantics id=\"S5.T3.1.1.1.m1.1a\"><mrow id=\"S5.T3.1.1.1.m1.1.1\" xref=\"S5.T3.1.1.1.m1.1.1.cmml\"><mi id=\"S5.T3.1.1.1.m1.1.1.2\" xref=\"S5.T3.1.1.1.m1.1.1.2.cmml\">σ</mi><mo id=\"S5.T3.1.1.1.m1.1.1.1\" xref=\"S5.T3.1.1.1.m1.1.1.1.cmml\">=</mo><mi id=\"S5.T3.1.1.1.m1.1.1.3\" xref=\"S5.T3.1.1.1.m1.1.1.3.cmml\"></mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.1.1.1.m1.1b\"><apply id=\"S5.T3.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.1.1.1.m1.1.1\"><eq id=\"S5.T3.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T3.1.1.1.m1.1.1.1\"></eq><ci id=\"S5.T3.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T3.1.1.1.m1.1.1.2\">𝜎</ci><csymbol cd=\"latexml\" id=\"S5.T3.1.1.1.m1.1.1.3.cmml\" xref=\"S5.T3.1.1.1.m1.1.1.3\">absent</csymbol></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.1.1.1.m1.1c\">\\sigma=</annotation></semantics></math>ReLU)</td>\n<td id=\"S5.T3.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">87.02</td>\n<td id=\"S5.T3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S5.T3.1.1.3.1\" class=\"ltx_text ltx_font_bold\">64.12</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Table III depicts the results obtained by adding the attention mechanisms to the baseline model. For these experiments we used only element-wise multiplication as fusion strategy, given that it presented the best performance in our previous experiments. We observe that attention is a crucial mechanism for VQA, leading to an ≈6%absentpercent6\\approx 6\\% accuracy improvement."
        ]
    },
    "S5.T4": {
        "caption": "TABLE IV: Comparison of the models on VQA2 Test-Standard set. The models were trained on the union of VQA 2.0 trainval split and VisualGenome [38] train split. All is the overall OpenEnded accuracy (higher is better). Yes/No, Numbers, and Others are subsets that correspond to answers types. * scores reported from [21].",
        "table": "<table id=\"S5.T4.9\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.9.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T4.9.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S5.T4.9.1.1.1.1\" class=\"ltx_text\">Model</span></td>\n<td id=\"S5.T4.9.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"4\">VQA2.0 Test-Dev</td>\n<td id=\"S5.T4.9.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"4\">VQA2.0 Test-Std</td>\n</tr>\n<tr id=\"S5.T4.9.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T4.9.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">All</td>\n<td id=\"S5.T4.9.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Yes/No</td>\n<td id=\"S5.T4.9.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Num.</td>\n<td id=\"S5.T4.9.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Other</td>\n<td id=\"S5.T4.9.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">All</td>\n<td id=\"S5.T4.9.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Yes/No</td>\n<td id=\"S5.T4.9.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Num.</td>\n<td id=\"S5.T4.9.2.2.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Other</td>\n</tr>\n<tr id=\"S5.T4.9.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T4.9.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">MCB* <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">23</a>]</cite>\n</td>\n<td id=\"S5.T4.9.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-</td>\n<td id=\"S5.T4.9.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-</td>\n<td id=\"S5.T4.9.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-</td>\n<td id=\"S5.T4.9.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-</td>\n<td id=\"S5.T4.9.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">62.27</td>\n<td id=\"S5.T4.9.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">78.82</td>\n<td id=\"S5.T4.9.3.3.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">38.28</td>\n<td id=\"S5.T4.9.3.3.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">53.36</td>\n</tr>\n<tr id=\"S5.T4.9.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T4.9.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">ReasonNet* <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib53\" title=\"\" class=\"ltx_ref\">53</a>]</cite>\n</td>\n<td id=\"S5.T4.9.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td id=\"S5.T4.9.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td id=\"S5.T4.9.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td id=\"S5.T4.9.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td id=\"S5.T4.9.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_r\">64.61</td>\n<td id=\"S5.T4.9.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_r\">78.86</td>\n<td id=\"S5.T4.9.4.4.8\" class=\"ltx_td ltx_align_center ltx_border_r\">41.98</td>\n<td id=\"S5.T4.9.4.4.9\" class=\"ltx_td ltx_align_center ltx_border_r\">57.39</td>\n</tr>\n<tr id=\"S5.T4.9.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T4.9.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Tips&amp;Tricks* <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">54</a>]</cite>\n</td>\n<td id=\"S5.T4.9.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">65.32</td>\n<td id=\"S5.T4.9.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">81.82</td>\n<td id=\"S5.T4.9.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">44.21</td>\n<td id=\"S5.T4.9.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\">56.05</td>\n<td id=\"S5.T4.9.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_r\">65.67</td>\n<td id=\"S5.T4.9.5.5.7\" class=\"ltx_td ltx_align_center ltx_border_r\">82.20</td>\n<td id=\"S5.T4.9.5.5.8\" class=\"ltx_td ltx_align_center ltx_border_r\">43.90</td>\n<td id=\"S5.T4.9.5.5.9\" class=\"ltx_td ltx_align_center ltx_border_r\">56.26</td>\n</tr>\n<tr id=\"S5.T4.9.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T4.9.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">\n<span id=\"S5.T4.9.6.6.1.1\" class=\"ltx_text ltx_font_smallcaps\">block</span>* <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a>]</cite>\n</td>\n<td id=\"S5.T4.9.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">67.58</td>\n<td id=\"S5.T4.9.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">83.60</td>\n<td id=\"S5.T4.9.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\">47.33</td>\n<td id=\"S5.T4.9.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\">58.51</td>\n<td id=\"S5.T4.9.6.6.6\" class=\"ltx_td ltx_align_center ltx_border_r\">67.92</td>\n<td id=\"S5.T4.9.6.6.7\" class=\"ltx_td ltx_align_center ltx_border_r\">83.98</td>\n<td id=\"S5.T4.9.6.6.8\" class=\"ltx_td ltx_align_center ltx_border_r\">46.77</td>\n<td id=\"S5.T4.9.6.6.9\" class=\"ltx_td ltx_align_center ltx_border_r\">58.79</td>\n</tr>\n<tr id=\"S5.T4.9.7.7\" class=\"ltx_tr\">\n<td id=\"S5.T4.9.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">BERT-GRU-Faster-TopDown</td>\n<td id=\"S5.T4.9.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">67.16</td>\n<td id=\"S5.T4.9.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">84.76</td>\n<td id=\"S5.T4.9.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">44.82</td>\n<td id=\"S5.T4.9.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">57.23</td>\n<td id=\"S5.T4.9.7.7.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">67.28</td>\n<td id=\"S5.T4.9.7.7.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">84.75</td>\n<td id=\"S5.T4.9.7.7.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">44.90</td>\n<td id=\"S5.T4.9.7.7.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">57.20</td>\n</tr>\n<tr id=\"S5.T4.9.8.8\" class=\"ltx_tr\">\n<td id=\"S5.T4.9.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\">BERT-GRU-Faster-CoAttention</td>\n<td id=\"S5.T4.9.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">67.18</td>\n<td id=\"S5.T4.9.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">84.85</td>\n<td id=\"S5.T4.9.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">45.92</td>\n<td id=\"S5.T4.9.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">56.84</td>\n<td id=\"S5.T4.9.8.8.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">67.39</td>\n<td id=\"S5.T4.9.8.8.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">85.00</td>\n<td id=\"S5.T4.9.8.8.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">46.20</td>\n<td id=\"S5.T4.9.8.8.9\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">56.91</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Tables IV and V show that our best architecture outperforms all competitors but block, in both Test-Standard (Table IV) and Test-Dev sets (Table V). Despite block presenting a marginal advantage in accuracy, we have shown in this paper that by carefully analyzing each individual component we are capable of generating a method, without any bells and whistles, that is on par with much more complex methods.\nFor instance, block and MCB require 18M and 32M parameters respectively for the fusion scheme alone, while our fusion approach is parameter-free.\nMoreover, our model performs far better than [23], [53], and [54], which are also arguably much more complex methods."
        ]
    },
    "S5.T5": {
        "caption": "TABLE V: Comparison of the models on VQA2 test-dev set. All is the overall OpenEnded accuracy (higher is better). Yes/No, Numbers, and Others are subsets that correspond to answers types. * scores reported from [21].",
        "table": "<table id=\"S5.T5.9\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T5.9.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.9.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S5.T5.9.1.1.1.1\" class=\"ltx_text\">Model</span></th>\n<td id=\"S5.T5.9.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"4\">VQA2.0 Test-Dev</td>\n</tr>\n<tr id=\"S5.T5.9.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T5.9.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">All</td>\n<td id=\"S5.T5.9.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Yes/No</td>\n<td id=\"S5.T5.9.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Num.</td>\n<td id=\"S5.T5.9.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Other</td>\n</tr>\n<tr id=\"S5.T5.9.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T5.9.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S5.T5.9.3.3.1.1\" class=\"ltx_text ltx_font_smallcaps\">Deeper-lstm-q</span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">4</a>]</cite>\n</th>\n<td id=\"S5.T5.9.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">51.95</td>\n<td id=\"S5.T5.9.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">70.42</td>\n<td id=\"S5.T5.9.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32.28</td>\n<td id=\"S5.T5.9.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">40.64</td>\n</tr>\n<tr id=\"S5.T5.9.4.4\" class=\"ltx_tr\">\n<th id=\"S5.T5.9.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">MCB* <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">23</a>]</cite>\n</th>\n<td id=\"S5.T5.9.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">61.23</td>\n<td id=\"S5.T5.9.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">79.73</td>\n<td id=\"S5.T5.9.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">39.13</td>\n<td id=\"S5.T5.9.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\">50.45</td>\n</tr>\n<tr id=\"S5.T5.9.5.5\" class=\"ltx_tr\">\n<th id=\"S5.T5.9.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">\n<span id=\"S5.T5.9.5.5.1.1\" class=\"ltx_text ltx_font_smallcaps\">block</span>* <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a>]</cite>\n</th>\n<td id=\"S5.T5.9.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">66.41</td>\n<td id=\"S5.T5.9.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">82.86</td>\n<td id=\"S5.T5.9.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">44.76</td>\n<td id=\"S5.T5.9.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\">57.30</td>\n</tr>\n<tr id=\"S5.T5.9.6.6\" class=\"ltx_tr\">\n<th id=\"S5.T5.9.6.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">BERT-GRU-Faster-CoAttention</th>\n<td id=\"S5.T5.9.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">65.84</td>\n<td id=\"S5.T5.9.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">83.66</td>\n<td id=\"S5.T5.9.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">44.36</td>\n<td id=\"S5.T5.9.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">55.50</td>\n</tr>\n<tr id=\"S5.T5.9.7.7\" class=\"ltx_tr\">\n<th id=\"S5.T5.9.7.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">BERT-GRU-Faster-TopDown</th>\n<td id=\"S5.T5.9.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">66.02</td>\n<td id=\"S5.T5.9.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">83.72</td>\n<td id=\"S5.T5.9.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">44.88</td>\n<td id=\"S5.T5.9.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">55.77</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Tables IV and V show that our best architecture outperforms all competitors but block, in both Test-Standard (Table IV) and Test-Dev sets (Table V). Despite block presenting a marginal advantage in accuracy, we have shown in this paper that by carefully analyzing each individual component we are capable of generating a method, without any bells and whistles, that is on par with much more complex methods.\nFor instance, block and MCB require 18M and 32M parameters respectively for the fusion scheme alone, while our fusion approach is parameter-free.\nMoreover, our model performs far better than [23], [53], and [54], which are also arguably much more complex methods."
        ]
    }
}