{
    "id_table_1": {
        "caption": "Table 1:    Model performances on RAG tasks. Pop, TQA, Pub, Bio, Hotpot, MuSiQue, 2WikiMH denote PopQA, TriviaQA, PubHealth, Biography Generations, HotpotQA, MuSiQue-Ans, 2WikiMultihopQA. Acc, FS, SM, rg, mau, EM, and F1 denote accuracy, FactScore (factuality), str-em, rouge (correctness), MAUVE (fluency), exact match, and F1 scores.  # : evaluated using gpt-3.5-turbo-instruct instead of text-davinci-003.   : using 4-bit quantized model.   : using a proprietary retriever with Tree-of-Thought prompting.   :  Open-RAG  model with 7.8B total and 7.0B active parameters.   Gray  results are best performances with larger/proprietary models.",
        "table": "S3.T1.22.22",
        "footnotes": [],
        "references": [
            "If no retrieval is needed,  M G subscript M G \\mathcal{M}_{G} caligraphic_M start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT   generates the response using only the parametric knowledge of the LLM (i.e., return  o o o italic_o  as  y p  r  e  d subscript y p r e d y_{pred} italic_y start_POSTSUBSCRIPT italic_p italic_r italic_e italic_d end_POSTSUBSCRIPT ). If retrieval is needed, for both single- or multiple-hop from an external knowledge source  D = { d i } i = 1 N d D superscript subscript subscript d i i 1 subscript N d D=\\{d_{i}\\}_{i=1}^{N_{d}} italic_D = { italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , we use a user-defined frozen retriever  R R R italic_R  to retrieve the top- k k k italic_k  documents  S = { s t } t = 1 k S superscript subscript subscript s t t 1 k S=\\{s_{t}\\}_{t=1}^{k} italic_S = { italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , where each  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  consists of  { r j } j = 1 N H superscript subscript subscript r j j 1 subscript N H \\{r_{j}\\}_{j=1}^{N_{H}} { italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  with  r j  D subscript r j D r_{j}\\in D italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  italic_D  and  N H subscript N H N_{H} italic_N start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT  denoting the hop size. For each retrieved content  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ,  M G subscript M G \\mathcal{M}_{G} caligraphic_M start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT   generates a  Relevance  token, the output response  y t subscript y t y_{t} italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , a  Grounding  token, and a  Utility  token.  The  Relevance  tokens ( [Relevant/Irrelevant] ) indicate if  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is relevant to  q q q italic_q , the  Grounding  tokens ( [Fully Supported/Partially Supported/No Support] ) indicate if  y t subscript y t y_{t} italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is supported by  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , and the  Utility  tokens ( [U:1] - [U:5] ) define how useful  y t subscript y t y_{t} italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is to  q q q italic_q . We process each  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  in parallel and generate the final answer  y p  r  e  d subscript y p r e d y_{pred} italic_y start_POSTSUBSCRIPT italic_p italic_r italic_e italic_d end_POSTSUBSCRIPT  by ranking them (i.e., all  y t subscript y t y_{t} italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) based on the weighted sum of the normalized confidence of the corresponding predicted  Relevance ,  Grounding , and  Utility  tokens 3 3 3 For long-form generation, we use the same segment-level beam search strategy as in Self-RAG  Asai et al. ( 2024 )  to obtain the  Top- B B B italic_B  segments, where  B B B italic_B  is the beam size, and return the best sequence at the end of generation.  (see Figure  1 ).",
            "Here, we discuss our training data collection (Sec  2.2.1 ) and  parameter-efficient  MoE fine-tuning (Sec  2.2.2 ).",
            "To empower  Open-RAG  to tackle retrieval-free queries, as well as single- and multi-hop queries that require retrieval, we build our training data using various types of tasks and datasets. Given an input-output data pair ( q q q italic_q ,  y y y italic_y ) in an original dataset, we augment the data with  reflection  tokens (Sec.  2.1 ) leveraging ground truth annotation or critic LLM  C C C italic_C  to create supervised data. If the corresponding  Retrieval  token added by  C C C italic_C  is  [RT] , we further augment the data and create three different new instances accordingly as follows. First, we use  R R R italic_R  to retrieve the top- k k k italic_k  documents  S S S italic_S . For each retrieved document  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ,  C C C italic_C  evaluates whether  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is relevant or not and returns the  Relevance  token. To address both single- and multi-hop queries, we equip our data pipeline with a hop-unified heuristic: if at least one passage  { r j }  s t subscript r j subscript s t \\{r_{j}\\}\\in s_{t} { italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT }  italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is relevant, we add the  Relevance  token as  [Relevant] ; otherwise, we use  [Irrelevant] . When  [Relevant]  is predicted, to enable  M G subscript M G \\mathcal{M}_{G} caligraphic_M start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT   to contrast between useful and distractor contexts in  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  in a more fine-grained way, we design a data-contrastive heuristic: (i) for single-hop RAG datasets, we use  C C C italic_C  directly to label the  Grounding  token; (ii) for multi-hop RAG datasets, if all passages  { r j }  s t subscript r j subscript s t \\{r_{j}\\}\\in s_{t} { italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT }  italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  are individually predicted as  [RT] , then we add  [Fully Supported]  as the  Grounding  token; otherwise, we use  [Partially Supported] . Finally, regardless of the prediction of the  Relevance  token, we use  C C C italic_C  to provide a  Utility  score for  y y y italic_y  with respect to  q q q italic_q . We depict an example of the training data collection for a 2-hop question in Figure  2 .",
            "Training Data and Settings.   In our data curation process, as detailed in Section  2.2.1 , we compile a diverse set of instruction-following input-output pairs encompassing retrieval-free, single-hop, and multi-hop datasets requiring retrieval. For no-retrieval and single-hop datasets, we utilize 150K instruction-output pairs curated by Self-RAG.  For the multi-hop dataset, we randomly sample 16K two-hop instances from the HotpotQA  Yang et al. ( 2018b )  Distractor train split, each with 10 passages annotated with the ground truth  Relevance  tokens. Using our data collection method from Section  2.2.1 , we generate 28K new multi-hop training instances. All other  reflection  tokens are labeled by the Llama2 7B 7B {}_{\\textsc{7B}} start_FLOATSUBSCRIPT 7B end_FLOATSUBSCRIPT   Touvron et al. ( 2023 )  critic LLM in Self-RAG, which is distilled from GPT-4. Additional information regarding training is provided in Appendix Section  A . Following previous works and for a fair comparison, we use the Llama2 7B 7B {}_{\\textsc{7B}} start_FLOATSUBSCRIPT 7B end_FLOATSUBSCRIPT    Touvron et al. ( 2023 )  as the base RAG model  M G subscript M G \\mathcal{M}_{G} caligraphic_M start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT .  Open-RAG  is transformed into a MoE model with  N E = 8 subscript N E 8 N_{E}=8 italic_N start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT = 8  and  k = 2 k 2 k=2 italic_k = 2 , incorporating adapters with a dimension of  512 512 512 512 , totaling an additional (8  \\times  135M) adapter model parameters. Moreover, we train a larger version of  Open-RAG  based on Llama2 13B 13B {}_{\\textsc{13B}} start_FLOATSUBSCRIPT 13B end_FLOATSUBSCRIPT   with additional (8  \\times  213M) parameters to demonstrate the scalability of our framework. By  Open-RAG  model, we indicate  Open-RAG 7B+8  135M 7B+8 135M {}_{\\textsc{7B+8}\\times\\textsc{135M}} start_FLOATSUBSCRIPT 7B+8  135M end_FLOATSUBSCRIPT   if not explicitly mentioned.",
            "Comparison against baselines without retrieval.   Table  1  (top and middle blocks) shows the performance of open-source baselines without retrieval.  Open-RAG  demonstrates substantial performance gains over all supervised fine-tuned LLMs, many of which are larger in size (e.g., 65B CoVE) and even our  Open-RAG  outperforms ChatGPT across all metrics and tasks. Particularly in multi-hop reasoning tasks such as HotpotQA,  Open-RAG  achieves a significant EM score of 63.3%, surpassing Alpaca 13B 13B {}_{\\textsc{13B}} start_FLOATSUBSCRIPT 13B end_FLOATSUBSCRIPT s 0.7%. In contrast, while ChatGPT achieves a decent score of 22.4% EM in HotpotQA, its performance drops notably in other multi-hop tasks like MuSiQue, where it achieves only 3.1% EM while  Open-RAG  achieves a much higher score of 41.6% EM in MuSiQue, highlighting its robustness and effectiveness in complex query handling compared to both open-source and proprietary LLMs.",
            "Comparison against baselines with retrieval.  As shown in Table  1  (bottom),  Open-RAG  consistently outperforms existing open-source RAG models, even those larger in size. It achieves the top performance among non-proprietary LM-based models across all tasks, with the exception of TriviaQA and PubQA, where it is marginally surpassed (by 1.2% and 0.4%, respectively) by the larger Self-RAG 13B 13B {}_{\\textsc{13B}} start_FLOATSUBSCRIPT 13B end_FLOATSUBSCRIPT  model, and by Alpaca 13B 13B {}_{\\textsc{13B}} start_FLOATSUBSCRIPT 13B end_FLOATSUBSCRIPT  in a single metric within the ALCE-ASQA dataset.",
            "The complete breakdown of  Open-RAG  training dataset is displayed in Table  4 . Algorithm  1  shows the process of the multi-hop training data preparation.",
            "The weights of the  Relevance ,  Grounding   and  Utility   tokens types are 1.0, 1.0, and 0.5 respectively during inference of  Open-RAG  and Self-RAG. During long-form generation, we use the maximum depth of search of 7 and the size of the beam of 2 following Self-RAG. To evaluate the performance in the retrieval setting, we report the performance in the always retrieval setup in Table  1 .  Next, we employ greedy decoding for  Open-RAG  and Self-RAG; and top- p p p italic_p  (nucleus) sampling for open baseline models with temperature 0.8 and  p = 0.95 p 0.95 p=0.95 italic_p = 0.95 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Ablation study model performances",
        "table": "S4.T2.2.2",
        "footnotes": [],
        "references": [
            "Here, we discuss our training data collection (Sec  2.2.1 ) and  parameter-efficient  MoE fine-tuning (Sec  2.2.2 ).",
            "To empower  Open-RAG  to tackle retrieval-free queries, as well as single- and multi-hop queries that require retrieval, we build our training data using various types of tasks and datasets. Given an input-output data pair ( q q q italic_q ,  y y y italic_y ) in an original dataset, we augment the data with  reflection  tokens (Sec.  2.1 ) leveraging ground truth annotation or critic LLM  C C C italic_C  to create supervised data. If the corresponding  Retrieval  token added by  C C C italic_C  is  [RT] , we further augment the data and create three different new instances accordingly as follows. First, we use  R R R italic_R  to retrieve the top- k k k italic_k  documents  S S S italic_S . For each retrieved document  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ,  C C C italic_C  evaluates whether  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is relevant or not and returns the  Relevance  token. To address both single- and multi-hop queries, we equip our data pipeline with a hop-unified heuristic: if at least one passage  { r j }  s t subscript r j subscript s t \\{r_{j}\\}\\in s_{t} { italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT }  italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is relevant, we add the  Relevance  token as  [Relevant] ; otherwise, we use  [Irrelevant] . When  [Relevant]  is predicted, to enable  M G subscript M G \\mathcal{M}_{G} caligraphic_M start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT   to contrast between useful and distractor contexts in  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  in a more fine-grained way, we design a data-contrastive heuristic: (i) for single-hop RAG datasets, we use  C C C italic_C  directly to label the  Grounding  token; (ii) for multi-hop RAG datasets, if all passages  { r j }  s t subscript r j subscript s t \\{r_{j}\\}\\in s_{t} { italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT }  italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  are individually predicted as  [RT] , then we add  [Fully Supported]  as the  Grounding  token; otherwise, we use  [Partially Supported] . Finally, regardless of the prediction of the  Relevance  token, we use  C C C italic_C  to provide a  Utility  score for  y y y italic_y  with respect to  q q q italic_q . We depict an example of the training data collection for a 2-hop question in Figure  2 .",
            "Training Data and Settings.   In our data curation process, as detailed in Section  2.2.1 , we compile a diverse set of instruction-following input-output pairs encompassing retrieval-free, single-hop, and multi-hop datasets requiring retrieval. For no-retrieval and single-hop datasets, we utilize 150K instruction-output pairs curated by Self-RAG.  For the multi-hop dataset, we randomly sample 16K two-hop instances from the HotpotQA  Yang et al. ( 2018b )  Distractor train split, each with 10 passages annotated with the ground truth  Relevance  tokens. Using our data collection method from Section  2.2.1 , we generate 28K new multi-hop training instances. All other  reflection  tokens are labeled by the Llama2 7B 7B {}_{\\textsc{7B}} start_FLOATSUBSCRIPT 7B end_FLOATSUBSCRIPT   Touvron et al. ( 2023 )  critic LLM in Self-RAG, which is distilled from GPT-4. Additional information regarding training is provided in Appendix Section  A . Following previous works and for a fair comparison, we use the Llama2 7B 7B {}_{\\textsc{7B}} start_FLOATSUBSCRIPT 7B end_FLOATSUBSCRIPT    Touvron et al. ( 2023 )  as the base RAG model  M G subscript M G \\mathcal{M}_{G} caligraphic_M start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT .  Open-RAG  is transformed into a MoE model with  N E = 8 subscript N E 8 N_{E}=8 italic_N start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT = 8  and  k = 2 k 2 k=2 italic_k = 2 , incorporating adapters with a dimension of  512 512 512 512 , totaling an additional (8  \\times  135M) adapter model parameters. Moreover, we train a larger version of  Open-RAG  based on Llama2 13B 13B {}_{\\textsc{13B}} start_FLOATSUBSCRIPT 13B end_FLOATSUBSCRIPT   with additional (8  \\times  213M) parameters to demonstrate the scalability of our framework. By  Open-RAG  model, we indicate  Open-RAG 7B+8  135M 7B+8 135M {}_{\\textsc{7B+8}\\times\\textsc{135M}} start_FLOATSUBSCRIPT 7B+8  135M end_FLOATSUBSCRIPT   if not explicitly mentioned.",
            "Inference Data and Settings.   We assign the default weight of 1.0, 1.0, and 0.5 to  Relevance ,  Grounding , and  Utility  tokens respectively. Following Self-RAG,  we compare the model performances with always retrieval and vary the retrieval frequency as discussed in Sec  2.3  only to demonstrate optimum thresholding and performance-speed trade-offs. In multi-hop evaluations, from the corresponding retrieval candidate passages, we use Beam Retriever  Zhang et al. ( 2024a )  to retrieve  Top - 3 3 3 3  multi-hop contexts, each with the mentioned  N H subscript N H N_{H} italic_N start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT  number of passages. For single-hop tasks, we use Self-RAGs setup (See Appendix   B ).",
            "As discussed in Sec  2.3 , given the query, adaptive retrieval method provides a probability/confidence score from the model. By thresholding on that score, we can control the retrieval frequency and balance the performance-speed trade-off and this can also guide to determine when retrieval is needed. A better scoring method should achieve higher accuracy at any retrieval frequency.  In order to demonstrate our hybrid adaptive retrieval scoring over the existing reflection token probability-based method  f r  e  t subscript f r e t f_{ret} italic_f start_POSTSUBSCRIPT italic_r italic_e italic_t end_POSTSUBSCRIPT  in Self-RAG, in Figure  4 , we plot the downstream accuracy vs retrieval frequency (top), and accuracy vs confidence score (bottom) for PopQA, PubHealth, and TriviaQA datasets by sweeping across different threshold values    \\gamma italic_  (larger    \\gamma italic_  causes less retrieval) from 0 to 1. In Figure  4  (bottom), we notice that for  f m  e  a  n  p subscript f m e a n p f_{meanp} italic_f start_POSTSUBSCRIPT italic_m italic_e italic_a italic_n italic_p end_POSTSUBSCRIPT  or  f m  i  n  p subscript f m i n p f_{minp} italic_f start_POSTSUBSCRIPT italic_m italic_i italic_n italic_p end_POSTSUBSCRIPT , the accuracy increases with higher values of confidence while  f m  e  a  n  p subscript f m e a n p f_{meanp} italic_f start_POSTSUBSCRIPT italic_m italic_e italic_a italic_n italic_p end_POSTSUBSCRIPT  is more robust, showing monotonically increasing accuracy with higher confidence scores consistently in all dataset. But in the case of  f r  e  t subscript f r e t f_{ret} italic_f start_POSTSUBSCRIPT italic_r italic_e italic_t end_POSTSUBSCRIPT , no such pattern exists. Overall (top) as these benchmarks are knowledge-intensive, they typically perform better with retrieved contexts and our adaptive scoring shows a better determination of when to retrieve and when not  resulting in higher accuracy at any retrieval frequency. In fact, the advantage is more amplified in PubHealth where we can find a clear threshold confidence score which if achieved, retrieval data are found to be less effective than the parametric knowledge. This gives us a peak accuracy of 1% more than always retrieval, which can not be determined by Self-RAG.",
            "Sparse Upcycling Hyperparameters.  We experiment with different hyper-parameters of  Open-RAG  as shown in Table  2 . We observe that increasing the number of experts  N E subscript N E N_{E} italic_N start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT  slightly improves the performance in MuSiQue, and performance improvement in training longer (epoch 1 vs 2). Increasing the number of active experts  k k k italic_k  from 2 to 4 causes performance degradation showing the necessity of less active experts.",
            "We discuss the different soft retrieval constraints in Section  2.3  and Section  4.2 .  Moreover, we identify a bug  4 4 4 Implementation issue of soft-constraint in Self-RAG   in the implementation of soft-constraint for adaptive retrieval in Self-RAG where the implementation utilizes the log-probability of the  Retrieval  token instead of the probability."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Training Hyper-parameters.",
        "table": "A1.T3.6.6",
        "footnotes": [],
        "references": [
            "The sparse MoE  Open-RAG  model augments the FFN layer of the dense backbone LLM with a parameter-efficient MoE transformer block consisting of a set of expert layers  E = { E e } e = 1 N E E superscript subscript subscript E e e 1 subscript N E \\mathbf{E}=\\{\\mathcal{E}_{e}\\}_{e=1}^{N_{E}} bold_E = { caligraphic_E start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_e = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  along with an efficient routing mechanism as in Figure  3 . Each expert layer comprises a replicated original shared FFN layer weight, adapted by an adapter module  A e subscript A e \\mathcal{A}_{e} caligraphic_A start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  with parameters   e subscript  e \\theta_{e} italic_ start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT . To ensure parameter efficiency, in each expert, we keep the FFN layer frozen and train the adapter module  A e subscript A e \\mathcal{A}_{e} caligraphic_A start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  only. In this way, we are only required to store one FFN replica keeping the model size unchanged except for the increase in the parameters in the adapter and the router modules. The rest of the layers, such as Norm and Attention, are copied from the dense model.",
            "Inference Data and Settings.   We assign the default weight of 1.0, 1.0, and 0.5 to  Relevance ,  Grounding , and  Utility  tokens respectively. Following Self-RAG,  we compare the model performances with always retrieval and vary the retrieval frequency as discussed in Sec  2.3  only to demonstrate optimum thresholding and performance-speed trade-offs. In multi-hop evaluations, from the corresponding retrieval candidate passages, we use Beam Retriever  Zhang et al. ( 2024a )  to retrieve  Top - 3 3 3 3  multi-hop contexts, each with the mentioned  N H subscript N H N_{H} italic_N start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT  number of passages. For single-hop tasks, we use Self-RAGs setup (See Appendix   B ).",
            "As discussed in Sec  2.3 , given the query, adaptive retrieval method provides a probability/confidence score from the model. By thresholding on that score, we can control the retrieval frequency and balance the performance-speed trade-off and this can also guide to determine when retrieval is needed. A better scoring method should achieve higher accuracy at any retrieval frequency.  In order to demonstrate our hybrid adaptive retrieval scoring over the existing reflection token probability-based method  f r  e  t subscript f r e t f_{ret} italic_f start_POSTSUBSCRIPT italic_r italic_e italic_t end_POSTSUBSCRIPT  in Self-RAG, in Figure  4 , we plot the downstream accuracy vs retrieval frequency (top), and accuracy vs confidence score (bottom) for PopQA, PubHealth, and TriviaQA datasets by sweeping across different threshold values    \\gamma italic_  (larger    \\gamma italic_  causes less retrieval) from 0 to 1. In Figure  4  (bottom), we notice that for  f m  e  a  n  p subscript f m e a n p f_{meanp} italic_f start_POSTSUBSCRIPT italic_m italic_e italic_a italic_n italic_p end_POSTSUBSCRIPT  or  f m  i  n  p subscript f m i n p f_{minp} italic_f start_POSTSUBSCRIPT italic_m italic_i italic_n italic_p end_POSTSUBSCRIPT , the accuracy increases with higher values of confidence while  f m  e  a  n  p subscript f m e a n p f_{meanp} italic_f start_POSTSUBSCRIPT italic_m italic_e italic_a italic_n italic_p end_POSTSUBSCRIPT  is more robust, showing monotonically increasing accuracy with higher confidence scores consistently in all dataset. But in the case of  f r  e  t subscript f r e t f_{ret} italic_f start_POSTSUBSCRIPT italic_r italic_e italic_t end_POSTSUBSCRIPT , no such pattern exists. Overall (top) as these benchmarks are knowledge-intensive, they typically perform better with retrieved contexts and our adaptive scoring shows a better determination of when to retrieve and when not  resulting in higher accuracy at any retrieval frequency. In fact, the advantage is more amplified in PubHealth where we can find a clear threshold confidence score which if achieved, retrieval data are found to be less effective than the parametric knowledge. This gives us a peak accuracy of 1% more than always retrieval, which can not be determined by Self-RAG.",
            "We train both MoE and Dense models with LoRA rank 64, LoRA    \\alpha italic_  16, and LoRA dropout 0.1. We optimize the models with the AdamW optimizer with a linear learning rate scheduler and a weight decay of 0.0. Both models have a context length of 4096 for facilitating long-context multi-hop QAs.  Other training hyper-parameters are mentioned in Table  3 .",
            "We discuss the different soft retrieval constraints in Section  2.3  and Section  4.2 .  Moreover, we identify a bug  4 4 4 Implementation issue of soft-constraint in Self-RAG   in the implementation of soft-constraint for adaptive retrieval in Self-RAG where the implementation utilizes the log-probability of the  Retrieval  token instead of the probability."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  The generator LM training data statistics. Instruction-following and single-hop knowledge-intensive samples are from Self-RAG  Asai et al. ( 2024 ) . We curate the multi-hop knowledge-intensive samples with reflection tokens.",
        "table": "A1.T4.1",
        "footnotes": [],
        "references": [
            "As discussed in Sec  2.3 , given the query, adaptive retrieval method provides a probability/confidence score from the model. By thresholding on that score, we can control the retrieval frequency and balance the performance-speed trade-off and this can also guide to determine when retrieval is needed. A better scoring method should achieve higher accuracy at any retrieval frequency.  In order to demonstrate our hybrid adaptive retrieval scoring over the existing reflection token probability-based method  f r  e  t subscript f r e t f_{ret} italic_f start_POSTSUBSCRIPT italic_r italic_e italic_t end_POSTSUBSCRIPT  in Self-RAG, in Figure  4 , we plot the downstream accuracy vs retrieval frequency (top), and accuracy vs confidence score (bottom) for PopQA, PubHealth, and TriviaQA datasets by sweeping across different threshold values    \\gamma italic_  (larger    \\gamma italic_  causes less retrieval) from 0 to 1. In Figure  4  (bottom), we notice that for  f m  e  a  n  p subscript f m e a n p f_{meanp} italic_f start_POSTSUBSCRIPT italic_m italic_e italic_a italic_n italic_p end_POSTSUBSCRIPT  or  f m  i  n  p subscript f m i n p f_{minp} italic_f start_POSTSUBSCRIPT italic_m italic_i italic_n italic_p end_POSTSUBSCRIPT , the accuracy increases with higher values of confidence while  f m  e  a  n  p subscript f m e a n p f_{meanp} italic_f start_POSTSUBSCRIPT italic_m italic_e italic_a italic_n italic_p end_POSTSUBSCRIPT  is more robust, showing monotonically increasing accuracy with higher confidence scores consistently in all dataset. But in the case of  f r  e  t subscript f r e t f_{ret} italic_f start_POSTSUBSCRIPT italic_r italic_e italic_t end_POSTSUBSCRIPT , no such pattern exists. Overall (top) as these benchmarks are knowledge-intensive, they typically perform better with retrieved contexts and our adaptive scoring shows a better determination of when to retrieve and when not  resulting in higher accuracy at any retrieval frequency. In fact, the advantage is more amplified in PubHealth where we can find a clear threshold confidence score which if achieved, retrieval data are found to be less effective than the parametric knowledge. This gives us a peak accuracy of 1% more than always retrieval, which can not be determined by Self-RAG.",
            "The complete breakdown of  Open-RAG  training dataset is displayed in Table  4 . Algorithm  1  shows the process of the multi-hop training data preparation.",
            "We discuss the different soft retrieval constraints in Section  2.3  and Section  4.2 .  Moreover, we identify a bug  4 4 4 Implementation issue of soft-constraint in Self-RAG   in the implementation of soft-constraint for adaptive retrieval in Self-RAG where the implementation utilizes the log-probability of the  Retrieval  token instead of the probability."
        ]
    },
    "global_footnotes": [
        "With additional contexts if provided",
        "For long-form generation, we also use the",
        "token, which indicates that the model can continue to use information from the previous segment.",
        "For long-form generation, we use the same segment-level beam search strategy as in Self-RAG",
        "to obtain the",
        "segments, where",
        "is the beam size, and return the best sequence at the end of generation."
    ]
}