{
    "id_table_1": {
        "caption": "Table 1:  Comparison of the zero-shot classification accuracy between the baseline and CtrlSynth. We report top-1 accuracy for 20 commonly used downstream vision datasets, including 12 tasks in the VTAB benchmark  (Zhai et al.,  2020 )  and 8 other ones.",
        "table": "S4.T3.1.1",
        "footnotes": [
            ""
        ],
        "references": [
            "In this work, we aim to systematically control the synthetic pipeline for generating image-text data while accommodating different use cases ( e.g . , improving long-tail task performance, enhancing compositional reasoning of CLIP models, etc.). Our intuition is that large foundation models are already pretrained on a wide range of data and contain general knowledge about concepts, objects, and their relationships. For example, text-to-image models  ( e.g . , Rombach et al.,  2022 ; Podell et al.,  2024 )  can generate detailed high-quality images based on text instructions. Similarly, large language models (LLMs)   ( e.g . , OpenAI,  2022 ; Touvron et al.,  2023 )  have strong instruction-following capabilities, which can be used to control the text data generation. CtrlSynth leverages these large pretrained models to build a modular and controllable synthetic data generation pipeline. CtrlSynth allows users to apply explicit control instructions to guide data generation for images and texts. Unlike previous data synthesis works that use image-captioning models to directly generate captions given an image  ( e.g . , Li et al.,  2024 ; Lai et al.,  2024 ) , CtrlSynth decomposes image-to-text generation process into two separate steps, providing more fine-grained control to users for synthesizing data.  Figure   1  shows an overall architecture of the CtrlSynth pipeline. For an input image, CtrlSynth first uses a pretrained vision model to extract key objects, attributes, and their relations as visual tags. It then uses a text controller to create text synthesis instructions and guide the LLM to use visual tags to generate high-quality text outputs. Similarly, we devise an image controller that steers how the text prompts (or caption) can be used to guide the diffusion model to generate a desired image. Users can also feed the generated synthetic images into the tagging model again, forming a closed-loop data pipeline. Then users can start with synthetic or original images and texts and further generate more image-text pairs. The text and image controllers are modular, allowing users to control any part of the text or image generation process.",
            "CtrlSynth leverages semantic knowledge and reasoning skills of pretrained foundation models ( e.g . , large language and diffusion models) to generate diverse synthetic data samples in a controlled manner. Specifically, CtrlSynth consists of three foundation models: (1) a vision tagging model, (2) a large language model, and (3) a text-to-image model; plus the two text and image controllers. For a given real (   1a 1a 1a   in  Figure   1 ) or synthetic (   1c 1c 1c  ) input image, a  vision tagging model  (   2a 2a 2a  ) extracts visual tags ( e.g . , objects, attributes, and their relationships) (   1e 1e 1e  ). These tags describe the images visual concepts and semantic contexts. The  text controller  (   3a 3a 3a  ) takes the image tags and user-defined control policies as inputs and generates instructions for synthesizing new text. An example control policy is to edit the tags or optionally add the text (   1b 1b 1b  ) associated with the image. A  large language model  (   2b 2b 2b  ) then follows the instructions and generates the synthetic text (   1d 1d 1d  ). The  image controller  (   3b 3b 3b  ) operates on the given input text and applies user-defined image control policies to output instructions for image synthesis. An example policy is to specify the style for generating artistic, cinematic, or realistic images. A  text-to-image model  (   2c 2c 2c  ) takes an image synthesis instruction provided by the image controller as an input and produces a synthetic image as an output (   1c 1c 1c  ).",
            "The text controller accepts the visual tags of an image and a user-defined policy along with an optional original text as input and produces instructions to control the generation of synthetic text. In CtrlSynth, we study three predefined policies: (a) editing (remove, add, or replace) visual tags, (b) constraining the semantic meaning of a given sentence, and (c) styling the output text. Editing visual tags allows fine-grained control of synthetic visual content, for example, one can remove unwanted objects or attributes so they do not appear in the generated text. Constraining the meaning of synthetic text is useful in generating high-quality captions because many web-crawled captions are noisy. Enforcing the styling of output texts such as outputting into structured formats ( e.g . , JSON) makes the texts easier to use in downstream tasks. In our experiments, we use 10 example text control policies for synthesizing image captions (see  Section   A.1  for details).",
            "Flexible and diverse synthesis paths.  A data synthesis path ( S  P S P SP italic_S italic_P ) starts and ends with a data node (rounded box in  Figure   1 ). We define the following synthesis paths:",
            "S  P  ( 2 ) S P 2 SP(2) italic_S italic_P ( 2 ) :  1  a  2  a  1  e  1  b 3  a  2  b  1  d  1 a 2 a  1 e 1 b  3 a  2 b  1 d 1a\\rightarrow 2a\\rightarrow 1e\\xrightarrow{1b}3a\\rightarrow 2b\\rightarrow 1d 1 italic_a  2 italic_a  1 italic_e start_ARROW start_OVERACCENT 1 italic_b end_OVERACCENT  end_ARROW 3 italic_a  2 italic_b  1 italic_d . This path ( Figure   4(b) ) is similar to the previous path but a key difference is that it constrains the synthetic text to be faithful 3 3 3 Or the opposite depending on the user-specified policy  to an original text. We can consider it as using the VTM and LLM to synthesize an improved text over the original one. We will show later in  Section   4.5  that text samples generated from this path outperform previous works  (Lai et al.,  2024 ; Fan et al.,  2023 )  that rewrite noisy captions. We include the example prompts to reflect the control policies in  Section   A.1 .",
            "Table   10  lists the hyper-parameters used for pretraining on CC3M and CC12m. We use AdamW  (Loshchilov & Hutter,  2018 )  with default    \\beta italic_  values as an optimizer and binary cross-entropy loss as an objective function. We use cosine learning rate schedule  (Loshchilov & Hutter,  2022 ) . We use the CoreNet library  (Mehta et al.,  2024a ;  2022 )  for all pretraining experiments. We adapt the LIFT codebase  (Shi et al.,  2024 )  for fine-tuning long-tail tasks, main modifications include adding support for iteration-based training and data loader for multiple datasets."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Zero-shot top-1 accuracy between the baseline and CtrlSynth on 6 ImageNet datasets.",
        "table": "S4.T3.2.1",
        "footnotes": [],
        "references": [
            "An example of extracting visual tags from VTM is shown in  Footnote   2 . The tagging model can be either a multi-label image classifier  (Mehta et al.,  2024b )  that predicts diverse tags in the image, or a black box system ( e.g .  an API service) that takes the input image and outputs the tags.",
            "Tasks and Datasets.  We adopt the CLIP  (Radford et al.,  2021 )  model architecture for multimodal representation learning. For pretraining CLIP models, we use two public image-text datasets: CC3M  (Sharma et al.,  2018 )  and CC12M  (Changpinyo et al.,  2021 ) . To evaluate the representation quality of pretrained CLIP models, we measure the zero-shot performance on classification, retrieval, and compositional reasoning tasks. For image classification, we use 25 common vision datasets, including five ImageNet  (Deng et al.,  2009 ; Recht et al.,  2019 )  variants and the tasks from the VTAB benchmark  (Zhai et al.,  2020 ) . We list the detailed dataset information in  Section   A.2 . We use COCO  (Lin et al.,  2014 )  and Flickr30k  (Plummer et al.,  2015 )  for image-to-text and text-to-image retrieval tasks and report the metrics in recall@1. SugarCrepe  (Hsieh et al.,  2023 )  is a recent benchmark that measures the compositional understanding of vision-language models, we report the zero-shot accuracy numbers. Additionally, to study the effects of CtrlSynth on long-tail tasks, we evaluate the task accuracy of Places-LT and ImageNet-LT datasets  (Liu et al.,  2019 )  by augmenting the tail classes with CtrlSynth synthetic data.",
            "Setup.  We conduct experiments on the ImageNet-LT  (Liu et al.,  2019 )  and Places-LT  (Liu et al.,  2019 )  datasets. ImageNet-LT is a subset of the original ImageNet-2012  (Deng et al.,  2009 )  and contains 115.8K images from 1000 classes, with 5 to 1280 images per class. Places-LT is even more imbalanced and contains 62.5K images from 365 classes, with 5 to 4980 images per class. The test sets of both datasets are balanced. Following the same setup in  (Liu et al.,  2019 ) , we report the overall accuracy as well as the accuracy across the head ( > > > 100 images), medium (20  similar-to \\sim  100), and tail ( < < < 20) classes. We take the same baseline in  (Shi et al.,  2024 )  and fine-tune the classifier head of a pretrained CLIP model (ViT-B/16) for 10 epochs (or the same number of iterations for CtrlSynth). For CtrlSynth synthetic samples, we choose the synthetic path  S  P  ( 3 ) S P 3 SP(3) italic_S italic_P ( 3 )  to generate synthetic images for the tail classes. We mix the CtrlSynth image samples with the original training set of each dataset. We describe more details in  Section   A.2 .",
            "In this section, we evaluate the effectiveness of visual tags, the impact of using different pretrained models in the CtrlSynth pipeline, and mixing and filtering effects for CtrlSynth samples. We use the same text and image control policy described in  Section   3.2  for all settings. We experiment with CC3M dataset for CLIP pretraining and report the accuracy on the SugarCrepe benchmark, zero-shot accuracy of common downstream vision tasks (same tasks in  Table   3 ), and top1 accuracy on the ImageNet 1k validation set."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Zero-shot retrieval evaluation on the Flickr and COCO datasets. We report the recall@1 numbers. I2T means image-to-text retrieval, and T2I denotes text-to-image retrieval.",
        "table": "S4.T3.3.2",
        "footnotes": [],
        "references": [
            "Language Model.  Large language models (LLMs) have exhibited strong instruction-following capabilities. The goal of an LLM in CtrlSynth is to take an input textual instruction on how to generate a synthetic text that meets the requirements specified in the instruction. CtrlSynth employs the reasoning and composition capability of LLMs to recombine the visual image tags in the task instruction and compose new synthetic texts. The instruction for an LLM consists of three parts ( Figure   3 ):  (i)  task template  that specifies the details of the text synthesis task,  (ii)  task content  that contains the actual visual tags (phrases) and an optional caption paired with the image, and  (iii)  task constraint  that describes the style and formatting of the output text. Users can also apply custom policies over the instructions to guide the text synthesis process.",
            "Training and Baselines.  Note that CtrlSynth itself does not require any training. We conduct pretraining experiments on CLIP models to evaluate the quality of synthetic data. We use ViT-B/16  (Dosovitskiy et al.,  2020 )  architecture for the CLIP vision backbone. For a fair comparison, we train all models for the same number of iterations on the original dataset (baseline) and the dataset mixed with CtrlSynth augmented samples. We use CtrlSynth-cap to denote the original image and synthetic text pair  ( 1  a , 1  d ) 1 a 1 d (1a,1d) ( 1 italic_a , 1 italic_d )  from synthesis path  S  P  ( 1 ) S P 1 SP(1) italic_S italic_P ( 1 ) . CtrlSynth-img stands for the synthetic image and original text pair  ( 1  b , 1  c ) 1 b 1 c (1b,1c) ( 1 italic_b , 1 italic_c )  from synthesis path  S  P  ( 4 ) S P 4 SP(4) italic_S italic_P ( 4 ) . CtrlSynth-capimg means the synthetic image and text pair  ( 1  d , 1  c ) 1 d 1 c (1d,1c) ( 1 italic_d , 1 italic_c )  from synthesis path  S  P  ( 3 ) S P 3 SP(3) italic_S italic_P ( 3 ) . We define CtrlSynth-mix as taking one image-text pair from CtrlSynth-cap and another from CtrlSynth-capimg. We do not take CtrlSynth-img image-text pairs since we found the original texts are noisy and thus a substantial portion of synthetic images are bad quality. We refer CtrlSynth-mix as the default setting unless otherwise stated. We list detailed information in  Section   A.3 .",
            "Image Classification Evaluation.  We conduct the zero-shot evaluation for image classification tasks.  Table   3  shows the results across 20 commonly used vision datasets and  Table   3  shows the results of 6 ImageNet-related datasets. Notably, CtrlSynth outperforms the baseline consistently by 2.5% to 9.4% for the CLIP models trained on the CC3M and CC12M datasets. We observe that CtrlSynth significantly improves the zero-shot performance (by over 7.7%) by augmenting smaller datasets like CC3M, while the performance gains become smaller on larger datasets like CC12M.",
            "Image-Text Retrieval Evaluation.  We evaluate the zero-shot image-text retrieval performance for our CtrlSynth and baseline CLIP models and present the recall@1 results in  Table   3 . CtrlSynth substantially improves the text-to-image and image-to-text retrieval recall by up to 24% and 36% for the Flickr dataset, and overall improves recall by 23.4% on average for CC3M models. CtrlSynth also brings over 9% retrieval gains for CC12M models on average. The improvements show that data samples from CtrlSynth have better coverage of visual concepts.",
            "In this section, we evaluate the effectiveness of visual tags, the impact of using different pretrained models in the CtrlSynth pipeline, and mixing and filtering effects for CtrlSynth samples. We use the same text and image control policy described in  Section   3.2  for all settings. We experiment with CC3M dataset for CLIP pretraining and report the accuracy on the SugarCrepe benchmark, zero-shot accuracy of common downstream vision tasks (same tasks in  Table   3 ), and top1 accuracy on the ImageNet 1k validation set."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  We evaluate the compositional reasoning accuracy on the SugarCrepe  (Hsieh et al.,  2023 )  benchmark.",
        "table": "S4.T4.1",
        "footnotes": [
            ""
        ],
        "references": [
            "VTM, as a key component in CtrlSynth, can be a combination of an advanced captioning model  (Xiao et al.,  2024 )  that generates comprehensive image descriptions and an LLM that extracts the visual tags from the captions to decompose the visual semantics of an image into a set of fine-grained visual concepts.  Section   A.4  includes more details about this hybrid VTM. These fine-grained visual concepts can be easily modified and recomposed to create new visual contexts. This decompose-recompose feature of vision tags provides a large control space for synthesizing diverse texts.",
            "S  P  ( 1 ) S P 1 SP(1) italic_S italic_P ( 1 ) :  1  a  2  a  1  e  3  a  2  b  1  d  1 a 2 a  1 e  3 a  2 b  1 d 1a\\rightarrow 2a\\rightarrow 1e\\rightarrow 3a\\rightarrow 2b\\rightarrow 1d 1 italic_a  2 italic_a  1 italic_e  3 italic_a  2 italic_b  1 italic_d . This path ( Figure   4(a) ) means CtrlSynth generates a new text that describes the original image. The synthetic text  1  d 1 d 1d 1 italic_d  may not align with the semantics in the original image since the LLM can create new combinations of the visual tags and add information that does not exist in the image. Such new information provides useful semantic augmentation over the original image while containing similar visual concepts.",
            "S  P  ( 2 ) S P 2 SP(2) italic_S italic_P ( 2 ) :  1  a  2  a  1  e  1  b 3  a  2  b  1  d  1 a 2 a  1 e 1 b  3 a  2 b  1 d 1a\\rightarrow 2a\\rightarrow 1e\\xrightarrow{1b}3a\\rightarrow 2b\\rightarrow 1d 1 italic_a  2 italic_a  1 italic_e start_ARROW start_OVERACCENT 1 italic_b end_OVERACCENT  end_ARROW 3 italic_a  2 italic_b  1 italic_d . This path ( Figure   4(b) ) is similar to the previous path but a key difference is that it constrains the synthetic text to be faithful 3 3 3 Or the opposite depending on the user-specified policy  to an original text. We can consider it as using the VTM and LLM to synthesize an improved text over the original one. We will show later in  Section   4.5  that text samples generated from this path outperform previous works  (Lai et al.,  2024 ; Fan et al.,  2023 )  that rewrite noisy captions. We include the example prompts to reflect the control policies in  Section   A.1 .",
            "S  P  ( 3 ) S P 3 SP(3) italic_S italic_P ( 3 ) :  1  a  2  a  1  e  3  a  2  b  1  d  3  b  2  c  1  c  1 a 2 a  1 e  3 a  2 b  1 d  3 b  2 c  1 c 1a\\rightarrow 2a\\rightarrow 1e\\rightarrow 3a\\rightarrow 2b\\rightarrow 1d% \\rightarrow 3b\\rightarrow 2c\\rightarrow 1c 1 italic_a  2 italic_a  1 italic_e  3 italic_a  2 italic_b  1 italic_d  3 italic_b  2 italic_c  1 italic_c . This path ( Figure   4(c) ) provides both synthetic text ( 1  d 1 d 1d 1 italic_d ) and image ( 1  c 1 c 1c 1 italic_c ) samples.  1  c 1 c 1c 1 italic_c  can be an effective image sample that augments the original image ( 1  a 1 a 1a 1 italic_a ) or can be paired with ( 1  d 1 d 1d 1 italic_d ) to augment the original image-text pair ( 1  a 1 a 1a 1 italic_a  and  1  b 1 b 1b 1 italic_b ).",
            "S  P  ( 4 ) S P 4 SP(4) italic_S italic_P ( 4 ) :  1  b  3  b  2  c  1  c  1 b 3 b  2 c  1 c 1b\\rightarrow 3b\\rightarrow 2c\\rightarrow 1c 1 italic_b  3 italic_b  2 italic_c  1 italic_c . This path ( Figure   4(d) ) bypasses the language model and the original text is directly fed to the image controller and then generates a synthetic image ( 1  c 1 c 1c 1 italic_c ). The image sample could be a strong augmentation sample to the original image if the original text has a comprehensive and high-quality description.",
            "CtrlSynth Models.  For the VTM, we adopt a hybrid approach by default, we combine the tags from a captioning plus tag extraction pipeline and an advanced multi-label image classifier. We use a recent vision foundation model called Florence-large  (Xiao et al.,  2024 )  to generate detailed image descriptions and then extract the objects, attributes, and relations using the Qwen2-7B-Instruct  (Yang et al.,  2024a )  LLM. Then we use an accurate image classifier, the huge variant of CatLIP  (Mehta et al.,  2024b ) , to output multiple high-confidence objects and attributes. We show later in  Section   4.5  that this hybrid VTM provides the best visual tags compared with using individual approach alone. For the LLM, we use Mistral-NeMo-instruct model  (AI,  2024 )  by default due to its strong instruction-following capability. We choose the stable-diffusion-xl-base-1.0  (Podell et al.,  2024 )  for the text-to-image model by default. We describe the detailed setup in  Section   A.4 . In  Section   4.5 , we study different pretrained models for each of the three modules in CtrlSynth.",
            "Compositional Reasoning Results.  A key strength in CtrlSynth is the inclusion of visual tags that contain objects, attributes and relations from an image. To understand how the fine-grained visual attributes and relations affect visual reasoning performance, we evaluate CtrlSynth and baseline on the SugarCrepe  Hsieh et al. ( 2023 )  benchmark which measures the compositional reasoning capability of vision language models. We present the results in  Table   4 . CtrlSynth improves the baseline CLIP compositional reasoning by a large margin (4.5% for CC3M and 3% for CC12M on average). Note that most of the improvements come from the attribute and relation forms in the  Replace  and  Swap  categories, for example, CtrlSynth on CC3M improves the  Replace  relation accuracy by 4.3% and  Swap  attribute by 14.8%, indicating CtrlSynth models are robust to the attribute and relation changes.",
            "CtrlSynth Samples from Different Synthesis Paths.  CtrlSynth pipeline supports synthesizing images or texts from different paths, we evaluate their quality by measuring the downstream task accuracy of the CLIP models trained on them. The penultimate and last rows in  Section   4.5  show all CtrlSynth samples provides performance gains on downstream tasks, except the CtrlSynth-img samples where they do not improve compositional reasoning performance. CtrlSynth-img samples have the least augmentation benefits and are likely due to the original real texts are noisy and thus the generated images are not of high quality. Notably, mixing with synthetic captions (CtrlSynth-cap, CtrlSynth-capimg, and CtrlSynth-mix) provides meaningful augmentation benefits, highlight the importance of using LLMs to recombine the visual tags."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Comparison of the zero-shot classification accuracy between VeCLIP  (Vasu et al.,  2024 )  and CtrlSynth for CLIP trained on the CC3M. We report top-1 accuracy (%) for the VTAB benchmark  (Zhai et al.,  2020 )  across 9 tasks (6 from natural and 3 from specialized sets). We highlight the best numbers in  bold .",
        "table": "S4.T5.3.1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "Image-text Data Augmentation.  Much recent work aims to improve the caption quality of image-text pairs. For example, VeCLIP  (Lai et al.,  2024 ) , LaCLIP  (Fan et al.,  2023 ) , and ReCap  (Li et al.,  2024 )  leverage LLMs to synthesize new captions that are more informative and contain rich descriptions about the image. The key difference of CtrlSynth is that we provide more diverse and high-quality captions that outperform prior works (we will show in  Table   5  and  Table   6 ). This is because CtrlSynth breaks down the image semantics to allow more fine-grained control and recombination using LLM. Another line of work uses text-to-image models like diffusion models to generate synthetic images and augment downstream vision tasks. ALIA  (Dunlap et al.,  2023 )  uses language to guide the image editing process and provides domain-specific diversity to augment the image samples. DiffuseMix  (Islam et al.,  2024 )  augments image samples using diffusion models to blend original and generated images. EDA  (Trabucco et al.,  2023 )  generates variations of real images using diffusion models to maintain the semantics while augmenting image samples. These semantic image augmentation methods provide strong performance improvements on various vision datasets. Our CtrlSynth instead unifies the image and text synthesis via a closed-loop pipeline, it provides more flexibility and diverse synthetic samples while allowing more fine-grained control over the sample generation process.",
            "S  P  ( 2 ) S P 2 SP(2) italic_S italic_P ( 2 ) :  1  a  2  a  1  e  1  b 3  a  2  b  1  d  1 a 2 a  1 e 1 b  3 a  2 b  1 d 1a\\rightarrow 2a\\rightarrow 1e\\xrightarrow{1b}3a\\rightarrow 2b\\rightarrow 1d 1 italic_a  2 italic_a  1 italic_e start_ARROW start_OVERACCENT 1 italic_b end_OVERACCENT  end_ARROW 3 italic_a  2 italic_b  1 italic_d . This path ( Figure   4(b) ) is similar to the previous path but a key difference is that it constrains the synthetic text to be faithful 3 3 3 Or the opposite depending on the user-specified policy  to an original text. We can consider it as using the VTM and LLM to synthesize an improved text over the original one. We will show later in  Section   4.5  that text samples generated from this path outperform previous works  (Lai et al.,  2024 ; Fan et al.,  2023 )  that rewrite noisy captions. We include the example prompts to reflect the control policies in  Section   A.1 .",
            "CtrlSynth Models.  For the VTM, we adopt a hybrid approach by default, we combine the tags from a captioning plus tag extraction pipeline and an advanced multi-label image classifier. We use a recent vision foundation model called Florence-large  (Xiao et al.,  2024 )  to generate detailed image descriptions and then extract the objects, attributes, and relations using the Qwen2-7B-Instruct  (Yang et al.,  2024a )  LLM. Then we use an accurate image classifier, the huge variant of CatLIP  (Mehta et al.,  2024b ) , to output multiple high-confidence objects and attributes. We show later in  Section   4.5  that this hybrid VTM provides the best visual tags compared with using individual approach alone. For the LLM, we use Mistral-NeMo-instruct model  (AI,  2024 )  by default due to its strong instruction-following capability. We choose the stable-diffusion-xl-base-1.0  (Podell et al.,  2024 )  for the text-to-image model by default. We describe the detailed setup in  Section   A.4 . In  Section   4.5 , we study different pretrained models for each of the three modules in CtrlSynth.",
            "Comparison with Prior Work.  CtrlSynth pipeline is flexible and supports synthesizing data from different paths. Previous work like VeCLIP  (Lai et al.,  2024 )  and LaCLIP  (Fan et al.,  2023 )  synthesizing new texts for the images by improving the captions. Though it is impossible to have a completely fair comparison with them 4 4 4 Factors that prohibit apple-to-apple comparison include training software, variations of CC3M samples due to missing images, exact hardware set up, etc. , the synthetic texts from the synthesis path (2) in CtrlSynth provide similar effects. We present the results on CLIP ViT/B16 models trained on CC3M for the tasks reported in each work.  Table   5  shows that CtrlSynth outperforms VeCLIP on most VTAB datasets and improves zero-shot accuracy by 4.8% on average. CtrlSynth also surpasses VeCLIP by 7.9% on the ImageNet 1K dataset. We observe a similar trend when comparing CtrlSynth with LaCLIP in  Table   6 . Specifically, CtrlSynth achieves an average of 3.4% better accuracy than LaCLIP on 15 common datasets and 2.3% better accuracy on ImageNet 1K.",
            "To study the data efficiency of CtrlSynth samples, we plot the top1 zero-shot accuracy of the ImageNet validation set in  Figure   5  for the baseline and CtrlSynth CLIP models trained on CC3M. CtrlSynth reaches the 20% accuracy with 40% fewer iterations than the baseline, indicating that using CtrlSynth samples is more data-efficient.",
            "In this section, we provide the statistics for the synthetic samples from CtrlSynth.  Figure   6  shows examples of CtrlSynth images and texts compared with the original real samples. We observe that the text samples from CtrlSynth are usually longer and contain richer information about the image. On average, CtrlSynth texts have over 60 words while original captions contain 8 words. We plot the histogram of the number of words in  Figure   8  at  Section   A.5 .",
            "CtrlSynth Samples from Different Synthesis Paths.  CtrlSynth pipeline supports synthesizing images or texts from different paths, we evaluate their quality by measuring the downstream task accuracy of the CLIP models trained on them. The penultimate and last rows in  Section   4.5  show all CtrlSynth samples provides performance gains on downstream tasks, except the CtrlSynth-img samples where they do not improve compositional reasoning performance. CtrlSynth-img samples have the least augmentation benefits and are likely due to the original real texts are noisy and thus the generated images are not of high quality. Notably, mixing with synthetic captions (CtrlSynth-cap, CtrlSynth-capimg, and CtrlSynth-mix) provides meaningful augmentation benefits, highlight the importance of using LLMs to recombine the visual tags."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  We report the zero-shot performance on ImageNet 1K and 15 common downstream datasets for both LaCLIP  (Fan et al.,  2023 )  and CtrlSynth for CLIP trained on CC3M. We highlight the best numbers in  bold .",
        "table": "S4.T6.3.1",
        "footnotes": [
            ""
        ],
        "references": [
            "Image-text Data Augmentation.  Much recent work aims to improve the caption quality of image-text pairs. For example, VeCLIP  (Lai et al.,  2024 ) , LaCLIP  (Fan et al.,  2023 ) , and ReCap  (Li et al.,  2024 )  leverage LLMs to synthesize new captions that are more informative and contain rich descriptions about the image. The key difference of CtrlSynth is that we provide more diverse and high-quality captions that outperform prior works (we will show in  Table   5  and  Table   6 ). This is because CtrlSynth breaks down the image semantics to allow more fine-grained control and recombination using LLM. Another line of work uses text-to-image models like diffusion models to generate synthetic images and augment downstream vision tasks. ALIA  (Dunlap et al.,  2023 )  uses language to guide the image editing process and provides domain-specific diversity to augment the image samples. DiffuseMix  (Islam et al.,  2024 )  augments image samples using diffusion models to blend original and generated images. EDA  (Trabucco et al.,  2023 )  generates variations of real images using diffusion models to maintain the semantics while augmenting image samples. These semantic image augmentation methods provide strong performance improvements on various vision datasets. Our CtrlSynth instead unifies the image and text synthesis via a closed-loop pipeline, it provides more flexibility and diverse synthetic samples while allowing more fine-grained control over the sample generation process.",
            "Comparison with Prior Work.  CtrlSynth pipeline is flexible and supports synthesizing data from different paths. Previous work like VeCLIP  (Lai et al.,  2024 )  and LaCLIP  (Fan et al.,  2023 )  synthesizing new texts for the images by improving the captions. Though it is impossible to have a completely fair comparison with them 4 4 4 Factors that prohibit apple-to-apple comparison include training software, variations of CC3M samples due to missing images, exact hardware set up, etc. , the synthetic texts from the synthesis path (2) in CtrlSynth provide similar effects. We present the results on CLIP ViT/B16 models trained on CC3M for the tasks reported in each work.  Table   5  shows that CtrlSynth outperforms VeCLIP on most VTAB datasets and improves zero-shot accuracy by 4.8% on average. CtrlSynth also surpasses VeCLIP by 7.9% on the ImageNet 1K dataset. We observe a similar trend when comparing CtrlSynth with LaCLIP in  Table   6 . Specifically, CtrlSynth achieves an average of 3.4% better accuracy than LaCLIP on 15 common datasets and 2.3% better accuracy on ImageNet 1K.",
            "In this section, we provide the statistics for the synthetic samples from CtrlSynth.  Figure   6  shows examples of CtrlSynth images and texts compared with the original real samples. We observe that the text samples from CtrlSynth are usually longer and contain richer information about the image. On average, CtrlSynth texts have over 60 words while original captions contain 8 words. We plot the histogram of the number of words in  Figure   8  at  Section   A.5 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Long-tail accuracy on the ImagetNet-LT and Places-LT datasets for the baseline and CtrlSynth models.",
        "table": "S4.T7.1.1",
        "footnotes": [],
        "references": [
            "Self-filtering for better synthetic data.  Synthetic samples often suffer from degraded quality especially when running at large scale. Synthetic systems often rely on heuristics or rule-based filtering techniques to filter out bad-quality samples. Because CtrlSynth pipeline is closed-loop, it implicitly provides self-filtering functionality. To check the quality of the synthetic text, we can detect if the synthetic text ( 1  d 1 d 1d 1 italic_d ) contains the visual tags ( 1  e 1 e 1e 1 italic_e ), to filter out potentially misaligned or lower quality synthetic text samples, we define that at least some ratio  p f subscript p f p_{f} italic_p start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT  of the visual tags exist. For the synthetic image, we run it through the VTM again and output the visual tags, then we do the same check against the starting node text ( 1  b 1 b 1b 1 italic_b  or  1  d 1 d 1d 1 italic_d ). Later in  Figure   7(a) , we will show that self-filtering improves the synthetic samples.",
            "Key Results.   Table   7  shows that CtrlSynth improves the tail class accuracy by 21.3% on ImageNet-LT and by 16.2% on Places-LT. Synthetic samples from CtrlSynth also improve the overall and medium class accuracy by 3  similar-to \\sim  6%, though slightly decrease the head class accuracy.",
            "CtrlSynth provides off-the-shelf self-filtering to control the quality of synthetic samples. We study the effects of applying different filtering thresholds  p f subscript p f p_{f} italic_p start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT  for the synthetic text and image. We set the same filtering thresholds for both synthetic text and image samples. Intuitively, a higher threshold filters out more synthetic samples thus providing better quality samples that align with original real samples. On the contrary, a lower threshold keeps relatively less aligned samples but encourages more diverse samples.  Figure   7(a)  plots the zero-shot accuracy numbers of CLIP model on ImageNet under different threshold settings, we show that thresholds 10%  similar-to \\sim  30% provide similar accuracy numbers and setting the filtering threshold to 20% provides the best accuracy. Thresholds higher than 50% do not provide accuracy gains, likely because the aligned synthetic samples lack diversity and fail to augment the original samples.",
            "To better understand how the synthetic image text samples improve CLIP model training, we study different ratios ( p r subscript p r p_{r} italic_p start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) of mixing CtrlSynth samples with original real ones. During CLIP training, we randomly sample the original sample with probability  0 < p r < 1 0 subscript p r 1 0<p_{r}<1 0 < italic_p start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT < 1  and our sample with  1  p r 1 subscript p r 1-p_{r} 1 - italic_p start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT .  Figure   7(b)  shows that even adding a small portion ( < 20 absent 20 <20 < 20 %) of CtrlSynth samples improves the zero-shot accuracy while mixing with 50% provides best accuracy gains. Further higher mixing ratios show diminishing improvements though still better than the baseline that uses all real data."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Evaluation of using different models, visual tags, and synthetic samples in CtrlSynth. - denotes the same value from the last row (default setting).",
        "table": "S4.T8.1.1",
        "footnotes": [],
        "references": [
            "In this section, we provide the statistics for the synthetic samples from CtrlSynth.  Figure   6  shows examples of CtrlSynth images and texts compared with the original real samples. We observe that the text samples from CtrlSynth are usually longer and contain richer information about the image. On average, CtrlSynth texts have over 60 words while original captions contain 8 words. We plot the histogram of the number of words in  Figure   8  at  Section   A.5 .",
            "Different Pretrained Models.  We choose an alternate LLM and a different text-to-image model to understand how different pretrained models affect the quality of synthetic samples. CtrlSynth pipeline is flexible so we can easily swap the pretrained LLM and text-to-image models. Specifically, we use Qwen2-7B  (Yang et al.,  2024a )  for the LLM and Stable Diffusion 3 Medium  (Esser et al.,  2024 )  (SD3M) for the text-to-image model. Comparing the first and last rows in  Table   8 , we find using a smaller LLM like Qwen2-7B degrades the task performance on all three tasks, indicating that using a strong LLM is key to synthesizing high quality texts. The accuracy boost (+3%) on SugarCrepe benchmark shows the LLM is effective in recombining the visual tags in a compositional way to form diverse synthetic texts. We also point out that using a more recent diffusion model like SD3M provides similar task performance numbers, this is likely because SD3M has fewer (2B versus 3.5B) parameters compared to SDXL, limiting the image generation capability.",
            "Effectiveness of Visual Tags.  We study the effects of using different categories of visual tags,  i.e . , using only objects (Obj), objects plus attributes (Obj+Attr), and all categories including relations (Obj+Attr+Rel). In  Table   8 , comparing the second and last row, we show attributes marginally improve the CLIP performance on compositional reasoning but not much on zero-shot vision tasks. Importantly, visual relations improves the performance on all three tasks, and significantly improves compositional reasoning performance by over 4%.",
            "We plot the number of words for synthetic texts generated by CtrlSynth and compare them with original real texts in  Figure   8 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Details of evaluation datasets.",
        "table": "A1.T9.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "We list the vision datasets for evaluation in  Table   9 ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Training hyper-parameters.",
        "table": "A1.T10.st1.1.1",
        "footnotes": [],
        "references": [
            "Table   10  lists the hyper-parameters used for pretraining on CC3M and CC12m. We use AdamW  (Loshchilov & Hutter,  2018 )  with default    \\beta italic_  values as an optimizer and binary cross-entropy loss as an objective function. We use cosine learning rate schedule  (Loshchilov & Hutter,  2022 ) . We use the CoreNet library  (Mehta et al.,  2024a ;  2022 )  for all pretraining experiments. We adapt the LIFT codebase  (Shi et al.,  2024 )  for fine-tuning long-tail tasks, main modifications include adding support for iteration-based training and data loader for multiple datasets."
        ]
    },
    "id_table_11": {
        "caption": "(a)  Pretraining CLIP on CC3M and CC12M.",
        "table": "A1.T10.st2.1.1",
        "footnotes": [],
        "references": []
    }
}