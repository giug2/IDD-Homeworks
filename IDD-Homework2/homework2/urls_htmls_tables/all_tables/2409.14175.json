{
    "id_table_1": {
        "caption": "TABLE I:  Performance of Phi-2 model on the test dataset as obtained from submissions to Zindi",
        "table": "S5.T1.1",
        "footnotes": [],
        "references": [
            "To enable similarity search, we created associated embeddings for each chunk using an embedding model, as shown in Figure  1 . The choice of embedding model was influenced by models on the Massive Text Embedding Benchmark (MTEB) leaderboard   [ 16 ] . We opted for the best performing and easily accessible models, favouring the use of the models stella_en_400M_v5 and gte-Qwen2-1.5B-instruct  [ 17 ]  with 400M and 1.5B parameters respectively. We used these text embedding models from the Sentence Transformer library   [ 18 ]  and sped up the embedding process by batching the chunks, using a batch size of 64. The chunks and their corresponding embeddings were saved to disk to be used for context retrieval for the question answering task."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  Performance of Falcon7B model on the test dataset as obtained from submissions to Zindi",
        "table": "S5.T2.1",
        "footnotes": [],
        "references": [
            "For the chunk retrieval process, we employed a k-Nearest Neighbors (KNN) approach on the similarity scores between a question/query embedding and the chunk embeddings  [ 19 ] . The use of a dot product similarity score implies that higher scores denote higher similarity. Hence, we retrieve the top  k k k italic_k  similar chunks for a given question/query. The number of retrieved chunks is chosen such that the context length of the language model is not exceeded when creating the prompt. We used the top 2 chunks retrieved with each embedding model. Figure  2  shows how chunks are retrieved to form a context in the input prompt to an LLM. Additionally, we employed the use of the BM25   [ 20 ]  algorithm which is a statistical approach for information retrieval that measures similarity based on the frequency of terms from the query that appear in the chunks. The motivation behind this is to ensure that the retrieval also includes chunks containing specific terms used in the query but are not necessarily enforced in the neural embedding models.This enabled us to create a context that consists of chunks from 2 embedding models (stella_en_400M_v5 and gte-Qwen2-1.5B-instruct  [ 17 ] ) and BM25  [ 20 ] ."
        ]
    },
    "global_footnotes": []
}