{
    "id_table_1": {
        "caption": "Table 1:  Dataset Description",
        "table": "S3.T1.4.4",
        "footnotes": [],
        "references": [
            "To address this problem, we propose the framework illustrated in Figure  LABEL:fig:toy_example_b . The idea is to exploit another generative model,    \\Theta roman_ , to produce specific prompts that induce    \\Phi roman_  to generate elements identical or significantly similar to a subset of  D 2 subscript D 2 \\mathcal{D}_{2} caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . Specifically, given a value  v  D 2 v subscript D 2 v\\in\\mathcal{D}_{2} italic_v  caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , we aim at inferring a key  k  K k K k\\in K italic_k  italic_K , using    \\Theta roman_ , which attempts to force    \\Phi roman_  to generate either an exact copy or a slightly altered variant of  v v v italic_v . Formally, the framework is defined by Algorithm  1 , which allows finding violations, e.g., whether the data has been used without consent.",
            "Concerning synthesized data, we create group sequences adhering to distinct patterns. Each sequence is split into two parts: the first is generated from a multivariate Gaussian distribution with a specific mean, while the second is drawn from a different Gaussian distribution. As a result,  D t  r subscript D t r \\mathcal{D}_{tr} caligraphic_D start_POSTSUBSCRIPT italic_t italic_r end_POSTSUBSCRIPT ,  D v subscript D v \\mathcal{D}_{v} caligraphic_D start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , and  D n  c subscript D n c \\mathcal{D}_{nc} caligraphic_D start_POSTSUBSCRIPT italic_n italic_c end_POSTSUBSCRIPT  contain the same number of samples obtained from sequences belonging to differentiated patterns (which are devised by different Gaussian parameters). Through this scheme, we generated two distinct datasets with a  1 1 1 1 -minute granularity. The first dataset (named  Synthetic  in the following), contains three subsets that are well-separated and have no overlapping elements, i.e.,  D v  D c =  subscript D v subscript D c \\mathcal{D}_{v}\\cap\\mathcal{D}_{c}=\\emptyset caligraphic_D start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT  caligraphic_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT =   and  D n  c  D c =  subscript D n c subscript D c \\mathcal{D}_{nc}\\cap\\mathcal{D}_{c}=\\emptyset caligraphic_D start_POSTSUBSCRIPT italic_n italic_c end_POSTSUBSCRIPT  caligraphic_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT =  . The second dataset ( Synthetic-Overlap ) is characterized by overlapping Gaussian distributions that are used to generate both the first and third subsets. Table  1  summarizes the statistics of the datasets."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparative analysis of the framework with (Opt) and without (No-Opt) the speedup procedure.",
        "table": "S4.T2.40.40",
        "footnotes": [],
        "references": [
            "The rest of the paper is structured as follows. Section  2  reviews past research on membership inferences and similar attacks, while Section  3  presents our approach for generating prompts able to reveal the use of unauthorized data during the training phase. Section  4  showcases numerical results collected by considering realistic and synthetic datasets, and Section  5  concludes the paper and hints at future research directions.",
            "To find unauthorized usages of data or copyrighted information, we need to train our Prompt Generator    \\Theta roman_ . Its training process is illustrated in Figure  2  and formalized in Algorithm  2 .",
            "For speeding up the training, we consider the same input and output of the previous training algorithm (see Algorithm  2 ). Moreover, the algorithm takes as input two additional parameters, which are the patience value    \\alpha italic_  and the tolerance for patience    \\omega italic_ . Specifically, we initialize a list of indexes of elements in  D 2 subscript D 2 \\mathcal{D}_{2} caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . Then, the dataset  D 2 subscript D 2 \\mathcal{D}_{2} caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  is split into mini-batches  { B 1 , B 2 , ... } subscript B 1 subscript B 2 ... \\{\\mathcal{B}_{1},\\mathcal{B}_{2},\\ldots\\} { caligraphic_B start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_B start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... } . For each mini-batch  B B \\mathcal{B} caligraphic_B , we cumulatively collect within  errors errors \\mathit{errors} italic_errors  the errors    \\varepsilon italic_  resulting from the    \\Delta roman_  loss function between the predicted value    (   ( v ) )   v \\Phi(\\Theta(v)) roman_ ( roman_ ( italic_v ) )  and the actual value  v v v italic_v , as well as descend the gradient on the batch loss  g g g italic_g . Within the training procedure, a patience function is integrated, to manage when to drop data points based on the progression of the loss function. The idea of the patience mechanism is to avoid unnecessary iterations when there are no significant improvements in the loss, thus saving time and computational resources. For this, we keep track of the observed best loss value. If the current loss (namely,  mean  ( errors ) mean errors \\mathtt{mean}(\\mathit{errors}) typewriter_mean ( italic_errors ) ) does not improve above a minimum threshold    \\omega italic_  for a specific number of iterations    \\alpha italic_ , and the size of the index set  I I I italic_I  is above a predefined amount (one-third of the original dataset size in our framework), the algorithm cuts all the elements in  D 2 subscript D 2 \\mathcal{D}_{2} caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  that are above threshold value    \\tau italic_  that is closest to the  80 80 80 80 th percentile of the GPD fitted on the errors.",
            "Table  2  reports the results of the evaluation with (Opt) and without (No-Opt) the speedup procedure. In response to RQ1, the findings indicate that  CAP CAP \\mathsf{CAP} sansserif_CAP  effectively generates keys that compel the model    \\Phi roman_  to produce values used during its training, thereby uncovering copyright infringement. Notably, in almost all the real-world datasets, the copyrighted data consistently rank within the top  5 5 5 5  and  10 10 10 10  positions showing a precision equal or near to  100 100 100 100 % percent \\% % . By increasing the value of  K K K italic_K , we can observe a degradation of the performance. Furthermore, the model achieves an AUC-Gain close to  1 1 1 1 , demonstrating its high effectiveness in distinguishing between copyrighted and non-copyrighted instances. In other words, the model is nearly optimal in quickly identifying copyrighted data. To answer RQ2, we compare the performance and the running times of  CAP CAP \\mathsf{CAP} sansserif_CAP  with and without the speedup strategy. Specifically, the time optimization procedure proves to be highly effective in lowering training times without compromising the ability to detect copyright infringement. As reported in Table  2 , all metrics show minimal to no degradation when the optimization is applied. On the contrary, as illustrated in Figure  3 , the running times are substantially reduced across all datasets when the procedure is implemented.",
            "We point out that,  CAP CAP \\mathsf{CAP} sansserif_CAP  requires that the information to be checked exhibits traits of uniqueness, i.e., its distribution is specific enough to be considered representative of copyrighted material. Without this distinctiveness in the data, the model may struggle to accurately differentiate between original and derivative content. In such cases, it could either incorrectly attribute copyright to a generic piece of information or fail to distinguish between distributions that vary only slightly. This is illustrated in Figures  LABEL:fig:head_tsne  and  LABEL:fig:power_tsne , which depict a two-dimensional representation of datasets with different characteristics. The first dataset in Figure  LABEL:fig:head_tsne  is characterized by non-overlapping distributions, whereas Figure  LABEL:fig:power_tsne  showcases data characterized by two overlapping (and hence indistinguishable) slices of data. The behavior of  CAP CAP \\mathsf{CAP} sansserif_CAP  in these situations is illustrated in Table  2 . In fact, the low performance of the model on the  Electric Power Consumption  can be explained by the overlap between the copyrighted and non-copyrighted sets."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Comparative analysis of the framework with Synthetic and Synthetic-Overlap data.",
        "table": "S4.T3.10",
        "footnotes": [],
        "references": [
            "The rest of the paper is structured as follows. Section  2  reviews past research on membership inferences and similar attacks, while Section  3  presents our approach for generating prompts able to reveal the use of unauthorized data during the training phase. Section  4  showcases numerical results collected by considering realistic and synthetic datasets, and Section  5  concludes the paper and hints at future research directions.",
            "We exploit this property within  CAP CAP \\mathsf{CAP} sansserif_CAP  by devising a data reduction process that involves two steps. First, we fit a generalized Pareto distribution to the empirical error data. This involves estimating the parameters of the GPD that best represent the tail behavior of the error distribution. Second, we determine an appropriate threshold level based on the fitted GPD. In this context, we adopt the  80 80 80 80 th percentile as the reference threshold. This choice is guided by a heuristic application of the Pareto principle  (Wilkinson,  2006 ) , which posits that approximately  20 % percent 20 20\\% 20 %  of the causes are responsible for  80 % percent 80 80\\% 80 %  of the effects. The entire process is formalized in Algorithm  3 .",
            "In our experiments, based on the reference scenario from Section  3 , we create three balanced subsets from each real dataset using hierarchical clustering:  D t  r subscript D t r \\mathcal{D}_{tr} caligraphic_D start_POSTSUBSCRIPT italic_t italic_r end_POSTSUBSCRIPT ,  D v subscript D v \\mathcal{D}_{v} caligraphic_D start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , and  D n  c subscript D n c \\mathcal{D}_{nc} caligraphic_D start_POSTSUBSCRIPT italic_n italic_c end_POSTSUBSCRIPT . The first two subsets are used to train and validate model    \\Phi roman_ , forming dataset  D 1 subscript D 1 \\mathcal{D}_{1} caligraphic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . A small portion of  D t  r subscript D t r \\mathcal{D}_{tr} caligraphic_D start_POSTSUBSCRIPT italic_t italic_r end_POSTSUBSCRIPT , called  D c subscript D c \\mathcal{D}_{c} caligraphic_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , is sampled as copyrighted data used by    \\Phi roman_  without authorization. This data is combined with  D n  c subscript D n c \\mathcal{D}_{nc} caligraphic_D start_POSTSUBSCRIPT italic_n italic_c end_POSTSUBSCRIPT  (copyrighted data unseen by    \\Phi roman_ ) to form  D 2 subscript D 2 \\mathcal{D}_{2} caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT .",
            "Table  2  reports the results of the evaluation with (Opt) and without (No-Opt) the speedup procedure. In response to RQ1, the findings indicate that  CAP CAP \\mathsf{CAP} sansserif_CAP  effectively generates keys that compel the model    \\Phi roman_  to produce values used during its training, thereby uncovering copyright infringement. Notably, in almost all the real-world datasets, the copyrighted data consistently rank within the top  5 5 5 5  and  10 10 10 10  positions showing a precision equal or near to  100 100 100 100 % percent \\% % . By increasing the value of  K K K italic_K , we can observe a degradation of the performance. Furthermore, the model achieves an AUC-Gain close to  1 1 1 1 , demonstrating its high effectiveness in distinguishing between copyrighted and non-copyrighted instances. In other words, the model is nearly optimal in quickly identifying copyrighted data. To answer RQ2, we compare the performance and the running times of  CAP CAP \\mathsf{CAP} sansserif_CAP  with and without the speedup strategy. Specifically, the time optimization procedure proves to be highly effective in lowering training times without compromising the ability to detect copyright infringement. As reported in Table  2 , all metrics show minimal to no degradation when the optimization is applied. On the contrary, as illustrated in Figure  3 , the running times are substantially reduced across all datasets when the procedure is implemented.",
            "To further strengthen this aspect and answer RQ3, we have also conducted in-vitro experiments. The results, reported in Table  3 , show that on the  Synthetic-Overlap  dataset, where the distributions of the copyrighted and non-copyrighted sets overlap, the model struggles to accurately identify copyrighted data due to their similarity to the non-copyrighted ones. In contrast, on the  Synthetic  dataset, where data is generated using separate distributions, for all the values of  K K K italic_K , the model achieves perfect scores. Additionally, an AUC-Gain of  1.00 1.00 1.00 1.00  demonstrates the ability of  CAP CAP \\mathsf{CAP} sansserif_CAP  to distinguish between copyrighted and non-copyrighted data perfectly."
        ]
    }
}