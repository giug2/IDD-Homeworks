{
    "S6.T1": {
        "caption": "Table 1: Comparison of standard and meta-learning algorithms in terms of test RMSE in 5 meta-learning environments for regression. Reported are mean and standard deviation across 5 seeds. Our proposed method PACOH achieves the best performance across all tasks.",
        "table": null,
        "footnotes": [],
        "references": [
            "PACOH improves the predictive accuracy.\nUsing the meta-learning environments and baseline methods that we introduced in Sec. 6.1, we perform a comprehensive benchmark study. TableÂ 1 reports the results on the regression environments in terms of the root mean squared error (RMSE) on unseen test tasks. Among the approaches, PACOH-NN and PACOH-GP consistently perform best or are among the best methods. Similarly, PACOH-NN achieves the highest accuracy in the Omniglot classification environment (cf. Table 3). Overall, this demonstrates that the introduced meta-learning framework is not only sound, but also yields state-of-the-art empirical performance in practice."
        ]
    },
    "S6.T2": {
        "caption": "Table 2: Comparison of standard and meta-learning methods in terms of the test calibration error in 5 regression environments. We report the mean and standard deviation across 5 random seeds. PACOH yields the best uncertainty calibration in the majority of environments.",
        "table": null,
        "footnotes": [],
        "references": [
            "PACOH improves the predictive uncertainty.\nWe hypothesize that by acquiring the prior in a principled data-driven manner (e.g., with PACOH), we can improve the quality of the GPâ€™s and BNNâ€™s uncertainty estimates. To investigate the effect of meta-learned priors on the uncertainty estimates of the base learners, we compute the probabilistic predictorsâ€™ calibration errors, reported in TableÂ 2 andÂ 3. The calibration error measures the discrepancy between the predicted confidence regions and the actual frequencies of test data in the respective areasÂ (Kuleshov etÂ al., 2018). Note that, since MAML only produces point predictions, the concept of calibration does not apply to it. We observe that meta-learning priors with PACOH-NN consistently improves the Vanilla BNNâ€™s uncertainty estimates. Similarly, PACOH-GP yields a lower calibration error than the Vanilla GP in the majority of the envionments. For meta-learning environments where the task similarity is high, like SwissFEL and Berkeley-Sensor, the improvement is substantial.",
            "If the classifier is calibrated, we expect that the confidence of the classifier reflects itâ€™s accuracy on unseen test data, that is, acc(Bh)=conf(Bh)âˆ€h=1,â€¦.,H\\text{acc}(B_{h})=\\text{conf}(B_{h})~{}\\forall h=1,....,H. As proposed of Guo etÂ al. (2017), we use the expected calibration error (ECE) to quantify how much the classifier deviates from this criterion: More precisely, in Table 2, we report the ECE with the following definition:"
        ]
    },
    "S6.T3": {
        "caption": "Table 3: Comparison of meta-learning algorithms in terms of test accuracy and calibration error on the Omniglot environment. Among the methods, PACOH-NN makes the most accurate and best-calibrated class predictions.",
        "table": null,
        "footnotes": [],
        "references": [
            "PACOH improves the predictive accuracy.\nUsing the meta-learning environments and baseline methods that we introduced in Sec. 6.1, we perform a comprehensive benchmark study. TableÂ 1 reports the results on the regression environments in terms of the root mean squared error (RMSE) on unseen test tasks. Among the approaches, PACOH-NN and PACOH-GP consistently perform best or are among the best methods. Similarly, PACOH-NN achieves the highest accuracy in the Omniglot classification environment (cf. Table 3). Overall, this demonstrates that the introduced meta-learning framework is not only sound, but also yields state-of-the-art empirical performance in practice.",
            "PACOH improves the predictive uncertainty.\nWe hypothesize that by acquiring the prior in a principled data-driven manner (e.g., with PACOH), we can improve the quality of the GPâ€™s and BNNâ€™s uncertainty estimates. To investigate the effect of meta-learned priors on the uncertainty estimates of the base learners, we compute the probabilistic predictorsâ€™ calibration errors, reported in TableÂ 2 andÂ 3. The calibration error measures the discrepancy between the predicted confidence regions and the actual frequencies of test data in the respective areasÂ (Kuleshov etÂ al., 2018). Note that, since MAML only produces point predictions, the concept of calibration does not apply to it. We observe that meta-learning priors with PACOH-NN consistently improves the Vanilla BNNâ€™s uncertainty estimates. Similarly, PACOH-GP yields a lower calibration error than the Vanilla GP in the majority of the envionments. For meta-learning environments where the task similarity is high, like SwissFEL and Berkeley-Sensor, the improvement is substantial.",
            "PACOH combats meta-overfitting.\nAs Qin etÂ al. (2018) and Yin etÂ al. (2020) point out, many popular meta-learners (e.g., Finn etÂ al., 2017; Garnelo etÂ al., 2018) require a large number of meta-training tasks to generalize well. When presented with only a limited number of tasks, such algorithms suffer from severe meta-overfitting, adversely impacting their performance on unseen tasks from ğ’¯ğ’¯\\mathcal{T}. This can even lead to negative transfer, such that meta-learning actually hurts the performance when compared to standard learning. In our experiments, we also observe such failure cases: For instance, in the classification environment (cf. Table 3), MAML fails to improve upon the Vanilla BNN. Similarly, in the regression environments (cf. Table 3) we find that NPs, BMAML and MLL-GP often yield worse-calibrated predictive distributions than the Vanilla BNN and GP respectively.\nIn contrast, thanks to its theoretically principled construction, PACOH-NN is able to achieve positive transfer even when the tasks are diverse and small in number. In particular, the hyper-prior acts as meta-level regularizer by penalizing complex priors that are unlikely to convey useful inductive bias for unseen learning tasks."
        ]
    },
    "A5.T1": {
        "caption": "Table S1: Number of tasks nğ‘›n and samples per task misubscriptğ‘šğ‘–m_{i} for the different meta-learning environments.",
        "table": "<table id=\"A5.T1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A5.T1.2.3.1\" class=\"ltx_tr\">\n<th id=\"A5.T1.2.3.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\"/>\n<th id=\"A5.T1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Sinusoid</th>\n<th id=\"A5.T1.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Cauchy</th>\n<th id=\"A5.T1.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SwissFEL</th>\n<th id=\"A5.T1.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Physionet</th>\n<th id=\"A5.T1.2.3.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Berkeley</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A5.T1.1.1\" class=\"ltx_tr\">\n<th id=\"A5.T1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><math id=\"A5.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"n\" display=\"inline\"><semantics id=\"A5.T1.1.1.1.m1.1a\"><mi id=\"A5.T1.1.1.1.m1.1.1\" xref=\"A5.T1.1.1.1.m1.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"A5.T1.1.1.1.m1.1b\"><ci id=\"A5.T1.1.1.1.m1.1.1.cmml\" xref=\"A5.T1.1.1.1.m1.1.1\">ğ‘›</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T1.1.1.1.m1.1c\">n</annotation></semantics></math></th>\n<td id=\"A5.T1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">20</td>\n<td id=\"A5.T1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">20</td>\n<td id=\"A5.T1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">5</td>\n<td id=\"A5.T1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">100</td>\n<td id=\"A5.T1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">36</td>\n</tr>\n<tr id=\"A5.T1.2.2\" class=\"ltx_tr\">\n<th id=\"A5.T1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><math id=\"A5.T1.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"m_{i}\" display=\"inline\"><semantics id=\"A5.T1.2.2.1.m1.1a\"><msub id=\"A5.T1.2.2.1.m1.1.1\" xref=\"A5.T1.2.2.1.m1.1.1.cmml\"><mi id=\"A5.T1.2.2.1.m1.1.1.2\" xref=\"A5.T1.2.2.1.m1.1.1.2.cmml\">m</mi><mi id=\"A5.T1.2.2.1.m1.1.1.3\" xref=\"A5.T1.2.2.1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A5.T1.2.2.1.m1.1b\"><apply id=\"A5.T1.2.2.1.m1.1.1.cmml\" xref=\"A5.T1.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A5.T1.2.2.1.m1.1.1.1.cmml\" xref=\"A5.T1.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"A5.T1.2.2.1.m1.1.1.2.cmml\" xref=\"A5.T1.2.2.1.m1.1.1.2\">ğ‘š</ci><ci id=\"A5.T1.2.2.1.m1.1.1.3.cmml\" xref=\"A5.T1.2.2.1.m1.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T1.2.2.1.m1.1c\">m_{i}</annotation></semantics></math></th>\n<td id=\"A5.T1.2.2.2\" class=\"ltx_td ltx_align_center\">5</td>\n<td id=\"A5.T1.2.2.3\" class=\"ltx_td ltx_align_center\">20</td>\n<td id=\"A5.T1.2.2.4\" class=\"ltx_td ltx_align_center\">200</td>\n<td id=\"A5.T1.2.2.5\" class=\"ltx_td ltx_align_center\">4 - 24</td>\n<td id=\"A5.T1.2.2.6\" class=\"ltx_td ltx_align_center\">288</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "In this section, we provide further details on the meta-learning environments used in SectionÂ 5.3.\nInformation about the numbers of tasks and samples in the respective environments can be found in TableÂ S1."
        ]
    },
    "A5.T2": {
        "caption": "Table S2: MHC-I alleles used for meta-training and their corresponding number of meta-training samples misubscriptğ‘šğ‘–m_{i}.",
        "table": "<table id=\"A5.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A5.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"A5.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\">Allele</th>\n<th id=\"A5.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">A-0202</th>\n<th id=\"A5.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">A-0203</th>\n<th id=\"A5.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">A-0201</th>\n<th id=\"A5.T2.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">A-2301</th>\n<th id=\"A5.T2.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">A-2402</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A5.T2.1.1\" class=\"ltx_tr\">\n<th id=\"A5.T2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><math id=\"A5.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"m_{i}\" display=\"inline\"><semantics id=\"A5.T2.1.1.1.m1.1a\"><msub id=\"A5.T2.1.1.1.m1.1.1\" xref=\"A5.T2.1.1.1.m1.1.1.cmml\"><mi id=\"A5.T2.1.1.1.m1.1.1.2\" xref=\"A5.T2.1.1.1.m1.1.1.2.cmml\">m</mi><mi id=\"A5.T2.1.1.1.m1.1.1.3\" xref=\"A5.T2.1.1.1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A5.T2.1.1.1.m1.1b\"><apply id=\"A5.T2.1.1.1.m1.1.1.cmml\" xref=\"A5.T2.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A5.T2.1.1.1.m1.1.1.1.cmml\" xref=\"A5.T2.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"A5.T2.1.1.1.m1.1.1.2.cmml\" xref=\"A5.T2.1.1.1.m1.1.1.2\">ğ‘š</ci><ci id=\"A5.T2.1.1.1.m1.1.1.3.cmml\" xref=\"A5.T2.1.1.1.m1.1.1.3\">ğ‘–</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T2.1.1.1.m1.1c\">m_{i}</annotation></semantics></math></th>\n<td id=\"A5.T2.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">1446</td>\n<td id=\"A5.T2.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1442</td>\n<td id=\"A5.T2.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">3088</td>\n<td id=\"A5.T2.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">103</td>\n<td id=\"A5.T2.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">196</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We use 5 alleles to meta-learn a BNN prior. The alleles and the corresponding number of data points, available for meta-training, are listed in Table S2. The most genetically dissimilar allele (A-6901) is used for our bandit task. In each iteration, the experimenter (i.e. bandit algorithm) chooses to test one peptide among the pool of 813 candidates and receives rğ‘Ÿr as a reward feedback. Hence, we are concerned with a 813-arm bandit wherein the action atâˆˆ{1,â€¦,813}=ğ’œsubscriptğ‘ğ‘¡1â€¦813ğ’œa_{t}\\ \\in\\{1,...,813\\}=\\mathcal{A} in iteration tğ‘¡t corresponds to testing atsubscriptğ‘ğ‘¡a_{t}-th peptide candidate. In response, the algorithm receives the respective negative log-IC50subscriptIC50\\text{IC}_{50} as reward râ€‹(at)ğ‘Ÿsubscriptğ‘ğ‘¡r(a_{t})."
        ]
    }
}