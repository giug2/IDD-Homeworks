{
    "id_table_1": {
        "caption": "Table 1:  Best hyperparameter combinations for each environment and algorithm (Atari).",
        "table": "S16.EGx1",
        "footnotes": [],
        "references": [
            "Illustrative Example.  We now show how existing SPI baselines (PQI, SPIBB, CQL) fail to choose optimal actions while avoiding risky ones. To demonstrate how a pessimism-based approach fails, consider the first MDP in Figure  1 . There are three actions at the starting state  s 0 subscript s 0 s_{0} italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT : an optimal action,  a 0 subscript a 0 a_{0} italic_a start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , leading to a forest of sparsely-visited states with good outcomes; a risky action,  a 2 subscript a 2 a_{2} italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , leading to a forest of states with bad outcomes; and a suboptimal, risk-free action,  a 1 subscript a 1 a_{1} italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , that leads to a middle road and is chosen by the behavior most of the time. A pessimism-based approach like PQIwhich penalizes value estimates based on uncertaintyfails to learn that, even though the ( s , a s a s,a italic_s , italic_a ) pairs in the optimal forest are sparse in the data, we have enough observations to conclude that the good forest is the better choice. It should be noted that reducing the threshold for ( s , a s a s,a italic_s , italic_a ) density may result in taking the risky action.",
            "These two MDP scenarios illustrate how current SPI methods struggle with different types of action-risk dynamics. This intuition is supported empirically. In Figure  1  (left plot), we see that PQI fails to attain a high 5% conditional value at risk (CVaR 5%) (a metric used to measure safe improvement) even for relatively large dataset sizes in the forest MDP. Similarly, in the second MDP (right plot), CQL and SPIBB are not able to consistently learn a safe policy. In both cases, we can set an  N m  i  n subscript N m i n N_{min} italic_N start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT  parameter in DPRL to ignore actions that cannot be reliably estimated to have a good value. These examples also illuminate the settings in which DPRL performs particularly well: when there are systematic deviations from the behavior policy and when we are not required to act at all points in the state-action space.",
            "Proof sketch.  The key fact we exploit is the property that   DP subscript  DP \\pi_{\\text{DP}} italic_ start_POSTSUBSCRIPT DP end_POSTSUBSCRIPT  always takes an advantageous action with respect to  Q ^ b   V ^ b  subscript superscript ^ Q  b subscript superscript ^ V  b \\hat{Q}^{\\pi}_{\\text{b}}-\\hat{V}^{\\pi}_{\\text{b}} over^ start_ARG italic_Q end_ARG start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT b end_POSTSUBSCRIPT - over^ start_ARG italic_V end_ARG start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT b end_POSTSUBSCRIPT . We express the  Q ^ b  subscript superscript ^ Q  b \\hat{Q}^{\\pi}_{\\text{b}} over^ start_ARG italic_Q end_ARG start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT b end_POSTSUBSCRIPT  and  V ^ b  subscript superscript ^ V  b \\hat{V}^{\\pi}_{\\text{b}} over^ start_ARG italic_V end_ARG start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT b end_POSTSUBSCRIPT  as the first-visit monte-carlo average of the observed returns, making sure to split the returns into independent random variables (since the returns used to estimate the  Q Q Q italic_Q -values and  V V V italic_V -values may overlap). We then bound the advantage by using the fact that the advantage is zero for  ( s , a ) s a (s,a) ( italic_s , italic_a )  pairs for the states we defer, and bounded for the  ( s , a ) s a (s,a) ( italic_s , italic_a )  pairs that are observed at least  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  times. (See Appendix  10  for the full proof.)",
            "Comparison to baselines.  We summarize the main differences between our bound and prior work here. The bounds in prior work are reproduced in Appendix  11  for reference. Most importantly, our dependence on  | S | S \\left|{\\mathcal{S}}\\right| | caligraphic_S |  and  | A | A \\left|{\\mathcal{A}}\\right| | caligraphic_A |  comes indirectly through the  C  ( N  ) C subscript N C(N_{\\wedge}) italic_C ( italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT )  term, which differs from the direct dependence on  | S | S \\left|{\\mathcal{S}}\\right| | caligraphic_S |  and  | A | A \\left|{\\mathcal{A}}\\right| | caligraphic_A |  by SPI and pessimism-based methods  (Liu et al.,  2020 ; Kim and Oh,  2023 ) . Thus, our bound is much tighter when the behavior policy has only visited a small subset of the state-action space more than  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  times: we scale in the number of visited parts of the state-action space  C  ( N  ) C subscript N C(N_{\\wedge}) italic_C ( italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT )  rather than  | S |  | A | S A \\left|{\\mathcal{S}}\\right|\\left|{\\mathcal{A}}\\right| | caligraphic_S | | caligraphic_A | . This difference would be most present when the behavior policy and transition dynamics are close to deterministic, or when the size of dataset is small relative to the size of the state-action space. On the other hand, when the size of the dataset is large and the behavior distribution is closer to uniform, all bounds will be tight. Our dependence on the  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  parameter and effective horizon  1 / ( 1   ) 1 1  1/(1-\\gamma) 1 / ( 1 - italic_ )  matches the SPI  (Laroche et al.,  2019 ; Scholl et al.,  2022 ; Wienhoft et al.,  2023b )  literature. However, that we do not require access to   b subscript  b \\pi_{b} italic_ start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT . Unlike Corollary 2 of  (Liu et al.,  2020 ) , our bound does not have a dependence on the threshold  b b b italic_b  of the state-action density   ^  ( s , a ) ^  s a \\hat{\\mu}(s,a) over^ start_ARG italic_ end_ARG ( italic_s , italic_a ) , which has a direct correspondence to our  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  parameter as  b = N  / | D | b subscript N D b=N_{\\wedge}/|D| italic_b = italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT / | italic_D | . As a result, their bound gets looser as  b b b italic_b  gets smaller. This happens when the size of the dataset gets larger while keeping the set of  ( s , a ) s a (s,a) ( italic_s , italic_a )  pairs with  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  observations unchanged (i.e., more trajectories were added in the low-density regions). Our DPRL bound is unaffected by this superfluous extra data.",
            "DPRL achieves better theoretical bounds.  On the MDP and GridWorld (Figures  1  and 2 , we see that the DPRL provides safe policy improvement (CVaR). In GridWorld plots, we see that the CVaR without mean value being affected. The  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  parameter allows us to control the trade-off between performance and safety, as shown in Figure  2 (right): as  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  increases, the mean value (performance) decreases, but the safety increases. On both the MDP and GridWorld, we see that the DPRL provides tighter bounds compared to the baselines, as shown in Figure  1 (center). The bounds are tighter because the  C  ( N  ) C subscript N C(N_{\\wedge}) italic_C ( italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT )  term in our bound is much smaller than the  | S |  | A | S A \\left|{\\mathcal{S}}\\right|\\left|{\\mathcal{A}}\\right| | caligraphic_S | | caligraphic_A |  term in the SPIBB and PQI bounds. Also, since the bound is data-dependent, it does not degrade as quickly as SPIBB and PQI for increase in  | S | S \\left|{\\mathcal{S}}\\right| | caligraphic_S | .",
            "See Theorem 2.3 of  (Hajek and Raginsky,  2019 )     \\square    See  1",
            "We conducted extensive experiments with the following hyperparameter settings (see Table  1  for the best hyperparameters):"
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Architecture used by each DQN and Representation Learning Network (Atari)",
        "table": "S16.EGx2",
        "footnotes": [],
        "references": [
            "For each state  s  S DP s superscript S DP s\\in{\\mathcal{S}}^{\\text{DP}} italic_s  caligraphic_S start_POSTSUPERSCRIPT DP end_POSTSUPERSCRIPT , action  a  A s DP a subscript superscript A DP s a\\in{\\mathcal{A}}^{\\text{DP}}_{s} italic_a  caligraphic_A start_POSTSUPERSCRIPT DP end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , and next state  s   S DP superscript s  superscript S DP s^{\\prime}\\in{\\mathcal{S}}^{\\text{DP}} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  caligraphic_S start_POSTSUPERSCRIPT DP end_POSTSUPERSCRIPT , we say that a transition  ( s , a , s  , k ) s a superscript s  k (s,a,s^{\\prime},k) ( italic_s , italic_a , italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_k )  exists in a trajectory if  s  superscript s  s^{\\prime} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is the first decision point in the trajectory starting from  ( s , a ) s a (s,a) ( italic_s , italic_a )  and taking  k k k italic_k  steps (only considering first visits), and  r ~  ( n , s , a , s  , k ) ~ r n s a superscript s  k \\tilde{r}(n,s,a,s^{\\prime},k) over~ start_ARG italic_r end_ARG ( italic_n , italic_s , italic_a , italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_k )  is the discounted sum of rewards in the trajectory  n n n italic_n  starting from  ( s , a ) s a (s,a) ( italic_s , italic_a )  and ending in  s  superscript s  s^{\\prime} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  after  k k k italic_k  steps, and  0 0  if no such transition exists. Then, the counts  n ~  ( s , a , s  , k ) ~ n s a superscript s  k \\tilde{n}(s,a,s^{\\prime},k) over~ start_ARG italic_n end_ARG ( italic_s , italic_a , italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_k )  are defined as the number of times  ( s , a , s  , k ) s a superscript s  k (s,a,s^{\\prime},k) ( italic_s , italic_a , italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_k )  exists in the dataset, and  P ~  ( s  | s , a ) ~ P conditional superscript s  s a \\tilde{P}(s^{\\prime}|s,a) over~ start_ARG italic_P end_ARG ( italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT | italic_s , italic_a ) ,   ~  ( s , a , s  ) ~  s a superscript s  \\tilde{\\gamma}(s,a,s^{\\prime}) over~ start_ARG italic_ end_ARG ( italic_s , italic_a , italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  and  R ~  ( s , a ) ~ R s a \\tilde{R}(s,a) over~ start_ARG italic_R end_ARG ( italic_s , italic_a )  are computed from these counts (see Algorithm  2  in the appendix for details):",
            "We describe the MDPs in Section  4 . For both MDPs, we vary the number of trajectories in the datasets. For our  10  10 10 10 10\\times 10 10  10  GridWorld, we create datasets by simulating a careless expert which is optimal everywhere except in a few states, where it chooses the worst action with probability 0.9 (marked orange in Figure  2 ). We sample datasets of  n n n italic_n  trajectories,  n  { 10 , 25 , 50 , 100 , 500 } n 10 25 50 100 500 n\\in\\{10,25,50,100,500\\} italic_n  { 10 , 25 , 50 , 100 , 500 } .",
            "DPRL achieves better theoretical bounds.  On the MDP and GridWorld (Figures  1  and 2 , we see that the DPRL provides safe policy improvement (CVaR). In GridWorld plots, we see that the CVaR without mean value being affected. The  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  parameter allows us to control the trade-off between performance and safety, as shown in Figure  2 (right): as  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  increases, the mean value (performance) decreases, but the safety increases. On both the MDP and GridWorld, we see that the DPRL provides tighter bounds compared to the baselines, as shown in Figure  1 (center). The bounds are tighter because the  C  ( N  ) C subscript N C(N_{\\wedge}) italic_C ( italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT )  term in our bound is much smaller than the  | S |  | A | S A \\left|{\\mathcal{S}}\\right|\\left|{\\mathcal{A}}\\right| | caligraphic_S | | caligraphic_A |  term in the SPIBB and PQI bounds. Also, since the bound is data-dependent, it does not degrade as quickly as SPIBB and PQI for increase in  | S | S \\left|{\\mathcal{S}}\\right| | caligraphic_S | .",
            "DPRL better manages the bias/variance tradeoff than existing methods.  Figure  2 (center) provides an insight into how  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  achieves the trade-off between safety and performance. As  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  increases, the fraction of states where the optimal action is allowed decreases, leading to increase in bias. At the same time, the fraction of states where a better action is chosen increases (considering states where multiple actions are allowed). This suggests reduced variance in value estimation.",
            "\\square    See  2",
            "The GridWorld has 100 states and 4 actions. The agent must start at the bottom left cell and reach the top right cell. The dynamics are stochastic with going to the intended cell with a probability of  0.9 0.9 0.9 0.9  and a simulating a random action otherwise. The rewards are stochastic, and described in Figure  2 (A). We sampled 500 random datasets from the environment to evaluate the reliability of the algorithms. For DPRL, SPIBB and PQI, we tested the  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  parameter in  { 1 , 2 , 5 , 10 , 20 , 30 } 1 2 5 10 20 30 \\{1,2,5,10,20,30\\} { 1 , 2 , 5 , 10 , 20 , 30 }  and found  N  = 20 subscript N 20 N_{\\wedge}=20 italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT = 20  to be good for all the algorithms. For CQL, we varied    \\alpha italic_  in  { 0.01 , 0.05 , 0.1 } 0.01 0.05 0.1 \\{0.01,0.05,0.1\\} { 0.01 , 0.05 , 0.1 } ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  All Hyperparameters for DQN used for training the expert policy",
        "table": "S16.EGx3",
        "footnotes": [],
        "references": [
            "Continuous Case.  For continuous state spaces, we describe a variant of the DP algorithm that can be used to provide safe policy improvements for any given state (for pseudocode, see Algorithm  3  in the appendix). The algorithm involves storing the dataset  D D {\\mathcal{D}} caligraphic_D . We define the following distance metric over state-action pairs  ( s , a ) s a (s,a) ( italic_s , italic_a )  and states  s s s italic_s :"
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Original and Handcrafted Features with Weights and Data Types (MIMIC Dataset)",
        "table": "S16.EGx4",
        "footnotes": [],
        "references": [
            "We describe the MDPs in Section  4 . For both MDPs, we vary the number of trajectories in the datasets. For our  10  10 10 10 10\\times 10 10  10  GridWorld, we create datasets by simulating a careless expert which is optimal everywhere except in a few states, where it chooses the worst action with probability 0.9 (marked orange in Figure  2 ). We sample datasets of  n n n italic_n  trajectories,  n  { 10 , 25 , 50 , 100 , 500 } n 10 25 50 100 500 n\\in\\{10,25,50,100,500\\} italic_n  { 10 , 25 , 50 , 100 , 500 } .",
            "DPRL achieves good performance in high-dimensional settings.  On Atari domains, in Figure  5 , we observe that DPRL achieves good performance even with 100K samples and a careless expert who takes the worst action  50 % percent 50 50\\% 50 %  of the time. In the hypotension dataset with continuous states, DPRL is the only method that achieves higher OPE estimates than the behavior baseline (Figure  4 ). We also see how the ability to defer can be useful in such a complex task: for the chosen parameters  N  = 50 , r = 10 formulae-sequence subscript N 50 r 10 N_{\\wedge}=50,r=10 italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT = 50 , italic_r = 10 , DPRL defers more than  95 % percent 95 95\\% 95 %  of the time! Such a policy is also actionable, since the physician can review the actions much faster than with the other methods.",
            "To create the distance function, we used a weighted Euclidean metric with weights derived from  Gottesman et al. ( 2020 )  on this dataset and cohort. The final set of features includes the original features as well as a collection of handcrafted features, which were determined by domain experts to be relevant for the hypotension task. We provide the original set of features and the final set of features, along with their corresponding weights, in Table  4 ."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S16.EGx5",
        "footnotes": [],
        "references": [
            "DPRL achieves good performance in high-dimensional settings.  On Atari domains, in Figure  5 , we observe that DPRL achieves good performance even with 100K samples and a careless expert who takes the worst action  50 % percent 50 50\\% 50 %  of the time. In the hypotension dataset with continuous states, DPRL is the only method that achieves higher OPE estimates than the behavior baseline (Figure  4 ). We also see how the ability to defer can be useful in such a complex task: for the chosen parameters  N  = 50 , r = 10 formulae-sequence subscript N 50 r 10 N_{\\wedge}=50,r=10 italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT = 50 , italic_r = 10 , DPRL defers more than  95 % percent 95 95\\% 95 %  of the time! Such a policy is also actionable, since the physician can review the actions much faster than with the other methods."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "S16.EGx6",
        "footnotes": [],
        "references": [
            "We now describe our decision-point RL method, which addresses several of the challenges with existing algorithms. We provide bounds for this method in Sec.  6 ."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "S16.EGx7",
        "footnotes": [],
        "references": [
            "Access to behavior.  While SPIBB appears to be similar to DPRL in its use of a count parameter like  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT , it requires access to the true behavior policy at training timelike most SPI approaches  (Laroche et al.,  2019 ; Scholl et al.,  2022 ; Wienhoft et al.,  2023b ) . We show in Section  7  that its performance can be significantly worse without access to the behavior policy. In contrast, we do not require access to the behavior policy during training time. This is an important consideration in real-world systems, where it can be difficult to elicit the behavior policy. For example, when working with doctors, we cannot expect to know the functional form of their behavior. However, we do require access to the behavior policy for  evaluation , which can be achieved by either running a silent trial or by using off-policy evaluation (OPE). OPE in a learning-to-assist framework is a promising area of research, and beyond the scope of this work."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "S16.EGx8",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "",
        "table": "S16.EGx9",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "",
        "table": "S16.EGx10",
        "footnotes": [],
        "references": [
            "Proof sketch.  The key fact we exploit is the property that   DP subscript  DP \\pi_{\\text{DP}} italic_ start_POSTSUBSCRIPT DP end_POSTSUBSCRIPT  always takes an advantageous action with respect to  Q ^ b   V ^ b  subscript superscript ^ Q  b subscript superscript ^ V  b \\hat{Q}^{\\pi}_{\\text{b}}-\\hat{V}^{\\pi}_{\\text{b}} over^ start_ARG italic_Q end_ARG start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT b end_POSTSUBSCRIPT - over^ start_ARG italic_V end_ARG start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT b end_POSTSUBSCRIPT . We express the  Q ^ b  subscript superscript ^ Q  b \\hat{Q}^{\\pi}_{\\text{b}} over^ start_ARG italic_Q end_ARG start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT b end_POSTSUBSCRIPT  and  V ^ b  subscript superscript ^ V  b \\hat{V}^{\\pi}_{\\text{b}} over^ start_ARG italic_V end_ARG start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT b end_POSTSUBSCRIPT  as the first-visit monte-carlo average of the observed returns, making sure to split the returns into independent random variables (since the returns used to estimate the  Q Q Q italic_Q -values and  V V V italic_V -values may overlap). We then bound the advantage by using the fact that the advantage is zero for  ( s , a ) s a (s,a) ( italic_s , italic_a )  pairs for the states we defer, and bounded for the  ( s , a ) s a (s,a) ( italic_s , italic_a )  pairs that are observed at least  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  times. (See Appendix  10  for the full proof.)"
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "S16.EGx11",
        "footnotes": [],
        "references": [
            "Comparison to baselines.  We summarize the main differences between our bound and prior work here. The bounds in prior work are reproduced in Appendix  11  for reference. Most importantly, our dependence on  | S | S \\left|{\\mathcal{S}}\\right| | caligraphic_S |  and  | A | A \\left|{\\mathcal{A}}\\right| | caligraphic_A |  comes indirectly through the  C  ( N  ) C subscript N C(N_{\\wedge}) italic_C ( italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT )  term, which differs from the direct dependence on  | S | S \\left|{\\mathcal{S}}\\right| | caligraphic_S |  and  | A | A \\left|{\\mathcal{A}}\\right| | caligraphic_A |  by SPI and pessimism-based methods  (Liu et al.,  2020 ; Kim and Oh,  2023 ) . Thus, our bound is much tighter when the behavior policy has only visited a small subset of the state-action space more than  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  times: we scale in the number of visited parts of the state-action space  C  ( N  ) C subscript N C(N_{\\wedge}) italic_C ( italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT )  rather than  | S |  | A | S A \\left|{\\mathcal{S}}\\right|\\left|{\\mathcal{A}}\\right| | caligraphic_S | | caligraphic_A | . This difference would be most present when the behavior policy and transition dynamics are close to deterministic, or when the size of dataset is small relative to the size of the state-action space. On the other hand, when the size of the dataset is large and the behavior distribution is closer to uniform, all bounds will be tight. Our dependence on the  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  parameter and effective horizon  1 / ( 1   ) 1 1  1/(1-\\gamma) 1 / ( 1 - italic_ )  matches the SPI  (Laroche et al.,  2019 ; Scholl et al.,  2022 ; Wienhoft et al.,  2023b )  literature. However, that we do not require access to   b subscript  b \\pi_{b} italic_ start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT . Unlike Corollary 2 of  (Liu et al.,  2020 ) , our bound does not have a dependence on the threshold  b b b italic_b  of the state-action density   ^  ( s , a ) ^  s a \\hat{\\mu}(s,a) over^ start_ARG italic_ end_ARG ( italic_s , italic_a ) , which has a direct correspondence to our  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  parameter as  b = N  / | D | b subscript N D b=N_{\\wedge}/|D| italic_b = italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT / | italic_D | . As a result, their bound gets looser as  b b b italic_b  gets smaller. This happens when the size of the dataset gets larger while keeping the set of  ( s , a ) s a (s,a) ( italic_s , italic_a )  pairs with  N  subscript N N_{\\wedge} italic_N start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  observations unchanged (i.e., more trajectories were added in the low-density regions). Our DPRL bound is unaffected by this superfluous extra data."
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "S16.EGx12",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "S16.EGx13",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "S16.EGx14",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "S16.EGx15",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "S16.EGx16",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "S16.EGx17",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "S16.EGx18",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "S16.EGx19",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "S16.EGx20",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "",
        "table": "S16.EGx21",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "",
        "table": "S16.EGx22",
        "footnotes": [],
        "references": []
    },
    "id_table_23": {
        "caption": "",
        "table": "S16.EGx23",
        "footnotes": [],
        "references": []
    },
    "id_table_24": {
        "caption": "",
        "table": "S16.EGx24",
        "footnotes": [],
        "references": []
    },
    "id_table_25": {
        "caption": "",
        "table": "S15.T1.35",
        "footnotes": [],
        "references": []
    },
    "id_table_26": {
        "caption": "",
        "table": "S15.T2.2",
        "footnotes": [],
        "references": []
    },
    "id_table_27": {
        "caption": "",
        "table": "S15.T3.3",
        "footnotes": [],
        "references": []
    },
    "id_table_28": {
        "caption": "",
        "table": "S16.T4.1",
        "footnotes": [],
        "references": []
    }
}