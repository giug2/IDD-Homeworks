{
    "id_table_1": {
        "caption": "Table 1.  A comparison of the retrieval approaches on a 500 set sample of CRAG dev set against Accuracy and CRAG Score.",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "The results of this experiment are shown in Table  1 . From the results, the cross-encoder is the best performing system, thus we used it for our retriever. We suspect that with proper tuning TF-IDF would and ensembling would be much more performant overall, but as mentioned running extensive experimentation is difficult as it required LLM generation to get an overall accuracy score. Using an LLM to label passages as relevant or not is a possible approach to allow for tuning of just the retriever, however we did not explore this."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  A comparison across inference models test. Training on relabeled hittable targets hurts accuracy, but provides the best CRAG Score overall.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "We demonstrate the effectiveness of relabeling in Table  2 . We ran 3 different answer generation setups for the 500 sample Task 1 hold out set we created. The first is an unmodified Llama 3 8B model, the second is a LoRa model using the original targets, and the final a LoRa model with relabeled targets. As shown, using the original targets provide the best accuracy, but also worsens hallucinations over the base model. While using the relabeled targets hurts accuracy, it also substantially reduces hallucinations, providing the best CRAG Score amongst the three.",
            "As part of the competition, a manual evaluation was conducted on user submissions. The automatic evaluation throughout was dependent on scoring via GPT-3.5 Turbo, given that correct user submissions may not have been exact matches to the expected answer. However, issues such as prompt injection still pose problems for automatic evaluation via LLMs. The results of our system across the various aspects of the dataset are shown in Figure  2 . As can be seen by the results, our system suffers many of the problems the dataset is meant to expose with most RAG systems."
        ]
    },
    "global_footnotes": [
        "https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1",
        "https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2",
        "Incorrectly here simply means in the context of our retrieval system, not that the provided answer is not true."
    ]
}