{
    "S3.T1": {
        "caption": "Table 1: The network architecture of CRNN",
        "table": null,
        "footnotes": [],
        "references": [
            "Without loss of generality, in this study, we adopt a simple Convolutional Recurrent Neural Network (CRNN) as our network, which is similar to the baseline of Task 3 of DCASE 2022 Challenge[2] but with ACCDOA format. The network has three convolution blocks followed by a one-layer bidirectional gated recurrent unit (BiGRU). The network takes the concatenation of log-mel spectrograms and intensity vectors as input and predicts active sound events with corresponding Cartesian DOA vectors for each time step. The network architecture of CRNN is shown in Table 1."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: The performance of the Meta-SELD and fine-tuning methods from pre-trained SELD models. Both two methods are evaluated in 𝒬i𝚝𝚎𝚜𝚝subscriptsuperscript𝒬𝚝𝚎𝚜𝚝𝑖\\mathcal{Q}^{\\mathtt{test}}_{i}. Note that overall scores of the fine-tuning method and Meta-SELD compute the fast adaptation performance of each individual room and then micro-average.",
        "table": null,
        "footnotes": [],
        "references": [
            "Table 2 shows the performance of the Meta-SELD method compared with the fine-tuning method from the pre-trained SELD models. The pre-trained SELD models are trained without using samples from 𝒟𝚝𝚎𝚜𝚝subscript𝒟𝚝𝚎𝚜𝚝\\mathcal{D}_{\\mathtt{test}}.",
            "According to the last row of Table 2, the overall score, which is a micro average across all rooms, shows that all of ER≤20∘subscriptERabsentsuperscript20\\mathrm{ER}_{\\leq 20^{\\circ}}, F≤20∘subscriptFabsentsuperscript20\\mathrm{F}_{\\leq 20^{\\circ}}, LECDsubscriptLECD\\mathrm{LE}_{\\mathrm{CD}}, and LRCDsubscriptLRCD\\mathrm{LR}_{\\mathrm{CD}} are improved using Meta-SELD compared with the fine-tuning method. We observe a drop in ℰ𝚂𝙴𝙻𝙳subscriptℰ𝚂𝙴𝙻𝙳\\mathcal{E}_{\\mathtt{SELD}} in fold3_room4 and fold4_room8 even though some new samples of unseen environments are used for training. This may be due to the fact that the new samples do not have valid information for training. We also observe the Meta-SELD method improves ℰ𝚂𝙴𝙻𝙳subscriptℰ𝚂𝙴𝙻𝙳\\mathcal{E}_{\\mathtt{SELD}} by a large margin in fold3_room22, fold4_room2, and fold4_room23 where the pre-trained model has poor performance across all rooms.\nSpecifically, ER≤20∘subscriptERabsentsuperscript20\\mathrm{ER}_{\\leq 20^{\\circ}}, F≤20∘subscriptFabsentsuperscript20\\mathrm{F}_{\\leq 20^{\\circ}}, and LRCDsubscriptLRCD\\mathrm{LR}_{\\mathrm{CD}} of fold3_room22 and fold4_room23 outperform other methods. Meta-SELD mainly improves the performance of SED in fold3_room22 and fold4_room23. All metrics of fold4_room2 are improved in Meta-SELD compared with the fine-tuning method, especially in DOA estimation. In fold4_room2, all of the pre-trained model, the fine-tuning method, and Meta-SELD achieve LRCDsubscriptLRCD\\mathrm{LR}_{\\mathrm{CD}} of over 70%, but LECDsubscriptLECD\\mathrm{LE}_{\\mathrm{CD}} of three methods is always high compared with LECDsubscriptLECD\\mathrm{LE}_{\\mathrm{CD}} of other rooms. Meta-SELD decreases 14.8∘superscript14.814.8^{\\circ} and 8.3∘superscript8.38.3^{\\circ} of LECDsubscriptLECD\\mathrm{LE}_{\\mathrm{CD}} compared with the pre-trained model and the fine-tuning method in fold4_room2, hence directly leading to the increase of F≤20∘subscriptFabsentsuperscript20\\mathrm{F}_{\\leq 20^{\\circ}} and the decrease of ER≤20∘subscriptERabsentsuperscript20\\mathrm{ER}_{\\leq 20^{\\circ}}. However, performance degradation happens in fold3_room4, fold3_room7, fold3_room14, and fold4_room16, where Meta-SELD has the worst metric scores. There is no significant change in LECDsubscriptLECD\\mathrm{LE}_{\\mathrm{CD}}, and the decline in SED performance is the main factor. One of the possible reasons for this observation could be that there are some conflicts in optimizing Meta-SELD across a batch of rooms."
        ]
    }
}