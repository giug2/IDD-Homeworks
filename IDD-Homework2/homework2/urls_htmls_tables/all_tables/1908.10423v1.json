{
    "S2.T1": {
        "caption": "Table 1: Results on GLUE test sets. Metrics differ per task (explained in Appendix A) but the best result is highlighted.",
        "table": null,
        "footnotes": [],
        "references": [
            "We first use the three meta-learning algorithms with PPS sampling and present in TableÂ 1 the experimental results on the GLUE test set. Generally, the meta-learning algorithms achieve better performance than the strong baseline models, with Reptile performing the best."
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Effect of task distributions. We report the accuracy or Matthews correlation on development sets.",
        "table": null,
        "footnotes": [],
        "references": [
            "As we have mentioned above, we propose three different choices of the task distribution pâ€‹(T)ğ‘ğ‘‡p(T) in this paper. Here we train Reptile with these task distributions and test modelsâ€™ performance on the development set as shown in TableÂ 2."
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Effect of the number of update steps and the inner learning rate Î±ğ›¼\\alpha.",
        "table": null,
        "footnotes": [],
        "references": [
            "In this part, we test the effect of the number of update steps kğ‘˜k and the learning rate in the inner learning loop. The experimental results on the development sets are shown in TableÂ 3. We find that setting kğ‘˜k to 5 is the optimal strategy and more or fewer update steps may lead to worse performance.",
            "We also vary the inner learning rate Î±ğ›¼\\alpha and investigate its impact. The results are listed in TableÂ 3. We can see that larger Î±ğ›¼\\alpha may degrade the performance because the resulting gradients deviate a lot from normal ones.\nThe above two ablations studies demonstrate the importance of making the meta-gradient informative."
        ]
    },
    "A0.T4": {
        "caption": "Table 4: Basic information and statistics of the GLUE and SciTail datasetsÂ Williams etÂ al. (2018).",
        "table": null,
        "footnotes": [],
        "references": [
            "Basically, the GLUE dataset Â Wang etÂ al. (2019) consists of three types of tasks: single-sentence classification, similarity and paraphrase tasks, and inference tasks, as shown in TableÂ 4."
        ]
    },
    "A3.T5": {
        "caption": "Table 5: Accuracy numbers on the 10 probing tasksÂ Conneau etÂ al. (2018).",
        "table": null,
        "footnotes": [],
        "references": [
            "A probing task is a classification problem that requires the model to make predictions related to certain linguistic properties of sentences. The abbreviations for the 10 tasks are listed in TableÂ 5. Basically, these tasks are set to test the modelâ€™s abilities to capture surface, syntactic or semantic information. We refer the reader toÂ Conneau etÂ al. (2018) for details. We freeze all the parameters of the models and only train the classification layer for the probing tasks."
        ]
    }
}