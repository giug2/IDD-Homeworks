{
    "PAPER'S NUMBER OF TABLES": 4,
    "S2.T1": {
        "caption": "Table 1: Type of adversaries and their targets against DML systems",
        "table": "<table id=\"S2.T1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.1.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.1.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S2.T1.1.1.1.1\" class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\"><img src=\"/html/2010.09258/assets/x4.png\" id=\"S2.T1.1.1.1.1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"461\" height=\"133\" alt=\"[Uncaptioned image]\">\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "The biggest, but not the only, vulnerability of DML is the amount of communication needed between the parameter server and the clients. Like the highway robberies of old, privacy protection is at its weakest when data is in transit. Therefore, the more communication, the more opportunities there are for attack. The danger alerts in ",
                "FIGURE ",
                "3",
                " indicate likely points of intrusion by adversaries. They include, the parameter server as the brain of the system, the data manager, the individual clients who may or may not be secure, as well as any time data is transmitted from one device to another. If an attack is successful, the amount of data leaked depends on the location of the attack. Violating a client may only net one or two data partitions but successfully penetrating the parameter or data server may yield the entire database or the entire model.",
                "TABLE ",
                "1",
                " summarizes the types of adversaries and their targets against DML schemes. Spectators can only observe the algorithms, models and training process. These adversaries are most likely curious about the training data and model but cannot affect the learning process. Conversely , participant adversaries can do quite a lot more damage. For instance, a malicious parameter server could wreak havoc because of its strong and centralized power, whereas the damage done by an adversarial clients is more contained. Hence, higher-level devices in the architecture are more attractive to adversaries."
            ]
        ]
    },
    "S2.T2": {
        "caption": "Table 2: Type of adversaries and their targets against federated learning systems",
        "table": "<table id=\"S2.T2.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T2.1.1\" class=\"ltx_tr\">\n<td id=\"S2.T2.1.1.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S2.T2.1.1.1.1\" class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\"><img src=\"/html/2010.09258/assets/x6.png\" id=\"S2.T2.1.1.1.1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"461\" height=\"120\" alt=\"[Uncaptioned image]\">\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Even though federated learning was designed improve privacy and security, these frameworks still have vulnerabilities and security risks. Again, the danger signs in ",
                "FIGURE ",
                "4",
                " identify potential attack targets. First, the raw data on the client devices is an attractive target for adversaries. Even though these data are never transmitted, they are still open to inference attacks without proper privacy guarantees at the device level. Second, the master model is a very valuable prize, which could be targeted in either a master aggregrator or the central server. The different types of adversaries and their potential attack targets are summarized in ",
                "TABLE ",
                "2",
                ".",
                "As well as adversaries targets, we also cover potential roles of adversaries in ",
                "TABLE ",
                "2",
                ". In federated learning, an adversary can be either a spectator or a participant. Malicious spectators of the model’s training process can observe, but they cannot affect model performance, so the vulnerability here is one of an inference attack. The target might be either the model’s parameters or an attempt to glean sensitive information from the data. Malicious participants, however, can both observe and change individual updates, while malicious aggregators can observe the global parameter updates and control the results of averaging."
            ]
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Adversary capabilities in various attacks against federated learning",
        "table": "<table id=\"S4.T3.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T3.1.1.1.1\" class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\"><img src=\"/html/2010.09258/assets/x7.png\" id=\"S4.T3.1.1.1.1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"461\" height=\"156\" alt=\"[Uncaptioned image]\">\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "As mentioned above, the two broad types of attacks in federated learning are inference at the client level or performance degradation at the global level. Inference attacks seek sensitive information. Performance attacks, called poisoning attacks, have two levels of scope: ",
                "untargeted attacks",
                " and ",
                "targeted attacks",
                ". In an untargeted attack, the aim is to destroy the global model by reducing its accuracy ",
                "biggio2012poisoning",
                ". Targeted attacks aim to alter the model’s behavior on only one or a few specific tasks while maintaining acceptable performance on all other tasks ",
                "bhagoji2018analyzing",
                ".",
                "TABLE ",
                "3",
                " summarizes the adversaries’ capabilities for each of the different types of attacks. The strategy with poisoning attacks is to act as a malicious client and upload invalid updates so as to train the model with a malicious or undesirable dataset. In most cases, poisoning attacks are executed by a solo adversary, although multiple adversaries can easily collude to attack each training epoch. Some adversaries only execute an attack once per training epoch. Further, recall that in federated learning, only a subset of all the participants is randomly chosen for each epoch, so a client may only be chosen once during the entire training process. However, when there are only a limited number of clients participating in the learning task, an adversary may be able to execute repeated attacks across multiple training epochs. The last adversarial capability is model inspections. Some models are white-boxes where the model’s parameters are ‘public’; others are black boxes where they are not. Most attacks in federated learning are white-box attacks because all clients receive the parameters of the global model."
            ]
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Nasr el al. nasr2019comprehensive Membership inference attacks in federated learning setting",
        "table": "<table id=\"S4.T4.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.1.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.1.1.1\" class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\"><img src=\"/html/2010.09258/assets/x8.png\" id=\"S4.T4.1.1.1.1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"461\" height=\"94\" alt=\"[Uncaptioned image]\">\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Nasr et al. ",
                "nasr2019comprehensive",
                " recently present a comprehensive framework for analyzing data leaks with deep neural networks by executing membership inference attacks in a white-box setting. All major scenarios and adversarial capabilities in deep learning applications were considered, including model training and fine-tuning, adversaries with prior knowledge, colluding adversaries, and the vulnerabilities of SGD algorithms. A target dataset with one-hot encoding of the true labels is used to infer whether a record was included in the target model’s training set. Attackers are then able to compute the outputs of all the hidden layers, the loss, and the gradients of all layers of the target model for the given dataset. These computation results and true labels can then be used to construct the input features for an attack model consisting of convolutional neural network components and fully connected network components. Nasr and colleagues considered two roles for the attacker: first as a curious server then as a participant in a federated learning setting. A single attack model is used to process all the corresponding inputs over the observed model at once instead of running an individual independent membership inference attack on each participant’s model. Results from their experiments show that the last layer of the network leaks the most membership information. A summary of the different types of membership inference attacks in federated learning follows in ",
                "TABLE ",
                "4",
                "."
            ]
        ]
    }
}