{
    "id_table_1": {
        "caption": "TABLE I:  Statistics of our studied datasets.",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "In the  Retrieval  phase, the objective is to retrieve demonstrations that are highly relevant to the query for the subsequent  Generation  phase. For example, as depicted in Fig.  1  in the context of a Program Synthesis task, the process begins with a query input  I q  u  e  r  y subscript I q u e r y I_{query} italic_I start_POSTSUBSCRIPT italic_q italic_u italic_e italic_r italic_y end_POSTSUBSCRIPT  asking how to implement a specific function. The retriever returns Top  K K K italic_K  relevant demonstrations, forming the set  D K subscript D K D_{K} italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT , where each demonstration  d  e  m  o i d e m subscript o i demo_{i} italic_d italic_e italic_m italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is composed of similar requirements  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and corresponding code snippets  O i subscript O i O_{i} italic_O start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  as represented by  D K = { d  e  m  o  1 : ( I 1 , O 1 ) , d  e  m  o  2 : ( I 2 , O 2 ) , ... , d  e  m  o  K : ( I K , O K ) } subscript D K conditional-set d e m o 1 : subscript I 1 subscript O 1 d e m o 2 subscript I 2 subscript O 2 ... d e m o K : subscript I K subscript O K D_{K}=\\{demo1:(I_{1},O_{1}),demo2:(I_{2},O_{2}),\\dots,demoK:(I_{K},O_{K})\\} italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT = { italic_d italic_e italic_m italic_o 1 : ( italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_O start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , italic_d italic_e italic_m italic_o 2 : ( italic_I start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_O start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , ... , italic_d italic_e italic_m italic_o italic_K : ( italic_I start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT , italic_O start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ) } .",
            "In this section, we present our research questions (RQs),  studied dataset, retrievers, used prompt templates, implementation details, and our approach to RQs. We conduct all our experiments following the framework presented in Fig.  1 ."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  Prompt templates for the three studied tasks.",
        "table": "S3.T2.1",
        "footnotes": [],
        "references": [
            "The underperformance of BM25L may be attributed to the length preference bias it introduces. Specifically, BM25L modifies the term frequency normalization to favor longer documents [ 14 ] . As illustrated in Fig.  2 , the demonstration returned by BM25L consists of longest prompts. This inclination may cause the BM25L to select longer rather than relevant demonstrations, compromising its effectiveness."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III:  Hyper-parameter settings of approximate retrievers for each task.",
        "table": "S3.T2.1.2.1.2.1",
        "footnotes": [],
        "references": [
            "To understand why some retrievers outperform others, we examine which retrievers return better demonstrations using  RecallGold@K . As illustrated in Fig.  3 , we observe a strong positive correlation between the  RecallGold@K  scores of various retrievers and their effectiveness. The non-parametric Kendalls    \\tau italic_  correlations  [ 38 ]  are 0.75, 0.69, and 1.00 for Program Synthesis, Commit Generation, and Assertion Generation, respectively, with corresponding p-values of 0.04, 0.05, and 0.003. These findings suggest that the effectiveness of retrievers is significantly influenced by their ability to recall  gold demonstrations ."
        ]
    },
    "id_table_4": {
        "caption": "TABLE IV:  The results of retrieval approaches in terms of generation effectiveness and retrieval efficacy. A one-shot setting is employed to ensure a fair comparison and darker colors indicate superior results.(a) Program Synthesis  (b) Commit Generation   (c) Assertion Generation",
        "table": "S3.T2.1.2.1.3.1",
        "footnotes": [],
        "references": [
            "In addition to analyzing fixed-size knowledge base, we also investigate scalability in terms of efficiency. Fig.  4  illustrates the efficiency trends of each retriever as the knowledge base size increases. To simplify the comparison, we only plot BM25, as it exhibits similar efficiency to its variant, BM25L. At smaller knowledge base sizes (fewer than  10 3 superscript 10 3 10^{3} 10 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT  entries), BM25 shows greater efficiency. However, as the knowledge base expands, its efficiency decreases relative to that of dense retrievers. Dense retrievers, which utilize fixed-size numeric representations, become increasingly efficient compared to sparse retrievers that need to manage large document-term matrices. Furthermore, approximate dense retrievers such as HNSW and ANNOY exhibit significant scalability, maintaining a nearly flat efficiency curve as the knowledge base size increases. This demonstrates the advantages of using approximate dense retrievers for large-scale retrieval tasks."
        ]
    },
    "id_table_5": {
        "caption": "TABLE V:  Case study from Program Synthesis: LLMs can be easily distracted by noisy information.",
        "table": "S3.T2.1.2.1.4.1",
        "footnotes": [],
        "references": [
            "We also plot both efficiency and effectiveness of different retrievers in Fig.  5  to provide a trade-off perspective. Retrievers positioned in the upper-right corner represent a more favorable balance between efficiency and effectiveness."
        ]
    },
    "id_table_6": {
        "caption": "TABLE VI:  The average prompt length, query latency (ms), and cosine similarity (%) of demonstrations to the query using SBERT_SS with varying numbers of shots.",
        "table": "S3.T3.1",
        "footnotes": [],
        "references": [
            "Fig.  6  illustrates the relationships between effectiveness and efficiency (top row), retrieval precision and efficiency (middle row), retrieval precision and effectiveness (bottom row) using different hyper-parameter configurations. A high degree of approximation (i.e., a smaller hyper-parameter setting in Table  III ) always leads to higher efficiency (higher QPS), but lower retrieval precision and effectiveness since ignores more nodes inspected. Specifically, LSH consistently maintains stable effectiveness and precision across different QPS values, with only slight variations. HNSW achieves high QPS values across a range of parameters. However, at the highest QPS values, the retrieval precision and effectiveness of generated outputs drops significantly. ANNOY demonstrates similar trade-off to HNSW, though the decline in precision and effectiveness is less pronounced.",
            "Fig.  6  also shows that the trade-off between retrieval precision and effectiveness remain consistent due to their strong positive correlation. Both HNSW and ANNOYs Kendall    \\tau italic_  coefficients were statistically significant (p-value   \\leq   0.05). In contrast, LSH exhibited p-values of 0.23, 0.23, and 0.48 across the three tasks, potentially due to the low variance of LSHs effectiveness metrics. Therefore, optimizing these trade-off during the retrieval step is essential to achieve the optimal balance between efficiency and effectiveness in the final generation phase.   {mdframed} [roundcorner=5pt]   Finding 4:  A higher degree of approximation improves the efficiency of dense retrieval but reduces retrieval precision, which adversely affects overall effectiveness. This is because of the strong positive correlation between retrieval precision and effectiveness."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "S3.T3.1.2.1.1.1",
        "footnotes": [],
        "references": [
            "Fig.  7  illustrates the relationship between effectiveness and the number of demonstrations across different retrievers. Notably, more demonstrations do not always correlate with improved effectiveness. In Assertion Generation, the Exact Match rate consistently decreases as the number of demonstrations increases across all retrievers. In Program Synthesis, while three demonstrations yield better results than one when using BM25, adding more than three leads to a decline in effectiveness."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "S3.T3.1.3.2.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "",
        "table": "S3.T3.1.4.3.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "",
        "table": "S3.T4.15",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "S3.T4.15.16.1.6.1",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "S3.T4.15.16.1.8.1",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "S3.T4.15.18.3.6.1",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "S3.T4.15.18.3.8.1",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "S3.T4.15.20.5.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "S3.T4.15.20.5.6.1",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "S3.T4.15.20.5.8.1",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "S4.T5.1",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "S4.T6.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}