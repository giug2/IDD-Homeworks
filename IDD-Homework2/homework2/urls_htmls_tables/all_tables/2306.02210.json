{
    "PAPER'S NUMBER OF TABLES": 9,
    "S1.T1": {
        "caption": "Table 1: Comparison of GPT-FL with existing FL methods.",
        "table": "<table id=\"S1.T1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S1.T1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S1.T1.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table id=\"S1.T1.3.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S1.T1.3.1.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">External Data</span></td>\n</tr>\n</table>\n</td>\n<td id=\"S1.T1.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table id=\"S1.T1.3.1.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S1.T1.3.1.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Limited to</span></td>\n</tr>\n<tr id=\"S1.T1.3.1.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.3.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Smaller</span></td>\n</tr>\n<tr id=\"S1.T1.3.1.1.3.1.3\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.3.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.3.1.3.1.1\" class=\"ltx_text ltx_font_bold\">Client Model</span></td>\n</tr>\n</table>\n</td>\n<td id=\"S1.T1.3.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table id=\"S1.T1.3.1.1.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S1.T1.3.1.1.4.1.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.4.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Generate</span></td>\n</tr>\n<tr id=\"S1.T1.3.1.1.4.1.2\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.4.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Data</span></td>\n</tr>\n<tr id=\"S1.T1.3.1.1.4.1.3\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.4.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.4.1.3.1.1\" class=\"ltx_text ltx_font_bold\">during FL</span></td>\n</tr>\n</table>\n</td>\n<td id=\"S1.T1.3.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table id=\"S1.T1.3.1.1.5.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S1.T1.3.1.1.5.1.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.5.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.5.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Data</span></td>\n</tr>\n<tr id=\"S1.T1.3.1.1.5.1.2\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.5.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.5.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Generator</span></td>\n</tr>\n<tr id=\"S1.T1.3.1.1.5.1.3\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.5.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.5.1.3.1.1\" class=\"ltx_text ltx_font_bold\">Location</span></td>\n</tr>\n</table>\n</td>\n<td id=\"S1.T1.3.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table id=\"S1.T1.3.1.1.6.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S1.T1.3.1.1.6.1.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.6.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.6.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Client Access to</span></td>\n</tr>\n<tr id=\"S1.T1.3.1.1.6.1.2\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.6.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.6.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Public/Generated</span></td>\n</tr>\n<tr id=\"S1.T1.3.1.1.6.1.3\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.6.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.6.1.3.1.1\" class=\"ltx_text ltx_font_bold\">Data</span></td>\n</tr>\n</table>\n</td>\n<td id=\"S1.T1.3.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table id=\"S1.T1.3.1.1.7.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S1.T1.3.1.1.7.1.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.7.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.7.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Support</span></td>\n</tr>\n<tr id=\"S1.T1.3.1.1.7.1.2\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.7.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.7.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Data</span></td>\n</tr>\n<tr id=\"S1.T1.3.1.1.7.1.3\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.7.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.7.1.3.1.1\" class=\"ltx_text ltx_font_bold\">Modality</span></td>\n</tr>\n</table>\n</td>\n<td id=\"S1.T1.3.1.1.8\" class=\"ltx_td ltx_align_left ltx_border_tt\">\n<table id=\"S1.T1.3.1.1.8.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S1.T1.3.1.1.8.1.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.8.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.8.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Compatibility</span></td>\n</tr>\n<tr id=\"S1.T1.3.1.1.8.1.2\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.8.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.8.1.2.1.1\" class=\"ltx_text ltx_font_bold\">with Secure</span></td>\n</tr>\n<tr id=\"S1.T1.3.1.1.8.1.3\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.1.8.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S1.T1.3.1.1.8.1.3.1.1\" class=\"ltx_text ltx_font_bold\">Aggregation</span></td>\n</tr>\n</table>\n</td>\n</tr>\n<tr id=\"S1.T1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">FedAvg <cite class=\"ltx_cite ltx_citemacro_cite\">McMahan et al. (<a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>\n</td>\n<td id=\"S1.T1.3.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.3.1.2.2.1\" class=\"ltx_text\">No</span></td>\n<td id=\"S1.T1.3.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.3.1.2.3.1\" class=\"ltx_text\">No</span></td>\n<td id=\"S1.T1.3.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.3.1.2.4.1\" class=\"ltx_text\">N/A</span></td>\n<td id=\"S1.T1.3.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.3.1.2.5.1\" class=\"ltx_text\">N/A</span></td>\n<td id=\"S1.T1.3.1.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.3.1.2.6.1\" class=\"ltx_text\">N/A</span></td>\n<td id=\"S1.T1.3.1.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.3.1.2.7.1\" class=\"ltx_text\">Any</span></td>\n<td id=\"S1.T1.3.1.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.3.1.2.8.1\" class=\"ltx_text\">Yes</span></td>\n</tr>\n<tr id=\"S1.T1.3.1.3\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.3.1\" class=\"ltx_td ltx_align_left\">FedOpt <cite class=\"ltx_cite ltx_citemacro_cite\">Reddi et al. (<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n</tr>\n<tr id=\"S1.T1.3.1.4\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.4.1\" class=\"ltx_td ltx_align_left\">FedProx <cite class=\"ltx_cite ltx_citemacro_cite\">Sahu et al. (<a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>\n</td>\n</tr>\n<tr id=\"S1.T1.3.1.5\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.5.1\" class=\"ltx_td ltx_align_left\">SCAFFOLD <cite class=\"ltx_cite ltx_citemacro_cite\">Karimireddy et al. (<a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</td>\n</tr>\n<tr id=\"S1.T1.3.1.6\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.6.1\" class=\"ltx_td ltx_align_left ltx_border_t\">FedDF <cite class=\"ltx_cite ltx_citemacro_cite\">Lin et al. (<a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"S1.T1.3.1.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.3.1.6.2.1\" class=\"ltx_text\">\n<span id=\"S1.T1.3.1.6.2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S1.T1.3.1.6.2.1.1.1\" class=\"ltx_tr\">\n<span id=\"S1.T1.3.1.6.2.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Public Data</span></span>\n</span></span></td>\n<td id=\"S1.T1.3.1.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.3.1.6.3.1\" class=\"ltx_text\">No</span></td>\n<td id=\"S1.T1.3.1.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.3.1.6.4.1\" class=\"ltx_text\">N/A</span></td>\n<td id=\"S1.T1.3.1.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.3.1.6.5.1\" class=\"ltx_text\">N/A</span></td>\n<td id=\"S1.T1.3.1.6.6\" class=\"ltx_td ltx_align_center ltx_border_t\">Not Required</td>\n<td id=\"S1.T1.3.1.6.7\" class=\"ltx_td ltx_align_center ltx_border_t\">Any</td>\n<td id=\"S1.T1.3.1.6.8\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.3.1.6.8.1\" class=\"ltx_text\">No</span></td>\n</tr>\n<tr id=\"S1.T1.3.1.7\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.7.1\" class=\"ltx_td ltx_align_left\">MOON <cite class=\"ltx_cite ltx_citemacro_cite\">Li et al. (<a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>\n</td>\n<td id=\"S1.T1.3.1.7.2\" class=\"ltx_td ltx_align_center\">Required</td>\n<td id=\"S1.T1.3.1.7.3\" class=\"ltx_td ltx_align_center\">Only Image</td>\n</tr>\n<tr id=\"S1.T1.3.1.8\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.8.1\" class=\"ltx_td ltx_align_left\">DS-FL <cite class=\"ltx_cite ltx_citemacro_cite\">Itahara et al. (<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"S1.T1.3.1.8.2\" class=\"ltx_td ltx_align_center\">Required</td>\n<td id=\"S1.T1.3.1.8.3\" class=\"ltx_td ltx_align_center\">Any</td>\n</tr>\n<tr id=\"S1.T1.3.1.9\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.9.1\" class=\"ltx_td ltx_align_left\">Fed-ET <cite class=\"ltx_cite ltx_citemacro_cite\">Cho et al. (<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>\n</td>\n<td id=\"S1.T1.3.1.9.2\" class=\"ltx_td ltx_align_center\">Not Required</td>\n<td id=\"S1.T1.3.1.9.3\" class=\"ltx_td ltx_align_center\">Any</td>\n</tr>\n<tr id=\"S1.T1.3.1.10\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.10.1\" class=\"ltx_td ltx_align_left ltx_border_t\">FedGen <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et al. (<a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>\n</td>\n<td id=\"S1.T1.3.1.10.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.3.1.10.2.1\" class=\"ltx_text\">\n<span id=\"S1.T1.3.1.10.2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S1.T1.3.1.10.2.1.1.1\" class=\"ltx_tr\">\n<span id=\"S1.T1.3.1.10.2.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Generated Data</span></span>\n</span></span></td>\n<td id=\"S1.T1.3.1.10.3\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\"><span id=\"S1.T1.3.1.10.3.1\" class=\"ltx_text\">Yes</span></td>\n<td id=\"S1.T1.3.1.10.4\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\"><span id=\"S1.T1.3.1.10.4.1\" class=\"ltx_text\">Yes</span></td>\n<td id=\"S1.T1.3.1.10.5\" class=\"ltx_td ltx_align_center ltx_border_t\">Client</td>\n<td id=\"S1.T1.3.1.10.6\" class=\"ltx_td ltx_align_center ltx_border_t\">Required</td>\n<td id=\"S1.T1.3.1.10.7\" class=\"ltx_td ltx_align_center ltx_border_t\">Only Image</td>\n<td id=\"S1.T1.3.1.10.8\" class=\"ltx_td ltx_align_center ltx_border_t\">No</td>\n</tr>\n<tr id=\"S1.T1.3.1.11\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.11.1\" class=\"ltx_td ltx_align_left\">FedFTG <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et al. (<a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>\n</td>\n<td id=\"S1.T1.3.1.11.2\" class=\"ltx_td ltx_align_center\">Server</td>\n<td id=\"S1.T1.3.1.11.3\" class=\"ltx_td ltx_align_center\">Not Required</td>\n<td id=\"S1.T1.3.1.11.4\" class=\"ltx_td ltx_align_center\">Only Image</td>\n<td id=\"S1.T1.3.1.11.5\" class=\"ltx_td ltx_align_center\">No</td>\n</tr>\n<tr id=\"S1.T1.3.1.12\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.12.1\" class=\"ltx_td ltx_align_left\">DynaFed <cite class=\"ltx_cite ltx_citemacro_cite\">Pi et al. (<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>\n</td>\n<td id=\"S1.T1.3.1.12.2\" class=\"ltx_td ltx_align_center\">Server</td>\n<td id=\"S1.T1.3.1.12.3\" class=\"ltx_td ltx_align_center\">Not Required</td>\n<td id=\"S1.T1.3.1.12.4\" class=\"ltx_td ltx_align_center\">Only Image</td>\n<td id=\"S1.T1.3.1.12.5\" class=\"ltx_td ltx_align_center\">Yes</td>\n</tr>\n<tr id=\"S1.T1.3.1.13\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.1.13.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S1.T1.3.1.13.1.1\" class=\"ltx_text ltx_font_bold\">GPT-FL (Ours)</span></td>\n<td id=\"S1.T1.3.1.13.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S1.T1.3.1.13.2.1\" class=\"ltx_text ltx_font_bold\">No</span></td>\n<td id=\"S1.T1.3.1.13.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S1.T1.3.1.13.3.1\" class=\"ltx_text ltx_font_bold\">No</span></td>\n<td id=\"S1.T1.3.1.13.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S1.T1.3.1.13.4.1\" class=\"ltx_text ltx_font_bold\">Server</span></td>\n<td id=\"S1.T1.3.1.13.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S1.T1.3.1.13.5.1\" class=\"ltx_text ltx_font_bold\">Not Required</span></td>\n<td id=\"S1.T1.3.1.13.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S1.T1.3.1.13.6.1\" class=\"ltx_text ltx_font_bold\">Any</span></td>\n<td id=\"S1.T1.3.1.13.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S1.T1.3.1.13.7.1\" class=\"ltx_text ltx_font_bold\">Yes</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "The proposed GPT-FL exhibits multifold merits compared to prior arts (Table 1):\n(1) In contrast to public data-based FL methods, GPT-FL gets rid of the dependency on the availability of the desired public data, offering much more flexibility in its applications.\n(2) Compared to other generative data-based approaches, the leverage of generative pre-trained models and the decoupling between synthetic data generation from the federated training process make the generated synthetic data in GPT-FL not impacted by private data distribution on the clients and the structure of the model to be trained.\n(3) By leveraging the computational resources on the server, GPT-FL provides a much more efficient way to utilize external data by incorporating them into the pre-training of the downstream model, which significantly reduces the communication and computation costs of FL.\n(4) The generation of downstream models using synthetic data takes place on the server. As such, it thereby eliminates the need for clients to bear any additional computational burden.\n(5) Lastly, as GPT-FL does not alter the standard FL framework, it is fully compatible with secure aggregation protocols as in standard FL methods. More importantly, GPT-FL does not introduce any additional hyper-parameters beyond the standard FL framework. This significantly simplifies the hyper-parameter optimization process, making GPT-FL much more practically useful."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Model accuracy comparison between GPT-FL and existing FL methods. For public data-based methods MOON, FedDF, DS-FL and Fed-ET, the results on CIFAR-10 and CIFAR-100 are obtained from Cho et al. (2022), and the results on Flowers102 are marked as N/A given the practical challenge on finding a set of suitable public data that can boost its performance.",
        "table": "<table id=\"S4.T2.2.2\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T2.2.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T2.2.2.2.3.1\" class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td id=\"S4.T2.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T2.2.2.2.4.1\" class=\"ltx_text\">\n<span id=\"S4.T2.2.2.2.4.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S4.T2.2.2.2.4.1.1.1\" class=\"ltx_tr\">\n<span id=\"S4.T2.2.2.2.4.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T2.2.2.2.4.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Training</span></span></span>\n<span id=\"S4.T2.2.2.2.4.1.1.2\" class=\"ltx_tr\">\n<span id=\"S4.T2.2.2.2.4.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T2.2.2.2.4.1.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></span></span>\n</span></span></td>\n<td id=\"S4.T2.2.2.2.5\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">\n<span id=\"S4.T2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">High Data Heterogeneity</span> (<math id=\"S4.T2.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mathbf{\\alpha=0.1}\" display=\"inline\"><semantics id=\"S4.T2.1.1.1.1.m1.1a\"><mrow id=\"S4.T2.1.1.1.1.m1.1.1\" xref=\"S4.T2.1.1.1.1.m1.1.1.cmml\"><mi id=\"S4.T2.1.1.1.1.m1.1.1.2\" xref=\"S4.T2.1.1.1.1.m1.1.1.2.cmml\">α</mi><mo id=\"S4.T2.1.1.1.1.m1.1.1.1\" xref=\"S4.T2.1.1.1.1.m1.1.1.1.cmml\">=</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\" id=\"S4.T2.1.1.1.1.m1.1.1.3\" xref=\"S4.T2.1.1.1.1.m1.1.1.3.cmml\">0.1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.1.1.1.1.m1.1b\"><apply id=\"S4.T2.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.1.1.1.1.m1.1.1\"><eq id=\"S4.T2.1.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T2.1.1.1.1.m1.1.1.1\"></eq><ci id=\"S4.T2.1.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T2.1.1.1.1.m1.1.1.2\">𝛼</ci><cn type=\"float\" id=\"S4.T2.1.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T2.1.1.1.1.m1.1.1.3\">0.1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.1.1.1.1.m1.1c\">\\mathbf{\\alpha=0.1}</annotation></semantics></math>)</td>\n<td id=\"S4.T2.2.2.2.6\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S4.T2.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">\n<span id=\"S4.T2.2.2.2.2.1\" class=\"ltx_text ltx_font_bold\">Low Data Heterogeneity</span> (<math id=\"S4.T2.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\mathbf{\\alpha=0.5}\" display=\"inline\"><semantics id=\"S4.T2.2.2.2.2.m1.1a\"><mrow id=\"S4.T2.2.2.2.2.m1.1.1\" xref=\"S4.T2.2.2.2.2.m1.1.1.cmml\"><mi id=\"S4.T2.2.2.2.2.m1.1.1.2\" xref=\"S4.T2.2.2.2.2.m1.1.1.2.cmml\">α</mi><mo id=\"S4.T2.2.2.2.2.m1.1.1.1\" xref=\"S4.T2.2.2.2.2.m1.1.1.1.cmml\">=</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\" id=\"S4.T2.2.2.2.2.m1.1.1.3\" xref=\"S4.T2.2.2.2.2.m1.1.1.3.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.2.2.2.2.m1.1b\"><apply id=\"S4.T2.2.2.2.2.m1.1.1.cmml\" xref=\"S4.T2.2.2.2.2.m1.1.1\"><eq id=\"S4.T2.2.2.2.2.m1.1.1.1.cmml\" xref=\"S4.T2.2.2.2.2.m1.1.1.1\"></eq><ci id=\"S4.T2.2.2.2.2.m1.1.1.2.cmml\" xref=\"S4.T2.2.2.2.2.m1.1.1.2\">𝛼</ci><cn type=\"float\" id=\"S4.T2.2.2.2.2.m1.1.1.3.cmml\" xref=\"S4.T2.2.2.2.2.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.2.2.2.2.m1.1c\">\\mathbf{\\alpha=0.5}</annotation></semantics></math>)</td>\n</tr>\n<tr id=\"S4.T2.2.2.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.3.1\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S4.T2.2.2.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.2.2.3.2.1\" class=\"ltx_text ltx_font_bold\">CIFAR-10</span></td>\n<td id=\"S4.T2.2.2.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.2.2.3.3.1\" class=\"ltx_text ltx_font_bold\">CIFAR-100</span></td>\n<td id=\"S4.T2.2.2.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.2.2.3.4.1\" class=\"ltx_text ltx_font_bold\">Flowers102</span></td>\n<td id=\"S4.T2.2.2.3.5\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S4.T2.2.2.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.2.2.3.6.1\" class=\"ltx_text ltx_font_bold\">CIFAR-10</span></td>\n<td id=\"S4.T2.2.2.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.2.2.3.7.1\" class=\"ltx_text ltx_font_bold\">CIFAR-100</span></td>\n<td id=\"S4.T2.2.2.3.8\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span id=\"S4.T2.2.2.3.8.1\" class=\"ltx_text ltx_font_bold\">Flowers102</span></td>\n</tr>\n<tr id=\"S4.T2.2.2.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\">FedAvg</td>\n<td id=\"S4.T2.2.2.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\"><span id=\"S4.T2.2.2.4.2.1\" class=\"ltx_text\">VGG19</span></td>\n<td id=\"S4.T2.2.2.4.3\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S4.T2.2.2.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">71.19 (± 0.27)</td>\n<td id=\"S4.T2.2.2.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\">30.21 (± 0.32)</td>\n<td id=\"S4.T2.2.2.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\">30.30 (± 0.16)</td>\n<td id=\"S4.T2.2.2.4.7\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S4.T2.2.2.4.8\" class=\"ltx_td ltx_align_center ltx_border_t\">74.82 (± 0.23)</td>\n<td id=\"S4.T2.2.2.4.9\" class=\"ltx_td ltx_align_center ltx_border_t\">33.12 (± 0.13)</td>\n<td id=\"S4.T2.2.2.4.10\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">34.75 (± 0.90)</td>\n</tr>\n<tr id=\"S4.T2.2.2.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.5.1\" class=\"ltx_td ltx_align_center\">FedProx</td>\n<td id=\"S4.T2.2.2.5.2\" class=\"ltx_td\"></td>\n<td id=\"S4.T2.2.2.5.3\" class=\"ltx_td ltx_align_center\">72.45 (± 0.13)</td>\n<td id=\"S4.T2.2.2.5.4\" class=\"ltx_td ltx_align_center\">31.51 (± 0.11)</td>\n<td id=\"S4.T2.2.2.5.5\" class=\"ltx_td ltx_align_center\">33.23 (± 0.24)</td>\n<td id=\"S4.T2.2.2.5.6\" class=\"ltx_td\"></td>\n<td id=\"S4.T2.2.2.5.7\" class=\"ltx_td ltx_align_center\">75.24 (± 0.19)</td>\n<td id=\"S4.T2.2.2.5.8\" class=\"ltx_td ltx_align_center\">33.64 (± 0.08)</td>\n<td id=\"S4.T2.2.2.5.9\" class=\"ltx_td ltx_nopad_r ltx_align_center\">40.56 (± 0.19)</td>\n</tr>\n<tr id=\"S4.T2.2.2.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.6.1\" class=\"ltx_td ltx_align_center\">SCAFFOLD</td>\n<td id=\"S4.T2.2.2.6.2\" class=\"ltx_td\"></td>\n<td id=\"S4.T2.2.2.6.3\" class=\"ltx_td ltx_align_center\">75.12 (± 0.20)</td>\n<td id=\"S4.T2.2.2.6.4\" class=\"ltx_td ltx_align_center\">30.61 (± 0.57)</td>\n<td id=\"S4.T2.2.2.6.5\" class=\"ltx_td ltx_align_center\">26.75 (± 0.50)</td>\n<td id=\"S4.T2.2.2.6.6\" class=\"ltx_td\"></td>\n<td id=\"S4.T2.2.2.6.7\" class=\"ltx_td ltx_align_center\">78.69 (± 0.15)</td>\n<td id=\"S4.T2.2.2.6.8\" class=\"ltx_td ltx_align_center\">34.91 (± 0.61)</td>\n<td id=\"S4.T2.2.2.6.9\" class=\"ltx_td ltx_nopad_r ltx_align_center\">33.21 (± 0.41)</td>\n</tr>\n<tr id=\"S4.T2.2.2.7\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.7.1\" class=\"ltx_td ltx_align_center ltx_border_t\">MOON</td>\n<td id=\"S4.T2.2.2.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span id=\"S4.T2.2.2.7.2.1\" class=\"ltx_text\">VGG19</span></td>\n<td id=\"S4.T2.2.2.7.3\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S4.T2.2.2.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\">75.68 (± 0.51)</td>\n<td id=\"S4.T2.2.2.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\">33.72 (± 0.89)</td>\n<td id=\"S4.T2.2.2.7.6\" class=\"ltx_td ltx_align_center ltx_border_t\">N/A</td>\n<td id=\"S4.T2.2.2.7.7\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S4.T2.2.2.7.8\" class=\"ltx_td ltx_align_center ltx_border_t\">81.17 (± 0.41)</td>\n<td id=\"S4.T2.2.2.7.9\" class=\"ltx_td ltx_align_center ltx_border_t\">42.15 (± 0.72)</td>\n<td id=\"S4.T2.2.2.7.10\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">N/A</td>\n</tr>\n<tr id=\"S4.T2.2.2.8\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.8.1\" class=\"ltx_td ltx_align_center\">FedDF</td>\n<td id=\"S4.T2.2.2.8.2\" class=\"ltx_td\"></td>\n<td id=\"S4.T2.2.2.8.3\" class=\"ltx_td ltx_align_center\">73.81 (± 0.42)</td>\n<td id=\"S4.T2.2.2.8.4\" class=\"ltx_td ltx_align_center\">31.87 (± 0.46)</td>\n<td id=\"S4.T2.2.2.8.5\" class=\"ltx_td ltx_align_center\">N/A</td>\n<td id=\"S4.T2.2.2.8.6\" class=\"ltx_td\"></td>\n<td id=\"S4.T2.2.2.8.7\" class=\"ltx_td ltx_align_center\">76.55 (± 0.32)</td>\n<td id=\"S4.T2.2.2.8.8\" class=\"ltx_td ltx_align_center\">37.87 (± 0.31)</td>\n<td id=\"S4.T2.2.2.8.9\" class=\"ltx_td ltx_nopad_r ltx_align_center\">N/A</td>\n</tr>\n<tr id=\"S4.T2.2.2.9\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.9.1\" class=\"ltx_td ltx_align_center\">DS-FL</td>\n<td id=\"S4.T2.2.2.9.2\" class=\"ltx_td\"></td>\n<td id=\"S4.T2.2.2.9.3\" class=\"ltx_td ltx_align_center\">65.27 (± 0.53)</td>\n<td id=\"S4.T2.2.2.9.4\" class=\"ltx_td ltx_align_center\">29.12 (± 0.51)</td>\n<td id=\"S4.T2.2.2.9.5\" class=\"ltx_td ltx_align_center\">N/A</td>\n<td id=\"S4.T2.2.2.9.6\" class=\"ltx_td\"></td>\n<td id=\"S4.T2.2.2.9.7\" class=\"ltx_td ltx_align_center\">68.44 (± 0.47)</td>\n<td id=\"S4.T2.2.2.9.8\" class=\"ltx_td ltx_align_center\">33.56 (± 0.55)</td>\n<td id=\"S4.T2.2.2.9.9\" class=\"ltx_td ltx_nopad_r ltx_align_center\">N/A</td>\n</tr>\n<tr id=\"S4.T2.2.2.10\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.10.1\" class=\"ltx_td ltx_align_center\">Fed-ET</td>\n<td id=\"S4.T2.2.2.10.2\" class=\"ltx_td\"></td>\n<td id=\"S4.T2.2.2.10.3\" class=\"ltx_td ltx_align_center\">78.66 (± 0.31)</td>\n<td id=\"S4.T2.2.2.10.4\" class=\"ltx_td ltx_align_center\">35.78 (± 0.45)</td>\n<td id=\"S4.T2.2.2.10.5\" class=\"ltx_td ltx_align_center\">N/A</td>\n<td id=\"S4.T2.2.2.10.6\" class=\"ltx_td\"></td>\n<td id=\"S4.T2.2.2.10.7\" class=\"ltx_td ltx_align_center\">81.13 (± 0.28)</td>\n<td id=\"S4.T2.2.2.10.8\" class=\"ltx_td ltx_align_center\">41.58 (± 0.36)</td>\n<td id=\"S4.T2.2.2.10.9\" class=\"ltx_td ltx_nopad_r ltx_align_center\">N/A</td>\n</tr>\n<tr id=\"S4.T2.2.2.11\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.11.1\" class=\"ltx_td ltx_align_center ltx_border_t\">FedGen</td>\n<td id=\"S4.T2.2.2.11.2\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S4.T2.2.2.11.2.1\" class=\"ltx_text\">ConvNet <span id=\"footnote3\" class=\"ltx_note ltx_role_footnote\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et al. (<a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">2021</a>); Pi et al. (<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> only reported results on ConvNet. We tested these two methods on VGG19 but they are not converged.</span></span></span></span></td>\n<td id=\"S4.T2.2.2.11.3\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S4.T2.2.2.11.4\" class=\"ltx_td ltx_align_center ltx_border_t\">42.05 (± 0.93)</td>\n<td id=\"S4.T2.2.2.11.5\" class=\"ltx_td ltx_align_center ltx_border_t\">26.64 (± 0.66)</td>\n<td id=\"S4.T2.2.2.11.6\" class=\"ltx_td ltx_align_center ltx_border_t\">Not Converged</td>\n<td id=\"S4.T2.2.2.11.7\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S4.T2.2.2.11.8\" class=\"ltx_td ltx_align_center ltx_border_t\">54.86 (± 0.13)</td>\n<td id=\"S4.T2.2.2.11.9\" class=\"ltx_td ltx_align_center ltx_border_t\">34.03 (± 0.42)</td>\n<td id=\"S4.T2.2.2.11.10\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">Not Converged</td>\n</tr>\n<tr id=\"S4.T2.2.2.12\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.12.1\" class=\"ltx_td ltx_align_center\">DynaFed</td>\n<td id=\"S4.T2.2.2.12.2\" class=\"ltx_td\"></td>\n<td id=\"S4.T2.2.2.12.3\" class=\"ltx_td ltx_align_center\">71.59 (± 0.10)</td>\n<td id=\"S4.T2.2.2.12.4\" class=\"ltx_td ltx_align_center\">36.08 (± 0.15)</td>\n<td id=\"S4.T2.2.2.12.5\" class=\"ltx_td ltx_align_center\">Not Converged</td>\n<td id=\"S4.T2.2.2.12.6\" class=\"ltx_td\"></td>\n<td id=\"S4.T2.2.2.12.7\" class=\"ltx_td ltx_align_center\">75.66 (± 0.21)</td>\n<td id=\"S4.T2.2.2.12.8\" class=\"ltx_td ltx_align_center\">43.82 (± 0.30)</td>\n<td id=\"S4.T2.2.2.12.9\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Not Converged</td>\n</tr>\n<tr id=\"S4.T2.2.2.13\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.13.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"2\"><span id=\"S4.T2.2.2.13.1.1\" class=\"ltx_text ltx_font_bold\">GPT-FL</span></td>\n<td id=\"S4.T2.2.2.13.2\" class=\"ltx_td ltx_align_center ltx_border_t\">VGG19</td>\n<td id=\"S4.T2.2.2.13.3\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S4.T2.2.2.13.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.2.2.13.4.1\" class=\"ltx_text ltx_font_bold\">82.16 (± 0.13)</span></td>\n<td id=\"S4.T2.2.2.13.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.2.2.13.5.1\" class=\"ltx_text ltx_font_bold\">47.80 (± 0.32)</span></td>\n<td id=\"S4.T2.2.2.13.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.2.2.13.6.1\" class=\"ltx_text ltx_font_bold\">70.56 (± 0.34)</span></td>\n<td id=\"S4.T2.2.2.13.7\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S4.T2.2.2.13.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.2.2.13.8.1\" class=\"ltx_text ltx_font_bold\">82.17 (± 0.20)</span></td>\n<td id=\"S4.T2.2.2.13.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.2.2.13.9.1\" class=\"ltx_text ltx_font_bold\">48.39 (± 0.17)</span></td>\n<td id=\"S4.T2.2.2.13.10\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span id=\"S4.T2.2.2.13.10.1\" class=\"ltx_text ltx_font_bold\">74.84 (± 0.43)</span></td>\n</tr>\n<tr id=\"S4.T2.2.2.14\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.14.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">ConvNet</td>\n<td id=\"S4.T2.2.2.14.2\" class=\"ltx_td ltx_border_bb\"></td>\n<td id=\"S4.T2.2.2.14.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.2.2.14.3.1\" class=\"ltx_text ltx_font_bold\">72.62 (± 0.24)</span></td>\n<td id=\"S4.T2.2.2.14.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.2.2.14.4.1\" class=\"ltx_text ltx_font_bold\">42.66 (± 0.19)</span></td>\n<td id=\"S4.T2.2.2.14.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.2.2.14.5.1\" class=\"ltx_text ltx_font_bold\">37.91 (± 0.43)</span></td>\n<td id=\"S4.T2.2.2.14.6\" class=\"ltx_td ltx_border_bb\"></td>\n<td id=\"S4.T2.2.2.14.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.2.2.14.7.1\" class=\"ltx_text ltx_font_bold\">77.18 (± 0.21)</span></td>\n<td id=\"S4.T2.2.2.14.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.2.2.14.8.1\" class=\"ltx_text ltx_font_bold\">47.89 (± 0.28)</span></td>\n<td id=\"S4.T2.2.2.14.9\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span id=\"S4.T2.2.2.14.9.1\" class=\"ltx_text ltx_font_bold\">48.61 (± 0.51)</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Overall Performance.\nTable 2 summarizes our results.\nWe make three key observations:\n(1) GPT-FL consistently outperforms all the baselines we selected in Table 2 under both low and high data heterogeneity scenarios across all three datasets.\n(2) In direct comparison with state-of-the-art generated data-based FL methods, although FedGen and DynaFed perform reasonably well on CIFAR-10 and CIFAR-100, they do not converge on Flowers102 whose images have higher resolutions than CIFAR. Moreover, both FedGen and DynaFed fail to converge when training a larger VGG19 model on Flowers102 and even lower-resolution CIFAR-10/100. In contrast, GPT-FL not only converges but also achieves state-of-the-art accuracy on Flowers102. More importantly, GPT-FL is able to support larger model, and its accuracy is significantly higher than the smaller ConvNet.\n(3) For Flowers102, as both public data-based and generated data-based FL methods are confronted with challenges, the only viable options are standard FL methods and GPT-FL. As shown, with the same model, GPT-FL outperforms standard FL methods by a significant margin.",
            "Client Sampling Efficiency.\nOne critical hyper-parameter of FL is the client sampling rate in each communication round during the federated training process.\nIn Figure 4, we plot the test model accuracies obtained by GPT-FL under low, medium, and high client sampling rates on CIFAR-10/100 with VGG19 under high data heterogeneity.\nAs shown, even with a single participating client per round, GPT-FL is able to achieve 80.44% and 43.07% test accuracy on CIFAR-10 and CIFAR-100 respectively. This performance already surpasses all the other FL methods listed in Table 2, which employs 9 times more clients for training per round.\nThese results highlight the significant advantage of GPT-FL in client sampling efficiency over state-of-the-art FL methods, making GPT-FL a very attractive solution in challenging scenarios where not many clients can participate at the same time.",
            "To determine the optimal hyperparameters, we conducted a search within specified ranges. The client learning rate was searched in the range of 1.00E-09 to 1.00E-00, the server learning rate in the range of 1.00E-09 to 1.00E-00, weight decay in the range of 0.1 to 0.9, input batch size in the range of 8 to 256, and epoch number for centralized training in the range of 100 to 500. The hyperparameter settings for the public data-based methods and standard FL methods in Table 2 followed the settings from the previous work Cho et al. (2022). The specific hyperparameter selections for the other experiments are provided below.",
            "Hyperparameter Selection in Table 2. The detailed experiment setups for Table 2 are listed in Table 6, Table 7, Table 8 and Table 9. For the experiments related to FedGen555FedGen: https://github.com/zhuangdizhu/FedGen and DynaFed666DynaFed: https://github.com/pipilurj/DynaFed/tree/main, we evaluate them with their official implementation code on GitHub.",
            "As shown in Table 2, GPT-FL outperforms both generated data-based approaches FedGen and DynaFed significantly across all experimental conditions. One plausible reason for this could be associated with the quality of the generated synthetic data. Specifically, both FedGen and DynaFed rely on training MLP-based generator networks to ensemble user information in a data-free manner, where the lightweight generator may have limitations in generating high-fidelity data. The results of Flowers102 provide empirical evidence that such a lightweight generator has constrained capabilities in synthesizing image output on input images with larger sizes, making it challenging for the global model to converge. To illustrate this, Figure 9 and Figure 9 illustrate the synthetic images generated by GPT-FL and DynaFed, respectively. As shown, the learned generator of DynaFed fails to generate high-fidelity data as in GPT-FL."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Accuracy performance of the generated downstream model and standard FL on benchmark datasets. \"1x Synthetic\" represents the size of synthetic data is one time as the real data.",
        "table": "<table id=\"S4.T3.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T3.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S4.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S4.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">1x Synthetic</span></td>\n<td id=\"S4.T3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">2x Synthetic</span></td>\n<td id=\"S4.T3.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">3x Synthetic</span></td>\n<td id=\"S4.T3.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.1.1.1.6.1\" class=\"ltx_text ltx_font_bold\">FedAvg</span></td>\n<td id=\"S4.T3.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.1.1.1.7.1\" class=\"ltx_text ltx_font_bold\">FedOpt</span></td>\n</tr>\n<tr id=\"S4.T3.1.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\"><span id=\"S4.T3.1.1.2.1.1\" class=\"ltx_text\">Image Data</span></td>\n<td id=\"S4.T3.1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">CIFAR-10</td>\n<td id=\"S4.T3.1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">61.48 (± 0.08)</td>\n<td id=\"S4.T3.1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">67.41 (± 0.40)</td>\n<td id=\"S4.T3.1.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.1.1.2.5.1\" class=\"ltx_text ltx_font_bold\">75.65 (± 0.09)</span></td>\n<td id=\"S4.T3.1.1.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">64.48 (± 0.13)</td>\n<td id=\"S4.T3.1.1.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">72.68 (± 0.22)</td>\n</tr>\n<tr id=\"S4.T3.1.1.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\">CIFAR-100</td>\n<td id=\"S4.T3.1.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">24.70 (± 0.00)</td>\n<td id=\"S4.T3.1.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">33.41 (± 0.01)</td>\n<td id=\"S4.T3.1.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.1.1.3.4.1\" class=\"ltx_text ltx_font_bold\">41.76 (± 0.03)</span></td>\n<td id=\"S4.T3.1.1.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">25.89 (± 0.67)</td>\n<td id=\"S4.T3.1.1.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">20.85 (± 0.14)</td>\n</tr>\n<tr id=\"S4.T3.1.1.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Flowers102</td>\n<td id=\"S4.T3.1.1.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">24.94 (± 0.57)</td>\n<td id=\"S4.T3.1.1.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">28.26 (± 0.14)</td>\n<td id=\"S4.T3.1.1.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.1.1.4.4.1\" class=\"ltx_text ltx_font_bold\">31.29 (± 0.18)</span></td>\n<td id=\"S4.T3.1.1.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\">30.30 (± 0.16)</td>\n<td id=\"S4.T3.1.1.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\">26.43 (± 0.09)</td>\n</tr>\n<tr id=\"S4.T3.1.1.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.5.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"2\"><span id=\"S4.T3.1.1.5.1.1\" class=\"ltx_text\">Audio Data</span></td>\n<td id=\"S4.T3.1.1.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Google Command</td>\n<td id=\"S4.T3.1.1.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">24.78 (± 0.04)</td>\n<td id=\"S4.T3.1.1.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">25.65 (± 0.07)</td>\n<td id=\"S4.T3.1.1.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">26.24 (± 0.01)</td>\n<td id=\"S4.T3.1.1.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\">73.68 (± 0.49)</td>\n<td id=\"S4.T3.1.1.5.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.1.1.5.7.1\" class=\"ltx_text ltx_font_bold\">83.01 (± 0.23)</span></td>\n</tr>\n<tr id=\"S4.T3.1.1.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.6.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">ESC-50</td>\n<td id=\"S4.T3.1.1.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">6.89 (± 0.29)</td>\n<td id=\"S4.T3.1.1.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">8.68 (± 0.35)</td>\n<td id=\"S4.T3.1.1.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">12.72 (± 0.31)</td>\n<td id=\"S4.T3.1.1.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">22.76 (± 1.01)</td>\n<td id=\"S4.T3.1.1.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T3.1.1.6.6.1\" class=\"ltx_text ltx_font_bold\">32.49 (± 0.57)</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "To answer this question, we compare the model performance between generated downstream model by centralized training with synthetic data and the global model by standard FL training with private data on both image and audio benchmark datasets. Different from the previous section, we select ResNet18 and ResNet50 models for CIFAR-10 and CIFAR-100 dataset, respectively.\nWe choose the models proposed in the FedAudio Benchmark Zhang et al. (2023) for audio tasks.\nWe report the best F1 score for the audio datasets. The results are summarized in Table 3.",
            "Hyperparameter Selection in Table 3 and Table 4.\nFor the centralized training in Table 3 and Table 4, we used the following hyperparameter settings. For image data, the batch size was set to 32, and the optimizer was AdamW with weight decay set to 0.9 and cosine annealing learning rate decay. The initial learning rate was 1.00E-04 for CIFAR-10/CIFAR-100 and 3.00E-04 for Flowers102. For audio data, the batch size was set to 64, and the optimizer was Adam with weight decay set to 1.00E-04. The initial learning rate was 5.00E-05 for both datasets.",
            "For the standard FL training in Table 3 and Table 4, the hyperparameter settings are as follows. For image data, the batch size is set to 32, and SGD is used as the local optimizer with weight decay set to 5.00E-04. When using FedOpt as the server aggregator, Adam is chosen as the server optimizer. Specifically, for the CIFAR-10 dataset, the local learning rate is set to 1.00E-01 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 1.00E-02 and the server learning rate is set to 1.00E-03. For the CIFAR-100 dataset, the local learning rate is set to 1.00E-01 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, both the local and server learning rates are set to 1.00E-01. For the Flowers102 dataset, the local learning rate is set to 1.00E-01 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 1.00E-02 and the server learning rate is set to 1.00E-02. For all audio data, the experimental settings strictly follow the FedAudio benchmark Zhang et al. (2023).",
            "For the GPT-FL training in Table 3 and Table 4, the hyperparameter settings are as follows. For image data, the batch size is set to 32, and SGD is used as the local optimizer with weight decay set to 5.00E-04. When using FedOpt as the server aggregator, Adam is chosen as the server optimizer. Specifically, for the CIFAR-10 dataset, the local learning rate is set to 5.00E-04 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 3.00E-04 and the server learning rate is set to 7.00E-04. For the CIFAR-100 dataset, the local learning rate is set to 1.00E-04 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 5.00E-04 and the server learning rate is set to 1.00E-03. For the Flowers102 dataset, the local learning rate is set to 5.00E-03 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 1.00E-04 and the server learning rate is set to 1.00E-04. For audio data, the batch size is set to 16, and SGD is used as the local optimizer with weight decay set to 5.00E-04. When using FedOpt as the server aggregator, Adam is chosen as the server optimizer. We set the local learning rate to 5.00E-02 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 1.00E-03 and the server learning rate is set to 5.00E-04 for both two datasets."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Accuracy comparison between generated downstream model, standard FL and GPT-FL. Differ from the experiments shown in Table 1, the CIFAR-10 and Flowers102 datasets are trained with ResNet18 model and the CIFAR-100 dataset is trained with ResNet50 model. \"ΔΔ\\DeltaMetric\" represents the accuracy increment by GPT-FL on top of the generated downstream model.",
        "table": "<table id=\"S4.T4.8.6.6\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T4.3.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T4.3.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S4.T4.3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T4.3.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">3x Synthetic</span></td>\n<td id=\"S4.T4.3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T4.3.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">FedAvg</span></td>\n<td id=\"S4.T4.3.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T4.3.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">FedOpt</span></td>\n<td id=\"S4.T4.3.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T4.3.1.1.1.6.1\" class=\"ltx_text ltx_font_typewriter ltx_font_bold\">GPT-FL<span id=\"S4.T4.3.1.1.1.6.1.1\" class=\"ltx_text ltx_font_serif\"> w/ FedAvg</span></span></td>\n<td id=\"S4.T4.3.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T4.3.1.1.1.7.1\" class=\"ltx_text ltx_font_typewriter ltx_font_bold\">GPT-FL<span id=\"S4.T4.3.1.1.1.7.1.1\" class=\"ltx_text ltx_font_serif\"> w/ FedOpt</span></span></td>\n<td id=\"S4.T4.3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<math id=\"S4.T4.3.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\Delta\" display=\"inline\"><semantics id=\"S4.T4.3.1.1.1.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S4.T4.3.1.1.1.1.m1.1.1\" xref=\"S4.T4.3.1.1.1.1.m1.1.1.cmml\">Δ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.3.1.1.1.1.m1.1b\"><ci id=\"S4.T4.3.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.3.1.1.1.1.m1.1.1\">Δ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.3.1.1.1.1.m1.1c\">\\Delta</annotation></semantics></math><span id=\"S4.T4.3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Metric</span>\n</td>\n</tr>\n<tr id=\"S4.T4.4.2.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T4.4.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">CIFAR-10</td>\n<td id=\"S4.T4.4.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">75.65 (± 0.09)</td>\n<td id=\"S4.T4.4.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">64.48 (± 0.13)</td>\n<td id=\"S4.T4.4.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">72.68 (± 0.22)</td>\n<td id=\"S4.T4.4.2.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.4.2.2.2.6.1\" class=\"ltx_text ltx_font_bold\">81.38 (± 0.05)</span></td>\n<td id=\"S4.T4.4.2.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">79.08 (± 0.17)</td>\n<td id=\"S4.T4.4.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<math id=\"S4.T4.4.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mathbf{\\uparrow}\" display=\"inline\"><semantics id=\"S4.T4.4.2.2.2.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T4.4.2.2.2.1.m1.1.1\" xref=\"S4.T4.4.2.2.2.1.m1.1.1.cmml\">↑</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.4.2.2.2.1.m1.1b\"><ci id=\"S4.T4.4.2.2.2.1.m1.1.1.cmml\" xref=\"S4.T4.4.2.2.2.1.m1.1.1\">↑</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.4.2.2.2.1.m1.1c\">\\mathbf{\\uparrow}</annotation></semantics></math> <span id=\"S4.T4.4.2.2.2.1.1\" class=\"ltx_text ltx_font_bold\">5.73</span>\n</td>\n</tr>\n<tr id=\"S4.T4.5.3.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T4.5.3.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">CIFAR-100</td>\n<td id=\"S4.T4.5.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">41.76 (± 0.03)</td>\n<td id=\"S4.T4.5.3.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">25.89 (± 0.67)</td>\n<td id=\"S4.T4.5.3.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">20.85 (± 0.14)</td>\n<td id=\"S4.T4.5.3.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.5.3.3.3.6.1\" class=\"ltx_text ltx_font_bold\">62.83 (± 0.31)</span></td>\n<td id=\"S4.T4.5.3.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\">48.80 (± 0.12)</td>\n<td id=\"S4.T4.5.3.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<math id=\"S4.T4.5.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mathbf{\\uparrow}\" display=\"inline\"><semantics id=\"S4.T4.5.3.3.3.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T4.5.3.3.3.1.m1.1.1\" xref=\"S4.T4.5.3.3.3.1.m1.1.1.cmml\">↑</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.5.3.3.3.1.m1.1b\"><ci id=\"S4.T4.5.3.3.3.1.m1.1.1.cmml\" xref=\"S4.T4.5.3.3.3.1.m1.1.1\">↑</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.5.3.3.3.1.m1.1c\">\\mathbf{\\uparrow}</annotation></semantics></math> <span id=\"S4.T4.5.3.3.3.1.1\" class=\"ltx_text ltx_font_bold\">21.07</span>\n</td>\n</tr>\n<tr id=\"S4.T4.6.4.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T4.6.4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Flowers102</td>\n<td id=\"S4.T4.6.4.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">31.29 (± 0.18)</td>\n<td id=\"S4.T4.6.4.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">30.30 (± 0.16)</td>\n<td id=\"S4.T4.6.4.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\">26.43 (± 0.09)</td>\n<td id=\"S4.T4.6.4.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\">70.56 (± 0.34)</td>\n<td id=\"S4.T4.6.4.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.6.4.4.4.7.1\" class=\"ltx_text ltx_font_bold\">77.57 (± 0.03)</span></td>\n<td id=\"S4.T4.6.4.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<math id=\"S4.T4.6.4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mathbf{\\uparrow}\" display=\"inline\"><semantics id=\"S4.T4.6.4.4.4.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T4.6.4.4.4.1.m1.1.1\" xref=\"S4.T4.6.4.4.4.1.m1.1.1.cmml\">↑</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.6.4.4.4.1.m1.1b\"><ci id=\"S4.T4.6.4.4.4.1.m1.1.1.cmml\" xref=\"S4.T4.6.4.4.4.1.m1.1.1\">↑</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.6.4.4.4.1.m1.1c\">\\mathbf{\\uparrow}</annotation></semantics></math> <span id=\"S4.T4.6.4.4.4.1.1\" class=\"ltx_text ltx_font_bold\">46.28</span>\n</td>\n</tr>\n<tr id=\"S4.T4.7.5.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T4.7.5.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Google Command</td>\n<td id=\"S4.T4.7.5.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">26.24 (± 0.01)</td>\n<td id=\"S4.T4.7.5.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">73.68 (± 0.49)</td>\n<td id=\"S4.T4.7.5.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">83.01 (± 0.23)</td>\n<td id=\"S4.T4.7.5.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\">81.90 (± 0.20)</td>\n<td id=\"S4.T4.7.5.5.5.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.7.5.5.5.7.1\" class=\"ltx_text ltx_font_bold\">83.46 (± 0.11)</span></td>\n<td id=\"S4.T4.7.5.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<math id=\"S4.T4.7.5.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mathbf{\\uparrow}\" display=\"inline\"><semantics id=\"S4.T4.7.5.5.5.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T4.7.5.5.5.1.m1.1.1\" xref=\"S4.T4.7.5.5.5.1.m1.1.1.cmml\">↑</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.7.5.5.5.1.m1.1b\"><ci id=\"S4.T4.7.5.5.5.1.m1.1.1.cmml\" xref=\"S4.T4.7.5.5.5.1.m1.1.1\">↑</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.7.5.5.5.1.m1.1c\">\\mathbf{\\uparrow}</annotation></semantics></math> <span id=\"S4.T4.7.5.5.5.1.1\" class=\"ltx_text ltx_font_bold\">57.22</span>\n</td>\n</tr>\n<tr id=\"S4.T4.8.6.6.6\" class=\"ltx_tr\">\n<td id=\"S4.T4.8.6.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">ESC-50</td>\n<td id=\"S4.T4.8.6.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">12.72 (± 0.31)</td>\n<td id=\"S4.T4.8.6.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">22.76 (± 1.01)</td>\n<td id=\"S4.T4.8.6.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">32.49 (± 0.57)</td>\n<td id=\"S4.T4.8.6.6.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">41.80 (± 0.32)</td>\n<td id=\"S4.T4.8.6.6.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T4.8.6.6.6.7.1\" class=\"ltx_text ltx_font_bold\">43.46 (± 0.30)</span></td>\n<td id=\"S4.T4.8.6.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<math id=\"S4.T4.8.6.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mathbf{\\uparrow}\" display=\"inline\"><semantics id=\"S4.T4.8.6.6.6.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T4.8.6.6.6.1.m1.1.1\" xref=\"S4.T4.8.6.6.6.1.m1.1.1.cmml\">↑</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.8.6.6.6.1.m1.1b\"><ci id=\"S4.T4.8.6.6.6.1.m1.1.1.cmml\" xref=\"S4.T4.8.6.6.6.1.m1.1.1\">↑</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.8.6.6.6.1.m1.1c\">\\mathbf{\\uparrow}</annotation></semantics></math> <span id=\"S4.T4.8.6.6.6.1.1\" class=\"ltx_text ltx_font_bold\">30.74</span>\n</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We explore the benefits that GPT-FL provides for custom models that are built on top of downstream models generated from synthetic data. Specifically, we want to examine how fine-tuning these downstream models with private data under the FL framework can lead to performance improvements. To demonstrate how GPT-FL can be integrated with existing FL server optimizers, we evaluate its performance with both FedAvg and FedOpt as the server aggregator. Our experimental results are presented in Table 4.",
            "Effectiveness of Private Data.\nOur experiments demonstrate the effectiveness of incorporating private data with FL into the finetuning process of the downstream model generated from synthetic data. As shown in Table 4, regardless of the modality and quality of the synthetic data used to generate the downstream model, FL fine-tuning leads to significant performance gains, outperforming the ones trained solely with FL or CL combined with synthetic training by a large margin. Furthermore, we observe that fine-tuning with private data can especially benefit the cases for out-of-domain synthetic data, such as in the audio data. For example, GPT-FL with FedOpt could achieve 43.46 test accuracy in the ECS-50 dataset, which nearly provides two times increment than standard FL and three times increment than centralized training by synthetic data. These results suggest that leveraging private data with FL in the fine-tuning process can greatly enhance the performance of synthetic data-generated models, making them more suitable for real-world applications.",
            "Harmonization With Existing Pre-train Model.\nAs the standard FL framework, GPT-FL could also benefit from other existing pre-train models. Specifically, besides training from scratch, GPT-FL could utilize the existing pre-train model to start training the synthetic data to generate downstream model and finetune it again with private data in FL. Table 5 presents the performance evaluation of GPT-FL on top of the pre-trained models for image datasets. We follow the approach from prior work Nguyen et al. (2022) and use the ImageNet pre-trained model available in the PyTorch Torchvision library. Our experiments show that GPT-FL achieves better results compared to training solely with FL or synthetic data, as reported in Table 4. Notably, the improvement in performance is consistent across three image benchmark datasets, with a gain ranging from 1% to 11% compared to the results in Table 4. These results demonstrate that GPT-FL can effectively leverage pre-trained models to improve performance in the FL setting.",
            "Hyperparameter Selection in Table 3 and Table 4.\nFor the centralized training in Table 3 and Table 4, we used the following hyperparameter settings. For image data, the batch size was set to 32, and the optimizer was AdamW with weight decay set to 0.9 and cosine annealing learning rate decay. The initial learning rate was 1.00E-04 for CIFAR-10/CIFAR-100 and 3.00E-04 for Flowers102. For audio data, the batch size was set to 64, and the optimizer was Adam with weight decay set to 1.00E-04. The initial learning rate was 5.00E-05 for both datasets.",
            "For the standard FL training in Table 3 and Table 4, the hyperparameter settings are as follows. For image data, the batch size is set to 32, and SGD is used as the local optimizer with weight decay set to 5.00E-04. When using FedOpt as the server aggregator, Adam is chosen as the server optimizer. Specifically, for the CIFAR-10 dataset, the local learning rate is set to 1.00E-01 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 1.00E-02 and the server learning rate is set to 1.00E-03. For the CIFAR-100 dataset, the local learning rate is set to 1.00E-01 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, both the local and server learning rates are set to 1.00E-01. For the Flowers102 dataset, the local learning rate is set to 1.00E-01 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 1.00E-02 and the server learning rate is set to 1.00E-02. For all audio data, the experimental settings strictly follow the FedAudio benchmark Zhang et al. (2023).",
            "For the GPT-FL training in Table 3 and Table 4, the hyperparameter settings are as follows. For image data, the batch size is set to 32, and SGD is used as the local optimizer with weight decay set to 5.00E-04. When using FedOpt as the server aggregator, Adam is chosen as the server optimizer. Specifically, for the CIFAR-10 dataset, the local learning rate is set to 5.00E-04 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 3.00E-04 and the server learning rate is set to 7.00E-04. For the CIFAR-100 dataset, the local learning rate is set to 1.00E-04 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 5.00E-04 and the server learning rate is set to 1.00E-03. For the Flowers102 dataset, the local learning rate is set to 5.00E-03 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 1.00E-04 and the server learning rate is set to 1.00E-04. For audio data, the batch size is set to 16, and SGD is used as the local optimizer with weight decay set to 5.00E-04. When using FedOpt as the server aggregator, Adam is chosen as the server optimizer. We set the local learning rate to 5.00E-02 with FedAvg as the server aggregator, and for FedOpt as the server aggregator, the local learning rate is set to 1.00E-03 and the server learning rate is set to 5.00E-04 for both two datasets."
        ]
    },
    "S4.T5": {
        "caption": "Table 5: Accuracy performance comparison between generated downstream model, standard federated learning and GPT-FL. All the training is initialized by ImageNet-based pre-train model.",
        "table": "<table id=\"S4.T5.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T5.3.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T5.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S4.T5.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T5.3.1.1.2.1\" class=\"ltx_text ltx_font_bold\">3x Synthetic</span></td>\n<td id=\"S4.T5.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T5.3.1.1.3.1\" class=\"ltx_text ltx_font_bold\">FedAvg</span></td>\n<td id=\"S4.T5.3.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T5.3.1.1.4.1\" class=\"ltx_text ltx_font_bold\">FedOpt</span></td>\n<td id=\"S4.T5.3.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T5.3.1.1.5.1\" class=\"ltx_text ltx_font_typewriter ltx_font_bold\">GPT-FL<span id=\"S4.T5.3.1.1.5.1.1\" class=\"ltx_text ltx_font_serif\"> w/ FedAvg</span></span></td>\n<td id=\"S4.T5.3.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T5.3.1.1.6.1\" class=\"ltx_text ltx_font_typewriter ltx_font_bold\">GPT-FL<span id=\"S4.T5.3.1.1.6.1.1\" class=\"ltx_text ltx_font_serif\"> w/ FedOpt</span></span></td>\n</tr>\n<tr id=\"S4.T5.3.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">CIFAR-10</td>\n<td id=\"S4.T5.3.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">72.65 (± 0.05)</td>\n<td id=\"S4.T5.3.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">66.10 (± 0.03)</td>\n<td id=\"S4.T5.3.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">79.08 (± 0.39)</td>\n<td id=\"S4.T5.3.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">75.87 (± 0.73)</td>\n<td id=\"S4.T5.3.1.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T5.3.1.2.6.1\" class=\"ltx_text ltx_font_bold\">82.20 (± 0.61)</span></td>\n</tr>\n<tr id=\"S4.T5.3.1.3\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\">CIFAR-100</td>\n<td id=\"S4.T5.3.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">42.30 (± 0.01)</td>\n<td id=\"S4.T5.3.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">62.83 (± 0.03)</td>\n<td id=\"S4.T5.3.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">45.27 (± 0.10)</td>\n<td id=\"S4.T5.3.1.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T5.3.1.3.5.1\" class=\"ltx_text ltx_font_bold\">66.84 (± 0.05)</span></td>\n<td id=\"S4.T5.3.1.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">66.03 (± 0.02)</td>\n</tr>\n<tr id=\"S4.T5.3.1.4\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">Flowers102</td>\n<td id=\"S4.T5.3.1.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">41.05 (± 0.26)</td>\n<td id=\"S4.T5.3.1.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">80.73 (± 0.01)</td>\n<td id=\"S4.T5.3.1.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">87.33 (± 0.29)</td>\n<td id=\"S4.T5.3.1.4.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">86.18 (± 0.04)</td>\n<td id=\"S4.T5.3.1.4.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T5.3.1.4.6.1\" class=\"ltx_text ltx_font_bold\">88.66 (± 0.40)</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Harmonization With Existing Pre-train Model.\nAs the standard FL framework, GPT-FL could also benefit from other existing pre-train models. Specifically, besides training from scratch, GPT-FL could utilize the existing pre-train model to start training the synthetic data to generate downstream model and finetune it again with private data in FL. Table 5 presents the performance evaluation of GPT-FL on top of the pre-trained models for image datasets. We follow the approach from prior work Nguyen et al. (2022) and use the ImageNet pre-trained model available in the PyTorch Torchvision library. Our experiments show that GPT-FL achieves better results compared to training solely with FL or synthetic data, as reported in Table 4. Notably, the improvement in performance is consistent across three image benchmark datasets, with a gain ranging from 1% to 11% compared to the results in Table 4. These results demonstrate that GPT-FL can effectively leverage pre-trained models to improve performance in the FL setting.",
            "Hyperparameter Selection in Table 5. For the centralized training in Table 5, the hyperparameter selection is follows. For all image data, we set the batch size to 32, and choose AdamW Loshchilov & Hutter (2017) as the optimizer with weight decay equal to 0.9 and cosine annealing learning rate decay. For the CIFAR-10 dataset, we used an initial learning rate of 8.00E-06; for the CIFAR-100 dataset, we used an initial learning rate of 5.00E-06; for the Flowers102 dataset, we used an initial learning rate of 2.00E-05.",
            "For the standard FL training in Table 5, we use the hyperparameter setting as follows. For all image data, we set the batch size to 32, and choose SGD as the local optimizer with weight decay equal to 5.00E-04. With FedOpt as the server aggregator, we choose Adam as the server optimizer. For the CIFAR-10 dataset, we choose the local learning rate as 1.00E-01 with FedAvg as the server aggregator and choose the local learning rate as 1.00E-03 and the server learning rate as 1.00E-03 with FedOpt as the server aggregator. For the CIFAR-100 dataset, we choose the local learning rate as 1.00E-02 with FedAvg as the server aggregator and choose the local learning rate as 5.00E-03 and the server learning rate as 7.00E-03 with FedOpt as the server aggregator. For the Flowers102 dataset, we choose the local learning rate as 1.00E-02 with FedAvg as the server aggregator and choose the local learning rate as 1.00E-04 and the server learning rate as 5.00E-04 with FedOpt as the server aggregator.",
            "For GPT-FL training in Table 5, we use the hyperparameter setting as follows. For all image data, we set the batch size to 32, and choose SGD as the local optimizer with weight decay equal to 5.00E-04. With FedOpt as the server aggregator, we choose Adam as the server optimizer. For CIFAR-10 dataset, we choose the local learning rate as 1.00E-07 with FedAvg as the server aggregator and choose the local learning rate as 1.00E-07 and the server learning rate as 1.00E-05 with FedOpt as the server aggregator. For CIFAR-100 dataset, we choose the local learning rate as 1.00E-04 with FedAvg as the server aggregator and choose the local learning rate as 1.00E-04 and the server learning rate as 1.00E-05 with FedOpt as server aggregator. For Flowers102 dataset, we choose the local learning rate as 1.00E-02 with FedAvg as the server aggregator and choose the local learning rate as 1.00E-04 and the server learning rate as 1.00E-04 with FedOpt as the server aggregator."
        ]
    },
    "A1.T6": {
        "caption": "Table 6: Experimental setup details of GPT-FL with VGG19 in Table 2",
        "table": "<table id=\"A1.T6.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A1.T6.3.1.1\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"A1.T6.3.1.1.2\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"A1.T6.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A1.T6.3.1.1.3.1\" class=\"ltx_text ltx_font_bold\">CIFAR-10</span></td>\n<td id=\"A1.T6.3.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A1.T6.3.1.1.4.1\" class=\"ltx_text ltx_font_bold\">CIFAR-100</span></td>\n<td id=\"A1.T6.3.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A1.T6.3.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Flowers102</span></td>\n</tr>\n<tr id=\"A1.T6.3.1.2\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Local Epoch</td>\n<td id=\"A1.T6.3.1.2.2\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"A1.T6.3.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td id=\"A1.T6.3.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td id=\"A1.T6.3.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n</tr>\n<tr id=\"A1.T6.3.1.3\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.1.3.1\" class=\"ltx_td ltx_align_left\">Communication Rounds</td>\n<td id=\"A1.T6.3.1.3.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T6.3.1.3.3\" class=\"ltx_td ltx_align_center\">500</td>\n<td id=\"A1.T6.3.1.3.4\" class=\"ltx_td ltx_align_center\">500</td>\n<td id=\"A1.T6.3.1.3.5\" class=\"ltx_td ltx_align_center\">500</td>\n</tr>\n<tr id=\"A1.T6.3.1.4\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.1.4.1\" class=\"ltx_td ltx_align_left\">Cohort Size</td>\n<td id=\"A1.T6.3.1.4.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T6.3.1.4.3\" class=\"ltx_td ltx_align_center\">10</td>\n<td id=\"A1.T6.3.1.4.4\" class=\"ltx_td ltx_align_center\">10</td>\n<td id=\"A1.T6.3.1.4.5\" class=\"ltx_td ltx_align_center\">50</td>\n</tr>\n<tr id=\"A1.T6.3.1.5\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.1.5.1\" class=\"ltx_td ltx_align_left\">Batch Size</td>\n<td id=\"A1.T6.3.1.5.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T6.3.1.5.3\" class=\"ltx_td ltx_align_center\">32</td>\n<td id=\"A1.T6.3.1.5.4\" class=\"ltx_td ltx_align_center\">32</td>\n<td id=\"A1.T6.3.1.5.5\" class=\"ltx_td ltx_align_center\">32</td>\n</tr>\n<tr id=\"A1.T6.3.1.6\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.1.6.1\" class=\"ltx_td ltx_align_left\" rowspan=\"2\"><span id=\"A1.T6.3.1.6.1.1\" class=\"ltx_text\">Client Learning Rate</span></td>\n<td id=\"A1.T6.3.1.6.2\" class=\"ltx_td ltx_align_left\">High Data Heterogeneity</td>\n<td id=\"A1.T6.3.1.6.3\" class=\"ltx_td ltx_align_center\">1.00E-07</td>\n<td id=\"A1.T6.3.1.6.4\" class=\"ltx_td ltx_align_center\">1.00E-06</td>\n<td id=\"A1.T6.3.1.6.5\" class=\"ltx_td ltx_align_center\">5.00E-03</td>\n</tr>\n<tr id=\"A1.T6.3.1.7\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.1.7.1\" class=\"ltx_td ltx_align_left\">Low Data Heterogeneity</td>\n<td id=\"A1.T6.3.1.7.2\" class=\"ltx_td ltx_align_center\">1.00E-07</td>\n<td id=\"A1.T6.3.1.7.3\" class=\"ltx_td ltx_align_center\">1.00E-06</td>\n<td id=\"A1.T6.3.1.7.4\" class=\"ltx_td ltx_align_center\">5.00E-03</td>\n</tr>\n<tr id=\"A1.T6.3.1.8\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.1.8.1\" class=\"ltx_td ltx_align_left\">Optimizer</td>\n<td id=\"A1.T6.3.1.8.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T6.3.1.8.3\" class=\"ltx_td ltx_align_center\">SGD</td>\n<td id=\"A1.T6.3.1.8.4\" class=\"ltx_td ltx_align_center\">SGD</td>\n<td id=\"A1.T6.3.1.8.5\" class=\"ltx_td ltx_align_center\">SGD</td>\n</tr>\n<tr id=\"A1.T6.3.1.9\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.1.9.1\" class=\"ltx_td ltx_align_left\">Momentum</td>\n<td id=\"A1.T6.3.1.9.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T6.3.1.9.3\" class=\"ltx_td ltx_align_center\">0.9</td>\n<td id=\"A1.T6.3.1.9.4\" class=\"ltx_td ltx_align_center\">0.9</td>\n<td id=\"A1.T6.3.1.9.5\" class=\"ltx_td ltx_align_center\">0.9</td>\n</tr>\n<tr id=\"A1.T6.3.1.10\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.1.10.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">Weight Decay</td>\n<td id=\"A1.T6.3.1.10.2\" class=\"ltx_td ltx_border_bb\"></td>\n<td id=\"A1.T6.3.1.10.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">5.00E-04</td>\n<td id=\"A1.T6.3.1.10.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">5.00E-04</td>\n<td id=\"A1.T6.3.1.10.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">5.00E-04</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Hyperparameter Selection in Table 2. The detailed experiment setups for Table 2 are listed in Table 6, Table 7, Table 8 and Table 9. For the experiments related to FedGen555FedGen: https://github.com/zhuangdizhu/FedGen and DynaFed666DynaFed: https://github.com/pipilurj/DynaFed/tree/main, we evaluate them with their official implementation code on GitHub."
        ]
    },
    "A1.T7": {
        "caption": "Table 7: Experimental setup details of GPT-FL with ConvNet in Table 2",
        "table": "<table id=\"A1.T7.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A1.T7.3.1.1\" class=\"ltx_tr\">\n<td id=\"A1.T7.3.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"A1.T7.3.1.1.2\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"A1.T7.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A1.T7.3.1.1.3.1\" class=\"ltx_text ltx_font_bold\">CIFAR-10</span></td>\n<td id=\"A1.T7.3.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A1.T7.3.1.1.4.1\" class=\"ltx_text ltx_font_bold\">CIFAR-100</span></td>\n<td id=\"A1.T7.3.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A1.T7.3.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Flowers102</span></td>\n</tr>\n<tr id=\"A1.T7.3.1.2\" class=\"ltx_tr\">\n<td id=\"A1.T7.3.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Local Epoch</td>\n<td id=\"A1.T7.3.1.2.2\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"A1.T7.3.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td id=\"A1.T7.3.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td id=\"A1.T7.3.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n</tr>\n<tr id=\"A1.T7.3.1.3\" class=\"ltx_tr\">\n<td id=\"A1.T7.3.1.3.1\" class=\"ltx_td ltx_align_left\">Communication Rounds</td>\n<td id=\"A1.T7.3.1.3.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T7.3.1.3.3\" class=\"ltx_td ltx_align_center\">500</td>\n<td id=\"A1.T7.3.1.3.4\" class=\"ltx_td ltx_align_center\">500</td>\n<td id=\"A1.T7.3.1.3.5\" class=\"ltx_td ltx_align_center\">500</td>\n</tr>\n<tr id=\"A1.T7.3.1.4\" class=\"ltx_tr\">\n<td id=\"A1.T7.3.1.4.1\" class=\"ltx_td ltx_align_left\">Cohort Size</td>\n<td id=\"A1.T7.3.1.4.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T7.3.1.4.3\" class=\"ltx_td ltx_align_center\">10</td>\n<td id=\"A1.T7.3.1.4.4\" class=\"ltx_td ltx_align_center\">10</td>\n<td id=\"A1.T7.3.1.4.5\" class=\"ltx_td ltx_align_center\">50</td>\n</tr>\n<tr id=\"A1.T7.3.1.5\" class=\"ltx_tr\">\n<td id=\"A1.T7.3.1.5.1\" class=\"ltx_td ltx_align_left\">Batch Size</td>\n<td id=\"A1.T7.3.1.5.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T7.3.1.5.3\" class=\"ltx_td ltx_align_center\">32</td>\n<td id=\"A1.T7.3.1.5.4\" class=\"ltx_td ltx_align_center\">32</td>\n<td id=\"A1.T7.3.1.5.5\" class=\"ltx_td ltx_align_center\">32</td>\n</tr>\n<tr id=\"A1.T7.3.1.6\" class=\"ltx_tr\">\n<td id=\"A1.T7.3.1.6.1\" class=\"ltx_td ltx_align_left\" rowspan=\"2\"><span id=\"A1.T7.3.1.6.1.1\" class=\"ltx_text\">Client Learning Rate</span></td>\n<td id=\"A1.T7.3.1.6.2\" class=\"ltx_td ltx_align_left\">High Data Heterogeneity</td>\n<td id=\"A1.T7.3.1.6.3\" class=\"ltx_td ltx_align_center\">2.00E-07</td>\n<td id=\"A1.T7.3.1.6.4\" class=\"ltx_td ltx_align_center\">1.00E-04</td>\n<td id=\"A1.T7.3.1.6.5\" class=\"ltx_td ltx_align_center\">1.00E-04</td>\n</tr>\n<tr id=\"A1.T7.3.1.7\" class=\"ltx_tr\">\n<td id=\"A1.T7.3.1.7.1\" class=\"ltx_td ltx_align_left\">Low Data Heterogeneity</td>\n<td id=\"A1.T7.3.1.7.2\" class=\"ltx_td ltx_align_center\">5.00E-06</td>\n<td id=\"A1.T7.3.1.7.3\" class=\"ltx_td ltx_align_center\">1.00E-04</td>\n<td id=\"A1.T7.3.1.7.4\" class=\"ltx_td ltx_align_center\">5.00E-03</td>\n</tr>\n<tr id=\"A1.T7.3.1.8\" class=\"ltx_tr\">\n<td id=\"A1.T7.3.1.8.1\" class=\"ltx_td ltx_align_left\">Optimizer</td>\n<td id=\"A1.T7.3.1.8.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T7.3.1.8.3\" class=\"ltx_td ltx_align_center\">AdamW</td>\n<td id=\"A1.T7.3.1.8.4\" class=\"ltx_td ltx_align_center\">AdamW</td>\n<td id=\"A1.T7.3.1.8.5\" class=\"ltx_td ltx_align_center\">SGD</td>\n</tr>\n<tr id=\"A1.T7.3.1.9\" class=\"ltx_tr\">\n<td id=\"A1.T7.3.1.9.1\" class=\"ltx_td ltx_align_left\">Betas</td>\n<td id=\"A1.T7.3.1.9.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T7.3.1.9.3\" class=\"ltx_td ltx_align_center\">(0.9, 0.999)</td>\n<td id=\"A1.T7.3.1.9.4\" class=\"ltx_td ltx_align_center\">(0.9, 0.999)</td>\n<td id=\"A1.T7.3.1.9.5\" class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr id=\"A1.T7.3.1.10\" class=\"ltx_tr\">\n<td id=\"A1.T7.3.1.10.1\" class=\"ltx_td ltx_align_left\">Eps</td>\n<td id=\"A1.T7.3.1.10.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T7.3.1.10.3\" class=\"ltx_td ltx_align_center\">1.00E-08</td>\n<td id=\"A1.T7.3.1.10.4\" class=\"ltx_td ltx_align_center\">1.00E-08</td>\n<td id=\"A1.T7.3.1.10.5\" class=\"ltx_td ltx_align_center\">N/A</td>\n</tr>\n<tr id=\"A1.T7.3.1.11\" class=\"ltx_tr\">\n<td id=\"A1.T7.3.1.11.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">Weight Decay</td>\n<td id=\"A1.T7.3.1.11.2\" class=\"ltx_td ltx_border_bb\"></td>\n<td id=\"A1.T7.3.1.11.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">5.00E-04</td>\n<td id=\"A1.T7.3.1.11.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">5.00E-04</td>\n<td id=\"A1.T7.3.1.11.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">5.00E-04</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Hyperparameter Selection in Table 2. The detailed experiment setups for Table 2 are listed in Table 6, Table 7, Table 8 and Table 9. For the experiments related to FedGen555FedGen: https://github.com/zhuangdizhu/FedGen and DynaFed666DynaFed: https://github.com/pipilurj/DynaFed/tree/main, we evaluate them with their official implementation code on GitHub."
        ]
    },
    "A1.T8": {
        "caption": "Table 8: Experimental setup details of FedGen with ConvNet in Table 2",
        "table": "<table id=\"A1.T8.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A1.T8.1.1.1\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"A1.T8.1.1.1.2\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"A1.T8.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A1.T8.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">CIFAR-10</span></td>\n<td id=\"A1.T8.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A1.T8.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">CIFAR-100</span></td>\n<td id=\"A1.T8.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A1.T8.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Flowers102</span></td>\n</tr>\n<tr id=\"A1.T8.1.1.2\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Local Epoch</td>\n<td id=\"A1.T8.1.1.2.2\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"A1.T8.1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td id=\"A1.T8.1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">5</td>\n<td id=\"A1.T8.1.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">5</td>\n</tr>\n<tr id=\"A1.T8.1.1.3\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.1.3.1\" class=\"ltx_td ltx_align_left\">Communication Rounds</td>\n<td id=\"A1.T8.1.1.3.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T8.1.1.3.3\" class=\"ltx_td ltx_align_center\">500</td>\n<td id=\"A1.T8.1.1.3.4\" class=\"ltx_td ltx_align_center\">500</td>\n<td id=\"A1.T8.1.1.3.5\" class=\"ltx_td ltx_align_center\">500</td>\n</tr>\n<tr id=\"A1.T8.1.1.4\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.1.4.1\" class=\"ltx_td ltx_align_left\">Cohort Size</td>\n<td id=\"A1.T8.1.1.4.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T8.1.1.4.3\" class=\"ltx_td ltx_align_center\">10</td>\n<td id=\"A1.T8.1.1.4.4\" class=\"ltx_td ltx_align_center\">10</td>\n<td id=\"A1.T8.1.1.4.5\" class=\"ltx_td ltx_align_center\">50</td>\n</tr>\n<tr id=\"A1.T8.1.1.5\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.1.5.1\" class=\"ltx_td ltx_align_left\">Batch Size</td>\n<td id=\"A1.T8.1.1.5.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T8.1.1.5.3\" class=\"ltx_td ltx_align_center\">32</td>\n<td id=\"A1.T8.1.1.5.4\" class=\"ltx_td ltx_align_center\">32</td>\n<td id=\"A1.T8.1.1.5.5\" class=\"ltx_td ltx_align_center\">32</td>\n</tr>\n<tr id=\"A1.T8.1.1.6\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.1.6.1\" class=\"ltx_td ltx_align_left\">Generator Batch Size</td>\n<td id=\"A1.T8.1.1.6.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T8.1.1.6.3\" class=\"ltx_td ltx_align_center\">32</td>\n<td id=\"A1.T8.1.1.6.4\" class=\"ltx_td ltx_align_center\">32</td>\n<td id=\"A1.T8.1.1.6.5\" class=\"ltx_td ltx_align_center\">32</td>\n</tr>\n<tr id=\"A1.T8.1.1.7\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.1.7.1\" class=\"ltx_td ltx_align_left\" rowspan=\"2\"><span id=\"A1.T8.1.1.7.1.1\" class=\"ltx_text\">Client Learning Rate</span></td>\n<td id=\"A1.T8.1.1.7.2\" class=\"ltx_td ltx_align_left\">High Data Heterogeneity</td>\n<td id=\"A1.T8.1.1.7.3\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n<td id=\"A1.T8.1.1.7.4\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n<td id=\"A1.T8.1.1.7.5\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n</tr>\n<tr id=\"A1.T8.1.1.8\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.1.8.1\" class=\"ltx_td ltx_align_left\">Low Data Heterogeneity</td>\n<td id=\"A1.T8.1.1.8.2\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n<td id=\"A1.T8.1.1.8.3\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n<td id=\"A1.T8.1.1.8.4\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n</tr>\n<tr id=\"A1.T8.1.1.9\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.1.9.1\" class=\"ltx_td ltx_align_left\">Ensemble Learning Rate</td>\n<td id=\"A1.T8.1.1.9.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T8.1.1.9.3\" class=\"ltx_td ltx_align_center\">1.00E-04</td>\n<td id=\"A1.T8.1.1.9.4\" class=\"ltx_td ltx_align_center\">1.00E-04</td>\n<td id=\"A1.T8.1.1.9.5\" class=\"ltx_td ltx_align_center\">1.00E-04</td>\n</tr>\n<tr id=\"A1.T8.1.1.10\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.1.10.1\" class=\"ltx_td ltx_align_left\">Personal Learning Rate</td>\n<td id=\"A1.T8.1.1.10.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T8.1.1.10.3\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n<td id=\"A1.T8.1.1.10.4\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n<td id=\"A1.T8.1.1.10.5\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n</tr>\n<tr id=\"A1.T8.1.1.11\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.1.11.1\" class=\"ltx_td ltx_align_left\">Optimizer</td>\n<td id=\"A1.T8.1.1.11.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T8.1.1.11.3\" class=\"ltx_td ltx_align_center\">Adam</td>\n<td id=\"A1.T8.1.1.11.4\" class=\"ltx_td ltx_align_center\">Adam</td>\n<td id=\"A1.T8.1.1.11.5\" class=\"ltx_td ltx_align_center\">Adam</td>\n</tr>\n<tr id=\"A1.T8.1.1.12\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.1.12.1\" class=\"ltx_td ltx_align_left\">Betas</td>\n<td id=\"A1.T8.1.1.12.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T8.1.1.12.3\" class=\"ltx_td ltx_align_center\">(0.9, 0.999)</td>\n<td id=\"A1.T8.1.1.12.4\" class=\"ltx_td ltx_align_center\">(0.9, 0.999)</td>\n<td id=\"A1.T8.1.1.12.5\" class=\"ltx_td ltx_align_center\">(0.9, 0.999)</td>\n</tr>\n<tr id=\"A1.T8.1.1.13\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.1.13.1\" class=\"ltx_td ltx_align_left\">Eps</td>\n<td id=\"A1.T8.1.1.13.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T8.1.1.13.3\" class=\"ltx_td ltx_align_center\">1.00E-08</td>\n<td id=\"A1.T8.1.1.13.4\" class=\"ltx_td ltx_align_center\">1.00E-08</td>\n<td id=\"A1.T8.1.1.13.5\" class=\"ltx_td ltx_align_center\">1.00E-08</td>\n</tr>\n<tr id=\"A1.T8.1.1.14\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.1.14.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">Weight Decay</td>\n<td id=\"A1.T8.1.1.14.2\" class=\"ltx_td ltx_border_bb\"></td>\n<td id=\"A1.T8.1.1.14.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">1.00E-02</td>\n<td id=\"A1.T8.1.1.14.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">1.00E-02</td>\n<td id=\"A1.T8.1.1.14.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">1.00E-02</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Hyperparameter Selection in Table 2. The detailed experiment setups for Table 2 are listed in Table 6, Table 7, Table 8 and Table 9. For the experiments related to FedGen555FedGen: https://github.com/zhuangdizhu/FedGen and DynaFed666DynaFed: https://github.com/pipilurj/DynaFed/tree/main, we evaluate them with their official implementation code on GitHub."
        ]
    },
    "A1.T9": {
        "caption": "Table 9: Experimental setup details of DynaFed with ConvNet in Table 2",
        "table": "<table id=\"A1.T9.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A1.T9.1.1.1\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"A1.T9.1.1.1.2\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"A1.T9.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A1.T9.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">CIFAR-10</span></td>\n<td id=\"A1.T9.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A1.T9.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">CIFAR-100</span></td>\n<td id=\"A1.T9.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A1.T9.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Flowers102</span></td>\n</tr>\n<tr id=\"A1.T9.1.1.2\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Local Epoch</td>\n<td id=\"A1.T9.1.1.2.2\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"A1.T9.1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td id=\"A1.T9.1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td id=\"A1.T9.1.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n</tr>\n<tr id=\"A1.T9.1.1.3\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.3.1\" class=\"ltx_td ltx_align_left\">Communication Rounds</td>\n<td id=\"A1.T9.1.1.3.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T9.1.1.3.3\" class=\"ltx_td ltx_align_center\">500</td>\n<td id=\"A1.T9.1.1.3.4\" class=\"ltx_td ltx_align_center\">500</td>\n<td id=\"A1.T9.1.1.3.5\" class=\"ltx_td ltx_align_center\">500</td>\n</tr>\n<tr id=\"A1.T9.1.1.4\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.4.1\" class=\"ltx_td ltx_align_left\">Cohort Size</td>\n<td id=\"A1.T9.1.1.4.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T9.1.1.4.3\" class=\"ltx_td ltx_align_center\">10</td>\n<td id=\"A1.T9.1.1.4.4\" class=\"ltx_td ltx_align_center\">10</td>\n<td id=\"A1.T9.1.1.4.5\" class=\"ltx_td ltx_align_center\">50</td>\n</tr>\n<tr id=\"A1.T9.1.1.5\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.5.1\" class=\"ltx_td ltx_align_left\">Batch Size</td>\n<td id=\"A1.T9.1.1.5.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T9.1.1.5.3\" class=\"ltx_td ltx_align_center\">32</td>\n<td id=\"A1.T9.1.1.5.4\" class=\"ltx_td ltx_align_center\">32</td>\n<td id=\"A1.T9.1.1.5.5\" class=\"ltx_td ltx_align_center\">32</td>\n</tr>\n<tr id=\"A1.T9.1.1.6\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.6.1\" class=\"ltx_td ltx_align_left\">Synthetic Images Learning Rate</td>\n<td id=\"A1.T9.1.1.6.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T9.1.1.6.3\" class=\"ltx_td ltx_align_center\">5.00E-02</td>\n<td id=\"A1.T9.1.1.6.4\" class=\"ltx_td ltx_align_center\">5.00E-02</td>\n<td id=\"A1.T9.1.1.6.5\" class=\"ltx_td ltx_align_center\">5.00E-02</td>\n</tr>\n<tr id=\"A1.T9.1.1.7\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.7.1\" class=\"ltx_td ltx_align_left\">Distill Interval</td>\n<td id=\"A1.T9.1.1.7.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T9.1.1.7.3\" class=\"ltx_td ltx_align_center\">1</td>\n<td id=\"A1.T9.1.1.7.4\" class=\"ltx_td ltx_align_center\">1</td>\n<td id=\"A1.T9.1.1.7.5\" class=\"ltx_td ltx_align_center\">1</td>\n</tr>\n<tr id=\"A1.T9.1.1.8\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.8.1\" class=\"ltx_td ltx_align_left\">Distill Iteration</td>\n<td id=\"A1.T9.1.1.8.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T9.1.1.8.3\" class=\"ltx_td ltx_align_center\">20</td>\n<td id=\"A1.T9.1.1.8.4\" class=\"ltx_td ltx_align_center\">8</td>\n<td id=\"A1.T9.1.1.8.5\" class=\"ltx_td ltx_align_center\">20</td>\n</tr>\n<tr id=\"A1.T9.1.1.9\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.9.1\" class=\"ltx_td ltx_align_left\">Distill Step</td>\n<td id=\"A1.T9.1.1.9.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T9.1.1.9.3\" class=\"ltx_td ltx_align_center\">3000</td>\n<td id=\"A1.T9.1.1.9.4\" class=\"ltx_td ltx_align_center\">200</td>\n<td id=\"A1.T9.1.1.9.5\" class=\"ltx_td ltx_align_center\">500</td>\n</tr>\n<tr id=\"A1.T9.1.1.10\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.10.1\" class=\"ltx_td ltx_align_left\">Distill Learning Rate</td>\n<td id=\"A1.T9.1.1.10.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T9.1.1.10.3\" class=\"ltx_td ltx_align_center\">1.00E-04</td>\n<td id=\"A1.T9.1.1.10.4\" class=\"ltx_td ltx_align_center\">1.00E-04</td>\n<td id=\"A1.T9.1.1.10.5\" class=\"ltx_td ltx_align_center\">1.00E-04</td>\n</tr>\n<tr id=\"A1.T9.1.1.11\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.11.1\" class=\"ltx_td ltx_align_left\" rowspan=\"2\"><span id=\"A1.T9.1.1.11.1.1\" class=\"ltx_text\">Client Learning Rate</span></td>\n<td id=\"A1.T9.1.1.11.2\" class=\"ltx_td ltx_align_left\">High Data Heterogeneity</td>\n<td id=\"A1.T9.1.1.11.3\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n<td id=\"A1.T9.1.1.11.4\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n<td id=\"A1.T9.1.1.11.5\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n</tr>\n<tr id=\"A1.T9.1.1.12\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.12.1\" class=\"ltx_td ltx_align_left\">Low Data Heterogeneity</td>\n<td id=\"A1.T9.1.1.12.2\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n<td id=\"A1.T9.1.1.12.3\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n<td id=\"A1.T9.1.1.12.4\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n</tr>\n<tr id=\"A1.T9.1.1.13\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.13.1\" class=\"ltx_td ltx_align_left\">Ensemble Learning Rate</td>\n<td id=\"A1.T9.1.1.13.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T9.1.1.13.3\" class=\"ltx_td ltx_align_center\">1.00E-04</td>\n<td id=\"A1.T9.1.1.13.4\" class=\"ltx_td ltx_align_center\">1.00E-04</td>\n<td id=\"A1.T9.1.1.13.5\" class=\"ltx_td ltx_align_center\">1.00E-04</td>\n</tr>\n<tr id=\"A1.T9.1.1.14\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.14.1\" class=\"ltx_td ltx_align_left\">Personal Learning Rate</td>\n<td id=\"A1.T9.1.1.14.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T9.1.1.14.3\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n<td id=\"A1.T9.1.1.14.4\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n<td id=\"A1.T9.1.1.14.5\" class=\"ltx_td ltx_align_center\">1.00E-02</td>\n</tr>\n<tr id=\"A1.T9.1.1.15\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.15.1\" class=\"ltx_td ltx_align_left\">Optimizer</td>\n<td id=\"A1.T9.1.1.15.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T9.1.1.15.3\" class=\"ltx_td ltx_align_center\">Adam</td>\n<td id=\"A1.T9.1.1.15.4\" class=\"ltx_td ltx_align_center\">Adam</td>\n<td id=\"A1.T9.1.1.15.5\" class=\"ltx_td ltx_align_center\">Adam</td>\n</tr>\n<tr id=\"A1.T9.1.1.16\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.16.1\" class=\"ltx_td ltx_align_left\">Betas</td>\n<td id=\"A1.T9.1.1.16.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T9.1.1.16.3\" class=\"ltx_td ltx_align_center\">(0.9, 0.999)</td>\n<td id=\"A1.T9.1.1.16.4\" class=\"ltx_td ltx_align_center\">(0.9, 0.999)</td>\n<td id=\"A1.T9.1.1.16.5\" class=\"ltx_td ltx_align_center\">(0.9, 0.999)</td>\n</tr>\n<tr id=\"A1.T9.1.1.17\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.17.1\" class=\"ltx_td ltx_align_left\">Eps</td>\n<td id=\"A1.T9.1.1.17.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T9.1.1.17.3\" class=\"ltx_td ltx_align_center\">1.00E-08</td>\n<td id=\"A1.T9.1.1.17.4\" class=\"ltx_td ltx_align_center\">1.00E-08</td>\n<td id=\"A1.T9.1.1.17.5\" class=\"ltx_td ltx_align_center\">1.00E-08</td>\n</tr>\n<tr id=\"A1.T9.1.1.18\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.18.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">Weight Decay</td>\n<td id=\"A1.T9.1.1.18.2\" class=\"ltx_td ltx_border_bb\"></td>\n<td id=\"A1.T9.1.1.18.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">1.00E-02</td>\n<td id=\"A1.T9.1.1.18.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">1.00E-02</td>\n<td id=\"A1.T9.1.1.18.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">1.00E-02</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Hyperparameter Selection in Table 2. The detailed experiment setups for Table 2 are listed in Table 6, Table 7, Table 8 and Table 9. For the experiments related to FedGen555FedGen: https://github.com/zhuangdizhu/FedGen and DynaFed666DynaFed: https://github.com/pipilurj/DynaFed/tree/main, we evaluate them with their official implementation code on GitHub."
        ]
    }
}