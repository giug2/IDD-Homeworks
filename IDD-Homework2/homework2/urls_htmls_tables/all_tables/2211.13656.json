{
    "PAPER'S NUMBER OF TABLES": 6,
    "S2.T1": {
        "caption": "TABLE I: Related work on FL hyper-parameter optimization. We tag if (1) the work can run in an online and single trail manner and (2) the work targets system overheads of FL training. ",
        "table": "",
        "footnotes": "\n\n\n\n\n\nWork\nDescription\nSingle trial\nSystem\n\n\n\nFTSÂ [21]\noptimize client models\nâœ—\nâœ—\n\nDP-FTS-DEÂ [22]\ntrade-off privacy and utility\nâœ—\nâœ—\n\nAuto-FedRLÂ [23]\nimprove model accuracy\nâœ“\nâœ—\n\n[24]\nimprove training robustness\nâœ“\nâœ—\n\nFedExÂ [25]\nNAS based framework\nâœ—\nâœ—\n\nFLoRAÂ [26]\nNAS based framework\nâœ“\nâœ—\n\nFedTune (Ours)\na lightweight framework\nâœ“\nâœ“\n\n\n",
        "references": [
            "Designing HPO methods for FL is a new research area. Only a few approaches have touched FL HPO problems. TableÂ I lists several representative approaches, where we highlight whether the work can execute in an online and single trial manner and whether the work targets system overhead in FL training.\nFor example, BO has been integrated with FL to improve different client modelsÂ [21] and strength client privacyÂ [22]. Several approaches apply reinforcement learning to adjust FL hyper-parametersÂ [23, 24], which introduces extra complexity and loss of generality. FedEx is a general framework to optimize the round-to-accuracy of FL by exploiting the Neural Architecture Search (NAS) techniques of weight-sharing, which improves the baseline by several percentage pointsÂ [25]; FLoRA determines the global hyper-parameters by selecting the hyper-parameters that have good performances in local clientsÂ [26]. Recently, a benchmark suite for federated\nhyper-parameter optimizationÂ [31] is designed, whose effectiveness remains to be investigated."
        ]
    },
    "S3.T2": {
        "caption": "TABLE II: Different models used for the measurement study.",
        "table": "<table id=\"S3.T2.2.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T2.2.2.3.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.2.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row\">Model</th>\n<th id=\"S3.T2.2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">ResNet-10</th>\n<th id=\"S3.T2.2.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">ResNet-18</th>\n<th id=\"S3.T2.2.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">ResNet-26</th>\n<th id=\"S3.T2.2.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">ResNet-34</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.2.2.4.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.2.4.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\">#BasicBlock</th>\n<td id=\"S3.T2.2.2.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">[1, 1, 1, 1]</td>\n<td id=\"S3.T2.2.2.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">[2, 2, 2, 2]</td>\n<td id=\"S3.T2.2.2.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">[3, 3, 3, 3]</td>\n<td id=\"S3.T2.2.2.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">[3, 4, 6, 3]</td>\n</tr>\n<tr id=\"S3.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">#FLOP (<math id=\"S3.T2.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times 10^{6}\" display=\"inline\"><semantics id=\"S3.T2.1.1.1.1.m1.1a\"><mrow id=\"S3.T2.1.1.1.1.m1.1.1\" xref=\"S3.T2.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T2.1.1.1.1.m1.1.1.2\" xref=\"S3.T2.1.1.1.1.m1.1.1.2.cmml\"></mi><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S3.T2.1.1.1.1.m1.1.1.1\" xref=\"S3.T2.1.1.1.1.m1.1.1.1.cmml\">Ã—</mo><msup id=\"S3.T2.1.1.1.1.m1.1.1.3\" xref=\"S3.T2.1.1.1.1.m1.1.1.3.cmml\"><mn id=\"S3.T2.1.1.1.1.m1.1.1.3.2\" xref=\"S3.T2.1.1.1.1.m1.1.1.3.2.cmml\">10</mn><mn id=\"S3.T2.1.1.1.1.m1.1.1.3.3\" xref=\"S3.T2.1.1.1.1.m1.1.1.3.3.cmml\">6</mn></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.1.1.1.1.m1.1b\"><apply id=\"S3.T2.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T2.1.1.1.1.m1.1.1\"><times id=\"S3.T2.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T2.1.1.1.1.m1.1.1.1\"></times><csymbol cd=\"latexml\" id=\"S3.T2.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T2.1.1.1.1.m1.1.1.2\">absent</csymbol><apply id=\"S3.T2.1.1.1.1.m1.1.1.3.cmml\" xref=\"S3.T2.1.1.1.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S3.T2.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"S3.T2.1.1.1.1.m1.1.1.3\">superscript</csymbol><cn type=\"integer\" id=\"S3.T2.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"S3.T2.1.1.1.1.m1.1.1.3.2\">10</cn><cn type=\"integer\" id=\"S3.T2.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"S3.T2.1.1.1.1.m1.1.1.3.3\">6</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.1.1.1.1.m1.1c\">\\times 10^{6}</annotation></semantics></math>)</th>\n<td id=\"S3.T2.1.1.1.2\" class=\"ltx_td ltx_align_center\">12.5</td>\n<td id=\"S3.T2.1.1.1.3\" class=\"ltx_td ltx_align_center\">26.8</td>\n<td id=\"S3.T2.1.1.1.4\" class=\"ltx_td ltx_align_center\">41.1</td>\n<td id=\"S3.T2.1.1.1.5\" class=\"ltx_td ltx_align_center\">60.1</td>\n</tr>\n<tr id=\"S3.T2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">#Params (<math id=\"S3.T2.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times 10^{3}\" display=\"inline\"><semantics id=\"S3.T2.2.2.2.1.m1.1a\"><mrow id=\"S3.T2.2.2.2.1.m1.1.1\" xref=\"S3.T2.2.2.2.1.m1.1.1.cmml\"><mi id=\"S3.T2.2.2.2.1.m1.1.1.2\" xref=\"S3.T2.2.2.2.1.m1.1.1.2.cmml\"></mi><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S3.T2.2.2.2.1.m1.1.1.1\" xref=\"S3.T2.2.2.2.1.m1.1.1.1.cmml\">Ã—</mo><msup id=\"S3.T2.2.2.2.1.m1.1.1.3\" xref=\"S3.T2.2.2.2.1.m1.1.1.3.cmml\"><mn id=\"S3.T2.2.2.2.1.m1.1.1.3.2\" xref=\"S3.T2.2.2.2.1.m1.1.1.3.2.cmml\">10</mn><mn id=\"S3.T2.2.2.2.1.m1.1.1.3.3\" xref=\"S3.T2.2.2.2.1.m1.1.1.3.3.cmml\">3</mn></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.2.2.2.1.m1.1b\"><apply id=\"S3.T2.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T2.2.2.2.1.m1.1.1\"><times id=\"S3.T2.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T2.2.2.2.1.m1.1.1.1\"></times><csymbol cd=\"latexml\" id=\"S3.T2.2.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T2.2.2.2.1.m1.1.1.2\">absent</csymbol><apply id=\"S3.T2.2.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T2.2.2.2.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S3.T2.2.2.2.1.m1.1.1.3.1.cmml\" xref=\"S3.T2.2.2.2.1.m1.1.1.3\">superscript</csymbol><cn type=\"integer\" id=\"S3.T2.2.2.2.1.m1.1.1.3.2.cmml\" xref=\"S3.T2.2.2.2.1.m1.1.1.3.2\">10</cn><cn type=\"integer\" id=\"S3.T2.2.2.2.1.m1.1.1.3.3.cmml\" xref=\"S3.T2.2.2.2.1.m1.1.1.3.3\">3</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.2.2.2.1.m1.1c\">\\times 10^{3}</annotation></semantics></math>)</th>\n<td id=\"S3.T2.2.2.2.2\" class=\"ltx_td ltx_align_center\">79.7</td>\n<td id=\"S3.T2.2.2.2.3\" class=\"ltx_td ltx_align_center\">177.2</td>\n<td id=\"S3.T2.2.2.2.4\" class=\"ltx_td ltx_align_center\">274.6</td>\n<td id=\"S3.T2.2.2.2.5\" class=\"ltx_td ltx_align_center\">515.6</td>\n</tr>\n<tr id=\"S3.T2.2.2.5.2\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.2.5.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">Accuracy</th>\n<td id=\"S3.T2.2.2.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.88</td>\n<td id=\"S3.T2.2.2.5.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.90</td>\n<td id=\"S3.T2.2.2.5.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.90</td>\n<td id=\"S3.T2.2.2.5.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.92</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Model complexity. We also investigate how the model complexity influences the training overheads if a target accuracy is met. Although it is common knowledge that smaller models have better time and computation performance in other paradigms of model training, we are the first to report the four system overhead versus model complexity in the FL setting.\nWe use ResNetÂ [33] to build different models, as listed in Table II.\n",
            "Model Complexity.\nTable II tabulates the models for comparing training overheads versus model complexity. In this experiment, we select one participant (M=1ğ‘€1M=1) to train one pass (E=1ğ¸1E=1) on each training round.\nFig.Â 5 shows the normalized CompT, TransT, CompL, and TransL for different models. The x-axis is the target model accuracy, and the y-axis is the corresponding overhead to reach that model accuracy. Since only one client and one training pass are used on each round, CompT and CompL have the same normalized comparison, and so are TransT and TransL. The results show that smaller models are better in terms of all training aspects. In addition, it is interesting to note that heavier models have higher increase rates of overheads versus model accuracy. This means that model selection is especially essential for high model accuracy applications."
        ]
    },
    "S3.T3": {
        "caption": "TABLE III: System overheads versus the number of participants Mğ‘€M, the number of training passes Eğ¸E, and model complexity. â€˜<<â€™, â€˜==â€™, and â€˜>>â€™ means the smaller the better, does not matter, and the larger the better, respectively.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nTraining aspect\n\n\nMğ‘€M\n\n\n\nEğ¸E\n\nModel complexity\n\n\n\nCompT\n\n\n>>\n\n\n\n<<\n\n<<\n\nCompL\n\n\n<<\n\n\n\n<<\n\n<<\n\nTransT\n\n\n>>\n\n\n\n>>\n\n<<\n\nTransL\n\n\n<<\n\n\n\n>>\n\n<<\n\nModel Accuracy\n\n\n==\n\n\n\n==\n\n>>\n\n\n",
        "references": [
            "Based on our measurement study, we summarize systems overheads versus FL hyper-parameters in Table III. As we can see, CompT, TransT, CompL, and TransL conflict with each other in selecting the optimal Mğ‘€M and Eğ¸E.\nRegarding model complexity, smaller models have better system overheads if the model accuracy is satisfied. Please note that Table III is consistent with existing work (e.g., [12]), but is more comprehensive.\nThus, Table III is also valid for other datasets and ML models.",
            "We illustrate how to approximate Î”â€‹MÎ”ğ‘€\\Delta M. The process for Î”â€‹EÎ”ğ¸\\Delta E is similar.\nConsidering that each step makes a small adjustment of Mğ‘€M, âˆ‚tnâ€‹xâ€‹t/âˆ‚Msubscriptğ‘¡ğ‘›ğ‘¥ğ‘¡ğ‘€\\partial t_{nxt}/\\partial M can be represented by (+1)Ã—|tnâ€‹xâ€‹tâˆ’tcâ€‹uâ€‹r|1subscriptğ‘¡ğ‘›ğ‘¥ğ‘¡subscriptğ‘¡ğ‘ğ‘¢ğ‘Ÿ(+1)\\times|t_{nxt}-t_{cur}|, where (+1)1(+1) means CompT prefers larger Mğ‘€M according to TableÂ III. To estimate |tnâ€‹xâ€‹tâˆ’tcâ€‹uâ€‹r|subscriptğ‘¡ğ‘›ğ‘¥ğ‘¡subscriptğ‘¡ğ‘ğ‘¢ğ‘Ÿ|t_{nxt}-t_{cur}|, we apply a linear function Î·tâˆ’1Ã—|tcâ€‹uâ€‹râˆ’tpâ€‹râ€‹v|subscriptğœ‚ğ‘¡1subscriptğ‘¡ğ‘ğ‘¢ğ‘Ÿsubscriptğ‘¡ğ‘ğ‘Ÿğ‘£\\eta_{t-1}\\times|t_{cur}-t_{prv}| where Î·tâˆ’1=|tcâ€‹uâ€‹râˆ’tpâ€‹râ€‹v||tpâ€‹râ€‹vâˆ’tpâ€‹râ€‹vâ€‹pâ€‹râ€‹v|subscriptğœ‚ğ‘¡1subscriptğ‘¡ğ‘ğ‘¢ğ‘Ÿsubscriptğ‘¡ğ‘ğ‘Ÿğ‘£subscriptğ‘¡ğ‘ğ‘Ÿğ‘£subscriptğ‘¡ğ‘ğ‘Ÿğ‘£ğ‘ğ‘Ÿğ‘£\\eta_{t-1}=\\frac{|t_{cur}-t_{prv}|}{|t_{prv}-t_{prvprv}|} (tpâ€‹râ€‹vâ€‹pâ€‹râ€‹vsubscriptğ‘¡ğ‘ğ‘Ÿğ‘£ğ‘ğ‘Ÿğ‘£t_{prvprv} is the CompT at two steps before). Thus, Î·tâˆ’1subscriptğœ‚ğ‘¡1\\eta_{t-1} represents the slope of the linear function. Similarly, we have Î·qâˆ’1subscriptğœ‚ğ‘1\\eta_{q-1}, Î·zâˆ’1subscriptğœ‚ğ‘§1\\eta_{z-1}, Î·vâˆ’1subscriptğœ‚ğ‘£1\\eta_{v-1} for TransT, CompL, and TransL when calculating their derivatives over Mğ‘€M.\nAs a result, Î”â€‹MÎ”ğ‘€\\Delta M can be approximated as"
        ]
    },
    "S4.T4": {
        "caption": "TABLE IV: Performance of FedTune for the speech-to-command dataset when FedAdagrad is used for aggregation. \nâ€˜++â€™ is improvement and â€˜âˆ’-â€™ is degradation. Standard deviation in parentheses. ",
        "table": "",
        "footnotes": "\n\n\n\n\n\nÎ±ğ›¼\\alpha\nÎ²ğ›½\\beta\nÎ³ğ›¾\\gamma\nÎ´ğ›¿\\delta\nCompT (1012superscript101210^{12})\nTransT (106superscript10610^{6})\nCompL (1012superscript101210^{12})\nTransL (106superscript10610^{6})\nFinal M\nFinal E\nOverall\n\n\n\n-\n-\n-\n-\n0.94 (0.01)\n11.61 (0.10)\n5.97 (0.04)\n232.24 (1.99)\n20\n20\n-\n\n1.0\n0.0\n0.0\n0.0\n0.42 (0.02)\n50.19 (2.57)\n4.57 (0.22)\n2418.71 (240.91)\n57.33 (4.50)\n1.00 (0.00)\n+55.23% (2.22%)\n\n0.0\n1.0\n0.0\n0.0\n1.34 (0.22)\n7.68 (1.12)\n14.99 (2.73)\n289.82 (46.98)\n48.00 (2.16)\n48.00 (2.16)\n+33.87% (9.67%)\n\n0.0\n0.0\n1.0\n0.0\n1.02 (0.10)\n615.98 (97.52)\n1.76 (0.16)\n672.21 (91.62)\n1.00 (0.00)\n1.00 (0.00)\n+70.51% (2.75%)\n\n0.0\n0.0\n0.0\n1.0\n2.18 (0.47)\n35.47 (7.51)\n3.30 (0.22)\n76.47 (1.68)\n1.00 (0.00)\n46.67 (3.30)\n+67.07% (0.72%)\n\n0.5\n0.5\n0.0\n0.0\n0.82 (0.13)\n9.17 (1.26)\n9.13 (1.66)\n347.11 (54.31)\n47.33 (2.05)\n21.33 (4.78)\n+16.97% (9.68%)\n\n0.5\n0.0\n0.5\n0.0\n0.48 (0.04)\n81.42 (9.83)\n3.23 (0.14)\n1875.99 (155.21)\n25.00 (1.63)\n1.00 (0.00)\n+47.57% (3.43%)\n\n0.5\n0.0\n0.0\n0.5\n0.79 (0.10)\n11.59 (0.55)\n5.04 (0.89)\n241.86 (68.65)\n22.33 (5.79)\n15.67 (4.50)\n+5.82% (11.28%)\n\n0.0\n0.5\n0.5\n0.0\n0.83 (0.03)\n10.66 (0.15)\n5.16 (0.31)\n207.79 (6.08)\n21.00 (1.41)\n21.00 (1.41)\n+10.87% (2.83%)\n\n0.0\n0.5\n0.0\n0.5\n1.54 (0.16)\n11.48 (3.83)\n9.59 (3.52)\n190.52 (61.53)\n19.67 (14.82)\n49.00 (0.00)\n+9.55% (7.08%)\n\n0.0\n0.0\n0.5\n0.5\n1.69 (0.26)\n50.14 (8.21)\n2.70 (0.26)\n93.21 (8.48)\n1.00 (0.00)\n23.33 (2.49)\n+57.32% (3.76%)\n\n0.33\n0.33\n0.33\n0.0\n0.82 (0.07)\n11.59 (1.01)\n5.65 (0.27)\n255.35 (9.65)\n22.33 (2.62)\n15.67 (1.25)\n+6.09% (6.67%)\n\n0.33\n0.33\n0.0\n0.33\n1.06 (0.08)\n10.07 (0.90)\n8.10 (0.34)\n247.54 (29.18)\n26.33 (2.05)\n27.00 (2.16)\n-1.93% (7.40%)\n\n0.33\n0.0\n0.33\n0.33\n0.91 (0.19)\n18.23 (5.83)\n4.15 (1.13)\n229.26 (63.40)\n12.00 (1.41)\n14.00 (5.72)\n+11.66% (11.76%)\n\n0.0\n0.33\n0.33\n0.33\n1.13 (0.13)\n16.16 (3.36)\n4.51 (0.59)\n169.93 (25.84)\n9.00 (5.35)\n23.00 (4.55)\n+3.99% (6.19%)\n\n0.25\n0.25\n0.25\n0.25\n0.91 (0.10)\n9.73 (1.81)\n6.19 (0.76)\n207.34 (3.34)\n23.33 (5.44)\n22.67 (3.30)\n+6.51% (6.13%)\n\n\n",
        "references": [
            "Benchmarks and Baseline.\nWe evaluate FedTune on three datasets: speech-to-commandÂ [32], EMNISTÂ [34], and Cifar-100Â [35], and three aggregation methods: FedAvgÂ [8], FedNovaÂ [12], and FedAdagradÂ [36]. We set equal values for the combination of training preferences Î±ğ›¼\\alpha, Î²ğ›½\\beta, Î³ğ›¾\\gamma and Î´ğ›¿\\delta (see the first column in TableÂ IV). Therefore, for each dataset, we conduct 15 combinations of training preferences. We set target model accuracy for each dataset and measure CompT, TransT, CompL, and TransL for reaching the target model accuracy. We regard the practice of using fixed Mğ‘€M and Eğ¸E as the baseline and compare FedTune to the baseline by calculating Eq.Â (6). In the evaluation, the positive performance means FedTune reduces the system overheads and the negative performance means the degradation.\nWe implemented FedTune in PyTorch. All the experiments are conducted in a server with 24-GB Nvidia RTX A5000 GPUs.",
            "We present the details of traces when the speech-to-command dataset and the FedAdagrad aggregation method are used.\nTable IV tabulates the results, where we show the application preference (Î±ğ›¼\\alpha, Î²ğ›½\\beta, Î³ğ›¾\\gamma, and Î´ğ›¿\\delta), the system overheads (CompT, TransT, CompL, and TransL), the final Mğ‘€M and Eğ¸E when the training is finished, and the overall performance.\nWe report the average performance, as well as their standard deviations in parentheses. The first row is the baseline, which does not change hyper-parameters during the FL training.\nAs we can see from Table IV, FedTune can adapt to different training preferences. Specifically, FedTune reduces the system overhead up to 70.51% when the application only cares about computation load (i.e., Î³=1ğ›¾1\\gamma=1).\nOnly one preference (0.33, 0.33, 0, 0.33) results in a slightly degraded performance. On average, FedTune improves the overall performance by 26.75% for the speech-to-command dataset and the FedAdagrad aggregation method."
        ]
    },
    "S5.T5": {
        "caption": "TABLE V: Performance of FedTune for diverse datasets when FedAvg aggregation method is applied. ",
        "table": "",
        "footnotes": "\n\n\n\n\nDataset\nSpeech-command\nEMNIST\nCifar-100\n\nData Feature\nVoice\nHandwriting\nImage\n\nML Model\nResNet-10\n2-layer MLP\nResNet-10\n\nPerformance\n+22.48% (17.97%)\n+8.48% (5.51%)\n+9.33% (5.47%)\n\n",
        "references": [
            "Results for Diverse Datasets.\nTable V shows the overall performance of FedTune for different datasets when FedAvg is applied. We set the learning rate to 0.01 for the speech-to-command dataset and the EMNIST dataset, and 0.1 for the Cifar-100 dataset, all with the momentum of 0.9. We show the standard deviation in parenthesis. As shown, FedTune consistently improves the system performance across all the three datasets. In particular, FedTune reduces 22.48% system overhead of the speech-to-command dataset compared to the baseline by averaging the 15 combinations of training preferences.\nWe also observe that the FL training benefits more from FedTune if the training process needs more training rounds to converge. Our experiments with EMNISTÂ (small model) and Cifar100 (low target accuracy) only require a few dozens of training rounds to reach their target model accuracy, and thus their performance gains from FedTune are not significant. The observation is consistent with the decision-making process in FedTune, which increases/decreases hyper-parameters by only one at each step. We leave it as future work to augment FedTune to change hyper-parameters with adaptive degrees."
        ]
    },
    "S5.T6": {
        "caption": "TABLE VI: Performance of FedTune for diverse aggregation algorithms. Speech-to-command dataset and ResNet-10 are used in this experiment.",
        "table": "",
        "footnotes": "\n\n\n\n\nAggregator\nFedAvg\nFedNova\nFedAdagrad\n\nPerformance\n+22.48% (17.97%)\n+23.53% (6.64%)\n+26.75% (6.10%)\n\n",
        "references": [
            "Results for Different Aggregation Methods. Table VI shows the overall performance of FedTune for different aggregation methods when we use the speech-to-command dataset and the ResNet-10 model. We set the learning rate to 0.1, Î²1subscriptğ›½1\\beta_{1} to 0, and Ï„ğœ\\tau to 1e-3 in FedAdagrad. As shown, FedTune achieves consistent performance gain for diverse aggregation methods. In particular, FedTune reduces the system overhead of FedAdagrad by 26.75%."
        ]
    }
}