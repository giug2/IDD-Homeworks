{
    "S5.T1": {
        "caption": "Table 1: Consensus performance on VQA-Rephrasings dataset. CS(k) as defined in Eq.Â 2 is consensus score which is non-zero only if at least kğ‘˜k rephrasings are answered correctly, zero otherwise; averaged across all group of questions. ORI represent a split of questions from VQA-Rephrasings which are original questions from VQA v2.0 and their corresponding rephrasings are represented by the split REP. Models trained with our cycle-consistent (CC) framework consistently outperform their baseline counterparts at all values of kğ‘˜k.",
        "table": "<table id=\"S5.T1.5\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.5.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td id=\"S5.T1.5.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.5pt;padding-right:3.5pt;\" colspan=\"4\"><span id=\"S5.T1.5.1.1.2.1\" class=\"ltx_text ltx_font_bold\">CS(k)</span></td>\n<td id=\"S5.T1.5.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.5pt;padding-right:3.5pt;\" colspan=\"2\"><span id=\"S5.T1.5.1.1.3.1\" class=\"ltx_text ltx_font_bold\">VQA Accuracy</span></td>\n</tr>\n<tr id=\"S5.T1.5.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.2.2.1\" class=\"ltx_td\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"></td>\n<td id=\"S5.T1.5.2.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.2.2.2.1\" class=\"ltx_text ltx_font_bold\">k=1</span></td>\n<td id=\"S5.T1.5.2.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.2.2.3.1\" class=\"ltx_text ltx_font_bold\">k=2</span></td>\n<td id=\"S5.T1.5.2.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.2.2.4.1\" class=\"ltx_text ltx_font_bold\">k=3</span></td>\n<td id=\"S5.T1.5.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.2.2.5.1\" class=\"ltx_text ltx_font_bold\">k=4</span></td>\n<td id=\"S5.T1.5.2.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.2.2.6.1\" class=\"ltx_text ltx_font_bold\">ORI</span></td>\n<td id=\"S5.T1.5.2.2.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.2.2.7.1\" class=\"ltx_text ltx_font_bold\">REP</span></td>\n</tr>\n<tr id=\"S5.T1.5.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">MUTANÂ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">6</a>]</cite>\n</td>\n<td id=\"S5.T1.5.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">56.68</td>\n<td id=\"S5.T1.5.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">43.63</td>\n<td id=\"S5.T1.5.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">38.94</td>\n<td id=\"S5.T1.5.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">32.76</td>\n<td id=\"S5.T1.5.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">59.08</td>\n<td id=\"S5.T1.5.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">46.87</td>\n</tr>\n<tr id=\"S5.T1.5.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">BUTD Â <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">4</a>]</cite>\n</td>\n<td id=\"S5.T1.5.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">60.55</td>\n<td id=\"S5.T1.5.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">46.96</td>\n<td id=\"S5.T1.5.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">40.54</td>\n<td id=\"S5.T1.5.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">34.47</td>\n<td id=\"S5.T1.5.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">61.51</td>\n<td id=\"S5.T1.5.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">51.22</td>\n</tr>\n<tr id=\"S5.T1.5.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.5.5.1\" class=\"ltx_td ltx_align_left\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">BUTD + CC</td>\n<td id=\"S5.T1.5.5.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.5.5.2.1\" class=\"ltx_text ltx_font_bold\">61.66</span></td>\n<td id=\"S5.T1.5.5.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.5.5.3.1\" class=\"ltx_text ltx_font_bold\">50.79</span></td>\n<td id=\"S5.T1.5.5.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.5.5.4.1\" class=\"ltx_text ltx_font_bold\">44.68</span></td>\n<td id=\"S5.T1.5.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.5.5.5.1\" class=\"ltx_text ltx_font_bold\">42.55</span></td>\n<td id=\"S5.T1.5.5.5.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.5.5.6.1\" class=\"ltx_text ltx_font_bold\">62.44</span></td>\n<td id=\"S5.T1.5.5.5.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.5.5.7.1\" class=\"ltx_text ltx_font_bold\">52.58</span></td>\n</tr>\n<tr id=\"S5.T1.5.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.6.6.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">PythiaÂ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">46</a>]</cite>\n</td>\n<td id=\"S5.T1.5.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">63.43</td>\n<td id=\"S5.T1.5.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">52.03</td>\n<td id=\"S5.T1.5.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">45.94</td>\n<td id=\"S5.T1.5.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">39.49</td>\n<td id=\"S5.T1.5.6.6.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">64.08</td>\n<td id=\"S5.T1.5.6.6.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">54.20</td>\n</tr>\n<tr id=\"S5.T1.5.7.7\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.7.7.1\" class=\"ltx_td ltx_align_left\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Pythia + CC</td>\n<td id=\"S5.T1.5.7.7.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.7.7.2.1\" class=\"ltx_text ltx_font_bold\">64.36</span></td>\n<td id=\"S5.T1.5.7.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.7.7.3.1\" class=\"ltx_text ltx_font_bold\">55.45</span></td>\n<td id=\"S5.T1.5.7.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.7.7.4.1\" class=\"ltx_text ltx_font_bold\">50.92</span></td>\n<td id=\"S5.T1.5.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.7.7.5.1\" class=\"ltx_text ltx_font_bold\">44.30</span></td>\n<td id=\"S5.T1.5.7.7.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.7.7.6.1\" class=\"ltx_text ltx_font_bold\">64.52</span></td>\n<td id=\"S5.T1.5.7.7.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.7.7.7.1\" class=\"ltx_text ltx_font_bold\">55.65</span></td>\n</tr>\n<tr id=\"S5.T1.5.8.8\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.8.8.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">BANÂ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a>]</cite>\n</td>\n<td id=\"S5.T1.5.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">64.88</td>\n<td id=\"S5.T1.5.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">53.08</td>\n<td id=\"S5.T1.5.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">47.45</td>\n<td id=\"S5.T1.5.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">39.87</td>\n<td id=\"S5.T1.5.8.8.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">64.97</td>\n<td id=\"S5.T1.5.8.8.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">55.87</td>\n</tr>\n<tr id=\"S5.T1.5.9.9\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.9.9.1\" class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">BAN + CC</td>\n<td id=\"S5.T1.5.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.9.9.2.1\" class=\"ltx_text ltx_font_bold\">65.77</span></td>\n<td id=\"S5.T1.5.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.9.9.3.1\" class=\"ltx_text ltx_font_bold\">56.94</span></td>\n<td id=\"S5.T1.5.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.9.9.4.1\" class=\"ltx_text ltx_font_bold\">51.76</span></td>\n<td id=\"S5.T1.5.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.9.9.5.1\" class=\"ltx_text ltx_font_bold\">48.18</span></td>\n<td id=\"S5.T1.5.9.9.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.9.9.6.1\" class=\"ltx_text ltx_font_bold\">65.87</span></td>\n<td id=\"S5.T1.5.9.9.7\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S5.T1.5.9.9.7.1\" class=\"ltx_text ltx_font_bold\">56.59</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We measure the robustness of each of these models on our proposed VQA-Rephrasings dataset using the consensus score (Eq.Â 2). TableÂ 1 shows the consensus scores at different values of kğ‘˜k for several VQA models. We see that all models suffer significantly when measured for consistency across rephrasings.\nFor e.g., the performance of Pythia (winner of 2018 VQA challenge) is reduced to a consensus score of 39.49% at k=4ğ‘˜4k=4.\nSimilar trends are observed for MUTAN, BAN and BUTD. The drop increases with increasing kğ‘˜k, the number of rephrasings used to measure consistency. Models like BUTD, BAN and Pythia which use word-level encodings of the question suffer significant drops. It is interesting to note that even MUTAN which uses skip-thought based sentence encoding [22] suffers a drop when checked for consistency across rephrasings. We observe that BAN + CC model trained with our proposed cycle-consistent training framework consistently outperforms its counterpart BAN and all other models at all values of kğ‘˜k."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: VQA Performance and ablation studies on VQA v2.0 validation and test-dev splits. Each row in blocks represents a component of our cycle-consistent framework added to the previous row. First row in each block represents the baseline VQA model Fğ¹F. Q-consistency implies addition of a VQG module GğºG to generate rephrasings Qâ€²superscriptğ‘„â€²Q^{\\prime} from the image Iğ¼I and the predicted answer Aâ€²superscriptğ´â€²A^{\\prime} with an associated VQG loss â„’vâ€‹qâ€‹gâ€‹(Q,Qâ€²)subscriptâ„’ğ‘£ğ‘ğ‘”ğ‘„superscriptğ‘„â€²\\mathcal{L}_{vqg}(Q,Q^{\\prime}). A-consistency implies passing all the generated questions Qâ€²superscriptğ‘„â€²Q^{\\prime} to the VQA model Fğ¹F and an associated loss â„’câ€‹yâ€‹câ€‹lâ€‹eâ€‹(A,Aâ€²)subscriptâ„’ğ‘ğ‘¦ğ‘ğ‘™ğ‘’ğ´superscriptğ´â€²\\mathcal{L}_{cycle}(A,A^{\\prime}). Gating implies the use of gating mechanism to filter undesirable generated questions in Qâ€²superscriptğ‘„â€²Q^{\\prime} and passing the remaining to VQA model Fğ¹F. Models trained with our cycle-consistent (last row in each block) framework consistently outperform baselines.",
        "table": "<table id=\"S5.T2.23\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.23.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.23.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S5.T2.23.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td id=\"S5.T2.23.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S5.T2.23.1.1.2.1\" class=\"ltx_text ltx_font_bold\">val</span></td>\n<td id=\"S5.T2.23.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T2.23.1.1.3.1\" class=\"ltx_text ltx_font_bold\">test-dev</span></td>\n</tr>\n<tr id=\"S5.T2.23.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.23.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">MUTAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">6</a>]</cite>\n</th>\n<td id=\"S5.T2.23.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">61.04</td>\n<td id=\"S5.T2.23.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">63.20</td>\n</tr>\n<tr id=\"S5.T2.23.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.23.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">BUTD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">4</a>]</cite>\n</th>\n<td id=\"S5.T2.23.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">65.05</td>\n<td id=\"S5.T2.23.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">66.25</td>\n</tr>\n<tr id=\"S5.T2.23.4.4\" class=\"ltx_tr\">\n<th id=\"S5.T2.23.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">+ Q-consistency</th>\n<td id=\"S5.T2.23.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">65.38</td>\n<td id=\"S5.T2.23.4.4.3\" class=\"ltx_td ltx_align_center\">66.83</td>\n</tr>\n<tr id=\"S5.T2.23.5.5\" class=\"ltx_tr\">\n<th id=\"S5.T2.23.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Â Â â€ƒ+ A-consistency</th>\n<td id=\"S5.T2.23.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">60.84</td>\n<td id=\"S5.T2.23.5.5.3\" class=\"ltx_td ltx_align_center\">62.18</td>\n</tr>\n<tr id=\"S5.T2.23.6.6\" class=\"ltx_tr\">\n<th id=\"S5.T2.23.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Â Â â€ƒâ€ƒ+ Gating</th>\n<td id=\"S5.T2.23.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T2.23.6.6.2.1\" class=\"ltx_text ltx_font_bold\">65.53</span></td>\n<td id=\"S5.T2.23.6.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.23.6.6.3.1\" class=\"ltx_text ltx_font_bold\">67.55</span></td>\n</tr>\n<tr id=\"S5.T2.23.7.7\" class=\"ltx_tr\">\n<th id=\"S5.T2.23.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Pythia <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">46</a>]</cite>\n</th>\n<td id=\"S5.T2.23.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">65.78</td>\n<td id=\"S5.T2.23.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\">68.43</td>\n</tr>\n<tr id=\"S5.T2.23.8.8\" class=\"ltx_tr\">\n<th id=\"S5.T2.23.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">+ Q-consistency</th>\n<td id=\"S5.T2.23.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\">65.39</td>\n<td id=\"S5.T2.23.8.8.3\" class=\"ltx_td ltx_align_center\">68.58</td>\n</tr>\n<tr id=\"S5.T2.23.9.9\" class=\"ltx_tr\">\n<th id=\"S5.T2.23.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Â Â â€ƒ+ A-consistency</th>\n<td id=\"S5.T2.23.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r\">62.08</td>\n<td id=\"S5.T2.23.9.9.3\" class=\"ltx_td ltx_align_center\">63.77</td>\n</tr>\n<tr id=\"S5.T2.23.10.10\" class=\"ltx_tr\">\n<th id=\"S5.T2.23.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Â Â â€ƒâ€ƒ+ Gating</th>\n<td id=\"S5.T2.23.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T2.23.10.10.2.1\" class=\"ltx_text ltx_font_bold\">66.03</span></td>\n<td id=\"S5.T2.23.10.10.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.23.10.10.3.1\" class=\"ltx_text ltx_font_bold\">68.88</span></td>\n</tr>\n<tr id=\"S5.T2.23.11.11\" class=\"ltx_tr\">\n<th id=\"S5.T2.23.11.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">BAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a>]</cite>\n</th>\n<td id=\"S5.T2.23.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">66.04</td>\n<td id=\"S5.T2.23.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_t\">69.64</td>\n</tr>\n<tr id=\"S5.T2.23.12.12\" class=\"ltx_tr\">\n<th id=\"S5.T2.23.12.12.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">+ Q-consistency</th>\n<td id=\"S5.T2.23.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_r\">66.27</td>\n<td id=\"S5.T2.23.12.12.3\" class=\"ltx_td ltx_align_center\">69.69</td>\n</tr>\n<tr id=\"S5.T2.23.13.13\" class=\"ltx_tr\">\n<th id=\"S5.T2.23.13.13.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Â Â â€ƒ+ A-consistency</th>\n<td id=\"S5.T2.23.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_r\">64.96</td>\n<td id=\"S5.T2.23.13.13.3\" class=\"ltx_td ltx_align_center\">66.31</td>\n</tr>\n<tr id=\"S5.T2.23.14.14\" class=\"ltx_tr\">\n<th id=\"S5.T2.23.14.14.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Â Â â€ƒâ€ƒ+ Gating</th>\n<td id=\"S5.T2.23.14.14.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T2.23.14.14.2.1\" class=\"ltx_text ltx_font_bold\">66.77</span></td>\n<td id=\"S5.T2.23.14.14.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T2.23.14.14.3.1\" class=\"ltx_text ltx_font_bold\">69.87</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We find these design choices for question generation module, gating mechanism and late activation to be crucial for effectively training our model.\nWe demonstrate this empirically via ablation studies in TableÂ 2.\nAs we want to increase the robustness of the VQA model to all generated variations, the weights between VQA models which answer the original question and the generated rephrasing are shared. Our formulation of cycle-consistency in VQA can be also thought of as an online data-augmentation technique where the model is trained on several generated rephrasings of the same question and hence is more robust to such anomalies during inference. We show that with clever training strategy, coupled with attention and carefully chosen model architectures for question generation, incorporating cycle consistency for VQA is possible and not only leads to models that are better performing, but also more robust and consistent. In addition, we show that this robustness also imparts VQA models the ability to better predict their own failures.",
            "We now evaluate our approach and various ablations on the standard task of question answering on VQA v2.0 datasetÂ [10].\nWe compare the performance of several VQA models on the validation and test-dev splits of VQA v2.0. It consists of 443,757 training, 214,354 validation and 447,793 testing questions\nspanning over 82,783, 40,504 and 81,434 images respectively. TableÂ 2 shows the VQA scores of different models on validation and test-dev splits. We show that BUTD, Pythia and BAN models trained with our cycle-consistent framework outperform their corresponding baselines.",
            "We show the impact of each component of our cycle-consistent framework by performing ablation studies on our models. We study the marginal effect of components like question consistency (Q-consistency), answer consistency (A-consistency) and gating mechanism by adding them step-by-step to the base VQA model Fğ¹F.\nQ-consistency implies addition of a VQG module GğºG to generate rephrasings Qâ€²superscriptğ‘„â€²Q^{\\prime} from the image Iğ¼I and the predicted answer Aâ€²superscriptğ´â€²A^{\\prime} with an associated VQG loss â„’vâ€‹qâ€‹gâ€‹(Q,Qâ€²)subscriptâ„’ğ‘£ğ‘ğ‘”ğ‘„superscriptğ‘„â€²\\mathcal{L}_{vqg}(Q,Q^{\\prime}). As shown in TableÂ 2, we see that addition of question consistency slightly improves performance of each VQA model. Inline with observations inÂ [26], this shows that indeed models which can generate questions from the answer have better multi-modal understanding and in turn are better at visual question answering.\nA-consistency implies passing all the generated questions Qâ€²superscriptğ‘„â€²Q^{\\prime} to the VQA model Fğ¹F and an associated loss â„’câ€‹yâ€‹câ€‹lâ€‹eâ€‹(A,Aâ€²)subscriptâ„’ğ‘ğ‘¦ğ‘ğ‘™ğ‘’ğ´superscriptğ´â€²\\mathcal{L}_{cycle}(A,A^{\\prime}).\nAs seen in TableÂ 2, we see that naively passing all the generated questions to the VQA model Fğ¹F leads to significant reduction in performance than the base model Fğ¹F. This goes in line with our earlier discussion that not all questions generated are valid rephrasings of the original question and hence enforcing consistency between the answers of two invalid pairs of questions naturally leads to degradation in performance.\nFinally we show the effect of using our gating mechanism to filter undesirable generated questions in Qâ€²superscriptğ‘„â€²Q^{\\prime} and passing the remaining to VQA model Fğ¹F. We see that all VQA models perform consistently better when using a gating than just using Q-consistency."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Question Generation Performance on VQA v2.0 validation set, * signifies results on a constrained subset as done in [26]. CC represents models trained with our approach.",
        "table": "<table id=\"S5.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S5.T3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th id=\"S5.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S5.T3.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">BLEU-1</span></th>\n<th id=\"S5.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S5.T3.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">BLEU-2</span></th>\n<th id=\"S5.T3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S5.T3.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">BLEU-3</span></th>\n<th id=\"S5.T3.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S5.T3.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">BLEU-4</span></th>\n<th id=\"S5.T3.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S5.T3.1.1.1.6.1\" class=\"ltx_text ltx_font_bold\">ROUGE-L</span></th>\n<th id=\"S5.T3.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S5.T3.1.1.1.7.1\" class=\"ltx_text ltx_font_bold\">METEOR</span></th>\n<th id=\"S5.T3.1.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T3.1.1.1.8.1\" class=\"ltx_text ltx_font_bold\">CIDER</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">iQAN*Â <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a>]</cite>\n</td>\n<td id=\"S5.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.582</td>\n<td id=\"S5.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.467</td>\n<td id=\"S5.T3.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.385</td>\n<td id=\"S5.T3.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.320</td>\n<td id=\"S5.T3.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.617</td>\n<td id=\"S5.T3.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.276</td>\n<td id=\"S5.T3.1.2.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\">2.222</td>\n</tr>\n<tr id=\"S5.T3.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Pythia + CC*</td>\n<td id=\"S5.T3.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.1.3.2.2.1\" class=\"ltx_text ltx_font_bold\">0.708</span></td>\n<td id=\"S5.T3.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.1.3.2.3.1\" class=\"ltx_text ltx_font_bold\">0.561</span></td>\n<td id=\"S5.T3.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.1.3.2.4.1\" class=\"ltx_text ltx_font_bold\">0.438</span></td>\n<td id=\"S5.T3.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.1.3.2.5.1\" class=\"ltx_text ltx_font_bold\">0.339</span></td>\n<td id=\"S5.T3.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.1.3.2.6.1\" class=\"ltx_text ltx_font_bold\">0.627</span></td>\n<td id=\"S5.T3.1.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.1.3.2.7.1\" class=\"ltx_text ltx_font_bold\">0.284</span></td>\n<td id=\"S5.T3.1.3.2.8\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.1.3.2.8.1\" class=\"ltx_text ltx_font_bold\">2.301</span></td>\n</tr>\n<tr id=\"S5.T3.1.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">iVQAÂ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a>]</cite>\n</td>\n<td id=\"S5.T3.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.430</td>\n<td id=\"S5.T3.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.326</td>\n<td id=\"S5.T3.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.256</td>\n<td id=\"S5.T3.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.208</td>\n<td id=\"S5.T3.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.468</td>\n<td id=\"S5.T3.1.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.205</td>\n<td id=\"S5.T3.1.4.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\">1.714</td>\n</tr>\n<tr id=\"S5.T3.1.5.4\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">Pythia + CC</td>\n<td id=\"S5.T3.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T3.1.5.4.2.1\" class=\"ltx_text ltx_font_bold\">0.486</span></td>\n<td id=\"S5.T3.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T3.1.5.4.3.1\" class=\"ltx_text ltx_font_bold\">0.368</span></td>\n<td id=\"S5.T3.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T3.1.5.4.4.1\" class=\"ltx_text ltx_font_bold\">0.287</span></td>\n<td id=\"S5.T3.1.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T3.1.5.4.5.1\" class=\"ltx_text ltx_font_bold\">0.226</span></td>\n<td id=\"S5.T3.1.5.4.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T3.1.5.4.6.1\" class=\"ltx_text ltx_font_bold\">0.556</span></td>\n<td id=\"S5.T3.1.5.4.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T3.1.5.4.7.1\" class=\"ltx_text ltx_font_bold\">0.225</span></td>\n<td id=\"S5.T3.1.5.4.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T3.1.5.4.8.1\" class=\"ltx_text ltx_font_bold\">1.843</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Recall that our model also includes a VQG component which generates questions conditioned on an answer and image. Since the overall performance of our framework relies highly on the performance of question generation module, we evaluate our VQG component performance as well on commonly used image captioning metrics. We compare our VQG component to several answer-conditional VQG models on the VQA v2.0 dataset. We use standard image captioning metrics CIDErÂ [42], BLEUÂ [34], METEORÂ [7] and ROUGE-LÂ [27] as used inÂ [28]. We compare our approach to two recently proposed visual question generation approaches. iVQAÂ [28] uses a variational LSTM model trained with reinforcement learning to generate answer-specific questions for an image. Syntactic correctness, diversity and intent of the generated question are used to allocate rewards.\niQANÂ [26] generates answer-specific questions by modelling question generation as a dual task of question answering and sharing parameters between question answering and question generation modules. Since iQAN can only generate a specific type of questions, for a fair comparison, we compare to iQAN only on a subset of the dataset containing questions from these specific types. As shown in TableÂ 3, we observe that our question generation module trained with cycle-consistency consistently outperforms iVQAÂ [28] and iQANÂ [26] on all metrics. A few qualitative examples of answer conditioned questions generated by our VQG model can be seen in Fig.Â 3(b). Additional examples can also be found in the AppendixÂ D."
        ]
    },
    "S5.T4": {
        "caption": "Table 4:  Failure prediction performance on VQA v2.0 validation dataset. Each row in blocks represents a component added to the previous row. CC represents models trained with our cycle-consistent framework and FP represents models with an additional binary classification Failure Prediction submodule to predict if the predicted answer Aâ€²superscriptğ´â€²A^{\\prime} is correct given a question and image pair (Qğ‘„Q, Iğ¼I). For models trained without the FP module, scores are obtained by thresholding the answer confidences.",
        "table": "<table id=\"S5.T4.7\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.7.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.7.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S5.T4.7.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th id=\"S5.T4.7.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S5.T4.7.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Precision</span></th>\n<th id=\"S5.T4.7.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S5.T4.7.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Recall</span></th>\n<th id=\"S5.T4.7.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T4.7.1.1.4.1\" class=\"ltx_text ltx_font_bold\">F1</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.7.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.7.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">BUTD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">4</a>]</cite>\n</th>\n<td id=\"S5.T4.7.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.71</td>\n<td id=\"S5.T4.7.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.78</td>\n<td id=\"S5.T4.7.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.74</td>\n</tr>\n<tr id=\"S5.T4.7.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.7.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Â Â â€ƒ+ FP</th>\n<td id=\"S5.T4.7.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T4.7.3.2.2.1\" class=\"ltx_text ltx_font_bold\">0.74</span></td>\n<td id=\"S5.T4.7.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T4.7.3.2.3.1\" class=\"ltx_text ltx_font_bold\">0.85</span></td>\n<td id=\"S5.T4.7.3.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.7.3.2.4.1\" class=\"ltx_text ltx_font_bold\">0.79</span></td>\n</tr>\n<tr id=\"S5.T4.7.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T4.7.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">BUTD + CC</th>\n<td id=\"S5.T4.7.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.73</td>\n<td id=\"S5.T4.7.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.79</td>\n<td id=\"S5.T4.7.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.76</td>\n</tr>\n<tr id=\"S5.T4.7.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T4.7.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Â Â â€ƒâ€ƒ+ FP</th>\n<td id=\"S5.T4.7.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T4.7.5.4.2.1\" class=\"ltx_text ltx_font_bold\">0.78</span></td>\n<td id=\"S5.T4.7.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T4.7.5.4.3.1\" class=\"ltx_text ltx_font_bold\">0.83</span></td>\n<td id=\"S5.T4.7.5.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.7.5.4.4.1\" class=\"ltx_text ltx_font_bold\">0.80</span></td>\n</tr>\n<tr id=\"S5.T4.7.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T4.7.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Pythia <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">46</a>]</cite>\n</th>\n<td id=\"S5.T4.7.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.74</td>\n<td id=\"S5.T4.7.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.79</td>\n<td id=\"S5.T4.7.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.76</td>\n</tr>\n<tr id=\"S5.T4.7.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T4.7.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Â Â â€ƒ+ FP</th>\n<td id=\"S5.T4.7.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T4.7.7.6.2.1\" class=\"ltx_text ltx_font_bold\">0.76</span></td>\n<td id=\"S5.T4.7.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T4.7.7.6.3.1\" class=\"ltx_text ltx_font_bold\">0.88</span></td>\n<td id=\"S5.T4.7.7.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.7.7.6.4.1\" class=\"ltx_text ltx_font_bold\">0.82</span></td>\n</tr>\n<tr id=\"S5.T4.7.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T4.7.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Pythia + CC</th>\n<td id=\"S5.T4.7.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.77</td>\n<td id=\"S5.T4.7.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.81</td>\n<td id=\"S5.T4.7.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.77</td>\n</tr>\n<tr id=\"S5.T4.7.9.8\" class=\"ltx_tr\">\n<th id=\"S5.T4.7.9.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Â Â â€ƒâ€ƒ+ FP</th>\n<td id=\"S5.T4.7.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T4.7.9.8.2.1\" class=\"ltx_text ltx_font_bold\">0.82</span></td>\n<td id=\"S5.T4.7.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T4.7.9.8.3.1\" class=\"ltx_text ltx_font_bold\">0.84</span></td>\n<td id=\"S5.T4.7.9.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T4.7.9.8.4.1\" class=\"ltx_text ltx_font_bold\">0.83</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "In previous results, we show that by training models to generate and answer questions while being consistent across both tasks leads to improvement in performance and robustness. Another way of testing robustness of these models is to see if models can predict their own failures.\nA robust model is less confident about an incorrect answer and vice versa.\nMotivated by this, we seek to verify if models trained with our cycle-consistent framework can identify their own failures i.e. correctly identify if theyâ€™re wrong about a prediction.\nTo this end, we use two failure predictions schemes. First, we naively threshold the confidence of the predicted answer. All answers above a particular threshold are marked as correctly answered and vice versa. Second, we design a failure prediction binary classification module (FP), which predicts for a given image Iğ¼I, question Qğ‘„Q and answer Aâ€²superscriptğ´â€²A^{\\prime} (predicted by the base VQA model Fğ¹F), whether the predicted answer is correct for the given (I,Q)ğ¼ğ‘„(I,Q) pair. The FP module uses image and answer encoders similar to those used in the question generation module (SectionÂ 3.1) and makes use of the question representation from the base VQA model as the question encoding. These encodings are concatenated and passed to a linear layer for binary classification. The FP module is trained keeping the parameters of the base VQA model frozen.\nIn TableÂ 4, we show the failure prediction performance of the baseline VQA models and models trained with our proposed framework.\nIt shows that the cycle consistency framework, even without an explicit failure predictor module, makes the models more calibrated â€“ more capable of detecting their own failures.\nIn both settings: (a) when using naive confidence thresholding (not marked as â€œ+ FPâ€ in the Table) and (b) using a specifically designed submodule to detect failures (marked as â€œ+ FPâ€), models trained with our cycle-consistent training framework are better than their corresponding baselines.\nWe see similar improvments in detecting failures for both BUTD and Pythia models, which shows that our cycle-consistency framework is model agnostic.\nThis also shows that not only does cycle-consistent training make models robust to linguistic variations, but also allows them to be aware of their failures."
        ]
    }
}