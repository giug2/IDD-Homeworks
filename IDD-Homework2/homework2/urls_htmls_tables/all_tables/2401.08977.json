{
    "PAPER'S NUMBER OF TABLES": 8,
    "S3.T1": {
        "caption": "Table 1: Test accuracies of our and SOTA methods on CIFAR-10/100-LT with diverse imbalanced and heterogeneous data settings. GM/PM denotes Global/Personalized model.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nDataset\nNon-IID\nŒ±=0.5ùõº0.5\\alpha=0.5\nŒ±=1ùõº1\\alpha=1\n\nImbalance Factor\nIF=50\nIF=100\nIF=50\nIF=100\n\nMethod/Model\nGM\nPM\nGM\nPM\nGM\nPM\nGM\nPM\n\nCIFAR-10-LT\nFedAvg\n0.7988\n0.8722\n0.7214\n0.8664\n0.7890\n0.8916\n0.7231\n0.8935\n\nFedProx\n0.7869\n0.8653\n0.7160\n0.8617\n0.7790\n0.8888\n0.7266\n0.8897\n\nFedBN\n0.7604\n0.8837\n0.6984\n0.8847\n0.7596\n0.8916\n0.7033\n0.8925\n\nFedPer\n-\n0.8935\n-\n0.8931\n-\n0.8918\n-\n0.8999\n\nFedRep\n0.7938\n0.8988\n0.7218\n0.8984\n0.7816\n0.9043\n0.7271\n0.9065\n\nDitto\n0.7813\n0.8926\n0.7180\n0.8874\n0.7804\n0.8967\n0.7210\n0.8973\n\n\\cdashline2-10[4pt/3pt]\nFedROD\n0.7862\n0.9030\n0.7137\n0.8941\n0.7776\n0.9025\n0.7236\n0.9041\n\n\nFedBABU\n0.7851\n0.8664\n0.7280\n0.8613\n0.7819\n0.8914\n0.7316\n0.8898\n\n\nFedETF\n0.8056\n0.7446\n0.6709\n0.7323\n0.7453\n0.8106\n0.6615\n0.7917\n\n\nRatio Loss\n0.7995\n0.8791\n0.7290\n0.8724\n0.7857\n0.8938\n0.7342\n0.8934\n\n\nFedLoGe\n0.82770.8277\\bm{0.8277}\n0.91120.9112\\bm{0.9112}\n0.76720.7672\\bm{0.7672}\n0.90960.9096\\bm{0.9096}\n0.81890.8189\\bm{0.8189}\n0.91040.9104\\bm{0.9104}\n0.75930.7593\\bm{0.7593}\n0.90730.9073\\bm{0.9073}\n\nCIFAR-100-LT\nFedAvg\n0.4271\n0.6019\n0.3818\n0.6214\n0.4215\n0.6086\n0.3797\n0.6269\n\nFedProx\n0.4276\n0.6071\n0.3856\n0.6214\n0.4187\n0.6089\n0.3788\n0.6261\n\nFedBN\n0.4332\n0.6657\n0.3895\n0.6703\n0.4229\n0.6373\n0.3847\n0.6562\n\nFedPer\n-\n0.6888\n-\n0.7007\n-\n0.6476\n-\n0.6737\n\nFedRep\n0.4355\n0.6911\n0.3907\n0.6917\n0.4283\n0.6539\n0.3828\n0.6831\n\nDitto\n0.4312\n0.6441\n0.3847\n0.6588\n0.4188\n0.6182\n0.3822\n0.6320\n\n\\cdashline2-10[4pt/3pt]\nFedROD\n0.4384\n0.7124\n0.3919\n0.6919\n0.4289\n0.6667\n0.3902\n0.6917\n\n\nFedBABU\n0.4415\n0.6416\n0.3921\n0.6480\n0.4336\n0.6354\n0.3907\n0.6598\n\n\nFedETF\n0.4223\n0.6055\n0.3825\n0.6421\n0.4278\n0.6302\n0.4278\n0.6507\n\n\nRatio Loss\n0.4326\n0.6152\n0.3912\n0.6348\n0.4253\n0.6213\n0.3839\n0.6344\n\n\nFedLoGe\n0.47620.4762\\bm{0.4762}\n0.72290.7229\\bm{0.7229}\n0.42330.4233\\bm{0.4233}\n0.72850.7285\\bm{0.7285}\n0.48600.4860\\bm{0.4860}\n0.70990.7099\\bm{0.7099}\n0.43300.4330\\bm{0.4330}\n0.71950.7195\\bm{0.7195}\n\n\n",
        "references": [
            [
                "Overall, our framework ",
                "Fed-LoGe",
                " consists of three critical stages: representation learning with SSE-C, global feature realignment, and local feature realignment, for the training of the shared backbone ",
                "Œ∏",
                "ùúÉ",
                "\\theta",
                ", global auxiliary classifier ",
                "œà",
                "ùúì",
                "\\mathbf{\\psi}",
                ", and ",
                "K",
                "ùêæ",
                "K",
                " local classifiers (",
                "{",
                "œï",
                "k",
                "}",
                "k",
                "=",
                "1",
                "K",
                "superscript",
                "subscript",
                "subscript",
                "italic-œï",
                "ùëò",
                "ùëò",
                "1",
                "ùêæ",
                "\\{\\mathbf{\\phi}_{k}\\}_{k=1}^{K}",
                "), respectively.",
                "In the first stage, the server first constructs the SSE-C with Eq.¬†",
                "5",
                " before the training and then distributes it to all clients. Upon receiving SSE-C, each client fixes it as the classifier to train the backbone ",
                "Œ∏",
                "ùúÉ",
                "\\theta",
                ", global classifier ",
                "œà",
                "ùúì",
                "\\mathbf{\\psi}",
                ", and local classifier ",
                "œï",
                "k",
                "subscript",
                "italic-œï",
                "ùëò",
                "\\mathbf{\\phi}_{k}",
                " alternately. Specifically, we update ",
                "Œ∏",
                "ùúÉ",
                "\\theta",
                " with fixed ",
                "œà",
                "SSE-C",
                "subscript",
                "ùúì",
                "SSE-C",
                "\\mathbf{\\psi}_{\\text{SSE-C}}",
                ". Subsequently, the ",
                "Œ∏",
                "ùúÉ",
                "\\theta",
                " is frozen to update the global head ",
                "œà",
                "ùúì",
                "\\mathbf{\\psi}",
                " and each local classifier ",
                "œï",
                "k",
                "subscript",
                "italic-œï",
                "ùëò",
                "\\mathbf{\\phi}_{k}",
                ". At the end of each round, the ",
                "Œ∏",
                "ùúÉ",
                "\\theta",
                " and ",
                "œà",
                "ùúì",
                "\\mathbf{\\psi}",
                " are aggregated at the server, while ",
                "œï",
                "k",
                "subscript",
                "italic-œï",
                "ùëò",
                "\\mathbf{\\phi}_{k}",
                " is retained locally.",
                "Global adaptive feature realignment (GA-FR) is performed in the second stage, where each class vector is redistributed by the server according to its individual norms, as outlined by Eq.¬†",
                "6",
                ". Subsequently, in the third phase, personalized adaptive feature realignment (LA-FR) for the class vectors of the global auxiliary head ",
                "œà",
                "ùúì",
                "\\mathbf{\\psi}",
                " is performed. Following LA-FR, local finetuning could be further conducted to boost the model performance. A summary of ",
                "Fed-LoGe",
                " is given in Algorithm ",
                "1",
                "."
            ]
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Test accuracies of our and SOTA methods on ImageNet-LT and iNaturalist-160k with diverse heterogeneous data settings. ",
        "table": "",
        "footnotes": "\n\n\n\n\n\nDataset\nImageNet\nInaturalist\n\nMethod/Model\nMany\nMed\nFew\nGM\nPM\nMany\nMed\nFew\nGM\nPM\n\nFedAvg\n0.481\n0.307\n0.159\n0.329\n0.528\n0.591\n0.418\n0.238\n0.425\n0.590\n\nFedProx\n0.493\n0.318\n0.180\n0.343\n0.500\n0.525\n0.484\n0.223\n0.432\n0.596\n\nFedBN\n0.471\n0.300\n0.168\n0.319\n0.504\n0.573\n0.396\n0.221\n0.413\n0.563\n\nFedPer\n-\n-\n-\n\n0.653\n-\n-\n-\n-\n0.638\n\nFedRep\n0.460\n0.309\n0.187\n0.330\n0.574\n0.571\n0.453\n0.237\n0.429\n0.627\n\n\\cdashline1-11[4pt/3pt]\nDitto\n0.492\n0.319\n0.176\n0.342\n0.674\n0.5980.598\\bm{0.598}\n0.452\n0.243\n0.437\n0.584\n\nFedROD\n0.483\n0.305\n0.165\n0.331\n0.7033\n0.585\n0.416\n0.243\n0.421\n0.699\n\nFedBABU\n0.443\n0.240\n0.055\n0.230\n0.425\n0.561\n0.401\n0.199\n0.377\n0.696\n\nFedETF\n0.425\n0.239\n0.05\n0.222\n0.418\n0.587\n0.431\n0.245\n0.437\n0.713\n\nRatio Loss\n0.4950.495\\bm{0.495}\n0.337\n0.189\n0.351\n0.521\n0.587\n0.454\n0.290\n0.452\n0.589\n\nFedLoGe\n0.430\n0.3730.373\\bm{0.373}\n0.2850.285\\bm{0.285}\n0.3560.356\\bm{0.356}\n0.7260.726\\bm{0.726}\n0.519\n0.5080.508\\bm{0.508}\n0.4730.473\\bm{0.473}\n0.5030.503\\bm{0.503}\n0.7590.759\\bm{0.759}\n\n\n",
        "references": [
            [
                "Dataset, Models and Metrics:",
                "We consider image classification tasks for performance evaluation on benchmark long-tailed datasets: CIFAR-10/100-LT, ImageNet-LT, and iNaturalist-User-160k ",
                "(Van¬†Horn et¬†al., ",
                "2018",
                ")",
                ". The CIFAR-10/100-LT datasets are sampled into a long-tailed distribution employing an exponential distribution governed by the Imbalance Factor (",
                "IF",
                ") in ",
                "Cao et¬†al. (",
                "2019",
                ")",
                ".\nAll experiments are conducted with non-IID data partitions, implemented by the Dirichlet distributions-based approach with parameter ",
                "Œ±",
                "ùõº",
                "\\alpha",
                " to control the non-IIDness ",
                "(Chen & Chao, ",
                "2022",
                ")",
                ".ResNet-18 is trained over ",
                "K",
                "=",
                "40",
                "ùêæ",
                "40",
                "K=40",
                " clients on CIFAR-10-LT, while ResNet-34 and ResNet-50 are implemented on CIFAR-100-LT and ImageNet-LT, respectively, with ",
                "K",
                "=",
                "20",
                "ùêæ",
                "20",
                "K=20",
                " clients. The configurations for iNaturalist-160k align with those utilized for ImageNet-LT.\nWe use ",
                "Œ±",
                "=",
                "1",
                ",",
                "0.5",
                "ùõº",
                "1",
                "0.5",
                "\\alpha=1,0.5",
                " and ",
                "IF",
                " = ",
                "50",
                ",",
                "100",
                "50",
                "100",
                "50,100",
                " in CIFAR-10/100-LT. We use ",
                "Œ±",
                "=",
                "0.1",
                ",",
                "0.5",
                "ùõº",
                "0.1",
                "0.5",
                "\\alpha=0.1,0.5",
                " in ImageNet-LT and iNaturalist, respectively.",
                "A global balanced dataset is used for the calculation of test accuracy to evaluate global model (GM) performance. We also report the accuracy across many, medium, and few classes. The detailed categorization for many/med/few classes can be found in the Appendix.",
                "For personalized model (PM) evaluation, we use local test accuracy, and the local test set is sampled from the global test set.\nEach local test set has an identical distribution to the local training set. The accuracy of the PM is the arithmetic mean of local test accuracy across all clients.",
                "Compared Methods:",
                "\nIn addition to ",
                "FedAvg",
                " and ",
                "FedProx",
                "¬†",
                "(Li et¬†al., ",
                "2020a",
                ")",
                " which are included for reference, we consider two types of state-of-the-art baselines:\n(1) pFL methods, including ",
                "FedBN",
                "¬†",
                "(Li et¬†al., ",
                "2021b",
                ")",
                ", ",
                "FedPer",
                "¬†",
                "(Arivazhagan et¬†al., ",
                "2019",
                ")",
                ", ",
                "FedRep",
                "¬†",
                "(Collins et¬†al., ",
                "2021",
                ")",
                ", ",
                "Ditto",
                "¬†",
                "(Li et¬†al., ",
                "2021a",
                ")",
                ", ",
                "FedROD",
                "¬†",
                "(Chen & Chao, ",
                "2022",
                ")",
                ", have been applied during local training of each client.\n",
                "FedBN",
                " and ",
                "FedRep",
                " incorporate all weights during aggregation, but exclude the batch normalization layers/classifiers during assignment, enabling the observation of the global model‚Äôs performance.\n(2) Federated (Long-tailed) Representation learning, including ",
                "FedBABU",
                "¬†",
                "(Oh et¬†al., ",
                "2022",
                ")",
                ", ",
                "FedETF",
                "¬†",
                "(Li et¬†al., ",
                "2023",
                ")",
                " and ",
                "Ratio Loss",
                "¬†",
                "(Wang et¬†al., ",
                "2021",
                ")",
                "."
            ]
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Ablations of SSE-C and GLA-FR.",
        "table": "<table id=\"S4.T3.15.15\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.15.15.16.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.15.15.16.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">ETF</th>\n<th id=\"S4.T3.15.15.16.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">SSE-C</th>\n<th id=\"S4.T3.15.15.16.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GA-FR</th>\n<th id=\"S4.T3.15.15.16.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">LA-FR</th>\n<th id=\"S4.T3.15.15.16.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">GM</th>\n<th id=\"S4.T3.15.15.16.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">PM</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S4.T3.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S4.T3.1.1.1.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S4.T3.1.1.1.1.m1.1.1\" xref=\"S4.T3.1.1.1.1.m1.1.1.cmml\">‚úì</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.1.1.1.1.m1.1b\"><ci id=\"S4.T3.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.1.1.1.1.m1.1.1\">‚úì</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.1.1.1.1.m1.1c\">\\checkmark</annotation></semantics></math></td>\n<td id=\"S4.T3.1.1.1.2\" class=\"ltx_td ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T3.1.1.1.3\" class=\"ltx_td ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T3.1.1.1.4\" class=\"ltx_td ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T3.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.3825</td>\n<td id=\"S4.T3.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.6421</td>\n</tr>\n<tr id=\"S4.T3.2.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.2.2.2.2\" class=\"ltx_td\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T3.2.2.2.1\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S4.T3.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S4.T3.2.2.2.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S4.T3.2.2.2.1.m1.1.1\" xref=\"S4.T3.2.2.2.1.m1.1.1.cmml\">‚úì</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.2.2.2.1.m1.1b\"><ci id=\"S4.T3.2.2.2.1.m1.1.1.cmml\" xref=\"S4.T3.2.2.2.1.m1.1.1\">‚úì</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.2.2.2.1.m1.1c\">\\checkmark</annotation></semantics></math></td>\n<td id=\"S4.T3.2.2.2.3\" class=\"ltx_td\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T3.2.2.2.4\" class=\"ltx_td\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T3.2.2.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.4175</td>\n<td id=\"S4.T3.2.2.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.6865</td>\n</tr>\n<tr id=\"S4.T3.5.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.5.5.5.4\" class=\"ltx_td\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T3.3.3.3.1\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S4.T3.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S4.T3.3.3.3.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S4.T3.3.3.3.1.m1.1.1\" xref=\"S4.T3.3.3.3.1.m1.1.1.cmml\">‚úì</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.3.3.3.1.m1.1b\"><ci id=\"S4.T3.3.3.3.1.m1.1.1.cmml\" xref=\"S4.T3.3.3.3.1.m1.1.1\">‚úì</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.3.3.3.1.m1.1c\">\\checkmark</annotation></semantics></math></td>\n<td id=\"S4.T3.4.4.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S4.T3.4.4.4.2.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S4.T3.4.4.4.2.m1.1a\"><mi mathvariant=\"normal\" id=\"S4.T3.4.4.4.2.m1.1.1\" xref=\"S4.T3.4.4.4.2.m1.1.1.cmml\">‚úì</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.4.4.4.2.m1.1b\"><ci id=\"S4.T3.4.4.4.2.m1.1.1.cmml\" xref=\"S4.T3.4.4.4.2.m1.1.1\">‚úì</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.4.4.4.2.m1.1c\">\\checkmark</annotation></semantics></math></td>\n<td id=\"S4.T3.5.5.5.5\" class=\"ltx_td\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T3.5.5.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S4.T3.5.5.5.3.m1.1\" class=\"ltx_Math\" alttext=\"\\bm{0.4350}\" display=\"inline\"><semantics id=\"S4.T3.5.5.5.3.m1.1a\"><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\" id=\"S4.T3.5.5.5.3.m1.1.1\" xref=\"S4.T3.5.5.5.3.m1.1.1.cmml\">0.4350</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.5.5.5.3.m1.1b\"><cn type=\"float\" id=\"S4.T3.5.5.5.3.m1.1.1.cmml\" xref=\"S4.T3.5.5.5.3.m1.1.1\">0.4350</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.5.5.5.3.m1.1c\">\\bm{0.4350}</annotation></semantics></math></td>\n<td id=\"S4.T3.5.5.5.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.6868</td>\n</tr>\n<tr id=\"S4.T3.7.7.7\" class=\"ltx_tr\">\n<td id=\"S4.T3.7.7.7.3\" class=\"ltx_td\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T3.6.6.6.1\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S4.T3.6.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S4.T3.6.6.6.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S4.T3.6.6.6.1.m1.1.1\" xref=\"S4.T3.6.6.6.1.m1.1.1.cmml\">‚úì</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.6.6.6.1.m1.1b\"><ci id=\"S4.T3.6.6.6.1.m1.1.1.cmml\" xref=\"S4.T3.6.6.6.1.m1.1.1\">‚úì</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.6.6.6.1.m1.1c\">\\checkmark</annotation></semantics></math></td>\n<td id=\"S4.T3.7.7.7.4\" class=\"ltx_td\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T3.7.7.7.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S4.T3.7.7.7.2.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S4.T3.7.7.7.2.m1.1a\"><mi mathvariant=\"normal\" id=\"S4.T3.7.7.7.2.m1.1.1\" xref=\"S4.T3.7.7.7.2.m1.1.1.cmml\">‚úì</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.7.7.7.2.m1.1b\"><ci id=\"S4.T3.7.7.7.2.m1.1.1.cmml\" xref=\"S4.T3.7.7.7.2.m1.1.1\">‚úì</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.7.7.7.2.m1.1c\">\\checkmark</annotation></semantics></math></td>\n<td id=\"S4.T3.7.7.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.4175</td>\n<td id=\"S4.T3.7.7.7.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.7339</td>\n</tr>\n<tr id=\"S4.T3.10.10.10\" class=\"ltx_tr\">\n<td id=\"S4.T3.8.8.8.1\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S4.T3.8.8.8.1.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S4.T3.8.8.8.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S4.T3.8.8.8.1.m1.1.1\" xref=\"S4.T3.8.8.8.1.m1.1.1.cmml\">‚úì</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.8.8.8.1.m1.1b\"><ci id=\"S4.T3.8.8.8.1.m1.1.1.cmml\" xref=\"S4.T3.8.8.8.1.m1.1.1\">‚úì</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.8.8.8.1.m1.1c\">\\checkmark</annotation></semantics></math></td>\n<td id=\"S4.T3.10.10.10.4\" class=\"ltx_td\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T3.9.9.9.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S4.T3.9.9.9.2.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S4.T3.9.9.9.2.m1.1a\"><mi mathvariant=\"normal\" id=\"S4.T3.9.9.9.2.m1.1.1\" xref=\"S4.T3.9.9.9.2.m1.1.1.cmml\">‚úì</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.9.9.9.2.m1.1b\"><ci id=\"S4.T3.9.9.9.2.m1.1.1.cmml\" xref=\"S4.T3.9.9.9.2.m1.1.1\">‚úì</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.9.9.9.2.m1.1c\">\\checkmark</annotation></semantics></math></td>\n<td id=\"S4.T3.10.10.10.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S4.T3.10.10.10.3.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S4.T3.10.10.10.3.m1.1a\"><mi mathvariant=\"normal\" id=\"S4.T3.10.10.10.3.m1.1.1\" xref=\"S4.T3.10.10.10.3.m1.1.1.cmml\">‚úì</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.10.10.10.3.m1.1b\"><ci id=\"S4.T3.10.10.10.3.m1.1.1.cmml\" xref=\"S4.T3.10.10.10.3.m1.1.1\">‚úì</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.10.10.10.3.m1.1c\">\\checkmark</annotation></semantics></math></td>\n<td id=\"S4.T3.10.10.10.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.3988</td>\n<td id=\"S4.T3.10.10.10.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.7043</td>\n</tr>\n<tr id=\"S4.T3.15.15.15\" class=\"ltx_tr\">\n<td id=\"S4.T3.15.15.15.6\" class=\"ltx_td ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T3.11.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S4.T3.11.11.11.1.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S4.T3.11.11.11.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S4.T3.11.11.11.1.m1.1.1\" xref=\"S4.T3.11.11.11.1.m1.1.1.cmml\">‚úì</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.11.11.11.1.m1.1b\"><ci id=\"S4.T3.11.11.11.1.m1.1.1.cmml\" xref=\"S4.T3.11.11.11.1.m1.1.1\">‚úì</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.11.11.11.1.m1.1c\">\\checkmark</annotation></semantics></math></td>\n<td id=\"S4.T3.12.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S4.T3.12.12.12.2.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S4.T3.12.12.12.2.m1.1a\"><mi mathvariant=\"normal\" id=\"S4.T3.12.12.12.2.m1.1.1\" xref=\"S4.T3.12.12.12.2.m1.1.1.cmml\">‚úì</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.12.12.12.2.m1.1b\"><ci id=\"S4.T3.12.12.12.2.m1.1.1.cmml\" xref=\"S4.T3.12.12.12.2.m1.1.1\">‚úì</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.12.12.12.2.m1.1c\">\\checkmark</annotation></semantics></math></td>\n<td id=\"S4.T3.13.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S4.T3.13.13.13.3.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S4.T3.13.13.13.3.m1.1a\"><mi mathvariant=\"normal\" id=\"S4.T3.13.13.13.3.m1.1.1\" xref=\"S4.T3.13.13.13.3.m1.1.1.cmml\">‚úì</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.13.13.13.3.m1.1b\"><ci id=\"S4.T3.13.13.13.3.m1.1.1.cmml\" xref=\"S4.T3.13.13.13.3.m1.1.1\">‚úì</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.13.13.13.3.m1.1c\">\\checkmark</annotation></semantics></math></td>\n<td id=\"S4.T3.14.14.14.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S4.T3.14.14.14.4.m1.1\" class=\"ltx_Math\" alttext=\"\\bm{0.4350}\" display=\"inline\"><semantics id=\"S4.T3.14.14.14.4.m1.1a\"><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\" id=\"S4.T3.14.14.14.4.m1.1.1\" xref=\"S4.T3.14.14.14.4.m1.1.1.cmml\">0.4350</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.14.14.14.4.m1.1b\"><cn type=\"float\" id=\"S4.T3.14.14.14.4.m1.1.1.cmml\" xref=\"S4.T3.14.14.14.4.m1.1.1\">0.4350</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.14.14.14.4.m1.1c\">\\bm{0.4350}</annotation></semantics></math></td>\n<td id=\"S4.T3.15.15.15.5\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S4.T3.15.15.15.5.m1.1\" class=\"ltx_Math\" alttext=\"\\bm{0.7343}\" display=\"inline\"><semantics id=\"S4.T3.15.15.15.5.m1.1a\"><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\" id=\"S4.T3.15.15.15.5.m1.1.1\" xref=\"S4.T3.15.15.15.5.m1.1.1.cmml\">0.7343</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.15.15.15.5.m1.1b\"><cn type=\"float\" id=\"S4.T3.15.15.15.5.m1.1.1.cmml\" xref=\"S4.T3.15.15.15.5.m1.1.1\">0.7343</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.15.15.15.5.m1.1c\">\\bm{0.7343}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Dataset, Models and Metrics:",
                "We consider image classification tasks for performance evaluation on benchmark long-tailed datasets: CIFAR-10/100-LT, ImageNet-LT, and iNaturalist-User-160k ",
                "(Van¬†Horn et¬†al., ",
                "2018",
                ")",
                ". The CIFAR-10/100-LT datasets are sampled into a long-tailed distribution employing an exponential distribution governed by the Imbalance Factor (",
                "IF",
                ") in ",
                "Cao et¬†al. (",
                "2019",
                ")",
                ".\nAll experiments are conducted with non-IID data partitions, implemented by the Dirichlet distributions-based approach with parameter ",
                "Œ±",
                "ùõº",
                "\\alpha",
                " to control the non-IIDness ",
                "(Chen & Chao, ",
                "2022",
                ")",
                ".ResNet-18 is trained over ",
                "K",
                "=",
                "40",
                "ùêæ",
                "40",
                "K=40",
                " clients on CIFAR-10-LT, while ResNet-34 and ResNet-50 are implemented on CIFAR-100-LT and ImageNet-LT, respectively, with ",
                "K",
                "=",
                "20",
                "ùêæ",
                "20",
                "K=20",
                " clients. The configurations for iNaturalist-160k align with those utilized for ImageNet-LT.\nWe use ",
                "Œ±",
                "=",
                "1",
                ",",
                "0.5",
                "ùõº",
                "1",
                "0.5",
                "\\alpha=1,0.5",
                " and ",
                "IF",
                " = ",
                "50",
                ",",
                "100",
                "50",
                "100",
                "50,100",
                " in CIFAR-10/100-LT. We use ",
                "Œ±",
                "=",
                "0.1",
                ",",
                "0.5",
                "ùõº",
                "0.1",
                "0.5",
                "\\alpha=0.1,0.5",
                " in ImageNet-LT and iNaturalist, respectively.",
                "A global balanced dataset is used for the calculation of test accuracy to evaluate global model (GM) performance. We also report the accuracy across many, medium, and few classes. The detailed categorization for many/med/few classes can be found in the Appendix.",
                "For personalized model (PM) evaluation, we use local test accuracy, and the local test set is sampled from the global test set.\nEach local test set has an identical distribution to the local training set. The accuracy of the PM is the arithmetic mean of local test accuracy across all clients.",
                "Compared Methods:",
                "\nIn addition to ",
                "FedAvg",
                " and ",
                "FedProx",
                "¬†",
                "(Li et¬†al., ",
                "2020a",
                ")",
                " which are included for reference, we consider two types of state-of-the-art baselines:\n(1) pFL methods, including ",
                "FedBN",
                "¬†",
                "(Li et¬†al., ",
                "2021b",
                ")",
                ", ",
                "FedPer",
                "¬†",
                "(Arivazhagan et¬†al., ",
                "2019",
                ")",
                ", ",
                "FedRep",
                "¬†",
                "(Collins et¬†al., ",
                "2021",
                ")",
                ", ",
                "Ditto",
                "¬†",
                "(Li et¬†al., ",
                "2021a",
                ")",
                ", ",
                "FedROD",
                "¬†",
                "(Chen & Chao, ",
                "2022",
                ")",
                ", have been applied during local training of each client.\n",
                "FedBN",
                " and ",
                "FedRep",
                " incorporate all weights during aggregation, but exclude the batch normalization layers/classifiers during assignment, enabling the observation of the global model‚Äôs performance.\n(2) Federated (Long-tailed) Representation learning, including ",
                "FedBABU",
                "¬†",
                "(Oh et¬†al., ",
                "2022",
                ")",
                ", ",
                "FedETF",
                "¬†",
                "(Li et¬†al., ",
                "2023",
                ")",
                " and ",
                "Ratio Loss",
                "¬†",
                "(Wang et¬†al., ",
                "2021",
                ")",
                "."
            ]
        ]
    },
    "A1.T4": {
        "caption": "Table 4: Computational cost of Fed-LoGe.",
        "table": "<table id=\"A1.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T4.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Method</th>\n<th id=\"A1.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Time Cost/Round</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T4.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T4.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedAvg</th>\n<td id=\"A1.T4.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">3min11s</td>\n</tr>\n<tr id=\"A1.T4.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T4.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedProx</th>\n<td id=\"A1.T4.1.3.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">5min24s</td>\n</tr>\n<tr id=\"A1.T4.1.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T4.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedRep</th>\n<td id=\"A1.T4.1.4.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">4min07s</td>\n</tr>\n<tr id=\"A1.T4.1.5.4\" class=\"ltx_tr\">\n<th id=\"A1.T4.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedBABU</th>\n<td id=\"A1.T4.1.5.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">3min14s</td>\n</tr>\n<tr id=\"A1.T4.1.6.5\" class=\"ltx_tr\">\n<th id=\"A1.T4.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedLoGe</th>\n<td id=\"A1.T4.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">3min19s</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We evaluate the computational expense of ",
                "Fed-LoGe",
                " across its three stages. In the initial representation stage, the server is tasked with constructing the SSE-C. Setting the learning rate at 0.0001 and executing 10,000 optimization steps, we conducted the experiments three times on the PyTorch platform utilizing the NVIDIA GeForce RTX 3090; the average cost is 11 minutes and 52 seconds."
            ]
        ]
    },
    "A1.T5": {
        "caption": "Table 5: Construct measures of SSE-C ",
        "table": "",
        "footnotes": "\n\n\n\n\n\nSparse Ratio\nNorm\nMean of Norm\nVar of Norm\nMean of Angle\nVar of Angle\n\n\n\n0.6\n0.5\n0.50\n3.55e-12\n90.06\n0.09\n\n0.6\n1.0\n1.00\n4.75e-11\n90.06\n0.66\n\n0.6\n1.5\n1.50\n1.71e-11\n90.04\n1.21\n\n0.4\n0.5\n0.50\n1.10e-11\n90.06\n0.03\n\n0.4\n1.0\n1.00\n7.14e-12\n90.06\n0.15\n\n0.4\n1.5\n1.50\n3.5e-11\n90.06\n0.79\n\n\n",
        "references": [
            "We have individually computed the statistical measures (mean of norm, variance of norm, mean of mutual angles, and variance of mutual angles) of the classifier vectors within SSE-C under varying sparse ratios Œ≤ùõΩ\\beta and assigned norms Œ≥ùõæ\\gamma, as shown in Table 5. It is observable that the constructed SSE-C nearly fulfills the properties of an Equiangular Tight Frame (ETF): equal norms, and maximized, equal mutual angles. Different sparse ratios and assigned norms do not significantly influence the final construction."
        ]
    },
    "A1.T6": {
        "caption": "Table 6: Local adaptive feature alignment of real distribution and personalized norm",
        "table": "",
        "footnotes": "\n\n\n\n\n\nModel\nPM(Avg)\nPM(Macro)\nPM(Weighted)\n\n\n\nLA-LR (real distribution)\n0.6200\n0.2680\n0.5604\n\nLA-LR (norm)\n0.7273\n0.5316\n0.7129\n\n\n",
        "references": [
            [
                "Upon the backbone ",
                "Œ∏",
                "ùúÉ",
                "\\theta",
                ", trained through ",
                "œà",
                "S",
                "‚Äã",
                "S",
                "‚Äã",
                "E",
                "‚àí",
                "C",
                "subscript",
                "ùúì",
                "ùëÜ",
                "ùëÜ",
                "ùê∏",
                "ùê∂",
                "\\mathbf{\\psi}_{SSE-C}",
                ", we proceed with local adaptive feature realignment. In our methodology, local norm is employed to align features with personalized preference. It is intuitive to consider that the real local distribution is more representative of the model‚Äôs preference, and moreover, the local distribution is free for utilization without any computation. Consequently, we explore whether local norm or local distribution yields superior performance.",
                "On CIFAR100-LT with ",
                "Œ±",
                "=",
                "0.5",
                "ùõº",
                "0.5",
                "\\alpha=0.5",
                " and ",
                "IF",
                "=",
                "100",
                "IF",
                "100",
                "\\text{IF}=100",
                ", we realign personalized models employing both norm and distribution, calculating the arithmetic mean, macro average, and weighted average of all clients‚Äô performance. The results can be seen in Tab. ¬†",
                "6",
                ". We discern that utilizing norm for realignment is more effective than employing the real distribution. One rationale is that the norm more aptly signifies the model‚Äôs cognitive capacity over the dataset. For instance, although class A encompasses more samples than class B, B is more prone to misprediction owing to its high resemblance with other classes. In this scenario, realignment ought to adhere to model cognition, fortifying class A over B."
            ]
        ]
    },
    "A1.T7": {
        "caption": "Table 7: Ablations of projection layer and dot-regression loss.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nMethod\nMany\nMed\nFew\nAll\n\n\n\nETF\n0.6904\n0.4751\n0.1820\n0.3825\n\nETF+DR Loss\n0.6882\n0.4607\n0.2059\n0.3932\n\nETF+Proj+DR Loss\n0.6909\n0.5011\n0.2310\n0.4109\n\nETF+Proj+DR Loss+Sparse\n0.7013\n0.5142\n0.2479\n0.4237\n\nFedLoGe\n0.7137\n0.4989\n0.2179\n0.4249\n\nFedLoGe+Proj\n0.7014\n0.4642\n0.1887\n0.4004\n\n\n",
        "references": [
            [
                "In previous work, both the projection layer and dot-regression loss are deemed essential for training the backbone (",
                "Li et¬†al. (",
                "2023",
                "); Yang et¬†al. (",
                "2023b",
                ")",
                "). The projection layer, being a dense structure, necessitates substantial computational cost. However, owing to the advanced design of SSE-C, Fed-LoGe does not depend on the projection layer and dot-regression loss. We undertake experiments to investigate how the dot-regression loss and projection layer influence model performance based on ETF and FedLoGe, with the results documented in Tab.",
                "7",
                ". We ascertain that solely through the sparsity design of SSE-C, performance surpassing that of employing both the projection layer and dot-regression loss can be achieved."
            ]
        ]
    },
    "A1.T8": {
        "caption": "Table 8: Sparsification with different initialization of frozen classifiers",
        "table": "",
        "footnotes": "\n\n\n\n\n\nInitialization Method\nMany\nMed\nFew\nAll\n\n\n\nXavier\n0.6793\n0.4204\n0.1823\n0.3784\n\nSparse Xavier\n0.7089\n0.4977\n0.2189\n0.4237\n\nGassian\n0.6167\n0.3858\n0.1572\n0.3407\n\nSparse Gassian\n0.7119\n0.4969\n0.2117\n0.4209\n\nUniform\n0.6119\n0.3631\n0.1638\n0.3366\n\nSparse Uniform\n0.7141\n0.4927\n0.2174\n0.4231\n\nOrthogonal\n0.6844\n0.4446\n0.1868\n0.3882\n\nSparse Orthogonal\n0.7022\n0.4846\n0.2060\n0.4124\n\nKaiming Uniform\n0.6933\n0.4392\n0.1881\n0.3898\n\nSparse Kaiming Uniform\n0.7130\n0.4946\n0.2168\n0.4230\n\n\n",
        "references": [
            [
                "To further explore the effectiveness of sparsification for a fixed classifier, we have selected various classifier initialization methods (Xavier, Gaussian, Uniform, Orthogonal, Kaiming Uniform), and subsequently applied sparsification to classifiers constructed with these initializations. The results are documented in Tab.¬†",
                "8",
                ". It is observable that irrespective of the initialization method employed, post-sparsification and subsequent training of the fixed classifier yield exceptionally favorable performance."
            ]
        ]
    }
}