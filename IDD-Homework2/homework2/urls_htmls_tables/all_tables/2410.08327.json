{
    "id_table_1": {
        "caption": "Table 1:  Difference in performance between models trained on the synthetic data generated with ( D  = 8 subscript D italic- 8 D_{\\epsilon=8} italic_D start_POSTSUBSCRIPT italic_ = 8 end_POSTSUBSCRIPT ) and without ( D  =  subscript D italic- D_{\\epsilon=\\infty} italic_D start_POSTSUBSCRIPT italic_ =  end_POSTSUBSCRIPT ) DP and the models trained on real data ( D r  e  a  l subscript D r e a l D_{real} italic_D start_POSTSUBSCRIPT italic_r italic_e italic_a italic_l end_POSTSUBSCRIPT ) for multilabel ICD code classification with the top 3, 5, and 10 most frequent labels. Performance degradation greatly increases for more complex tasks.",
        "table": "S3.T1.41",
        "footnotes": [],
        "references": [
            "We create artificial canary samples that are contextually relevant to both domains and include PII such as names, emails, addresses, and numeric identifiers (details in the appendix in  Table   10  and  Table   9 ). Following the methodology outlined in  Yue et al. ( 2023 ) , we vary the number of injections of these canary samples into our training data for 1, 10, and 100 repetitions. For each canary, we generate 10,000 candidate sequences and rank the canaries based on their perplexity score.",
            "Tables  1  and  2  report results for classification tasks for all models, for the healthcare and CPS data respectively. Unsurprisingly, models trained on data generated from DP fine-tuned models generally under-perform models trained on real data or data generated without DP. Table  1  reports performance for varying task complexity by increasing number of labels  n n n italic_n  for our multilabel ICD-9 code classification task. For simpler tasks, e.g.  ICD-9 n = 3 subscript ICD-9 n 3 \\text{ICD-9}_{n=3} ICD-9 start_POSTSUBSCRIPT italic_n = 3 end_POSTSUBSCRIPT , there is a much smaller performance degradation and the  D  =  subscript D italic- D_{\\epsilon=\\infty} italic_D start_POSTSUBSCRIPT italic_ =  end_POSTSUBSCRIPT  (F1  \\approx   0.87) and  D  = 8 subscript D italic- 8 D_{\\epsilon=8} italic_D start_POSTSUBSCRIPT italic_ = 8 end_POSTSUBSCRIPT  (F1  \\approx   0.85) models are nearly comparable. In contrast, there is much larger performance degration for the more difficult  ICD-9 n = 10 subscript ICD-9 n 10 \\text{ICD-9}_{n=10} ICD-9 start_POSTSUBSCRIPT italic_n = 10 end_POSTSUBSCRIPT  task, where F1  \\approx   0.61 for  D  =  subscript D italic- D_{\\epsilon=\\infty} italic_D start_POSTSUBSCRIPT italic_ =  end_POSTSUBSCRIPT  and F1  \\approx   0.40 for  D  = 8 subscript D italic- 8 D_{\\epsilon=8} italic_D start_POSTSUBSCRIPT italic_ = 8 end_POSTSUBSCRIPT .",
            "Data generated by the ICL model resulted in higher-performing coreference systems than data generated by the fine-tuned models. It is likely that the larger model outputted generally more coherent data, through the lack of fine-tuning reduces controllability of generation, e.g., as evidence by the lower performance of the ICL model in  Table   1 .",
            "Figure   3  shows the reduction in private entity leakage in data generated from DP fine-tuned models compared to non-DP fine-tuned models. While notably reduced, some leakage still persists when using DP, even on decreasing the privacy budget further. While  Figure   2  compares rates of leakage of aggregated across all entities, it does not provide insight into how leakage for individual entities may occur. In  Figure   1  we conduct this analysis by manually selecting 8 names that occur in the real and synthetic CPS data and plotting the frequency of each name in each data type. For most names, frequency in the DP-generated synthetic data is less than in the original data, but this reduction does not always hold, even with a lower privacy budget (e.g., PII-7, PII-5 for   = 4 italic- 4 \\epsilon=4 italic_ = 4 ). In data generated without differential privacy, the frequency of names sometimes exceeds their frequency in the original data (e.g., PII-5, PII-6).",
            "Overall, our results are consistent with prior work in that we find only small performance degradation when training a model on DP-generated synthetic text as compared to real data for relatively less fine-grained (e.g.  ICD-9 n = 3 subscript ICD-9 n 3 \\text{ICD-9}_{n=3} ICD-9 start_POSTSUBSCRIPT italic_n = 3 end_POSTSUBSCRIPT , in  Table   1 ) classification tasks. Similarly, we do find evidence that DP reduces potential privacy leakage in that artificial canaries ( Table   4 ) and real entities ( Figure   2 ) are generated less frequently by DP-fine-tuned models.",
            "However, our evaluations also expose previously unexplored weaknesses to this approach. Model performance degrades much more sharply as task complexity increases (e.g.  ICD-9 n = 10 subscript ICD-9 n 10 \\text{ICD-9}_{n=10} ICD-9 start_POSTSUBSCRIPT italic_n = 10 end_POSTSUBSCRIPT  classification in  Table   1 , mention vs. coreference performance in  Table   3 ). These results suggest that DP-generated synthetic data may be of sufficient quality for certain NLP tasks and domains, but the quality degradation from DP is a limitation on broader use."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Difference in performance between models trained on data generated with differential privacy and models trained on real data, evaluated over CPS classification, for varying privacy budgets.",
        "table": "S3.T1.41.42.1.1.1",
        "footnotes": [],
        "references": [
            "Controllable generation approaches enable the generation of notes with specific properties. We primarily use them to enable classification-based utility evaluations (described in  2.2 ).",
            "We compute fairness metrics over the same control-code classification tasks as the utility evaluation ( 2.2 ). In data with available demographic information, we compare fairness classification for race and gender subgroups using equality difference (ED) and equalized odds (EO) metrics. For ED, for instance, False Positive Equality Difference (FPED) is the sum of the differences between the overall false positive rate (FPR) for the entire dataset and the FPR for each subgroup. EO constitutes a stricter notion of fairness by evaluating whether both the FPR and TPR rates are the same across all groups. In both cases, values closer to zero indicate that the model performs more uniformly across subgroups, with zero indicating perfect parity across subgroups. For reference, we formally define these metrics in  Appendix   C .",
            "Tables  1  and  2  report results for classification tasks for all models, for the healthcare and CPS data respectively. Unsurprisingly, models trained on data generated from DP fine-tuned models generally under-perform models trained on real data or data generated without DP. Table  1  reports performance for varying task complexity by increasing number of labels  n n n italic_n  for our multilabel ICD-9 code classification task. For simpler tasks, e.g.  ICD-9 n = 3 subscript ICD-9 n 3 \\text{ICD-9}_{n=3} ICD-9 start_POSTSUBSCRIPT italic_n = 3 end_POSTSUBSCRIPT , there is a much smaller performance degradation and the  D  =  subscript D italic- D_{\\epsilon=\\infty} italic_D start_POSTSUBSCRIPT italic_ =  end_POSTSUBSCRIPT  (F1  \\approx   0.87) and  D  = 8 subscript D italic- 8 D_{\\epsilon=8} italic_D start_POSTSUBSCRIPT italic_ = 8 end_POSTSUBSCRIPT  (F1  \\approx   0.85) models are nearly comparable. In contrast, there is much larger performance degration for the more difficult  ICD-9 n = 10 subscript ICD-9 n 10 \\text{ICD-9}_{n=10} ICD-9 start_POSTSUBSCRIPT italic_n = 10 end_POSTSUBSCRIPT  task, where F1  \\approx   0.61 for  D  =  subscript D italic- D_{\\epsilon=\\infty} italic_D start_POSTSUBSCRIPT italic_ =  end_POSTSUBSCRIPT  and F1  \\approx   0.40 for  D  = 8 subscript D italic- 8 D_{\\epsilon=8} italic_D start_POSTSUBSCRIPT italic_ = 8 end_POSTSUBSCRIPT .",
            "In the classification task with the CPS data ( Table   2 ), however, we notice a significant drop in performance for models trained over data generated with DP for both more generous ( D  = 8 subscript D italic- 8 D_{\\epsilon=8} italic_D start_POSTSUBSCRIPT italic_ = 8 end_POSTSUBSCRIPT ) and more restricted ( D  = 4 subscript D italic- 4 D_{\\epsilon=4} italic_D start_POSTSUBSCRIPT italic_ = 4 end_POSTSUBSCRIPT ) privacy budgets. From examining the data, this task is generally more difficult and the associations between the administrative label and the text in the real data can be quite subtle. It is likely that the generative model often fails to pick up on these associations, and noise introduced by DP further masks these subtleties.",
            "We perform analysis of actual leakage (e.g., appearance in generated text) using PII already present in the training data rather than inserted canaries. These entity-centric metrics ( Figure   2 ) show that while DP-generated data does contain fewer instances of potentially sensitive information, these entities are not removed from the data entirely, and there is still risk of leakage.",
            "Figure   3  shows the reduction in private entity leakage in data generated from DP fine-tuned models compared to non-DP fine-tuned models. While notably reduced, some leakage still persists when using DP, even on decreasing the privacy budget further. While  Figure   2  compares rates of leakage of aggregated across all entities, it does not provide insight into how leakage for individual entities may occur. In  Figure   1  we conduct this analysis by manually selecting 8 names that occur in the real and synthetic CPS data and plotting the frequency of each name in each data type. For most names, frequency in the DP-generated synthetic data is less than in the original data, but this reduction does not always hold, even with a lower privacy budget (e.g., PII-7, PII-5 for   = 4 italic- 4 \\epsilon=4 italic_ = 4 ). In data generated without differential privacy, the frequency of names sometimes exceeds their frequency in the original data (e.g., PII-5, PII-6).",
            "The results provide further evidence that, while training models with differential privacy may decrease the risk of information memorization, it does not provide a failsafe. There is a notable disparity in the frequency of phrases from the training data reproduced in these datasets:  D  =  subscript D italic- D_{\\epsilon=\\infty} italic_D start_POSTSUBSCRIPT italic_ =  end_POSTSUBSCRIPT  contains nearly 1.6 times as many phrases as the  D  = 8 subscript D italic- 8 D_{\\epsilon=8} italic_D start_POSTSUBSCRIPT italic_ = 8 end_POSTSUBSCRIPT , but the phrase leakage from  D  = 8 subscript D italic- 8 D_{\\epsilon=8} italic_D start_POSTSUBSCRIPT italic_ = 8 end_POSTSUBSCRIPT  is still non-zero. On the other hand, while  D I  C  L subscript D I C L D_{ICL} italic_D start_POSTSUBSCRIPT italic_I italic_C italic_L end_POSTSUBSCRIPT  is 0.6 times the size of the other datasets, it seems to regurgitate contextual information about these entities from the in-context samples less frequently. However, results from  Figure   2  indicate that it still poses privacy risks, as the ICL tends to reproduce these entities, even if not the contexts in which they appear.",
            "Overall, our results are consistent with prior work in that we find only small performance degradation when training a model on DP-generated synthetic text as compared to real data for relatively less fine-grained (e.g.  ICD-9 n = 3 subscript ICD-9 n 3 \\text{ICD-9}_{n=3} ICD-9 start_POSTSUBSCRIPT italic_n = 3 end_POSTSUBSCRIPT , in  Table   1 ) classification tasks. Similarly, we do find evidence that DP reduces potential privacy leakage in that artificial canaries ( Table   4 ) and real entities ( Figure   2 ) are generated less frequently by DP-fine-tuned models.",
            "Furthermore, despite claims that differentially private training of language models can effectively  eliminate  the risk of privacy leakage  (Yue et al.,  2023 ; Mattern et al.,  2022a ) , our experiments indicate that there is also a substantial risk of data leakage (Tables  4 - 5 ,  Figure   2 ), especially for some types of PII. These results are consistent with risks of leakage identified in sentence-level applications of differential privacy  (Lukas et al.,  2023 ) ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  F1 scores for coreference and mention detection over entities from human-annotated test splits of the CPS and i2b2/VA datasets. All synthetic datasets are annotated with silver labels. There is general performance degradation for synthetic data generated with (  = 8 italic- 8 \\epsilon=8 italic_ = 8 ) and without DP (  =  italic- \\epsilon=\\infty italic_ =  ) as compared to real data. The performance degradation is more noticeable in coreference metrics than mention detection metrics.",
        "table": "S3.T1.41.42.1.3.1",
        "footnotes": [],
        "references": [
            "As canary evaluations are only a proxy for assessing potential privacy risks and may not be comprehensive, we directly leverage entity markers in our datasets to evaluate privacy concerns (we provide details on data-specific entity definitions in  3 ).",
            "Table  3  reports coreference results. For comparison, we report  D r  e  a  l  ( g  o  l  d ) subscript D r e a l g o l d D_{real(gold)} italic_D start_POSTSUBSCRIPT italic_r italic_e italic_a italic_l ( italic_g italic_o italic_l italic_d ) end_POSTSUBSCRIPT , model performance when trained over gold in-domain data, which represents the best possible performance we can obtain with human annotations and  D r  e  a  l  ( s  i  l  v  e  r ) subscript D r e a l s i l v e r D_{real(silver)} italic_D start_POSTSUBSCRIPT italic_r italic_e italic_a italic_l ( italic_s italic_i italic_l italic_v italic_e italic_r ) end_POSTSUBSCRIPT , model performance when trained over silver annotated real data. The  15 point performance difference in F1 between these two setups represents the performance degradation we should expect to see as a result of inevitable cascading errors from the silver annotations.",
            "Figure   3  shows the reduction in private entity leakage in data generated from DP fine-tuned models compared to non-DP fine-tuned models. While notably reduced, some leakage still persists when using DP, even on decreasing the privacy budget further. While  Figure   2  compares rates of leakage of aggregated across all entities, it does not provide insight into how leakage for individual entities may occur. In  Figure   1  we conduct this analysis by manually selecting 8 names that occur in the real and synthetic CPS data and plotting the frequency of each name in each data type. For most names, frequency in the DP-generated synthetic data is less than in the original data, but this reduction does not always hold, even with a lower privacy budget (e.g., PII-7, PII-5 for   = 4 italic- 4 \\epsilon=4 italic_ = 4 ). In data generated without differential privacy, the frequency of names sometimes exceeds their frequency in the original data (e.g., PII-5, PII-6).",
            "However, our evaluations also expose previously unexplored weaknesses to this approach. Model performance degrades much more sharply as task complexity increases (e.g.  ICD-9 n = 10 subscript ICD-9 n 10 \\text{ICD-9}_{n=10} ICD-9 start_POSTSUBSCRIPT italic_n = 10 end_POSTSUBSCRIPT  classification in  Table   1 , mention vs. coreference performance in  Table   3 ). These results suggest that DP-generated synthetic data may be of sufficient quality for certain NLP tasks and domains, but the quality degradation from DP is a limitation on broader use.",
            "We further find substantial variance not only in the task difficulty, but also across data sets. Co-reference performance degradation from real to synthetic data is markedly worse for MIMIC than CPS ( Table   3 ). These differences could be due to a number of factors, such as the similarity between each private data set and the model pre-training data. Regardless, these results emphasize the importance of evaluating on in-domain data, as results may not generalize."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Rank and perplexity metrics for 10-insertion canary attacks over MIMIC and CPS data (0, 1 and 100 insertions, reported in  Appendix   E , are similar). Each column is formatted as   =  /  = 8 italic- italic- 8 \\epsilon=\\infty/\\epsilon=8 italic_ =  / italic_ = 8  . Higher rank and lower perplexity typically indicate decreased risk of leakage. DP reduces but does not eliminate privacy risks for all canaries, and metrics are generally unstable.",
        "table": "S3.T1.41.42.1.4.1",
        "footnotes": [
            ""
        ],
        "references": [
            "Table   4  reports results for canary attacks. The DP fine-tuned models exhibit higher perplexity scores for all the canaries, demonstrating that models trained with DP are less likely to output phrases from training data. There is also a relatively sharper drop in perplexity for models fine-tuned without DP as the number of canary insertions increase ( Appendix   E ). However, our entity-centric evaluation demonstrates that canary evaluations may not be effective in assessing a models potential for privacy leakage. It should also be noted that while DP improves (increases) rank for some canaries, it decreases rank for others. The canarys rank is also highly dependent on the choice of candidate comparisons, making these metrics easy to skew.",
            "Overall, our results are consistent with prior work in that we find only small performance degradation when training a model on DP-generated synthetic text as compared to real data for relatively less fine-grained (e.g.  ICD-9 n = 3 subscript ICD-9 n 3 \\text{ICD-9}_{n=3} ICD-9 start_POSTSUBSCRIPT italic_n = 3 end_POSTSUBSCRIPT , in  Table   1 ) classification tasks. Similarly, we do find evidence that DP reduces potential privacy leakage in that artificial canaries ( Table   4 ) and real entities ( Figure   2 ) are generated less frequently by DP-fine-tuned models.",
            "Furthermore, despite claims that differentially private training of language models can effectively  eliminate  the risk of privacy leakage  (Yue et al.,  2023 ; Mattern et al.,  2022a ) , our experiments indicate that there is also a substantial risk of data leakage (Tables  4 - 5 ,  Figure   2 ), especially for some types of PII. These results are consistent with risks of leakage identified in sentence-level applications of differential privacy  (Lukas et al.,  2023 ) .",
            "In  Table   4  we report the full set of canary results (for 1, 10, and 100 insertions, for each canary type). Results are generally similar across different numbers of insertions, in that DP generally reduces rank and perplexity, thus improving privacy, but does not eliminate all risk of leakage."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Unique contexts in which entities in the real data appear in the synthetic data. Surrounding context word lengths vary from 1 to 4.",
        "table": "S3.T1.41.42.1.5.1",
        "footnotes": [],
        "references": [
            "Leaked identifiers are potentially more harmful if additional information about an individual is leaked alongside their identity. We assess this risk in  Table   5 , where we gauge how often  sequences  of length 1-4 containing these leaked entities appear in the generated outputs, rather than examining entities in isolation.",
            "Furthermore, despite claims that differentially private training of language models can effectively  eliminate  the risk of privacy leakage  (Yue et al.,  2023 ; Mattern et al.,  2022a ) , our experiments indicate that there is also a substantial risk of data leakage (Tables  4 - 5 ,  Figure   2 ), especially for some types of PII. These results are consistent with risks of leakage identified in sentence-level applications of differential privacy  (Lukas et al.,  2023 ) ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Fairness evaluation for the MIMIC-III  ICD-9 n = 10 subscript ICD-9 n 10 \\text{ICD-9}_{n=10} ICD-9 start_POSTSUBSCRIPT italic_n = 10 end_POSTSUBSCRIPT  task, for the gender and race categories. Higher values indicate poorer group fairness performance. We report additional fairness metrics in  Appendix   C  in  Table   7  that show similar trends.",
        "table": "S3.T2.10",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "We report the FNED and Equalized Odds (EO) metrics for the results from the  ICD-9 n = 10 subscript ICD-9 n 10 \\text{ICD-9}_{n=10} ICD-9 start_POSTSUBSCRIPT italic_n = 10 end_POSTSUBSCRIPT  multilabel classification tasks in  Table   6 . The metrics reflect the difference in model performance for the gender and race/ethnicity subgroups with more than 100 samples in the test set, with a larger value indicating more disparate performance across the subgroups. While the gender metrics indicate minimal performance differences, the race/ethnicity metrics show significant disparities. The disparate performance increases for models trained over the data generated from the DP model ( D  = 8 subscript D italic- 8 D_{\\epsilon=8} italic_D start_POSTSUBSCRIPT italic_ = 8 end_POSTSUBSCRIPT ) as compared to the model without DP ( D  =  subscript D italic- D_{\\epsilon=\\infty} italic_D start_POSTSUBSCRIPT italic_ =  end_POSTSUBSCRIPT ). The model trained with DP ( D  = 8 subscript D italic- 8 D_{\\epsilon=8} italic_D start_POSTSUBSCRIPT italic_ = 8 end_POSTSUBSCRIPT ) exhibits the most disparate performance across these subgroups, followed by the  D I  C  L subscript D I C L D_{ICL} italic_D start_POSTSUBSCRIPT italic_I italic_C italic_L end_POSTSUBSCRIPT , although the latter provides better utility for the classification and coreference tasks."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Fairness evaluation for the MIMIC-III  ICD-9 n = 10 subscript ICD-9 n 10 \\text{ICD-9}_{n=10} ICD-9 start_POSTSUBSCRIPT italic_n = 10 end_POSTSUBSCRIPT  task, for the gender and race categories.",
        "table": "S4.T3.5",
        "footnotes": [],
        "references": []
    },
    "id_table_8": {
        "caption": "Table 8:  Rank and perplexity metrics for canary attacks over MIMIC and CPS data. Each column is formatted as   =  /  = 8 italic- italic- 8 \\epsilon=\\infty/\\epsilon=8 italic_ =  / italic_ = 8  . Perplexity scores suggest that DP reduces privacy metrics for all canaries, and generally show similar privacy improvements.",
        "table": "S4.T4.3",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "Table 9:  The canaries inserted into the training data for the models fine-tuned to generate synthetic MIMIC-III data.",
        "table": "S4.T5.4",
        "footnotes": [],
        "references": [
            "We create artificial canary samples that are contextually relevant to both domains and include PII such as names, emails, addresses, and numeric identifiers (details in the appendix in  Table   10  and  Table   9 ). Following the methodology outlined in  Yue et al. ( 2023 ) , we vary the number of injections of these canary samples into our training data for 1, 10, and 100 repetitions. For each canary, we generate 10,000 candidate sequences and rank the canaries based on their perplexity score."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  The canaries inserted into the training data for the models fine-tuned to generate synthetic CPS data.",
        "table": "S4.T6.8",
        "footnotes": [],
        "references": [
            "We create artificial canary samples that are contextually relevant to both domains and include PII such as names, emails, addresses, and numeric identifiers (details in the appendix in  Table   10  and  Table   9 ). Following the methodology outlined in  Yue et al. ( 2023 ) , we vary the number of injections of these canary samples into our training data for 1, 10, and 100 repetitions. For each canary, we generate 10,000 candidate sequences and rank the canaries based on their perplexity score."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Analysis for the MIMIC-III dataset of all the unique contexts in which entities of from all categories from the training data appear in the synthetic data, considering surrounding context word lengths varying from 1 to 4.  D r  e  a  l subscript D r e a l D_{real} italic_D start_POSTSUBSCRIPT italic_r italic_e italic_a italic_l end_POSTSUBSCRIPT  corresponds to the training data the generative models were trained on.",
        "table": "A2.T7.8",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "Table 12:  Analysis for the CPS data of all the unique contexts in which entities of from all categories from the training data appear in the synthetic data, considering surrounding context word lengths varying from 1 to 4.  D r  e  a  l subscript D r e a l D_{real} italic_D start_POSTSUBSCRIPT italic_r italic_e italic_a italic_l end_POSTSUBSCRIPT  corresponds to the training data the generative models were trained on.",
        "table": "A5.T8.3",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "A5.T9.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A5.T9.1.1.2.1.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A5.T9.1.1.3.2.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A5.T9.1.1.4.3.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "A5.T9.1.1.5.4.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "A5.T10.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "A5.T10.1.1.2.1.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "A5.T10.1.1.3.2.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "",
        "table": "A5.T10.1.1.4.3.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "",
        "table": "A5.T10.1.1.5.4.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_23": {
        "caption": "",
        "table": "A5.T11.7.7",
        "footnotes": [],
        "references": []
    },
    "id_table_24": {
        "caption": "",
        "table": "A5.T11.7.7.8.1.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_25": {
        "caption": "",
        "table": "A5.T11.7.7.8.1.4.1",
        "footnotes": [],
        "references": []
    },
    "id_table_26": {
        "caption": "",
        "table": "A5.T11.7.7.8.1.5.1",
        "footnotes": [],
        "references": []
    },
    "id_table_27": {
        "caption": "",
        "table": "A5.T11.7.7.8.1.6.1",
        "footnotes": [],
        "references": []
    },
    "id_table_28": {
        "caption": "",
        "table": "A5.T12.6.6",
        "footnotes": [],
        "references": []
    },
    "id_table_29": {
        "caption": "",
        "table": "A5.T12.3.3.3.5.1",
        "footnotes": [],
        "references": []
    },
    "id_table_30": {
        "caption": "",
        "table": "A5.T12.3.3.3.6.1",
        "footnotes": [],
        "references": []
    },
    "id_table_31": {
        "caption": "",
        "table": "A5.T12.3.3.3.7.1",
        "footnotes": [],
        "references": []
    },
    "id_table_32": {
        "caption": "",
        "table": "A5.T12.2.2.2.2.2",
        "footnotes": [],
        "references": []
    },
    "id_table_33": {
        "caption": "",
        "table": "A5.T12.3.3.3.3.1",
        "footnotes": [],
        "references": []
    }
}