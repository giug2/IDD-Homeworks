{
    "id_table_1": {
        "caption": "Table 2:  Comparison of Spearmans correlation results on STS tasks, where the value highlighted in bold is the best value, and the value underlined is the second-best value. : results from  Miao et al. ( 2023 ) ,    \\clubsuit  : results from  Wang et al. ( 2024 ) ,    \\spadesuit  : results from  Liu et al. ( 2023 ) ,       {\\dagger}{\\dagger}   : results from  Zhang et al. ( 2023 ) . *: we reproduce the results with the officially released corpus from  Zhang et al. ( 2023 ) .",
        "table": "S1.T1.1.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "High Data Noise.  Unsupervised sentence representation learning often suffers from data noise caused by confusing negative samples, which mainly arise from two sources. First, traditional methods generate datasets by duplicating samples to create positive instances, leading to negatives with similar surface-level semantics that affect the models performance  (Miao et al.,  2023 ) . Second, in data synthesis, differences in semantic distributions across domains can cause the LLMs criteria for distinguishing between positive and negative samples to misalign with the target domain, introducing additional noise  (Huang et al.,  2023 ; Poerner and Schutze,  2019 ) . The existing MultiCSR method attempts to remove noisy samples using linear programming, but this can eliminate potentially valuable samples and reduce data diversity. Figure  1  compares various baselines on the STS-Benchmark development set. The results show that the prediction of false positives outnumber false negatives, and data synthesis in SynCSE increases false negatives, further supporting the above analysis.",
            "In this paper, we propose a pipeline-based data augmentation method using LLMs and introduce the Gaussian-decayed gradient-assisted Contrastive Sentence Embedding (GCSE) model to improve the performance of unsupervised sentence embedding methods. To address the issue of  low data diversity , we begin by extracting entities and quantities from the data samples and constructing a knowledge graph (KG) with the extracted data. Next, we create a sentence construction prompt using the extracted knowledge to guide LLM in generating more diverse positive samples. To tackle  high data noise , we employ an evaluation model to annotate the synthesized data and initially filter out false samples. However, this procedure is ineffective in filtering out false negatives with similar surface-level semantics. Therefore, we propose the GCSE model that employs a Gaussian-decayed function to calculate the prediction distinctions between GCSE and the evaluation model. This prevents the model from being influenced by false hard negatives in the initial steps, allowing it to use other in-batch negative sample losses to optimize the gradient. We highlight the key innovations of our approach in Table  1 : (i) We are the first to incorporate fine-grained knowledge for sample synthesis in LLM-based methods. (ii) Unlike MultiCSRs denoising approach, our method retains more false samples for training rather than discarding them. (iii) Our data selection strategy focuses on domain-specific samples, using a local LLM with fewer samples for synthesis, leading to improved performance. Experimental results demonstrate the efficiency of our model, outperforming previous best methods in average scores for semantic textual similarity (STS) tasks by 0.28 with BERT-base, 0.23 with BERT-large, and 0.46 with RoBERTa-large.",
            "Knowledge Extraction and Integration.  The variety and relationships between samples directly impact model performance in sentence representation learning. A major challenge with existing LLM-based data synthesis methods is the limited diversity they generate for each short text. To trade off the low diversity of the generated samples with their relevance to the domain semantic space, we first design an extraction prompt to obtain entities and quantities from the given data. Formally, we denote the extraction prompt as  P e subscript P e \\mathcal{P}_{e} caligraphic_P start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT , and LLM  L L \\mathcal{L} caligraphic_L , suppose we finally extract instances with  d d d italic_d  sample number, the knowledge set  K i = { k i  1 , ... , k i  n } subscript K i subscript k i 1 ... subscript k i n \\mathcal{K}_{i}=\\left\\{k_{i1},\\dots,k_{in}\\right\\} caligraphic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_k start_POSTSUBSCRIPT italic_i 1 end_POSTSUBSCRIPT , ... , italic_k start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT }  of each instance  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is computed in Equation  1 , where  t j subscript t j t_{j} italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ,  c j subscript c j c_{j} italic_c start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  and  q j subscript q j q_{j} italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  represent the entity text, entity type and quantity of  k i subscript k i k_{i} italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .  n i subscript n i n_{i} italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the size of  K i subscript K i \\mathcal{K}_{i} caligraphic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , and  F  (  ) F  \\mathcal{F}(\\cdot) caligraphic_F (  )  is the formatting function that convert text to triplet. Next, we integrate all knowledge by establishing an entity knowledge graph  G =  V , E  G V E \\mathcal{G}=\\langle V,E\\rangle caligraphic_G =  italic_V , italic_E  , where the node set  V V V italic_V  contains all the   t , c , q  t c q \\langle t,c,q\\rangle  italic_t , italic_c , italic_q   from  K K \\mathcal{K} caligraphic_K :",
            "Analysis of entities and quantities awareness:  We analyze GCSE awareness of entities and quantities by constructing a dataset using the data synthesis method in Section  3.1  on the STS-Benchmark development set. Then, the similarity scores of each triplet in the dataset are annotated by two supervised pre-trained models: sup-simcse-bert-large and sup-simcse-roberta-large. The final label is the average score of the similarity calculated by both models. We evaluate Spearmans correlation scores of GCSE and the other three strong baselines on the backbone of the BERT-base model, and the results are shown in Table  5 . Our GCSE achieves the best result and outperforms RankCSE by 14.03%. In this case, both SynCSE and GCSE achieve significant improvements over methods without LLM. This might be due to the similarity of the semantic representation space between the training set and the development set, both of which are synthesized via LLM. Nevertheless, GCSE shows a notable enhancement in performance of 2.19% compared to SynCSE, demonstrating that its understanding of the entities and quantities in sentences has enhanced to a certain degree.",
            "We employ the Gaussian-decayed function on SynCSE and sample SynCSE training data with a sample size the same as our synthetic data to evaluate the efficacy of the proposed Gaussian-decayed function and our domain-oriented selection strategy in the ablation experiment. The data sample size is 64k, and the weight of    \\sigma italic_  in  G  (  ) G  G(\\cdot) italic_G (  )  is assigned the same value as specified in Section  4.1 . The results of various policies implemented in SynCSE are presented in Table  9 . w sampled denotes the utilization of purely the sampled data in SynCSE, and a performance decrease can be observed when training on a reduced number of samples without extra configurations. w sampled & G.D. denotes the additional incorporation of  G  (  ) G  G(\\cdot) italic_G (  )  based on w sampled. w G.D. indicates the results by training on the full dataset utilizing  G  (  ) G  G(\\cdot) italic_G (  ) . In both configurations, the average performance outperforms the vanilla model, illustrating the modules efficacy. w sampled & domain & G.D. denotes the concurrent utilization of sample data, domain data, and  G  (  ) G  G(\\cdot) italic_G (  ) , with a sample size of 48k for the SynCSE dataset and 16k for the synthesized domain dataset. The results reveal that \"w sampled & domain & G.D.\" attains the second-best performance, suggesting that incorporating domain data can decrease the required training samples while enhancing model efficacy.",
            "In this section, we utilize contrastive learning on ChatGLM-3 with a low-rank adapter (LoRA) layer, and we gather other sentence embedding results from LLMs via in-context learning (ICL) to evaluate the alignment of LLM-generated similarities with the gold labels, as presented in Table  10 . The results indicate that even ChatGPT with ICL cannot surpass the performance of contrastive learning methods based on encoder models. The existing methodology of LLM for generating sentence embeddings is unsatisfactory. Although some works utilize prompts without extra training, the efficacy of sentence embedding primarily relies on the quality of prompt engineering. Furthermore, employing the contrastive learning fine-tuning method for LLMs achieves no substantial benefit in semantic textual similarity tasks, underutilizing the capabilities of LLMs. To reduce expenses, we assert that fully leveraging the capabilities of LLMs for distilling smaller models is the better option."
        ]
    },
    "id_table_2": {
        "caption": "Table 3:  Comparison of Mean Average Precision (MAP) results on reranking tasks, where the value highlighted in bold is the best value, and the value underlined is the second-best value.       {\\dagger}{\\dagger}   : results from  Zhang et al. ( 2023 ) . *: we reproduce the results with the officially released corpus from  Zhang et al. ( 2023 ) .",
        "table": "S4.T2.14.14",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "In this section, we present the data augmentation pipeline via LLM and the specific structure of the GCSE. As shown in Figure  2 , we start by using a data augmentation pipeline to synthesize new samples from the source data, and then train our model with the filtered synthetic data.",
            "STS Tasks:  The overall results of the STS tasks are shown in Table  2 . Our approach achieves state-of-the-art performance across the BERT-base, BERT-large, and RoBERTa-large backbones compared to other unsupervised baselines. This highlights the applicability of our method, as it can be effectively applied to multiple models. Compared to the standard unsupervised SimCSE, Spearmans correlation of our approach is improved by an average of 17.24% on the base models and 3.44% on the large models. On the strong baseline RankCSE, our approach achieved a 1.36% improvement over its average performance, demonstrating the effectiveness of the LLM data synthesis process. Furthermore, we compare two baseline models: SynCSE and MultiCSR, both of which utilize LLM as the data synthesis model. We specifically analyze the results of using ChatGPT for both models. The results show that our approach outperforms both models in most cases; in the case of using RoBERTa-base, our method is slightly behind MultiCSR by 0.35% and still achieves the second-best result. It should be noted that the ChatGLM-3(6b) we use is much more lightweight than ChatGPT(about 175b) in parameters. Additionally, our method only utilizes 14% of the sample size compared to the other two methods that employ the entire NLI datasets. This demonstrates the effectiveness of our data synthesis strategy and domain-oriented sample selection strategy."
        ]
    },
    "id_table_3": {
        "caption": "Table 4:  Ablation studies of STS tasks on BERT-base. Other PLMs yield similar patterns to BERT-base.",
        "table": "S4.T3.8.8",
        "footnotes": [],
        "references": [
            "In the data augmentation pipeline, we utilize both domain data and partial general data to balance domain-specific relevance and general-domain applicability. We start by extracting knowledge from the source data and then synthesize new data for our model training. The detailed structure of the pipeline is shown in Figure  3 .",
            "The edges  E E E italic_E  consist of hard edges  E r subscript E r E_{r} italic_E start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  and soft edges  E s subscript E s E_{s} italic_E start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT . As shown in Equations  3  and  4 ,  E r subscript E r E_{r} italic_E start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  represents the relationship between the entity text, type and quantity of each  k  K k K k\\in\\mathcal{K} italic_k  caligraphic_K , and  E s subscript E s E_{s} italic_E start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  indicates the relationship between entity text in  k i  j subscript k i j k_{ij} italic_k start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  and other entity text or type in the same instance  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .",
            "Reranking Tasks:  Table  3  presents the MAP results of our approach and related baselines on the reranking benchmark, and all models are evaluated on the test sets of the reranking benchmark without using the training sets. The results indicate that various approaches exhibit varying performance on different datasets, which can be attributed to the distinct semantic distribution and evaluation scale of each dataset. Our GCSE outperforms SynCSE by 0.39% in average MAP score and achieves the best results in all backbone models, demonstrating the efficacy of our approach in enhancing the precision of unsupervised ranking tasks.",
            "Analysis of entities and quantities awareness:  We analyze GCSE awareness of entities and quantities by constructing a dataset using the data synthesis method in Section  3.1  on the STS-Benchmark development set. Then, the similarity scores of each triplet in the dataset are annotated by two supervised pre-trained models: sup-simcse-bert-large and sup-simcse-roberta-large. The final label is the average score of the similarity calculated by both models. We evaluate Spearmans correlation scores of GCSE and the other three strong baselines on the backbone of the BERT-base model, and the results are shown in Table  5 . Our GCSE achieves the best result and outperforms RankCSE by 14.03%. In this case, both SynCSE and GCSE achieve significant improvements over methods without LLM. This might be due to the similarity of the semantic representation space between the training set and the development set, both of which are synthesized via LLM. Nevertheless, GCSE shows a notable enhancement in performance of 2.19% compared to SynCSE, demonstrating that its understanding of the entities and quantities in sentences has enhanced to a certain degree."
        ]
    },
    "id_table_4": {
        "caption": "Table 6:  Examples of data synthesis prompts, where  { v  a  r  i  a  b  l  e  n  a  m  e } v a r i a b l e n a m e \\left\\{variable~{}name\\right\\} { italic_v italic_a italic_r italic_i italic_a italic_b italic_l italic_e italic_n italic_a italic_m italic_e }  refers to a varibale.",
        "table": "S4.T4.1.1",
        "footnotes": [],
        "references": [
            "The edges  E E E italic_E  consist of hard edges  E r subscript E r E_{r} italic_E start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  and soft edges  E s subscript E s E_{s} italic_E start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT . As shown in Equations  3  and  4 ,  E r subscript E r E_{r} italic_E start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  represents the relationship between the entity text, type and quantity of each  k  K k K k\\in\\mathcal{K} italic_k  caligraphic_K , and  E s subscript E s E_{s} italic_E start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  indicates the relationship between entity text in  k i  j subscript k i j k_{ij} italic_k start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  and other entity text or type in the same instance  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .",
            "where    \\alpha italic_ ,    \\beta italic_  are the threshold for positives and negatives, respectively.  x k subscript x k x_{k} italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT  denotes a randomly selected instance from in-batch data. We can set a high value for    \\alpha italic_  to reduce false positive samples. However, filtering out false negatives in synthetic data is more challenging. In theory, smaller    \\beta italic_  can reduce more false negatives, but samples with low similarity to the source instance are easy to distinguish due to significant surface-level differences. As a result, training on these samples does not effectively improve the models ability to distinguish fine-grained false positives. Therefore, we opt for a higher value of    \\beta italic_ . During training, we use a Gaussian-decayed function to align the distances of hard negative samples between the GCSE encoder  E E E italic_E  and the frozen encoder  E  superscript E  E^{\\prime} italic_E start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . As shown in Figure  4 , for each mini-batch of triplet inputs, both  E E E italic_E  and  E  superscript E  E^{\\prime} italic_E start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  compute similarity scores for the negative samples and their corresponding source instances. The loss for each instance in GCSE is defined as:",
            "Ablation Studies:  We analyze the impact of each module or strategy in our GCSE and report the results in Table  4 . First, w/o stage-2 refers to the results obtained without training in the second stage. This leads to a significant decrease in performance compared to the default model, which is the performance of the evaluation model and is similar to the conventional unsupervised SimCSE. Then, w randomly refers to the direct use of the instance itself as a positive sample in the combination dataset of domain and general data, while randomly selecting a negative instance from the dataset. We can observe that its performance in this case is even worse than the evaluation model. This demonstrates that the diversity of positive samples and the quality of negative samples significantly impact the performance of the model. w/o filtering indicates the results of training by skipping evaluation model filtering and directly using the data synthesized by LLM. The results show that the performance of the model is significantly affected when false positive and negative samples are introduced without filtering. We investigate the impact of the Gaussian-decayed function by removing it, and the results are shown in w/o decay. We can observe that the default model performs better overall than when the Gaussian-decayed function is removed, indicating that it can filter out potential false negative sample noise. Finally, we analyze the necessity of including general data and domain data in w/o general and w/o domain respectively. It can be observed that removing either of them results in a decline in performance, which indicates the significance of domain data and the essentiality of general data in our method.",
            "We employ the Gaussian-decayed function on SynCSE and sample SynCSE training data with a sample size the same as our synthetic data to evaluate the efficacy of the proposed Gaussian-decayed function and our domain-oriented selection strategy in the ablation experiment. The data sample size is 64k, and the weight of    \\sigma italic_  in  G  (  ) G  G(\\cdot) italic_G (  )  is assigned the same value as specified in Section  4.1 . The results of various policies implemented in SynCSE are presented in Table  9 . w sampled denotes the utilization of purely the sampled data in SynCSE, and a performance decrease can be observed when training on a reduced number of samples without extra configurations. w sampled & G.D. denotes the additional incorporation of  G  (  ) G  G(\\cdot) italic_G (  )  based on w sampled. w G.D. indicates the results by training on the full dataset utilizing  G  (  ) G  G(\\cdot) italic_G (  ) . In both configurations, the average performance outperforms the vanilla model, illustrating the modules efficacy. w sampled & domain & G.D. denotes the concurrent utilization of sample data, domain data, and  G  (  ) G  G(\\cdot) italic_G (  ) , with a sample size of 48k for the SynCSE dataset and 16k for the synthesized domain dataset. The results reveal that \"w sampled & domain & G.D.\" attains the second-best performance, suggesting that incorporating domain data can decrease the required training samples while enhancing model efficacy."
        ]
    },
    "id_table_5": {
        "caption": "Table 7:  Comparison of different sentence embedding models accuracy on transfer tasks.    \\spadesuit  : results from  Liu et al. [ 2023 ] ,    \\clubsuit  : results from  Wang et al. [ 2024 ] ,       {\\dagger}{\\dagger}   : results from  Zhang et al. [ 2023 ] . *: we reproduce the results with the officially released corpus from  Zhang et al. [ 2023 ] .",
        "table": "S4.T5.1",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Analysis of entities and quantities awareness:  We analyze GCSE awareness of entities and quantities by constructing a dataset using the data synthesis method in Section  3.1  on the STS-Benchmark development set. Then, the similarity scores of each triplet in the dataset are annotated by two supervised pre-trained models: sup-simcse-bert-large and sup-simcse-roberta-large. The final label is the average score of the similarity calculated by both models. We evaluate Spearmans correlation scores of GCSE and the other three strong baselines on the backbone of the BERT-base model, and the results are shown in Table  5 . Our GCSE achieves the best result and outperforms RankCSE by 14.03%. In this case, both SynCSE and GCSE achieve significant improvements over methods without LLM. This might be due to the similarity of the semantic representation space between the training set and the development set, both of which are synthesized via LLM. Nevertheless, GCSE shows a notable enhancement in performance of 2.19% compared to SynCSE, demonstrating that its understanding of the entities and quantities in sentences has enhanced to a certain degree."
        ]
    },
    "id_table_6": {
        "caption": "Table 8:  Case studies on model prediction similarity with gold labels in the STS-Benchmark development set, where Gold represents the label score of the sentence pair (ranging from zero to five). The similarity scores of all models are multiplied by a coefficient of five for better comparison, and the value in parentheses denotes the RMS error between the predicted score and the label. Words highlighted in blue denote the entity alteration in the sentence-pair, whereas words in yellow indicate the quantities that change inside the sentence-pair.",
        "table": "A1.T6.20.20",
        "footnotes": [],
        "references": [
            "Impact on the ratio between domain and general data:  Figure  6  presents the trend of the GCSE Spearmans correlation result as the proportion of general data introduced increases, where d represents that only using the domain data. The results show that adding a certain amount of general data improves performance on STS tasks. However, when the size of general data exceeds three times that of domain data, performance starts to decline. This suggests that incorporating a moderate amount of external data enhances the uniformity of sentence embeddings. But as the out-of-domain data grows, the influence of domain-specific data on training weakens. Overall, the results indicate that domain data improves the models ability to represent target domain sentences, while general data helps with sentence embedding uniformity.",
            "Impact of the Gaussian-decayed:  To further investigate the effectiveness of the Gaussian-decayed function, we analyze the GCSE performance against the weight of    \\sigma italic_  on the synthesized data, both with and without filtering. As shown in Figure  6 , we use the synthesized data without filtering to evaluate the efficacy of the Gaussian-decayed function in eliminating false negative samples, and results are presented in Figure  6  (b). It is clear that the models performance improves as the weight of    \\sigma italic_  grows. This suggests that a greater    \\sigma italic_  weight enhances the models effectiveness in mitigating the impact of false negative samples. It is important to acknowledge that a higher    \\sigma italic_  does not necessarily indicate better performance. As shown in Figure  6  (a), an increase in    \\sigma italic_  at the initial stage contributes to enhancing the models performance. Nevertheless, as the weight of    \\sigma italic_  increases, the performance of backbones generally declines, resulting in the model adhering too strictly to the established guidelines. Consequently, it impacts the efficacy of learning from the hard negative samples. We further use the density plots to visualize the prediction on the STS-Benchmark development set in Figure  7 . These models are trained on the synthesized data without filtering. We can observe that in Figure  7  (a), the distribution of prediction results for labels   4 absent 4 \\geq 4  4  is significantly shifted to the left. Compared with the results in Figure  7  (b), this issue is effectively alleviated, demonstrating the effectiveness of the Gaussian-decayed function in reducing the influence of false negative samples. To further verify the applicability of the Gaussian-decayed function, we applied it to SynCSE and verified the performance in Appendix  D .",
            "In this section, we provide the specifics of our prompts for knowledge extraction and integration, and data synthesis. The particular prompts are presented in Table  6 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 9:  Ablation studies of sample size and the Gaussian-decayed function by utilizing SynCSE. *: we reproduce the results with the officially released corpus from  Zhang et al. [ 2023 ] .",
        "table": "A2.T7.18.18",
        "footnotes": [
            ""
        ],
        "references": [
            "Impact of the Gaussian-decayed:  To further investigate the effectiveness of the Gaussian-decayed function, we analyze the GCSE performance against the weight of    \\sigma italic_  on the synthesized data, both with and without filtering. As shown in Figure  6 , we use the synthesized data without filtering to evaluate the efficacy of the Gaussian-decayed function in eliminating false negative samples, and results are presented in Figure  6  (b). It is clear that the models performance improves as the weight of    \\sigma italic_  grows. This suggests that a greater    \\sigma italic_  weight enhances the models effectiveness in mitigating the impact of false negative samples. It is important to acknowledge that a higher    \\sigma italic_  does not necessarily indicate better performance. As shown in Figure  6  (a), an increase in    \\sigma italic_  at the initial stage contributes to enhancing the models performance. Nevertheless, as the weight of    \\sigma italic_  increases, the performance of backbones generally declines, resulting in the model adhering too strictly to the established guidelines. Consequently, it impacts the efficacy of learning from the hard negative samples. We further use the density plots to visualize the prediction on the STS-Benchmark development set in Figure  7 . These models are trained on the synthesized data without filtering. We can observe that in Figure  7  (a), the distribution of prediction results for labels   4 absent 4 \\geq 4  4  is significantly shifted to the left. Compared with the results in Figure  7  (b), this issue is effectively alleviated, demonstrating the effectiveness of the Gaussian-decayed function in reducing the influence of false negative samples. To further verify the applicability of the Gaussian-decayed function, we applied it to SynCSE and verified the performance in Appendix  D .",
            "We also evaluate our GCSE follow the same settings as SimCSE on seven transfer tasks: MR  [Pang and Lee,  2005 ] , CR  [Hu and Liu,  2004 ] , SUBJ  [Pang and Lee,  2004 ] , MPQA  [Wiebe et al.,  2005 ] , SST2  [Socher et al.,  2013 ] , TREC  [Voorhees and Tice,  2000 ] , and MRPC  [Voorhees and Tice,  2000 ] . The results are shown in Table  7 , it can be observed that our GCSE achieve best performance on BERT-base, BERT-large and RoBERTa-large. In RoBERTa-base, our method also shows comparable results, demonstrating the potential capability in downstream tasks."
        ]
    },
    "id_table_8": {
        "caption": "Table 10:  Performance comparison of different LLMs on STS tasks. : results from  Miao et al. [ 2023 ] ,    \\clubsuit  : results from  Wang et al. [ 2024 ]",
        "table": "A3.T8.1.1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "To further verify the improvement in our methods awareness of entity and quantity, we selected five sample sets from the STS-Benchmark development set that explicitly contained alterations in entity or quantity within the sentence-pair, and presented the prediction cosine-similarity scores of GCSE and related methodologies with the backbone of BERT-base in Table  8 . We can observe from the results that the prediction score of our model achieves the minimum root-mean-square error compared to the label in most cases, which indicates that our model has a stronger capacity to distinguish information."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A4.T9.1.1",
        "footnotes": [],
        "references": [
            "We employ the Gaussian-decayed function on SynCSE and sample SynCSE training data with a sample size the same as our synthetic data to evaluate the efficacy of the proposed Gaussian-decayed function and our domain-oriented selection strategy in the ablation experiment. The data sample size is 64k, and the weight of    \\sigma italic_  in  G  (  ) G  G(\\cdot) italic_G (  )  is assigned the same value as specified in Section  4.1 . The results of various policies implemented in SynCSE are presented in Table  9 . w sampled denotes the utilization of purely the sampled data in SynCSE, and a performance decrease can be observed when training on a reduced number of samples without extra configurations. w sampled & G.D. denotes the additional incorporation of  G  (  ) G  G(\\cdot) italic_G (  )  based on w sampled. w G.D. indicates the results by training on the full dataset utilizing  G  (  ) G  G(\\cdot) italic_G (  ) . In both configurations, the average performance outperforms the vanilla model, illustrating the modules efficacy. w sampled & domain & G.D. denotes the concurrent utilization of sample data, domain data, and  G  (  ) G  G(\\cdot) italic_G (  ) , with a sample size of 48k for the SynCSE dataset and 16k for the synthesized domain dataset. The results reveal that \"w sampled & domain & G.D.\" attains the second-best performance, suggesting that incorporating domain data can decrease the required training samples while enhancing model efficacy."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A5.T10.3",
        "footnotes": [],
        "references": [
            "In this section, we utilize contrastive learning on ChatGLM-3 with a low-rank adapter (LoRA) layer, and we gather other sentence embedding results from LLMs via in-context learning (ICL) to evaluate the alignment of LLM-generated similarities with the gold labels, as presented in Table  10 . The results indicate that even ChatGPT with ICL cannot surpass the performance of contrastive learning methods based on encoder models. The existing methodology of LLM for generating sentence embeddings is unsatisfactory. Although some works utilize prompts without extra training, the efficacy of sentence embedding primarily relies on the quality of prompt engineering. Furthermore, employing the contrastive learning fine-tuning method for LLMs achieves no substantial benefit in semantic textual similarity tasks, underutilizing the capabilities of LLMs. To reduce expenses, we assert that fully leveraging the capabilities of LLMs for distilling smaller models is the better option."
        ]
    }
}