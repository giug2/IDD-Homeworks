{
    "id_table_1": {
        "caption": "Table 1:  Multimodal dataset statistics.",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "To achieve high quality  A m  m subscript A m m A_{mm} italic_A start_POSTSUBSCRIPT italic_m italic_m end_POSTSUBSCRIPT , we propose MuRAR, as illustrated in Figure  1 .  The MuRAR framework consists of three main components: text answer generation (Section  2.1 ), source-based multimodal retrieval (Section  2.2 ), and multimodal answer refinement (Section  2.3 ).  The text answer generation component uses a RAG approach  Lewis et al. ( 2020 ); Gao et al. ( 2023 ) , first retrieving relevant text document snippets  S = { s 1 , s 2 , ... , s n }  D S S subscript s 1 subscript s 2 ... subscript s n subscript D S S=\\{s_{1},s_{2},...,s_{n}\\}\\in D_{S} italic_S = { italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }  italic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  based on user query  q q q italic_q  and then generating the text answer  A t subscript A t A_{t} italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  by prompting an LLM.   2 2 todo:  2 briefly explain the source attribution   Then, we apply source attribution on  A t subscript A t A_{t} italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  to identify the text answer snippets  a [ i , j ]  A t subscript a i j subscript A t a_{[i,j]}\\in A_{t} italic_a start_POSTSUBSCRIPT [ italic_i , italic_j ] end_POSTSUBSCRIPT  italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , where a text answer snippet is a continuous text span in  A t subscript A t A_{t} italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .  Next we retrieve multimodal data, namely,  I = { i 1 , i 2 , ... , i m }  D I I subscript i 1 subscript i 2 ... subscript i m subscript D I I=\\{i_{1},i_{2},...,i_{m}\\}\\in D_{I} italic_I = { italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_i start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }  italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ,  T = { t 1 , t 2 , ... , t k }  D T T subscript t 1 subscript t 2 ... subscript t k subscript D T T=\\{t_{1},t_{2},...,t_{k}\\}\\in D_{T} italic_T = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }  italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , and  V = { v 1 , v 2 , ... , v l }  D V V subscript v 1 subscript v 2 ... subscript v l subscript D V V=\\{v_{1},v_{2},...,v_{l}\\}\\in D_{V} italic_V = { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_v start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT }  italic_D start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT  that are relevant to the text answer snippets  a [ i , j ] subscript a i j a_{[i,j]} italic_a start_POSTSUBSCRIPT [ italic_i , italic_j ] end_POSTSUBSCRIPT .  Finally, the multimodal answer refinement component generates the final multimodal answer  A m  m subscript A m m A_{mm} italic_A start_POSTSUBSCRIPT italic_m italic_m end_POSTSUBSCRIPT  by incorporating the retrieved multimodal data and text response snippets.",
            "The multimodal dataset was curated from 2,173 documentation pages from Adobe Experience League  2 2 2 https://experienceleague.adobe.com/ .  The dataset, as shown in Table  1 , encompasses four primary modalities: text, images, tables, and videos, along with their associated metadata such as contextual text and URLs. The textual data includes both pure text and tabular content, while the visual data consists of images and videos. See the design details in   A.1  for more information on data collection."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Evaluation results for multimodal answers generated by GPT-3.5 and GPT-4.",
        "table": "S5.T2.1",
        "footnotes": [],
        "references": [
            "To achieve high quality  A m  m subscript A m m A_{mm} italic_A start_POSTSUBSCRIPT italic_m italic_m end_POSTSUBSCRIPT , we propose MuRAR, as illustrated in Figure  1 .  The MuRAR framework consists of three main components: text answer generation (Section  2.1 ), source-based multimodal retrieval (Section  2.2 ), and multimodal answer refinement (Section  2.3 ).  The text answer generation component uses a RAG approach  Lewis et al. ( 2020 ); Gao et al. ( 2023 ) , first retrieving relevant text document snippets  S = { s 1 , s 2 , ... , s n }  D S S subscript s 1 subscript s 2 ... subscript s n subscript D S S=\\{s_{1},s_{2},...,s_{n}\\}\\in D_{S} italic_S = { italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }  italic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  based on user query  q q q italic_q  and then generating the text answer  A t subscript A t A_{t} italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  by prompting an LLM.   2 2 todo:  2 briefly explain the source attribution   Then, we apply source attribution on  A t subscript A t A_{t} italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  to identify the text answer snippets  a [ i , j ]  A t subscript a i j subscript A t a_{[i,j]}\\in A_{t} italic_a start_POSTSUBSCRIPT [ italic_i , italic_j ] end_POSTSUBSCRIPT  italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , where a text answer snippet is a continuous text span in  A t subscript A t A_{t} italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .  Next we retrieve multimodal data, namely,  I = { i 1 , i 2 , ... , i m }  D I I subscript i 1 subscript i 2 ... subscript i m subscript D I I=\\{i_{1},i_{2},...,i_{m}\\}\\in D_{I} italic_I = { italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_i start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }  italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ,  T = { t 1 , t 2 , ... , t k }  D T T subscript t 1 subscript t 2 ... subscript t k subscript D T T=\\{t_{1},t_{2},...,t_{k}\\}\\in D_{T} italic_T = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }  italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , and  V = { v 1 , v 2 , ... , v l }  D V V subscript v 1 subscript v 2 ... subscript v l subscript D V V=\\{v_{1},v_{2},...,v_{l}\\}\\in D_{V} italic_V = { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_v start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT }  italic_D start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT  that are relevant to the text answer snippets  a [ i , j ] subscript a i j a_{[i,j]} italic_a start_POSTSUBSCRIPT [ italic_i , italic_j ] end_POSTSUBSCRIPT .  Finally, the multimodal answer refinement component generates the final multimodal answer  A m  m subscript A m m A_{mm} italic_A start_POSTSUBSCRIPT italic_m italic_m end_POSTSUBSCRIPT  by incorporating the retrieved multimodal data and text response snippets.",
            "As shown in Figure  2 , when a user queries AI assistant, for instance, asking What is a good tutorial for creating a schema?, the process begins with the users query (A). The MuRAR first generates a text answer based on the text snippet retrieval (B), and recognizes text answer snippets by source attribution (C).",
            "The psychometric evaluation focused on three key aspects: usefulness, readability, and relevance. As shown in Table  2 , when examine the results generated by both GPT-3.5 and GPT-4, the usefulness metric achieved an average score of 3.47, while readability and relevance scored 3.63 and 3.78, respectively. These scores, all above the midpoint of the scale as reflected in Figure  3 , suggest that our approach performs well in producing useful, readable, and relevant output. Qualitative feedback from annotators further supports this conclusion, indicating that the multi-modal generation provides informative additions to the text, is understandable through its placement, and remains relevant to the associated content. In addition, the average preference rating of 0.86 demonstrates a strong overall preference for our method compared to the text-only alternative. When comparing GPT-3.5 with GPT-4, we found that using GPT-4 as the backbone LLM increased all the metrics, showcasing its superior performance in generating high-quality, multi-modal content within our framework."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Per-annotator average scores and preference.",
        "table": "A1.T3.1",
        "footnotes": [],
        "references": [
            "To achieve high quality  A m  m subscript A m m A_{mm} italic_A start_POSTSUBSCRIPT italic_m italic_m end_POSTSUBSCRIPT , we propose MuRAR, as illustrated in Figure  1 .  The MuRAR framework consists of three main components: text answer generation (Section  2.1 ), source-based multimodal retrieval (Section  2.2 ), and multimodal answer refinement (Section  2.3 ).  The text answer generation component uses a RAG approach  Lewis et al. ( 2020 ); Gao et al. ( 2023 ) , first retrieving relevant text document snippets  S = { s 1 , s 2 , ... , s n }  D S S subscript s 1 subscript s 2 ... subscript s n subscript D S S=\\{s_{1},s_{2},...,s_{n}\\}\\in D_{S} italic_S = { italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }  italic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  based on user query  q q q italic_q  and then generating the text answer  A t subscript A t A_{t} italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  by prompting an LLM.   2 2 todo:  2 briefly explain the source attribution   Then, we apply source attribution on  A t subscript A t A_{t} italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  to identify the text answer snippets  a [ i , j ]  A t subscript a i j subscript A t a_{[i,j]}\\in A_{t} italic_a start_POSTSUBSCRIPT [ italic_i , italic_j ] end_POSTSUBSCRIPT  italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , where a text answer snippet is a continuous text span in  A t subscript A t A_{t} italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .  Next we retrieve multimodal data, namely,  I = { i 1 , i 2 , ... , i m }  D I I subscript i 1 subscript i 2 ... subscript i m subscript D I I=\\{i_{1},i_{2},...,i_{m}\\}\\in D_{I} italic_I = { italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_i start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }  italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ,  T = { t 1 , t 2 , ... , t k }  D T T subscript t 1 subscript t 2 ... subscript t k subscript D T T=\\{t_{1},t_{2},...,t_{k}\\}\\in D_{T} italic_T = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }  italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , and  V = { v 1 , v 2 , ... , v l }  D V V subscript v 1 subscript v 2 ... subscript v l subscript D V V=\\{v_{1},v_{2},...,v_{l}\\}\\in D_{V} italic_V = { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_v start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT }  italic_D start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT  that are relevant to the text answer snippets  a [ i , j ] subscript a i j a_{[i,j]} italic_a start_POSTSUBSCRIPT [ italic_i , italic_j ] end_POSTSUBSCRIPT .  Finally, the multimodal answer refinement component generates the final multimodal answer  A m  m subscript A m m A_{mm} italic_A start_POSTSUBSCRIPT italic_m italic_m end_POSTSUBSCRIPT  by incorporating the retrieved multimodal data and text response snippets.",
            "The psychometric evaluation focused on three key aspects: usefulness, readability, and relevance. As shown in Table  2 , when examine the results generated by both GPT-3.5 and GPT-4, the usefulness metric achieved an average score of 3.47, while readability and relevance scored 3.63 and 3.78, respectively. These scores, all above the midpoint of the scale as reflected in Figure  3 , suggest that our approach performs well in producing useful, readable, and relevant output. Qualitative feedback from annotators further supports this conclusion, indicating that the multi-modal generation provides informative additions to the text, is understandable through its placement, and remains relevant to the associated content. In addition, the average preference rating of 0.86 demonstrates a strong overall preference for our method compared to the text-only alternative. When comparing GPT-3.5 with GPT-4, we found that using GPT-4 as the backbone LLM increased all the metrics, showcasing its superior performance in generating high-quality, multi-modal content within our framework.",
            "Due to space constraints, we include additional human evaluation results in the Appendix. The average score and preference per annotator are presented in Table  3 . The standard deviation of evaluation metrics for multimodal answers generated by GPT-3.5 and GPT-4 is detailed in Table  4 . Overall inter-annotator agreement, along with specific agreements for different models and metrics, can be found in Table  5 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Standard deviation of evaluation metrics for multimodal answers generated by GPT-3.5 and GPT-4.",
        "table": "A1.T4.1",
        "footnotes": [],
        "references": [
            "Our text answer generation follows the RAG  Lewis et al. ( 2020 )  style method.  Given user queries, we fine-tune a pre-trained text embedding model  Reimers and Gurevych ( 2019 )  on an annotated dataset, and apply this model to embed text snippets which are stored in a vector index.  The index is used to retrieve relevant text document snippets  S S S italic_S  based on the user query  q q q italic_q , for which the top  5 5 5 5  text document snippets are selected.  The LLM is then prompted with the user queries and the retrieved text snippets to generate answer  A t subscript A t A_{t} italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , the text prompt can be found in Appendix  A.4 .",
            "Once the relevant multimodal responses are retrieved, the LLM is prompted to generate the final answer.  This process involves providing the LLM with the user question, initial text answer, and initial multimodal response along with their context information.  For each multimodal response, placeholders are added to the initial text answer at the corresponding source locations. Each placeholder includes the multimodal data and its context information, ensuring that the LLM accurately integrates relevant content without generating irrelevant details. Additionally, a few example responses are provided to guide the model in producing higher quality and more consistent outputs.  The final prompt is formulated by concatenating the user question, modified initial text answer, multimodal data, their context information, and example responses. The prompt example can be found in Appendix  A.4 .",
            "Due to space constraints, we include additional human evaluation results in the Appendix. The average score and preference per annotator are presented in Table  3 . The standard deviation of evaluation metrics for multimodal answers generated by GPT-3.5 and GPT-4 is detailed in Table  4 . Overall inter-annotator agreement, along with specific agreements for different models and metrics, can be found in Table  5 .",
            "The prompt for text answer generation can be found in Figure  4 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Inter-annotator agreement.",
        "table": "A1.T5.3",
        "footnotes": [],
        "references": [
            "We calculated two inter-annotator agreement measures: Krippendorffs alpha (0.4179, moderate agreement) and Cohens kappa between top annotators (0.71, substantial agreement). The lower Krippendorffs alphas can be explained through the annotator-specific analysis (Table  5 ), which revealed lower average scores for Annotators 2 (3.0619, SD = 0.5648, 26% of annotations) and 4 (3.3836, SD = 0.253, 5.8% of annotations) compared to others.",
            "Due to space constraints, we include additional human evaluation results in the Appendix. The average score and preference per annotator are presented in Table  3 . The standard deviation of evaluation metrics for multimodal answers generated by GPT-3.5 and GPT-4 is detailed in Table  4 . Overall inter-annotator agreement, along with specific agreements for different models and metrics, can be found in Table  5 .",
            "The prompt for multimodal answer refinement can be found in Figure  5 ."
        ]
    },
    "global_footnotes": [
        "https://experienceleague.adobe.com/en/docs/experience-platform/ai-assistant/home"
    ]
}