{
    "PAPER'S NUMBER OF TABLES": 18,
    "S4.T1": {
        "caption": "Table 1:  Computation time for Inverting Gradients running on a NVIDIA A100 80GB GPU. Small batches have more total batches and take more time for the whole dataset. Computation time is significantly higher for CIFAR-10 given the more complex model.",
        "table": "<table id=\"S4.T1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" colspan=\"4\"><span id=\"S4.T1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">MNIST</span></th>\n</tr>\n<tr id=\"S4.T1.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Batch size</span></th>\n<th id=\"S4.T1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Model</span></th>\n<th id=\"S4.T1.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T1.2.2.2.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T1.2.2.2.3.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.2.2.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T1.2.2.2.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Time per</span></td>\n</tr>\n<tr id=\"S4.T1.2.2.2.3.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.2.2.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T1.2.2.2.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">batch (s)</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T1.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T1.2.2.2.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T1.2.2.2.4.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.2.2.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T1.2.2.2.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Time for entire</span></td>\n</tr>\n<tr id=\"S4.T1.2.2.2.4.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.2.2.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T1.2.2.2.4.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">dataset (days)</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">8</span></th>\n<td id=\"S4.T1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T1.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">DNN</span></td>\n<td id=\"S4.T1.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">54.79</span></td>\n<td id=\"S4.T1.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.3.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">4.76</span></td>\n</tr>\n<tr id=\"S4.T1.2.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><span id=\"S4.T1.2.4.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">16</span></th>\n<td id=\"S4.T1.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">62.34</span></td>\n<td id=\"S4.T1.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.2.4.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">2.71</span></td>\n</tr>\n<tr id=\"S4.T1.2.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><span id=\"S4.T1.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">32</span></th>\n<td id=\"S4.T1.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.2.5.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">64.32</span></td>\n<td id=\"S4.T1.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.2.5.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">1.40</span></td>\n</tr>\n<tr id=\"S4.T1.2.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.6.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" colspan=\"4\"><span id=\"S4.T1.2.6.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">CIFAR-10</span></th>\n</tr>\n<tr id=\"S4.T1.2.7.5\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.7.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Batch size</span></th>\n<th id=\"S4.T1.2.7.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.7.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Model</span></th>\n<th id=\"S4.T1.2.7.5.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T1.2.7.5.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T1.2.7.5.3.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.7.5.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T1.2.7.5.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Time per</span></td>\n</tr>\n<tr id=\"S4.T1.2.7.5.3.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.7.5.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T1.2.7.5.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">batch (s)</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T1.2.7.5.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T1.2.7.5.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T1.2.7.5.4.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.7.5.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T1.2.7.5.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Time for entire</span></td>\n</tr>\n<tr id=\"S4.T1.2.7.5.4.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.7.5.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T1.2.7.5.4.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">dataset (days)</span></td>\n</tr>\n</table>\n</th>\n</tr>\n<tr id=\"S4.T1.2.8.6\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.8.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.8.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">4</span></th>\n<td id=\"S4.T1.2.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T1.2.8.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">ResNet-18</span></td>\n<td id=\"S4.T1.2.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.8.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">422.81</span></td>\n<td id=\"S4.T1.2.8.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.8.6.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">61.17</span></td>\n</tr>\n<tr id=\"S4.T1.2.9.7\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.9.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><span id=\"S4.T1.2.9.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">8</span></th>\n<td id=\"S4.T1.2.9.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.2.9.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">437.54</span></td>\n<td id=\"S4.T1.2.9.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.2.9.7.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">31.65</span></td>\n</tr>\n<tr id=\"S4.T1.2.10.8\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.10.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"><span id=\"S4.T1.2.10.8.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">16</span></th>\n<td id=\"S4.T1.2.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T1.2.10.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">435.33</span></td>\n<td id=\"S4.T1.2.10.8.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T1.2.10.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">15.75</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 1 shows the amount of time taken by Inverting Gradients run on a NVIDIA A100 80GB GPU for MNIST and CIFAR-10 on a DNN and ResNet-18 respectively. There is some variance in the average amount of time based on batch size, but when attacking the entire dataset, smaller batch sizes always take longer time as the number of total batches is much larger. CIFAR-10 takes significantly longer to reconstruct each batch given the more complex model of ResNet-18 compared to a DNN. For a batch size of 4, CIFAR-10 takes 422.81 seconds per batch. Iterating across the entire dataset on a single GPU takes 61.17 days."
        ]
    },
    "S4.T2": {
        "caption": "Table 2:  (a) Average PSNR↑↑\\uparrow / SSIM↑↑\\uparrow scores of gradient inversion (Inverting Gradients) reconstructions for various batch sizes and (b) number of leaked images/leakage rate of linear layer leakage attack (LOKI) for several FC layer sizes.",
        "table": "<table id=\"S4.T2.st1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.st1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.st1.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" colspan=\"3\"><span id=\"S4.T2.st1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">CIFAR-10</span></th>\n<th id=\"S4.T2.st1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\"><span id=\"S4.T2.st1.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">MNIST</span></th>\n</tr>\n<tr id=\"S4.T2.st1.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.st1.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Batch size</span></th>\n<th id=\"S4.T2.st1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">PSNR</span></th>\n<th id=\"S4.T2.st1.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">SSIM</span></th>\n<th id=\"S4.T2.st1.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Batch size</span></th>\n<th id=\"S4.T2.st1.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">PSNR</span></th>\n<th id=\"S4.T2.st1.2.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">SSIM</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.st1.2.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">4</span></td>\n<td id=\"S4.T2.st1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">27.88</span></td>\n<td id=\"S4.T2.st1.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.9067</span></td>\n<td id=\"S4.T2.st1.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">8</span></td>\n<td id=\"S4.T2.st1.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">23.16</span></td>\n<td id=\"S4.T2.st1.2.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.6996</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\"><span id=\"S4.T2.st1.2.4.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">8</span></td>\n<td id=\"S4.T2.st1.2.4.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">20.77</span></td>\n<td id=\"S4.T2.st1.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.4.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.7215</span></td>\n<td id=\"S4.T2.st1.2.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.4.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">16</span></td>\n<td id=\"S4.T2.st1.2.4.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">19.15</span></td>\n<td id=\"S4.T2.st1.2.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.4.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.6506</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r\"><span id=\"S4.T2.st1.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">16</span></td>\n<td id=\"S4.T2.st1.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T2.st1.2.5.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">15.94</span></td>\n<td id=\"S4.T2.st1.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.st1.2.5.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.3978</span></td>\n<td id=\"S4.T2.st1.2.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.st1.2.5.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">32</span></td>\n<td id=\"S4.T2.st1.2.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T2.st1.2.5.3.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">15.78</span></td>\n<td id=\"S4.T2.st1.2.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.st1.2.5.3.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.4537</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 2(a) gives the average PSNR and SSIM metrics (where a higher score indicates greater similarity with the ground truth) for the reconstructions across the entire dataset of CIFAR-10 and MNIST from Inverting Gradients. Increasing the batch size results in a lower PSNR and SSIM score for both CIFAR-10 and MNIST.",
            "Table 2(b) reports the number of leaked images for LOKI. For an FC factor of 444, LOKI leaks 78.93%percent78.9378.93\\%, 76.61%percent76.6176.61\\%, and 75.15%percent75.1575.15\\% of images on the CIFAR-10, MNIST, and Tiny ImageNet datasets. Increasing the FC layer size increases the model size overhead, but results in a higher leakage rate as images are more likely to activate different neurons. Decreasing the size creates a smaller model size overhead but results in lower leakage rate.",
            "Table 3(a) and Figure 2(a) show the results for CIFAR-10 for centralized training, federated learning (FedAvg), and training with leaked data. Centralized training achieves a 94.38%percent94.3894.38\\% accuracy and federated learning achieves a 72.76%percent72.7672.76\\% and 68.71%percent68.7168.71\\% peak accuracy with 10 and 50 clients respectively. Centralized and FL indeed provide the upper bound and the lower bound of the accuracy here. Across all attack settings, GI and LLL achieve higher model accuracy compared to federated learning. The reconstruction quality of GI is adversely affected by larger batch sizes and in turn also impacts the final model performance. With a batch size of 4, 8, and 16, models trained on the leaked data from GI achieves 90.34%percent90.3490.34\\%, 86.16%percent86.1686.16\\%, and 76.83%percent76.8376.83\\% accuracy. With even larger batch sizes, it is likely that the model performance will drop below federated learning. The performance of models using LLL data is more stable. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the accuracy only drops from 93.16%percent93.1693.16\\% to 88.86%percent88.8688.86\\%. This is spite of a large drop in the number of leaked images from 43788 (87.58%percent87.5887.58\\%) to 18242 (36.48%percent36.4836.48\\%) as shown in Table 2(b). This is a subtle and consequential result — the downstream training task can still be accomplished despite a big drop in the amount of data leakage. All prior work had stopped at the stage of evaluating the proportion of leakage and therefore had missed out on this insight.",
            "Quality of reconstruction is an important concern for GI attacks. As the batch size continues to increase, the reconstruction quality decreases. Table 2(a) also shows this relationship. From the previous experiments, we also see a negative relationship between the increasing batch size and the usefulness of the leaked data in downstream model training."
        ]
    },
    "S4.T2.st1": {
        "caption": "(a) Gradient Inversion PSNR/SSIM",
        "table": "<table id=\"S4.T2.st1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.st1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.st1.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" colspan=\"3\"><span id=\"S4.T2.st1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">CIFAR-10</span></th>\n<th id=\"S4.T2.st1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\"><span id=\"S4.T2.st1.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">MNIST</span></th>\n</tr>\n<tr id=\"S4.T2.st1.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.st1.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Batch size</span></th>\n<th id=\"S4.T2.st1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">PSNR</span></th>\n<th id=\"S4.T2.st1.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">SSIM</span></th>\n<th id=\"S4.T2.st1.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Batch size</span></th>\n<th id=\"S4.T2.st1.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">PSNR</span></th>\n<th id=\"S4.T2.st1.2.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">SSIM</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.st1.2.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">4</span></td>\n<td id=\"S4.T2.st1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">27.88</span></td>\n<td id=\"S4.T2.st1.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.9067</span></td>\n<td id=\"S4.T2.st1.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">8</span></td>\n<td id=\"S4.T2.st1.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">23.16</span></td>\n<td id=\"S4.T2.st1.2.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.6996</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\"><span id=\"S4.T2.st1.2.4.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">8</span></td>\n<td id=\"S4.T2.st1.2.4.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">20.77</span></td>\n<td id=\"S4.T2.st1.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.4.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.7215</span></td>\n<td id=\"S4.T2.st1.2.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.4.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">16</span></td>\n<td id=\"S4.T2.st1.2.4.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">19.15</span></td>\n<td id=\"S4.T2.st1.2.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.4.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.6506</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r\"><span id=\"S4.T2.st1.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">16</span></td>\n<td id=\"S4.T2.st1.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T2.st1.2.5.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">15.94</span></td>\n<td id=\"S4.T2.st1.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.st1.2.5.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.3978</span></td>\n<td id=\"S4.T2.st1.2.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.st1.2.5.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">32</span></td>\n<td id=\"S4.T2.st1.2.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T2.st1.2.5.3.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">15.78</span></td>\n<td id=\"S4.T2.st1.2.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.st1.2.5.3.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.4537</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 1 shows the amount of time taken by Inverting Gradients run on a NVIDIA A100 80GB GPU for MNIST and CIFAR-10 on a DNN and ResNet-18 respectively. There is some variance in the average amount of time based on batch size, but when attacking the entire dataset, smaller batch sizes always take longer time as the number of total batches is much larger. CIFAR-10 takes significantly longer to reconstruct each batch given the more complex model of ResNet-18 compared to a DNN. For a batch size of 4, CIFAR-10 takes 422.81 seconds per batch. Iterating across the entire dataset on a single GPU takes 61.17 days.",
            "Table 2(a) gives the average PSNR and SSIM metrics (where a higher score indicates greater similarity with the ground truth) for the reconstructions across the entire dataset of CIFAR-10 and MNIST from Inverting Gradients. Increasing the batch size results in a lower PSNR and SSIM score for both CIFAR-10 and MNIST.",
            "Table 2(b) reports the number of leaked images for LOKI. For an FC factor of 444, LOKI leaks 78.93%percent78.9378.93\\%, 76.61%percent76.6176.61\\%, and 75.15%percent75.1575.15\\% of images on the CIFAR-10, MNIST, and Tiny ImageNet datasets. Increasing the FC layer size increases the model size overhead, but results in a higher leakage rate as images are more likely to activate different neurons. Decreasing the size creates a smaller model size overhead but results in lower leakage rate.",
            "Table 3(a) and Figure 2(a) show the results for CIFAR-10 for centralized training, federated learning (FedAvg), and training with leaked data. Centralized training achieves a 94.38%percent94.3894.38\\% accuracy and federated learning achieves a 72.76%percent72.7672.76\\% and 68.71%percent68.7168.71\\% peak accuracy with 10 and 50 clients respectively. Centralized and FL indeed provide the upper bound and the lower bound of the accuracy here. Across all attack settings, GI and LLL achieve higher model accuracy compared to federated learning. The reconstruction quality of GI is adversely affected by larger batch sizes and in turn also impacts the final model performance. With a batch size of 4, 8, and 16, models trained on the leaked data from GI achieves 90.34%percent90.3490.34\\%, 86.16%percent86.1686.16\\%, and 76.83%percent76.8376.83\\% accuracy. With even larger batch sizes, it is likely that the model performance will drop below federated learning. The performance of models using LLL data is more stable. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the accuracy only drops from 93.16%percent93.1693.16\\% to 88.86%percent88.8688.86\\%. This is spite of a large drop in the number of leaked images from 43788 (87.58%percent87.5887.58\\%) to 18242 (36.48%percent36.4836.48\\%) as shown in Table 2(b). This is a subtle and consequential result — the downstream training task can still be accomplished despite a big drop in the amount of data leakage. All prior work had stopped at the stage of evaluating the proportion of leakage and therefore had missed out on this insight.",
            "Table 3(b) and Figure 2(b) show the results for MNIST. LLL performs better than the FedAvg baseline in all cases. With an FC size factor of 8, LOKI achieves a 98.82%percent98.8298.82\\% test accuracy (nearly the same regardless of the FC size factor), only 0.07%percent0.070.07\\% lower than centralized at 98.89%percent98.8998.89\\%. GI performs slightly worse than federated learning, achieving 95.96%percent95.9695.96\\%, 95.50%percent95.5095.50\\%, and 94.29%percent94.2994.29\\% at batch sizes 8, 16, and 32 compared to 96.17%percent96.1796.17\\% and 96.18%percent96.1896.18\\% with 10 and 50 clients in FedAvg. We believe this is because the noise in the reconstructions hampers model performance with GI. This affects MNIST more severely than for CIFAR because the MNIST images are more sparse and sensitive to noise.",
            "The top-1 validation accuracy for Tiny ImageNet are given in Table 3(c) and Figure 2(c). The total number of leaked images hurts LLL more here than in the other two datasets. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the validation accuracy drops from 46.70%percent46.7046.70\\% to 35.20%percent35.2035.20\\%, an 11.5%percent11.511.5\\% drop in performance. Federated learning achieves 37.00%percent37.0037.00\\% and 35.06%percent35.0635.06\\% accuracy with 10 and 50 clients respectively. FL with 10 clients has slightly better accuracy than LLL with an FC size factor of 1. However, any LLL setting with a larger FC layer achieves higher accuracy than both settings in federated learning.",
            "Table 4 shows the leakage rate of LOKI in FedAvg compared to FedSGD. Using CSF=500CSF500\\textit{CSF}=500, the leakage rate of LOKI in FedAvg is substantially higher than the leakage rates in the FedSGD setting. Using an FC layer of half the size, the leakage rate in FedAvg is comparable to that of FedSGD. For example, LOKI FedAvg factor 1 achieves a leakage rate of 58.39%percent58.3958.39\\% compared to LOKI FedSGD factor 2 which achieves a leakage rate of 59.76%percent59.7659.76\\%.",
            "Table 5 shows the testing accuracy of LOKI FedAvg for the different FC layer sizes. The test accuracies are very comparable to the FedSGD settings. However, with a smaller FC layer size factor, the effect of having a larger total number of leaked images becomes visible. With FC factor 1, LOKI FedAvg achieves 91.11%percent91.1191.11\\% accuracy while LOKI FedSGD achieves 88.86%percent88.8688.86\\%.",
            "Quality of reconstruction is an important concern for GI attacks. As the batch size continues to increase, the reconstruction quality decreases. Table 2(a) also shows this relationship. From the previous experiments, we also see a negative relationship between the increasing batch size and the usefulness of the leaked data in downstream model training.",
            "Here, we explore whether poor quality reconstructions are still useful for training models. Using the CIFAR-10 leaked dataset created using Inverting Gradients on a batch size of 16, we first sort the reconstructions by their PSNR value. Our first set of experiments removes the worst images from training. Table 6(a) shows the percent of images above the PSNR threshold and the final test accuracy when trained on that data. Compared to the baseline accuracy of 76.83%percent76.8376.83\\% achieved by using the entire leaked dataset, even removing the worst images with a PSNR <12absent12<12 from the training set results in a small but non-zero performance loss.\nThen we take a different look at this question by now training on the worst images.\nTable 6(b) shows the test accuracy of models trained on all images below a PSNR threshold. Even when training on the lowest quality images with a PSNR <12absent12<12, the model achieves a 45.05%percent45.0545.05\\% final testing accuracy, significantly worse than the baseline, but much above zero.",
            "While some reconstructions have very low quality, only images of the same label will be swapped within the batch during label matching. This is a desirable property for training on leaked data. As long as the set of labels is leaked correctly, the optimization process will match images to a correct label.\nSo even though the image quality is poor (and can be entirely unrecognizable), since the label is correctly matched, such an image still does help in the training (this buttresses the message from Tables 6(a) and 6(b)). Naturally, as the PSNR goes down, the image helps less and less."
        ]
    },
    "S4.T2.st2": {
        "caption": "(b) Linear layer attack leakage rate",
        "table": "<table id=\"S4.T2.st2.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.st2.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S4.T2.st2.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\"><span id=\"S4.T2.st2.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">CIFAR-10</span></th>\n<th id=\"S4.T2.st2.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\"><span id=\"S4.T2.st2.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">MNIST</span></th>\n<th id=\"S4.T2.st2.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\"><span id=\"S4.T2.st2.2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Tiny ImageNet</span></th>\n</tr>\n<tr id=\"S4.T2.st2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">\n<table id=\"S4.T2.st2.2.2.2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T2.st2.2.2.2.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.st2.2.2.2.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S4.T2.st2.2.2.2.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FC</span></td>\n</tr>\n<tr id=\"S4.T2.st2.2.2.2.1.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.st2.2.2.2.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S4.T2.st2.2.2.2.1.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">factor</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T2.st2.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<table id=\"S4.T2.st2.2.2.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T2.st2.2.2.2.2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.st2.2.2.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T2.st2.2.2.2.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Leaked</span></td>\n</tr>\n<tr id=\"S4.T2.st2.2.2.2.2.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.st2.2.2.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T2.st2.2.2.2.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">images</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T2.st2.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T2.st2.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">Percent</span></th>\n<th id=\"S4.T2.st2.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<table id=\"S4.T2.st2.2.2.2.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T2.st2.2.2.2.4.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.st2.2.2.2.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T2.st2.2.2.2.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Leaked</span></td>\n</tr>\n<tr id=\"S4.T2.st2.2.2.2.4.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.st2.2.2.2.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T2.st2.2.2.2.4.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">images</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T2.st2.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T2.st2.2.2.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">Percent</span></th>\n<th id=\"S4.T2.st2.2.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<table id=\"S4.T2.st2.2.2.2.6.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T2.st2.2.2.2.6.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.st2.2.2.2.6.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T2.st2.2.2.2.6.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Leaked</span></td>\n</tr>\n<tr id=\"S4.T2.st2.2.2.2.6.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.st2.2.2.2.6.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T2.st2.2.2.2.6.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">images</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T2.st2.2.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T2.st2.2.2.2.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">Percent</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.st2.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T2.st2.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">8</span></th>\n<td id=\"S4.T2.st2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st2.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">43788</span></td>\n<td id=\"S4.T2.st2.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.st2.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">87.58</span></td>\n<td id=\"S4.T2.st2.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st2.2.3.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">52548</span></td>\n<td id=\"S4.T2.st2.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.st2.2.3.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">87.58</span></td>\n<td id=\"S4.T2.st2.2.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st2.2.3.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">85804</span></td>\n<td id=\"S4.T2.st2.2.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.st2.2.3.1.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">85.80</span></td>\n</tr>\n<tr id=\"S4.T2.st2.2.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><span id=\"S4.T2.st2.2.4.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">4</span></th>\n<td id=\"S4.T2.st2.2.4.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">39464</span></td>\n<td id=\"S4.T2.st2.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st2.2.4.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">78.93</span></td>\n<td id=\"S4.T2.st2.2.4.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.4.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">45966</span></td>\n<td id=\"S4.T2.st2.2.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st2.2.4.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.61</span></td>\n<td id=\"S4.T2.st2.2.4.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.4.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">75149</span></td>\n<td id=\"S4.T2.st2.2.4.2.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st2.2.4.2.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.15</span></td>\n</tr>\n<tr id=\"S4.T2.st2.2.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><span id=\"S4.T2.st2.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">2</span></th>\n<td id=\"S4.T2.st2.2.5.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.5.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">29882</span></td>\n<td id=\"S4.T2.st2.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st2.2.5.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">59.76</span></td>\n<td id=\"S4.T2.st2.2.5.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.5.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">35795</span></td>\n<td id=\"S4.T2.st2.2.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st2.2.5.3.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">59.66</span></td>\n<td id=\"S4.T2.st2.2.5.3.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.5.3.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">58012</span></td>\n<td id=\"S4.T2.st2.2.5.3.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st2.2.5.3.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">58.01</span></td>\n</tr>\n<tr id=\"S4.T2.st2.2.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.2.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"><span id=\"S4.T2.st2.2.6.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">1</span></th>\n<td id=\"S4.T2.st2.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T2.st2.2.6.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">18242</span></td>\n<td id=\"S4.T2.st2.2.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.st2.2.6.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">36.48</span></td>\n<td id=\"S4.T2.st2.2.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T2.st2.2.6.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">21815</span></td>\n<td id=\"S4.T2.st2.2.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.st2.2.6.4.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">36.36</span></td>\n<td id=\"S4.T2.st2.2.6.4.6\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T2.st2.2.6.4.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">35393</span></td>\n<td id=\"S4.T2.st2.2.6.4.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.st2.2.6.4.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">35.39</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 1 shows the amount of time taken by Inverting Gradients run on a NVIDIA A100 80GB GPU for MNIST and CIFAR-10 on a DNN and ResNet-18 respectively. There is some variance in the average amount of time based on batch size, but when attacking the entire dataset, smaller batch sizes always take longer time as the number of total batches is much larger. CIFAR-10 takes significantly longer to reconstruct each batch given the more complex model of ResNet-18 compared to a DNN. For a batch size of 4, CIFAR-10 takes 422.81 seconds per batch. Iterating across the entire dataset on a single GPU takes 61.17 days.",
            "Table 2(a) gives the average PSNR and SSIM metrics (where a higher score indicates greater similarity with the ground truth) for the reconstructions across the entire dataset of CIFAR-10 and MNIST from Inverting Gradients. Increasing the batch size results in a lower PSNR and SSIM score for both CIFAR-10 and MNIST.",
            "Table 2(b) reports the number of leaked images for LOKI. For an FC factor of 444, LOKI leaks 78.93%percent78.9378.93\\%, 76.61%percent76.6176.61\\%, and 75.15%percent75.1575.15\\% of images on the CIFAR-10, MNIST, and Tiny ImageNet datasets. Increasing the FC layer size increases the model size overhead, but results in a higher leakage rate as images are more likely to activate different neurons. Decreasing the size creates a smaller model size overhead but results in lower leakage rate.",
            "Table 3(a) and Figure 2(a) show the results for CIFAR-10 for centralized training, federated learning (FedAvg), and training with leaked data. Centralized training achieves a 94.38%percent94.3894.38\\% accuracy and federated learning achieves a 72.76%percent72.7672.76\\% and 68.71%percent68.7168.71\\% peak accuracy with 10 and 50 clients respectively. Centralized and FL indeed provide the upper bound and the lower bound of the accuracy here. Across all attack settings, GI and LLL achieve higher model accuracy compared to federated learning. The reconstruction quality of GI is adversely affected by larger batch sizes and in turn also impacts the final model performance. With a batch size of 4, 8, and 16, models trained on the leaked data from GI achieves 90.34%percent90.3490.34\\%, 86.16%percent86.1686.16\\%, and 76.83%percent76.8376.83\\% accuracy. With even larger batch sizes, it is likely that the model performance will drop below federated learning. The performance of models using LLL data is more stable. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the accuracy only drops from 93.16%percent93.1693.16\\% to 88.86%percent88.8688.86\\%. This is spite of a large drop in the number of leaked images from 43788 (87.58%percent87.5887.58\\%) to 18242 (36.48%percent36.4836.48\\%) as shown in Table 2(b). This is a subtle and consequential result — the downstream training task can still be accomplished despite a big drop in the amount of data leakage. All prior work had stopped at the stage of evaluating the proportion of leakage and therefore had missed out on this insight.",
            "Table 3(b) and Figure 2(b) show the results for MNIST. LLL performs better than the FedAvg baseline in all cases. With an FC size factor of 8, LOKI achieves a 98.82%percent98.8298.82\\% test accuracy (nearly the same regardless of the FC size factor), only 0.07%percent0.070.07\\% lower than centralized at 98.89%percent98.8998.89\\%. GI performs slightly worse than federated learning, achieving 95.96%percent95.9695.96\\%, 95.50%percent95.5095.50\\%, and 94.29%percent94.2994.29\\% at batch sizes 8, 16, and 32 compared to 96.17%percent96.1796.17\\% and 96.18%percent96.1896.18\\% with 10 and 50 clients in FedAvg. We believe this is because the noise in the reconstructions hampers model performance with GI. This affects MNIST more severely than for CIFAR because the MNIST images are more sparse and sensitive to noise.",
            "The top-1 validation accuracy for Tiny ImageNet are given in Table 3(c) and Figure 2(c). The total number of leaked images hurts LLL more here than in the other two datasets. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the validation accuracy drops from 46.70%percent46.7046.70\\% to 35.20%percent35.2035.20\\%, an 11.5%percent11.511.5\\% drop in performance. Federated learning achieves 37.00%percent37.0037.00\\% and 35.06%percent35.0635.06\\% accuracy with 10 and 50 clients respectively. FL with 10 clients has slightly better accuracy than LLL with an FC size factor of 1. However, any LLL setting with a larger FC layer achieves higher accuracy than both settings in federated learning.",
            "Table 4 shows the leakage rate of LOKI in FedAvg compared to FedSGD. Using CSF=500CSF500\\textit{CSF}=500, the leakage rate of LOKI in FedAvg is substantially higher than the leakage rates in the FedSGD setting. Using an FC layer of half the size, the leakage rate in FedAvg is comparable to that of FedSGD. For example, LOKI FedAvg factor 1 achieves a leakage rate of 58.39%percent58.3958.39\\% compared to LOKI FedSGD factor 2 which achieves a leakage rate of 59.76%percent59.7659.76\\%.",
            "Table 5 shows the testing accuracy of LOKI FedAvg for the different FC layer sizes. The test accuracies are very comparable to the FedSGD settings. However, with a smaller FC layer size factor, the effect of having a larger total number of leaked images becomes visible. With FC factor 1, LOKI FedAvg achieves 91.11%percent91.1191.11\\% accuracy while LOKI FedSGD achieves 88.86%percent88.8688.86\\%.",
            "Quality of reconstruction is an important concern for GI attacks. As the batch size continues to increase, the reconstruction quality decreases. Table 2(a) also shows this relationship. From the previous experiments, we also see a negative relationship between the increasing batch size and the usefulness of the leaked data in downstream model training.",
            "Here, we explore whether poor quality reconstructions are still useful for training models. Using the CIFAR-10 leaked dataset created using Inverting Gradients on a batch size of 16, we first sort the reconstructions by their PSNR value. Our first set of experiments removes the worst images from training. Table 6(a) shows the percent of images above the PSNR threshold and the final test accuracy when trained on that data. Compared to the baseline accuracy of 76.83%percent76.8376.83\\% achieved by using the entire leaked dataset, even removing the worst images with a PSNR <12absent12<12 from the training set results in a small but non-zero performance loss.\nThen we take a different look at this question by now training on the worst images.\nTable 6(b) shows the test accuracy of models trained on all images below a PSNR threshold. Even when training on the lowest quality images with a PSNR <12absent12<12, the model achieves a 45.05%percent45.0545.05\\% final testing accuracy, significantly worse than the baseline, but much above zero.",
            "While some reconstructions have very low quality, only images of the same label will be swapped within the batch during label matching. This is a desirable property for training on leaked data. As long as the set of labels is leaked correctly, the optimization process will match images to a correct label.\nSo even though the image quality is poor (and can be entirely unrecognizable), since the label is correctly matched, such an image still does help in the training (this buttresses the message from Tables 6(a) and 6(b)). Naturally, as the PSNR goes down, the image helps less and less."
        ]
    },
    "S4.T3": {
        "caption": "Table 3:  (a) CIFAR-10 and (b) MNIST test accuracy, (c) Tiny ImageNet top-1 validation accuracy trained from scratch with leaked data. CIFAR-10 and Tiny ImageNet are trained with a ResNet-18 and MNIST is trained with a DNN. For the two attacks (gradient inversion and linear layer leakage), training happens with leaked data. Second column indicates the number of clients in federated learning (FedAvg), the batch size for the gradient inversion attack, and the fully-connected layer size factor for linear layer leakage. Best accuracy is used for FedAvg and final accuracy for all other settings.",
        "table": "<table id=\"S4.T3.st1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.st1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.st1.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S4.T3.st1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T3.st1.2.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T3.st1.2.1.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st1.2.1.1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Num. clients /</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.1.1.2.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.1.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st1.2.1.1.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Batch size /</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.1.1.2.1.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.1.1.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st1.2.1.1.2.1.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FC factor</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T3.st1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T3.st1.2.1.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T3.st1.2.1.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.1.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st1.2.1.1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Test</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.1.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.1.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st1.2.1.1.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">accuracy</span></td>\n</tr>\n</table>\n</th>\n</tr>\n<tr id=\"S4.T3.st1.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.st1.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Centralized</span></th>\n<th id=\"S4.T3.st1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">-</span></th>\n<th id=\"S4.T3.st1.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">94.38</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.st1.2.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T3.st1.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedAvg</span></td>\n<td id=\"S4.T3.st1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></td>\n<td id=\"S4.T3.st1.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.76</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.4.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></td>\n<td id=\"S4.T3.st1.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">68.71</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T3.st1.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">\n<span id=\"S4.T3.st1.2.5.3.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S4.T3.st1.2.5.3.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S4.T3.st1.2.5.3.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Gradient inversion</span></span>\n<span id=\"S4.T3.st1.2.5.3.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S4.T3.st1.2.5.3.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">(Inverting grads <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite>)</span></span>\n</span></span></td>\n<td id=\"S4.T3.st1.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.5.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">4</span></td>\n<td id=\"S4.T3.st1.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.5.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">90.34</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.6.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.6.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">8</span></td>\n<td id=\"S4.T3.st1.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.6.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">86.16</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.7.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.7.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.7.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">16</span></td>\n<td id=\"S4.T3.st1.2.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.7.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.83</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.8.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.8.6.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S4.T3.st1.2.8.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">\n<span id=\"S4.T3.st1.2.8.6.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S4.T3.st1.2.8.6.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S4.T3.st1.2.8.6.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Linear layer leakage</span></span>\n<span id=\"S4.T3.st1.2.8.6.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S4.T3.st1.2.8.6.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">(LOKI <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite>)</span></span>\n</span></span></td>\n<td id=\"S4.T3.st1.2.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.8.6.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">8</span></td>\n<td id=\"S4.T3.st1.2.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.8.6.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">93.16</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.9.7\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.9.7.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.9.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">4</span></td>\n<td id=\"S4.T3.st1.2.9.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.9.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">92.94</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.10.8\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.10.8.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.10.8.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">2</span></td>\n<td id=\"S4.T3.st1.2.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.10.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">91.90</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.11.9\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.11.9.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T3.st1.2.11.9.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">1</span></td>\n<td id=\"S4.T3.st1.2.11.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T3.st1.2.11.9.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">88.86</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 3(a) and Figure 2(a) show the results for CIFAR-10 for centralized training, federated learning (FedAvg), and training with leaked data. Centralized training achieves a 94.38%percent94.3894.38\\% accuracy and federated learning achieves a 72.76%percent72.7672.76\\% and 68.71%percent68.7168.71\\% peak accuracy with 10 and 50 clients respectively. Centralized and FL indeed provide the upper bound and the lower bound of the accuracy here. Across all attack settings, GI and LLL achieve higher model accuracy compared to federated learning. The reconstruction quality of GI is adversely affected by larger batch sizes and in turn also impacts the final model performance. With a batch size of 4, 8, and 16, models trained on the leaked data from GI achieves 90.34%percent90.3490.34\\%, 86.16%percent86.1686.16\\%, and 76.83%percent76.8376.83\\% accuracy. With even larger batch sizes, it is likely that the model performance will drop below federated learning. The performance of models using LLL data is more stable. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the accuracy only drops from 93.16%percent93.1693.16\\% to 88.86%percent88.8688.86\\%. This is spite of a large drop in the number of leaked images from 43788 (87.58%percent87.5887.58\\%) to 18242 (36.48%percent36.4836.48\\%) as shown in Table 2(b). This is a subtle and consequential result — the downstream training task can still be accomplished despite a big drop in the amount of data leakage. All prior work had stopped at the stage of evaluating the proportion of leakage and therefore had missed out on this insight.",
            "Table 3(b) and Figure 2(b) show the results for MNIST. LLL performs better than the FedAvg baseline in all cases. With an FC size factor of 8, LOKI achieves a 98.82%percent98.8298.82\\% test accuracy (nearly the same regardless of the FC size factor), only 0.07%percent0.070.07\\% lower than centralized at 98.89%percent98.8998.89\\%. GI performs slightly worse than federated learning, achieving 95.96%percent95.9695.96\\%, 95.50%percent95.5095.50\\%, and 94.29%percent94.2994.29\\% at batch sizes 8, 16, and 32 compared to 96.17%percent96.1796.17\\% and 96.18%percent96.1896.18\\% with 10 and 50 clients in FedAvg. We believe this is because the noise in the reconstructions hampers model performance with GI. This affects MNIST more severely than for CIFAR because the MNIST images are more sparse and sensitive to noise.",
            "The top-1 validation accuracy for Tiny ImageNet are given in Table 3(c) and Figure 2(c). The total number of leaked images hurts LLL more here than in the other two datasets. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the validation accuracy drops from 46.70%percent46.7046.70\\% to 35.20%percent35.2035.20\\%, an 11.5%percent11.511.5\\% drop in performance. Federated learning achieves 37.00%percent37.0037.00\\% and 35.06%percent35.0635.06\\% accuracy with 10 and 50 clients respectively. FL with 10 clients has slightly better accuracy than LLL with an FC size factor of 1. However, any LLL setting with a larger FC layer achieves higher accuracy than both settings in federated learning."
        ]
    },
    "S4.T3.st1": {
        "caption": "(a) CIFAR-10",
        "table": "<table id=\"S4.T3.st1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.st1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.st1.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S4.T3.st1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T3.st1.2.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T3.st1.2.1.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st1.2.1.1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Num. clients /</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.1.1.2.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.1.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st1.2.1.1.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Batch size /</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.1.1.2.1.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.1.1.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st1.2.1.1.2.1.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FC factor</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T3.st1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T3.st1.2.1.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T3.st1.2.1.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.1.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st1.2.1.1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Test</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.1.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.1.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st1.2.1.1.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">accuracy</span></td>\n</tr>\n</table>\n</th>\n</tr>\n<tr id=\"S4.T3.st1.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.st1.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Centralized</span></th>\n<th id=\"S4.T3.st1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">-</span></th>\n<th id=\"S4.T3.st1.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">94.38</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.st1.2.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T3.st1.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedAvg</span></td>\n<td id=\"S4.T3.st1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></td>\n<td id=\"S4.T3.st1.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.76</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.4.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></td>\n<td id=\"S4.T3.st1.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">68.71</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T3.st1.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">\n<span id=\"S4.T3.st1.2.5.3.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S4.T3.st1.2.5.3.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S4.T3.st1.2.5.3.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Gradient inversion</span></span>\n<span id=\"S4.T3.st1.2.5.3.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S4.T3.st1.2.5.3.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">(Inverting grads <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite>)</span></span>\n</span></span></td>\n<td id=\"S4.T3.st1.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.5.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">4</span></td>\n<td id=\"S4.T3.st1.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.5.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">90.34</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.6.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.6.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">8</span></td>\n<td id=\"S4.T3.st1.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.6.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">86.16</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.7.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.7.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.7.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">16</span></td>\n<td id=\"S4.T3.st1.2.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.7.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.83</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.8.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.8.6.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S4.T3.st1.2.8.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">\n<span id=\"S4.T3.st1.2.8.6.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S4.T3.st1.2.8.6.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S4.T3.st1.2.8.6.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Linear layer leakage</span></span>\n<span id=\"S4.T3.st1.2.8.6.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S4.T3.st1.2.8.6.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">(LOKI <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite>)</span></span>\n</span></span></td>\n<td id=\"S4.T3.st1.2.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.8.6.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">8</span></td>\n<td id=\"S4.T3.st1.2.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st1.2.8.6.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">93.16</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.9.7\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.9.7.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.9.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">4</span></td>\n<td id=\"S4.T3.st1.2.9.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.9.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">92.94</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.10.8\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.10.8.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.10.8.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">2</span></td>\n<td id=\"S4.T3.st1.2.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st1.2.10.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">91.90</span></td>\n</tr>\n<tr id=\"S4.T3.st1.2.11.9\" class=\"ltx_tr\">\n<td id=\"S4.T3.st1.2.11.9.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T3.st1.2.11.9.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">1</span></td>\n<td id=\"S4.T3.st1.2.11.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T3.st1.2.11.9.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">88.86</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 1 shows the amount of time taken by Inverting Gradients run on a NVIDIA A100 80GB GPU for MNIST and CIFAR-10 on a DNN and ResNet-18 respectively. There is some variance in the average amount of time based on batch size, but when attacking the entire dataset, smaller batch sizes always take longer time as the number of total batches is much larger. CIFAR-10 takes significantly longer to reconstruct each batch given the more complex model of ResNet-18 compared to a DNN. For a batch size of 4, CIFAR-10 takes 422.81 seconds per batch. Iterating across the entire dataset on a single GPU takes 61.17 days.",
            "Table 2(a) gives the average PSNR and SSIM metrics (where a higher score indicates greater similarity with the ground truth) for the reconstructions across the entire dataset of CIFAR-10 and MNIST from Inverting Gradients. Increasing the batch size results in a lower PSNR and SSIM score for both CIFAR-10 and MNIST.",
            "Table 2(b) reports the number of leaked images for LOKI. For an FC factor of 444, LOKI leaks 78.93%percent78.9378.93\\%, 76.61%percent76.6176.61\\%, and 75.15%percent75.1575.15\\% of images on the CIFAR-10, MNIST, and Tiny ImageNet datasets. Increasing the FC layer size increases the model size overhead, but results in a higher leakage rate as images are more likely to activate different neurons. Decreasing the size creates a smaller model size overhead but results in lower leakage rate.",
            "Table 3(a) and Figure 2(a) show the results for CIFAR-10 for centralized training, federated learning (FedAvg), and training with leaked data. Centralized training achieves a 94.38%percent94.3894.38\\% accuracy and federated learning achieves a 72.76%percent72.7672.76\\% and 68.71%percent68.7168.71\\% peak accuracy with 10 and 50 clients respectively. Centralized and FL indeed provide the upper bound and the lower bound of the accuracy here. Across all attack settings, GI and LLL achieve higher model accuracy compared to federated learning. The reconstruction quality of GI is adversely affected by larger batch sizes and in turn also impacts the final model performance. With a batch size of 4, 8, and 16, models trained on the leaked data from GI achieves 90.34%percent90.3490.34\\%, 86.16%percent86.1686.16\\%, and 76.83%percent76.8376.83\\% accuracy. With even larger batch sizes, it is likely that the model performance will drop below federated learning. The performance of models using LLL data is more stable. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the accuracy only drops from 93.16%percent93.1693.16\\% to 88.86%percent88.8688.86\\%. This is spite of a large drop in the number of leaked images from 43788 (87.58%percent87.5887.58\\%) to 18242 (36.48%percent36.4836.48\\%) as shown in Table 2(b). This is a subtle and consequential result — the downstream training task can still be accomplished despite a big drop in the amount of data leakage. All prior work had stopped at the stage of evaluating the proportion of leakage and therefore had missed out on this insight.",
            "Table 3(b) and Figure 2(b) show the results for MNIST. LLL performs better than the FedAvg baseline in all cases. With an FC size factor of 8, LOKI achieves a 98.82%percent98.8298.82\\% test accuracy (nearly the same regardless of the FC size factor), only 0.07%percent0.070.07\\% lower than centralized at 98.89%percent98.8998.89\\%. GI performs slightly worse than federated learning, achieving 95.96%percent95.9695.96\\%, 95.50%percent95.5095.50\\%, and 94.29%percent94.2994.29\\% at batch sizes 8, 16, and 32 compared to 96.17%percent96.1796.17\\% and 96.18%percent96.1896.18\\% with 10 and 50 clients in FedAvg. We believe this is because the noise in the reconstructions hampers model performance with GI. This affects MNIST more severely than for CIFAR because the MNIST images are more sparse and sensitive to noise.",
            "The top-1 validation accuracy for Tiny ImageNet are given in Table 3(c) and Figure 2(c). The total number of leaked images hurts LLL more here than in the other two datasets. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the validation accuracy drops from 46.70%percent46.7046.70\\% to 35.20%percent35.2035.20\\%, an 11.5%percent11.511.5\\% drop in performance. Federated learning achieves 37.00%percent37.0037.00\\% and 35.06%percent35.0635.06\\% accuracy with 10 and 50 clients respectively. FL with 10 clients has slightly better accuracy than LLL with an FC size factor of 1. However, any LLL setting with a larger FC layer achieves higher accuracy than both settings in federated learning.",
            "Table 4 shows the leakage rate of LOKI in FedAvg compared to FedSGD. Using CSF=500CSF500\\textit{CSF}=500, the leakage rate of LOKI in FedAvg is substantially higher than the leakage rates in the FedSGD setting. Using an FC layer of half the size, the leakage rate in FedAvg is comparable to that of FedSGD. For example, LOKI FedAvg factor 1 achieves a leakage rate of 58.39%percent58.3958.39\\% compared to LOKI FedSGD factor 2 which achieves a leakage rate of 59.76%percent59.7659.76\\%.",
            "Table 5 shows the testing accuracy of LOKI FedAvg for the different FC layer sizes. The test accuracies are very comparable to the FedSGD settings. However, with a smaller FC layer size factor, the effect of having a larger total number of leaked images becomes visible. With FC factor 1, LOKI FedAvg achieves 91.11%percent91.1191.11\\% accuracy while LOKI FedSGD achieves 88.86%percent88.8688.86\\%.",
            "Quality of reconstruction is an important concern for GI attacks. As the batch size continues to increase, the reconstruction quality decreases. Table 2(a) also shows this relationship. From the previous experiments, we also see a negative relationship between the increasing batch size and the usefulness of the leaked data in downstream model training.",
            "Here, we explore whether poor quality reconstructions are still useful for training models. Using the CIFAR-10 leaked dataset created using Inverting Gradients on a batch size of 16, we first sort the reconstructions by their PSNR value. Our first set of experiments removes the worst images from training. Table 6(a) shows the percent of images above the PSNR threshold and the final test accuracy when trained on that data. Compared to the baseline accuracy of 76.83%percent76.8376.83\\% achieved by using the entire leaked dataset, even removing the worst images with a PSNR <12absent12<12 from the training set results in a small but non-zero performance loss.\nThen we take a different look at this question by now training on the worst images.\nTable 6(b) shows the test accuracy of models trained on all images below a PSNR threshold. Even when training on the lowest quality images with a PSNR <12absent12<12, the model achieves a 45.05%percent45.0545.05\\% final testing accuracy, significantly worse than the baseline, but much above zero.",
            "While some reconstructions have very low quality, only images of the same label will be swapped within the batch during label matching. This is a desirable property for training on leaked data. As long as the set of labels is leaked correctly, the optimization process will match images to a correct label.\nSo even though the image quality is poor (and can be entirely unrecognizable), since the label is correctly matched, such an image still does help in the training (this buttresses the message from Tables 6(a) and 6(b)). Naturally, as the PSNR goes down, the image helps less and less."
        ]
    },
    "S4.T3.st2": {
        "caption": "(b) MNIST",
        "table": "<table id=\"S4.T3.st2.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.st2.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.st2.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S4.T3.st2.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T3.st2.2.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T3.st2.2.1.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.st2.2.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st2.2.1.1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Num. clients /</span></td>\n</tr>\n<tr id=\"S4.T3.st2.2.1.1.2.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.st2.2.1.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st2.2.1.1.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Batch size /</span></td>\n</tr>\n<tr id=\"S4.T3.st2.2.1.1.2.1.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.st2.2.1.1.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st2.2.1.1.2.1.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FC factor</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T3.st2.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T3.st2.2.1.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T3.st2.2.1.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.st2.2.1.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st2.2.1.1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Test</span></td>\n</tr>\n<tr id=\"S4.T3.st2.2.1.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.st2.2.1.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st2.2.1.1.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">accuracy</span></td>\n</tr>\n</table>\n</th>\n</tr>\n<tr id=\"S4.T3.st2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.st2.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T3.st2.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Centralized</span></th>\n<th id=\"S4.T3.st2.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T3.st2.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">-</span></th>\n<th id=\"S4.T3.st2.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T3.st2.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">98.89</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.st2.2.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.st2.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T3.st2.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedAvg</span></td>\n<td id=\"S4.T3.st2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st2.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></td>\n<td id=\"S4.T3.st2.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st2.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.17</span></td>\n</tr>\n<tr id=\"S4.T3.st2.2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.st2.2.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st2.2.4.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></td>\n<td id=\"S4.T3.st2.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st2.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.18</span></td>\n</tr>\n<tr id=\"S4.T3.st2.2.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.st2.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T3.st2.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">\n<span id=\"S4.T3.st2.2.5.3.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S4.T3.st2.2.5.3.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S4.T3.st2.2.5.3.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Gradient inversion</span></span>\n<span id=\"S4.T3.st2.2.5.3.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S4.T3.st2.2.5.3.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">(Inverting grads <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite>)</span></span>\n</span></span></td>\n<td id=\"S4.T3.st2.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st2.2.5.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">8</span></td>\n<td id=\"S4.T3.st2.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st2.2.5.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.96</span></td>\n</tr>\n<tr id=\"S4.T3.st2.2.6.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.st2.2.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st2.2.6.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">16</span></td>\n<td id=\"S4.T3.st2.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st2.2.6.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.50</span></td>\n</tr>\n<tr id=\"S4.T3.st2.2.7.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.st2.2.7.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st2.2.7.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">32</span></td>\n<td id=\"S4.T3.st2.2.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st2.2.7.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">94.29</span></td>\n</tr>\n<tr id=\"S4.T3.st2.2.8.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.st2.2.8.6.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S4.T3.st2.2.8.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">\n<span id=\"S4.T3.st2.2.8.6.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S4.T3.st2.2.8.6.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S4.T3.st2.2.8.6.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Linear layer leakage</span></span>\n<span id=\"S4.T3.st2.2.8.6.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S4.T3.st2.2.8.6.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">(LOKI <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite>)</span></span>\n</span></span></td>\n<td id=\"S4.T3.st2.2.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st2.2.8.6.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">8</span></td>\n<td id=\"S4.T3.st2.2.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st2.2.8.6.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">98.82</span></td>\n</tr>\n<tr id=\"S4.T3.st2.2.9.7\" class=\"ltx_tr\">\n<td id=\"S4.T3.st2.2.9.7.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st2.2.9.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">4</span></td>\n<td id=\"S4.T3.st2.2.9.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st2.2.9.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">98.70</span></td>\n</tr>\n<tr id=\"S4.T3.st2.2.10.8\" class=\"ltx_tr\">\n<td id=\"S4.T3.st2.2.10.8.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st2.2.10.8.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">2</span></td>\n<td id=\"S4.T3.st2.2.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st2.2.10.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">98.72</span></td>\n</tr>\n<tr id=\"S4.T3.st2.2.11.9\" class=\"ltx_tr\">\n<td id=\"S4.T3.st2.2.11.9.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T3.st2.2.11.9.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">1</span></td>\n<td id=\"S4.T3.st2.2.11.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T3.st2.2.11.9.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">98.46</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 1 shows the amount of time taken by Inverting Gradients run on a NVIDIA A100 80GB GPU for MNIST and CIFAR-10 on a DNN and ResNet-18 respectively. There is some variance in the average amount of time based on batch size, but when attacking the entire dataset, smaller batch sizes always take longer time as the number of total batches is much larger. CIFAR-10 takes significantly longer to reconstruct each batch given the more complex model of ResNet-18 compared to a DNN. For a batch size of 4, CIFAR-10 takes 422.81 seconds per batch. Iterating across the entire dataset on a single GPU takes 61.17 days.",
            "Table 2(a) gives the average PSNR and SSIM metrics (where a higher score indicates greater similarity with the ground truth) for the reconstructions across the entire dataset of CIFAR-10 and MNIST from Inverting Gradients. Increasing the batch size results in a lower PSNR and SSIM score for both CIFAR-10 and MNIST.",
            "Table 2(b) reports the number of leaked images for LOKI. For an FC factor of 444, LOKI leaks 78.93%percent78.9378.93\\%, 76.61%percent76.6176.61\\%, and 75.15%percent75.1575.15\\% of images on the CIFAR-10, MNIST, and Tiny ImageNet datasets. Increasing the FC layer size increases the model size overhead, but results in a higher leakage rate as images are more likely to activate different neurons. Decreasing the size creates a smaller model size overhead but results in lower leakage rate.",
            "Table 3(a) and Figure 2(a) show the results for CIFAR-10 for centralized training, federated learning (FedAvg), and training with leaked data. Centralized training achieves a 94.38%percent94.3894.38\\% accuracy and federated learning achieves a 72.76%percent72.7672.76\\% and 68.71%percent68.7168.71\\% peak accuracy with 10 and 50 clients respectively. Centralized and FL indeed provide the upper bound and the lower bound of the accuracy here. Across all attack settings, GI and LLL achieve higher model accuracy compared to federated learning. The reconstruction quality of GI is adversely affected by larger batch sizes and in turn also impacts the final model performance. With a batch size of 4, 8, and 16, models trained on the leaked data from GI achieves 90.34%percent90.3490.34\\%, 86.16%percent86.1686.16\\%, and 76.83%percent76.8376.83\\% accuracy. With even larger batch sizes, it is likely that the model performance will drop below federated learning. The performance of models using LLL data is more stable. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the accuracy only drops from 93.16%percent93.1693.16\\% to 88.86%percent88.8688.86\\%. This is spite of a large drop in the number of leaked images from 43788 (87.58%percent87.5887.58\\%) to 18242 (36.48%percent36.4836.48\\%) as shown in Table 2(b). This is a subtle and consequential result — the downstream training task can still be accomplished despite a big drop in the amount of data leakage. All prior work had stopped at the stage of evaluating the proportion of leakage and therefore had missed out on this insight.",
            "Table 3(b) and Figure 2(b) show the results for MNIST. LLL performs better than the FedAvg baseline in all cases. With an FC size factor of 8, LOKI achieves a 98.82%percent98.8298.82\\% test accuracy (nearly the same regardless of the FC size factor), only 0.07%percent0.070.07\\% lower than centralized at 98.89%percent98.8998.89\\%. GI performs slightly worse than federated learning, achieving 95.96%percent95.9695.96\\%, 95.50%percent95.5095.50\\%, and 94.29%percent94.2994.29\\% at batch sizes 8, 16, and 32 compared to 96.17%percent96.1796.17\\% and 96.18%percent96.1896.18\\% with 10 and 50 clients in FedAvg. We believe this is because the noise in the reconstructions hampers model performance with GI. This affects MNIST more severely than for CIFAR because the MNIST images are more sparse and sensitive to noise.",
            "The top-1 validation accuracy for Tiny ImageNet are given in Table 3(c) and Figure 2(c). The total number of leaked images hurts LLL more here than in the other two datasets. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the validation accuracy drops from 46.70%percent46.7046.70\\% to 35.20%percent35.2035.20\\%, an 11.5%percent11.511.5\\% drop in performance. Federated learning achieves 37.00%percent37.0037.00\\% and 35.06%percent35.0635.06\\% accuracy with 10 and 50 clients respectively. FL with 10 clients has slightly better accuracy than LLL with an FC size factor of 1. However, any LLL setting with a larger FC layer achieves higher accuracy than both settings in federated learning.",
            "Table 4 shows the leakage rate of LOKI in FedAvg compared to FedSGD. Using CSF=500CSF500\\textit{CSF}=500, the leakage rate of LOKI in FedAvg is substantially higher than the leakage rates in the FedSGD setting. Using an FC layer of half the size, the leakage rate in FedAvg is comparable to that of FedSGD. For example, LOKI FedAvg factor 1 achieves a leakage rate of 58.39%percent58.3958.39\\% compared to LOKI FedSGD factor 2 which achieves a leakage rate of 59.76%percent59.7659.76\\%.",
            "Table 5 shows the testing accuracy of LOKI FedAvg for the different FC layer sizes. The test accuracies are very comparable to the FedSGD settings. However, with a smaller FC layer size factor, the effect of having a larger total number of leaked images becomes visible. With FC factor 1, LOKI FedAvg achieves 91.11%percent91.1191.11\\% accuracy while LOKI FedSGD achieves 88.86%percent88.8688.86\\%.",
            "Quality of reconstruction is an important concern for GI attacks. As the batch size continues to increase, the reconstruction quality decreases. Table 2(a) also shows this relationship. From the previous experiments, we also see a negative relationship between the increasing batch size and the usefulness of the leaked data in downstream model training.",
            "Here, we explore whether poor quality reconstructions are still useful for training models. Using the CIFAR-10 leaked dataset created using Inverting Gradients on a batch size of 16, we first sort the reconstructions by their PSNR value. Our first set of experiments removes the worst images from training. Table 6(a) shows the percent of images above the PSNR threshold and the final test accuracy when trained on that data. Compared to the baseline accuracy of 76.83%percent76.8376.83\\% achieved by using the entire leaked dataset, even removing the worst images with a PSNR <12absent12<12 from the training set results in a small but non-zero performance loss.\nThen we take a different look at this question by now training on the worst images.\nTable 6(b) shows the test accuracy of models trained on all images below a PSNR threshold. Even when training on the lowest quality images with a PSNR <12absent12<12, the model achieves a 45.05%percent45.0545.05\\% final testing accuracy, significantly worse than the baseline, but much above zero.",
            "While some reconstructions have very low quality, only images of the same label will be swapped within the batch during label matching. This is a desirable property for training on leaked data. As long as the set of labels is leaked correctly, the optimization process will match images to a correct label.\nSo even though the image quality is poor (and can be entirely unrecognizable), since the label is correctly matched, such an image still does help in the training (this buttresses the message from Tables 6(a) and 6(b)). Naturally, as the PSNR goes down, the image helps less and less."
        ]
    },
    "S4.T3.st3": {
        "caption": "(c) Tiny ImageNet",
        "table": "<table id=\"S4.T3.st3.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.st3.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.st3.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S4.T3.st3.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T3.st3.2.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T3.st3.2.1.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.st3.2.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st3.2.1.1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Num. clients /</span></td>\n</tr>\n<tr id=\"S4.T3.st3.2.1.1.2.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.st3.2.1.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st3.2.1.1.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FC factor</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T3.st3.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T3.st3.2.1.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T3.st3.2.1.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.st3.2.1.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st3.2.1.1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Validation</span></td>\n</tr>\n<tr id=\"S4.T3.st3.2.1.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.st3.2.1.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.st3.2.1.1.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">accuracy</span></td>\n</tr>\n</table>\n</th>\n</tr>\n<tr id=\"S4.T3.st3.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.st3.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T3.st3.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Centralized</span></th>\n<th id=\"S4.T3.st3.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T3.st3.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">-</span></th>\n<th id=\"S4.T3.st3.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T3.st3.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">48.55</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.st3.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.st3.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T3.st3.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedAvg</span></th>\n<td id=\"S4.T3.st3.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st3.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></td>\n<td id=\"S4.T3.st3.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st3.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">37.00</span></td>\n</tr>\n<tr id=\"S4.T3.st3.2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.st3.2.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st3.2.4.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></td>\n<td id=\"S4.T3.st3.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st3.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">35.06</span></td>\n</tr>\n<tr id=\"S4.T3.st3.2.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.st3.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S4.T3.st3.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">\n<span id=\"S4.T3.st3.2.5.3.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S4.T3.st3.2.5.3.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S4.T3.st3.2.5.3.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Linear layer leakage</span></span>\n<span id=\"S4.T3.st3.2.5.3.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S4.T3.st3.2.5.3.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">(LOKI <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite>)</span></span>\n</span></span></th>\n<td id=\"S4.T3.st3.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st3.2.5.3.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">8</span></td>\n<td id=\"S4.T3.st3.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.st3.2.5.3.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">46.70</span></td>\n</tr>\n<tr id=\"S4.T3.st3.2.6.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.st3.2.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st3.2.6.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">4</span></td>\n<td id=\"S4.T3.st3.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st3.2.6.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">45.09</span></td>\n</tr>\n<tr id=\"S4.T3.st3.2.7.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.st3.2.7.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st3.2.7.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">2</span></td>\n<td id=\"S4.T3.st3.2.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.st3.2.7.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">41.15</span></td>\n</tr>\n<tr id=\"S4.T3.st3.2.8.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.st3.2.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T3.st3.2.8.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">1</span></td>\n<td id=\"S4.T3.st3.2.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T3.st3.2.8.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">35.20</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 1 shows the amount of time taken by Inverting Gradients run on a NVIDIA A100 80GB GPU for MNIST and CIFAR-10 on a DNN and ResNet-18 respectively. There is some variance in the average amount of time based on batch size, but when attacking the entire dataset, smaller batch sizes always take longer time as the number of total batches is much larger. CIFAR-10 takes significantly longer to reconstruct each batch given the more complex model of ResNet-18 compared to a DNN. For a batch size of 4, CIFAR-10 takes 422.81 seconds per batch. Iterating across the entire dataset on a single GPU takes 61.17 days.",
            "Table 2(a) gives the average PSNR and SSIM metrics (where a higher score indicates greater similarity with the ground truth) for the reconstructions across the entire dataset of CIFAR-10 and MNIST from Inverting Gradients. Increasing the batch size results in a lower PSNR and SSIM score for both CIFAR-10 and MNIST.",
            "Table 2(b) reports the number of leaked images for LOKI. For an FC factor of 444, LOKI leaks 78.93%percent78.9378.93\\%, 76.61%percent76.6176.61\\%, and 75.15%percent75.1575.15\\% of images on the CIFAR-10, MNIST, and Tiny ImageNet datasets. Increasing the FC layer size increases the model size overhead, but results in a higher leakage rate as images are more likely to activate different neurons. Decreasing the size creates a smaller model size overhead but results in lower leakage rate.",
            "Table 3(a) and Figure 2(a) show the results for CIFAR-10 for centralized training, federated learning (FedAvg), and training with leaked data. Centralized training achieves a 94.38%percent94.3894.38\\% accuracy and federated learning achieves a 72.76%percent72.7672.76\\% and 68.71%percent68.7168.71\\% peak accuracy with 10 and 50 clients respectively. Centralized and FL indeed provide the upper bound and the lower bound of the accuracy here. Across all attack settings, GI and LLL achieve higher model accuracy compared to federated learning. The reconstruction quality of GI is adversely affected by larger batch sizes and in turn also impacts the final model performance. With a batch size of 4, 8, and 16, models trained on the leaked data from GI achieves 90.34%percent90.3490.34\\%, 86.16%percent86.1686.16\\%, and 76.83%percent76.8376.83\\% accuracy. With even larger batch sizes, it is likely that the model performance will drop below federated learning. The performance of models using LLL data is more stable. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the accuracy only drops from 93.16%percent93.1693.16\\% to 88.86%percent88.8688.86\\%. This is spite of a large drop in the number of leaked images from 43788 (87.58%percent87.5887.58\\%) to 18242 (36.48%percent36.4836.48\\%) as shown in Table 2(b). This is a subtle and consequential result — the downstream training task can still be accomplished despite a big drop in the amount of data leakage. All prior work had stopped at the stage of evaluating the proportion of leakage and therefore had missed out on this insight.",
            "Table 3(b) and Figure 2(b) show the results for MNIST. LLL performs better than the FedAvg baseline in all cases. With an FC size factor of 8, LOKI achieves a 98.82%percent98.8298.82\\% test accuracy (nearly the same regardless of the FC size factor), only 0.07%percent0.070.07\\% lower than centralized at 98.89%percent98.8998.89\\%. GI performs slightly worse than federated learning, achieving 95.96%percent95.9695.96\\%, 95.50%percent95.5095.50\\%, and 94.29%percent94.2994.29\\% at batch sizes 8, 16, and 32 compared to 96.17%percent96.1796.17\\% and 96.18%percent96.1896.18\\% with 10 and 50 clients in FedAvg. We believe this is because the noise in the reconstructions hampers model performance with GI. This affects MNIST more severely than for CIFAR because the MNIST images are more sparse and sensitive to noise.",
            "The top-1 validation accuracy for Tiny ImageNet are given in Table 3(c) and Figure 2(c). The total number of leaked images hurts LLL more here than in the other two datasets. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the validation accuracy drops from 46.70%percent46.7046.70\\% to 35.20%percent35.2035.20\\%, an 11.5%percent11.511.5\\% drop in performance. Federated learning achieves 37.00%percent37.0037.00\\% and 35.06%percent35.0635.06\\% accuracy with 10 and 50 clients respectively. FL with 10 clients has slightly better accuracy than LLL with an FC size factor of 1. However, any LLL setting with a larger FC layer achieves higher accuracy than both settings in federated learning.",
            "Table 4 shows the leakage rate of LOKI in FedAvg compared to FedSGD. Using CSF=500CSF500\\textit{CSF}=500, the leakage rate of LOKI in FedAvg is substantially higher than the leakage rates in the FedSGD setting. Using an FC layer of half the size, the leakage rate in FedAvg is comparable to that of FedSGD. For example, LOKI FedAvg factor 1 achieves a leakage rate of 58.39%percent58.3958.39\\% compared to LOKI FedSGD factor 2 which achieves a leakage rate of 59.76%percent59.7659.76\\%.",
            "Table 5 shows the testing accuracy of LOKI FedAvg for the different FC layer sizes. The test accuracies are very comparable to the FedSGD settings. However, with a smaller FC layer size factor, the effect of having a larger total number of leaked images becomes visible. With FC factor 1, LOKI FedAvg achieves 91.11%percent91.1191.11\\% accuracy while LOKI FedSGD achieves 88.86%percent88.8688.86\\%.",
            "Quality of reconstruction is an important concern for GI attacks. As the batch size continues to increase, the reconstruction quality decreases. Table 2(a) also shows this relationship. From the previous experiments, we also see a negative relationship between the increasing batch size and the usefulness of the leaked data in downstream model training.",
            "Here, we explore whether poor quality reconstructions are still useful for training models. Using the CIFAR-10 leaked dataset created using Inverting Gradients on a batch size of 16, we first sort the reconstructions by their PSNR value. Our first set of experiments removes the worst images from training. Table 6(a) shows the percent of images above the PSNR threshold and the final test accuracy when trained on that data. Compared to the baseline accuracy of 76.83%percent76.8376.83\\% achieved by using the entire leaked dataset, even removing the worst images with a PSNR <12absent12<12 from the training set results in a small but non-zero performance loss.\nThen we take a different look at this question by now training on the worst images.\nTable 6(b) shows the test accuracy of models trained on all images below a PSNR threshold. Even when training on the lowest quality images with a PSNR <12absent12<12, the model achieves a 45.05%percent45.0545.05\\% final testing accuracy, significantly worse than the baseline, but much above zero.",
            "While some reconstructions have very low quality, only images of the same label will be swapped within the batch during label matching. This is a desirable property for training on leaked data. As long as the set of labels is leaked correctly, the optimization process will match images to a correct label.\nSo even though the image quality is poor (and can be entirely unrecognizable), since the label is correctly matched, such an image still does help in the training (this buttresses the message from Tables 6(a) and 6(b)). Naturally, as the PSNR goes down, the image helps less and less."
        ]
    },
    "S4.T4": {
        "caption": "Table 4:  Leakage rate %percent\\% (leaked images) of LOKI in FedSGD and FedAvg on CIFAR-10 for several FC layer sizes. FedAvg with half the FC layer size has comparable leakage rate to FedSGD.",
        "table": "<table id=\"S4.T4.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T4.4.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.4.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">\n<table id=\"S4.T4.4.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T4.4.1.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T4.4.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S4.T4.4.1.1.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FC size</span></td>\n</tr>\n<tr id=\"S4.T4.4.1.1.1.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T4.4.1.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S4.T4.4.1.1.1.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">factor</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T4.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T4.4.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T4.4.1.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T4.4.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T4.4.1.1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">LOKI</span></td>\n</tr>\n<tr id=\"S4.T4.4.1.1.2.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T4.4.1.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T4.4.1.1.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedSGD</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T4.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T4.4.1.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T4.4.1.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T4.4.1.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T4.4.1.1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">LOKI</span></td>\n</tr>\n<tr id=\"S4.T4.4.1.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T4.4.1.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T4.4.1.1.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedAvg</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.4.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.4.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T4.4.2.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">8</span></th>\n<td id=\"S4.T4.4.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.4.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">87.58 (43788)</span></td>\n<td id=\"S4.T4.4.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.4.2.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">92.19 (46097)</span></td>\n</tr>\n<tr id=\"S4.T4.4.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.4.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><span id=\"S4.T4.4.3.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">4</span></th>\n<td id=\"S4.T4.4.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.4.3.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">78.93 (39464)</span></td>\n<td id=\"S4.T4.4.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.4.3.2.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">85.98 (42989)</span></td>\n</tr>\n<tr id=\"S4.T4.4.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T4.4.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><span id=\"S4.T4.4.4.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">2</span></th>\n<td id=\"S4.T4.4.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.4.4.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">59.76 (29882)</span></td>\n<td id=\"S4.T4.4.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.4.4.3.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">75.29 (37645)</span></td>\n</tr>\n<tr id=\"S4.T4.4.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T4.4.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"><span id=\"S4.T4.4.5.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">1</span></th>\n<td id=\"S4.T4.4.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T4.4.5.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">36.48 (18242)</span></td>\n<td id=\"S4.T4.4.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T4.4.5.4.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">58.39 (29196)</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 4 shows the leakage rate of LOKI in FedAvg compared to FedSGD. Using CSF=500CSF500\\textit{CSF}=500, the leakage rate of LOKI in FedAvg is substantially higher than the leakage rates in the FedSGD setting. Using an FC layer of half the size, the leakage rate in FedAvg is comparable to that of FedSGD. For example, LOKI FedAvg factor 1 achieves a leakage rate of 58.39%percent58.3958.39\\% compared to LOKI FedSGD factor 2 which achieves a leakage rate of 59.76%percent59.7659.76\\%."
        ]
    },
    "S4.T5": {
        "caption": "Table 5:  CIFAR-10 test accuracy with LOKI FedAvg.",
        "table": "<table id=\"S4.T5.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T5.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T5.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S4.T5.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">\n<table id=\"S4.T5.2.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T5.2.1.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T5.2.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T5.2.1.1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FC factor</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T5.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T5.2.1.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T5.2.1.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T5.2.1.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T5.2.1.1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedSGD</span></td>\n</tr>\n<tr id=\"S4.T5.2.1.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T5.2.1.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T5.2.1.1.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">accuracy</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T5.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T5.2.1.1.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T5.2.1.1.4.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T5.2.1.1.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T5.2.1.1.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedAvg</span></td>\n</tr>\n<tr id=\"S4.T5.2.1.1.4.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T5.2.1.1.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T5.2.1.1.4.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">accuracy</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T5.2.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T5.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S4.T5.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">\n<span id=\"S4.T5.2.2.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S4.T5.2.2.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S4.T5.2.2.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Linear layer</span></span>\n<span id=\"S4.T5.2.2.1.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S4.T5.2.2.1.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">leakage (LOKI <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">30</span></a>]</cite>)</span></span>\n</span></span></th>\n<th id=\"S4.T5.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T5.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">8</span></th>\n<td id=\"S4.T5.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">93.16</span></td>\n<td id=\"S4.T5.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.2.2.1.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">93.31</span></td>\n</tr>\n<tr id=\"S4.T5.2.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T5.2.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T5.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">4</span></th>\n<td id=\"S4.T5.2.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T5.2.3.2.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">92.94</span></td>\n<td id=\"S4.T5.2.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T5.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">92.88</span></td>\n</tr>\n<tr id=\"S4.T5.2.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T5.2.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T5.2.4.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">2</span></th>\n<td id=\"S4.T5.2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T5.2.4.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">91.90</span></td>\n<td id=\"S4.T5.2.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T5.2.4.3.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">92.35</span></td>\n</tr>\n<tr id=\"S4.T5.2.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T5.2.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\"><span id=\"S4.T5.2.5.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">1</span></th>\n<td id=\"S4.T5.2.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T5.2.5.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">88.86</span></td>\n<td id=\"S4.T5.2.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T5.2.5.4.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">91.11</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 5 shows the testing accuracy of LOKI FedAvg for the different FC layer sizes. The test accuracies are very comparable to the FedSGD settings. However, with a smaller FC layer size factor, the effect of having a larger total number of leaked images becomes visible. With FC factor 1, LOKI FedAvg achieves 91.11%percent91.1191.11\\% accuracy while LOKI FedSGD achieves 88.86%percent88.8688.86\\%."
        ]
    },
    "S4.T6": {
        "caption": "Table 6:  Training models using CIFAR-10 leaked data from inverting gradients batch size 16. Only the reconstructions with a PSNR (a) above and (b) below the threshold are used in training.",
        "table": "<table id=\"S4.T6.st1.5.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T6.st1.5.5.6.1\" class=\"ltx_tr\">\n<th id=\"S4.T6.st1.5.5.6.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T6.st1.5.5.6.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">PSNR</span></th>\n<th id=\"S4.T6.st1.5.5.6.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T6.st1.5.5.6.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T6.st1.5.5.6.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T6.st1.5.5.6.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T6.st1.5.5.6.1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">% imgs</span></td>\n</tr>\n<tr id=\"S4.T6.st1.5.5.6.1.2.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T6.st1.5.5.6.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T6.st1.5.5.6.1.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">kept</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T6.st1.5.5.6.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T6.st1.5.5.6.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T6.st1.5.5.6.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T6.st1.5.5.6.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T6.st1.5.5.6.1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Test</span></td>\n</tr>\n<tr id=\"S4.T6.st1.5.5.6.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T6.st1.5.5.6.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T6.st1.5.5.6.1.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">accuracy</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T6.st1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T6.st1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><math id=\"S4.T6.st1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;20\" display=\"inline\"><semantics id=\"S4.T6.st1.1.1.1.1.m1.1a\"><mrow id=\"S4.T6.st1.1.1.1.1.m1.1.1\" xref=\"S4.T6.st1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S4.T6.st1.1.1.1.1.m1.1.1.2\" xref=\"S4.T6.st1.1.1.1.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S4.T6.st1.1.1.1.1.m1.1.1.1\" xref=\"S4.T6.st1.1.1.1.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S4.T6.st1.1.1.1.1.m1.1.1.3\" xref=\"S4.T6.st1.1.1.1.1.m1.1.1.3.cmml\">20</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.st1.1.1.1.1.m1.1b\"><apply id=\"S4.T6.st1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T6.st1.1.1.1.1.m1.1.1\"><gt id=\"S4.T6.st1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T6.st1.1.1.1.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S4.T6.st1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T6.st1.1.1.1.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S4.T6.st1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T6.st1.1.1.1.1.m1.1.1.3\">20</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.st1.1.1.1.1.m1.1c\">&gt;20</annotation></semantics></math></th>\n<td id=\"S4.T6.st1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T6.st1.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">15.88</span></td>\n<td id=\"S4.T6.st1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T6.st1.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.04</span></td>\n</tr>\n<tr id=\"S4.T6.st1.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T6.st1.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S4.T6.st1.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;18\" display=\"inline\"><semantics id=\"S4.T6.st1.2.2.2.1.m1.1a\"><mrow id=\"S4.T6.st1.2.2.2.1.m1.1.1\" xref=\"S4.T6.st1.2.2.2.1.m1.1.1.cmml\"><mi id=\"S4.T6.st1.2.2.2.1.m1.1.1.2\" xref=\"S4.T6.st1.2.2.2.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S4.T6.st1.2.2.2.1.m1.1.1.1\" xref=\"S4.T6.st1.2.2.2.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S4.T6.st1.2.2.2.1.m1.1.1.3\" xref=\"S4.T6.st1.2.2.2.1.m1.1.1.3.cmml\">18</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.st1.2.2.2.1.m1.1b\"><apply id=\"S4.T6.st1.2.2.2.1.m1.1.1.cmml\" xref=\"S4.T6.st1.2.2.2.1.m1.1.1\"><gt id=\"S4.T6.st1.2.2.2.1.m1.1.1.1.cmml\" xref=\"S4.T6.st1.2.2.2.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S4.T6.st1.2.2.2.1.m1.1.1.2.cmml\" xref=\"S4.T6.st1.2.2.2.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S4.T6.st1.2.2.2.1.m1.1.1.3.cmml\" xref=\"S4.T6.st1.2.2.2.1.m1.1.1.3\">18</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.st1.2.2.2.1.m1.1c\">&gt;18</annotation></semantics></math></th>\n<td id=\"S4.T6.st1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st1.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">27.59</span></td>\n<td id=\"S4.T6.st1.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st1.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.52</span></td>\n</tr>\n<tr id=\"S4.T6.st1.3.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T6.st1.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S4.T6.st1.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;16\" display=\"inline\"><semantics id=\"S4.T6.st1.3.3.3.1.m1.1a\"><mrow id=\"S4.T6.st1.3.3.3.1.m1.1.1\" xref=\"S4.T6.st1.3.3.3.1.m1.1.1.cmml\"><mi id=\"S4.T6.st1.3.3.3.1.m1.1.1.2\" xref=\"S4.T6.st1.3.3.3.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S4.T6.st1.3.3.3.1.m1.1.1.1\" xref=\"S4.T6.st1.3.3.3.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S4.T6.st1.3.3.3.1.m1.1.1.3\" xref=\"S4.T6.st1.3.3.3.1.m1.1.1.3.cmml\">16</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.st1.3.3.3.1.m1.1b\"><apply id=\"S4.T6.st1.3.3.3.1.m1.1.1.cmml\" xref=\"S4.T6.st1.3.3.3.1.m1.1.1\"><gt id=\"S4.T6.st1.3.3.3.1.m1.1.1.1.cmml\" xref=\"S4.T6.st1.3.3.3.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S4.T6.st1.3.3.3.1.m1.1.1.2.cmml\" xref=\"S4.T6.st1.3.3.3.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S4.T6.st1.3.3.3.1.m1.1.1.3.cmml\" xref=\"S4.T6.st1.3.3.3.1.m1.1.1.3\">16</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.st1.3.3.3.1.m1.1c\">&gt;16</annotation></semantics></math></th>\n<td id=\"S4.T6.st1.3.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st1.3.3.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">40.78</span></td>\n<td id=\"S4.T6.st1.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st1.3.3.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.73</span></td>\n</tr>\n<tr id=\"S4.T6.st1.4.4.4\" class=\"ltx_tr\">\n<th id=\"S4.T6.st1.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S4.T6.st1.4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;14\" display=\"inline\"><semantics id=\"S4.T6.st1.4.4.4.1.m1.1a\"><mrow id=\"S4.T6.st1.4.4.4.1.m1.1.1\" xref=\"S4.T6.st1.4.4.4.1.m1.1.1.cmml\"><mi id=\"S4.T6.st1.4.4.4.1.m1.1.1.2\" xref=\"S4.T6.st1.4.4.4.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S4.T6.st1.4.4.4.1.m1.1.1.1\" xref=\"S4.T6.st1.4.4.4.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S4.T6.st1.4.4.4.1.m1.1.1.3\" xref=\"S4.T6.st1.4.4.4.1.m1.1.1.3.cmml\">14</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.st1.4.4.4.1.m1.1b\"><apply id=\"S4.T6.st1.4.4.4.1.m1.1.1.cmml\" xref=\"S4.T6.st1.4.4.4.1.m1.1.1\"><gt id=\"S4.T6.st1.4.4.4.1.m1.1.1.1.cmml\" xref=\"S4.T6.st1.4.4.4.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S4.T6.st1.4.4.4.1.m1.1.1.2.cmml\" xref=\"S4.T6.st1.4.4.4.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S4.T6.st1.4.4.4.1.m1.1.1.3.cmml\" xref=\"S4.T6.st1.4.4.4.1.m1.1.1.3\">14</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.st1.4.4.4.1.m1.1c\">&gt;14</annotation></semantics></math></th>\n<td id=\"S4.T6.st1.4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st1.4.4.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.22</span></td>\n<td id=\"S4.T6.st1.4.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st1.4.4.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.48</span></td>\n</tr>\n<tr id=\"S4.T6.st1.5.5.5\" class=\"ltx_tr\">\n<th id=\"S4.T6.st1.5.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"><math id=\"S4.T6.st1.5.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;12\" display=\"inline\"><semantics id=\"S4.T6.st1.5.5.5.1.m1.1a\"><mrow id=\"S4.T6.st1.5.5.5.1.m1.1.1\" xref=\"S4.T6.st1.5.5.5.1.m1.1.1.cmml\"><mi id=\"S4.T6.st1.5.5.5.1.m1.1.1.2\" xref=\"S4.T6.st1.5.5.5.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S4.T6.st1.5.5.5.1.m1.1.1.1\" xref=\"S4.T6.st1.5.5.5.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S4.T6.st1.5.5.5.1.m1.1.1.3\" xref=\"S4.T6.st1.5.5.5.1.m1.1.1.3.cmml\">12</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.st1.5.5.5.1.m1.1b\"><apply id=\"S4.T6.st1.5.5.5.1.m1.1.1.cmml\" xref=\"S4.T6.st1.5.5.5.1.m1.1.1\"><gt id=\"S4.T6.st1.5.5.5.1.m1.1.1.1.cmml\" xref=\"S4.T6.st1.5.5.5.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S4.T6.st1.5.5.5.1.m1.1.1.2.cmml\" xref=\"S4.T6.st1.5.5.5.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S4.T6.st1.5.5.5.1.m1.1.1.3.cmml\" xref=\"S4.T6.st1.5.5.5.1.m1.1.1.3\">12</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.st1.5.5.5.1.m1.1c\">&gt;12</annotation></semantics></math></th>\n<td id=\"S4.T6.st1.5.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T6.st1.5.5.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">84.42</span></td>\n<td id=\"S4.T6.st1.5.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T6.st1.5.5.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.16</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Here, we explore whether poor quality reconstructions are still useful for training models. Using the CIFAR-10 leaked dataset created using Inverting Gradients on a batch size of 16, we first sort the reconstructions by their PSNR value. Our first set of experiments removes the worst images from training. Table 6(a) shows the percent of images above the PSNR threshold and the final test accuracy when trained on that data. Compared to the baseline accuracy of 76.83%percent76.8376.83\\% achieved by using the entire leaked dataset, even removing the worst images with a PSNR <12absent12<12 from the training set results in a small but non-zero performance loss.\nThen we take a different look at this question by now training on the worst images.\nTable 6(b) shows the test accuracy of models trained on all images below a PSNR threshold. Even when training on the lowest quality images with a PSNR <12absent12<12, the model achieves a 45.05%percent45.0545.05\\% final testing accuracy, significantly worse than the baseline, but much above zero."
        ]
    },
    "S4.T6.st1": {
        "caption": "(a) PSNR above threshold",
        "table": "<table id=\"S4.T6.st1.5.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T6.st1.5.5.6.1\" class=\"ltx_tr\">\n<th id=\"S4.T6.st1.5.5.6.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T6.st1.5.5.6.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">PSNR</span></th>\n<th id=\"S4.T6.st1.5.5.6.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T6.st1.5.5.6.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T6.st1.5.5.6.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T6.st1.5.5.6.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T6.st1.5.5.6.1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">% imgs</span></td>\n</tr>\n<tr id=\"S4.T6.st1.5.5.6.1.2.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T6.st1.5.5.6.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T6.st1.5.5.6.1.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">kept</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T6.st1.5.5.6.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T6.st1.5.5.6.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T6.st1.5.5.6.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T6.st1.5.5.6.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T6.st1.5.5.6.1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Test</span></td>\n</tr>\n<tr id=\"S4.T6.st1.5.5.6.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T6.st1.5.5.6.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T6.st1.5.5.6.1.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">accuracy</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T6.st1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T6.st1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><math id=\"S4.T6.st1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;20\" display=\"inline\"><semantics id=\"S4.T6.st1.1.1.1.1.m1.1a\"><mrow id=\"S4.T6.st1.1.1.1.1.m1.1.1\" xref=\"S4.T6.st1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S4.T6.st1.1.1.1.1.m1.1.1.2\" xref=\"S4.T6.st1.1.1.1.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S4.T6.st1.1.1.1.1.m1.1.1.1\" xref=\"S4.T6.st1.1.1.1.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S4.T6.st1.1.1.1.1.m1.1.1.3\" xref=\"S4.T6.st1.1.1.1.1.m1.1.1.3.cmml\">20</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.st1.1.1.1.1.m1.1b\"><apply id=\"S4.T6.st1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T6.st1.1.1.1.1.m1.1.1\"><gt id=\"S4.T6.st1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T6.st1.1.1.1.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S4.T6.st1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T6.st1.1.1.1.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S4.T6.st1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T6.st1.1.1.1.1.m1.1.1.3\">20</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.st1.1.1.1.1.m1.1c\">&gt;20</annotation></semantics></math></th>\n<td id=\"S4.T6.st1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T6.st1.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">15.88</span></td>\n<td id=\"S4.T6.st1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T6.st1.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.04</span></td>\n</tr>\n<tr id=\"S4.T6.st1.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T6.st1.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S4.T6.st1.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;18\" display=\"inline\"><semantics id=\"S4.T6.st1.2.2.2.1.m1.1a\"><mrow id=\"S4.T6.st1.2.2.2.1.m1.1.1\" xref=\"S4.T6.st1.2.2.2.1.m1.1.1.cmml\"><mi id=\"S4.T6.st1.2.2.2.1.m1.1.1.2\" xref=\"S4.T6.st1.2.2.2.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S4.T6.st1.2.2.2.1.m1.1.1.1\" xref=\"S4.T6.st1.2.2.2.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S4.T6.st1.2.2.2.1.m1.1.1.3\" xref=\"S4.T6.st1.2.2.2.1.m1.1.1.3.cmml\">18</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.st1.2.2.2.1.m1.1b\"><apply id=\"S4.T6.st1.2.2.2.1.m1.1.1.cmml\" xref=\"S4.T6.st1.2.2.2.1.m1.1.1\"><gt id=\"S4.T6.st1.2.2.2.1.m1.1.1.1.cmml\" xref=\"S4.T6.st1.2.2.2.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S4.T6.st1.2.2.2.1.m1.1.1.2.cmml\" xref=\"S4.T6.st1.2.2.2.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S4.T6.st1.2.2.2.1.m1.1.1.3.cmml\" xref=\"S4.T6.st1.2.2.2.1.m1.1.1.3\">18</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.st1.2.2.2.1.m1.1c\">&gt;18</annotation></semantics></math></th>\n<td id=\"S4.T6.st1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st1.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">27.59</span></td>\n<td id=\"S4.T6.st1.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st1.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.52</span></td>\n</tr>\n<tr id=\"S4.T6.st1.3.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T6.st1.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S4.T6.st1.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;16\" display=\"inline\"><semantics id=\"S4.T6.st1.3.3.3.1.m1.1a\"><mrow id=\"S4.T6.st1.3.3.3.1.m1.1.1\" xref=\"S4.T6.st1.3.3.3.1.m1.1.1.cmml\"><mi id=\"S4.T6.st1.3.3.3.1.m1.1.1.2\" xref=\"S4.T6.st1.3.3.3.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S4.T6.st1.3.3.3.1.m1.1.1.1\" xref=\"S4.T6.st1.3.3.3.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S4.T6.st1.3.3.3.1.m1.1.1.3\" xref=\"S4.T6.st1.3.3.3.1.m1.1.1.3.cmml\">16</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.st1.3.3.3.1.m1.1b\"><apply id=\"S4.T6.st1.3.3.3.1.m1.1.1.cmml\" xref=\"S4.T6.st1.3.3.3.1.m1.1.1\"><gt id=\"S4.T6.st1.3.3.3.1.m1.1.1.1.cmml\" xref=\"S4.T6.st1.3.3.3.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S4.T6.st1.3.3.3.1.m1.1.1.2.cmml\" xref=\"S4.T6.st1.3.3.3.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S4.T6.st1.3.3.3.1.m1.1.1.3.cmml\" xref=\"S4.T6.st1.3.3.3.1.m1.1.1.3\">16</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.st1.3.3.3.1.m1.1c\">&gt;16</annotation></semantics></math></th>\n<td id=\"S4.T6.st1.3.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st1.3.3.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">40.78</span></td>\n<td id=\"S4.T6.st1.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st1.3.3.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.73</span></td>\n</tr>\n<tr id=\"S4.T6.st1.4.4.4\" class=\"ltx_tr\">\n<th id=\"S4.T6.st1.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S4.T6.st1.4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;14\" display=\"inline\"><semantics id=\"S4.T6.st1.4.4.4.1.m1.1a\"><mrow id=\"S4.T6.st1.4.4.4.1.m1.1.1\" xref=\"S4.T6.st1.4.4.4.1.m1.1.1.cmml\"><mi id=\"S4.T6.st1.4.4.4.1.m1.1.1.2\" xref=\"S4.T6.st1.4.4.4.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S4.T6.st1.4.4.4.1.m1.1.1.1\" xref=\"S4.T6.st1.4.4.4.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S4.T6.st1.4.4.4.1.m1.1.1.3\" xref=\"S4.T6.st1.4.4.4.1.m1.1.1.3.cmml\">14</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.st1.4.4.4.1.m1.1b\"><apply id=\"S4.T6.st1.4.4.4.1.m1.1.1.cmml\" xref=\"S4.T6.st1.4.4.4.1.m1.1.1\"><gt id=\"S4.T6.st1.4.4.4.1.m1.1.1.1.cmml\" xref=\"S4.T6.st1.4.4.4.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S4.T6.st1.4.4.4.1.m1.1.1.2.cmml\" xref=\"S4.T6.st1.4.4.4.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S4.T6.st1.4.4.4.1.m1.1.1.3.cmml\" xref=\"S4.T6.st1.4.4.4.1.m1.1.1.3\">14</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.st1.4.4.4.1.m1.1c\">&gt;14</annotation></semantics></math></th>\n<td id=\"S4.T6.st1.4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st1.4.4.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.22</span></td>\n<td id=\"S4.T6.st1.4.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st1.4.4.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.48</span></td>\n</tr>\n<tr id=\"S4.T6.st1.5.5.5\" class=\"ltx_tr\">\n<th id=\"S4.T6.st1.5.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"><math id=\"S4.T6.st1.5.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;12\" display=\"inline\"><semantics id=\"S4.T6.st1.5.5.5.1.m1.1a\"><mrow id=\"S4.T6.st1.5.5.5.1.m1.1.1\" xref=\"S4.T6.st1.5.5.5.1.m1.1.1.cmml\"><mi id=\"S4.T6.st1.5.5.5.1.m1.1.1.2\" xref=\"S4.T6.st1.5.5.5.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S4.T6.st1.5.5.5.1.m1.1.1.1\" xref=\"S4.T6.st1.5.5.5.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S4.T6.st1.5.5.5.1.m1.1.1.3\" xref=\"S4.T6.st1.5.5.5.1.m1.1.1.3.cmml\">12</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.st1.5.5.5.1.m1.1b\"><apply id=\"S4.T6.st1.5.5.5.1.m1.1.1.cmml\" xref=\"S4.T6.st1.5.5.5.1.m1.1.1\"><gt id=\"S4.T6.st1.5.5.5.1.m1.1.1.1.cmml\" xref=\"S4.T6.st1.5.5.5.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S4.T6.st1.5.5.5.1.m1.1.1.2.cmml\" xref=\"S4.T6.st1.5.5.5.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S4.T6.st1.5.5.5.1.m1.1.1.3.cmml\" xref=\"S4.T6.st1.5.5.5.1.m1.1.1.3\">12</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.st1.5.5.5.1.m1.1c\">&gt;12</annotation></semantics></math></th>\n<td id=\"S4.T6.st1.5.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T6.st1.5.5.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">84.42</span></td>\n<td id=\"S4.T6.st1.5.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T6.st1.5.5.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.16</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 1 shows the amount of time taken by Inverting Gradients run on a NVIDIA A100 80GB GPU for MNIST and CIFAR-10 on a DNN and ResNet-18 respectively. There is some variance in the average amount of time based on batch size, but when attacking the entire dataset, smaller batch sizes always take longer time as the number of total batches is much larger. CIFAR-10 takes significantly longer to reconstruct each batch given the more complex model of ResNet-18 compared to a DNN. For a batch size of 4, CIFAR-10 takes 422.81 seconds per batch. Iterating across the entire dataset on a single GPU takes 61.17 days.",
            "Table 2(a) gives the average PSNR and SSIM metrics (where a higher score indicates greater similarity with the ground truth) for the reconstructions across the entire dataset of CIFAR-10 and MNIST from Inverting Gradients. Increasing the batch size results in a lower PSNR and SSIM score for both CIFAR-10 and MNIST.",
            "Table 2(b) reports the number of leaked images for LOKI. For an FC factor of 444, LOKI leaks 78.93%percent78.9378.93\\%, 76.61%percent76.6176.61\\%, and 75.15%percent75.1575.15\\% of images on the CIFAR-10, MNIST, and Tiny ImageNet datasets. Increasing the FC layer size increases the model size overhead, but results in a higher leakage rate as images are more likely to activate different neurons. Decreasing the size creates a smaller model size overhead but results in lower leakage rate.",
            "Table 3(a) and Figure 2(a) show the results for CIFAR-10 for centralized training, federated learning (FedAvg), and training with leaked data. Centralized training achieves a 94.38%percent94.3894.38\\% accuracy and federated learning achieves a 72.76%percent72.7672.76\\% and 68.71%percent68.7168.71\\% peak accuracy with 10 and 50 clients respectively. Centralized and FL indeed provide the upper bound and the lower bound of the accuracy here. Across all attack settings, GI and LLL achieve higher model accuracy compared to federated learning. The reconstruction quality of GI is adversely affected by larger batch sizes and in turn also impacts the final model performance. With a batch size of 4, 8, and 16, models trained on the leaked data from GI achieves 90.34%percent90.3490.34\\%, 86.16%percent86.1686.16\\%, and 76.83%percent76.8376.83\\% accuracy. With even larger batch sizes, it is likely that the model performance will drop below federated learning. The performance of models using LLL data is more stable. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the accuracy only drops from 93.16%percent93.1693.16\\% to 88.86%percent88.8688.86\\%. This is spite of a large drop in the number of leaked images from 43788 (87.58%percent87.5887.58\\%) to 18242 (36.48%percent36.4836.48\\%) as shown in Table 2(b). This is a subtle and consequential result — the downstream training task can still be accomplished despite a big drop in the amount of data leakage. All prior work had stopped at the stage of evaluating the proportion of leakage and therefore had missed out on this insight.",
            "Table 3(b) and Figure 2(b) show the results for MNIST. LLL performs better than the FedAvg baseline in all cases. With an FC size factor of 8, LOKI achieves a 98.82%percent98.8298.82\\% test accuracy (nearly the same regardless of the FC size factor), only 0.07%percent0.070.07\\% lower than centralized at 98.89%percent98.8998.89\\%. GI performs slightly worse than federated learning, achieving 95.96%percent95.9695.96\\%, 95.50%percent95.5095.50\\%, and 94.29%percent94.2994.29\\% at batch sizes 8, 16, and 32 compared to 96.17%percent96.1796.17\\% and 96.18%percent96.1896.18\\% with 10 and 50 clients in FedAvg. We believe this is because the noise in the reconstructions hampers model performance with GI. This affects MNIST more severely than for CIFAR because the MNIST images are more sparse and sensitive to noise.",
            "The top-1 validation accuracy for Tiny ImageNet are given in Table 3(c) and Figure 2(c). The total number of leaked images hurts LLL more here than in the other two datasets. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the validation accuracy drops from 46.70%percent46.7046.70\\% to 35.20%percent35.2035.20\\%, an 11.5%percent11.511.5\\% drop in performance. Federated learning achieves 37.00%percent37.0037.00\\% and 35.06%percent35.0635.06\\% accuracy with 10 and 50 clients respectively. FL with 10 clients has slightly better accuracy than LLL with an FC size factor of 1. However, any LLL setting with a larger FC layer achieves higher accuracy than both settings in federated learning.",
            "Table 4 shows the leakage rate of LOKI in FedAvg compared to FedSGD. Using CSF=500CSF500\\textit{CSF}=500, the leakage rate of LOKI in FedAvg is substantially higher than the leakage rates in the FedSGD setting. Using an FC layer of half the size, the leakage rate in FedAvg is comparable to that of FedSGD. For example, LOKI FedAvg factor 1 achieves a leakage rate of 58.39%percent58.3958.39\\% compared to LOKI FedSGD factor 2 which achieves a leakage rate of 59.76%percent59.7659.76\\%.",
            "Table 5 shows the testing accuracy of LOKI FedAvg for the different FC layer sizes. The test accuracies are very comparable to the FedSGD settings. However, with a smaller FC layer size factor, the effect of having a larger total number of leaked images becomes visible. With FC factor 1, LOKI FedAvg achieves 91.11%percent91.1191.11\\% accuracy while LOKI FedSGD achieves 88.86%percent88.8688.86\\%.",
            "Quality of reconstruction is an important concern for GI attacks. As the batch size continues to increase, the reconstruction quality decreases. Table 2(a) also shows this relationship. From the previous experiments, we also see a negative relationship between the increasing batch size and the usefulness of the leaked data in downstream model training.",
            "Here, we explore whether poor quality reconstructions are still useful for training models. Using the CIFAR-10 leaked dataset created using Inverting Gradients on a batch size of 16, we first sort the reconstructions by their PSNR value. Our first set of experiments removes the worst images from training. Table 6(a) shows the percent of images above the PSNR threshold and the final test accuracy when trained on that data. Compared to the baseline accuracy of 76.83%percent76.8376.83\\% achieved by using the entire leaked dataset, even removing the worst images with a PSNR <12absent12<12 from the training set results in a small but non-zero performance loss.\nThen we take a different look at this question by now training on the worst images.\nTable 6(b) shows the test accuracy of models trained on all images below a PSNR threshold. Even when training on the lowest quality images with a PSNR <12absent12<12, the model achieves a 45.05%percent45.0545.05\\% final testing accuracy, significantly worse than the baseline, but much above zero.",
            "While some reconstructions have very low quality, only images of the same label will be swapped within the batch during label matching. This is a desirable property for training on leaked data. As long as the set of labels is leaked correctly, the optimization process will match images to a correct label.\nSo even though the image quality is poor (and can be entirely unrecognizable), since the label is correctly matched, such an image still does help in the training (this buttresses the message from Tables 6(a) and 6(b)). Naturally, as the PSNR goes down, the image helps less and less."
        ]
    },
    "S4.T6.st2": {
        "caption": "(b) PSNR below threshold",
        "table": "<table id=\"S4.T6.st2.5.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T6.st2.5.5.6.1\" class=\"ltx_tr\">\n<th id=\"S4.T6.st2.5.5.6.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T6.st2.5.5.6.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">PSNR</span></th>\n<th id=\"S4.T6.st2.5.5.6.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T6.st2.5.5.6.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T6.st2.5.5.6.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T6.st2.5.5.6.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T6.st2.5.5.6.1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">% imgs</span></td>\n</tr>\n<tr id=\"S4.T6.st2.5.5.6.1.2.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T6.st2.5.5.6.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T6.st2.5.5.6.1.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">kept</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S4.T6.st2.5.5.6.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S4.T6.st2.5.5.6.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T6.st2.5.5.6.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T6.st2.5.5.6.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T6.st2.5.5.6.1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Test</span></td>\n</tr>\n<tr id=\"S4.T6.st2.5.5.6.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T6.st2.5.5.6.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T6.st2.5.5.6.1.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">accuracy</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T6.st2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T6.st2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><math id=\"S4.T6.st2.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"&lt;20\" display=\"inline\"><semantics id=\"S4.T6.st2.1.1.1.1.m1.1a\"><mrow id=\"S4.T6.st2.1.1.1.1.m1.1.1\" xref=\"S4.T6.st2.1.1.1.1.m1.1.1.cmml\"><mi id=\"S4.T6.st2.1.1.1.1.m1.1.1.2\" xref=\"S4.T6.st2.1.1.1.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S4.T6.st2.1.1.1.1.m1.1.1.1\" xref=\"S4.T6.st2.1.1.1.1.m1.1.1.1.cmml\">&lt;</mo><mn mathsize=\"70%\" id=\"S4.T6.st2.1.1.1.1.m1.1.1.3\" xref=\"S4.T6.st2.1.1.1.1.m1.1.1.3.cmml\">20</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.st2.1.1.1.1.m1.1b\"><apply id=\"S4.T6.st2.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T6.st2.1.1.1.1.m1.1.1\"><lt id=\"S4.T6.st2.1.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T6.st2.1.1.1.1.m1.1.1.1\"></lt><csymbol cd=\"latexml\" id=\"S4.T6.st2.1.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T6.st2.1.1.1.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S4.T6.st2.1.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T6.st2.1.1.1.1.m1.1.1.3\">20</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.st2.1.1.1.1.m1.1c\">&lt;20</annotation></semantics></math></th>\n<td id=\"S4.T6.st2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T6.st2.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">84.12</span></td>\n<td id=\"S4.T6.st2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T6.st2.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.03</span></td>\n</tr>\n<tr id=\"S4.T6.st2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T6.st2.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S4.T6.st2.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"&lt;18\" display=\"inline\"><semantics id=\"S4.T6.st2.2.2.2.1.m1.1a\"><mrow id=\"S4.T6.st2.2.2.2.1.m1.1.1\" xref=\"S4.T6.st2.2.2.2.1.m1.1.1.cmml\"><mi id=\"S4.T6.st2.2.2.2.1.m1.1.1.2\" xref=\"S4.T6.st2.2.2.2.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S4.T6.st2.2.2.2.1.m1.1.1.1\" xref=\"S4.T6.st2.2.2.2.1.m1.1.1.1.cmml\">&lt;</mo><mn mathsize=\"70%\" id=\"S4.T6.st2.2.2.2.1.m1.1.1.3\" xref=\"S4.T6.st2.2.2.2.1.m1.1.1.3.cmml\">18</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.st2.2.2.2.1.m1.1b\"><apply id=\"S4.T6.st2.2.2.2.1.m1.1.1.cmml\" xref=\"S4.T6.st2.2.2.2.1.m1.1.1\"><lt id=\"S4.T6.st2.2.2.2.1.m1.1.1.1.cmml\" xref=\"S4.T6.st2.2.2.2.1.m1.1.1.1\"></lt><csymbol cd=\"latexml\" id=\"S4.T6.st2.2.2.2.1.m1.1.1.2.cmml\" xref=\"S4.T6.st2.2.2.2.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S4.T6.st2.2.2.2.1.m1.1.1.3.cmml\" xref=\"S4.T6.st2.2.2.2.1.m1.1.1.3\">18</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.st2.2.2.2.1.m1.1c\">&lt;18</annotation></semantics></math></th>\n<td id=\"S4.T6.st2.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st2.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.41</span></td>\n<td id=\"S4.T6.st2.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st2.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">67.09</span></td>\n</tr>\n<tr id=\"S4.T6.st2.3.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T6.st2.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S4.T6.st2.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"&lt;16\" display=\"inline\"><semantics id=\"S4.T6.st2.3.3.3.1.m1.1a\"><mrow id=\"S4.T6.st2.3.3.3.1.m1.1.1\" xref=\"S4.T6.st2.3.3.3.1.m1.1.1.cmml\"><mi id=\"S4.T6.st2.3.3.3.1.m1.1.1.2\" xref=\"S4.T6.st2.3.3.3.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S4.T6.st2.3.3.3.1.m1.1.1.1\" xref=\"S4.T6.st2.3.3.3.1.m1.1.1.1.cmml\">&lt;</mo><mn mathsize=\"70%\" id=\"S4.T6.st2.3.3.3.1.m1.1.1.3\" xref=\"S4.T6.st2.3.3.3.1.m1.1.1.3.cmml\">16</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.st2.3.3.3.1.m1.1b\"><apply id=\"S4.T6.st2.3.3.3.1.m1.1.1.cmml\" xref=\"S4.T6.st2.3.3.3.1.m1.1.1\"><lt id=\"S4.T6.st2.3.3.3.1.m1.1.1.1.cmml\" xref=\"S4.T6.st2.3.3.3.1.m1.1.1.1\"></lt><csymbol cd=\"latexml\" id=\"S4.T6.st2.3.3.3.1.m1.1.1.2.cmml\" xref=\"S4.T6.st2.3.3.3.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S4.T6.st2.3.3.3.1.m1.1.1.3.cmml\" xref=\"S4.T6.st2.3.3.3.1.m1.1.1.3\">16</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.st2.3.3.3.1.m1.1c\">&lt;16</annotation></semantics></math></th>\n<td id=\"S4.T6.st2.3.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st2.3.3.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">59.22</span></td>\n<td id=\"S4.T6.st2.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st2.3.3.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.71</span></td>\n</tr>\n<tr id=\"S4.T6.st2.4.4.4\" class=\"ltx_tr\">\n<th id=\"S4.T6.st2.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S4.T6.st2.4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"&lt;14\" display=\"inline\"><semantics id=\"S4.T6.st2.4.4.4.1.m1.1a\"><mrow id=\"S4.T6.st2.4.4.4.1.m1.1.1\" xref=\"S4.T6.st2.4.4.4.1.m1.1.1.cmml\"><mi id=\"S4.T6.st2.4.4.4.1.m1.1.1.2\" xref=\"S4.T6.st2.4.4.4.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S4.T6.st2.4.4.4.1.m1.1.1.1\" xref=\"S4.T6.st2.4.4.4.1.m1.1.1.1.cmml\">&lt;</mo><mn mathsize=\"70%\" id=\"S4.T6.st2.4.4.4.1.m1.1.1.3\" xref=\"S4.T6.st2.4.4.4.1.m1.1.1.3.cmml\">14</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.st2.4.4.4.1.m1.1b\"><apply id=\"S4.T6.st2.4.4.4.1.m1.1.1.cmml\" xref=\"S4.T6.st2.4.4.4.1.m1.1.1\"><lt id=\"S4.T6.st2.4.4.4.1.m1.1.1.1.cmml\" xref=\"S4.T6.st2.4.4.4.1.m1.1.1.1\"></lt><csymbol cd=\"latexml\" id=\"S4.T6.st2.4.4.4.1.m1.1.1.2.cmml\" xref=\"S4.T6.st2.4.4.4.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S4.T6.st2.4.4.4.1.m1.1.1.3.cmml\" xref=\"S4.T6.st2.4.4.4.1.m1.1.1.3\">14</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.st2.4.4.4.1.m1.1c\">&lt;14</annotation></semantics></math></th>\n<td id=\"S4.T6.st2.4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st2.4.4.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">39.78</span></td>\n<td id=\"S4.T6.st2.4.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T6.st2.4.4.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">58.29</span></td>\n</tr>\n<tr id=\"S4.T6.st2.5.5.5\" class=\"ltx_tr\">\n<th id=\"S4.T6.st2.5.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"><math id=\"S4.T6.st2.5.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"&lt;12\" display=\"inline\"><semantics id=\"S4.T6.st2.5.5.5.1.m1.1a\"><mrow id=\"S4.T6.st2.5.5.5.1.m1.1.1\" xref=\"S4.T6.st2.5.5.5.1.m1.1.1.cmml\"><mi id=\"S4.T6.st2.5.5.5.1.m1.1.1.2\" xref=\"S4.T6.st2.5.5.5.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S4.T6.st2.5.5.5.1.m1.1.1.1\" xref=\"S4.T6.st2.5.5.5.1.m1.1.1.1.cmml\">&lt;</mo><mn mathsize=\"70%\" id=\"S4.T6.st2.5.5.5.1.m1.1.1.3\" xref=\"S4.T6.st2.5.5.5.1.m1.1.1.3.cmml\">12</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.st2.5.5.5.1.m1.1b\"><apply id=\"S4.T6.st2.5.5.5.1.m1.1.1.cmml\" xref=\"S4.T6.st2.5.5.5.1.m1.1.1\"><lt id=\"S4.T6.st2.5.5.5.1.m1.1.1.1.cmml\" xref=\"S4.T6.st2.5.5.5.1.m1.1.1.1\"></lt><csymbol cd=\"latexml\" id=\"S4.T6.st2.5.5.5.1.m1.1.1.2.cmml\" xref=\"S4.T6.st2.5.5.5.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S4.T6.st2.5.5.5.1.m1.1.1.3.cmml\" xref=\"S4.T6.st2.5.5.5.1.m1.1.1.3\">12</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.st2.5.5.5.1.m1.1c\">&lt;12</annotation></semantics></math></th>\n<td id=\"S4.T6.st2.5.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T6.st2.5.5.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">15.58</span></td>\n<td id=\"S4.T6.st2.5.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T6.st2.5.5.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">45.05</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 1 shows the amount of time taken by Inverting Gradients run on a NVIDIA A100 80GB GPU for MNIST and CIFAR-10 on a DNN and ResNet-18 respectively. There is some variance in the average amount of time based on batch size, but when attacking the entire dataset, smaller batch sizes always take longer time as the number of total batches is much larger. CIFAR-10 takes significantly longer to reconstruct each batch given the more complex model of ResNet-18 compared to a DNN. For a batch size of 4, CIFAR-10 takes 422.81 seconds per batch. Iterating across the entire dataset on a single GPU takes 61.17 days.",
            "Table 2(a) gives the average PSNR and SSIM metrics (where a higher score indicates greater similarity with the ground truth) for the reconstructions across the entire dataset of CIFAR-10 and MNIST from Inverting Gradients. Increasing the batch size results in a lower PSNR and SSIM score for both CIFAR-10 and MNIST.",
            "Table 2(b) reports the number of leaked images for LOKI. For an FC factor of 444, LOKI leaks 78.93%percent78.9378.93\\%, 76.61%percent76.6176.61\\%, and 75.15%percent75.1575.15\\% of images on the CIFAR-10, MNIST, and Tiny ImageNet datasets. Increasing the FC layer size increases the model size overhead, but results in a higher leakage rate as images are more likely to activate different neurons. Decreasing the size creates a smaller model size overhead but results in lower leakage rate.",
            "Table 3(a) and Figure 2(a) show the results for CIFAR-10 for centralized training, federated learning (FedAvg), and training with leaked data. Centralized training achieves a 94.38%percent94.3894.38\\% accuracy and federated learning achieves a 72.76%percent72.7672.76\\% and 68.71%percent68.7168.71\\% peak accuracy with 10 and 50 clients respectively. Centralized and FL indeed provide the upper bound and the lower bound of the accuracy here. Across all attack settings, GI and LLL achieve higher model accuracy compared to federated learning. The reconstruction quality of GI is adversely affected by larger batch sizes and in turn also impacts the final model performance. With a batch size of 4, 8, and 16, models trained on the leaked data from GI achieves 90.34%percent90.3490.34\\%, 86.16%percent86.1686.16\\%, and 76.83%percent76.8376.83\\% accuracy. With even larger batch sizes, it is likely that the model performance will drop below federated learning. The performance of models using LLL data is more stable. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the accuracy only drops from 93.16%percent93.1693.16\\% to 88.86%percent88.8688.86\\%. This is spite of a large drop in the number of leaked images from 43788 (87.58%percent87.5887.58\\%) to 18242 (36.48%percent36.4836.48\\%) as shown in Table 2(b). This is a subtle and consequential result — the downstream training task can still be accomplished despite a big drop in the amount of data leakage. All prior work had stopped at the stage of evaluating the proportion of leakage and therefore had missed out on this insight.",
            "Table 3(b) and Figure 2(b) show the results for MNIST. LLL performs better than the FedAvg baseline in all cases. With an FC size factor of 8, LOKI achieves a 98.82%percent98.8298.82\\% test accuracy (nearly the same regardless of the FC size factor), only 0.07%percent0.070.07\\% lower than centralized at 98.89%percent98.8998.89\\%. GI performs slightly worse than federated learning, achieving 95.96%percent95.9695.96\\%, 95.50%percent95.5095.50\\%, and 94.29%percent94.2994.29\\% at batch sizes 8, 16, and 32 compared to 96.17%percent96.1796.17\\% and 96.18%percent96.1896.18\\% with 10 and 50 clients in FedAvg. We believe this is because the noise in the reconstructions hampers model performance with GI. This affects MNIST more severely than for CIFAR because the MNIST images are more sparse and sensitive to noise.",
            "The top-1 validation accuracy for Tiny ImageNet are given in Table 3(c) and Figure 2(c). The total number of leaked images hurts LLL more here than in the other two datasets. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the validation accuracy drops from 46.70%percent46.7046.70\\% to 35.20%percent35.2035.20\\%, an 11.5%percent11.511.5\\% drop in performance. Federated learning achieves 37.00%percent37.0037.00\\% and 35.06%percent35.0635.06\\% accuracy with 10 and 50 clients respectively. FL with 10 clients has slightly better accuracy than LLL with an FC size factor of 1. However, any LLL setting with a larger FC layer achieves higher accuracy than both settings in federated learning.",
            "Table 4 shows the leakage rate of LOKI in FedAvg compared to FedSGD. Using CSF=500CSF500\\textit{CSF}=500, the leakage rate of LOKI in FedAvg is substantially higher than the leakage rates in the FedSGD setting. Using an FC layer of half the size, the leakage rate in FedAvg is comparable to that of FedSGD. For example, LOKI FedAvg factor 1 achieves a leakage rate of 58.39%percent58.3958.39\\% compared to LOKI FedSGD factor 2 which achieves a leakage rate of 59.76%percent59.7659.76\\%.",
            "Table 5 shows the testing accuracy of LOKI FedAvg for the different FC layer sizes. The test accuracies are very comparable to the FedSGD settings. However, with a smaller FC layer size factor, the effect of having a larger total number of leaked images becomes visible. With FC factor 1, LOKI FedAvg achieves 91.11%percent91.1191.11\\% accuracy while LOKI FedSGD achieves 88.86%percent88.8688.86\\%.",
            "Quality of reconstruction is an important concern for GI attacks. As the batch size continues to increase, the reconstruction quality decreases. Table 2(a) also shows this relationship. From the previous experiments, we also see a negative relationship between the increasing batch size and the usefulness of the leaked data in downstream model training.",
            "Here, we explore whether poor quality reconstructions are still useful for training models. Using the CIFAR-10 leaked dataset created using Inverting Gradients on a batch size of 16, we first sort the reconstructions by their PSNR value. Our first set of experiments removes the worst images from training. Table 6(a) shows the percent of images above the PSNR threshold and the final test accuracy when trained on that data. Compared to the baseline accuracy of 76.83%percent76.8376.83\\% achieved by using the entire leaked dataset, even removing the worst images with a PSNR <12absent12<12 from the training set results in a small but non-zero performance loss.\nThen we take a different look at this question by now training on the worst images.\nTable 6(b) shows the test accuracy of models trained on all images below a PSNR threshold. Even when training on the lowest quality images with a PSNR <12absent12<12, the model achieves a 45.05%percent45.0545.05\\% final testing accuracy, significantly worse than the baseline, but much above zero.",
            "While some reconstructions have very low quality, only images of the same label will be swapped within the batch during label matching. This is a desirable property for training on leaked data. As long as the set of labels is leaked correctly, the optimization process will match images to a correct label.\nSo even though the image quality is poor (and can be entirely unrecognizable), since the label is correctly matched, such an image still does help in the training (this buttresses the message from Tables 6(a) and 6(b)). Naturally, as the PSNR goes down, the image helps less and less."
        ]
    },
    "S7.T7": {
        "caption": "Table 7:  Federated learning test accuracy on CIFAR-10, MNIST, and Tiny ImageNet. A bias of 0.5 is used for the non-IID training. The same settings are used between FedSGD and FedAvg outside of the number of rounds. The number of rounds in FedSGD is 3×3\\times the number of rounds in FedAvg (3 local iterations in FedAvg).",
        "table": "<table id=\"S7.T7.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S7.T7.4.1.1\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" colspan=\"4\"><span id=\"S7.T7.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">CIFAR-10</span></th>\n</tr>\n<tr id=\"S7.T7.4.2.2\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.2.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S7.T7.4.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">\n<table id=\"S7.T7.4.2.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S7.T7.4.2.2.2.1.1\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.2.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.2.2.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Number</span></td>\n</tr>\n<tr id=\"S7.T7.4.2.2.2.1.2\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.2.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.2.2.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">of clients</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S7.T7.4.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S7.T7.4.2.2.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S7.T7.4.2.2.3.1.1\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.2.2.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.2.2.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">IID (I) or</span></td>\n</tr>\n<tr id=\"S7.T7.4.2.2.3.1.2\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.2.2.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.2.2.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Non-IID (N)</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S7.T7.4.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S7.T7.4.2.2.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S7.T7.4.2.2.4.1.1\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.2.2.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.2.2.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Test</span></td>\n</tr>\n<tr id=\"S7.T7.4.2.2.4.1.2\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.2.2.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.2.2.4.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Acc.</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S7.T7.4.3.1\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.3.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedAvg</span></th>\n<th id=\"S7.T7.4.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></th>\n<td id=\"S7.T7.4.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.3.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">I</span></td>\n<td id=\"S7.T7.4.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.3.1.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">75.13</span></td>\n</tr>\n<tr id=\"S7.T7.4.4.2\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.4.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.4.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.4.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></th>\n<td id=\"S7.T7.4.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.4.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">N</span></td>\n<td id=\"S7.T7.4.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.4.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.76</span></td>\n</tr>\n<tr id=\"S7.T7.4.5.3\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.5.3.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.5.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.5.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></th>\n<td id=\"S7.T7.4.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.5.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">I</span></td>\n<td id=\"S7.T7.4.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.5.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.45</span></td>\n</tr>\n<tr id=\"S7.T7.4.6.4\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.6.4.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.6.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.6.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></th>\n<td id=\"S7.T7.4.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.6.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">N</span></td>\n<td id=\"S7.T7.4.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.6.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">68.71</span></td>\n</tr>\n<tr id=\"S7.T7.4.7.5\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.7.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedSGD</span></th>\n<th id=\"S7.T7.4.7.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.7.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></th>\n<td id=\"S7.T7.4.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.7.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">I</span></td>\n<td id=\"S7.T7.4.7.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.7.5.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.24</span></td>\n</tr>\n<tr id=\"S7.T7.4.8.6\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.8.6.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.8.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.8.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></th>\n<td id=\"S7.T7.4.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.8.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">N</span></td>\n<td id=\"S7.T7.4.8.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.8.6.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">68.78</span></td>\n</tr>\n<tr id=\"S7.T7.4.9.7\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.9.7.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.9.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.9.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></th>\n<td id=\"S7.T7.4.9.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.9.7.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">I</span></td>\n<td id=\"S7.T7.4.9.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.9.7.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">65.95</span></td>\n</tr>\n<tr id=\"S7.T7.4.10.8\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.10.8.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.10.8.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.10.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></th>\n<td id=\"S7.T7.4.10.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.10.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">N</span></td>\n<td id=\"S7.T7.4.10.8.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.10.8.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.88</span></td>\n</tr>\n<tr id=\"S7.T7.4.11.9\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.11.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" colspan=\"4\"><span id=\"S7.T7.4.11.9.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">MNIST</span></th>\n</tr>\n<tr id=\"S7.T7.4.12.10\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.12.10.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S7.T7.4.12.10.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">\n<table id=\"S7.T7.4.12.10.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S7.T7.4.12.10.2.1.1\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.12.10.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.12.10.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Number</span></td>\n</tr>\n<tr id=\"S7.T7.4.12.10.2.1.2\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.12.10.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.12.10.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">of clients</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S7.T7.4.12.10.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S7.T7.4.12.10.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S7.T7.4.12.10.3.1.1\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.12.10.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.12.10.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">IID (I) or</span></td>\n</tr>\n<tr id=\"S7.T7.4.12.10.3.1.2\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.12.10.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.12.10.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Non-IID (N)</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S7.T7.4.12.10.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S7.T7.4.12.10.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S7.T7.4.12.10.4.1.1\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.12.10.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.12.10.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Test</span></td>\n</tr>\n<tr id=\"S7.T7.4.12.10.4.1.2\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.12.10.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.12.10.4.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Acc.</span></td>\n</tr>\n</table>\n</th>\n</tr>\n<tr id=\"S7.T7.4.13.11\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.13.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.13.11.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedAvg</span></th>\n<th id=\"S7.T7.4.13.11.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.13.11.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></th>\n<td id=\"S7.T7.4.13.11.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.13.11.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">I</span></td>\n<td id=\"S7.T7.4.13.11.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.13.11.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.62</span></td>\n</tr>\n<tr id=\"S7.T7.4.14.12\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.14.12.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.14.12.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.14.12.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></th>\n<td id=\"S7.T7.4.14.12.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.14.12.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">N</span></td>\n<td id=\"S7.T7.4.14.12.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.14.12.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.17</span></td>\n</tr>\n<tr id=\"S7.T7.4.15.13\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.15.13.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.15.13.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.15.13.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></th>\n<td id=\"S7.T7.4.15.13.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.15.13.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">I</span></td>\n<td id=\"S7.T7.4.15.13.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.15.13.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.68</span></td>\n</tr>\n<tr id=\"S7.T7.4.16.14\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.16.14.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.16.14.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.16.14.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></th>\n<td id=\"S7.T7.4.16.14.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.16.14.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">N</span></td>\n<td id=\"S7.T7.4.16.14.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.16.14.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.18</span></td>\n</tr>\n<tr id=\"S7.T7.4.17.15\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.17.15.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.17.15.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedSGD</span></th>\n<th id=\"S7.T7.4.17.15.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.17.15.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></th>\n<td id=\"S7.T7.4.17.15.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.17.15.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">I</span></td>\n<td id=\"S7.T7.4.17.15.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.17.15.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.68</span></td>\n</tr>\n<tr id=\"S7.T7.4.18.16\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.18.16.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.18.16.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.18.16.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></th>\n<td id=\"S7.T7.4.18.16.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.18.16.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">N</span></td>\n<td id=\"S7.T7.4.18.16.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.18.16.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.76</span></td>\n</tr>\n<tr id=\"S7.T7.4.19.17\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.19.17.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.19.17.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.19.17.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></th>\n<td id=\"S7.T7.4.19.17.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.19.17.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">I</span></td>\n<td id=\"S7.T7.4.19.17.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.19.17.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">96.84</span></td>\n</tr>\n<tr id=\"S7.T7.4.20.18\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.20.18.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.20.18.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.20.18.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></th>\n<td id=\"S7.T7.4.20.18.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.20.18.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">N</span></td>\n<td id=\"S7.T7.4.20.18.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.20.18.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.83</span></td>\n</tr>\n<tr id=\"S7.T7.4.21.19\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.21.19.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" colspan=\"4\"><span id=\"S7.T7.4.21.19.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Tiny ImageNet</span></th>\n</tr>\n<tr id=\"S7.T7.4.22.20\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.22.20.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S7.T7.4.22.20.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">\n<table id=\"S7.T7.4.22.20.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S7.T7.4.22.20.2.1.1\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.22.20.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.22.20.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Number</span></td>\n</tr>\n<tr id=\"S7.T7.4.22.20.2.1.2\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.22.20.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.22.20.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">of clients</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S7.T7.4.22.20.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S7.T7.4.22.20.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S7.T7.4.22.20.3.1.1\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.22.20.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.22.20.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">IID (I) or</span></td>\n</tr>\n<tr id=\"S7.T7.4.22.20.3.1.2\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.22.20.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.22.20.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Non-IID (N)</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S7.T7.4.22.20.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S7.T7.4.22.20.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S7.T7.4.22.20.4.1.1\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.22.20.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.22.20.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Test</span></td>\n</tr>\n<tr id=\"S7.T7.4.22.20.4.1.2\" class=\"ltx_tr\">\n<td id=\"S7.T7.4.22.20.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S7.T7.4.22.20.4.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Acc.</span></td>\n</tr>\n</table>\n</th>\n</tr>\n<tr id=\"S7.T7.4.23.21\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.23.21.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.23.21.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedAvg</span></th>\n<th id=\"S7.T7.4.23.21.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.23.21.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></th>\n<td id=\"S7.T7.4.23.21.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.23.21.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">I</span></td>\n<td id=\"S7.T7.4.23.21.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.23.21.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">37.18</span></td>\n</tr>\n<tr id=\"S7.T7.4.24.22\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.24.22.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.24.22.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.24.22.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></th>\n<td id=\"S7.T7.4.24.22.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.24.22.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">N</span></td>\n<td id=\"S7.T7.4.24.22.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.24.22.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">37.00</span></td>\n</tr>\n<tr id=\"S7.T7.4.25.23\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.25.23.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.25.23.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.25.23.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></th>\n<td id=\"S7.T7.4.25.23.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.25.23.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">I</span></td>\n<td id=\"S7.T7.4.25.23.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.25.23.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">38.84</span></td>\n</tr>\n<tr id=\"S7.T7.4.26.24\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.26.24.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.26.24.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.26.24.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></th>\n<td id=\"S7.T7.4.26.24.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.26.24.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">N</span></td>\n<td id=\"S7.T7.4.26.24.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.26.24.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">35.06</span></td>\n</tr>\n<tr id=\"S7.T7.4.27.25\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.27.25.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.27.25.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedSGD</span></th>\n<th id=\"S7.T7.4.27.25.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.27.25.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></th>\n<td id=\"S7.T7.4.27.25.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.27.25.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">I</span></td>\n<td id=\"S7.T7.4.27.25.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T7.4.27.25.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">35.56</span></td>\n</tr>\n<tr id=\"S7.T7.4.28.26\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.28.26.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.28.26.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.28.26.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></th>\n<td id=\"S7.T7.4.28.26.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.28.26.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">N</span></td>\n<td id=\"S7.T7.4.28.26.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.28.26.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">34.27</span></td>\n</tr>\n<tr id=\"S7.T7.4.29.27\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.29.27.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.29.27.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S7.T7.4.29.27.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></th>\n<td id=\"S7.T7.4.29.27.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.29.27.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">I</span></td>\n<td id=\"S7.T7.4.29.27.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T7.4.29.27.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">32.77</span></td>\n</tr>\n<tr id=\"S7.T7.4.30.28\" class=\"ltx_tr\">\n<th id=\"S7.T7.4.30.28.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"></th>\n<th id=\"S7.T7.4.30.28.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\"><span id=\"S7.T7.4.30.28.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></th>\n<td id=\"S7.T7.4.30.28.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S7.T7.4.30.28.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">N</span></td>\n<td id=\"S7.T7.4.30.28.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S7.T7.4.30.28.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">26.56</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Table ",
                "7",
                " shows additional test accuracy in federated learning on CIFAR-10, MNIST, and Tiny ImageNet. We include results for both IID and non-IID (with bias",
                "=",
                "0.5",
                "absent",
                "0.5",
                "=0.5",
                "). For FedSGD training, we use ",
                "3",
                "×",
                "3\\times",
                " the number of rounds compared to FedAvg (so the models have seen the same amount data in both cases, as we have 3 local iterations in FedAvg). All other settings are the same. An (expected) observed trend is that IID training outperforms non-IID. Both CIFAR-10 and Tiny ImageNet in FedAvg perform better than FedSGD in all settings. For MNIST, the performance is similar regardless of FedAvg or FedSGD, IID or non-IID, achieving around ",
                "96",
                "%",
                "percent",
                "96",
                "96\\%",
                " accuracy across the board."
            ]
        ]
    },
    "S9.T8": {
        "caption": "Table 8:  Leakage rate and test accuracy on CIFAR-10 for LOKI, Robbing the Fed, and Trap Weights in FedSGD. FC layer size factors of 8, 4, and 2 used with a batch size of 64. Models trained from scratch on leaked data.",
        "table": "<table id=\"S9.T8.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S9.T8.2.1.1\" class=\"ltx_tr\">\n<th id=\"S9.T8.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S9.T8.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">\n<table id=\"S9.T8.2.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S9.T8.2.1.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S9.T8.2.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S9.T8.2.1.1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FC size</span></td>\n</tr>\n<tr id=\"S9.T8.2.1.1.2.1.2\" class=\"ltx_tr\">\n<td id=\"S9.T8.2.1.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S9.T8.2.1.1.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">factor</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S9.T8.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S9.T8.2.1.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S9.T8.2.1.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S9.T8.2.1.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S9.T8.2.1.1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Leakage</span></td>\n</tr>\n<tr id=\"S9.T8.2.1.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S9.T8.2.1.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S9.T8.2.1.1.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">rate</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S9.T8.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S9.T8.2.1.1.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S9.T8.2.1.1.4.1.1\" class=\"ltx_tr\">\n<td id=\"S9.T8.2.1.1.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S9.T8.2.1.1.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Test</span></td>\n</tr>\n<tr id=\"S9.T8.2.1.1.4.1.2\" class=\"ltx_tr\">\n<td id=\"S9.T8.2.1.1.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S9.T8.2.1.1.4.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">accuracy</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S9.T8.2.2.1\" class=\"ltx_tr\">\n<th id=\"S9.T8.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S9.T8.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">LOKI</span></th>\n<th id=\"S9.T8.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S9.T8.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">8</span></th>\n<td id=\"S9.T8.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S9.T8.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">87.58</span></td>\n<td id=\"S9.T8.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S9.T8.2.2.1.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">93.16</span></td>\n</tr>\n<tr id=\"S9.T8.2.3.2\" class=\"ltx_tr\">\n<th id=\"S9.T8.2.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S9.T8.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">4</span></th>\n<td id=\"S9.T8.2.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S9.T8.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">78.93</span></td>\n<td id=\"S9.T8.2.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S9.T8.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">92.94</span></td>\n</tr>\n<tr id=\"S9.T8.2.4.3\" class=\"ltx_tr\">\n<th id=\"S9.T8.2.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S9.T8.2.4.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">2</span></th>\n<td id=\"S9.T8.2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S9.T8.2.4.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">59.76</span></td>\n<td id=\"S9.T8.2.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S9.T8.2.4.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">91.90</span></td>\n</tr>\n<tr id=\"S9.T8.2.5.4\" class=\"ltx_tr\">\n<th id=\"S9.T8.2.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S9.T8.2.5.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">\n<span id=\"S9.T8.2.5.4.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S9.T8.2.5.4.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S9.T8.2.5.4.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Robbing</span></span>\n<span id=\"S9.T8.2.5.4.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S9.T8.2.5.4.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">the Fed</span></span>\n</span></span></th>\n<th id=\"S9.T8.2.5.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S9.T8.2.5.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">8</span></th>\n<td id=\"S9.T8.2.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S9.T8.2.5.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">87.50</span></td>\n<td id=\"S9.T8.2.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S9.T8.2.5.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">93.10</span></td>\n</tr>\n<tr id=\"S9.T8.2.6.5\" class=\"ltx_tr\">\n<th id=\"S9.T8.2.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S9.T8.2.6.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">4</span></th>\n<td id=\"S9.T8.2.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S9.T8.2.6.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">78.97</span></td>\n<td id=\"S9.T8.2.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S9.T8.2.6.5.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">93.02</span></td>\n</tr>\n<tr id=\"S9.T8.2.7.6\" class=\"ltx_tr\">\n<th id=\"S9.T8.2.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S9.T8.2.7.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">2</span></th>\n<td id=\"S9.T8.2.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S9.T8.2.7.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">59.72</span></td>\n<td id=\"S9.T8.2.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S9.T8.2.7.6.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">92.12</span></td>\n</tr>\n<tr id=\"S9.T8.2.8.7\" class=\"ltx_tr\">\n<th id=\"S9.T8.2.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S9.T8.2.8.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">\n<span id=\"S9.T8.2.8.7.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S9.T8.2.8.7.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S9.T8.2.8.7.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Trap</span></span>\n<span id=\"S9.T8.2.8.7.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S9.T8.2.8.7.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Weights</span></span>\n</span></span></th>\n<th id=\"S9.T8.2.8.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S9.T8.2.8.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">8</span></th>\n<td id=\"S9.T8.2.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S9.T8.2.8.7.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">58.11</span></td>\n<td id=\"S9.T8.2.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S9.T8.2.8.7.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">91.84</span></td>\n</tr>\n<tr id=\"S9.T8.2.9.8\" class=\"ltx_tr\">\n<th id=\"S9.T8.2.9.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S9.T8.2.9.8.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">4</span></th>\n<td id=\"S9.T8.2.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S9.T8.2.9.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">45.92</span></td>\n<td id=\"S9.T8.2.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S9.T8.2.9.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">90.09</span></td>\n</tr>\n<tr id=\"S9.T8.2.10.9\" class=\"ltx_tr\">\n<th id=\"S9.T8.2.10.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\"><span id=\"S9.T8.2.10.9.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">2</span></th>\n<td id=\"S9.T8.2.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S9.T8.2.10.9.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">30.46</span></td>\n<td id=\"S9.T8.2.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S9.T8.2.10.9.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">86.38</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Table ",
                "8",
                " shows the leakage rate and test accuracy on CIFAR-10 for LOKI ",
                "[",
                "30",
                "]",
                ", Robbing the Fed ",
                "[",
                "5",
                "]",
                ", and trap weights ",
                "[",
                "1",
                "]",
                ". Attacks are done in FedSGD with a batch size of 64. LOKI and Robbing the Fed have no additional parameters besides the FC size factor (FC layer size = FC size factor",
                "×",
                "\\times",
                "batch size). For trap weights, in addition to the FC size factor, a scaling factor of 0.96 achieves the highest leakage rate for each FC size factor (checked by 0.1 increments). LOKI and Robbing the Fed achieve very similar leakage rates and model performances. Trap weights has lower leakage rate than both other methods and, as a result, lower model performance for the same FC size factors."
            ]
        ]
    },
    "S10.T9": {
        "caption": "Table 9:  Training models using CIFAR-10 leaked data from inverting gradients batch size 16. Only the reconstructions with an SSIM (a) above and (b) below the threshold are used in training.",
        "table": "<table id=\"S10.T9.st1.6.6\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S10.T9.st1.6.6.7.1\" class=\"ltx_tr\">\n<th id=\"S10.T9.st1.6.6.7.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S10.T9.st1.6.6.7.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">SSIM</span></th>\n<th id=\"S10.T9.st1.6.6.7.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S10.T9.st1.6.6.7.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S10.T9.st1.6.6.7.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S10.T9.st1.6.6.7.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S10.T9.st1.6.6.7.1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">% imgs</span></td>\n</tr>\n<tr id=\"S10.T9.st1.6.6.7.1.2.1.2\" class=\"ltx_tr\">\n<td id=\"S10.T9.st1.6.6.7.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S10.T9.st1.6.6.7.1.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">kept</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S10.T9.st1.6.6.7.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S10.T9.st1.6.6.7.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S10.T9.st1.6.6.7.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S10.T9.st1.6.6.7.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S10.T9.st1.6.6.7.1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Test</span></td>\n</tr>\n<tr id=\"S10.T9.st1.6.6.7.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S10.T9.st1.6.6.7.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S10.T9.st1.6.6.7.1.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">accuracy</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S10.T9.st1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S10.T9.st1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><math id=\"S10.T9.st1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;0.7\" display=\"inline\"><semantics id=\"S10.T9.st1.1.1.1.1.m1.1a\"><mrow id=\"S10.T9.st1.1.1.1.1.m1.1.1\" xref=\"S10.T9.st1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S10.T9.st1.1.1.1.1.m1.1.1.2\" xref=\"S10.T9.st1.1.1.1.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st1.1.1.1.1.m1.1.1.1\" xref=\"S10.T9.st1.1.1.1.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st1.1.1.1.1.m1.1.1.3\" xref=\"S10.T9.st1.1.1.1.1.m1.1.1.3.cmml\">0.7</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st1.1.1.1.1.m1.1b\"><apply id=\"S10.T9.st1.1.1.1.1.m1.1.1.cmml\" xref=\"S10.T9.st1.1.1.1.1.m1.1.1\"><gt id=\"S10.T9.st1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S10.T9.st1.1.1.1.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S10.T9.st1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S10.T9.st1.1.1.1.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S10.T9.st1.1.1.1.1.m1.1.1.3\">0.7</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st1.1.1.1.1.m1.1c\">&gt;0.7</annotation></semantics></math></th>\n<td id=\"S10.T9.st1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S10.T9.st1.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">17.25</span></td>\n<td id=\"S10.T9.st1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S10.T9.st1.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.32</span></td>\n</tr>\n<tr id=\"S10.T9.st1.2.2.2\" class=\"ltx_tr\">\n<th id=\"S10.T9.st1.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S10.T9.st1.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;0.6\" display=\"inline\"><semantics id=\"S10.T9.st1.2.2.2.1.m1.1a\"><mrow id=\"S10.T9.st1.2.2.2.1.m1.1.1\" xref=\"S10.T9.st1.2.2.2.1.m1.1.1.cmml\"><mi id=\"S10.T9.st1.2.2.2.1.m1.1.1.2\" xref=\"S10.T9.st1.2.2.2.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st1.2.2.2.1.m1.1.1.1\" xref=\"S10.T9.st1.2.2.2.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st1.2.2.2.1.m1.1.1.3\" xref=\"S10.T9.st1.2.2.2.1.m1.1.1.3.cmml\">0.6</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st1.2.2.2.1.m1.1b\"><apply id=\"S10.T9.st1.2.2.2.1.m1.1.1.cmml\" xref=\"S10.T9.st1.2.2.2.1.m1.1.1\"><gt id=\"S10.T9.st1.2.2.2.1.m1.1.1.1.cmml\" xref=\"S10.T9.st1.2.2.2.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S10.T9.st1.2.2.2.1.m1.1.1.2.cmml\" xref=\"S10.T9.st1.2.2.2.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st1.2.2.2.1.m1.1.1.3.cmml\" xref=\"S10.T9.st1.2.2.2.1.m1.1.1.3\">0.6</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st1.2.2.2.1.m1.1c\">&gt;0.6</annotation></semantics></math></th>\n<td id=\"S10.T9.st1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st1.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">30.26</span></td>\n<td id=\"S10.T9.st1.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st1.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.68</span></td>\n</tr>\n<tr id=\"S10.T9.st1.3.3.3\" class=\"ltx_tr\">\n<th id=\"S10.T9.st1.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S10.T9.st1.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;0.5\" display=\"inline\"><semantics id=\"S10.T9.st1.3.3.3.1.m1.1a\"><mrow id=\"S10.T9.st1.3.3.3.1.m1.1.1\" xref=\"S10.T9.st1.3.3.3.1.m1.1.1.cmml\"><mi id=\"S10.T9.st1.3.3.3.1.m1.1.1.2\" xref=\"S10.T9.st1.3.3.3.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st1.3.3.3.1.m1.1.1.1\" xref=\"S10.T9.st1.3.3.3.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st1.3.3.3.1.m1.1.1.3\" xref=\"S10.T9.st1.3.3.3.1.m1.1.1.3.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st1.3.3.3.1.m1.1b\"><apply id=\"S10.T9.st1.3.3.3.1.m1.1.1.cmml\" xref=\"S10.T9.st1.3.3.3.1.m1.1.1\"><gt id=\"S10.T9.st1.3.3.3.1.m1.1.1.1.cmml\" xref=\"S10.T9.st1.3.3.3.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S10.T9.st1.3.3.3.1.m1.1.1.2.cmml\" xref=\"S10.T9.st1.3.3.3.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st1.3.3.3.1.m1.1.1.3.cmml\" xref=\"S10.T9.st1.3.3.3.1.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st1.3.3.3.1.m1.1c\">&gt;0.5</annotation></semantics></math></th>\n<td id=\"S10.T9.st1.3.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st1.3.3.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">44.76</span></td>\n<td id=\"S10.T9.st1.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st1.3.3.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.94</span></td>\n</tr>\n<tr id=\"S10.T9.st1.4.4.4\" class=\"ltx_tr\">\n<th id=\"S10.T9.st1.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S10.T9.st1.4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;0.4\" display=\"inline\"><semantics id=\"S10.T9.st1.4.4.4.1.m1.1a\"><mrow id=\"S10.T9.st1.4.4.4.1.m1.1.1\" xref=\"S10.T9.st1.4.4.4.1.m1.1.1.cmml\"><mi id=\"S10.T9.st1.4.4.4.1.m1.1.1.2\" xref=\"S10.T9.st1.4.4.4.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st1.4.4.4.1.m1.1.1.1\" xref=\"S10.T9.st1.4.4.4.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st1.4.4.4.1.m1.1.1.3\" xref=\"S10.T9.st1.4.4.4.1.m1.1.1.3.cmml\">0.4</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st1.4.4.4.1.m1.1b\"><apply id=\"S10.T9.st1.4.4.4.1.m1.1.1.cmml\" xref=\"S10.T9.st1.4.4.4.1.m1.1.1\"><gt id=\"S10.T9.st1.4.4.4.1.m1.1.1.1.cmml\" xref=\"S10.T9.st1.4.4.4.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S10.T9.st1.4.4.4.1.m1.1.1.2.cmml\" xref=\"S10.T9.st1.4.4.4.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st1.4.4.4.1.m1.1.1.3.cmml\" xref=\"S10.T9.st1.4.4.4.1.m1.1.1.3\">0.4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st1.4.4.4.1.m1.1c\">&gt;0.4</annotation></semantics></math></th>\n<td id=\"S10.T9.st1.4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st1.4.4.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">61.71</span></td>\n<td id=\"S10.T9.st1.4.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st1.4.4.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.61</span></td>\n</tr>\n<tr id=\"S10.T9.st1.5.5.5\" class=\"ltx_tr\">\n<th id=\"S10.T9.st1.5.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S10.T9.st1.5.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;0.3\" display=\"inline\"><semantics id=\"S10.T9.st1.5.5.5.1.m1.1a\"><mrow id=\"S10.T9.st1.5.5.5.1.m1.1.1\" xref=\"S10.T9.st1.5.5.5.1.m1.1.1.cmml\"><mi id=\"S10.T9.st1.5.5.5.1.m1.1.1.2\" xref=\"S10.T9.st1.5.5.5.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st1.5.5.5.1.m1.1.1.1\" xref=\"S10.T9.st1.5.5.5.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st1.5.5.5.1.m1.1.1.3\" xref=\"S10.T9.st1.5.5.5.1.m1.1.1.3.cmml\">0.3</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st1.5.5.5.1.m1.1b\"><apply id=\"S10.T9.st1.5.5.5.1.m1.1.1.cmml\" xref=\"S10.T9.st1.5.5.5.1.m1.1.1\"><gt id=\"S10.T9.st1.5.5.5.1.m1.1.1.1.cmml\" xref=\"S10.T9.st1.5.5.5.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S10.T9.st1.5.5.5.1.m1.1.1.2.cmml\" xref=\"S10.T9.st1.5.5.5.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st1.5.5.5.1.m1.1.1.3.cmml\" xref=\"S10.T9.st1.5.5.5.1.m1.1.1.3\">0.3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st1.5.5.5.1.m1.1c\">&gt;0.3</annotation></semantics></math></th>\n<td id=\"S10.T9.st1.5.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st1.5.5.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">80.56</span></td>\n<td id=\"S10.T9.st1.5.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st1.5.5.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">77.02</span></td>\n</tr>\n<tr id=\"S10.T9.st1.6.6.6\" class=\"ltx_tr\">\n<th id=\"S10.T9.st1.6.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"><math id=\"S10.T9.st1.6.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;0.2\" display=\"inline\"><semantics id=\"S10.T9.st1.6.6.6.1.m1.1a\"><mrow id=\"S10.T9.st1.6.6.6.1.m1.1.1\" xref=\"S10.T9.st1.6.6.6.1.m1.1.1.cmml\"><mi id=\"S10.T9.st1.6.6.6.1.m1.1.1.2\" xref=\"S10.T9.st1.6.6.6.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st1.6.6.6.1.m1.1.1.1\" xref=\"S10.T9.st1.6.6.6.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st1.6.6.6.1.m1.1.1.3\" xref=\"S10.T9.st1.6.6.6.1.m1.1.1.3.cmml\">0.2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st1.6.6.6.1.m1.1b\"><apply id=\"S10.T9.st1.6.6.6.1.m1.1.1.cmml\" xref=\"S10.T9.st1.6.6.6.1.m1.1.1\"><gt id=\"S10.T9.st1.6.6.6.1.m1.1.1.1.cmml\" xref=\"S10.T9.st1.6.6.6.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S10.T9.st1.6.6.6.1.m1.1.1.2.cmml\" xref=\"S10.T9.st1.6.6.6.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st1.6.6.6.1.m1.1.1.3.cmml\" xref=\"S10.T9.st1.6.6.6.1.m1.1.1.3\">0.2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st1.6.6.6.1.m1.1c\">&gt;0.2</annotation></semantics></math></th>\n<td id=\"S10.T9.st1.6.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S10.T9.st1.6.6.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.55</span></td>\n<td id=\"S10.T9.st1.6.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S10.T9.st1.6.6.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">77.31</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Table ",
                "9",
                " shows the test accuracy of models trained while removing images based on the SSIM. Table ",
                "9(a)",
                " shows accuracy when only images ",
                "above",
                " an SSIM threshold are used. Table ",
                "9(b)",
                " shows accuracy when images ",
                "below",
                " an SSIM threshold are used. For SSIM, removing a set of the worst images with SSIM ",
                "<",
                "0.2",
                "absent",
                "0.2",
                "<0.2",
                " or ",
                "<",
                "0.3",
                "absent",
                "0.3",
                "<0.3",
                " results in a small model performance increase compared to when all images are included (which achieves ",
                "76.83",
                "%",
                "percent",
                "76.83",
                "76.83\\%",
                "). Similar to PSNR, training on a set of the worst quality reconstructions (SSIM ",
                "<",
                "0.2",
                "absent",
                "0.2",
                "<0.2",
                ") achieves ",
                "32.36",
                "%",
                "percent",
                "32.36",
                "32.36\\%",
                " accuracy, a higher accuracy than random guessing, but much lower performance compared to the baseline."
            ]
        ]
    },
    "S10.T9.st1": {
        "caption": "(a) PSNR above threshold",
        "table": "<table id=\"S10.T9.st1.6.6\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S10.T9.st1.6.6.7.1\" class=\"ltx_tr\">\n<th id=\"S10.T9.st1.6.6.7.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S10.T9.st1.6.6.7.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">SSIM</span></th>\n<th id=\"S10.T9.st1.6.6.7.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S10.T9.st1.6.6.7.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S10.T9.st1.6.6.7.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S10.T9.st1.6.6.7.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S10.T9.st1.6.6.7.1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">% imgs</span></td>\n</tr>\n<tr id=\"S10.T9.st1.6.6.7.1.2.1.2\" class=\"ltx_tr\">\n<td id=\"S10.T9.st1.6.6.7.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S10.T9.st1.6.6.7.1.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">kept</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S10.T9.st1.6.6.7.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S10.T9.st1.6.6.7.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S10.T9.st1.6.6.7.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S10.T9.st1.6.6.7.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S10.T9.st1.6.6.7.1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Test</span></td>\n</tr>\n<tr id=\"S10.T9.st1.6.6.7.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S10.T9.st1.6.6.7.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S10.T9.st1.6.6.7.1.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">accuracy</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S10.T9.st1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S10.T9.st1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><math id=\"S10.T9.st1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;0.7\" display=\"inline\"><semantics id=\"S10.T9.st1.1.1.1.1.m1.1a\"><mrow id=\"S10.T9.st1.1.1.1.1.m1.1.1\" xref=\"S10.T9.st1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S10.T9.st1.1.1.1.1.m1.1.1.2\" xref=\"S10.T9.st1.1.1.1.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st1.1.1.1.1.m1.1.1.1\" xref=\"S10.T9.st1.1.1.1.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st1.1.1.1.1.m1.1.1.3\" xref=\"S10.T9.st1.1.1.1.1.m1.1.1.3.cmml\">0.7</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st1.1.1.1.1.m1.1b\"><apply id=\"S10.T9.st1.1.1.1.1.m1.1.1.cmml\" xref=\"S10.T9.st1.1.1.1.1.m1.1.1\"><gt id=\"S10.T9.st1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S10.T9.st1.1.1.1.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S10.T9.st1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S10.T9.st1.1.1.1.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S10.T9.st1.1.1.1.1.m1.1.1.3\">0.7</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st1.1.1.1.1.m1.1c\">&gt;0.7</annotation></semantics></math></th>\n<td id=\"S10.T9.st1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S10.T9.st1.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">17.25</span></td>\n<td id=\"S10.T9.st1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S10.T9.st1.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.32</span></td>\n</tr>\n<tr id=\"S10.T9.st1.2.2.2\" class=\"ltx_tr\">\n<th id=\"S10.T9.st1.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S10.T9.st1.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;0.6\" display=\"inline\"><semantics id=\"S10.T9.st1.2.2.2.1.m1.1a\"><mrow id=\"S10.T9.st1.2.2.2.1.m1.1.1\" xref=\"S10.T9.st1.2.2.2.1.m1.1.1.cmml\"><mi id=\"S10.T9.st1.2.2.2.1.m1.1.1.2\" xref=\"S10.T9.st1.2.2.2.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st1.2.2.2.1.m1.1.1.1\" xref=\"S10.T9.st1.2.2.2.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st1.2.2.2.1.m1.1.1.3\" xref=\"S10.T9.st1.2.2.2.1.m1.1.1.3.cmml\">0.6</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st1.2.2.2.1.m1.1b\"><apply id=\"S10.T9.st1.2.2.2.1.m1.1.1.cmml\" xref=\"S10.T9.st1.2.2.2.1.m1.1.1\"><gt id=\"S10.T9.st1.2.2.2.1.m1.1.1.1.cmml\" xref=\"S10.T9.st1.2.2.2.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S10.T9.st1.2.2.2.1.m1.1.1.2.cmml\" xref=\"S10.T9.st1.2.2.2.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st1.2.2.2.1.m1.1.1.3.cmml\" xref=\"S10.T9.st1.2.2.2.1.m1.1.1.3\">0.6</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st1.2.2.2.1.m1.1c\">&gt;0.6</annotation></semantics></math></th>\n<td id=\"S10.T9.st1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st1.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">30.26</span></td>\n<td id=\"S10.T9.st1.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st1.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.68</span></td>\n</tr>\n<tr id=\"S10.T9.st1.3.3.3\" class=\"ltx_tr\">\n<th id=\"S10.T9.st1.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S10.T9.st1.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;0.5\" display=\"inline\"><semantics id=\"S10.T9.st1.3.3.3.1.m1.1a\"><mrow id=\"S10.T9.st1.3.3.3.1.m1.1.1\" xref=\"S10.T9.st1.3.3.3.1.m1.1.1.cmml\"><mi id=\"S10.T9.st1.3.3.3.1.m1.1.1.2\" xref=\"S10.T9.st1.3.3.3.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st1.3.3.3.1.m1.1.1.1\" xref=\"S10.T9.st1.3.3.3.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st1.3.3.3.1.m1.1.1.3\" xref=\"S10.T9.st1.3.3.3.1.m1.1.1.3.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st1.3.3.3.1.m1.1b\"><apply id=\"S10.T9.st1.3.3.3.1.m1.1.1.cmml\" xref=\"S10.T9.st1.3.3.3.1.m1.1.1\"><gt id=\"S10.T9.st1.3.3.3.1.m1.1.1.1.cmml\" xref=\"S10.T9.st1.3.3.3.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S10.T9.st1.3.3.3.1.m1.1.1.2.cmml\" xref=\"S10.T9.st1.3.3.3.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st1.3.3.3.1.m1.1.1.3.cmml\" xref=\"S10.T9.st1.3.3.3.1.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st1.3.3.3.1.m1.1c\">&gt;0.5</annotation></semantics></math></th>\n<td id=\"S10.T9.st1.3.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st1.3.3.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">44.76</span></td>\n<td id=\"S10.T9.st1.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st1.3.3.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.94</span></td>\n</tr>\n<tr id=\"S10.T9.st1.4.4.4\" class=\"ltx_tr\">\n<th id=\"S10.T9.st1.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S10.T9.st1.4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;0.4\" display=\"inline\"><semantics id=\"S10.T9.st1.4.4.4.1.m1.1a\"><mrow id=\"S10.T9.st1.4.4.4.1.m1.1.1\" xref=\"S10.T9.st1.4.4.4.1.m1.1.1.cmml\"><mi id=\"S10.T9.st1.4.4.4.1.m1.1.1.2\" xref=\"S10.T9.st1.4.4.4.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st1.4.4.4.1.m1.1.1.1\" xref=\"S10.T9.st1.4.4.4.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st1.4.4.4.1.m1.1.1.3\" xref=\"S10.T9.st1.4.4.4.1.m1.1.1.3.cmml\">0.4</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st1.4.4.4.1.m1.1b\"><apply id=\"S10.T9.st1.4.4.4.1.m1.1.1.cmml\" xref=\"S10.T9.st1.4.4.4.1.m1.1.1\"><gt id=\"S10.T9.st1.4.4.4.1.m1.1.1.1.cmml\" xref=\"S10.T9.st1.4.4.4.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S10.T9.st1.4.4.4.1.m1.1.1.2.cmml\" xref=\"S10.T9.st1.4.4.4.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st1.4.4.4.1.m1.1.1.3.cmml\" xref=\"S10.T9.st1.4.4.4.1.m1.1.1.3\">0.4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st1.4.4.4.1.m1.1c\">&gt;0.4</annotation></semantics></math></th>\n<td id=\"S10.T9.st1.4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st1.4.4.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">61.71</span></td>\n<td id=\"S10.T9.st1.4.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st1.4.4.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.61</span></td>\n</tr>\n<tr id=\"S10.T9.st1.5.5.5\" class=\"ltx_tr\">\n<th id=\"S10.T9.st1.5.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S10.T9.st1.5.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;0.3\" display=\"inline\"><semantics id=\"S10.T9.st1.5.5.5.1.m1.1a\"><mrow id=\"S10.T9.st1.5.5.5.1.m1.1.1\" xref=\"S10.T9.st1.5.5.5.1.m1.1.1.cmml\"><mi id=\"S10.T9.st1.5.5.5.1.m1.1.1.2\" xref=\"S10.T9.st1.5.5.5.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st1.5.5.5.1.m1.1.1.1\" xref=\"S10.T9.st1.5.5.5.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st1.5.5.5.1.m1.1.1.3\" xref=\"S10.T9.st1.5.5.5.1.m1.1.1.3.cmml\">0.3</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st1.5.5.5.1.m1.1b\"><apply id=\"S10.T9.st1.5.5.5.1.m1.1.1.cmml\" xref=\"S10.T9.st1.5.5.5.1.m1.1.1\"><gt id=\"S10.T9.st1.5.5.5.1.m1.1.1.1.cmml\" xref=\"S10.T9.st1.5.5.5.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S10.T9.st1.5.5.5.1.m1.1.1.2.cmml\" xref=\"S10.T9.st1.5.5.5.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st1.5.5.5.1.m1.1.1.3.cmml\" xref=\"S10.T9.st1.5.5.5.1.m1.1.1.3\">0.3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st1.5.5.5.1.m1.1c\">&gt;0.3</annotation></semantics></math></th>\n<td id=\"S10.T9.st1.5.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st1.5.5.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">80.56</span></td>\n<td id=\"S10.T9.st1.5.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st1.5.5.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">77.02</span></td>\n</tr>\n<tr id=\"S10.T9.st1.6.6.6\" class=\"ltx_tr\">\n<th id=\"S10.T9.st1.6.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"><math id=\"S10.T9.st1.6.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;0.2\" display=\"inline\"><semantics id=\"S10.T9.st1.6.6.6.1.m1.1a\"><mrow id=\"S10.T9.st1.6.6.6.1.m1.1.1\" xref=\"S10.T9.st1.6.6.6.1.m1.1.1.cmml\"><mi id=\"S10.T9.st1.6.6.6.1.m1.1.1.2\" xref=\"S10.T9.st1.6.6.6.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st1.6.6.6.1.m1.1.1.1\" xref=\"S10.T9.st1.6.6.6.1.m1.1.1.1.cmml\">&gt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st1.6.6.6.1.m1.1.1.3\" xref=\"S10.T9.st1.6.6.6.1.m1.1.1.3.cmml\">0.2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st1.6.6.6.1.m1.1b\"><apply id=\"S10.T9.st1.6.6.6.1.m1.1.1.cmml\" xref=\"S10.T9.st1.6.6.6.1.m1.1.1\"><gt id=\"S10.T9.st1.6.6.6.1.m1.1.1.1.cmml\" xref=\"S10.T9.st1.6.6.6.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S10.T9.st1.6.6.6.1.m1.1.1.2.cmml\" xref=\"S10.T9.st1.6.6.6.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st1.6.6.6.1.m1.1.1.3.cmml\" xref=\"S10.T9.st1.6.6.6.1.m1.1.1.3\">0.2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st1.6.6.6.1.m1.1c\">&gt;0.2</annotation></semantics></math></th>\n<td id=\"S10.T9.st1.6.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S10.T9.st1.6.6.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.55</span></td>\n<td id=\"S10.T9.st1.6.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S10.T9.st1.6.6.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">77.31</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 1 shows the amount of time taken by Inverting Gradients run on a NVIDIA A100 80GB GPU for MNIST and CIFAR-10 on a DNN and ResNet-18 respectively. There is some variance in the average amount of time based on batch size, but when attacking the entire dataset, smaller batch sizes always take longer time as the number of total batches is much larger. CIFAR-10 takes significantly longer to reconstruct each batch given the more complex model of ResNet-18 compared to a DNN. For a batch size of 4, CIFAR-10 takes 422.81 seconds per batch. Iterating across the entire dataset on a single GPU takes 61.17 days.",
            "Table 2(a) gives the average PSNR and SSIM metrics (where a higher score indicates greater similarity with the ground truth) for the reconstructions across the entire dataset of CIFAR-10 and MNIST from Inverting Gradients. Increasing the batch size results in a lower PSNR and SSIM score for both CIFAR-10 and MNIST.",
            "Table 2(b) reports the number of leaked images for LOKI. For an FC factor of 444, LOKI leaks 78.93%percent78.9378.93\\%, 76.61%percent76.6176.61\\%, and 75.15%percent75.1575.15\\% of images on the CIFAR-10, MNIST, and Tiny ImageNet datasets. Increasing the FC layer size increases the model size overhead, but results in a higher leakage rate as images are more likely to activate different neurons. Decreasing the size creates a smaller model size overhead but results in lower leakage rate.",
            "Table 3(a) and Figure 2(a) show the results for CIFAR-10 for centralized training, federated learning (FedAvg), and training with leaked data. Centralized training achieves a 94.38%percent94.3894.38\\% accuracy and federated learning achieves a 72.76%percent72.7672.76\\% and 68.71%percent68.7168.71\\% peak accuracy with 10 and 50 clients respectively. Centralized and FL indeed provide the upper bound and the lower bound of the accuracy here. Across all attack settings, GI and LLL achieve higher model accuracy compared to federated learning. The reconstruction quality of GI is adversely affected by larger batch sizes and in turn also impacts the final model performance. With a batch size of 4, 8, and 16, models trained on the leaked data from GI achieves 90.34%percent90.3490.34\\%, 86.16%percent86.1686.16\\%, and 76.83%percent76.8376.83\\% accuracy. With even larger batch sizes, it is likely that the model performance will drop below federated learning. The performance of models using LLL data is more stable. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the accuracy only drops from 93.16%percent93.1693.16\\% to 88.86%percent88.8688.86\\%. This is spite of a large drop in the number of leaked images from 43788 (87.58%percent87.5887.58\\%) to 18242 (36.48%percent36.4836.48\\%) as shown in Table 2(b). This is a subtle and consequential result — the downstream training task can still be accomplished despite a big drop in the amount of data leakage. All prior work had stopped at the stage of evaluating the proportion of leakage and therefore had missed out on this insight.",
            "Table 3(b) and Figure 2(b) show the results for MNIST. LLL performs better than the FedAvg baseline in all cases. With an FC size factor of 8, LOKI achieves a 98.82%percent98.8298.82\\% test accuracy (nearly the same regardless of the FC size factor), only 0.07%percent0.070.07\\% lower than centralized at 98.89%percent98.8998.89\\%. GI performs slightly worse than federated learning, achieving 95.96%percent95.9695.96\\%, 95.50%percent95.5095.50\\%, and 94.29%percent94.2994.29\\% at batch sizes 8, 16, and 32 compared to 96.17%percent96.1796.17\\% and 96.18%percent96.1896.18\\% with 10 and 50 clients in FedAvg. We believe this is because the noise in the reconstructions hampers model performance with GI. This affects MNIST more severely than for CIFAR because the MNIST images are more sparse and sensitive to noise.",
            "The top-1 validation accuracy for Tiny ImageNet are given in Table 3(c) and Figure 2(c). The total number of leaked images hurts LLL more here than in the other two datasets. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the validation accuracy drops from 46.70%percent46.7046.70\\% to 35.20%percent35.2035.20\\%, an 11.5%percent11.511.5\\% drop in performance. Federated learning achieves 37.00%percent37.0037.00\\% and 35.06%percent35.0635.06\\% accuracy with 10 and 50 clients respectively. FL with 10 clients has slightly better accuracy than LLL with an FC size factor of 1. However, any LLL setting with a larger FC layer achieves higher accuracy than both settings in federated learning.",
            "Table 4 shows the leakage rate of LOKI in FedAvg compared to FedSGD. Using CSF=500CSF500\\textit{CSF}=500, the leakage rate of LOKI in FedAvg is substantially higher than the leakage rates in the FedSGD setting. Using an FC layer of half the size, the leakage rate in FedAvg is comparable to that of FedSGD. For example, LOKI FedAvg factor 1 achieves a leakage rate of 58.39%percent58.3958.39\\% compared to LOKI FedSGD factor 2 which achieves a leakage rate of 59.76%percent59.7659.76\\%.",
            "Table 5 shows the testing accuracy of LOKI FedAvg for the different FC layer sizes. The test accuracies are very comparable to the FedSGD settings. However, with a smaller FC layer size factor, the effect of having a larger total number of leaked images becomes visible. With FC factor 1, LOKI FedAvg achieves 91.11%percent91.1191.11\\% accuracy while LOKI FedSGD achieves 88.86%percent88.8688.86\\%.",
            "Quality of reconstruction is an important concern for GI attacks. As the batch size continues to increase, the reconstruction quality decreases. Table 2(a) also shows this relationship. From the previous experiments, we also see a negative relationship between the increasing batch size and the usefulness of the leaked data in downstream model training.",
            "Here, we explore whether poor quality reconstructions are still useful for training models. Using the CIFAR-10 leaked dataset created using Inverting Gradients on a batch size of 16, we first sort the reconstructions by their PSNR value. Our first set of experiments removes the worst images from training. Table 6(a) shows the percent of images above the PSNR threshold and the final test accuracy when trained on that data. Compared to the baseline accuracy of 76.83%percent76.8376.83\\% achieved by using the entire leaked dataset, even removing the worst images with a PSNR <12absent12<12 from the training set results in a small but non-zero performance loss.\nThen we take a different look at this question by now training on the worst images.\nTable 6(b) shows the test accuracy of models trained on all images below a PSNR threshold. Even when training on the lowest quality images with a PSNR <12absent12<12, the model achieves a 45.05%percent45.0545.05\\% final testing accuracy, significantly worse than the baseline, but much above zero.",
            "While some reconstructions have very low quality, only images of the same label will be swapped within the batch during label matching. This is a desirable property for training on leaked data. As long as the set of labels is leaked correctly, the optimization process will match images to a correct label.\nSo even though the image quality is poor (and can be entirely unrecognizable), since the label is correctly matched, such an image still does help in the training (this buttresses the message from Tables 6(a) and 6(b)). Naturally, as the PSNR goes down, the image helps less and less."
        ]
    },
    "S10.T9.st2": {
        "caption": "(b) PSNR below threshold",
        "table": "<table id=\"S10.T9.st2.6.6\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S10.T9.st2.6.6.7.1\" class=\"ltx_tr\">\n<th id=\"S10.T9.st2.6.6.7.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S10.T9.st2.6.6.7.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">SSIM</span></th>\n<th id=\"S10.T9.st2.6.6.7.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S10.T9.st2.6.6.7.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S10.T9.st2.6.6.7.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S10.T9.st2.6.6.7.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S10.T9.st2.6.6.7.1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">% imgs</span></td>\n</tr>\n<tr id=\"S10.T9.st2.6.6.7.1.2.1.2\" class=\"ltx_tr\">\n<td id=\"S10.T9.st2.6.6.7.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S10.T9.st2.6.6.7.1.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">kept</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S10.T9.st2.6.6.7.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<table id=\"S10.T9.st2.6.6.7.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S10.T9.st2.6.6.7.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S10.T9.st2.6.6.7.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S10.T9.st2.6.6.7.1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Test</span></td>\n</tr>\n<tr id=\"S10.T9.st2.6.6.7.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S10.T9.st2.6.6.7.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S10.T9.st2.6.6.7.1.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">accuracy</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S10.T9.st2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S10.T9.st2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><math id=\"S10.T9.st2.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"&lt;0.7\" display=\"inline\"><semantics id=\"S10.T9.st2.1.1.1.1.m1.1a\"><mrow id=\"S10.T9.st2.1.1.1.1.m1.1.1\" xref=\"S10.T9.st2.1.1.1.1.m1.1.1.cmml\"><mi id=\"S10.T9.st2.1.1.1.1.m1.1.1.2\" xref=\"S10.T9.st2.1.1.1.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st2.1.1.1.1.m1.1.1.1\" xref=\"S10.T9.st2.1.1.1.1.m1.1.1.1.cmml\">&lt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st2.1.1.1.1.m1.1.1.3\" xref=\"S10.T9.st2.1.1.1.1.m1.1.1.3.cmml\">0.7</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st2.1.1.1.1.m1.1b\"><apply id=\"S10.T9.st2.1.1.1.1.m1.1.1.cmml\" xref=\"S10.T9.st2.1.1.1.1.m1.1.1\"><lt id=\"S10.T9.st2.1.1.1.1.m1.1.1.1.cmml\" xref=\"S10.T9.st2.1.1.1.1.m1.1.1.1\"></lt><csymbol cd=\"latexml\" id=\"S10.T9.st2.1.1.1.1.m1.1.1.2.cmml\" xref=\"S10.T9.st2.1.1.1.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st2.1.1.1.1.m1.1.1.3.cmml\" xref=\"S10.T9.st2.1.1.1.1.m1.1.1.3\">0.7</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st2.1.1.1.1.m1.1c\">&lt;0.7</annotation></semantics></math></th>\n<td id=\"S10.T9.st2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S10.T9.st2.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">82.75</span></td>\n<td id=\"S10.T9.st2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S10.T9.st2.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.94</span></td>\n</tr>\n<tr id=\"S10.T9.st2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S10.T9.st2.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S10.T9.st2.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"&lt;0.6\" display=\"inline\"><semantics id=\"S10.T9.st2.2.2.2.1.m1.1a\"><mrow id=\"S10.T9.st2.2.2.2.1.m1.1.1\" xref=\"S10.T9.st2.2.2.2.1.m1.1.1.cmml\"><mi id=\"S10.T9.st2.2.2.2.1.m1.1.1.2\" xref=\"S10.T9.st2.2.2.2.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st2.2.2.2.1.m1.1.1.1\" xref=\"S10.T9.st2.2.2.2.1.m1.1.1.1.cmml\">&lt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st2.2.2.2.1.m1.1.1.3\" xref=\"S10.T9.st2.2.2.2.1.m1.1.1.3.cmml\">0.6</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st2.2.2.2.1.m1.1b\"><apply id=\"S10.T9.st2.2.2.2.1.m1.1.1.cmml\" xref=\"S10.T9.st2.2.2.2.1.m1.1.1\"><lt id=\"S10.T9.st2.2.2.2.1.m1.1.1.1.cmml\" xref=\"S10.T9.st2.2.2.2.1.m1.1.1.1\"></lt><csymbol cd=\"latexml\" id=\"S10.T9.st2.2.2.2.1.m1.1.1.2.cmml\" xref=\"S10.T9.st2.2.2.2.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st2.2.2.2.1.m1.1.1.3.cmml\" xref=\"S10.T9.st2.2.2.2.1.m1.1.1.3\">0.6</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st2.2.2.2.1.m1.1c\">&lt;0.6</annotation></semantics></math></th>\n<td id=\"S10.T9.st2.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st2.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">69.74</span></td>\n<td id=\"S10.T9.st2.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st2.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">61.66</span></td>\n</tr>\n<tr id=\"S10.T9.st2.3.3.3\" class=\"ltx_tr\">\n<th id=\"S10.T9.st2.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S10.T9.st2.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"&lt;0.5\" display=\"inline\"><semantics id=\"S10.T9.st2.3.3.3.1.m1.1a\"><mrow id=\"S10.T9.st2.3.3.3.1.m1.1.1\" xref=\"S10.T9.st2.3.3.3.1.m1.1.1.cmml\"><mi id=\"S10.T9.st2.3.3.3.1.m1.1.1.2\" xref=\"S10.T9.st2.3.3.3.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st2.3.3.3.1.m1.1.1.1\" xref=\"S10.T9.st2.3.3.3.1.m1.1.1.1.cmml\">&lt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st2.3.3.3.1.m1.1.1.3\" xref=\"S10.T9.st2.3.3.3.1.m1.1.1.3.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st2.3.3.3.1.m1.1b\"><apply id=\"S10.T9.st2.3.3.3.1.m1.1.1.cmml\" xref=\"S10.T9.st2.3.3.3.1.m1.1.1\"><lt id=\"S10.T9.st2.3.3.3.1.m1.1.1.1.cmml\" xref=\"S10.T9.st2.3.3.3.1.m1.1.1.1\"></lt><csymbol cd=\"latexml\" id=\"S10.T9.st2.3.3.3.1.m1.1.1.2.cmml\" xref=\"S10.T9.st2.3.3.3.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st2.3.3.3.1.m1.1.1.3.cmml\" xref=\"S10.T9.st2.3.3.3.1.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st2.3.3.3.1.m1.1c\">&lt;0.5</annotation></semantics></math></th>\n<td id=\"S10.T9.st2.3.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st2.3.3.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">55.24</span></td>\n<td id=\"S10.T9.st2.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st2.3.3.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">57.34</span></td>\n</tr>\n<tr id=\"S10.T9.st2.4.4.4\" class=\"ltx_tr\">\n<th id=\"S10.T9.st2.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S10.T9.st2.4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"&lt;0.4\" display=\"inline\"><semantics id=\"S10.T9.st2.4.4.4.1.m1.1a\"><mrow id=\"S10.T9.st2.4.4.4.1.m1.1.1\" xref=\"S10.T9.st2.4.4.4.1.m1.1.1.cmml\"><mi id=\"S10.T9.st2.4.4.4.1.m1.1.1.2\" xref=\"S10.T9.st2.4.4.4.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st2.4.4.4.1.m1.1.1.1\" xref=\"S10.T9.st2.4.4.4.1.m1.1.1.1.cmml\">&lt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st2.4.4.4.1.m1.1.1.3\" xref=\"S10.T9.st2.4.4.4.1.m1.1.1.3.cmml\">0.4</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st2.4.4.4.1.m1.1b\"><apply id=\"S10.T9.st2.4.4.4.1.m1.1.1.cmml\" xref=\"S10.T9.st2.4.4.4.1.m1.1.1\"><lt id=\"S10.T9.st2.4.4.4.1.m1.1.1.1.cmml\" xref=\"S10.T9.st2.4.4.4.1.m1.1.1.1\"></lt><csymbol cd=\"latexml\" id=\"S10.T9.st2.4.4.4.1.m1.1.1.2.cmml\" xref=\"S10.T9.st2.4.4.4.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st2.4.4.4.1.m1.1.1.3.cmml\" xref=\"S10.T9.st2.4.4.4.1.m1.1.1.3\">0.4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st2.4.4.4.1.m1.1c\">&lt;0.4</annotation></semantics></math></th>\n<td id=\"S10.T9.st2.4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st2.4.4.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">38.29</span></td>\n<td id=\"S10.T9.st2.4.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st2.4.4.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">51.56</span></td>\n</tr>\n<tr id=\"S10.T9.st2.5.5.5\" class=\"ltx_tr\">\n<th id=\"S10.T9.st2.5.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><math id=\"S10.T9.st2.5.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"&lt;0.3\" display=\"inline\"><semantics id=\"S10.T9.st2.5.5.5.1.m1.1a\"><mrow id=\"S10.T9.st2.5.5.5.1.m1.1.1\" xref=\"S10.T9.st2.5.5.5.1.m1.1.1.cmml\"><mi id=\"S10.T9.st2.5.5.5.1.m1.1.1.2\" xref=\"S10.T9.st2.5.5.5.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st2.5.5.5.1.m1.1.1.1\" xref=\"S10.T9.st2.5.5.5.1.m1.1.1.1.cmml\">&lt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st2.5.5.5.1.m1.1.1.3\" xref=\"S10.T9.st2.5.5.5.1.m1.1.1.3.cmml\">0.3</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st2.5.5.5.1.m1.1b\"><apply id=\"S10.T9.st2.5.5.5.1.m1.1.1.cmml\" xref=\"S10.T9.st2.5.5.5.1.m1.1.1\"><lt id=\"S10.T9.st2.5.5.5.1.m1.1.1.1.cmml\" xref=\"S10.T9.st2.5.5.5.1.m1.1.1.1\"></lt><csymbol cd=\"latexml\" id=\"S10.T9.st2.5.5.5.1.m1.1.1.2.cmml\" xref=\"S10.T9.st2.5.5.5.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st2.5.5.5.1.m1.1.1.3.cmml\" xref=\"S10.T9.st2.5.5.5.1.m1.1.1.3\">0.3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st2.5.5.5.1.m1.1c\">&lt;0.3</annotation></semantics></math></th>\n<td id=\"S10.T9.st2.5.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st2.5.5.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">19.44</span></td>\n<td id=\"S10.T9.st2.5.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S10.T9.st2.5.5.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">47.00</span></td>\n</tr>\n<tr id=\"S10.T9.st2.6.6.6\" class=\"ltx_tr\">\n<th id=\"S10.T9.st2.6.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"><math id=\"S10.T9.st2.6.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\"&lt;0.2\" display=\"inline\"><semantics id=\"S10.T9.st2.6.6.6.1.m1.1a\"><mrow id=\"S10.T9.st2.6.6.6.1.m1.1.1\" xref=\"S10.T9.st2.6.6.6.1.m1.1.1.cmml\"><mi id=\"S10.T9.st2.6.6.6.1.m1.1.1.2\" xref=\"S10.T9.st2.6.6.6.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"S10.T9.st2.6.6.6.1.m1.1.1.1\" xref=\"S10.T9.st2.6.6.6.1.m1.1.1.1.cmml\">&lt;</mo><mn mathsize=\"70%\" id=\"S10.T9.st2.6.6.6.1.m1.1.1.3\" xref=\"S10.T9.st2.6.6.6.1.m1.1.1.3.cmml\">0.2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S10.T9.st2.6.6.6.1.m1.1b\"><apply id=\"S10.T9.st2.6.6.6.1.m1.1.1.cmml\" xref=\"S10.T9.st2.6.6.6.1.m1.1.1\"><lt id=\"S10.T9.st2.6.6.6.1.m1.1.1.1.cmml\" xref=\"S10.T9.st2.6.6.6.1.m1.1.1.1\"></lt><csymbol cd=\"latexml\" id=\"S10.T9.st2.6.6.6.1.m1.1.1.2.cmml\" xref=\"S10.T9.st2.6.6.6.1.m1.1.1.2\">absent</csymbol><cn type=\"float\" id=\"S10.T9.st2.6.6.6.1.m1.1.1.3.cmml\" xref=\"S10.T9.st2.6.6.6.1.m1.1.1.3\">0.2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S10.T9.st2.6.6.6.1.m1.1c\">&lt;0.2</annotation></semantics></math></th>\n<td id=\"S10.T9.st2.6.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S10.T9.st2.6.6.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">4.45</span></td>\n<td id=\"S10.T9.st2.6.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S10.T9.st2.6.6.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">32.36</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 1 shows the amount of time taken by Inverting Gradients run on a NVIDIA A100 80GB GPU for MNIST and CIFAR-10 on a DNN and ResNet-18 respectively. There is some variance in the average amount of time based on batch size, but when attacking the entire dataset, smaller batch sizes always take longer time as the number of total batches is much larger. CIFAR-10 takes significantly longer to reconstruct each batch given the more complex model of ResNet-18 compared to a DNN. For a batch size of 4, CIFAR-10 takes 422.81 seconds per batch. Iterating across the entire dataset on a single GPU takes 61.17 days.",
            "Table 2(a) gives the average PSNR and SSIM metrics (where a higher score indicates greater similarity with the ground truth) for the reconstructions across the entire dataset of CIFAR-10 and MNIST from Inverting Gradients. Increasing the batch size results in a lower PSNR and SSIM score for both CIFAR-10 and MNIST.",
            "Table 2(b) reports the number of leaked images for LOKI. For an FC factor of 444, LOKI leaks 78.93%percent78.9378.93\\%, 76.61%percent76.6176.61\\%, and 75.15%percent75.1575.15\\% of images on the CIFAR-10, MNIST, and Tiny ImageNet datasets. Increasing the FC layer size increases the model size overhead, but results in a higher leakage rate as images are more likely to activate different neurons. Decreasing the size creates a smaller model size overhead but results in lower leakage rate.",
            "Table 3(a) and Figure 2(a) show the results for CIFAR-10 for centralized training, federated learning (FedAvg), and training with leaked data. Centralized training achieves a 94.38%percent94.3894.38\\% accuracy and federated learning achieves a 72.76%percent72.7672.76\\% and 68.71%percent68.7168.71\\% peak accuracy with 10 and 50 clients respectively. Centralized and FL indeed provide the upper bound and the lower bound of the accuracy here. Across all attack settings, GI and LLL achieve higher model accuracy compared to federated learning. The reconstruction quality of GI is adversely affected by larger batch sizes and in turn also impacts the final model performance. With a batch size of 4, 8, and 16, models trained on the leaked data from GI achieves 90.34%percent90.3490.34\\%, 86.16%percent86.1686.16\\%, and 76.83%percent76.8376.83\\% accuracy. With even larger batch sizes, it is likely that the model performance will drop below federated learning. The performance of models using LLL data is more stable. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the accuracy only drops from 93.16%percent93.1693.16\\% to 88.86%percent88.8688.86\\%. This is spite of a large drop in the number of leaked images from 43788 (87.58%percent87.5887.58\\%) to 18242 (36.48%percent36.4836.48\\%) as shown in Table 2(b). This is a subtle and consequential result — the downstream training task can still be accomplished despite a big drop in the amount of data leakage. All prior work had stopped at the stage of evaluating the proportion of leakage and therefore had missed out on this insight.",
            "Table 3(b) and Figure 2(b) show the results for MNIST. LLL performs better than the FedAvg baseline in all cases. With an FC size factor of 8, LOKI achieves a 98.82%percent98.8298.82\\% test accuracy (nearly the same regardless of the FC size factor), only 0.07%percent0.070.07\\% lower than centralized at 98.89%percent98.8998.89\\%. GI performs slightly worse than federated learning, achieving 95.96%percent95.9695.96\\%, 95.50%percent95.5095.50\\%, and 94.29%percent94.2994.29\\% at batch sizes 8, 16, and 32 compared to 96.17%percent96.1796.17\\% and 96.18%percent96.1896.18\\% with 10 and 50 clients in FedAvg. We believe this is because the noise in the reconstructions hampers model performance with GI. This affects MNIST more severely than for CIFAR because the MNIST images are more sparse and sensitive to noise.",
            "The top-1 validation accuracy for Tiny ImageNet are given in Table 3(c) and Figure 2(c). The total number of leaked images hurts LLL more here than in the other two datasets. Between an FC layer size of 512 (factor 8) and 64 (factor 1), the validation accuracy drops from 46.70%percent46.7046.70\\% to 35.20%percent35.2035.20\\%, an 11.5%percent11.511.5\\% drop in performance. Federated learning achieves 37.00%percent37.0037.00\\% and 35.06%percent35.0635.06\\% accuracy with 10 and 50 clients respectively. FL with 10 clients has slightly better accuracy than LLL with an FC size factor of 1. However, any LLL setting with a larger FC layer achieves higher accuracy than both settings in federated learning.",
            "Table 4 shows the leakage rate of LOKI in FedAvg compared to FedSGD. Using CSF=500CSF500\\textit{CSF}=500, the leakage rate of LOKI in FedAvg is substantially higher than the leakage rates in the FedSGD setting. Using an FC layer of half the size, the leakage rate in FedAvg is comparable to that of FedSGD. For example, LOKI FedAvg factor 1 achieves a leakage rate of 58.39%percent58.3958.39\\% compared to LOKI FedSGD factor 2 which achieves a leakage rate of 59.76%percent59.7659.76\\%.",
            "Table 5 shows the testing accuracy of LOKI FedAvg for the different FC layer sizes. The test accuracies are very comparable to the FedSGD settings. However, with a smaller FC layer size factor, the effect of having a larger total number of leaked images becomes visible. With FC factor 1, LOKI FedAvg achieves 91.11%percent91.1191.11\\% accuracy while LOKI FedSGD achieves 88.86%percent88.8688.86\\%.",
            "Quality of reconstruction is an important concern for GI attacks. As the batch size continues to increase, the reconstruction quality decreases. Table 2(a) also shows this relationship. From the previous experiments, we also see a negative relationship between the increasing batch size and the usefulness of the leaked data in downstream model training.",
            "Here, we explore whether poor quality reconstructions are still useful for training models. Using the CIFAR-10 leaked dataset created using Inverting Gradients on a batch size of 16, we first sort the reconstructions by their PSNR value. Our first set of experiments removes the worst images from training. Table 6(a) shows the percent of images above the PSNR threshold and the final test accuracy when trained on that data. Compared to the baseline accuracy of 76.83%percent76.8376.83\\% achieved by using the entire leaked dataset, even removing the worst images with a PSNR <12absent12<12 from the training set results in a small but non-zero performance loss.\nThen we take a different look at this question by now training on the worst images.\nTable 6(b) shows the test accuracy of models trained on all images below a PSNR threshold. Even when training on the lowest quality images with a PSNR <12absent12<12, the model achieves a 45.05%percent45.0545.05\\% final testing accuracy, significantly worse than the baseline, but much above zero.",
            "While some reconstructions have very low quality, only images of the same label will be swapped within the batch during label matching. This is a desirable property for training on leaked data. As long as the set of labels is leaked correctly, the optimization process will match images to a correct label.\nSo even though the image quality is poor (and can be entirely unrecognizable), since the label is correctly matched, such an image still does help in the training (this buttresses the message from Tables 6(a) and 6(b)). Naturally, as the PSNR goes down, the image helps less and less."
        ]
    }
}