{
    "S3.T1": {
        "caption": "TABLE I: Annotated question-answer pairs in our VQA framework",
        "table": "<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S3.T1.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Action category</span></td>\n<td id=\"S3.T1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_border_tt\">\n<span id=\"S3.T1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.1.2.1.1\" class=\"ltx_p\"><span id=\"S3.T1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Question</span></span>\n</span>\n</td>\n<td id=\"S3.T1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Answer</span></td>\n</tr>\n<tr id=\"S3.T1.1.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Go straight</td>\n<td id=\"S3.T1.1.2.2\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T1.1.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.2.2.1.1\" class=\"ltx_p\">Why is the car going straight?</span>\n</span>\n</td>\n<td id=\"S3.T1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">Because the road is clear.</td>\n</tr>\n<tr id=\"S3.T1.1.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.3.1\" class=\"ltx_td ltx_align_center\">Turn left</td>\n<td id=\"S3.T1.1.3.2\" class=\"ltx_td ltx_align_justify\">\n<span id=\"S3.T1.1.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.3.2.1.1\" class=\"ltx_p\">Why is the car turning to the left?</span>\n</span>\n</td>\n<td id=\"S3.T1.1.3.3\" class=\"ltx_td ltx_align_center\">Because the road is bending to the left.</td>\n</tr>\n<tr id=\"S3.T1.1.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.4.1\" class=\"ltx_td ltx_align_center\">Turn left at T-junction</td>\n<td id=\"S3.T1.1.4.2\" class=\"ltx_td ltx_align_justify\">\n<span id=\"S3.T1.1.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.4.2.1.1\" class=\"ltx_p\">Why is the car turning left at T-junction?</span>\n</span>\n</td>\n<td id=\"S3.T1.1.4.3\" class=\"ltx_td ltx_align_center\">Because there is no obstacle on the right side and turning left can be performed safely.</td>\n</tr>\n<tr id=\"S3.T1.1.5\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.5.1\" class=\"ltx_td ltx_align_center\">Turn right</td>\n<td id=\"S3.T1.1.5.2\" class=\"ltx_td ltx_align_justify\">\n<span id=\"S3.T1.1.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.5.2.1.1\" class=\"ltx_p\">Why is the car turning to the right?</span>\n</span>\n</td>\n<td id=\"S3.T1.1.5.3\" class=\"ltx_td ltx_align_center\">Because the road is bending to the right.</td>\n</tr>\n<tr id=\"S3.T1.1.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.6.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">Turn right at T-junction</td>\n<td id=\"S3.T1.1.6.2\" class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span id=\"S3.T1.1.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.6.2.1.1\" class=\"ltx_p\">Why is the car turning right at T-junction?</span>\n</span>\n</td>\n<td id=\"S3.T1.1.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">Because there is no obstacle on the left side and turning right can be performed safely.</td>\n</tr>\n</table>\n",
        "footnotes": "",
        "references": [
            "the simulator. We then convert this recorded video to image sequences uniformly. Finally, we select five specific action categories in the extracted driving frames and annotate them using question-answer pairs that justify the car’s action in the scene (Table I). The high-level description of the components and overall architecture is provided in Figure 6. Given such a setup, the objective of our architecture is to predict the correct answer to a posed question about an autonomous car’s performed action in an unseen driving scene. The details of the data collection, data annotation, and question-answering steps are described in the following subsections.",
            "As we obtained the driving video with the DDPG agent, we selected 5 action categories (go straight, turn left, turn right, turn left at T-junction, and turn right at T-junction), and extracted consecutive frames uniformly (30 frames per second) for 5 video segments. We then chose 10 frames from each segment. We ensured that these frames were extracted from driving segments, where the car followed the predefined route and performed the relevant action safely without lane departure or collision. We distinguish left and right turns in the current line from left and right turns at T-junction as in the latter an ego car also has an alternative route. So, our training data includes 5 action categories with 50 high-quality frames per category, denoting a total of 250 driving scenes obtained from the recorded video. We manually annotated the training data with five causal question-answer (QA) pairs (see Table I) ensuring the annotations reflected the scene correctly. Each of 250 frames has its single and scenario-related annotation. As test data, we selected a collection of 100 frames from both Town 1 and Town 2 on the CARLA simulator, as the map of Town 2 is similar to Town 1. Similar to the training data annotation, we selected 20 frames for each action category from various segments of Town 1 and Town 2 and annotated each of them with a relevant QA pair. The goal is to assess the generalization ability of the employed VQA framework on these action categories in unseen traffic scenarios.",
            "On the data collection side, we trained the DDPG agent on the CARLA 0.9.11 version in 500 episodes using a TensorFlow backend to get a driving video. As described above, we used 250 frames from Town 1 for training our VQA network and evaluated its performance on 100 frames collected from Town 1 and Town 2 (Figure 3). We used the PyTorch backend for training and evaluating our VQA architecture. The experiments were performed on an NVIDIA RTX 3090 GPU machine with a 32 GB memory size. All the frames were set to have a size of 640 ×\\times 480 both in training and test. As we have ground-truth answers (see Table I) for the asked question about an image, we compare the top prediction of our model on the test data (i.e., an answer with the highest softmax probability score) with these ground-truth answers. Thus, we use accuracy as an evaluation metric, which is defined as follows:"
        ]
    },
    "S3.T2": {
        "caption": "TABLE II: The training parameters of DDPG on CARLA",
        "table": "<table id=\"S3.T2.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S3.T2.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.1\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T2.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.1.1.1\" class=\"ltx_p\">Actor learning rate</span>\n</span>\n</td>\n<td id=\"S3.T2.1.1.2\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T2.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.2.1.1\" class=\"ltx_p\">Critic learning rate</span>\n</span>\n</td>\n<td id=\"S3.T2.1.1.3\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T2.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.3.1.1\" class=\"ltx_p\">Target network hyper-parameter</span>\n</span>\n</td>\n<td id=\"S3.T2.1.1.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T2.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.4.1.1\" class=\"ltx_p\">Replay buffer size</span>\n</span>\n</td>\n<td id=\"S3.T2.1.1.5\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T2.1.1.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.5.1.1\" class=\"ltx_p\">Batch size</span>\n</span>\n</td>\n<td id=\"S3.T2.1.1.6\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T2.1.1.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.6.1.1\" class=\"ltx_p\">Discount factor</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.2\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.2.1\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_t\">\n<span id=\"S3.T2.1.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.1.1.1\" class=\"ltx_p\">0.0001</span>\n</span>\n</td>\n<td id=\"S3.T2.1.2.2\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_t\">\n<span id=\"S3.T2.1.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.2.1.1\" class=\"ltx_p\">0.001</span>\n</span>\n</td>\n<td id=\"S3.T2.1.2.3\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_t\">\n<span id=\"S3.T2.1.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.3.1.1\" class=\"ltx_p\">0.001</span>\n</span>\n</td>\n<td id=\"S3.T2.1.2.4\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_t\">\n<span id=\"S3.T2.1.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.4.1.1\" class=\"ltx_p\">100000</span>\n</span>\n</td>\n<td id=\"S3.T2.1.2.5\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_t\">\n<span id=\"S3.T2.1.2.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.5.1.1\" class=\"ltx_p\">32</span>\n</span>\n</td>\n<td id=\"S3.T2.1.2.6\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_t\">\n<span id=\"S3.T2.1.2.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.6.1.1\" class=\"ltx_p\">0.99</span>\n</span>\n</td>\n</tr>\n</table>\n",
        "footnotes": "",
        "references": [
            "Finally, action space is continuous and can receive values from the interval [-1,1]. By defining this setting, we trained our agent in Town 1. The training parameters of DDPG can be seen in Table II."
        ]
    },
    "S3.T3": {
        "caption": "TABLE III: Number of correct predictions for each action category",
        "table": "<table id=\"S3.T3.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S3.T3.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T3.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Go straight</span></td>\n<td id=\"S3.T3.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T3.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Turn left</span></td>\n<td id=\"S3.T3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T3.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Turn left at T-junction</span></td>\n<td id=\"S3.T3.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T3.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Turn right</span></td>\n<td id=\"S3.T3.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T3.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Turn right at T-junction</span></td>\n<td id=\"S3.T3.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T3.1.1.6.1\" class=\"ltx_text ltx_font_bold\">Total</span></td>\n</tr>\n<tr id=\"S3.T3.1.2\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">20/20</td>\n<td id=\"S3.T3.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0/20</td>\n<td id=\"S3.T3.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">20/20</td>\n<td id=\"S3.T3.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">20/20</td>\n<td id=\"S3.T3.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">20/20</td>\n<td id=\"S3.T3.1.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">80/100</td>\n</tr>\n</table>\n",
        "footnotes": "",
        "references": []
    }
}