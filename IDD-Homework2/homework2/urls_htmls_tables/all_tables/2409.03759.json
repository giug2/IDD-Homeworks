{
    "id_table_1": {
        "caption": "Table 1.  VERA LLM-Based RAG Evaluation Metrics on 500 Perfectly Relevant MS MARCO TREC 2023 Query-Passage Pairs",
        "table": "S5.T1.1",
        "footnotes": [],
        "references": [
            "Like existing LLM-based RAG evaluation system such as RAGAS or ARES  (Saad-Falcon et al . ,  2023 ) , VERA has measured the following LLM-based evaluation metrics. The prompts to create the metrics are listed in Appendix  8.1 .",
            "Retrieval Recall:  This metric evaluates the systems effectiveness in fetching all relevant information related to a query from the given context, ensuring that no significant data is omitted. This metric is determined by assessing whether each piece of information in the answer is explicitly supported by the context. The retrieval recall metric is calculated based on the proportion of sentences in the answer that are classified as [Supported by Context], which is used in the evaluation metric prompt in Appendix  8.1 . This involves:",
            "For domain-specific synthetic data generation, we employ Anthropic V3 Haiku to create high-quality synthetic queries and responses tailored for our experimental needs. This models advanced generative capabilities ensure that the synthetic datasets are both diverse and closely aligned with the task-specific requirements. For the evaluation of responses, we utilize Anthropic V3 Sonnet, which serves as our LLM judge. The examples of synthetic generation prompts, evaluation prompts and RAG summarization prompt using Llama 3 supported in POE Web UI  (Thorne et al . ,  2018 )  are in Appendix  8.1 .",
            "In this experiment, we evaluate the performance of several RAG systems by comparing perfectly relevant and irrelevant query-passages pairs on faithfulness, answer relevance, retrieval recall, retrieval precision, as well as the logit values returned by the cross-encoder when performing the aggregation step (Agg column in tables) . Findings are presented in Table  1  and Appendix Table  3 . All the metric values in both tables are reported as mean.",
            "The experimental results demonstrate that Llama3, a powerful open-source LLM, performs comparably to established models like Anthropics Claude V3 Haiku. In Table  1 , these models manage effective fact-checking and capture semantic relationships well, as indicated by high faithfulness and relevance scores. Additionally, the retrieval recall and precision are reasonably high, suggesting that the models retrieve most relevant information accurately. In the opposite way, the low precision score in Appendix Table  3  may suggest that the queries either fall outside the scope of the covered topics in the knowledge base, or that the topics within the knowledge base are too varied relative to the generality of the queries. The Agg-Logit scores in the comparison between different model configurations highlight the nuanced performance differences across various metrics.",
            "Lastly, the performance of these powerful LLMs and embedding models have been compared to that of a weaker LLM (T-5 FLAN Base) and embedding model (all-MiniLM-L6-v2) as a baseline, to highlight the differences in the evaluation metrics. As seen in both Table  1  and Table  2 , the individual evaluation metrics as well as the Agg-Logit scores are consistently lower when using a weaker LLM + embedding model, regardless of the scenario of evaluating against a perfectly relevant or irrelevant dataset."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  VERA LLM-Based RAG Evaluation Metrics on 500 Irrelevant MS MARCO TREC 2023 Query-Passage Pairs",
        "table": "S5.T2.1",
        "footnotes": [],
        "references": [
            "Unbiased Estimator:   The bootstrap estimator serves as an unbiased estimator for metrics based on LLMs, effectively estimating the expectation of the original estimator and its bootstrap distribution. Detailed assumptions and mathematical derivations supporting this conclusion are outlined in Appendix  8.2 .",
            "Lastly, the performance of these powerful LLMs and embedding models have been compared to that of a weaker LLM (T-5 FLAN Base) and embedding model (all-MiniLM-L6-v2) as a baseline, to highlight the differences in the evaluation metrics. As seen in both Table  1  and Table  2 , the individual evaluation metrics as well as the Agg-Logit scores are consistently lower when using a weaker LLM + embedding model, regardless of the scenario of evaluating against a perfectly relevant or irrelevant dataset.",
            "As outlined in section  3.3 , we utilized bootstrap statistics to analyze a synthetic dataset described in section  4.2  and the results are in Table  3 . We used bootstrap sampling with replacement on the synthetic query sets and the overall passage set. In the synthetic query sets, we have 200 synthetic queries in each set and we labeled them in Tale  3  as Sales, Basketball and Random based on the topics. This approach enabled us to calculate critical statistical measures like the mean and variance, providing a robust foundation for assessing the thematic topicality of the data repository. We used sample size 50 and bootstrapping size 500 to ensure fairly stable convergence of the statistics for each metric and each query set. This comparative analysis helps in quantifying the document repositorys content topicality to distinguish and accurately process content relevant to its designated domain."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  VERA Bootstrap Statistics on Three Comparative QuerySet",
        "table": "S5.T3.1",
        "footnotes": [],
        "references": [
            "In this experiment, we evaluate the performance of several RAG systems by comparing perfectly relevant and irrelevant query-passages pairs on faithfulness, answer relevance, retrieval recall, retrieval precision, as well as the logit values returned by the cross-encoder when performing the aggregation step (Agg column in tables) . Findings are presented in Table  1  and Appendix Table  3 . All the metric values in both tables are reported as mean.",
            "The experimental results demonstrate that Llama3, a powerful open-source LLM, performs comparably to established models like Anthropics Claude V3 Haiku. In Table  1 , these models manage effective fact-checking and capture semantic relationships well, as indicated by high faithfulness and relevance scores. Additionally, the retrieval recall and precision are reasonably high, suggesting that the models retrieve most relevant information accurately. In the opposite way, the low precision score in Appendix Table  3  may suggest that the queries either fall outside the scope of the covered topics in the knowledge base, or that the topics within the knowledge base are too varied relative to the generality of the queries. The Agg-Logit scores in the comparison between different model configurations highlight the nuanced performance differences across various metrics.",
            "As outlined in section  3.3 , we utilized bootstrap statistics to analyze a synthetic dataset described in section  4.2  and the results are in Table  3 . We used bootstrap sampling with replacement on the synthetic query sets and the overall passage set. In the synthetic query sets, we have 200 synthetic queries in each set and we labeled them in Tale  3  as Sales, Basketball and Random based on the topics. This approach enabled us to calculate critical statistical measures like the mean and variance, providing a robust foundation for assessing the thematic topicality of the data repository. We used sample size 50 and bootstrapping size 500 to ensure fairly stable convergence of the statistics for each metric and each query set. This comparative analysis helps in quantifying the document repositorys content topicality to distinguish and accurately process content relevant to its designated domain."
        ]
    },
    "global_footnotes": []
}