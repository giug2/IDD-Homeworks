{
    "id_table_1": {
        "caption": "Table 1:  Comparison of existing fair models with our model over different key areas of interest: Fair Representation, Generative Models, and data-free training.",
        "table": "S2.T1.1.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "So, from these motivations, in this work, we present Fair4Free, a novel generative approach for generating high-fidelity synthetic fair data, where we use knowledge distillation to distil the fair representation. The main contribution of our work is that while distilling the fair representation, we do not use any training data; we only use noise as input for the distilled model, so the approach for distillation is entirely data-free. The data-free distillation mitigates the issue of the dataset being sensitive and inaccessible. We first train a Variational Autoencoder (VAE) to learn the fair representation, then use it as a teacher model and take a smaller architecture (student model) to distil the fair representation. After distilling the fair representation, we use the trained decoder (from VAE) and student model to reconstruct high-fidelity fair synthetic samples. Using a smaller architecture also allows the possibility of deploying the model in edge devices. We do substantial experiments with both tabular and image data and show that our data-free distillation-based generative model performs better on the fairness, utility and synthetic samples than the state-of-the-art models. Table  1  shows the high-level comparison of other works with ours.",
            "Data fairness can be measured from different viewpoints, i.e., a model can be fair if it shows equal performance for all the demographics. Over the years, different approaches have been shown to create a fair model that satisfies group or individual fairness. This work focuses on creating a generative model that provides group fairness. To achieve this, a model should follow definitions  1  and  2 .",
            "The first stage of our fair generative model is to learn a fair representation from biased data,  D D D italic_D . We train a Variational Autoencoder (VAE) for this task and use the encoder ( E  subscript E italic- \\mathcal{E}_{\\phi} caligraphic_E start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ) to get the fair representation. The encoder creates representation  z = E   ( x , s ) z subscript E italic- x s z=\\mathcal{E}_{\\phi}(x,s) italic_z = caligraphic_E start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x , italic_s ) , here  ( x , s )  D x s D (x,s)\\in D ( italic_x , italic_s )  italic_D ,  x  X x X x\\in\\mathcal{X} italic_x  caligraphic_X  represents non-sensitive attributes and  s  S s S s\\in\\mathcal{S} italic_s  caligraphic_S  represents sensitive attributes. Then the decoder reconstruct samples,  x  = D   ( z , s ) superscript x  subscript D  z s x^{\\prime}=\\mathcal{D}_{\\theta}(z,s) italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = caligraphic_D start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_z , italic_s ) . For learning the fair representation, along with the reconstruction loss and KL-divergence loss of VAE, distance correlation minimization loss  V  2  ( z , s ) subscript superscript V 2 italic- z s \\mathcal{V}^{2}_{\\phi}(z,s) caligraphic_V start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_z , italic_s )  and penalty    { 0 , 1 , 2 , ... , 9 }  0 1 2 ... 9 \\beta\\in\\{0,1,2,...,9\\} italic_  { 0 , 1 , 2 , ... , 9 }  is used, which is stated in Equation  1   (Liu et al.,  2022 ) . Steps of learning fair representation can be found in Algorithm  1 .",
            "Besides the empirical quality of our synthetic samples, we also show visual analysis. We show the synthetic image samples from both  CelebA  and  Colored-MNIST  in Figure  1  and  2 . We use Smiling as the sensitive attribute for the  CelebA  and Colors (Red, Green, Blue) for the  MNIST  dataset.",
            "This work presents Fair4Free, a data-free distillation-based fair generative model. With this approach, we generate high-fidelity fair synthetic samples with the help of knowledge distillation. We distil the fair representation from the trained model (teacher model) to another architecture (student model). During the distillation process, we do not use any training data; thus, the training of the student model is data-free. This helps when the dataset is unavailable for security and privacy reasons. Also, as we use a small architecture, we reduce the computational cost, and we can deploy the model into an edge device with better performance than the teacher model. Tables  2  and  3  show that our model is over-performing the state-of-the-art models in the perspective of data utility, fairness and synthetic quality. Besides empirical evaluation, Figure  1 ,  2  shows the synthetic data samples, and Figure  4  shows the usefulness of fair synthetic samples in decision-making."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Fair Synthetic Samples evaluation, Dataset:  Adult-Income . Bold indicates the best result. Synthetic Utility evaluation is only for generative models (TabFairGAN, Decaf, FLDGMs)",
        "table": "S6.T2.96.96",
        "footnotes": [],
        "references": [
            "Data fairness can be measured from different viewpoints, i.e., a model can be fair if it shows equal performance for all the demographics. Over the years, different approaches have been shown to create a fair model that satisfies group or individual fairness. This work focuses on creating a generative model that provides group fairness. To achieve this, a model should follow definitions  1  and  2 .",
            "In this step, we take the trained fair encoder,  E  subscript E italic- \\mathcal{E}_{\\phi} caligraphic_E start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  from step 1 and distil the knowledge of fair representation ( z z z italic_z ) to another architecture,  E   subscript superscript E   \\mathcal{E}^{\\prime}_{\\psi} caligraphic_E start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  (we use less number of hidden features than used in  E  subscript E italic- \\mathcal{E}_{\\phi} caligraphic_E start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ) by first creating the latent fair representation,  z = E   ( x , s ) , z subscript E italic- x s z=\\mathcal{E}_{\\phi}(x,s), italic_z = caligraphic_E start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x , italic_s ) ,  where,  x , s  D x s D x,s\\in D italic_x , italic_s  italic_D . The main contribution of this work is to while distilling the latent space,  z z z italic_z , to the model  E   subscript superscript E   \\mathcal{E}^{\\prime}_{\\psi} caligraphic_E start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT , we do not use any training data,  ( x , s ) x s (x,s) ( italic_x , italic_s ) . We feed Gaussian noise,  n  N  ( 0 , 1 ) similar-to n N 0 1 n\\sim\\mathcal{N}(0,1) italic_n  caligraphic_N ( 0 , 1 )  to the distilled model, and it produce some representation  z  = E    ( n ) superscript z  subscript superscript E   n z^{\\prime}=\\mathcal{E}^{\\prime}_{\\psi}(n) italic_z start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = caligraphic_E start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_n ) . We use a combination of distillation loss between the distilled representation ( z  superscript z  z^{\\prime} italic_z start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) and fair representation ( z z z italic_z ) and KL-divergence loss on the output of the distilled model  (Sikder et al.,  2024a ) . The overall loss function is stated in Equation  2 .",
            "Here, we use  L1-loss  as  L L \\mathcal{L} caligraphic_L  for the distillation loss. Algorithm  2  shows the process of data-free distillation. After the distillation, we use the distilled model,  E   subscript superscript E   \\mathcal{E}^{\\prime}_{\\psi} caligraphic_E start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT , and trained decoder,  D  subscript D  \\mathcal{D}_{\\theta} caligraphic_D start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  from stage 1 to reconstruct high-fidelity fair synthetic samples,  x ^ = D   ( E    ( n ) ) ^ x subscript D  subscript superscript E   n \\hat{x}=\\mathcal{D}_{\\theta}(\\mathcal{E}^{\\prime}_{\\psi}(n)) over^ start_ARG italic_x end_ARG = caligraphic_D start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( caligraphic_E start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_n ) ) ,  n  N  ( 0 , 1 ) similar-to n N 0 1 n\\sim\\mathcal{N}(0,1) italic_n  caligraphic_N ( 0 , 1 ) .",
            "We show the performance of our model in the downstreaming task regarding fairness and data utility for the Adult-Income dataset in Table  2  and the Compas dataset in Table  3 . We use the {gender, race} as sensitive attributes and record the results for both tables. For the Adult-Income dataset, we predict the income class for an individual given their attributes (both sensitive and non-sensitive) as downstreaming task. And for the Compas dataset, we predict the re-offend probability for an inmate given their previous records.",
            "We measure the  Accuracy, Recall  and  F1-score  from the downstreaming task. We observe from Table  2  that our synthetic samples achieve 5% and 8% better performance in fairness and utility compared to FLDGMs (state-of-the-art model). In Table  3 , the performance of our samples Synthetic utility (Coverage) is better than all other methods. We also achieve a balance of fairness and accuracy scores compared with other models.",
            "Besides the empirical quality of our synthetic samples, we also show visual analysis. We show the synthetic image samples from both  CelebA  and  Colored-MNIST  in Figure  1  and  2 . We use Smiling as the sensitive attribute for the  CelebA  and Colors (Red, Green, Blue) for the  MNIST  dataset.",
            "This work presents Fair4Free, a data-free distillation-based fair generative model. With this approach, we generate high-fidelity fair synthetic samples with the help of knowledge distillation. We distil the fair representation from the trained model (teacher model) to another architecture (student model). During the distillation process, we do not use any training data; thus, the training of the student model is data-free. This helps when the dataset is unavailable for security and privacy reasons. Also, as we use a small architecture, we reduce the computational cost, and we can deploy the model into an edge device with better performance than the teacher model. Tables  2  and  3  show that our model is over-performing the state-of-the-art models in the perspective of data utility, fairness and synthetic quality. Besides empirical evaluation, Figure  1 ,  2  shows the synthetic data samples, and Figure  4  shows the usefulness of fair synthetic samples in decision-making."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Fair Synthetic Samples evaluation, Dataset:  Compas . Bold indicates the best result. Synthetic Utility evaluation is only for generative models (TabFairGAN, Decaf, FLDGMs)",
        "table": "S6.T3.96.96",
        "footnotes": [],
        "references": [
            "We evaluate the performance of the distilled fair representation and fair synthetic samples regarding fairness, utility and synthetic sample quality (only for the synthetic samples). For both fairness and utility evaluation, we run a downstreaming task (explained in section  5.3 ) and evaluate the performance of the synthetic samples on Accuracy, F1-Score, Recall (utility metrics) and Demographic Parity Ratio (DPR)  (Weerts et al.,  2023 ) , Equalized Odds Ratio (EOR)  (Weerts et al.,  2023 )  (fairness utility). Along with utility and fairness evaluation, we also measure the synthetic data quality of the fair generative model. We use the Density and Coverage  (Alaa et al.,  2022 )  metrics to validate if the generated samples have the same distribution as the original samples.",
            "We show the performance of our model in the downstreaming task regarding fairness and data utility for the Adult-Income dataset in Table  2  and the Compas dataset in Table  3 . We use the {gender, race} as sensitive attributes and record the results for both tables. For the Adult-Income dataset, we predict the income class for an individual given their attributes (both sensitive and non-sensitive) as downstreaming task. And for the Compas dataset, we predict the re-offend probability for an inmate given their previous records.",
            "We measure the  Accuracy, Recall  and  F1-score  from the downstreaming task. We observe from Table  2  that our synthetic samples achieve 5% and 8% better performance in fairness and utility compared to FLDGMs (state-of-the-art model). In Table  3 , the performance of our samples Synthetic utility (Coverage) is better than all other methods. We also achieve a balance of fairness and accuracy scores compared with other models.",
            "Synthetic samples should perform better for both data utility and synthetic quality. Overall, the performance of the synthetic quality of Fair4Free is better than other generative models. Such as, for Table  3 , though TabFairGAN and Decaf perform better in the utility metrics, their synthetic utility performance is worse than our scores. We achieve 85% better scores in the Coverage metrics than the Decaf (Compas dataset) and 12% better performance than the FLDGMs in the  Adult-Income  dataset.",
            "For the tabular dataset, we use the PCA and t-SNE plots to show how closely the distribution matches the distilled representation and the original fair representation. We observe from Figure  3  that the distributions closely match each other; this signifies that our data-free distillation method transfers the knowledge correctly.",
            "This work presents Fair4Free, a data-free distillation-based fair generative model. With this approach, we generate high-fidelity fair synthetic samples with the help of knowledge distillation. We distil the fair representation from the trained model (teacher model) to another architecture (student model). During the distillation process, we do not use any training data; thus, the training of the student model is data-free. This helps when the dataset is unavailable for security and privacy reasons. Also, as we use a small architecture, we reduce the computational cost, and we can deploy the model into an edge device with better performance than the teacher model. Tables  2  and  3  show that our model is over-performing the state-of-the-art models in the perspective of data utility, fairness and synthetic quality. Besides empirical evaluation, Figure  1 ,  2  shows the synthetic data samples, and Figure  4  shows the usefulness of fair synthetic samples in decision-making."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Hyperparameters for the Distillation Model of Fair4Free (Tabular Dataset)",
        "table": "A1.T4.6",
        "footnotes": [],
        "references": [
            "Along with the empirical evaluation and visual evaluation, we also test the feature importance of the original and synthetic data in downstreaming task. We set up a task, i.e., for the  Compas  dataset, given the inmates records, to predict how likely the person will re-offend. Figure  4  shows the feature importance for both original and synthetic data (in this case, we use  Gender (sex)  as a sensitive attribute.). We observe that, for the original dataset, gender plays a vital role in reaching the decision, on the contrary, our synthetic samples do not rely on the sensitive attribute for making a decision. This also proves the usefulness of our synthetic samples for fair decision-making process.",
            "This work presents Fair4Free, a data-free distillation-based fair generative model. With this approach, we generate high-fidelity fair synthetic samples with the help of knowledge distillation. We distil the fair representation from the trained model (teacher model) to another architecture (student model). During the distillation process, we do not use any training data; thus, the training of the student model is data-free. This helps when the dataset is unavailable for security and privacy reasons. Also, as we use a small architecture, we reduce the computational cost, and we can deploy the model into an edge device with better performance than the teacher model. Tables  2  and  3  show that our model is over-performing the state-of-the-art models in the perspective of data utility, fairness and synthetic quality. Besides empirical evaluation, Figure  1 ,  2  shows the synthetic data samples, and Figure  4  shows the usefulness of fair synthetic samples in decision-making.",
            "To train the benchmarking models, we use FairX  (Sikder et al.,  2024b ) , a fair benchmarking tool. FairX uses the same hyperparameters for the respective benchmarking models, specified in their original publication. For our generative model, we follow the same architecture of FairDisco  (Liu et al.,  2022 )  for the VAE setup (teacher model) and Table  4  shows the hyperparameters for the distillation model (student model)."
        ]
    }
}