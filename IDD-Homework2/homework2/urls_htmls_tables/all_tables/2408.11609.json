{
    "id_table_1": {
        "caption": "Table 1.  Consistency analysis of Human and GPT-4 on five dimensions. Arug. refers to Argument, and r means the Pearson correlation coefficient.",
        "table": "S5.T1.1",
        "footnotes": [],
        "references": [
            "With the advancement of natural language processing (NLP), particularly large language models (LLMs), numerous text-generation systems have been proposed to enhance the effectiveness and efficiency of individuals across various fields, such as education, medicine and law  (Adeshola and Adepoju,  2023 ; Wu et al . ,  2023b ; Cheng et al . ,  2023 ) . Commentary is a type of article that contains diverse arguments and compelling evidence, which aims to provide readers with a deep understanding of certain events. As Figure.  1  shows, a commentator usually spends several hours writing a commentary, which includes mining arguments, searching for evidence, and embellishing the article. Given the continuous nature of news, their workload is substantial. Therefore, exploring the application of LLMs in commentary generation is worthwhile.",
            "\\bullet   The structure should be regular and complete. As Figure.  1  shows, the commentary should follow a total division structure.",
            "Peg , within the scope of commentary generation, denotes the specific aspect of the event that the commentary is responding to or building upon. It acts as an anchor for the commentary. For instance, in Figure.  1 , the peg is Age of smokers decrease.",
            "In our experiments, we utilize GPT-4 for automatic evaluation by crafting specific prompts for each dimension. To validate GPT-4s accuracy, we compare its scores against those from human annotators for 30 randomly selected commentaries, calculating the Pearson correlation coefficient  (Cohen et al . ,  2009 )  for each dimension. As Tab.  1  illustrates, the Pearson correlation coefficient of each dimension surpasses 0.6, which proves GPT-4 is competent for this task."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Evaluation of commentary generated by baseline LLMs using GPT-4. Reference is the published commentaries. Bold indicates the highest score within the 20B scale baselines, and an asterisk (*) denotes the highest score among all baselines.",
        "table": "S5.T2.1",
        "footnotes": [],
        "references": [
            "In this section, we delve into Xinyus comprehensive technical route. Figure.  2  shows the overall framework of Xinyu. In Section 4.1, we introduce the five main generative components used in detail during the commentary generation process. In Section 4.2, we shift our focus to the two auxiliary components essential for meeting the advanced requirements of the commentary, including the argument ranking model and the construction of an evidence database. Note that without these two auxiliary components, the system can still fulfill the fundamental requirements.",
            "Based on the models size, we split them into two types: LLMs larger than 20B and LLMs smaller than 20B. Tab.  2  shows the results of commentary generation with GPT-4s evaluation. We report the results of the ablation study in Tab.  3 , Tab.  4 , Tab.  5 , and Figure.  5 .",
            "From Tab.  2 , we can conclude that:  (1) Generally, the bigger the language models size, the better it does. However, GPT-4s leading advantage in this task is not as pronounced as in other generative tasks.  (2) When looking at models within the 20 billion parameter size, our method achieved the best results in most of the metrics.  (3) Compared to large-scale LLMs such as GPT-4, our method attained the best overall score, primarily due to our superior performance in the advanced requirements of argument and evidence.  (4) Theres not a huge gap between the scores of the different methods. This is largely because GPT-4 is generally not harsh in its scoring, rarely giving out very low scores."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  GPT-4s evaluation of commentary generated by our framework with different base models. Bold indicates the highest score within the 20B scale baselines, and an asterisk (*) denotes the highest score among all baselines.",
        "table": "S5.T3.1",
        "footnotes": [],
        "references": [
            "The base model of Xinyu is LLaMA2-13B  (Touvron et al . ,  2023 ) , and we specifically adapted it to better accommodate the nuances of the Chinese language. This adaptation involved expanding the LLaMA-13B tokenizer with an additional 28,000 Chinese tokens. To further optimize the model, we continued pre-training on LLaMA-13B using a corpus comprising 500B tokens, which contains both English and Chinese corpus. For supervised fine-tuning, we not only utilized the dataset introduced in Section 4 but also incorporated the general SFT dataset   https://huggingface.co/datasets/BelleGroup/train_2M_CN  to maintain consistency with the data distribution of previous training phases. The amount of SFT data is 400,000 and the distribution of it is shown in Figure.  3 .  Our training process leveraged the Megatron-DeepSpeed framework. The continued pre-training phase lasted 20 days on 128 Nvidia A800 80G GPUs, while the supervised fine-tuning (SFT) process took 2 days on 8 Nvidia A800 80G GPUs.",
            "Based on the models size, we split them into two types: LLMs larger than 20B and LLMs smaller than 20B. Tab.  2  shows the results of commentary generation with GPT-4s evaluation. We report the results of the ablation study in Tab.  3 , Tab.  4 , Tab.  5 , and Figure.  5 .",
            "From Tab.  3 , we have the following observations:  (1) Our framework significantly enhances the performance of large-sized base models. For instance, the overall score of Qwen-72B increased from 7.37 to 7.82.  (2) GPT-4 achieved the best performance with an overall score of 8.3, and our Xinyu ranks just behind GPT-4. Considering the size of the model, our method has greater potential in practice.  (3) For the 20B scale base models such as Qwen-14B-Chat, using our framework actually decreased their performance. This might be due to these base models inherent limitations in generating text step-by-step. This also demonstrates the effectiveness of our SFT."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  Results of ablation study on Argument Ranking Model.",
        "table": "S5.T4.1",
        "footnotes": [],
        "references": [
            "Based on the models size, we split them into two types: LLMs larger than 20B and LLMs smaller than 20B. Tab.  2  shows the results of commentary generation with GPT-4s evaluation. We report the results of the ablation study in Tab.  3 , Tab.  4 , Tab.  5 , and Figure.  5 .",
            "From Tab.  4 , we have the following observations:  (1) The implementation of the argument ranking model has significantly improved the effectiveness of argumentation, underscoring its impact.  (2) Enhancements are observed across all metrics, illustrating the interrelationship among these aspects of commentary.",
            "Figure  4  presents three commentaries on a certain peg generated by Xinyu, GPT-4, and Baichuan2-13B-Chat, respectively. All three commentaries exhibit good language fluency and structural coherence, highlighting the capabilities of these large language models (LLMs). However, the commentary from Baichuan2-13B-Chat focuses solely on facts without offering specific arguments. In contrast, both GPT-4 and Xinyu provide detailed arguments. Notably, Xinyus commentary stands out by presenting more convincing evidence and demonstrating a logical correlation in its supporting arguments."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.  Results of ablation study on Evidence Database. The Kappa value of Timeliness exceeds 0.78.",
        "table": "S5.T5.1",
        "footnotes": [],
        "references": [
            "Based on the models size, we split them into two types: LLMs larger than 20B and LLMs smaller than 20B. Tab.  2  shows the results of commentary generation with GPT-4s evaluation. We report the results of the ablation study in Tab.  3 , Tab.  4 , Tab.  5 , and Figure.  5 .",
            "From Tab.  5 , we can find that:  (1) The implementation of retrieval augmented generation (RAG) significantly enhances the generation of evidence, with scores improving from 5.60 to 7.41. Additionally, the timeliness of the generated evidence also saw an increase, rising from 8.85 to 9.30.  (2) After incorporating the book dataset into RAG, its performance experienced further improvements.  (3) The improvement proves the effectiveness of our evidence dataset.",
            "From Figure.  5 , we have the following observations:  (1) Utilizing Xinyus assistance can significantly increase writing speed, and the average time for a commentary speeds up from more than 4 hours to 20 mins.  (2) Moreover, commentaries generated with LLMs have achieved the same scores as manual writing, demonstrating the practicality of our system."
        ]
    },
    "global_footnotes": [
        "* These authors contributed equally to this work.",
        "* These authors contributed equally to this work.",
        "These authors contributed equally to this work.",
        "These authors contributed equally to this work.",
        "Corresponding Author.",
        "Note the user can interact with Xinyu by providing additional input or editing the output at each step and here we mainly describe the automatic process."
    ]
}