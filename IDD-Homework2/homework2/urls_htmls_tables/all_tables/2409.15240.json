{
    "id_table_1": {
        "caption": "Table 1:  Evaluation Aspects of responses in representative research. ED is short for EmpatheticDialogues  Rashkin et al. ( 2019 ) , and ESC is short for Emotional Supporting Conversation  Liu et al. ( 2021 ) .",
        "table": "S2.T1.1",
        "footnotes": [],
        "references": [
            "Another difference from metrics in previous studies lies in the scoring standard, which was at a Likert scale while the requirement of each point in our metrics is strictly listed to reduce ambiguity. For instance, Naturalness contains 3 sub-aspects corresponding to 3 points, where each point can be rated from 0 to 1 and summed up to 3 at most. The difference between MemBench and previous works is listed in Table  1 .",
            "After retrieving a subset of memories, the LLM needs to select the most appropriate memory for the dialogue, this process is called memory recognition in two-stage theory. The model responsible for selection and generation is denoted as  M 2 subscript M 2 \\textup{M}_{2} M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . According to psychological research  Austin et al. ( 2018 ) , criteria for selecting and utilizing memories vary across different tasks of active or passive recall, as well as within each subcategory of these tasks, such as different emotions or various people, events, and objects. The detailed criteria are shown in Table  10  in the Appendix. Besides the recognition, the models are asked to inject suitable memory into the response properly. Given a sample  D i , P i , R i subscript D i subscript P i subscript R i \\mathcal{D}_{i},P_{i},\\mathcal{R}_{i} caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , where  P i subscript P i P_{i} italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the candidate set of memories,  R i subscript R i \\mathcal{R}_{i} caligraphic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the reference response, the target is to generate a response that semantically similar to  R i subscript R i \\mathcal{R}_{i} caligraphic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .",
            "The automatic results are displayed in  8 . The full result is in Appendix  11 .  As we can see, the GLM-4 can achieve the highest semantic similarity while Qwen2-72B earned the highest Rouge-L. However, the gap between Qwen2-72B and GLM-4 is marginal. For Dist-1 and 2, the Ziya-Character got the highest results. It may be attributed to its training on multiple diverse character corpora in SFT. However, it is opposite to the conclusion in the human evaluation that GPT4-Turbo is the best and Qwen2-72B ranked second. Therefore,  automatic evaluation fails to measure the results from LLMs . We also tried judging with GPT4 using various promptings and found its scores were unreliable.",
            "The memory-recalling procedure contains both passive and proactive types. Passive recalling is mainly based on context similarity. Proactive recalling criteria are listed in Table  10 . These situations are the most basic ones that should be recalled but not limited to these in reality.",
            "Results in the Chinese version are in Table  11 .",
            "We calculate the spearman correlation between annotators for each setting and report the average correlation scores for all aspects over models in table  12 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Statistical details of data. Avg Len. is short for average length per turn.",
        "table": "S3.T2.1",
        "footnotes": [],
        "references": [
            "MemBench encompasses two tasks related to conversational memory recall: proactive recall of emotional memories and passive recall of objective facts. The proactive recall involves four types: happy, sad, anxious, and disappointed memories. On the other hand, the passive recall of objective facts encompasses activities, objects, and social relationships. Social relationships are further categorized into positive and negative relationships. The data distribution is illustrated in Figure  2 . Considering the two-phase nature of memory utilization, we also divide the testing of a memory dialogue system into two stages: memory recall and response generation.",
            "We calculate the spearman correlation between annotators for each setting and report the average correlation scores for all aspects over models in table  12 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  A memory sample.",
        "table": "S3.T3.1",
        "footnotes": [],
        "references": [
            "When in setting 1 where golden memory was provided, all the models had over 2.2 scores in ES Skill. But in reality (setting 3), scores on ES Skill drop significantly. Therefore, The ES Skill Proficiency is related to Memory Injection ability. To dig out the correlation, we draw the distribution of ES skill grades with varied memory-injection scores in Figure  3 . We can find  higher memory-injection ability leads to higher ES skillfulness . A 3.0 memory injection score can lead to a 3.0 ES Skill in 61%, which emphasizes the importance of memory-augmented ability in ES."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Memory recall performance in English version and Chinese version.",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": [
            "For a comprehensive evaluation of the memory recall performance of the embedding model  M 1 subscript M 1 \\textup{M}_{1} M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , and considering the different levels of relevance among memories for a particular dialogue, we measured Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), Normalized Discounted Cumulative Gain (nDCG), Recall, Precision and calculated their geometric mean across different scopes. The detailed metrics are illustrated in Tab. 4 . MRR places greater emphasis on the top-ranked position, focusing more on the golden summary  m  subscript m m_{*} italic_m start_POSTSUBSCRIPT  end_POSTSUBSCRIPT . MAP uniformly considers all relevant summaries, while nDCG accounts for the relevance and positions of different memories. The geometric mean is to comprehensively assess their retrieval quality while mitigating the impact of outliers.",
            "From the embedding results in Tab.  4 , it can be observed that OpenAI embedding achieved the best performance on both English and Chinese datasets. The bilingual BGM-M3 also demonstrated commendable performance among the open-source models in the English testbed. For the Chinese testbed, Acge is the best open-source model in our setting."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Human Evaluation Results on memory recognition and response generation. The prompts for setting 2 and setting 3 particularly emphasize the naturalness and style requirements to improve the performance. The highest score for each aspect per task is in bold.",
        "table": "S5.T5.1",
        "footnotes": [],
        "references": [
            "Table  5  depicts LLMs average scores of each aspect on all three tasks in the Chinese version."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Side-by-side evaluation results on the responses with and without memory.",
        "table": "S5.T6.1",
        "footnotes": [],
        "references": [
            "The Side-by-side evaluation results on intimacy of responses with and without memory are in Table  6 . As the table indicates, responses with memory injection are almost (no less than 69.4% probability) better than those without memory. The win rate grows higher as the model becomes stronger. For example, GPT4-Turbo can produce 61.9% more intimate response with memory while Ziya-Character can only produce 47.5% win rate. It is attributed to their stronger memory injection ability and accordingly the ES skill. It implies that  intimacy performance is highly related to memory injection ability."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  The memory injection rate in best candidates voted by humans.",
        "table": "S5.T7.1",
        "footnotes": [],
        "references": [
            "We also investigate peoples preferences by asking 5 annotators to select the best responses and see whether they are equipped with memory in setting 3. Results of the ratio of containing memory in voted responses are listed in Table  7 . On average, 73% best responses are with memory, which shows people prefer memory-aware replies. We think it can provide better emotional support."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Automatic Evaluation Results on Chinese testbed. Full results in Table  11 . The automatic measurements are inconsistent with human evaluation.",
        "table": "S5.T8.1",
        "footnotes": [],
        "references": [
            "The automatic results are displayed in  8 . The full result is in Appendix  11 .  As we can see, the GLM-4 can achieve the highest semantic similarity while Qwen2-72B earned the highest Rouge-L. However, the gap between Qwen2-72B and GLM-4 is marginal. For Dist-1 and 2, the Ziya-Character got the highest results. It may be attributed to its training on multiple diverse character corpora in SFT. However, it is opposite to the conclusion in the human evaluation that GPT4-Turbo is the best and Qwen2-72B ranked second. Therefore,  automatic evaluation fails to measure the results from LLMs . We also tried judging with GPT4 using various promptings and found its scores were unreliable."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Definition of the scenes in MemBench. Each memory is annotated with one scene. If a dialogue is involved in multiple scenes, the priority will be Disease>Emotions>Preference>Activities>Others.",
        "table": "A1.T9.1",
        "footnotes": [],
        "references": [
            "Since the memory-recalling procedure is related to emotions and other surroundings, we first curated 17 emotional categories according to cognitive science and psychological theories  Ekman ( 1992 ); PLUTCHIK ( 1980 ); Sabour et al. ( 2024 ) . Then we asked a psychology major graduate to decide what kind of memories should be recalled to regulate certain emotions for proactive memory recall. Finally, 4 emotions including happiness, sadness, anxiety, and disappointment were selected. We also define 5 scenes to describe the topic, physical state and surroundings of the users, which contain Preferences, Activity, Disease, Emotions and Others. The definition of each scene is in the Table  9  in Appendix.",
            "The definition of scenes in memory is listed in Table  9 ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  The proactive memory recalling criteria. \"|\" represents \"or\".",
        "table": "A2.T10.1",
        "footnotes": [],
        "references": [
            "After retrieving a subset of memories, the LLM needs to select the most appropriate memory for the dialogue, this process is called memory recognition in two-stage theory. The model responsible for selection and generation is denoted as  M 2 subscript M 2 \\textup{M}_{2} M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . According to psychological research  Austin et al. ( 2018 ) , criteria for selecting and utilizing memories vary across different tasks of active or passive recall, as well as within each subcategory of these tasks, such as different emotions or various people, events, and objects. The detailed criteria are shown in Table  10  in the Appendix. Besides the recognition, the models are asked to inject suitable memory into the response properly. Given a sample  D i , P i , R i subscript D i subscript P i subscript R i \\mathcal{D}_{i},P_{i},\\mathcal{R}_{i} caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , where  P i subscript P i P_{i} italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the candidate set of memories,  R i subscript R i \\mathcal{R}_{i} caligraphic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the reference response, the target is to generate a response that semantically similar to  R i subscript R i \\mathcal{R}_{i} caligraphic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .",
            "The memory-recalling procedure contains both passive and proactive types. Passive recalling is mainly based on context similarity. Proactive recalling criteria are listed in Table  10 . These situations are the most basic ones that should be recalled but not limited to these in reality."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Automatic Evaluation Results in Chinese version.",
        "table": "A4.T11.1",
        "footnotes": [],
        "references": [
            "The automatic results are displayed in  8 . The full result is in Appendix  11 .  As we can see, the GLM-4 can achieve the highest semantic similarity while Qwen2-72B earned the highest Rouge-L. However, the gap between Qwen2-72B and GLM-4 is marginal. For Dist-1 and 2, the Ziya-Character got the highest results. It may be attributed to its training on multiple diverse character corpora in SFT. However, it is opposite to the conclusion in the human evaluation that GPT4-Turbo is the best and Qwen2-72B ranked second. Therefore,  automatic evaluation fails to measure the results from LLMs . We also tried judging with GPT4 using various promptings and found its scores were unreliable.",
            "Results in the Chinese version are in Table  11 ."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  The average correlation of human annotation in memory recognition and response generation. According to psychological assessment standards, the correlation between annotators in the first two settings are strong while that in setting 3 is moderate.",
        "table": "A5.T12.1",
        "footnotes": [],
        "references": [
            "We calculate the spearman correlation between annotators for each setting and report the average correlation scores for all aspects over models in table  12 ."
        ]
    },
    "global_footnotes": [
        "GPT4-Turbo-0429,",
        "We used the best one according to our experiments.",
        "in June 2024. Leaderboard:"
    ]
}