{
    "PAPER'S NUMBER OF TABLES": 5,
    "S6.T1": {
        "caption": "Table 1. Deploy and Aggregation Time Analysis over Contract Variation",
        "table": "<table id=\"S6.T1.st1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T1.st1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T1.st1.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"># Contracts</th>\n<th id=\"S6.T1.st1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Contract Deploy</th>\n<th id=\"S6.T1.st1.2.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Parameter Aggregate</th>\n</tr>\n<tr id=\"S6.T1.st1.2.2.2\" class=\"ltx_tr\">\n<th id=\"S6.T1.st1.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Deployed</th>\n<th id=\"S6.T1.st1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Time</th>\n<th id=\"S6.T1.st1.2.2.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">Time</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T1.st1.2.3.1\" class=\"ltx_tr\">\n<td id=\"S6.T1.st1.2.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td id=\"S6.T1.st1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">45 seconds</td>\n<td id=\"S6.T1.st1.2.3.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">4 seconds</td>\n</tr>\n<tr id=\"S6.T1.st1.2.4.2\" class=\"ltx_tr\">\n<td id=\"S6.T1.st1.2.4.2.1\" class=\"ltx_td ltx_align_center\">2</td>\n<td id=\"S6.T1.st1.2.4.2.2\" class=\"ltx_td ltx_align_center\">71 seconds</td>\n<td id=\"S6.T1.st1.2.4.2.3\" class=\"ltx_td ltx_align_left\">3.5 seconds</td>\n</tr>\n<tr id=\"S6.T1.st1.2.5.3\" class=\"ltx_tr\">\n<td id=\"S6.T1.st1.2.5.3.1\" class=\"ltx_td ltx_align_center\">4</td>\n<td id=\"S6.T1.st1.2.5.3.2\" class=\"ltx_td ltx_align_center\">93 seconds</td>\n<td id=\"S6.T1.st1.2.5.3.3\" class=\"ltx_td ltx_align_left\">3.5 seconds</td>\n</tr>\n<tr id=\"S6.T1.st1.2.6.4\" class=\"ltx_tr\">\n<td id=\"S6.T1.st1.2.6.4.1\" class=\"ltx_td ltx_align_center\">8</td>\n<td id=\"S6.T1.st1.2.6.4.2\" class=\"ltx_td ltx_align_center\">144 seconds</td>\n<td id=\"S6.T1.st1.2.6.4.3\" class=\"ltx_td ltx_align_left\">3 seconds</td>\n</tr>\n<tr id=\"S6.T1.st1.2.7.5\" class=\"ltx_tr\">\n<td id=\"S6.T1.st1.2.7.5.1\" class=\"ltx_td ltx_align_center\">16</td>\n<td id=\"S6.T1.st1.2.7.5.2\" class=\"ltx_td ltx_align_center\">243 seconds</td>\n<td id=\"S6.T1.st1.2.7.5.3\" class=\"ltx_td ltx_align_left\">2.8 seconds</td>\n</tr>\n<tr id=\"S6.T1.st1.2.8.6\" class=\"ltx_tr\">\n<td id=\"S6.T1.st1.2.8.6.1\" class=\"ltx_td ltx_align_center\">32</td>\n<td id=\"S6.T1.st1.2.8.6.2\" class=\"ltx_td ltx_align_center\">443 seconds</td>\n<td id=\"S6.T1.st1.2.8.6.3\" class=\"ltx_td ltx_align_left\">2.5 seconds</td>\n</tr>\n<tr id=\"S6.T1.st1.2.9.7\" class=\"ltx_tr\">\n<td id=\"S6.T1.st1.2.9.7.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">48</td>\n<td id=\"S6.T1.st1.2.9.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">503 seconds</td>\n<td id=\"S6.T1.st1.2.9.7.3\" class=\"ltx_td ltx_align_left ltx_border_bb\">2.3 seconds</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "The aggregation time as seen in Table 1(a), for any Federated Learning signSGD setting is always less than 5 seconds. The results shown in Table 1(a) are recorded to demonstrate the worst case timing. Realistically, the parameters sent by each worker node would not be above 300 million parameters. Therefore, we have shown that even in the worst case scenario, aggregation happens within 5 seconds.",
            "Thus, if we add up the results seen in Table 1(a) and Figure 4, for each worker node, with the current system, for 5-10 epochs, the total time taken for the aggregation and parameter loading to run takes less than 1.5 minute.",
            "The aggregation time as seen in Table LABEL:tab:FedAvg, for any Federated Averaging setting is less than 10 seconds constantly because it is being done on the ledger and will consume only one transaction to return the results. Similar to Table 1(a), the results shown in Table LABEL:tab:FedAvg demonstrate the worst case timing for aggregating 300 million parameters."
        ]
    },
    "S6.T1.st1": {
        "caption": "(a) signSGD",
        "table": "<table id=\"S6.T1.st1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T1.st1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T1.st1.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"># Contracts</th>\n<th id=\"S6.T1.st1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Contract Deploy</th>\n<th id=\"S6.T1.st1.2.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Parameter Aggregate</th>\n</tr>\n<tr id=\"S6.T1.st1.2.2.2\" class=\"ltx_tr\">\n<th id=\"S6.T1.st1.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Deployed</th>\n<th id=\"S6.T1.st1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Time</th>\n<th id=\"S6.T1.st1.2.2.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">Time</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T1.st1.2.3.1\" class=\"ltx_tr\">\n<td id=\"S6.T1.st1.2.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td id=\"S6.T1.st1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">45 seconds</td>\n<td id=\"S6.T1.st1.2.3.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">4 seconds</td>\n</tr>\n<tr id=\"S6.T1.st1.2.4.2\" class=\"ltx_tr\">\n<td id=\"S6.T1.st1.2.4.2.1\" class=\"ltx_td ltx_align_center\">2</td>\n<td id=\"S6.T1.st1.2.4.2.2\" class=\"ltx_td ltx_align_center\">71 seconds</td>\n<td id=\"S6.T1.st1.2.4.2.3\" class=\"ltx_td ltx_align_left\">3.5 seconds</td>\n</tr>\n<tr id=\"S6.T1.st1.2.5.3\" class=\"ltx_tr\">\n<td id=\"S6.T1.st1.2.5.3.1\" class=\"ltx_td ltx_align_center\">4</td>\n<td id=\"S6.T1.st1.2.5.3.2\" class=\"ltx_td ltx_align_center\">93 seconds</td>\n<td id=\"S6.T1.st1.2.5.3.3\" class=\"ltx_td ltx_align_left\">3.5 seconds</td>\n</tr>\n<tr id=\"S6.T1.st1.2.6.4\" class=\"ltx_tr\">\n<td id=\"S6.T1.st1.2.6.4.1\" class=\"ltx_td ltx_align_center\">8</td>\n<td id=\"S6.T1.st1.2.6.4.2\" class=\"ltx_td ltx_align_center\">144 seconds</td>\n<td id=\"S6.T1.st1.2.6.4.3\" class=\"ltx_td ltx_align_left\">3 seconds</td>\n</tr>\n<tr id=\"S6.T1.st1.2.7.5\" class=\"ltx_tr\">\n<td id=\"S6.T1.st1.2.7.5.1\" class=\"ltx_td ltx_align_center\">16</td>\n<td id=\"S6.T1.st1.2.7.5.2\" class=\"ltx_td ltx_align_center\">243 seconds</td>\n<td id=\"S6.T1.st1.2.7.5.3\" class=\"ltx_td ltx_align_left\">2.8 seconds</td>\n</tr>\n<tr id=\"S6.T1.st1.2.8.6\" class=\"ltx_tr\">\n<td id=\"S6.T1.st1.2.8.6.1\" class=\"ltx_td ltx_align_center\">32</td>\n<td id=\"S6.T1.st1.2.8.6.2\" class=\"ltx_td ltx_align_center\">443 seconds</td>\n<td id=\"S6.T1.st1.2.8.6.3\" class=\"ltx_td ltx_align_left\">2.5 seconds</td>\n</tr>\n<tr id=\"S6.T1.st1.2.9.7\" class=\"ltx_tr\">\n<td id=\"S6.T1.st1.2.9.7.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">48</td>\n<td id=\"S6.T1.st1.2.9.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">503 seconds</td>\n<td id=\"S6.T1.st1.2.9.7.3\" class=\"ltx_td ltx_align_left ltx_border_bb\">2.3 seconds</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Federated Learning (FL) is a distributed, and decentralized machine learning protocol. By executing FL, a set of agents can jointly train a model without sharing their datasets with each other, or a third-party. This makes FL particularly suitable for settings where data privacy is desired.",
            "Federated Learning (McMahan et al., 2016a) (FL) is a multi-round machine learning protocol that is run between an aggregation server and a set of agents. FL allows the participating agents to collaboratively train a model without sharing their data with each other, or with a third-party. At a high level, each agent first locally trains a model on his dataset, and then send his model to the server for aggregation. In return, the server aggregates the received models,\nand returns the aggregated model back to agents for the next round of training. The rounds can simply go on until the trained model reaches some desired performance metric (e.g., accuracy) on a validation dataset maintained by the server. Since the data does not leave its owner, FL is particularly suitable for settings where privacy-sensitive data is involved. A vast range of organizations can collaborate on training a model via FL, and obtain a better performing model with respect to a model that is only trained on locally available data, while maintaining privacy of the data.",
            "To make FL accountable, we leverage blockchains since they are compatible with the decentralized nature of FL, provide practical immutability, and Turing-complete computation on the logged data via smart contracts. The challenge in this context is to design a blockchain architecture with a low communication and latency overhead such that it can be seamlessly incorporated to the FL data flow. We address this problem by designing a hybrid architecture consisting of both a public, and a private blockchain. This is because latency in public blockchains is too high to run any computation intensive algorithms, and the data on the public blockchains can be accessed by anyone. On the other hand, even though private blockchains are communication efficient, and address privacy challenges by allowing sensitive data to be seen only by an approved set of participants, they do not allow for public accountability since transactions are approved by a predetermined set of users, and cannot be accessed publicly.\nThus, by combining public, and private blockchains in a novel architecture, we alleviate the weakness of each, and have an architecture that meets the needs of FL (cf. Figure 1 for an overview of our framework).",
            "Training time attacks against machine learning models can roughly be classified into two categories: targeted (Bhagoji et al., 2019; Bagdasaryan et al., 2020; Chen\net al., 2017; Liu et al., 2018), and untargeted attacks (Blanchard et al., 2017; Bernstein et al., 2018).\nIn untargeted attacks, the adversarial task is to make the model converge to a sub-optimal minima, or to make the model completely diverge. Such attacks have been also referred as convergence attacks, and to some extend, they are easily detectable by observing the model’s accuracy on a validation data.",
            "The smart contract hosts an array per worker node to store the cryptographic (i.e., SHA256) hashes of each parameter set sent to the private blockchain based server.\nThe hash value is calculated individually by each worker node off the chain on their own local machine and sent to the public smart contract as shown in Step 1B of Figure 1. The primary purpose served here is for any worker to hold any offending worker nodes accountable in the case a trojan is detected. As the public chain is transparent and immutable, the worker node can merely download the SHA256, retrieve a recreated SHA256 from the parameter store on the private chain and verify if the SHA256 stored on the public chain matches the private chain created SHA256.",
            "We use openssl to generate self-signed Certificate Authorities (CA) and issue client certificates to the peer nodes signed by the CAs. Nodejs scripts are written to control the deployment of chaincodes and creation of channels in a command-driven routine. A nodejs script is implemented to act as the liaison between the hyperledger fabric and ethereum networks. We use etheruem smart contract auto-generation technique for penalty payments on the public chain. Appropriate security groups are implemented for the endorsing peer based EC2 instances to communicate with each other. Each account on the Ropsten Ethereum chain are given 10 Ether to begin with and are requested from the Ropsten Ethereum faucet available on the internet. Binary to base64 converter is implemented for signSGD to convert participant provided parameters into base64 to upload to the chaincode. We use the peer binary provided by Hyperledger Fabric to upload parameters in a repeatable fashion.",
            "The aggregation time as seen in Table 1(a), for any Federated Learning signSGD setting is always less than 5 seconds. The results shown in Table 1(a) are recorded to demonstrate the worst case timing. Realistically, the parameters sent by each worker node would not be above 300 million parameters. Therefore, we have shown that even in the worst case scenario, aggregation happens within 5 seconds.",
            "Thus, if we add up the results seen in Table 1(a) and Figure 4, for each worker node, with the current system, for 5-10 epochs, the total time taken for the aggregation and parameter loading to run takes less than 1.5 minute.",
            "The aggregation time as seen in Table LABEL:tab:FedAvg, for any Federated Averaging setting is less than 10 seconds constantly because it is being done on the ledger and will consume only one transaction to return the results. Similar to Table 1(a), the results shown in Table LABEL:tab:FedAvg demonstrate the worst case timing for aggregating 300 million parameters.",
            "Thus, if we add up the results seen in Table LABEL:tab:FedAvg and Figure 4, for each worker node, with the current system, for 5-10 epochs, the total time taken for the complete FedAvg algorithm to run takes less than 12 minutes.",
            "On the other hand, a malicious worker node can try to attack the system by introducing a backdoor /trojan. Assuming the detection technique work with high probability, the participants may be held accountable for an attack.\nSince all participants commit the SHA256 Hashes of their parametric updates to the public smart contract, we can easily detect incorrect disclosure of updates. If for any reason, the participant introduces a backdoor, then the federated learning algorithm can be re-simulated on the private chain using the updates as the test data using the appropriate off-chain detection technique.",
            "In order to address this aforementioned problem, we have proposed a general blockchain framework that integrates both public and private blockchains to achieve decentralization, immutability and transparency to discourage backdoor attacks on federated learning algorithms by holding the responsible parties accountable. We have shown that our framework facilitates the adaptation to any federated learning algorithm rendering a plug-and-play setting. We have presented the implementation of the Federated Averaging and signSGD algorithm\nover our general blockchain framework and based on our empirical results, conclude that our proposed approach maintains the communication-efficient nature of these algorithms."
        ]
    },
    "S6.T1.st2": {
        "caption": "",
        "table": "",
        "footnotes": "",
        "references": [
            "Federated Learning (FL) is a distributed, and decentralized machine learning protocol. By executing FL, a set of agents can jointly train a model without sharing their datasets with each other, or a third-party. This makes FL particularly suitable for settings where data privacy is desired.",
            "Federated Learning (McMahan et al., 2016a) (FL) is a multi-round machine learning protocol that is run between an aggregation server and a set of agents. FL allows the participating agents to collaboratively train a model without sharing their data with each other, or with a third-party. At a high level, each agent first locally trains a model on his dataset, and then send his model to the server for aggregation. In return, the server aggregates the received models,\nand returns the aggregated model back to agents for the next round of training. The rounds can simply go on until the trained model reaches some desired performance metric (e.g., accuracy) on a validation dataset maintained by the server. Since the data does not leave its owner, FL is particularly suitable for settings where privacy-sensitive data is involved. A vast range of organizations can collaborate on training a model via FL, and obtain a better performing model with respect to a model that is only trained on locally available data, while maintaining privacy of the data.",
            "To make FL accountable, we leverage blockchains since they are compatible with the decentralized nature of FL, provide practical immutability, and Turing-complete computation on the logged data via smart contracts. The challenge in this context is to design a blockchain architecture with a low communication and latency overhead such that it can be seamlessly incorporated to the FL data flow. We address this problem by designing a hybrid architecture consisting of both a public, and a private blockchain. This is because latency in public blockchains is too high to run any computation intensive algorithms, and the data on the public blockchains can be accessed by anyone. On the other hand, even though private blockchains are communication efficient, and address privacy challenges by allowing sensitive data to be seen only by an approved set of participants, they do not allow for public accountability since transactions are approved by a predetermined set of users, and cannot be accessed publicly.\nThus, by combining public, and private blockchains in a novel architecture, we alleviate the weakness of each, and have an architecture that meets the needs of FL (cf. Figure 1 for an overview of our framework).",
            "Training time attacks against machine learning models can roughly be classified into two categories: targeted (Bhagoji et al., 2019; Bagdasaryan et al., 2020; Chen\net al., 2017; Liu et al., 2018), and untargeted attacks (Blanchard et al., 2017; Bernstein et al., 2018).\nIn untargeted attacks, the adversarial task is to make the model converge to a sub-optimal minima, or to make the model completely diverge. Such attacks have been also referred as convergence attacks, and to some extend, they are easily detectable by observing the model’s accuracy on a validation data.",
            "The smart contract hosts an array per worker node to store the cryptographic (i.e., SHA256) hashes of each parameter set sent to the private blockchain based server.\nThe hash value is calculated individually by each worker node off the chain on their own local machine and sent to the public smart contract as shown in Step 1B of Figure 1. The primary purpose served here is for any worker to hold any offending worker nodes accountable in the case a trojan is detected. As the public chain is transparent and immutable, the worker node can merely download the SHA256, retrieve a recreated SHA256 from the parameter store on the private chain and verify if the SHA256 stored on the public chain matches the private chain created SHA256.",
            "We use openssl to generate self-signed Certificate Authorities (CA) and issue client certificates to the peer nodes signed by the CAs. Nodejs scripts are written to control the deployment of chaincodes and creation of channels in a command-driven routine. A nodejs script is implemented to act as the liaison between the hyperledger fabric and ethereum networks. We use etheruem smart contract auto-generation technique for penalty payments on the public chain. Appropriate security groups are implemented for the endorsing peer based EC2 instances to communicate with each other. Each account on the Ropsten Ethereum chain are given 10 Ether to begin with and are requested from the Ropsten Ethereum faucet available on the internet. Binary to base64 converter is implemented for signSGD to convert participant provided parameters into base64 to upload to the chaincode. We use the peer binary provided by Hyperledger Fabric to upload parameters in a repeatable fashion.",
            "The aggregation time as seen in Table 1(a), for any Federated Learning signSGD setting is always less than 5 seconds. The results shown in Table 1(a) are recorded to demonstrate the worst case timing. Realistically, the parameters sent by each worker node would not be above 300 million parameters. Therefore, we have shown that even in the worst case scenario, aggregation happens within 5 seconds.",
            "Thus, if we add up the results seen in Table 1(a) and Figure 4, for each worker node, with the current system, for 5-10 epochs, the total time taken for the aggregation and parameter loading to run takes less than 1.5 minute.",
            "The aggregation time as seen in Table LABEL:tab:FedAvg, for any Federated Averaging setting is less than 10 seconds constantly because it is being done on the ledger and will consume only one transaction to return the results. Similar to Table 1(a), the results shown in Table LABEL:tab:FedAvg demonstrate the worst case timing for aggregating 300 million parameters.",
            "Thus, if we add up the results seen in Table LABEL:tab:FedAvg and Figure 4, for each worker node, with the current system, for 5-10 epochs, the total time taken for the complete FedAvg algorithm to run takes less than 12 minutes.",
            "On the other hand, a malicious worker node can try to attack the system by introducing a backdoor /trojan. Assuming the detection technique work with high probability, the participants may be held accountable for an attack.\nSince all participants commit the SHA256 Hashes of their parametric updates to the public smart contract, we can easily detect incorrect disclosure of updates. If for any reason, the participant introduces a backdoor, then the federated learning algorithm can be re-simulated on the private chain using the updates as the test data using the appropriate off-chain detection technique.",
            "In order to address this aforementioned problem, we have proposed a general blockchain framework that integrates both public and private blockchains to achieve decentralization, immutability and transparency to discourage backdoor attacks on federated learning algorithms by holding the responsible parties accountable. We have shown that our framework facilitates the adaptation to any federated learning algorithm rendering a plug-and-play setting. We have presented the implementation of the Federated Averaging and signSGD algorithm\nover our general blockchain framework and based on our empirical results, conclude that our proposed approach maintains the communication-efficient nature of these algorithms."
        ]
    },
    "A1.T2": {
        "caption": "Table 2. Hyperparameters for small setting.",
        "table": "<table id=\"A1.T2.2.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T2.2.2.2\" class=\"ltx_tr\">\n<th id=\"A1.T2.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">R</th>\n<th id=\"A1.T2.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">K</th>\n<th id=\"A1.T2.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">F</th>\n<th id=\"A1.T2.2.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">C</th>\n<th id=\"A1.T2.2.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">E</th>\n<th id=\"A1.T2.2.2.2.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">B</th>\n<th id=\"A1.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"A1.T2.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\eta\" display=\"inline\"><semantics id=\"A1.T2.1.1.1.1.m1.1a\"><mi id=\"A1.T2.1.1.1.1.m1.1.1\" xref=\"A1.T2.1.1.1.1.m1.1.1.cmml\">η</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T2.1.1.1.1.m1.1b\"><ci id=\"A1.T2.1.1.1.1.m1.1.1.cmml\" xref=\"A1.T2.1.1.1.1.m1.1.1\">𝜂</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T2.1.1.1.1.m1.1c\">\\eta</annotation></semantics></math></th>\n<th id=\"A1.T2.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"A1.T2.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\kappa\" display=\"inline\"><semantics id=\"A1.T2.2.2.2.2.m1.1a\"><mi id=\"A1.T2.2.2.2.2.m1.1.1\" xref=\"A1.T2.2.2.2.2.m1.1.1.cmml\">κ</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T2.2.2.2.2.m1.1b\"><ci id=\"A1.T2.2.2.2.2.m1.1.1.cmml\" xref=\"A1.T2.2.2.2.2.m1.1.1\">𝜅</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T2.2.2.2.2.m1.1c\">\\kappa</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T2.2.2.3.1\" class=\"ltx_tr\">\n<td id=\"A1.T2.2.2.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">100</td>\n<td id=\"A1.T2.2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">10</td>\n<td id=\"A1.T2.2.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.1</td>\n<td id=\"A1.T2.2.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">1</td>\n<td id=\"A1.T2.2.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">2</td>\n<td id=\"A1.T2.2.2.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">256</td>\n<td id=\"A1.T2.2.2.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">1</td>\n<td id=\"A1.T2.2.2.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">1000</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We remind the notation we used for our experiments, and report the hyperparameters accordingly.",
                "R: Number of rounds",
                "K: Total number of agents",
                "F: Fraction of corrupt agents",
                "C: Fraction of selected agents for training in a round",
                "E: Number of epochs in local training",
                "B: Batch size of local training",
                "η",
                "𝜂",
                "\\eta",
                ": Server’s learning rate",
                "κ",
                "𝜅",
                "\\kappa",
                ": Number of parameter to compute ",
                "L",
                "2",
                "subscript",
                "𝐿",
                "2",
                "L_{2}",
                " norm for (cf. Section ",
                "3.6",
                ".)"
            ]
        ]
    },
    "A1.T3": {
        "caption": "Table 3. Hyperparameters for large setting.",
        "table": "<table id=\"A1.T3.2.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T3.2.2.2\" class=\"ltx_tr\">\n<th id=\"A1.T3.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">R</th>\n<th id=\"A1.T3.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">K</th>\n<th id=\"A1.T3.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">F</th>\n<th id=\"A1.T3.2.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">C</th>\n<th id=\"A1.T3.2.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">E</th>\n<th id=\"A1.T3.2.2.2.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">B</th>\n<th id=\"A1.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"A1.T3.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\eta\" display=\"inline\"><semantics id=\"A1.T3.1.1.1.1.m1.1a\"><mi id=\"A1.T3.1.1.1.1.m1.1.1\" xref=\"A1.T3.1.1.1.1.m1.1.1.cmml\">η</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T3.1.1.1.1.m1.1b\"><ci id=\"A1.T3.1.1.1.1.m1.1.1.cmml\" xref=\"A1.T3.1.1.1.1.m1.1.1\">𝜂</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T3.1.1.1.1.m1.1c\">\\eta</annotation></semantics></math></th>\n<th id=\"A1.T3.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"A1.T3.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\kappa\" display=\"inline\"><semantics id=\"A1.T3.2.2.2.2.m1.1a\"><mi id=\"A1.T3.2.2.2.2.m1.1.1\" xref=\"A1.T3.2.2.2.2.m1.1.1.cmml\">κ</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T3.2.2.2.2.m1.1b\"><ci id=\"A1.T3.2.2.2.2.m1.1.1.cmml\" xref=\"A1.T3.2.2.2.2.m1.1.1\">𝜅</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T3.2.2.2.2.m1.1c\">\\kappa</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T3.2.2.3.1\" class=\"ltx_tr\">\n<td id=\"A1.T3.2.2.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">200</td>\n<td id=\"A1.T3.2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">100</td>\n<td id=\"A1.T3.2.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.1</td>\n<td id=\"A1.T3.2.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.1</td>\n<td id=\"A1.T3.2.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">5</td>\n<td id=\"A1.T3.2.2.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">32</td>\n<td id=\"A1.T3.2.2.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">1</td>\n<td id=\"A1.T3.2.2.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">1000</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We remind the notation we used for our experiments, and report the hyperparameters accordingly.",
                "R: Number of rounds",
                "K: Total number of agents",
                "F: Fraction of corrupt agents",
                "C: Fraction of selected agents for training in a round",
                "E: Number of epochs in local training",
                "B: Batch size of local training",
                "η",
                "𝜂",
                "\\eta",
                ": Server’s learning rate",
                "κ",
                "𝜅",
                "\\kappa",
                ": Number of parameter to compute ",
                "L",
                "2",
                "subscript",
                "𝐿",
                "2",
                "L_{2}",
                " norm for (cf. Section ",
                "3.6",
                ".)"
            ]
        ]
    }
}