{
    "id_table_1": {
        "caption": "Table 1:    Sliding window perplexity of different context window extension models on PG19, Proof-pile, BookCorpus, Wikitext-103. All experiments are conducted on one 3090 24GB GPU. LongLLaMA-3B and MemLong-3B marked with    means evaluating without Memory, and LongLLaMA-3B marked with    means evaluating with infinite memory. We also evaluate MemLong with 4K/32K Memory scenarios. \"- / 6.95\" indicates that the model results in an Out of Memory (OOM) error on a single GPU, while on dual GPUs it yields the corresponding result.",
        "table": "S3.T1.15.15",
        "footnotes": [],
        "references": [
            "In this work, we propose MemLong, an efficient and lightweight method to extending the context window of LLMs.  The key idea is to store past contexts and knowledge in a non-trainable memory bank and further leverages these stored embeddings to retrieve chunk-level key-value ( K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V ) pairs for input into the model. .  MemLong is applicable to any decoder-only pretrained language models by incorporating (1) an additional  ret-mem  component for memory and retrieval, and (2) a  retrieval causal attention  module for integrating local and memory information.  The memory and retrieval process of MemLong is illustrated in Figure  1 (b). During generation,one text that exceeds the models maximum processing length is stored as context information in a Memory Bank. Subsequently, given a recently generated text chunk in a long document, we use the retriever to explicitly retrieve past information, obtaining additional context information through index alignment.",
            "The results are shown in Table  1 .  We employ Perplexity (PPL) as the evaluation metric for the language model. Lower PPL indicates stronger language modeling capabilities.  Compared to the two fully fine-tuned models, OpenLLaMA-3B and LLaMA-2-7B, our model demonstrates comparable performance across multiple datasets when test lengths are within their pre-trained limits (2048 for OpenLLaMA-3B and 4096 for LLaMA-2-7B). However, once the test lengths exceed these pre-trained limits, our model continues to reduce perplexity even beyond the fine-tuning length of 1024 and the pre-trained length of 2048, showcasing its superior generalizability. In contrast, the OpenLLaMA-3B and LLaMA-2-7B models fail to generalize to inputs beyond their pre-trained lengths and exhibit significantly increased memory overhead due to the quadratic complexity of attention.  We also compare our model with LongLoRA. Although the proposed Shifted Sparse Attention in LongLoRA significantly reduces memory usage, it also diminishes the models performance on short texts.  In contrast, LongLLaMA, which  K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V  pairs can also be stored, suffers from OOM issues when test lengths become excessively long due to its infinitely growing memory usage.  Positional encoding models have strong generalization capabilities. However, the performance of such methods can only guarantee that the generation performance over long distances does not degrade.   Compared to their methods, MemLong leverages an external retriever to handle longer input tokens and achieve better perplexity improvements.At the same time, because of the high storage efficiency, MemLong can effectively control the use of GPU to avoid OOM problems."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:    Accuracy [%] of 4-shot and 20-shot ICL on 5 NLU tasks (SST-2, MR, Subj, SST-5, MPQA). We compare MemLong with both the vanilla model (OpenLLaMA) and the memory-augmented model (LongLLaMA). Across a diverse range of experimental settings, our method consistently shows competitive performance.",
        "table": "S4.T2.5.5",
        "footnotes": [],
        "references": [
            "As shown in Figure  2 , the  Ret-Mem  module comprises a  Retriever  and a  Memory  component for information exchange.  Initially, we define the Memory component as  M M \\mathcal{M} caligraphic_M  and the Retriever as  R R \\mathcal{R} caligraphic_R , and their corresponding operations  M  (  ) M  \\mathcal{M}(\\cdot) caligraphic_M (  )  and  R  (  ) R  \\mathcal{R}(\\cdot) caligraphic_R (  ) . Furthermore, we specify the dimension of the model as  d m  o  d  e  l subscript d m o d e l d_{model} italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT  , the dimension of the retriever as  d r  e  t subscript d r e t d_{ret} italic_d start_POSTSUBSCRIPT italic_r italic_e italic_t end_POSTSUBSCRIPT .  The  Memory  module includes two segments:  K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V  pairs and corresponding Representation Embeddings. The dimension for both keys and values is represented as  R d m  o  d  e  l superscript R subscript d m o d e l \\mathbb{R}^{d_{model}} blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  and for Embeddings as  R d r  e  t superscript R subscript d r e t \\mathbb{R}^{d_{ret}} blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_r italic_e italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT . It is crucial to emphasize that the actual retrieval process involves the embeddings representing the chunks, not the  K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V  pairs.  The  Retriever  is essentially a pretrained dense embedder with excellent representation capabilities. MemLong use it to encode each chunk into Representation Embeddings. Since it produces a one-dimensional representation vector for one chunk, the memory footprint remains minimal even if the memory size is substantial.",
            "As illustrated in Figure  2 , each step involves an input of a chunk  c i subscript c i c_{i} italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , where the original text for that chunk is  t i subscript t i t_{i} italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .  In the  lower layers  where the model is frozen, the standard causal attention is applied to the entire  c i subscript c i c_{i} italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .  For the final layer of the  lower layers , we refer to it as the  memory layer .  Following each traversal of the  memory layer , two key operations are performed. The first operation is retrieval, depicted by the red line, where  t i subscript t i t_{i} italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is utilized to fetch the most pertinent  K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V  pairs. The second operation, indicated by the blue line, involves caching the acquired  K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V  pairs along with their associated chunk representation.  Within the models  upper layers , the retrieved  K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V  pairs are integrated with the current input context, subsequently tuning the model parameters to calibrate the retrieval reference.  Subsequent sections will explore the various facets of the MemLong framework and their intricacies, encompassing Retriever and Dynamic Memory Management (    3.2 ), Attention Reformulation (    3.3 ), and Inference with MemLong (    3.4 ).",
            "The memory process synchronously stores the  K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V  pairs from the  memory layer  and the  representation embedding  previous calculated for retrieval , ensuring that indices for  K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V  pairs correspond accurately to their representation embeddings (see Figure  2 , right, blue line). For every possible chunk memory  c m = c i superscript c m subscript c i c^{m}=c_{i} italic_c start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , and its corresponding text chunk  t m = t i superscript t m subscript t i t^{m}=t_{i} italic_t start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , we divide the memory process into two parts: the first part details how to cache the  K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V  pairs, and the second part explains how to store the corresponding representations. Firstly, we input  c m superscript c m c^{m} italic_c start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  into the MemLong and get the output from the  memory layer . It is worth noting that, since the lower layers are frozen during training, we can ensure that the distribution of the output  K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V  pairs is consistent. This consistency is crucial for avoiding the distribution shift issue, which was previously observed in models like MemTrm  Wu et al. ( 2022 ) . Our memory operation is highly efficient because it only involves storing the representations needed for retrieval,  r m = r q superscript r m superscript r q r^{m}=r^{q} italic_r start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_r start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT , thereby avoiding redundancy. After the retrieval for all chunk pairs is complete, the memory operationdenoted as  M  ( k , v ; r m ) M k v superscript r m \\mathcal{M}(k,v;r^{m}) caligraphic_M ( italic_k , italic_v ; italic_r start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) synchronously updates the memory with both the Key-Value pairs and their corresponding representations.",
            "Traditional in-context learning (ICL;  Brown et al.,  2020 ) inputs few-shot non-parameterized demonstration examples along with the query into the model. However, these methods are typically constrained by the models input length. In this experiment, since MemLong can store examples in a parameterized form within its memory, we primarily investigate whether MemLong can effectively utilize the knowledge stored in its memory to enhance its emergent abilities. The results are shown in Table  2 .  Compared to OpenLLaMA,which rely solely on non-parametric knowledge , given the same number of in-context demonstrations, MemLong can utilize additional demonstrations stored in its memory. The performance further increases or remains consistent with more demonstrations in the memory.  In our comparative analysis against LongLLaMA, it was observed that our model outperforms LongLLaMA across the majority of datasets under the same conditions of preserving In-Memory Demonstrations.  It is important to highlight that our model operates with significantly lower training parameters (200M  V. S.  0.3B) and fine-tuning data volume (0.5B  V. S.  5B) compared to LongLLaMA.  This underscores our models efficiency in leveraging an external retriever for information acquisition, demonstrating a superior ability to synthesize and utilize knowledge effectively with substantially fewer resources."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Different retrieval layers can affect MemLongs performance. MemLong marked with    means evaluating without Memory. The size of all methods using Memory is set to 32768. RA means retrieval across all upper layers; TA means training all params without freeze; RP means retrieval across fewer upper layers; RPL means retrieval acorss much fewer upper layers.",
        "table": "S5.T3.3.3",
        "footnotes": [],
        "references": [
            "As illustrated in Figure  2 , each step involves an input of a chunk  c i subscript c i c_{i} italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , where the original text for that chunk is  t i subscript t i t_{i} italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .  In the  lower layers  where the model is frozen, the standard causal attention is applied to the entire  c i subscript c i c_{i} italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .  For the final layer of the  lower layers , we refer to it as the  memory layer .  Following each traversal of the  memory layer , two key operations are performed. The first operation is retrieval, depicted by the red line, where  t i subscript t i t_{i} italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is utilized to fetch the most pertinent  K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V  pairs. The second operation, indicated by the blue line, involves caching the acquired  K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V  pairs along with their associated chunk representation.  Within the models  upper layers , the retrieved  K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V  pairs are integrated with the current input context, subsequently tuning the model parameters to calibrate the retrieval reference.  Subsequent sections will explore the various facets of the MemLong framework and their intricacies, encompassing Retriever and Dynamic Memory Management (    3.2 ), Attention Reformulation (    3.3 ), and Inference with MemLong (    3.4 ).",
            "In the trainable  upper layers  of the model, we revised the attentions to fuse with long-term memory.  As illustrated in Figure  3 , unlike the traditional Transformer decoder layers that utilize Multi-Head Attention  Vaswani et al. ( 2017 ) , we propose a  Retrieval Causal Attention  to extend it to a joint-attention mechanism and propose a long-term memory fusion process to enable each token to attend on both local contexts and chunk-level past contexts which have complete and continuous semantics. With the head-wise hidden state output from previous layer  H l  1  R | x |  d m  o  d  e  l superscript H l 1 superscript R x subscript d m o d e l H^{l-1}\\in\\mathbb{R}^{\\lvert x\\rvert\\times d_{model}} italic_H start_POSTSUPERSCRIPT italic_l - 1 end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT | italic_x |  italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  and the corresponding retrieved key-value pairs are  z ~ q = { K i ~ , V i ~ } i = 1   R k    d m  o  d  e  l superscript ~ z q superscript subscript ~ subscript K i ~ subscript V i i 1  superscript R k  subscript d m o d e l {\\tilde{z}}^{q}={\\{\\tilde{K_{i}},\\tilde{V_{i}}\\}}_{i=1}^{\\omega}\\in\\mathbb{R}^%  {k\\times\\tau\\times d_{model}} over~ start_ARG italic_z end_ARG start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT = { over~ start_ARG italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG , over~ start_ARG italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_k  italic_  italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , the output hidden state for the next layer  H l superscript H l H^{l} italic_H start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT  is computed as:",
            "As shown in Figure  4 , (the pink line) and Table  3  (RPL+TH), the model performs best when the number of retrieval layers is set to [13,17,21,25]. It is empirically believed that if retrieval information is introduced into all upper layers of the model, it leads to a decrease in the models attention to local context. Therefore, selecting retrieval layers at appropriate intervals can actually enhance the models capabilities."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  The specific parameters of different setting names.",
        "table": "A0.T4.3.3",
        "footnotes": [],
        "references": [
            "As illustrated in Figure  2 , each step involves an input of a chunk  c i subscript c i c_{i} italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , where the original text for that chunk is  t i subscript t i t_{i} italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .  In the  lower layers  where the model is frozen, the standard causal attention is applied to the entire  c i subscript c i c_{i} italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .  For the final layer of the  lower layers , we refer to it as the  memory layer .  Following each traversal of the  memory layer , two key operations are performed. The first operation is retrieval, depicted by the red line, where  t i subscript t i t_{i} italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is utilized to fetch the most pertinent  K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V  pairs. The second operation, indicated by the blue line, involves caching the acquired  K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V  pairs along with their associated chunk representation.  Within the models  upper layers , the retrieved  K  -  V K - V \\mathtt{K}\\mbox{-}\\mathtt{V} typewriter_K - typewriter_V  pairs are integrated with the current input context, subsequently tuning the model parameters to calibrate the retrieval reference.  Subsequent sections will explore the various facets of the MemLong framework and their intricacies, encompassing Retriever and Dynamic Memory Management (    3.2 ), Attention Reformulation (    3.3 ), and Inference with MemLong (    3.4 ).",
            "During the training phase, we explore the effects of varying retrieval layers on the model and examine whether the distribution shift problem, as discussed in MemTrm  Wu et al. ( 2022 ) , could be adequately resolved by our approach.  As mentioned before, Our method proposes a low-cost solution for distribution shifts. As shown in Figure  4 , the brown line (the line at the top of the picture; the training method is similar to MemTrm fine-tuning all parameters of the model and all layers after the  memory layer  are involved in the retrieval) is significantly worse than all other ours methods (even the most unreasonable settings) in terms of performance and fitting speed. We will analyze the performance of the reasoning stage later.",
            "As shown in Figure  4 , (the pink line) and Table  3  (RPL+TH), the model performs best when the number of retrieval layers is set to [13,17,21,25]. It is empirically believed that if retrieval information is introduced into all upper layers of the model, it leads to a decrease in the models attention to local context. Therefore, selecting retrieval layers at appropriate intervals can actually enhance the models capabilities.",
            "As shown in   4 , we list the variable values corresponding to different setting names in the ablation experiment."
        ]
    },
    "global_footnotes": [
        "Our code is available at"
    ]
}