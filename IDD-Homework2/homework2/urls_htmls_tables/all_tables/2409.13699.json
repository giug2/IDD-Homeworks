{
    "id_table_1": {
        "caption": "TABLE I:  Distribution of law articles and question Lengths in the Original Legal Document Dataset",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_2": {
        "caption": "TABLE II:  Search Results with Original Scores",
        "table": "S5.T4.1",
        "footnotes": [],
        "references": [
            "Overall, the goal behind this loop-like system is to add complexity to reordering of chunks and have them be more accurately relevant, by directly leveraging the output answer. In serving as a re-ranking approach, this can be an alternative to the traditional cross-encoder, which tends to be slower and biased, especially with the increased volume and diversity of data today. In Figure  2 , this process is shown using red arrows."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III:  Search Results with Normalized Scores",
        "table": "S5.T5.1",
        "footnotes": [],
        "references": [
            "Our process begins by optimizing the prompt to instruct the model to generate accurate answers based on relevant retrieved documents and the users native query. This is often necessary to counteract inadvertent typos and vagueness in the users questions, which might significantly impact the performance of keyword-based search, and improve clarity. Once the LLM generates a refined version of the query, we use this to search for the actual documents (or article chunks) relevant to the response, essentially verifying the sources used for reasoning. An example can be found in Figure  3 . It is to be noted that while the refined query can technically be passed directly to a LLM for an answer or to confirm the existence of a possible answer, general-purpose language models might not be trained on enough niche Vietnamese legal data to fully understand this, which might lead to situations where an answerable question is classified as the opposite. Therefore, we refrain on this practice."
        ]
    },
    "global_footnotes": []
}