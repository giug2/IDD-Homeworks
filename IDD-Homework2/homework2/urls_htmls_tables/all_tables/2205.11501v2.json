{
    "S5.T1": {
        "caption": "Table 1: Accuracy scores for VCR test set. VQA-GNN outperforms SGEITL+VLBERT model on Q‚Üí‚Üí\\rightarrowAR metric by 3.2%, and achieves competitive accuracy with SOTA methods, which have a close number of parameters but SOTA methods require a large amount of image caption data in pre-training process (over 13x larger than our model), e.g., ‚ÄùUNITER-L‚Äù, ‚ÄùERNIE-ViL-B‚Äù, ‚ÄùRESERVE-B‚Äù. Moreover, ‚ÄùRESERVE-L+VQA-GNN‚Äù outperforms RESERVE-L by 2.3% on Q‚Üí‚Üí\\rightarrowAR metric.",
        "table": "<table id=\"S5.T1.4.4\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.4.4.5.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.4.5.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T1.4.4.5.1.1.1\" class=\"ltx_text\">Model</span></th>\n<td id=\"S5.T1.4.4.5.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"># Image-caption</td>\n<td id=\"S5.T1.4.4.5.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T1.4.4.5.1.3.1\" class=\"ltx_text\">Parameters</span></td>\n<td id=\"S5.T1.4.4.5.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T1.4.4.5.1.4.1\" class=\"ltx_text\">Structured knowledge</span></td>\n<td id=\"S5.T1.4.4.5.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">Test Acc.(%)</td>\n</tr>\n<tr id=\"S5.T1.3.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.3.3.4\" class=\"ltx_td ltx_align_center\">in pretraining</td>\n<td id=\"S5.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Q<math id=\"S5.T1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"inline\"><semantics id=\"S5.T1.1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S5.T1.1.1.1.1.m1.1.1\" xref=\"S5.T1.1.1.1.1.m1.1.1.cmml\">‚Üí</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.1.1.1.1.m1.1b\"><ci id=\"S5.T1.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T1.1.1.1.1.m1.1.1\">‚Üí</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.1.1.1.1.m1.1c\">\\rightarrow</annotation></semantics></math>A</td>\n<td id=\"S5.T1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">QA<math id=\"S5.T1.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"inline\"><semantics id=\"S5.T1.2.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S5.T1.2.2.2.2.m1.1.1\" xref=\"S5.T1.2.2.2.2.m1.1.1.cmml\">‚Üí</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.2.2.2.2.m1.1b\"><ci id=\"S5.T1.2.2.2.2.m1.1.1.cmml\" xref=\"S5.T1.2.2.2.2.m1.1.1\">‚Üí</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.2.2.2.2.m1.1c\">\\rightarrow</annotation></semantics></math>R</td>\n<td id=\"S5.T1.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">Q<math id=\"S5.T1.3.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"inline\"><semantics id=\"S5.T1.3.3.3.3.m1.1a\"><mo stretchy=\"false\" id=\"S5.T1.3.3.3.3.m1.1.1\" xref=\"S5.T1.3.3.3.3.m1.1.1.cmml\">‚Üí</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.3.3.3.3.m1.1b\"><ci id=\"S5.T1.3.3.3.3.m1.1.1.cmml\" xref=\"S5.T1.3.3.3.3.m1.1.1\">‚Üí</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.3.3.3.3.m1.1c\">\\rightarrow</annotation></semantics></math>AR</td>\n</tr>\n<tr id=\"S5.T1.4.4.6.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.4.6.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">ViLBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a>]</cite>\n</th>\n<td id=\"S5.T1.4.4.6.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">3.3M</td>\n<td id=\"S5.T1.4.4.6.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T1.4.4.6.2.3.1\" class=\"ltx_text ltx_font_bold\">221M</span></td>\n<td id=\"S5.T1.4.4.6.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">No</td>\n<td id=\"S5.T1.4.4.6.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">73.3</td>\n<td id=\"S5.T1.4.4.6.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">74.6</td>\n<td id=\"S5.T1.4.4.6.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">54.8</td>\n</tr>\n<tr id=\"S5.T1.4.4.7.3\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.4.7.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">VLBERT-L <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">37</a>]</cite>\n</th>\n<td id=\"S5.T1.4.4.7.3.2\" class=\"ltx_td ltx_align_center\">3.3M</td>\n<td id=\"S5.T1.4.4.7.3.3\" class=\"ltx_td ltx_align_center\">383M</td>\n<td id=\"S5.T1.4.4.7.3.4\" class=\"ltx_td ltx_align_center\">No</td>\n<td id=\"S5.T1.4.4.7.3.5\" class=\"ltx_td ltx_align_center\">75.8</td>\n<td id=\"S5.T1.4.4.7.3.6\" class=\"ltx_td ltx_align_center\">78.4</td>\n<td id=\"S5.T1.4.4.7.3.7\" class=\"ltx_td ltx_align_center\">59.7</td>\n</tr>\n<tr id=\"S5.T1.4.4.4\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.4.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SGEITL+VLBERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">42</a>]</cite>\n</th>\n<td id=\"S5.T1.4.4.4.3\" class=\"ltx_td ltx_align_center\">290k</td>\n<td id=\"S5.T1.4.4.4.1\" class=\"ltx_td ltx_align_center\">\n<math id=\"S5.T1.4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\geq\" display=\"inline\"><semantics id=\"S5.T1.4.4.4.1.m1.1a\"><mo id=\"S5.T1.4.4.4.1.m1.1.1\" xref=\"S5.T1.4.4.4.1.m1.1.1.cmml\">‚â•</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.4.4.4.1.m1.1b\"><geq id=\"S5.T1.4.4.4.1.m1.1.1.cmml\" xref=\"S5.T1.4.4.4.1.m1.1.1\"></geq></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.4.4.4.1.m1.1c\">\\geq</annotation></semantics></math> 383M</td>\n<td id=\"S5.T1.4.4.4.4\" class=\"ltx_td ltx_align_center\">Yes</td>\n<td id=\"S5.T1.4.4.4.5\" class=\"ltx_td ltx_align_center\">76.0</td>\n<td id=\"S5.T1.4.4.4.6\" class=\"ltx_td ltx_align_center\">78.0</td>\n<td id=\"S5.T1.4.4.4.7\" class=\"ltx_td ltx_align_center\">59.6</td>\n</tr>\n<tr id=\"S5.T1.4.4.8.4\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.4.8.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">UNITER-(B/L)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">6</a>]</cite>\n</th>\n<td id=\"S5.T1.4.4.8.4.2\" class=\"ltx_td ltx_align_center\">9.5M</td>\n<td id=\"S5.T1.4.4.8.4.3\" class=\"ltx_td ltx_align_center\">154M/378M</td>\n<td id=\"S5.T1.4.4.8.4.4\" class=\"ltx_td ltx_align_center\">No</td>\n<td id=\"S5.T1.4.4.8.4.5\" class=\"ltx_td ltx_align_center\">75.0/77.3</td>\n<td id=\"S5.T1.4.4.8.4.6\" class=\"ltx_td ltx_align_center\">77.2/80.8</td>\n<td id=\"S5.T1.4.4.8.4.7\" class=\"ltx_td ltx_align_center\">58.2/62.8</td>\n</tr>\n<tr id=\"S5.T1.4.4.9.5\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.4.9.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ERNIE-ViL-(B/L) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">49</a>]</cite>\n</th>\n<td id=\"S5.T1.4.4.9.5.2\" class=\"ltx_td ltx_align_center\">3.8M</td>\n<td id=\"S5.T1.4.4.9.5.3\" class=\"ltx_td ltx_align_center\">212M/533M</td>\n<td id=\"S5.T1.4.4.9.5.4\" class=\"ltx_td ltx_align_center\">No</td>\n<td id=\"S5.T1.4.4.9.5.5\" class=\"ltx_td ltx_align_center\">77.0/79.2</td>\n<td id=\"S5.T1.4.4.9.5.6\" class=\"ltx_td ltx_align_center\">80.3/83.5</td>\n<td id=\"S5.T1.4.4.9.5.7\" class=\"ltx_td ltx_align_center\">62.1/66.3</td>\n</tr>\n<tr id=\"S5.T1.4.4.10.6\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.4.10.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"S5.T1.4.4.10.6.1.1\" class=\"ltx_text ltx_font_bold\">VQA-GNN (Ours)</span></th>\n<td id=\"S5.T1.4.4.10.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T1.4.4.10.6.2.1\" class=\"ltx_text ltx_font_bold\">290k</span></td>\n<td id=\"S5.T1.4.4.10.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\">372M</td>\n<td id=\"S5.T1.4.4.10.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\">Yes</td>\n<td id=\"S5.T1.4.4.10.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\">77.9</td>\n<td id=\"S5.T1.4.4.10.6.6\" class=\"ltx_td ltx_align_center ltx_border_t\">80.0</td>\n<td id=\"S5.T1.4.4.10.6.7\" class=\"ltx_td ltx_align_center ltx_border_t\">62.8</td>\n</tr>\n<tr id=\"S5.T1.4.4.11.7\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.4.11.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MERLOT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">52</a>]</cite>\n</th>\n<td id=\"S5.T1.4.4.11.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\">180M</td>\n<td id=\"S5.T1.4.4.11.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\">223M</td>\n<td id=\"S5.T1.4.4.11.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\">No</td>\n<td id=\"S5.T1.4.4.11.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\">80.6</td>\n<td id=\"S5.T1.4.4.11.7.6\" class=\"ltx_td ltx_align_center ltx_border_t\">80.4</td>\n<td id=\"S5.T1.4.4.11.7.7\" class=\"ltx_td ltx_align_center ltx_border_t\">65.1</td>\n</tr>\n<tr id=\"S5.T1.4.4.12.8\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.4.12.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">RESERVE-(B/L) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">51</a>]</cite>\n</th>\n<td id=\"S5.T1.4.4.12.8.2\" class=\"ltx_td ltx_align_center\">1B</td>\n<td id=\"S5.T1.4.4.12.8.3\" class=\"ltx_td ltx_align_center\">200M/644M</td>\n<td id=\"S5.T1.4.4.12.8.4\" class=\"ltx_td ltx_align_center\">No</td>\n<td id=\"S5.T1.4.4.12.8.5\" class=\"ltx_td ltx_align_center\">79.3/84.0</td>\n<td id=\"S5.T1.4.4.12.8.6\" class=\"ltx_td ltx_align_center\">78.7/84.9</td>\n<td id=\"S5.T1.4.4.12.8.7\" class=\"ltx_td ltx_align_center\">62.6/72.0</td>\n</tr>\n<tr id=\"S5.T1.4.4.13.9\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.4.13.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">RESERVE-L + <span id=\"S5.T1.4.4.13.9.1.1\" class=\"ltx_text ltx_font_bold\">VQA-GNN (Ours)</span>\n</th>\n<td id=\"S5.T1.4.4.13.9.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">1B</td>\n<td id=\"S5.T1.4.4.13.9.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">1B</td>\n<td id=\"S5.T1.4.4.13.9.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">Yes</td>\n<td id=\"S5.T1.4.4.13.9.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T1.4.4.13.9.5.1\" class=\"ltx_text ltx_font_bold\">85.3</span></td>\n<td id=\"S5.T1.4.4.13.9.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T1.4.4.13.9.6.1\" class=\"ltx_text ltx_font_bold\">86.9</span></td>\n<td id=\"S5.T1.4.4.13.9.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T1.4.4.13.9.7.1\" class=\"ltx_text ltx_font_bold\">74.3</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Comparison with state-of-the-art methods.\nWe compared VQA-GNN with state-of-the-art methods on the VCR test set in Table 1. Compared with the unidirectional fusion method SGEITL+VLBERT that can boost multimodal transformer model VLBERT by incorporating visual scene graphs, VQA-GNN is a multimodal GNN-based bidirectional fusion method built on the multimodal semantic graph. Both were not pretrained on the large-scale dataset.\nVQA-GNN improves SGEITL+VLBERT on the Q‚Üí‚Üí\\rightarrowAR metric by 3.2%, and further reduces over 11M training parameters.\nWe think that the structured multimodal semantic graph provides much more commonsense knowledge related to QA and original image than SGEITL, and the multimodal GNN-based bidirectional fusion method works much better on unifying unstructured and structured multimodal knowledge than multimodal transformer models. Moreover, since we retrieve commonsense knowledge from structured multimodal semantic graphs directly, VQA-GNN is a cost-effective approach compared to multimodal transformer models that consume much GPU resources to learn commonsense knowledge with large parameters.",
            "We also demonstrate the effectiveness of VQA-GNN by comparing it with state-of-the-art multimodal transformer models that were pretrained across text and images and were finetuned on the VCR dataset. As shown in Table 1, the larger image caption data and parameters, the higher performance the model can achieve. In contrast, VQA-GNN trained with VCR dataset with 290‚ÄãK290ùêæ290K image-caption pairs performs similarly to UNITER-L that requires over 32x larger image-caption data than us in pretraining process. These results suggest that VQA-GNN obtaining structured context knowledge inferred from image-level and concept-level knowledge sources is as effective as the pretraining process for previous methods.\nMoreover, VQA-GNN can further enhance RESERVE-L performance on both Q‚Üí‚Üí\\rightarrowA and QA‚Üí‚Üí\\rightarrowR, and finally improves the score by 2.3% on Q‚Üí‚Üí\\rightarrowAR metric. As correcting some questions requires the model to understand commonsense knowledge related to image context and have good reasoning ability, it is difficult for multimodal transformer methods including RESERVE-L.\nOn the other hand, VQA-GNN not only structures a joint semantic graph to provide commonsense knowledge related to image context but also has a good reasoning ability thanks to its multimodal GNN module.\nAdditionally, in the supplementary material, we detail the results compared to baselines pretrained only with the VCR dataset, as well as the evaluation of different question types."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: All modules in the multimodal semantic graph help boost the final performance. Here, ‚Äúscene-graph‚Äù includes node z and node p, ‚Äúconcept-graph‚Äù includes node z.",
        "table": "<table id=\"S5.T2.3.3\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Model</th>\n<th id=\"S5.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Val Acc.(%) (Q<math id=\"S5.T2.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"inline\"><semantics id=\"S5.T2.1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S5.T2.1.1.1.1.m1.1.1\" xref=\"S5.T2.1.1.1.1.m1.1.1.cmml\">‚Üí</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.1.1.1.1.m1.1b\"><ci id=\"S5.T2.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T2.1.1.1.1.m1.1.1\">‚Üí</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.1.1.1.1.m1.1c\">\\rightarrow</annotation></semantics></math>A)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.3.3.4.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.3.3.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Node p (Vinvl)</th>\n<td id=\"S5.T2.3.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">43.5</td>\n</tr>\n<tr id=\"S5.T2.3.3.5.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.3.3.5.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Node z (RoBERTa-L)</th>\n<td id=\"S5.T2.3.3.5.2.2\" class=\"ltx_td ltx_align_center\">53.8</td>\n</tr>\n<tr id=\"S5.T2.3.3.6.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.3.3.6.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">concept-graph</th>\n<td id=\"S5.T2.3.3.6.3.2\" class=\"ltx_td ltx_align_center\">69.0</td>\n</tr>\n<tr id=\"S5.T2.3.3.7.4\" class=\"ltx_tr\">\n<th id=\"S5.T2.3.3.7.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">scene-graph</th>\n<td id=\"S5.T2.3.3.7.4.2\" class=\"ltx_td ltx_align_center\">73.7</td>\n</tr>\n<tr id=\"S5.T2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">concept-graph + scene-graph (w/o node <math id=\"S5.T2.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"p\" display=\"inline\"><semantics id=\"S5.T2.2.2.2.1.m1.1a\"><mi id=\"S5.T2.2.2.2.1.m1.1.1\" xref=\"S5.T2.2.2.2.1.m1.1.1.cmml\">p</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.2.2.2.1.m1.1b\"><ci id=\"S5.T2.2.2.2.1.m1.1.1.cmml\" xref=\"S5.T2.2.2.2.1.m1.1.1\">ùëù</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.2.2.2.1.m1.1c\">p</annotation></semantics></math>)</th>\n<td id=\"S5.T2.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">75.1</td>\n</tr>\n<tr id=\"S5.T2.3.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">concept-graph + scene-graph (w/ node <math id=\"S5.T2.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"p\" display=\"inline\"><semantics id=\"S5.T2.3.3.3.1.m1.1a\"><mi id=\"S5.T2.3.3.3.1.m1.1.1\" xref=\"S5.T2.3.3.3.1.m1.1.1.cmml\">p</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.3.3.3.1.m1.1b\"><ci id=\"S5.T2.3.3.3.1.m1.1.1.cmml\" xref=\"S5.T2.3.3.3.1.m1.1.1\">ùëù</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.3.3.3.1.m1.1c\">p</annotation></semantics></math>)</th>\n<td id=\"S5.T2.3.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T2.3.3.3.2.1\" class=\"ltx_text ltx_font_bold\">77.1</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Effectiveness of the multimodal semantic graph.\nTo further study the behavior of modules in the multimodal semantic graph, and quantitatively evaluate pretrained models used in this work (e.g., RoBERTa-L, scene-graph[scene graph generator], concept-graph[conceptNet KG]), we report the performance of using different node representations in Table 2.\nWe respectively build classification models by applying Node p and Node z to get their validation accuracy on Q‚Üí‚Üí\\rightarrowA subtask. The scene-graph structured by connecting Node p and Node z with extracted visual scene graph improves over 25%percent2525\\% on average of these two nodes. In terms of concept-graph, it is structured by connecting Node z with retrieved conceptual triplets from ConcepNet KG, improving Node z‚Äôs performance by 15.2%percent15.215.2\\%. We further compare VQA-GNN on ‚Äúscene-graph + concept-graph‚Äù w/ and w/o Node pùëùp, and the result shows that including Node p can further improve the performance by 2%percent22\\%. We believe that the Node p representing global visual knowledge associated with the correct answer is able to pass visual commonsense knowledge to the multimodal semantic graph, and it is effective besides employing ConcepNet KG to obtain textual commonsense knowledge [48]."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Ablation 1 and Ablation 2 indicate a single GNN on the multimodal semantic graph w/o and w/ direct cross-modal edges, respectively (Figure 4). VQA-GNN with two modality-specialized GNNs on the multimodal semantic graph achieves the best score.",
        "table": "<table id=\"S5.T3.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Model</th>\n<th id=\"S5.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Val Acc.(%) (Q<math id=\"S5.T3.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"inline\"><semantics id=\"S5.T3.1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S5.T3.1.1.1.1.m1.1.1\" xref=\"S5.T3.1.1.1.1.m1.1.1.cmml\">‚Üí</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.1.1.1.1.m1.1b\"><ci id=\"S5.T3.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1\">‚Üí</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.1.1.1.1.m1.1c\">\\rightarrow</annotation></semantics></math>A)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Ablation 1 (single GNN)</th>\n<td id=\"S5.T3.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">73.0</td>\n</tr>\n<tr id=\"S5.T3.1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Ablation 2 (single GNN w/ cross-modal edges)</th>\n<td id=\"S5.T3.1.1.3.2.2\" class=\"ltx_td ltx_align_center\">70.6</td>\n</tr>\n<tr id=\"S5.T3.1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">VQA-GNN (two modality-specialized GNNs)</th>\n<td id=\"S5.T3.1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S5.T3.1.1.4.3.2.1\" class=\"ltx_text ltx_font_bold\">75.1</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Analysis of the multimodal GNN method. \nTo analyze the effect of the multimodal GNN method on mitigating the multimodal gap in performing inter-modal message passing, we compared the final VQA-GNN with two single GNNs built on multimodal semantic graphs with and without direct cross-modal edges in Figure 4. As the results of VCR validation set shown in Table 3, the final VQA-GNN built with the multimodal GNN on the multimodal semantic graph improves the accuracy of both ablative architecture by over 2%. We believe that the multimodal GNN built by two modality-specific GNNs can effectively avoid directly aggregating nodes from scene-graph and concept-graph to alleviate the modality gap. As a result, the inter-modal message passing can be improved. We further explored the aggregation process for some node samples to demonstrate why the two ablation architectures fail to alleviate the modality gap. Here, mùí©‚Äã(u)(k)subscriptsuperscriptùëöùëòùí©ùë¢m^{(k)}_{\\mathcal{N}(u)} represents the aggregated messages from all neighbors of node uùë¢u at the kùëòk-th iteration."
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Accuracy scores on the GQA validation set. All models are trained under the realistic setup of not using the annotated semantic functional programs.",
        "table": "<table id=\"S5.T4.2.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Model</th>\n<th id=\"S5.T4.2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Visual SG</th>\n<th id=\"S5.T4.2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Textual SG</th>\n<th id=\"S5.T4.2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Val Acc.(%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">SGEITL<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">42</a>]</cite>\n</th>\n<td id=\"S5.T4.2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">‚úì</td>\n<td id=\"S5.T4.2.1.2.1.3\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S5.T4.2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">53.3</td>\n</tr>\n<tr id=\"S5.T4.2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CFR<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">29</a>]</cite>\n</th>\n<td id=\"S5.T4.2.1.3.2.2\" class=\"ltx_td ltx_align_center\">‚úì</td>\n<td id=\"S5.T4.2.1.3.2.3\" class=\"ltx_td ltx_align_center\">‚úì</td>\n<td id=\"S5.T4.2.1.3.2.4\" class=\"ltx_td ltx_align_center\">73.6</td>\n</tr>\n<tr id=\"S5.T4.2.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T4.2.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GCN<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a>]</cite>\n</th>\n<td id=\"S5.T4.2.1.4.3.2\" class=\"ltx_td\"></td>\n<td id=\"S5.T4.2.1.4.3.3\" class=\"ltx_td ltx_align_center\">‚úì</td>\n<td id=\"S5.T4.2.1.4.3.4\" class=\"ltx_td ltx_align_center\">85.7</td>\n</tr>\n<tr id=\"S5.T4.2.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T4.2.1.5.4.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"></th>\n<td id=\"S5.T4.2.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">‚úì</td>\n<td id=\"S5.T4.2.1.5.4.3\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S5.T4.2.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">58.9</td>\n</tr>\n<tr id=\"S5.T4.2.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T4.2.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">VQA-GNN</th>\n<td id=\"S5.T4.2.1.6.5.2\" class=\"ltx_td\"></td>\n<td id=\"S5.T4.2.1.6.5.3\" class=\"ltx_td ltx_align_center\">‚úì</td>\n<td id=\"S5.T4.2.1.6.5.4\" class=\"ltx_td ltx_align_center\">87.9</td>\n</tr>\n<tr id=\"S5.T4.2.1.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T4.2.1.7.6.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\"></th>\n<td id=\"S5.T4.2.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">‚úì</td>\n<td id=\"S5.T4.2.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">‚úì</td>\n<td id=\"S5.T4.2.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T4.2.1.7.6.4.1\" class=\"ltx_text ltx_font_bold\">90.3</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Comparison with baselines. We also compared VQA-GNN with baseline models on GQA dataset, under the realistic setup of not using the annotated semantic functional programs (see ¬ß5.1). As the results shown in Table 4, our model achieves validation accuracy of 58.9% for visual SG and 87.9% for textual SG. Compared with SGEITL [42] and GCN [21] which are unidirectional fusion methods, our method performs bidirectional fusion to unify unstructured and structured knowledge, and improved the reasoning ability of SGEITL by 5.6% and GCN by 2.2%. Moreover, by inter-connecting the visual and textual SG, our method achieves validation accuracy of 90.3% and further suggests its efficacy in performing inter-modal message passing."
        ]
    },
    "S5.T5": {
        "caption": "Table 5: Ablation results on the effect of our proposed bidirectional fusion for GQA.",
        "table": "<table id=\"S5.T5.5.5\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T5.2.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T5.2.2.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Method</th>\n<th id=\"S5.T5.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Val Acc.(%) <math id=\"S5.T5.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S5.T5.1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S5.T5.1.1.1.1.m1.1.1\" xref=\"S5.T5.1.1.1.1.m1.1.1.cmml\">‚Üë</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.1.1.1.1.m1.1b\"><ci id=\"S5.T5.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T5.1.1.1.1.m1.1.1\">‚Üë</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.1.1.1.1.m1.1c\">\\uparrow</annotation></semantics></math>\n</th>\n<th id=\"S5.T5.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Inference time (ms) <math id=\"S5.T5.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S5.T5.2.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S5.T5.2.2.2.2.m1.1.1\" xref=\"S5.T5.2.2.2.2.m1.1.1.cmml\">‚Üì</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.2.2.2.2.m1.1b\"><ci id=\"S5.T5.2.2.2.2.m1.1.1.cmml\" xref=\"S5.T5.2.2.2.2.m1.1.1\">‚Üì</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.2.2.2.2.m1.1c\">\\downarrow</annotation></semantics></math>\n</th>\n<td id=\"S5.T5.2.2.2.4\" class=\"ltx_td ltx_border_tt\"></td>\n</tr>\n<tr id=\"S5.T5.3.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.3.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Average pooling</th>\n<td id=\"S5.T5.3.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\">62.3 (<math id=\"S5.T5.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm 0.40\" display=\"inline\"><semantics id=\"S5.T5.3.3.3.1.m1.1a\"><mrow id=\"S5.T5.3.3.3.1.m1.1.1\" xref=\"S5.T5.3.3.3.1.m1.1.1.cmml\"><mo id=\"S5.T5.3.3.3.1.m1.1.1a\" xref=\"S5.T5.3.3.3.1.m1.1.1.cmml\">¬±</mo><mn id=\"S5.T5.3.3.3.1.m1.1.1.2\" xref=\"S5.T5.3.3.3.1.m1.1.1.2.cmml\">0.40</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.3.3.3.1.m1.1b\"><apply id=\"S5.T5.3.3.3.1.m1.1.1.cmml\" xref=\"S5.T5.3.3.3.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S5.T5.3.3.3.1.m1.1.1.1.cmml\" xref=\"S5.T5.3.3.3.1.m1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S5.T5.3.3.3.1.m1.1.1.2.cmml\" xref=\"S5.T5.3.3.3.1.m1.1.1.2\">0.40</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.3.3.3.1.m1.1c\">\\pm 0.40</annotation></semantics></math>)</td>\n<td id=\"S5.T5.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.3.3.3.3.1\" class=\"ltx_text ltx_font_bold\">5.2</span></td>\n<td id=\"S5.T5.3.3.3.4\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr id=\"S5.T5.4.4.4\" class=\"ltx_tr\">\n<th id=\"S5.T5.4.4.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Unidirectional fusion</th>\n<td id=\"S5.T5.4.4.4.1\" class=\"ltx_td ltx_align_center\">86.3 (<math id=\"S5.T5.4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm 0.01\" display=\"inline\"><semantics id=\"S5.T5.4.4.4.1.m1.1a\"><mrow id=\"S5.T5.4.4.4.1.m1.1.1\" xref=\"S5.T5.4.4.4.1.m1.1.1.cmml\"><mo id=\"S5.T5.4.4.4.1.m1.1.1a\" xref=\"S5.T5.4.4.4.1.m1.1.1.cmml\">¬±</mo><mn id=\"S5.T5.4.4.4.1.m1.1.1.2\" xref=\"S5.T5.4.4.4.1.m1.1.1.2.cmml\">0.01</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.4.4.4.1.m1.1b\"><apply id=\"S5.T5.4.4.4.1.m1.1.1.cmml\" xref=\"S5.T5.4.4.4.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S5.T5.4.4.4.1.m1.1.1.1.cmml\" xref=\"S5.T5.4.4.4.1.m1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S5.T5.4.4.4.1.m1.1.1.2.cmml\" xref=\"S5.T5.4.4.4.1.m1.1.1.2\">0.01</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.4.4.4.1.m1.1c\">\\pm 0.01</annotation></semantics></math>)</td>\n<td id=\"S5.T5.4.4.4.3\" class=\"ltx_td ltx_align_center\">8.6</td>\n<td id=\"S5.T5.4.4.4.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S5.T5.5.5.5\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.5.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Bidirectional fusion <span id=\"S5.T5.5.5.5.2.1\" class=\"ltx_text ltx_font_bold\">(ours)</span>\n</th>\n<td id=\"S5.T5.5.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S5.T5.5.5.5.1.1\" class=\"ltx_text ltx_font_bold\">90.3</span> (<math id=\"S5.T5.5.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm 0.03\" display=\"inline\"><semantics id=\"S5.T5.5.5.5.1.m1.1a\"><mrow id=\"S5.T5.5.5.5.1.m1.1.1\" xref=\"S5.T5.5.5.5.1.m1.1.1.cmml\"><mo id=\"S5.T5.5.5.5.1.m1.1.1a\" xref=\"S5.T5.5.5.5.1.m1.1.1.cmml\">¬±</mo><mn id=\"S5.T5.5.5.5.1.m1.1.1.2\" xref=\"S5.T5.5.5.5.1.m1.1.1.2.cmml\">0.03</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.5.5.5.1.m1.1b\"><apply id=\"S5.T5.5.5.5.1.m1.1.1.cmml\" xref=\"S5.T5.5.5.5.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S5.T5.5.5.5.1.m1.1.1.1.cmml\" xref=\"S5.T5.5.5.5.1.m1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S5.T5.5.5.5.1.m1.1.1.2.cmml\" xref=\"S5.T5.5.5.5.1.m1.1.1.2\">0.03</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.5.5.5.1.m1.1c\">\\pm 0.03</annotation></semantics></math>)</td>\n<td id=\"S5.T5.5.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">5.5</td>\n<td id=\"S5.T5.5.5.5.4\" class=\"ltx_td ltx_border_bb\"></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Ablation study on the bidirectional fusion.\nTo fairly study the effect of bidirectional fusion for improving concept-level reasoning, we evaluated the performance of VQA-GNN with and without structured multimodal knowledge-enhanced question representations. We show their difference in Figure 5, compared with the unidirectional fusion, the bidirectional fusion approach is able to utilize the message aggregated from scene-graph and concept-graph in node zùëßz to predict the correct answer. It facilitates the joint reasoning ability of VQA-GNN in capturing bidirectional interactions between unstructured node zùëßz and structured multimodal semantic graph. As a result in Table 5, the bidirectional fusion approach further improved the performance of the unidirectional fusion approach by 4%. We also compared our approach with an average pooling method that simply averages all node representations. We indeed find that this ablation performs significantly worse than others, which suggests that our approach can capture special relationship information between different nodes but average pooling cannot."
        ]
    }
}