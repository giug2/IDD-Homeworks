{
    "id_table_1": {
        "caption": "Table 1 :  Test classification accuracy on CV-MNIST  ( M = 500 ) M 500 (M=500) ( italic_M = 500 )  and CV-CIFAR-10  ( M = 50 , 000 ) M 50 000 (M=50{,}000) ( italic_M = 50 , 000 ) .",
        "table": "A3.EGx1",
        "footnotes": [],
        "references": [
            "Reflecting on the challenges and benefits of both RVNNs and CVNNs, we target a framework that leverages the simplicity in training offered by RVNNs while offering improved generalization in the processing of complex signals. In this context, multi-view representation fusion emerges as a potential framework, proposing that different perspectives  or views  of data can provide complementary information, thereby enhancing learning and generalization  (Sun,  2013 ; Xu et al.,  2013 ; Lahat et al.,  2015 ; Zhao et al.,  2017 ; Yan et al.,  2021 ) . Deriving from this principle, we introduce the  Steinmetz Neural Network  architecture, which is designed to process the real ( X R m superscript subscript X R m X_{R}^{m} italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) and the imaginary ( X I m superscript subscript X I m X_{I}^{m} italic_X start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) parts of complex signal representations as separate views before joint processing. We formalize how this architecture leverages the complementarity principle of multi-view learning in Section  3.1 .",
            "In the context of neural networks information-theoretic foundations,  Tishby and Zaslavsky ( 2015 )  used the Data Processing Inequality to frame the architecture shown in Fig.  1 . This architecture aligns with the classical RVNN architecture from CVNN literature  (Trabelsi et al.,  2018 ) . Here,    (  )   \\xi(\\cdot) italic_ (  )  is the true function,  h  (   (  ) ) h   h(\\psi(\\cdot)) italic_h ( italic_ (  ) )  is the neural network, and  Y ^ m = h  (   ( X m ) ) superscript ^ Y m h  superscript X m \\smash{\\hat{Y}}^{m}=h(\\psi(X^{m})) over^ start_ARG italic_Y end_ARG start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_h ( italic_ ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) )  are the predictions  accordingly, we have that  I  ( Y m ; X m )  I  ( Y m ; Z m )  I  ( Y m ; Y ^ m ) I superscript Y m superscript X m I superscript Y m superscript Z m I superscript Y m superscript ^ Y m I(Y^{m};X^{m})\\geq I(Y^{m};Z^{m})\\geq I(Y^{m};\\smash{\\hat{Y}}^{m}) italic_I ( italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  italic_I ( italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  italic_I ( italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; over^ start_ARG italic_Y end_ARG start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) . Regarding practical implementation, the input space,  [ [ X R m ] T , [ X I m ] T ] T  R 2  d  N superscript superscript delimited-[] superscript subscript X R m T superscript delimited-[] superscript subscript X I m T T superscript R 2 d N \\smash{[[X_{R}^{m}]^{T},[X_{I}^{m}]^{T}]^{T}\\in\\mathbb{R}^{2dN}} [ [ italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , [ italic_X start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT 2 italic_d italic_N end_POSTSUPERSCRIPT , is jointly processed by     (  ) superscript   \\psi^{*}(\\cdot) italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT (  ) , using individual channels for  X R m superscript subscript X R m X_{R}^{m} italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  X I m superscript subscript X I m X_{I}^{m} italic_X start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  to form the latent space,  [ [ Z R m ] T , [ Z I m ] T ] T  R 2  l  N superscript superscript delimited-[] superscript subscript Z R m T superscript delimited-[] superscript subscript Z I m T T superscript R 2 l N \\smash{[[Z_{R}^{m}]^{T},[Z_{I}^{m}]^{T}]^{T}\\in\\mathbb{R}^{2lN}} [ [ italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , [ italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT 2 italic_l italic_N end_POSTSUPERSCRIPT , which is then jointly processed by  h   (  ) superscript h  h^{*}(\\cdot) italic_h start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT (  ) , using individual channels for  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , to obtain the output space,  Y ^ m  C k superscript ^ Y m superscript C k \\smash{\\hat{Y}^{m}\\in\\mathbb{C}^{k}} over^ start_ARG italic_Y end_ARG start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  blackboard_C start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT .",
            "We note that  K Z R m , Z I m = 0 d  N  d  N subscript K superscript subscript Z R m superscript subscript Z I m subscript 0 d N d N \\mathbf{K}_{Z_{R}^{m},Z_{I}^{m}}=\\mathbf{0}_{dN\\times dN} bold_K start_POSTSUBSCRIPT italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = bold_0 start_POSTSUBSCRIPT italic_d italic_N  italic_d italic_N end_POSTSUBSCRIPT  in the separate-then-joint processing approach, since  f  (  ) f  f(\\cdot) italic_f (  )  and  g  (  ) g  g(\\cdot) italic_g (  )  do not consider interactions between  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , as summarized in Section  B.1  of the Appendix. To measure the magnitude of the data interactions across both approaches, we consider the  L p , q subscript L p q L_{p,q} italic_L start_POSTSUBSCRIPT italic_p , italic_q end_POSTSUBSCRIPT  norm, with  p , q  1 p q 1 p,q\\geq 1 italic_p , italic_q  1 , of   J subscript  J \\mathbf{\\Sigma_{J}} bold_ start_POSTSUBSCRIPT bold_J end_POSTSUBSCRIPT  and   S subscript  S \\mathbf{\\Sigma_{S}} bold_ start_POSTSUBSCRIPT bold_S end_POSTSUBSCRIPT . As   J subscript  J \\mathbf{\\Sigma_{J}} bold_ start_POSTSUBSCRIPT bold_J end_POSTSUBSCRIPT  includes the aforementioned cross-covariance matrix of  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , it follows that    J  p , q    S  p , q subscript norm subscript  J p q subscript norm subscript  S p q \\|\\mathbf{\\Sigma_{J}}\\|_{p,q}\\geq\\|\\mathbf{\\Sigma_{S}}\\|_{p,q}  bold_ start_POSTSUBSCRIPT bold_J end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_p , italic_q end_POSTSUBSCRIPT   bold_ start_POSTSUBSCRIPT bold_S end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_p , italic_q end_POSTSUBSCRIPT , as shown in Corollary  3.1 .",
            "Theorem  4.3  informs us that  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  is achievable when we enforce  Z I m =   ( Z R m ) superscript subscript Z I m italic- superscript subscript Z R m Z_{I}^{m}=\\phi(Z_{R}^{m}) italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_ ( italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , where    (  ) italic-  \\phi(\\cdot) italic_ (  )  is a deterministic, bijective function. We note that as the Steinmetz neural network is trained to minimize the average loss on the training dataset (via empirical risk minimization), we expect  Z m superscript Z m Z^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  to become more informative about the labels, whereby  I  ( Y m ; Z m ) I superscript Y m superscript Z m I(Y^{m};Z^{m}) italic_I ( italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  increases. We now further extend this result to Theorem  4.1 , through which we obtain a smaller upper bound on the generalization error.",
            "In feature engineering literature, selecting features that are orthogonal to others is a common strategy to minimize redundancy and improve model performance  (Chaudhry et al.,  2020 ) . For our purposes, with  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  as the latent features, we aim to utilize a function,    (  ) italic-  \\phi(\\cdot) italic_ (  ) , which ensures these features are as orthogonal as possible. This notion aligns with the principle of using non-redundant features to improve predictive accuracy. We revisit the analytic signal construction from Section  2.1.1 , wherein the real and imaginary parts are related by the DHT, and are orthogonal to each other. As such, if we consider    (  ) = H  {  } italic-  H  \\phi(\\cdot)=\\mathcal{H}\\{\\cdot\\} italic_ (  ) = caligraphic_H {  } , where  Z m = Z R m + i  Z I m superscript Z m superscript subscript Z R m i superscript subscript Z I m Z^{m}=Z_{R}^{m}+iZ_{I}^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT + italic_i italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , with  Z I m = H  { Z R m } superscript subscript Z I m H superscript subscript Z R m Z_{I}^{m}=\\mathcal{H}\\{Z_{R}^{m}\\} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = caligraphic_H { italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT } , then it follows that  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  are orthogonal (see Section  B.2  of the Appendix). We formalize this in Corollary  5.1 .",
            "Consider  { x R m , m  V } superscript subscript x R m m V \\{x_{R}^{m},m\\in\\mathcal{V}\\} { italic_x start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_m  caligraphic_V }  and  { x I m , m  V } superscript subscript x I m m V \\{x_{I}^{m},m\\in\\mathcal{V}\\} { italic_x start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_m  caligraphic_V }  from the training dataset, with  x R m , x I m  R d  N superscript subscript x R m superscript subscript x I m superscript R d N x_{R}^{m},x_{I}^{m}\\in\\mathbb{R}^{dN} italic_x start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_x start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d italic_N end_POSTSUPERSCRIPT . It follows that  z R m = g  ( x R m ) superscript subscript z R m g superscript subscript x R m z_{R}^{m}=g(x_{R}^{m}) italic_z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_g ( italic_x start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  and  z I m = f  ( x I m ) superscript subscript z I m f superscript subscript x I m z_{I}^{m}=f(x_{I}^{m}) italic_z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , wherein  z R m , z I m  R l  N superscript subscript z R m superscript subscript z I m superscript R l N \\smash{z_{R}^{m},z_{I}^{m}\\in\\mathbb{R}^{lN}} italic_z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_l italic_N end_POSTSUPERSCRIPT . To implement the Hilbert consistency penalty, we make use of the discrete Fourier transform (DFT), leveraging its properties in relation to phase shifts  we consider  F R m = F  { z R m }  C l  N superscript subscript F R m F superscript subscript z R m superscript C l N F_{R}^{m}=\\mathcal{F}\\{z_{R}^{m}\\}\\in\\mathbb{C}^{lN} italic_F start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = caligraphic_F { italic_z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT }  blackboard_C start_POSTSUPERSCRIPT italic_l italic_N end_POSTSUPERSCRIPT  as the DFT of  z R m superscript subscript z R m z_{R}^{m} italic_z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , where  b b b italic_b  is the frequency index. Eq. ( 18 ) summarizes the frequency domain implementation of this phase shift.",
            "We train a RVNN, CVNN, Steinmetz neural network, and analytic neural network using Cross Entropy Loss to classify the images in CV-MNIST and CV-CIFAR-10. These neural network architectures and hyperparameter choices are described in Section  C  of the Appendix, wherein we select  l  N = 64 l N 64 lN=64 italic_l italic_N = 64 , and leverage the Adam optimizer  (Kingma and Ba,  2014 )  to train each architecture using a fixed learning rate. The empirical results pertaining to this first experiment are depicted in Table  1  and Figure  3 . On CV-MNIST, we observe that the Steinmetz and analytic neural networks achieve faster convergence and improved generalization over the classical RVNN and CVNN approaches. The analytic neural network also achieves the highest classification accuracy. On CV-CIFAR-10, we similarly observe that the Steinmetz and analytic neural networks achieve improved generalization, with the analytic neural network again achieving the highest classification accuracy.",
            "For completeness, we prove Corollary  3.1  from the main text. Per the notation outlined in Section  3.1 , we first note the expansions of the auto-covariance matrices,  K X R m subscript K superscript subscript X R m \\mathbf{K}_{X_{R}^{m}} bold_K start_POSTSUBSCRIPT italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT end_POSTSUBSCRIPT  and  K X I m subscript K superscript subscript X I m \\mathbf{K}_{X_{I}^{m}} bold_K start_POSTSUBSCRIPT italic_X start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT end_POSTSUBSCRIPT :",
            "We now prove Corollary  5.1  from the main text, which claims  Z I m = H  { Z R m } superscript subscript Z I m H superscript subscript Z R m Z_{I}^{m}=\\mathcal{H}\\{Z_{R}^{m}\\} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = caligraphic_H { italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT }  enforces orthogonality between  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT . Let  F R m = F  { Z R m }  C l  N superscript subscript F R m F superscript subscript Z R m superscript C l N \\mathbf{F}_{R}^{m}=\\mathcal{F}\\{Z_{R}^{m}\\}\\in\\mathbb{C}^{lN} bold_F start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = caligraphic_F { italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT }  blackboard_C start_POSTSUPERSCRIPT italic_l italic_N end_POSTSUPERSCRIPT  denote the DFT of  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and let  H R m  C l  N superscript subscript H R m superscript C l N \\mathbf{H}_{R}^{m}\\in\\mathbb{C}^{lN} bold_H start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  blackboard_C start_POSTSUPERSCRIPT italic_l italic_N end_POSTSUPERSCRIPT  denote the frequency components of  H  { Z R m } H superscript subscript Z R m \\mathcal{H}\\{Z_{R}^{m}\\} caligraphic_H { italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT } . We consider the inner product    ,   : Z  Z  R  0 :    Z Z subscript R absent 0 \\langle\\cdot,\\cdot\\rangle:\\mathcal{Z}\\times\\mathcal{Z}\\rightarrow\\mathbb{R}_{% \\geq 0}   ,   : caligraphic_Z  caligraphic_Z  blackboard_R start_POSTSUBSCRIPT  0 end_POSTSUBSCRIPT , wherein:",
            "We now substitute Eq. ( 18 ) into the above expression, and expand the  H R m  [ b ] superscript subscript H R m delimited-[] b \\mathbf{H}_{R}^{m}[b] bold_H start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT [ italic_b ]  term."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Test MSE (magnitude and phase) for channel identification task with   = 2 / 2  2 2 \\rho=\\sqrt{2}/2 italic_ = square-root start_ARG 2 end_ARG / 2 .",
        "table": "A3.EGx2",
        "footnotes": [],
        "references": [
            "The organization of this paper is as follows. In Section  2 , we review complex and analytic signal representations, and survey the related work from CVNN literature. In Section  3 , we present the Steinmetz neural network architecture and discuss its theoretical foundations. In Section  4 , we summarize the consistency constraint and provide generalization error bounds for the Steinmetz network. In Section  5 , we introduce the analytic neural network and the Hilbert transform consistency penalty. In Section  6 , we present numerical simulations on benchmark datasets and a synthetic experiment. In Section  7 , we summarize our work. Our main contributions are summarized below:",
            "Building upon this architecture, the Steinmetz neural network postulates the Markov chain depicted in Fig.  2 , where the random variables,  Z I m = f  ( X I m ) superscript subscript Z I m f superscript subscript X I m Z_{I}^{m}=f(X_{I}^{m}) italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_f ( italic_X start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  and  Z R m = g  ( Z R m ) superscript subscript Z R m g superscript subscript Z R m Z_{R}^{m}=g(Z_{R}^{m}) italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_g ( italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , are the respective outputs of the parallel subnetworks,  f  (  ) f  f(\\cdot) italic_f (  )  and  g  (  ) g  g(\\cdot) italic_g (  ) , where  Z m = Z R m + i  Z I m superscript Z m superscript subscript Z R m i superscript subscript Z I m Z^{m}=Z_{R}^{m}+iZ_{I}^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT + italic_i italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  denotes the latent representation, and  Y ^ m = h  ( Z m ) superscript ^ Y m h superscript Z m \\smash{\\hat{Y}}^{m}=h(Z^{m}) over^ start_ARG italic_Y end_ARG start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_h ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  denotes the predictions yielded by the shared network,  h  (  ) h  h(\\cdot) italic_h (  )   1 1 1 For practical implementation, we mean center  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  as the final step before concatenation. .",
            "Per the construction in Fig.  2 , the Steinmetz neural network is given by:",
            "This upper bound captures the tradeoff between how well the latent space encapsulates information about the labels, and how much the encoder overfits the training distribution, wherein smaller values of  [ I  ( X m ; Z m | Y m ) + I  ( S ;   S ) ] delimited-[] I superscript X m conditional superscript Z m superscript Y m I S subscript  subscript  S [I(X^{m};Z^{m}|Y^{m})+I(S;\\theta_{\\psi_{S}})] [ italic_I ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT | italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) + italic_I ( italic_S ; italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ]  yield a smaller upper bound on the generalization error. Accordingly, we pose the following inquiry:  is it possible to leverage the Steinmetz neural network architecture to obtain a smaller upper bound on the generalization error ? To this end, we establish the existence of a lower bound,  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , on  [ I  ( X m ; Z m | Y m ) + I  ( S ;   S ) ] delimited-[] I superscript X m conditional superscript Z m superscript Y m I S subscript  subscript  S [I(X^{m};Z^{m}|Y^{m})+I(S;\\theta_{\\psi_{S}})] [ italic_I ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT | italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) + italic_I ( italic_S ; italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ]  that is achievable using a constraint on the latent space of the Steinmetz neural network. We formalize this in Corollary  4.2 .",
            "We have proven Corollary  4.2  in Section  A  of the Appendix, and present the lower bound,  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , in Theorem  4.3 , wherein there exists a consistency constraint ensuring the achievability of  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) .",
            "In feature engineering literature, selecting features that are orthogonal to others is a common strategy to minimize redundancy and improve model performance  (Chaudhry et al.,  2020 ) . For our purposes, with  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  as the latent features, we aim to utilize a function,    (  ) italic-  \\phi(\\cdot) italic_ (  ) , which ensures these features are as orthogonal as possible. This notion aligns with the principle of using non-redundant features to improve predictive accuracy. We revisit the analytic signal construction from Section  2.1.1 , wherein the real and imaginary parts are related by the DHT, and are orthogonal to each other. As such, if we consider    (  ) = H  {  } italic-  H  \\phi(\\cdot)=\\mathcal{H}\\{\\cdot\\} italic_ (  ) = caligraphic_H {  } , where  Z m = Z R m + i  Z I m superscript Z m superscript subscript Z R m i superscript subscript Z I m Z^{m}=Z_{R}^{m}+iZ_{I}^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT + italic_i italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , with  Z I m = H  { Z R m } superscript subscript Z I m H superscript subscript Z R m Z_{I}^{m}=\\mathcal{H}\\{Z_{R}^{m}\\} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = caligraphic_H { italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT } , then it follows that  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  are orthogonal (see Section  B.2  of the Appendix). We formalize this in Corollary  5.1 .",
            "We implement the Hilbert consistency penalty by penalizing the average error between  H  { z R m } H superscript subscript z R m \\mathcal{H}\\{z_{R}^{m}\\} caligraphic_H { italic_z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT }  and  z I m superscript subscript z I m z_{I}^{m} italic_z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , denoted as  L H subscript L H \\mathcal{L}_{\\mathcal{H}} caligraphic_L start_POSTSUBSCRIPT caligraphic_H end_POSTSUBSCRIPT , where  l H subscript l H \\ell_{\\mathcal{H}} roman_l start_POSTSUBSCRIPT caligraphic_H end_POSTSUBSCRIPT  is the relevant error metric. This is summarized in Definition  5.2 .",
            "We evaluate our proposed Steinmetz and analytic networks on the benchmark channel identification task from  (Scardapane et al.,  2020 ; Bouboulis et al.,  2015 ) . Let  X m = 1   2  X   m + i    X ~ m superscript X m 1 superscript  2 superscript   X m i  superscript ~ X m \\smash{X^{m}=\\sqrt{1-\\rho^{2}}\\bar{X}\\vphantom{X}^{m}+i\\rho\\tilde{X}\\vphantom{% X}^{m}} italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = square-root start_ARG 1 - italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG over  start_ARG italic_X end_ARG start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT + italic_i italic_ over~ start_ARG italic_X end_ARG start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  denote the input to the channel, wherein  X   m superscript   X m \\smash{\\bar{X}\\vphantom{X}^{m}} over  start_ARG italic_X end_ARG start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  X ~ m superscript ~ X m \\smash{\\tilde{X}\\vphantom{X}^{m}} over~ start_ARG italic_X end_ARG start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  are Gaussian random variables, and    \\rho italic_  determines the circularity of the signal. Here,  d  N = 5 d N 5 dN=5 italic_d italic_N = 5  denotes to the length of the input sequence, an embedding of the channel inputs over  d  N d N dN italic_d italic_N  time steps. The channel output,  Y m  C k superscript Y m superscript C k Y^{m}\\in\\mathbb{C}^{k} italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  blackboard_C start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , is formed using a linear filter, a memoryless nonlinearity, and by adding white Gaussian noise to achieve an SNR of  5 5 5 5  dB, where  X m superscript X m X^{m} italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Y m superscript Y m Y^{m} italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  are equivalent to  s n subscript s n s_{n} italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  and  r n subscript r n r_{n} italic_r start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  from  (Scardapane et al.,  2020 ) . We consider  M = 1000 M 1000 M=1000 italic_M = 1000  training examples and  1000 1000 1000 1000  test examples. Each of the real-valued architectures output a  2  k = 2 2 k 2 2k=2 2 italic_k = 2 -dimensional vector,  [ [ Y ^ R m ] T , [ Y ^ I m ] T ] T superscript superscript delimited-[] superscript subscript ^ Y R m T superscript delimited-[] superscript subscript ^ Y I m T T \\smash{[[\\hat{Y}_{R}^{m}]^{T},[\\hat{Y}_{I}^{m}]^{T}]^{T}} [ [ over^ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , [ over^ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , where  Y ^ R m , Y ^ I m  R superscript subscript ^ Y R m superscript subscript ^ Y I m R \\smash{\\hat{Y}_{R}^{m},\\hat{Y}_{I}^{m}\\in\\mathbb{R}} over^ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , over^ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  blackboard_R , and the CVNN outputs a  k = 1 k 1 k=1 italic_k = 1 -dimensional scalar,  Y ^ m = Y ^ R m + i  Y ^ I m superscript ^ Y m superscript subscript ^ Y R m i superscript subscript ^ Y I m \\smash{\\hat{Y}^{m}=\\hat{Y}_{R}^{m}+i\\hat{Y}_{I}^{m}} over^ start_ARG italic_Y end_ARG start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = over^ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT + italic_i over^ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , which we reshape to form  [ [ Y ^ R m ] T , [ Y ^ I m ] T ] T superscript superscript delimited-[] superscript subscript ^ Y R m T superscript delimited-[] superscript subscript ^ Y I m T T \\smash{[[\\hat{Y}_{R}^{m}]^{T},[\\hat{Y}_{I}^{m}]^{T}]^{T}} [ [ over^ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , [ over^ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT . We train our proposed Steinmetz and analytic neural networks, the RVNN, and the CVNN using MSE Loss, minimizing the distance between  [ [ Y ^ R m ] T , [ Y ^ I m ] T ] T superscript superscript delimited-[] superscript subscript ^ Y R m T superscript delimited-[] superscript subscript ^ Y I m T T \\smash{[[\\hat{Y}_{R}^{m}]^{T},[\\hat{Y}_{I}^{m}]^{T}]^{T}} [ [ over^ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , [ over^ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  and  [ [ Y R m ] T , [ Y I m ] T ] T superscript superscript delimited-[] superscript subscript Y R m T superscript delimited-[] superscript subscript Y I m T T \\smash{[[Y_{R}^{m}]^{T},[Y_{I}^{m}]^{T}]^{T}} [ [ italic_Y start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , [ italic_Y start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT . We select  l  N = 64 l N 64 lN=64 italic_l italic_N = 64 , and use the Adam optimizer  (Kingma and Ba,  2014 )  to train each architecture using a fixed learning rate. We compute and report the MSE between the predicted and true magnitudes and phases on the test dataset in Table  2 .",
            "From Table  2 , we see while the magnitude prediction error is comparable between the CVNN, RVNN, and the Steinmetz and analytic neural networks, the latter pair observes a much lower phase prediction error. This analysis suggests that the Steinmetz neural network might be preferred in scenarios where the extraction of accurate phase information is critical.",
            "For improved control over the generalization error, we form a smaller upper bound on    s  s \\Delta s roman_ italic_s  by deriving a lower bound,  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , on  [ I  ( X m ; Z m )  I  ( Y m ; Z m ) + I  ( S ;   S ) ] delimited-[] I superscript X m superscript Z m I superscript Y m superscript Z m I S subscript  subscript  S [I(X^{m};Z^{m})-I(Y^{m};Z^{m})+I(S;\\theta_{\\psi_{S}})] [ italic_I ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) - italic_I ( italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) + italic_I ( italic_S ; italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ] . We previously stated  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  is achievable by imposing a constraint on the latent representation,  Z m superscript Z m Z^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  (see Corollary  4.2 ).",
            "We now prove Corollary  4.2  and derive  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) . We consider the expansion of the term  I  ( X m ; Z m ) I superscript X m superscript Z m I(X^{m};Z^{m}) italic_I ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) :",
            "This lower bound is achievable when there exists a deterministic function,    (  ) italic-  \\phi(\\cdot) italic_ (  ) , relating  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , wherein  H  ( i  Z I m | Z R m ) = 0 H conditional i superscript subscript Z I m superscript subscript Z R m 0 H(iZ_{I}^{m}|Z_{R}^{m})=0 italic_H ( italic_i italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT | italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) = 0 . We formalize this in Lemma  A.2 , where    (  ) italic-  \\phi(\\cdot) italic_ (  )  is bijective.",
            "We note Eq. ( 29 ) follows from Section  2 , since  S  P  M similar-to S superscript P tensor-product absent M S\\sim P^{\\otimes M} italic_S  italic_P start_POSTSUPERSCRIPT  italic_M end_POSTSUPERSCRIPT . Thus,  H  ( X i , Y i | X j , Y j ) = H  ( X i , Y i ) H superscript X i conditional superscript Y i superscript X j superscript Y j H superscript X i superscript Y i H(X^{i},Y^{i}|X^{j},Y^{j})=H(X^{i},Y^{i}) italic_H ( italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | italic_X start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ) = italic_H ( italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) ,   i , j  V for-all i j V \\forall i,j\\in\\mathcal{V}  italic_i , italic_j  caligraphic_V . Expanding  H  ( X m , Y m ) H superscript X m superscript Y m H(X^{m},Y^{m}) italic_H ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , we have that:",
            "As in Lemma  A.2 , this lower bound is achievable when there exists a deterministic function,    (  ) italic-  \\phi(\\cdot) italic_ (  ) , relating  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , wherein we have  H  ( i  Z I m | X R m , Y m ) = H  ( i    ( f  ( X R m ) ) | X R m ) = 0 H conditional i superscript subscript Z I m superscript subscript X R m superscript Y m H conditional i italic- f superscript subscript X R m superscript subscript X R m 0 H(iZ_{I}^{m}|X_{R}^{m},Y^{m})=H(i\\phi(f(X_{R}^{m}))|X_{R}^{m})=0 italic_H ( italic_i italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT | italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) = italic_H ( italic_i italic_ ( italic_f ( italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) ) | italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) = 0 . We formalize this in Lemma  A.3 , where    (  ) italic-  \\phi(\\cdot) italic_ (  )  is bijective.",
            "Paralleling Eq. ( 29 ), we note that Eq. ( 32 ) also follows from Section  2 , since  S  P  M similar-to S superscript P tensor-product absent M S\\sim P^{\\otimes M} italic_S  italic_P start_POSTSUPERSCRIPT  italic_M end_POSTSUPERSCRIPT . Accordingly,  H  ( X i , Y i | X j , Y j ,   S ) = H  ( X i , Y i |   S ) H superscript X i conditional superscript Y i superscript X j superscript Y j subscript  subscript  S H superscript X i conditional superscript Y i subscript  subscript  S H(X^{i},Y^{i}|X^{j},Y^{j},\\theta_{\\psi_{S}})=H(X^{i},Y^{i}|\\theta_{\\psi_{S}}) italic_H ( italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | italic_X start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) = italic_H ( italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ,   i , j  V for-all i j V \\forall i,j\\in\\mathcal{V}  italic_i , italic_j  caligraphic_V . Expanding  H  ( X m , Y m |   S ) H superscript X m conditional superscript Y m subscript  subscript  S H(X^{m},Y^{m}|\\theta_{\\psi_{S}}) italic_H ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT | italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) , we have that:",
            "Suppose we are given  X R m + i  X I m superscript subscript X R m i superscript subscript X I m X_{R}^{m}+iX_{I}^{m} italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT + italic_i italic_X start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and   g S subscript  subscript g S \\theta_{g_{S}} italic_ start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT . It follows that  Z R m = g S  ( X R m ) superscript subscript Z R m subscript g S superscript subscript X R m Z_{R}^{m}=g_{S}(X_{R}^{m}) italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  is deterministic. We now impose the constraint presented in Lemma  A.2  and  A.3 , wherein there exists a bijective, deterministic function,    (  ) italic-  \\phi(\\cdot) italic_ (  ) , such that  Z I m =   ( Z R m ) ,  m  V formulae-sequence superscript subscript Z I m italic- superscript subscript Z R m for-all m V Z_{I}^{m}=\\phi(Z_{R}^{m}),\\,\\forall m\\in\\mathcal{V} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_ ( italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) ,  italic_m  caligraphic_V . It follows that  Z I m =   ( g S  ( X R m ) ) superscript subscript Z I m italic- subscript g S superscript subscript X R m Z_{I}^{m}=\\phi(g_{S}(X_{R}^{m})) italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_ ( italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) )  is deterministic. Per Eq. ( 34 ), the following equalities now hold under the imposed constraint:",
            "We also recall the Markov chain presented in Figure  2 . The Steinmetz network architecture informs us that  Y m superscript Y m Y^{m} italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  does not reduce the uncertainty in   f S subscript  subscript f S \\theta_{f_{S}} italic_ start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT  given  X I m superscript subscript X I m X_{I}^{m} italic_X start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT . Therefore, we have that:",
            "We can now determine the overall lower bound,  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , on  [ I  ( X m ; Z m )  I  ( Y m ; Z m ) + I  ( S ;   S ) ] delimited-[] I superscript X m superscript Z m I superscript Y m superscript Z m I S subscript  subscript  S [I(X^{m};Z^{m})-I(Y^{m};Z^{m})+I(S;\\theta_{\\psi_{S}})] [ italic_I ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) - italic_I ( italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) + italic_I ( italic_S ; italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ]  by substituting Eq. ( 27 ), ( 30 ), and ( 38 ) into Eq. ( 8 ):",
            "Revisiting Corollary  4.2 , we note that  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  is an achievable lower bound, with equality observed when the imposed condition,  f  F m f superscript F m f\\in\\mathcal{F}^{m} italic_f  caligraphic_F start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , delineates  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  as being related by a deterministic, bijective function,    (  ) italic-  \\phi(\\cdot) italic_ (  ) . We summarize this result in Lemma  A.5 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Neural network training hyperparameters (grouped by dataset).",
        "table": "A3.EGx3",
        "footnotes": [],
        "references": [
            "The organization of this paper is as follows. In Section  2 , we review complex and analytic signal representations, and survey the related work from CVNN literature. In Section  3 , we present the Steinmetz neural network architecture and discuss its theoretical foundations. In Section  4 , we summarize the consistency constraint and provide generalization error bounds for the Steinmetz network. In Section  5 , we introduce the analytic neural network and the Hilbert transform consistency penalty. In Section  6 , we present numerical simulations on benchmark datasets and a synthetic experiment. In Section  7 , we summarize our work. Our main contributions are summarized below:",
            "Reflecting on the challenges and benefits of both RVNNs and CVNNs, we target a framework that leverages the simplicity in training offered by RVNNs while offering improved generalization in the processing of complex signals. In this context, multi-view representation fusion emerges as a potential framework, proposing that different perspectives  or views  of data can provide complementary information, thereby enhancing learning and generalization  (Sun,  2013 ; Xu et al.,  2013 ; Lahat et al.,  2015 ; Zhao et al.,  2017 ; Yan et al.,  2021 ) . Deriving from this principle, we introduce the  Steinmetz Neural Network  architecture, which is designed to process the real ( X R m superscript subscript X R m X_{R}^{m} italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) and the imaginary ( X I m superscript subscript X I m X_{I}^{m} italic_X start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) parts of complex signal representations as separate views before joint processing. We formalize how this architecture leverages the complementarity principle of multi-view learning in Section  3.1 .",
            "We note that  K Z R m , Z I m = 0 d  N  d  N subscript K superscript subscript Z R m superscript subscript Z I m subscript 0 d N d N \\mathbf{K}_{Z_{R}^{m},Z_{I}^{m}}=\\mathbf{0}_{dN\\times dN} bold_K start_POSTSUBSCRIPT italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = bold_0 start_POSTSUBSCRIPT italic_d italic_N  italic_d italic_N end_POSTSUBSCRIPT  in the separate-then-joint processing approach, since  f  (  ) f  f(\\cdot) italic_f (  )  and  g  (  ) g  g(\\cdot) italic_g (  )  do not consider interactions between  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , as summarized in Section  B.1  of the Appendix. To measure the magnitude of the data interactions across both approaches, we consider the  L p , q subscript L p q L_{p,q} italic_L start_POSTSUBSCRIPT italic_p , italic_q end_POSTSUBSCRIPT  norm, with  p , q  1 p q 1 p,q\\geq 1 italic_p , italic_q  1 , of   J subscript  J \\mathbf{\\Sigma_{J}} bold_ start_POSTSUBSCRIPT bold_J end_POSTSUBSCRIPT  and   S subscript  S \\mathbf{\\Sigma_{S}} bold_ start_POSTSUBSCRIPT bold_S end_POSTSUBSCRIPT . As   J subscript  J \\mathbf{\\Sigma_{J}} bold_ start_POSTSUBSCRIPT bold_J end_POSTSUBSCRIPT  includes the aforementioned cross-covariance matrix of  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , it follows that    J  p , q    S  p , q subscript norm subscript  J p q subscript norm subscript  S p q \\|\\mathbf{\\Sigma_{J}}\\|_{p,q}\\geq\\|\\mathbf{\\Sigma_{S}}\\|_{p,q}  bold_ start_POSTSUBSCRIPT bold_J end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_p , italic_q end_POSTSUBSCRIPT   bold_ start_POSTSUBSCRIPT bold_S end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_p , italic_q end_POSTSUBSCRIPT , as shown in Corollary  3.1 .",
            "We have proven Corollary  4.2  in Section  A  of the Appendix, and present the lower bound,  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , in Theorem  4.3 , wherein there exists a consistency constraint ensuring the achievability of  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) .",
            "Theorem  4.3  informs us that  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  is achievable when we enforce  Z I m =   ( Z R m ) superscript subscript Z I m italic- superscript subscript Z R m Z_{I}^{m}=\\phi(Z_{R}^{m}) italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_ ( italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , where    (  ) italic-  \\phi(\\cdot) italic_ (  )  is a deterministic, bijective function. We note that as the Steinmetz neural network is trained to minimize the average loss on the training dataset (via empirical risk minimization), we expect  Z m superscript Z m Z^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  to become more informative about the labels, whereby  I  ( Y m ; Z m ) I superscript Y m superscript Z m I(Y^{m};Z^{m}) italic_I ( italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  increases. We now further extend this result to Theorem  4.1 , through which we obtain a smaller upper bound on the generalization error.",
            "We train a RVNN, CVNN, Steinmetz neural network, and analytic neural network using Cross Entropy Loss to classify the images in CV-MNIST and CV-CIFAR-10. These neural network architectures and hyperparameter choices are described in Section  C  of the Appendix, wherein we select  l  N = 64 l N 64 lN=64 italic_l italic_N = 64 , and leverage the Adam optimizer  (Kingma and Ba,  2014 )  to train each architecture using a fixed learning rate. The empirical results pertaining to this first experiment are depicted in Table  1  and Figure  3 . On CV-MNIST, we observe that the Steinmetz and analytic neural networks achieve faster convergence and improved generalization over the classical RVNN and CVNN approaches. The analytic neural network also achieves the highest classification accuracy. On CV-CIFAR-10, we similarly observe that the Steinmetz and analytic neural networks achieve improved generalization, with the analytic neural network again achieving the highest classification accuracy.",
            "As in Lemma  A.2 , this lower bound is achievable when there exists a deterministic function,    (  ) italic-  \\phi(\\cdot) italic_ (  ) , relating  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , wherein we have  H  ( i  Z I m | X R m , Y m ) = H  ( i    ( f  ( X R m ) ) | X R m ) = 0 H conditional i superscript subscript Z I m superscript subscript X R m superscript Y m H conditional i italic- f superscript subscript X R m superscript subscript X R m 0 H(iZ_{I}^{m}|X_{R}^{m},Y^{m})=H(i\\phi(f(X_{R}^{m}))|X_{R}^{m})=0 italic_H ( italic_i italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT | italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) = italic_H ( italic_i italic_ ( italic_f ( italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) ) | italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) = 0 . We formalize this in Lemma  A.3 , where    (  ) italic-  \\phi(\\cdot) italic_ (  )  is bijective.",
            "Paralleling Eq. ( 29 ), we note that Eq. ( 32 ) also follows from Section  2 , since  S  P  M similar-to S superscript P tensor-product absent M S\\sim P^{\\otimes M} italic_S  italic_P start_POSTSUPERSCRIPT  italic_M end_POSTSUPERSCRIPT . Accordingly,  H  ( X i , Y i | X j , Y j ,   S ) = H  ( X i , Y i |   S ) H superscript X i conditional superscript Y i superscript X j superscript Y j subscript  subscript  S H superscript X i conditional superscript Y i subscript  subscript  S H(X^{i},Y^{i}|X^{j},Y^{j},\\theta_{\\psi_{S}})=H(X^{i},Y^{i}|\\theta_{\\psi_{S}}) italic_H ( italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | italic_X start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) = italic_H ( italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ,   i , j  V for-all i j V \\forall i,j\\in\\mathcal{V}  italic_i , italic_j  caligraphic_V . Expanding  H  ( X m , Y m |   S ) H superscript X m conditional superscript Y m subscript  subscript  S H(X^{m},Y^{m}|\\theta_{\\psi_{S}}) italic_H ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT | italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) , we have that:",
            "Suppose we are given  X R m + i  X I m superscript subscript X R m i superscript subscript X I m X_{R}^{m}+iX_{I}^{m} italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT + italic_i italic_X start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and   g S subscript  subscript g S \\theta_{g_{S}} italic_ start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT . It follows that  Z R m = g S  ( X R m ) superscript subscript Z R m subscript g S superscript subscript X R m Z_{R}^{m}=g_{S}(X_{R}^{m}) italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  is deterministic. We now impose the constraint presented in Lemma  A.2  and  A.3 , wherein there exists a bijective, deterministic function,    (  ) italic-  \\phi(\\cdot) italic_ (  ) , such that  Z I m =   ( Z R m ) ,  m  V formulae-sequence superscript subscript Z I m italic- superscript subscript Z R m for-all m V Z_{I}^{m}=\\phi(Z_{R}^{m}),\\,\\forall m\\in\\mathcal{V} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_ ( italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) ,  italic_m  caligraphic_V . It follows that  Z I m =   ( g S  ( X R m ) ) superscript subscript Z I m italic- subscript g S superscript subscript X R m Z_{I}^{m}=\\phi(g_{S}(X_{R}^{m})) italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_ ( italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) )  is deterministic. Per Eq. ( 34 ), the following equalities now hold under the imposed constraint:",
            "We can now determine the overall lower bound,  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , on  [ I  ( X m ; Z m )  I  ( Y m ; Z m ) + I  ( S ;   S ) ] delimited-[] I superscript X m superscript Z m I superscript Y m superscript Z m I S subscript  subscript  S [I(X^{m};Z^{m})-I(Y^{m};Z^{m})+I(S;\\theta_{\\psi_{S}})] [ italic_I ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) - italic_I ( italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) + italic_I ( italic_S ; italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ]  by substituting Eq. ( 27 ), ( 30 ), and ( 38 ) into Eq. ( 8 ):",
            "This result is also summarized in Theorem  4.3  of the main text. We extend this result to derive the smaller upper bound on the generalization error,    s  s \\Delta s roman_ italic_s , provided in Theorem  4.4  of the main text.",
            "For completeness, we prove Corollary  3.1  from the main text. Per the notation outlined in Section  3.1 , we first note the expansions of the auto-covariance matrices,  K X R m subscript K superscript subscript X R m \\mathbf{K}_{X_{R}^{m}} bold_K start_POSTSUBSCRIPT italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT end_POSTSUBSCRIPT  and  K X I m subscript K superscript subscript X I m \\mathbf{K}_{X_{I}^{m}} bold_K start_POSTSUBSCRIPT italic_X start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT end_POSTSUBSCRIPT :",
            "The relevant hyperparameters used to train the neural networks from Appendix Section  C  are provided in Table  3 . All results presented in the main text were produced using these hyperparameter choices."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "A3.EGx4",
        "footnotes": [],
        "references": [
            "The organization of this paper is as follows. In Section  2 , we review complex and analytic signal representations, and survey the related work from CVNN literature. In Section  3 , we present the Steinmetz neural network architecture and discuss its theoretical foundations. In Section  4 , we summarize the consistency constraint and provide generalization error bounds for the Steinmetz network. In Section  5 , we introduce the analytic neural network and the Hilbert transform consistency penalty. In Section  6 , we present numerical simulations on benchmark datasets and a synthetic experiment. In Section  7 , we summarize our work. Our main contributions are summarized below:",
            "This higher norm indicates reduced interpretability, as the presence of cross-covariance terms implies that joint-only processing must not only handle  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  individually, but also their interactions, which can complicate the representation process. As such,   S subscript  S \\mathbf{\\Sigma_{S}} bold_ start_POSTSUBSCRIPT bold_S end_POSTSUBSCRIPT  has a smaller  L p , q subscript L p q L_{p,q} italic_L start_POSTSUBSCRIPT italic_p , italic_q end_POSTSUBSCRIPT  norm and associated representational complexity, enabling the Steinmetz network to separately extract  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  without the added burden of accounting for interactions. This Steinmetz architecture can also be leveraged to obtain a smaller upper bound on the generalization error, which we discuss in Section  4 .",
            "This upper bound captures the tradeoff between how well the latent space encapsulates information about the labels, and how much the encoder overfits the training distribution, wherein smaller values of  [ I  ( X m ; Z m | Y m ) + I  ( S ;   S ) ] delimited-[] I superscript X m conditional superscript Z m superscript Y m I S subscript  subscript  S [I(X^{m};Z^{m}|Y^{m})+I(S;\\theta_{\\psi_{S}})] [ italic_I ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT | italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) + italic_I ( italic_S ; italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ]  yield a smaller upper bound on the generalization error. Accordingly, we pose the following inquiry:  is it possible to leverage the Steinmetz neural network architecture to obtain a smaller upper bound on the generalization error ? To this end, we establish the existence of a lower bound,  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , on  [ I  ( X m ; Z m | Y m ) + I  ( S ;   S ) ] delimited-[] I superscript X m conditional superscript Z m superscript Y m I S subscript  subscript  S [I(X^{m};Z^{m}|Y^{m})+I(S;\\theta_{\\psi_{S}})] [ italic_I ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT | italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) + italic_I ( italic_S ; italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ]  that is achievable using a constraint on the latent space of the Steinmetz neural network. We formalize this in Corollary  4.2 .",
            "We have proven Corollary  4.2  in Section  A  of the Appendix, and present the lower bound,  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , in Theorem  4.3 , wherein there exists a consistency constraint ensuring the achievability of  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) .",
            "Theorem  4.3  informs us that  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  is achievable when we enforce  Z I m =   ( Z R m ) superscript subscript Z I m italic- superscript subscript Z R m Z_{I}^{m}=\\phi(Z_{R}^{m}) italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_ ( italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , where    (  ) italic-  \\phi(\\cdot) italic_ (  )  is a deterministic, bijective function. We note that as the Steinmetz neural network is trained to minimize the average loss on the training dataset (via empirical risk minimization), we expect  Z m superscript Z m Z^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  to become more informative about the labels, whereby  I  ( Y m ; Z m ) I superscript Y m superscript Z m I(Y^{m};Z^{m}) italic_I ( italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  increases. We now further extend this result to Theorem  4.1 , through which we obtain a smaller upper bound on the generalization error.",
            "As detailed in Section  4 , by enforcing a constraint within the latent representation,  Z m superscript Z m Z^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , such that  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , are related by a deterministic, bijective function,    (  ) italic-  \\phi(\\cdot) italic_ (  ) , we can leverage improved control over the generalization error,    s  s \\Delta s roman_ italic_s . A natural question that follows is:  which    (  ) italic-  \\phi(\\cdot) italic_ (  )  should be chosen to improve predictive performance ? To address this, we consider a configuration that focuses on the properties and predictive advantages of orthogonal latent representations.",
            "This experimental setup allows us to gauge how the signal-to-noise ratio (SNR) influences the efficacy of our Steinmetz and analytic neural networks versus the RVNN and CVNN. The empirical results pertaining to this second experiment are depicted in Figure  4 . We see that across both CV-MNIST and CV-CIFAR-10, the Steinmetz and analytic neural networks are far more resilient to additive noise.",
            "As outlined in Section  4 , our aim is to exploit the Steinmetz neural network architecture by deriving a consistency constraint that allows for improved control over the generalization error,    s  s \\Delta s roman_ italic_s . Recall Eq.  7 , which provides an upper bound on    s  s \\Delta s roman_ italic_s  in terms of the mutual information between  X m superscript X m X^{m} italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z m superscript Z m Z^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , between  Y m superscript Y m Y^{m} italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z m superscript Z m Z^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , and between  S S S italic_S  and    S subscript  subscript  S \\theta_{\\psi_{S}} italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT   [Kawaguchi et al.,  2023 ] .",
            "For improved control over the generalization error, we form a smaller upper bound on    s  s \\Delta s roman_ italic_s  by deriving a lower bound,  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , on  [ I  ( X m ; Z m )  I  ( Y m ; Z m ) + I  ( S ;   S ) ] delimited-[] I superscript X m superscript Z m I superscript Y m superscript Z m I S subscript  subscript  S [I(X^{m};Z^{m})-I(Y^{m};Z^{m})+I(S;\\theta_{\\psi_{S}})] [ italic_I ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) - italic_I ( italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) + italic_I ( italic_S ; italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ] . We previously stated  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  is achievable by imposing a constraint on the latent representation,  Z m superscript Z m Z^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  (see Corollary  4.2 ).",
            "We now prove Corollary  4.2  and derive  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) . We consider the expansion of the term  I  ( X m ; Z m ) I superscript X m superscript Z m I(X^{m};Z^{m}) italic_I ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) :",
            "Suppose we are given  X R m + i  X I m superscript subscript X R m i superscript subscript X I m X_{R}^{m}+iX_{I}^{m} italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT + italic_i italic_X start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and   g S subscript  subscript g S \\theta_{g_{S}} italic_ start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT . It follows that  Z R m = g S  ( X R m ) superscript subscript Z R m subscript g S superscript subscript X R m Z_{R}^{m}=g_{S}(X_{R}^{m}) italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  is deterministic. We now impose the constraint presented in Lemma  A.2  and  A.3 , wherein there exists a bijective, deterministic function,    (  ) italic-  \\phi(\\cdot) italic_ (  ) , such that  Z I m =   ( Z R m ) ,  m  V formulae-sequence superscript subscript Z I m italic- superscript subscript Z R m for-all m V Z_{I}^{m}=\\phi(Z_{R}^{m}),\\,\\forall m\\in\\mathcal{V} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_ ( italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) ,  italic_m  caligraphic_V . It follows that  Z I m =   ( g S  ( X R m ) ) superscript subscript Z I m italic- subscript g S superscript subscript X R m Z_{I}^{m}=\\phi(g_{S}(X_{R}^{m})) italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_ ( italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) )  is deterministic. Per Eq. ( 34 ), the following equalities now hold under the imposed constraint:",
            "We summarize the achievability of this lower bound in Lemma  A.4 .",
            "Revisiting Corollary  4.2 , we note that  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  is an achievable lower bound, with equality observed when the imposed condition,  f  F m f superscript F m f\\in\\mathcal{F}^{m} italic_f  caligraphic_F start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , delineates  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  as being related by a deterministic, bijective function,    (  ) italic-  \\phi(\\cdot) italic_ (  ) . We summarize this result in Lemma  A.5 .",
            "This result is also summarized in Theorem  4.3  of the main text. We extend this result to derive the smaller upper bound on the generalization error,    s  s \\Delta s roman_ italic_s , provided in Theorem  4.4  of the main text."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A3.EGx5",
        "footnotes": [],
        "references": [
            "The organization of this paper is as follows. In Section  2 , we review complex and analytic signal representations, and survey the related work from CVNN literature. In Section  3 , we present the Steinmetz neural network architecture and discuss its theoretical foundations. In Section  4 , we summarize the consistency constraint and provide generalization error bounds for the Steinmetz network. In Section  5 , we introduce the analytic neural network and the Hilbert transform consistency penalty. In Section  6 , we present numerical simulations on benchmark datasets and a synthetic experiment. In Section  7 , we summarize our work. Our main contributions are summarized below:",
            "In feature engineering literature, selecting features that are orthogonal to others is a common strategy to minimize redundancy and improve model performance  (Chaudhry et al.,  2020 ) . For our purposes, with  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  as the latent features, we aim to utilize a function,    (  ) italic-  \\phi(\\cdot) italic_ (  ) , which ensures these features are as orthogonal as possible. This notion aligns with the principle of using non-redundant features to improve predictive accuracy. We revisit the analytic signal construction from Section  2.1.1 , wherein the real and imaginary parts are related by the DHT, and are orthogonal to each other. As such, if we consider    (  ) = H  {  } italic-  H  \\phi(\\cdot)=\\mathcal{H}\\{\\cdot\\} italic_ (  ) = caligraphic_H {  } , where  Z m = Z R m + i  Z I m superscript Z m superscript subscript Z R m i superscript subscript Z I m Z^{m}=Z_{R}^{m}+iZ_{I}^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT + italic_i italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , with  Z I m = H  { Z R m } superscript subscript Z I m H superscript subscript Z R m Z_{I}^{m}=\\mathcal{H}\\{Z_{R}^{m}\\} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = caligraphic_H { italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT } , then it follows that  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  are orthogonal (see Section  B.2  of the Appendix). We formalize this in Corollary  5.1 .",
            "We implement the Hilbert consistency penalty by penalizing the average error between  H  { z R m } H superscript subscript z R m \\mathcal{H}\\{z_{R}^{m}\\} caligraphic_H { italic_z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT }  and  z I m superscript subscript z I m z_{I}^{m} italic_z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , denoted as  L H subscript L H \\mathcal{L}_{\\mathcal{H}} caligraphic_L start_POSTSUBSCRIPT caligraphic_H end_POSTSUBSCRIPT , where  l H subscript l H \\ell_{\\mathcal{H}} roman_l start_POSTSUBSCRIPT caligraphic_H end_POSTSUBSCRIPT  is the relevant error metric. This is summarized in Definition  5.2 .",
            "Revisiting Corollary  4.2 , we note that  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  is an achievable lower bound, with equality observed when the imposed condition,  f  F m f superscript F m f\\in\\mathcal{F}^{m} italic_f  caligraphic_F start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , delineates  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  as being related by a deterministic, bijective function,    (  ) italic-  \\phi(\\cdot) italic_ (  ) . We summarize this result in Lemma  A.5 .",
            "We now prove Corollary  5.1  from the main text, which claims  Z I m = H  { Z R m } superscript subscript Z I m H superscript subscript Z R m Z_{I}^{m}=\\mathcal{H}\\{Z_{R}^{m}\\} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = caligraphic_H { italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT }  enforces orthogonality between  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z I m superscript subscript Z I m Z_{I}^{m} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT . Let  F R m = F  { Z R m }  C l  N superscript subscript F R m F superscript subscript Z R m superscript C l N \\mathbf{F}_{R}^{m}=\\mathcal{F}\\{Z_{R}^{m}\\}\\in\\mathbb{C}^{lN} bold_F start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = caligraphic_F { italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT }  blackboard_C start_POSTSUPERSCRIPT italic_l italic_N end_POSTSUPERSCRIPT  denote the DFT of  Z R m superscript subscript Z R m Z_{R}^{m} italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and let  H R m  C l  N superscript subscript H R m superscript C l N \\mathbf{H}_{R}^{m}\\in\\mathbb{C}^{lN} bold_H start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  blackboard_C start_POSTSUPERSCRIPT italic_l italic_N end_POSTSUPERSCRIPT  denote the frequency components of  H  { Z R m } H superscript subscript Z R m \\mathcal{H}\\{Z_{R}^{m}\\} caligraphic_H { italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT } . We consider the inner product    ,   : Z  Z  R  0 :    Z Z subscript R absent 0 \\langle\\cdot,\\cdot\\rangle:\\mathcal{Z}\\times\\mathcal{Z}\\rightarrow\\mathbb{R}_{% \\geq 0}   ,   : caligraphic_Z  caligraphic_Z  blackboard_R start_POSTSUBSCRIPT  0 end_POSTSUBSCRIPT , wherein:",
            "The Steinmetz Network is designed to handle both real and imaginary components of the input data separately before combining them for the final prediction. This architecture can be applied to both classification and regression tasks (see Figure  5 ).",
            "The Real-valued neural network (RVNN) architecture is a straightforward and effective approach for handling both real and imaginary components by concatenating them and processing them together. It can be used for both classification and regression tasks (see Figure  5 )."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A3.EGx6",
        "footnotes": [],
        "references": [
            "The organization of this paper is as follows. In Section  2 , we review complex and analytic signal representations, and survey the related work from CVNN literature. In Section  3 , we present the Steinmetz neural network architecture and discuss its theoretical foundations. In Section  4 , we summarize the consistency constraint and provide generalization error bounds for the Steinmetz network. In Section  5 , we introduce the analytic neural network and the Hilbert transform consistency penalty. In Section  6 , we present numerical simulations on benchmark datasets and a synthetic experiment. In Section  7 , we summarize our work. Our main contributions are summarized below:",
            "Suppose   S subscript  S \\psi_{S} italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ,  f S subscript f S f_{S} italic_f start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , and  g S subscript g S g_{S} italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  are stochastic transformations, where    S = (  f S ,  g S ) subscript  subscript  S subscript  subscript f S subscript  subscript g S \\theta_{\\psi_{S}}=(\\theta_{f_{S}},\\theta_{g_{S}}) italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT = ( italic_ start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT )  is a random variable denoting the parameters of   S subscript  S \\psi_{S} italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , with the variables,   f S subscript  subscript f S \\theta_{f_{S}} italic_ start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT  and   g S subscript  subscript g S \\theta_{g_{S}} italic_ start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT , parameterizing  f S subscript f S f_{S} italic_f start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  and  g S subscript g S g_{S} italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , respectively. Per  (Federici et al.,  2020 ; Fischer,  2020 ; Lee et al.,  2021 ) , obtaining an optimal latent representation,  Z m superscript Z m Z^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , can be formulated as minimizing the mutual information between  X m superscript X m X^{m} italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z m superscript Z m Z^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , conditioned on  Y m superscript Y m Y^{m} italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT . However, as explored by  (Hafez-Kolahi et al.,  2020 ) , this framework does not hold when the encoder,   s subscript  s \\psi_{s} italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , is learned with the training dataset,  s s s italic_s . To avoid this counterexample,  (Kawaguchi et al.,  2023 )  proposed an additional term that captures how much information from  S S S italic_S  is used to train the encoder,   S subscript  S \\psi_{S} italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT . Now, in the context of our Steinmetz neural network architecture, obtaining the optimal  Z m superscript Z m Z^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  can be found by minimizing the expression in Eq. ( 6 ), where    S  R c ,  f S  R c 1 ,  g S  R c 2 formulae-sequence subscript  subscript  S superscript R c formulae-sequence subscript  subscript f S superscript R subscript c 1 subscript  subscript g S superscript R subscript c 2 \\theta_{\\psi_{S}}\\in\\mathbb{R}^{c},\\theta_{f_{S}}\\in\\mathbb{R}^{c_{1}},\\theta_% {g_{S}}\\in\\mathbb{R}^{c_{2}} italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT .",
            "Per Eq ( 6 ), the optimal latent representation,  Z m superscript Z m Z^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , best captures relevant information from  X m superscript X m X^{m} italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  about  Y m superscript Y m Y^{m} italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  while also considering the influence of  S S S italic_S  on the encoder parameters,    S subscript  subscript  S \\theta_{\\psi_{S}} italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT . We now consider the upper bound on the generalization error over the training dataset,    s  s \\Delta s roman_ italic_s , which is an adapted version of the bound originally proposed by  (Kawaguchi et al.,  2023 ) .",
            "The Complex-Valued Neural Network (CVNN) is designed to handle complex-valued data by treating the real and imaginary parts jointly as complex numbers. It can be used in classification and regression tasks (see Figure  6 ). For classification tasks, we take the magnitude of the  fc3  layer output."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A3.EGx7",
        "footnotes": [],
        "references": [
            "The organization of this paper is as follows. In Section  2 , we review complex and analytic signal representations, and survey the related work from CVNN literature. In Section  3 , we present the Steinmetz neural network architecture and discuss its theoretical foundations. In Section  4 , we summarize the consistency constraint and provide generalization error bounds for the Steinmetz network. In Section  5 , we introduce the analytic neural network and the Hilbert transform consistency penalty. In Section  6 , we present numerical simulations on benchmark datasets and a synthetic experiment. In Section  7 , we summarize our work. Our main contributions are summarized below:",
            "As outlined in Section  4 , our aim is to exploit the Steinmetz neural network architecture by deriving a consistency constraint that allows for improved control over the generalization error,    s  s \\Delta s roman_ italic_s . Recall Eq.  7 , which provides an upper bound on    s  s \\Delta s roman_ italic_s  in terms of the mutual information between  X m superscript X m X^{m} italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z m superscript Z m Z^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , between  Y m superscript Y m Y^{m} italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and  Z m superscript Z m Z^{m} italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , and between  S S S italic_S  and    S subscript  subscript  S \\theta_{\\psi_{S}} italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT   [Kawaguchi et al.,  2023 ] .",
            "We can now determine the overall lower bound,  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , on  [ I  ( X m ; Z m )  I  ( Y m ; Z m ) + I  ( S ;   S ) ] delimited-[] I superscript X m superscript Z m I superscript Y m superscript Z m I S subscript  subscript  S [I(X^{m};Z^{m})-I(Y^{m};Z^{m})+I(S;\\theta_{\\psi_{S}})] [ italic_I ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) - italic_I ( italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) + italic_I ( italic_S ; italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ]  by substituting Eq. ( 27 ), ( 30 ), and ( 38 ) into Eq. ( 8 ):"
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A3.EGx8",
        "footnotes": [],
        "references": [
            "Consider  { x R m , m  V } superscript subscript x R m m V \\{x_{R}^{m},m\\in\\mathcal{V}\\} { italic_x start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_m  caligraphic_V }  and  { x I m , m  V } superscript subscript x I m m V \\{x_{I}^{m},m\\in\\mathcal{V}\\} { italic_x start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_m  caligraphic_V }  from the training dataset, with  x R m , x I m  R d  N superscript subscript x R m superscript subscript x I m superscript R d N x_{R}^{m},x_{I}^{m}\\in\\mathbb{R}^{dN} italic_x start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_x start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d italic_N end_POSTSUPERSCRIPT . It follows that  z R m = g  ( x R m ) superscript subscript z R m g superscript subscript x R m z_{R}^{m}=g(x_{R}^{m}) italic_z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_g ( italic_x start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  and  z I m = f  ( x I m ) superscript subscript z I m f superscript subscript x I m z_{I}^{m}=f(x_{I}^{m}) italic_z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , wherein  z R m , z I m  R l  N superscript subscript z R m superscript subscript z I m superscript R l N \\smash{z_{R}^{m},z_{I}^{m}\\in\\mathbb{R}^{lN}} italic_z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_l italic_N end_POSTSUPERSCRIPT . To implement the Hilbert consistency penalty, we make use of the discrete Fourier transform (DFT), leveraging its properties in relation to phase shifts  we consider  F R m = F  { z R m }  C l  N superscript subscript F R m F superscript subscript z R m superscript C l N F_{R}^{m}=\\mathcal{F}\\{z_{R}^{m}\\}\\in\\mathbb{C}^{lN} italic_F start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = caligraphic_F { italic_z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT }  blackboard_C start_POSTSUPERSCRIPT italic_l italic_N end_POSTSUPERSCRIPT  as the DFT of  z R m superscript subscript z R m z_{R}^{m} italic_z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , where  b b b italic_b  is the frequency index. Eq. ( 18 ) summarizes the frequency domain implementation of this phase shift.",
            "We can now determine the overall lower bound,  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , on  [ I  ( X m ; Z m )  I  ( Y m ; Z m ) + I  ( S ;   S ) ] delimited-[] I superscript X m superscript Z m I superscript Y m superscript Z m I S subscript  subscript  S [I(X^{m};Z^{m})-I(Y^{m};Z^{m})+I(S;\\theta_{\\psi_{S}})] [ italic_I ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) - italic_I ( italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) + italic_I ( italic_S ; italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ]  by substituting Eq. ( 27 ), ( 30 ), and ( 38 ) into Eq. ( 8 ):",
            "We now substitute Eq. ( 18 ) into the above expression, and expand the  H R m  [ b ] superscript subscript H R m delimited-[] b \\mathbf{H}_{R}^{m}[b] bold_H start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT [ italic_b ]  term."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A3.EGx9",
        "footnotes": [],
        "references": [
            "We note Eq. ( 29 ) follows from Section  2 , since  S  P  M similar-to S superscript P tensor-product absent M S\\sim P^{\\otimes M} italic_S  italic_P start_POSTSUPERSCRIPT  italic_M end_POSTSUPERSCRIPT . Thus,  H  ( X i , Y i | X j , Y j ) = H  ( X i , Y i ) H superscript X i conditional superscript Y i superscript X j superscript Y j H superscript X i superscript Y i H(X^{i},Y^{i}|X^{j},Y^{j})=H(X^{i},Y^{i}) italic_H ( italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | italic_X start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ) = italic_H ( italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) ,   i , j  V for-all i j V \\forall i,j\\in\\mathcal{V}  italic_i , italic_j  caligraphic_V . Expanding  H  ( X m , Y m ) H superscript X m superscript Y m H(X^{m},Y^{m}) italic_H ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , we have that:",
            "Paralleling Eq. ( 29 ), we note that Eq. ( 32 ) also follows from Section  2 , since  S  P  M similar-to S superscript P tensor-product absent M S\\sim P^{\\otimes M} italic_S  italic_P start_POSTSUPERSCRIPT  italic_M end_POSTSUPERSCRIPT . Accordingly,  H  ( X i , Y i | X j , Y j ,   S ) = H  ( X i , Y i |   S ) H superscript X i conditional superscript Y i superscript X j superscript Y j subscript  subscript  S H superscript X i conditional superscript Y i subscript  subscript  S H(X^{i},Y^{i}|X^{j},Y^{j},\\theta_{\\psi_{S}})=H(X^{i},Y^{i}|\\theta_{\\psi_{S}}) italic_H ( italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | italic_X start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) = italic_H ( italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ,   i , j  V for-all i j V \\forall i,j\\in\\mathcal{V}  italic_i , italic_j  caligraphic_V . Expanding  H  ( X m , Y m |   S ) H superscript X m conditional superscript Y m subscript  subscript  S H(X^{m},Y^{m}|\\theta_{\\psi_{S}}) italic_H ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT | italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) , we have that:"
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A3.EGx10",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "A3.EGx11",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "A3.EGx12",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "A3.EGx13",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A3.EGx14",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A3.EGx15",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A3.EGx16",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "A3.EGx17",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "S6.T1.20.16",
        "footnotes": [],
        "references": [
            "Consider  { x R m , m  V } superscript subscript x R m m V \\{x_{R}^{m},m\\in\\mathcal{V}\\} { italic_x start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_m  caligraphic_V }  and  { x I m , m  V } superscript subscript x I m m V \\{x_{I}^{m},m\\in\\mathcal{V}\\} { italic_x start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_m  caligraphic_V }  from the training dataset, with  x R m , x I m  R d  N superscript subscript x R m superscript subscript x I m superscript R d N x_{R}^{m},x_{I}^{m}\\in\\mathbb{R}^{dN} italic_x start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_x start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d italic_N end_POSTSUPERSCRIPT . It follows that  z R m = g  ( x R m ) superscript subscript z R m g superscript subscript x R m z_{R}^{m}=g(x_{R}^{m}) italic_z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_g ( italic_x start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  and  z I m = f  ( x I m ) superscript subscript z I m f superscript subscript x I m z_{I}^{m}=f(x_{I}^{m}) italic_z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , wherein  z R m , z I m  R l  N superscript subscript z R m superscript subscript z I m superscript R l N \\smash{z_{R}^{m},z_{I}^{m}\\in\\mathbb{R}^{lN}} italic_z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_l italic_N end_POSTSUPERSCRIPT . To implement the Hilbert consistency penalty, we make use of the discrete Fourier transform (DFT), leveraging its properties in relation to phase shifts  we consider  F R m = F  { z R m }  C l  N superscript subscript F R m F superscript subscript z R m superscript C l N F_{R}^{m}=\\mathcal{F}\\{z_{R}^{m}\\}\\in\\mathbb{C}^{lN} italic_F start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = caligraphic_F { italic_z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT }  blackboard_C start_POSTSUPERSCRIPT italic_l italic_N end_POSTSUPERSCRIPT  as the DFT of  z R m superscript subscript z R m z_{R}^{m} italic_z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , where  b b b italic_b  is the frequency index. Eq. ( 18 ) summarizes the frequency domain implementation of this phase shift.",
            "We now substitute Eq. ( 18 ) into the above expression, and expand the  H R m  [ b ] superscript subscript H R m delimited-[] b \\mathbf{H}_{R}^{m}[b] bold_H start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT [ italic_b ]  term."
        ]
    },
    "id_table_19": {
        "caption": "",
        "table": "S6.T2.14.12",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "A3.EGx18",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "",
        "table": "A3.EGx19",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "",
        "table": "A3.EGx20",
        "footnotes": [],
        "references": []
    },
    "id_table_23": {
        "caption": "",
        "table": "A3.EGx21",
        "footnotes": [],
        "references": []
    },
    "id_table_24": {
        "caption": "",
        "table": "A3.EGx22",
        "footnotes": [],
        "references": []
    },
    "id_table_25": {
        "caption": "",
        "table": "A3.EGx23",
        "footnotes": [],
        "references": []
    },
    "id_table_26": {
        "caption": "",
        "table": "A3.EGx24",
        "footnotes": [],
        "references": []
    },
    "id_table_27": {
        "caption": "",
        "table": "A3.EGx25",
        "footnotes": [],
        "references": [
            "We can now determine the overall lower bound,  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , on  [ I  ( X m ; Z m )  I  ( Y m ; Z m ) + I  ( S ;   S ) ] delimited-[] I superscript X m superscript Z m I superscript Y m superscript Z m I S subscript  subscript  S [I(X^{m};Z^{m})-I(Y^{m};Z^{m})+I(S;\\theta_{\\psi_{S}})] [ italic_I ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) - italic_I ( italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) + italic_I ( italic_S ; italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ]  by substituting Eq. ( 27 ), ( 30 ), and ( 38 ) into Eq. ( 8 ):"
        ]
    },
    "id_table_28": {
        "caption": "",
        "table": "A3.EGx26",
        "footnotes": [],
        "references": []
    },
    "id_table_29": {
        "caption": "",
        "table": "A3.EGx27",
        "footnotes": [],
        "references": [
            "We note Eq. ( 29 ) follows from Section  2 , since  S  P  M similar-to S superscript P tensor-product absent M S\\sim P^{\\otimes M} italic_S  italic_P start_POSTSUPERSCRIPT  italic_M end_POSTSUPERSCRIPT . Thus,  H  ( X i , Y i | X j , Y j ) = H  ( X i , Y i ) H superscript X i conditional superscript Y i superscript X j superscript Y j H superscript X i superscript Y i H(X^{i},Y^{i}|X^{j},Y^{j})=H(X^{i},Y^{i}) italic_H ( italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | italic_X start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ) = italic_H ( italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) ,   i , j  V for-all i j V \\forall i,j\\in\\mathcal{V}  italic_i , italic_j  caligraphic_V . Expanding  H  ( X m , Y m ) H superscript X m superscript Y m H(X^{m},Y^{m}) italic_H ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , we have that:",
            "Paralleling Eq. ( 29 ), we note that Eq. ( 32 ) also follows from Section  2 , since  S  P  M similar-to S superscript P tensor-product absent M S\\sim P^{\\otimes M} italic_S  italic_P start_POSTSUPERSCRIPT  italic_M end_POSTSUPERSCRIPT . Accordingly,  H  ( X i , Y i | X j , Y j ,   S ) = H  ( X i , Y i |   S ) H superscript X i conditional superscript Y i superscript X j superscript Y j subscript  subscript  S H superscript X i conditional superscript Y i subscript  subscript  S H(X^{i},Y^{i}|X^{j},Y^{j},\\theta_{\\psi_{S}})=H(X^{i},Y^{i}|\\theta_{\\psi_{S}}) italic_H ( italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | italic_X start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) = italic_H ( italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ,   i , j  V for-all i j V \\forall i,j\\in\\mathcal{V}  italic_i , italic_j  caligraphic_V . Expanding  H  ( X m , Y m |   S ) H superscript X m conditional superscript Y m subscript  subscript  S H(X^{m},Y^{m}|\\theta_{\\psi_{S}}) italic_H ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT | italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) , we have that:"
        ]
    },
    "id_table_30": {
        "caption": "",
        "table": "A3.EGx28",
        "footnotes": [],
        "references": [
            "We can now determine the overall lower bound,  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , on  [ I  ( X m ; Z m )  I  ( Y m ; Z m ) + I  ( S ;   S ) ] delimited-[] I superscript X m superscript Z m I superscript Y m superscript Z m I S subscript  subscript  S [I(X^{m};Z^{m})-I(Y^{m};Z^{m})+I(S;\\theta_{\\psi_{S}})] [ italic_I ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) - italic_I ( italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) + italic_I ( italic_S ; italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ]  by substituting Eq. ( 27 ), ( 30 ), and ( 38 ) into Eq. ( 8 ):"
        ]
    },
    "id_table_31": {
        "caption": "",
        "table": "A3.EGx29",
        "footnotes": [],
        "references": []
    },
    "id_table_32": {
        "caption": "",
        "table": "A3.EGx30",
        "footnotes": [],
        "references": [
            "Paralleling Eq. ( 29 ), we note that Eq. ( 32 ) also follows from Section  2 , since  S  P  M similar-to S superscript P tensor-product absent M S\\sim P^{\\otimes M} italic_S  italic_P start_POSTSUPERSCRIPT  italic_M end_POSTSUPERSCRIPT . Accordingly,  H  ( X i , Y i | X j , Y j ,   S ) = H  ( X i , Y i |   S ) H superscript X i conditional superscript Y i superscript X j superscript Y j subscript  subscript  S H superscript X i conditional superscript Y i subscript  subscript  S H(X^{i},Y^{i}|X^{j},Y^{j},\\theta_{\\psi_{S}})=H(X^{i},Y^{i}|\\theta_{\\psi_{S}}) italic_H ( italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | italic_X start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) = italic_H ( italic_X start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ,   i , j  V for-all i j V \\forall i,j\\in\\mathcal{V}  italic_i , italic_j  caligraphic_V . Expanding  H  ( X m , Y m |   S ) H superscript X m conditional superscript Y m subscript  subscript  S H(X^{m},Y^{m}|\\theta_{\\psi_{S}}) italic_H ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT | italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) , we have that:"
        ]
    },
    "id_table_33": {
        "caption": "",
        "table": "A3.EGx31",
        "footnotes": [],
        "references": []
    },
    "id_table_34": {
        "caption": "",
        "table": "A3.EGx32",
        "footnotes": [],
        "references": [
            "Suppose we are given  X R m + i  X I m superscript subscript X R m i superscript subscript X I m X_{R}^{m}+iX_{I}^{m} italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT + italic_i italic_X start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and   g S subscript  subscript g S \\theta_{g_{S}} italic_ start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT . It follows that  Z R m = g S  ( X R m ) superscript subscript Z R m subscript g S superscript subscript X R m Z_{R}^{m}=g_{S}(X_{R}^{m}) italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )  is deterministic. We now impose the constraint presented in Lemma  A.2  and  A.3 , wherein there exists a bijective, deterministic function,    (  ) italic-  \\phi(\\cdot) italic_ (  ) , such that  Z I m =   ( Z R m ) ,  m  V formulae-sequence superscript subscript Z I m italic- superscript subscript Z R m for-all m V Z_{I}^{m}=\\phi(Z_{R}^{m}),\\,\\forall m\\in\\mathcal{V} italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_ ( italic_Z start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) ,  italic_m  caligraphic_V . It follows that  Z I m =   ( g S  ( X R m ) ) superscript subscript Z I m italic- subscript g S superscript subscript X R m Z_{I}^{m}=\\phi(g_{S}(X_{R}^{m})) italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = italic_ ( italic_g start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) )  is deterministic. Per Eq. ( 34 ), the following equalities now hold under the imposed constraint:"
        ]
    },
    "id_table_35": {
        "caption": "",
        "table": "A3.EGx33",
        "footnotes": [],
        "references": []
    },
    "id_table_36": {
        "caption": "",
        "table": "A3.EGx34",
        "footnotes": [],
        "references": []
    },
    "id_table_37": {
        "caption": "",
        "table": "A3.EGx35",
        "footnotes": [],
        "references": []
    },
    "id_table_38": {
        "caption": "",
        "table": "A3.EGx36",
        "footnotes": [],
        "references": [
            "We can now determine the overall lower bound,  D  ( Z m ) D superscript Z m \\mathcal{D}(Z^{m}) caligraphic_D ( italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , on  [ I  ( X m ; Z m )  I  ( Y m ; Z m ) + I  ( S ;   S ) ] delimited-[] I superscript X m superscript Z m I superscript Y m superscript Z m I S subscript  subscript  S [I(X^{m};Z^{m})-I(Y^{m};Z^{m})+I(S;\\theta_{\\psi_{S}})] [ italic_I ( italic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) - italic_I ( italic_Y start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ; italic_Z start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) + italic_I ( italic_S ; italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) ]  by substituting Eq. ( 27 ), ( 30 ), and ( 38 ) into Eq. ( 8 ):"
        ]
    },
    "id_table_39": {
        "caption": "",
        "table": "A3.T3.3.3",
        "footnotes": [],
        "references": []
    }
}