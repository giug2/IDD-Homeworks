{
    "id_table_1": {
        "caption": "Table 1:  Specs of DL Models on ImageNet-1K (1K categories). Top-1 and Top-5 are accuracy percentages. Size is in MB. Minimum input size: ResNet-50 and EffNet-B0: 1  \\times  1, ViT Base and Swin-Tiny: 224  \\times  224, ConvNeXt Tiny: 32  \\times  32.",
        "table": "S3.T1.7",
        "footnotes": [],
        "references": [
            "The model utilizes a contrastive learning loss, structured to maximize the similarity of correct image-text pairs while minimizing the similarity of incorrect ones, incorporating a margin-based penalty for better discrimination. (Equations ( 1 ,  2 ,  3 ,  4 )."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Summary of Base Model Training (Resnet50 + mBERT) Parameters",
        "table": "S3.T2.1",
        "footnotes": [],
        "references": [
            "The model utilizes a contrastive learning loss, structured to maximize the similarity of correct image-text pairs while minimizing the similarity of incorrect ones, incorporating a margin-based penalty for better discrimination. (Equations ( 1 ,  2 ,  3 ,  4 )."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Performance Metrics for Various Models Across Different Datasets",
        "table": "S4.T3.1.1",
        "footnotes": [],
        "references": [
            "To support the Azerbaijani language, as it is low resource language, this study incorporates image-text pairs in Azerbaijani. What we have done is, we have translated more than half a million captions from COCO dataset from English to Azerbaijani, and 90% of Flickr30k captions (Figure  3 ).",
            "The model utilizes a contrastive learning loss, structured to maximize the similarity of correct image-text pairs while minimizing the similarity of incorrect ones, incorporating a margin-based penalty for better discrimination. (Equations ( 1 ,  2 ,  3 ,  4 )."
        ]
    },
    "id_table_4": {
        "caption": "Note: Bold values indicate in-domain performance. MultiBERT refers to Base Multilingual BERT.",
        "table": "A1.T4.1",
        "footnotes": [],
        "references": [
            "We employed a multilingual BERT  Devlin et al. ( 2019b )  as our text encoder, optimized for low-resource languages with tokenization support for over 104 languages, ideal for applications requiring diverse linguistic support (see Figure  4 ).",
            "The model utilizes a contrastive learning loss, structured to maximize the similarity of correct image-text pairs while minimizing the similarity of incorrect ones, incorporating a margin-based penalty for better discrimination. (Equations ( 1 ,  2 ,  3 ,  4 )."
        ]
    },
    "global_footnotes": []
}