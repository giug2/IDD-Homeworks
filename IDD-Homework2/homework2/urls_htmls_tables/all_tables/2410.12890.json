{
    "id_table_1": {
        "caption": "Table 1:  Statistics of the two datasets, with the number of documents and questions for evaluation, as well as training and validation (tuning) samples generated through our data augmentation method.",
        "table": "S3.E1",
        "footnotes": [],
        "references": [
            "We aim to find hard negative training samples to enhance the models discriminative power. To that end, we first vectorize and ingest all documents to create a vector store, using a pretrained embedding model. Now, for each query  q i  Q subscript q i Q q_{i}\\in Q italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  italic_Q , we use the retriever to locate the top 50 most similar documents. Next, we filter the documents, selecting those that have similarity scores between 50% and 70% and are not in the top  k = 5 k 5 k=5 italic_k = 5 , to keep near misses while ruling out accidental matches. We found these hyperparameters empirically by running trials on a held-out 15% of the training set. In this way, we obtain  D  = { d 1  , d 2  , ... , d m  } superscript D subscript superscript d 1 subscript superscript d 2 ... subscript superscript d m D^{-}=\\{d^{-}_{1},d^{-}_{2},\\ldots,d^{-}_{m}\\} italic_D start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT = { italic_d start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_d start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_d start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }  as negative training samples (cf. Algorithm  1 ).",
            "In this work, we also utilize contrastive learning to fine-tune the model using a model fusion approach (Figure  1 ); specifically the  BAAI/bge-large-en-v1.5  model, is fine-tuned on a domain-specific dataset obtained using the technique discussed in Section  3.1 . We first initialize two models,  B  G  E B G E BGE italic_B italic_G italic_E  and  B  G  E  B G superscript E  BGE^{\\prime} italic_B italic_G italic_E start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , with the pretrained BGE weights, freezing the weights of  B  G  E  B G superscript E  BGE^{\\prime} italic_B italic_G italic_E start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  while allowing only  B  G  E B G E BGE italic_B italic_G italic_E  to undergo fine-tuning  [ 26 ] . Given a query  q i subscript q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , we pass it through both  B  G  E B G E BGE italic_B italic_G italic_E  and  B  G  E  B G superscript E  BGE^{\\prime} italic_B italic_G italic_E start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  to obtain the representations of the  [CLS]  token, denoted as  E C  L  S subscript E C L S E_{CLS} italic_E start_POSTSUBSCRIPT italic_C italic_L italic_S end_POSTSUBSCRIPT  and  E C  L  S  subscript superscript E  C L S E^{\\prime}_{CLS} italic_E start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_C italic_L italic_S end_POSTSUBSCRIPT , respectively. Next, we interpolate these two representations to create a final representation  E C  L  S subscript E C L S E_{CLS} italic_E start_POSTSUBSCRIPT italic_C italic_L italic_S end_POSTSUBSCRIPT :",
            "To evaluate the retriever, we require both the question and the supporting document (evidence). With the SQUAD & RAG dataset, this is straightforward due to the inherent mapping present within the raw dataset, allowing us to create a dataset of question-document (context) pairs for evaluation. However, for the TOURISM dataset, our annotation team manually curated a question from each document. For fine-tuning, we generated a dataset exclusively from documents and synthetic questions. To perform hyperparameter tuning, we set aside 15% of the generated data as a validation set for each of the datasets. Table  1  summarizes the sizes of these datasets.",
            "Fine-tuned BGE:  We fine-tune the BGE model according to the method outlined by  [ 26 ]  on the dataset generated synthetically with our approach, and subsequently compute the metrics on the evaluation data. Figure  1 (A) illustrates this technique."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:   Results on TOURISM, RAG, and SQUAD datasets using the baselines defined in the paper and REFINE (our approach). For all evaluation metrics, higher values indicate better performance.",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "To evaluate the retrievers performance, we initially ingest documents from each dataset into a FAISS vector store  [ 5 ] , separately. Then, for every query in the question set, we derived its representation using the model fine-tuned through our proposed method. Subsequently, we used this representation to fetch relevant documents from the vector store, utilizing the retriever from the LangChain library. For most experiments, and unless indicated otherwise, we fixed the top_k parameter at 3 (retrieving the three most relevant documents). (The number of retrieved documents is commonly notated as @k following the metric, e.g., Recall@3.). We computed the evaluation metrics for all the baseline approaches described in Section  4.2 . We want to highlight that we did not concentrate on hyperparameter tuning. However, to account for the variability in results due to different random initializations, we report the average performance across five experimental runs with different random seeds for the models that involved training.",
            "The key findings from the part 1 experiment, where results were obtained on the TOURISM (private), SQUAD (public) and RAG (public) datasets, are presented in Table  2 . Our approach of using synthetic data combined with model fusion boosted the performance of the retriever significantly, leading to improvements of  5.79% ,  6.58%  and  0.32%  in Recall@3 for the TOURISM, SQUAD and RAG datasets, respectively compared to the baseline Vanilla BGE model. The gain in recall did not come at the expense of precision (MAP), which also improved. The substantial improvement observed on the TOURISM dataset highlights the importance of fine-tuning in specific domain settings. Furthermore, even the standard BGE fine-tuning on synthetic data led to improved metrics across both datasets, except for recall on the RAG dataset. An important observation was that the TOURISM dataset was comprised mostly of structured tabular data. Upon further analysis, we observed that the vanilla BGE failed to correctly identify documents for the queries, especially when the queries involved specific details within tables. For instance, when querying \"Which hotels phone number is xxxx?\" where xxxx refers to a value within a table cell, the vanilla BGE model struggled to provide accurate representations, resulting in unreliable cosine similarity. However, with augmented data used for fine-tuning, performance improved notably, particularly in the case of structured data retrieval.",
            "Figure  2  plots the performance of the three models when different values of top_k were used in the retriever. Recall was measured with  top  _  k = 1 , 2 , 3 , 4 , 5 top _ k 1 2 3 4 5 \\mathrm{top\\_k}=1,2,3,4,5 roman_top _ roman_k = 1 , 2 , 3 , 4 , 5 . The figure shows that REFINE outperforms the other baselines across the different top_k settings on both datasets. A significant improvement can be observed in the TOURISM and SQUAD dataset, even for  top  _  k = 1 top _ k 1 \\mathrm{top\\_k}=1 roman_top _ roman_k = 1  w.r.t vanilla model. This demonstrates the superior performance of REFINE when retrieving only one or few relevant documents. Refer Table  4  in Supplementary material for the detailed results. We also want to highlight that in preliminary experiments, we attempted unsupervised fine-tuning on the unlabeled training data. However, this approach led to a degradation in performance. The reason for this is that the objective of pre-training is to reconstruct the text, and the pre-trained model cannot be directly used for similarity computation. Instead, it requires fine-tuning to achieve that capability  [ 26 ,  6 ] ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:   The results shown represent the models performance when it was trained on a specific dataset and evaluated on an out-of-domain (OOD) dataset. The row with a \"-\" indicates the metrics for the vanilla (pretrained) BGE model, which did not undergo fine-tuning.",
        "table": "S5.T2.1.1",
        "footnotes": [],
        "references": [
            "In this work, we also utilize contrastive learning to fine-tune the model using a model fusion approach (Figure  1 ); specifically the  BAAI/bge-large-en-v1.5  model, is fine-tuned on a domain-specific dataset obtained using the technique discussed in Section  3.1 . We first initialize two models,  B  G  E B G E BGE italic_B italic_G italic_E  and  B  G  E  B G superscript E  BGE^{\\prime} italic_B italic_G italic_E start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , with the pretrained BGE weights, freezing the weights of  B  G  E  B G superscript E  BGE^{\\prime} italic_B italic_G italic_E start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  while allowing only  B  G  E B G E BGE italic_B italic_G italic_E  to undergo fine-tuning  [ 26 ] . Given a query  q i subscript q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , we pass it through both  B  G  E B G E BGE italic_B italic_G italic_E  and  B  G  E  B G superscript E  BGE^{\\prime} italic_B italic_G italic_E start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  to obtain the representations of the  [CLS]  token, denoted as  E C  L  S subscript E C L S E_{CLS} italic_E start_POSTSUBSCRIPT italic_C italic_L italic_S end_POSTSUBSCRIPT  and  E C  L  S  subscript superscript E  C L S E^{\\prime}_{CLS} italic_E start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_C italic_L italic_S end_POSTSUBSCRIPT , respectively. Next, we interpolate these two representations to create a final representation  E C  L  S subscript E C L S E_{CLS} italic_E start_POSTSUBSCRIPT italic_C italic_L italic_S end_POSTSUBSCRIPT :",
            "Our evaluation is based on the metrics defined in Section  4.3 . We present our findings in two parts: 1) The enhancement in retrieval performance achieved through the generation of synthetic data and the model fusion approach. 2) We discuss how the REFINE approach not only outperforms other baseline methods but also enables the model to maintain its general capability, thereby mitigating catastrophic forgetting",
            "We further conducted experiments to evaluate the impact of fine-tuning on the generalizability of a model trained on domain-specific data. While fine-tuning generally improves performance within the targeted domain (in-domain), it often results in a loss of generalization across other domains (out-of-domain), a phenomenon known as catastrophic forgetting  [ 1 ] . Our findings, presented in Table  3 , show the performance of a model initially trained on the SQUAD dataset and then evaluated on the RAG dataset, serving as an out-of-domain test case (OOD). We also compared our results with the LM-cocktail method, a popular model-merging technique. Our approach improved performance, achieving a recall of  0.938  compared to baseline methods. Interestingly, even fine-tuned BGE exhibited improved rather than degraded performance. We want to highlight that our primary focus was on enhancing the retriever for the specific dataset, we aimed to examine the effects of catastrophic forgetting and acknowledge the need for further exploration in future studies. Therefore, we limited our analysis to the comparison between the SQuAD and RAG datasets, rather than examining all possible combinations. Additionally, when we reversed the training and test datasets (using RAG as the training data and SQUAD as the test data), a similar trend was observed. Fine-tuned BGE, LM-cocktail showed performance improvements, with REFINE outperforming others achieving a recall of  0.896 , highlighting the generalizability of our approach."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:   Recall at different top_k values for the baselines defined in the paper and REFINE (our approach).",
        "table": "S5.T3.1",
        "footnotes": [],
        "references": [
            "To evaluate the retrievers performance, we initially ingest documents from each dataset into a FAISS vector store  [ 5 ] , separately. Then, for every query in the question set, we derived its representation using the model fine-tuned through our proposed method. Subsequently, we used this representation to fetch relevant documents from the vector store, utilizing the retriever from the LangChain library. For most experiments, and unless indicated otherwise, we fixed the top_k parameter at 3 (retrieving the three most relevant documents). (The number of retrieved documents is commonly notated as @k following the metric, e.g., Recall@3.). We computed the evaluation metrics for all the baseline approaches described in Section  4.2 . We want to highlight that we did not concentrate on hyperparameter tuning. However, to account for the variability in results due to different random initializations, we report the average performance across five experimental runs with different random seeds for the models that involved training.",
            "Our evaluation is based on the metrics defined in Section  4.3 . We present our findings in two parts: 1) The enhancement in retrieval performance achieved through the generation of synthetic data and the model fusion approach. 2) We discuss how the REFINE approach not only outperforms other baseline methods but also enables the model to maintain its general capability, thereby mitigating catastrophic forgetting",
            "Figure  2  plots the performance of the three models when different values of top_k were used in the retriever. Recall was measured with  top  _  k = 1 , 2 , 3 , 4 , 5 top _ k 1 2 3 4 5 \\mathrm{top\\_k}=1,2,3,4,5 roman_top _ roman_k = 1 , 2 , 3 , 4 , 5 . The figure shows that REFINE outperforms the other baselines across the different top_k settings on both datasets. A significant improvement can be observed in the TOURISM and SQUAD dataset, even for  top  _  k = 1 top _ k 1 \\mathrm{top\\_k}=1 roman_top _ roman_k = 1  w.r.t vanilla model. This demonstrates the superior performance of REFINE when retrieving only one or few relevant documents. Refer Table  4  in Supplementary material for the detailed results. We also want to highlight that in preliminary experiments, we attempted unsupervised fine-tuning on the unlabeled training data. However, this approach led to a degradation in performance. The reason for this is that the objective of pre-training is to reconstruct the text, and the pre-trained model cannot be directly used for similarity computation. Instead, it requires fine-tuning to achieve that capability  [ 26 ,  6 ] .",
            "The detailed recall results at different top_k values for both datasets are presented in Table  4 . Our approach significantly outperforms all the baselines except in two cases. For the TOURISM dataset, LM-cocktail performed slightly better than REFINE for  top  _  k = 5 top _ k 5 \\mathrm{top\\_k}=5 roman_top _ roman_k = 5 , while  snowflake-arctic-embed-l  performed better for  top  _  k = 1 top _ k 1 \\mathrm{top\\_k}=1 roman_top _ roman_k = 1  in case of RAG. Notably, on the TOURISM & SQUAD dataset, OpenAIs  text-embedding-3-large  embedding model demonstrated significantly better performance than other embedding models used in a vanilla (without fine-tuning) setting."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "Pt0.A1.T4.1.1",
        "footnotes": [],
        "references": []
    }
}