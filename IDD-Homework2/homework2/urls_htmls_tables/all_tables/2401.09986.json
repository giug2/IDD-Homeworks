{
    "PAPER'S NUMBER OF TABLES": 2,
    "S4.T1": {
        "caption": "Table 1: Average classification accuracy, model training loss, and number of federated learning training rounds needed to achieve the maximum accuracy of the T𝑇T=4 case for each dataset-model combination. Quantitative results suggest the effectiveness of exploiting low temperatures for model training in federated learning both in terms of model accuracy and training efficiency.",
        "table": "",
        "footnotes": "\n\n\n\n\n\n\nFEMNIST-DNN\nCIFAR10-CNN\nCIFAR100-ResNet18\n\nTemperature (T𝑇T)\nAccuracy (%)\nLoss\nNum. of Round\nAccuracy (%)\nLoss\nNum. of Round\nAccuracy (%)\nLoss\nNum. of Round\n\nT=4.0𝑇4.0T=4.0\n73.60 (-5.06)\n0.893\n299 (0.44×)\n37.58 (-3.76)\n1.724\n299 (0.84×)\n28.05 (-0.20)\n2.984\n288 (0.69×)\n\nT=2.0𝑇2.0T=2.0\n77.20 (-1.46)\n0.749\n205 (0.64×)\n40.35 (-0.99)\n1.638\n270 (0.93×)\n27.89 (-0.36)\n2.918\n288 (0.69×)\n\nT=1.0𝑇1.0T=1.0\n78.66 (0.00)\n0.684\n132 (1.00×)\n41.34 (0.00)\n1.594\n251 (1.00×)\n28.25 (0.00)\n2.918\n200 (1×)\n\nT=0.5𝑇0.5T=0.5\n79.78 (+1.13)\n0.654\n88 (1.50×)\n41.52 (+0.18)\n1.551\n161 (1.56×)\n29.77 (+1.52)\n2.875\n178 (1.12×)\n\nT=0.25𝑇0.25T=0.25\n79.50 (+0.85)\n0.650\n57 (2.32×)\n43.41 (+2.07)\n1.532\n132 (1.90×)\n30.63 (+2.38)\n2.997\n152 (1.32×)\n\nT=0.05𝑇0.05T=0.05\n80.86 (+2.20)\n0.643\n22 (6.00×)\n44.71 (+3.37)\n1.528\n106 (2.37×)\n27.05 (-2.72)\n4.228\n239 (0.74×)\n\n\n",
        "references": [
            "The average accuracy for all federated learning clients after 300 training rounds presented in Table 1 confirm our observations, indicating that with T𝑇T=0.05 the CIFAR10 case shows a 3.37% improvement in accuracy compared to the T𝑇T=1 case. Even for other cases, the use of a low temperature value show promising results in terms of accuracy. We emphasize once more that this does not mean that the accuracy will always perform better at the lowest temperature. Rather, our results suggest the need to carefully explore the fractional temperature space in the federated learning training process.",
            "From the quantitative values in Table 1 where we plot the average loss observed at all federated learning clients after 300 training rounds (120 for the FEMNIST-DNN @ T𝑇T=0.05), we can make similar observations. Especially, for FEMNIST-DNN, we see a six-fold improvement in loss values compared to T𝑇T=1. For the CIFAR100-ResNet18 case, however, we notice that the lowest loss convergence is in fact the T𝑇T=0.25 case rather than 0.05. This observation confirms the need to properly consider and explore fractional temperature values for client-side model training.",
            "Finally, the last columns for each test case in Table 1 we present the number of federated learning training rounds needed to achieve a preset target accuracy for different temperature values using for model training. This number provides an understanding of how efficient the local training processes are when different temperature values are applied. S",
            "From the results in Table 1 we can notice that with T𝑇T=0.05 the FEMNIST-DNN case achieves the target accuracy after only 22 training rounds. We see similar trends of low temperatures positively affecting the training efficiency throughout our results. This is from the generally aggressiveness of training behaviors when applying lower temperature training. Given a single training sample, while high temperature-based training conservatively absorbs the available information, lower temperatures are more progressive in making model changes. Given the small datasets used for local training and the need to effectively deliver its knowledge to the server for aggregation, we see FLex&\\&Chill\nas an effective way to train client-side models in the federated learning process.",
            "As Table 2 shows, applying FLex&\\&Chill, thus a lower T𝑇T does not induce a positive impact on the performance of centralized learning scenarios. In fact, for centralized learning, controlling T𝑇T for the training process does not show a noticeable impact. We see the same phenomena for the federated learning system with iid samples as well, suggesting that the performance enhancements we see in Table 1 is not an effect of the federated learning process itself. We do point out that holding iid datasets in federated learning will show overall higher performance compared to non-iid configurations (regardless from T𝑇T), but is not a practical case to assume when deploying federated learning systems. Rather, as we discussed, the impact of exploiting low training temperatures is beneficial when dealing with non-iid datasets."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Average inference accuracy and model training loss for CIFAR10-CNN with different configurations. The impact of FLex&\\&Chill is prominent for federated learning scenarios with non-iid datasets, whereas its impact is less when elsewise.",
        "table": "",
        "footnotes": "\n\n\n\n\n\n\nCentralized - iid\nFederated - iid\nFederated - non-iid\n\nT𝑇T\nAcc. (%)\nLoss\nAcc. (%)\nLoss\nAcc. (%)\nLoss\n\n4.0\n58.05\n1.152\n48.22\n1.473\n37.58\n1.724\n\n2.0\n63.27\n1.041\n51.90\n1.403\n40.35\n1.638\n\n1.0\n65.18\n1.044\n54.66\n1.325\n41.34\n1.594\n\n0.5\n64.19\n1.030\n53.19\n1.320\n41.52\n1.551\n\n0.25\n63.88\n1.092\n54.64\n1.265\n43.41\n1.532\n\n0.05\n60.98\n1.227\n47.60\n1.701\n44.71\n1.528\n\n\n",
        "references": [
            "As Table 2 shows, applying FLex&\\&Chill, thus a lower T𝑇T does not induce a positive impact on the performance of centralized learning scenarios. In fact, for centralized learning, controlling T𝑇T for the training process does not show a noticeable impact. We see the same phenomena for the federated learning system with iid samples as well, suggesting that the performance enhancements we see in Table 1 is not an effect of the federated learning process itself. We do point out that holding iid datasets in federated learning will show overall higher performance compared to non-iid configurations (regardless from T𝑇T), but is not a practical case to assume when deploying federated learning systems. Rather, as we discussed, the impact of exploiting low training temperatures is beneficial when dealing with non-iid datasets."
        ]
    }
}