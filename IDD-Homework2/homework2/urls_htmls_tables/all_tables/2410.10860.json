{
    "id_table_1": {
        "caption": "Table 1:   Statistics of the data before and after pruning",
        "table": "S3.T1.1.1",
        "footnotes": [],
        "references": [
            "To combat these discriminatory practices, legislation such as the Fair Housing Act  (U.S. Department of Housing and Urban Development (1968),  HUD )  and the Equal Credit Opportunity Act  (Staff in the Office of Technology and The Division of Privacy and Identity Protection,  2024 )  were enacted to ensure fair treatment in real estate transactions. Real estate agents, brokers, and financial institutions are required to comply with these regulations. However, the growing use of AI-driven chatbots in real estate brings new complexities, particularly as large language models (LLMs) are prone to replicating and amplifying biases learned from data, inadvertently violating these laws. Figure  1  illustrates a case where GPT-4o as a state of the art model violates the fair housing regulations.",
            "We built a three-part dataset including general instructions, safety instructions, and dialog. In this section we explain how each segment (split) of the dataset was built. Safety alignment is inherently a long-tail distribution problem, making it crucial to ensure that optimizing for safety does not compromise performance on the main tasks. The first question we needed to address was identifying the domain of tasks that a real estate chatbot should excel in. To achieve this, we employed a combination of automation and human intervention to build a comprehensive taxonomy of topics relevant to discussions and interactions between a real estate chatbot and users. Our focus was primarily on knowledge-intensive real estate instructions rather than inquiries requiring real-time information, such as home listings or current market trends. At the time of writing this paper, GPT-4o  ( OpenAI,  )  is one of the most powerful LLMs, particularly in knowledge-intensive benchmarks such as MMLU  (Hendrycks et al.,  2020 ) . This is why we chose to use it as our generator LLM. Table  1  summarizes the statistics of our proposed dataset (More examples and details can be found in appendix  A ).",
            "To generate a diverse set of instructions and responses, we utilize a prompting approach similar to GenQA  (Chen et al.,  2024 ) , but with some important differences. Our pipeline consists of three main stages: 1) A human-LLM collaboration for generating a diverse and high quality set of real estate topics, 2) diverse and challenging instruction generation, and 3) response generation. For the first stage, in order to ensure quality, diversity and coverage of different real estate topics the authors of the paper cleaned and prepared a set of 90 real estate topics (More details on this step can be found in appendix  A.1 .)",
            "Since it is also important for the model to interact with users in a natural, multi-turn conversational setup, we generated a set of multi-turn interactions. To do this, we followed a similar approach to Section  3.1 , but instead of making two calls to the LLM, we asked it to generate a long conversation in a single call, and we post-processed the conversations afterward. (Details and prompts used in this stage are explained in appendix  A.2.3 .) We refer to this proportion of the data as the  dialog split .",
            "To ensure a dataset of diverse instructions and responses while avoiding semantically and lexically duplicate instructions, we aim to prune the data. This is particularly important when holding out a set of examples for evaluating our final tuned models, as we want to avoid having leaked examples from the training set in the evaluation set. We iterate over all the examples in each split of the data and remove those with a similarity above a certain threshold. Algorithm  1  outlines the procedure for pruning the data. (More details of the model and configurations we use for pruning can be found in appendix  A.4 .)",
            "We use LoRA  (Hu et al.,  2021 )  adaptors to fine-tune llama3-8b-instruct on our proposed dataset. We fine-tune the model for 5 epochs or until the validation loss on 200 held out examples from general instruct split ceases to decrease. Additionally, we hold out 200 examples from each data split for further testing of performance and safety. (More information about the training setup and LoRA configurations used can be found in appendix  C .) We also perform an ablation study of the effect of the dialog split and the size of the safety data in  D.1  and different LoRA adaptor sizes (as reported in the appendix  D.2 ).",
            "We measure helpfulness on the general instructions split of the data and safety on the safety split. To achieve this, we define four different criteria ( helpfulness with reference, helpfulness without reference, safety with reference, safety without reference ) and use the G-Eval  Liu et al. ( 2023 )  approach to score the models responses. We have chosen to use both metrics with reference (using references from GPT-4o during the data generation process) and without reference to avoid biasing the evaluation towards GPT-4o responses as the ground truth. We employ GPT-4 as the evaluator model in all cases 3 3 3 At the time of writing this paper, gpt-4o didnt provide generated token probability which is required by G-Eval method  and run the two helpfulness metrics on the general instruction split and the two safety metrics on the safety split of the test set. (The criteria used for each of the metrics are described in appendix  B.1 .)",
            "Figure  3  depicts the range of scores that each of the models get on each of our proposed G-Eval metrics. It can be seen that the metrics with references better capture the nuances in the answers as they are able to compare with a ground truth. This is while there is a low variance in the scores given by reference less responses. Therefore, we also compare the head-to-head win rate of the models according to their metric scores for each test case. We set a threshold of 1% to highlight more significant win/lose rates. That is, if two models scores fall within one percent of each other, we call it a tie. Figure  4  illustrates this comparison. Each cell represents the win rate of the left hand model versus the top model. Note that the scores wouldnt sum up to 100 since there are also ties. On helpfulness with reference, our model beats all of the baselines except GPT-4o which there is a win rate of 34%, lose rate of 38% and 28% ties. This is intuitive as the ground truth responses are also given by GPT-4o. On the safety with reference, our model significantly outperforms the baselines but you can see that when there are no references and the responses are solely evaluated based on evaluator models knowledge, most of the scores are fairly close to each other. However, we can see that our proposed model outperforms the base llama3-8b model by a significant margin and wins 42% of the times while loosing 8% and getting ties 50% of the times. In appendix  B.1.2  you can find some example evaluations.",
            "Same as general single turn instructions, we randomly select a topic ( TOPIC ) from a pool of 18 most common real estate topics that resulted from section  A.1  but instead of subtopics, we ask it to generate 50 conversation scenarios and then randomly select one ( N ) and ask the model to generate a long and helpful conversation. The resulting dataset consists of dialogs with an average of 10 turns. Figure  6  illustrates the distribution of dialog lengths.",
            "We utilize  all-mpnet-base-v2 , a pre-trained sentence semantic similarity model from the Sentence Transformers library  (Reimers and Gurevych,  2019 ) , which ranks first among their suite of models based on average performance in semantic search and sentence embedding. For the  general instructions  and  dialog  splits, we use a threshold of 0.9, while for the  safety  split, we use a threshold of 0.95 to prune the data. Note that for pruning we only compare similarities between user instructions. In case of the  dialog  split, we concatenate user instructions and consider it as a single instance for pruning. Table  1  shows the statistics of our final proposed dataset, and Figure  10  illustrates the distribution of the nearest neighbor examples in the dataset for each split before and after pruning.",
            "Figures  11  and  12  demonstrate a comparison of two responses generated by our model versus llama3-8b-instruct and their corresponding scores given by our G-Eval based metrics. Note that the helpfulness metrics are measured on the general split examples and the safety metrics are measured on the safety split of the data.",
            "In order to compare the helpfulness and safety of the two models given a judge LLM, we use the prompts given in  13  and  14  respectively. These prompts are designed to evaluate the performance of the models throughout the full multi-turn interaction with the user. Given the same set of queries from a user we run those queries through two separate models and record the full conversation. Then we will feed the conversations into the given prompts in  assistant-a-conv  and  assistant-b-conv  place holders. In order to mitigate position bias and make sure the judge LLM would not get biased towards which model comes first or last we switch the two conversations and run the judge LLM again. If the judgements among the two runs contradict each other, we call it a tie. A model is only the winner for an example test case when the judge elects it as the winner in both of the runs.",
            "We use 25 percent of the safety split of our data and set a rank of 128 and alpha of 256 for the LoRA adaptor and apply it on all linear modules according to the ablation studies we conduct in appendix  D.1  and  D.2 .",
            "In this section we analyze the effect of the safety data splits size and dialog data on the overall performance and safety of the resulting models. To do so, we build four training datasets each containing 25%, 50%, 75% and 100% of the safety data. For each of the datasets we also create two variants: one with the dialog split and one without the dialog split which is noted by  single . We follow the same training procedure for all the models and measure the G-Eval scores with GPT-4o references. Figure  18  demonstrates the results.",
            "We experiment with different LoRA architectures in order to find the best setup for our problem. We apply LoRA adaptors on all of the linear transformations in the network. It is a good practice to set an alpha twice the size of rank. So we set perform three experiments with (r=32, alpha=64), (r=64, alpha=128), (r=128, alpha=256) and (r=256, alpha=512) and also try different rank to alpha ratios: (r=256, alpha=256) and (r=512, alpha=256). Figure  19  summarizes our results on the held-out test set. We observe that the model with alpha=256 and r=128 outperforms the other structures on both safety and helpfulness."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparison of the model performances across four metrics. Best model results are bolded and second best results are underlined.",
        "table": "S4.T2.1.1",
        "footnotes": [],
        "references": [
            "For the second step, we use a conditional generator prompt which takes a random topic from our pool of selected topics, tries to generate 50 sub-topics, and picks one randomly (the randomness is enforced by the prompt generator) this ensures that we uniformly sample from different topics and sub-topics. The LLM is then asked to write a challenging question about the chosen topic and sub-topic. (Appendix  A.2  explains the prompt details.) In the last stage, we post-process the generated response, extract the question, and prompt the LLM separately to obtain the response. The reason behind multiple LLM calls, rather than asking for both the question and response in a single call, is that we observed when the LLM is prompted for both, the responses are shorter and less helpful than when the question is asked separately. We refer to this proportion of the data as the  general instructions split . Figure  2  demonstrates the pipeline of stage 2 and 3.",
            "To begin with, we utilized the dataset provided by  (Bagalkotkar et al.,  2024 ) , which consists of around 10K non-compliant queries  2 2 2 Here we use the term non-compliant to refer to queries that can lead the model to generate non-compliant behavior. . We also used the classifier they trained on their dataset and ran it over the dataset to collect examples that were most certainly classified as non-compliant. Afterward, we designed a prompt (detailed in appendix  A.2.2 ) to force the model to regard the input query as a potential non-compliance and follow the following desired safety behavior:",
            "Since it is also important for the model to interact with users in a natural, multi-turn conversational setup, we generated a set of multi-turn interactions. To do this, we followed a similar approach to Section  3.1 , but instead of making two calls to the LLM, we asked it to generate a long conversation in a single call, and we post-processed the conversations afterward. (Details and prompts used in this stage are explained in appendix  A.2.3 .) We refer to this proportion of the data as the  dialog split .",
            "We use LoRA  (Hu et al.,  2021 )  adaptors to fine-tune llama3-8b-instruct on our proposed dataset. We fine-tune the model for 5 epochs or until the validation loss on 200 held out examples from general instruct split ceases to decrease. Additionally, we hold out 200 examples from each data split for further testing of performance and safety. (More information about the training setup and LoRA configurations used can be found in appendix  C .) We also perform an ablation study of the effect of the dialog split and the size of the safety data in  D.1  and different LoRA adaptor sizes (as reported in the appendix  D.2 ).",
            "We compare our model versus the baselines on the held-out test data. Table  2  shows the average score of each model across the test splits on our four proposed metrics. First, we observe that our model outperforms all baselines except GPT-4o on the helpfulness metric, and in the case of having no reference, it even outperforms GPT-4o. Second, on the safety dimensionparticularly the \"without reference\" metric, which purely measures the models safetyour model outperforms all open-source LLaMA-3 baselines, although it falls short of GPT-4 and GPT-4o. The \"safety with reference\" metric is highest for our model, indicating its superior performance in following the defined safety behavior. Comparing with the base model, LLaMA3-8b-instruct, we observe that not only did we enhance its safety and compliance, but we also significantly filled its knowledge gap in the real estate domain.",
            "Figure  3  depicts the range of scores that each of the models get on each of our proposed G-Eval metrics. It can be seen that the metrics with references better capture the nuances in the answers as they are able to compare with a ground truth. This is while there is a low variance in the scores given by reference less responses. Therefore, we also compare the head-to-head win rate of the models according to their metric scores for each test case. We set a threshold of 1% to highlight more significant win/lose rates. That is, if two models scores fall within one percent of each other, we call it a tie. Figure  4  illustrates this comparison. Each cell represents the win rate of the left hand model versus the top model. Note that the scores wouldnt sum up to 100 since there are also ties. On helpfulness with reference, our model beats all of the baselines except GPT-4o which there is a win rate of 34%, lose rate of 38% and 28% ties. This is intuitive as the ground truth responses are also given by GPT-4o. On the safety with reference, our model significantly outperforms the baselines but you can see that when there are no references and the responses are solely evaluated based on evaluator models knowledge, most of the scores are fairly close to each other. However, we can see that our proposed model outperforms the base llama3-8b model by a significant margin and wins 42% of the times while loosing 8% and getting ties 50% of the times. In appendix  B.1.2  you can find some example evaluations.",
            "Inspired by MT-Bench  (Zheng et al.,  2023 ) , we developed two judge prompts to assess and judge the best model on helpfulness and safety respectively. We use GPT-4o as the judge LLM for this comparison. Assuming that the user is going to interact with the system with a set of fixed queries, we generate responses to those queries using two different models and then ask the judge LLM to choose the best model-based on the criteria. (Appendix  B.2  outlines the prompts used for building the judge LLM and brings some example judgements.)",
            "Table  3  summarizes the performance comparison of our proposed model versus baselines on both benchmarks. Our proposed model significantly outperforms the baselines on safety and is preferred over all baselines in helpfulness except GPT-4o. (Judging examples of both safety and helpfulness along with more details can be found in appendix  B.2 .)",
            "Figures  11  and  12  demonstrate a comparison of two responses generated by our model versus llama3-8b-instruct and their corresponding scores given by our G-Eval based metrics. Note that the helpfulness metrics are measured on the general split examples and the safety metrics are measured on the safety split of the data.",
            "We use 25 percent of the safety split of our data and set a rank of 128 and alpha of 256 for the LoRA adaptor and apply it on all linear modules according to the ablation studies we conduct in appendix  D.1  and  D.2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Head to head comparison of the performance on our two proposed benchmarks. If the win column is bolded it represents that our model is superior. If the lose column is bolded it means that the other model has a higher win rate",
        "table": "S4.T3.1.1",
        "footnotes": [],
        "references": [
            "Our results show that by focusing on compliance-specific data and tuning, we can significantly improve both the safety and helpfulness of LLMs in real estate applications. Section  3  will go over the process of generating the synthetic dataset. In section  4  we discuss our fine-tuning approach and section  5  will go over our evaluation setup and results.",
            "Since it is also important for the model to interact with users in a natural, multi-turn conversational setup, we generated a set of multi-turn interactions. To do this, we followed a similar approach to Section  3.1 , but instead of making two calls to the LLM, we asked it to generate a long conversation in a single call, and we post-processed the conversations afterward. (Details and prompts used in this stage are explained in appendix  A.2.3 .) We refer to this proportion of the data as the  dialog split .",
            "Figure  3  depicts the range of scores that each of the models get on each of our proposed G-Eval metrics. It can be seen that the metrics with references better capture the nuances in the answers as they are able to compare with a ground truth. This is while there is a low variance in the scores given by reference less responses. Therefore, we also compare the head-to-head win rate of the models according to their metric scores for each test case. We set a threshold of 1% to highlight more significant win/lose rates. That is, if two models scores fall within one percent of each other, we call it a tie. Figure  4  illustrates this comparison. Each cell represents the win rate of the left hand model versus the top model. Note that the scores wouldnt sum up to 100 since there are also ties. On helpfulness with reference, our model beats all of the baselines except GPT-4o which there is a win rate of 34%, lose rate of 38% and 28% ties. This is intuitive as the ground truth responses are also given by GPT-4o. On the safety with reference, our model significantly outperforms the baselines but you can see that when there are no references and the responses are solely evaluated based on evaluator models knowledge, most of the scores are fairly close to each other. However, we can see that our proposed model outperforms the base llama3-8b model by a significant margin and wins 42% of the times while loosing 8% and getting ties 50% of the times. In appendix  B.1.2  you can find some example evaluations.",
            "Table  3  summarizes the performance comparison of our proposed model versus baselines on both benchmarks. Our proposed model significantly outperforms the baselines on safety and is preferred over all baselines in helpfulness except GPT-4o. (Judging examples of both safety and helpfulness along with more details can be found in appendix  B.2 .)",
            "Prior work extensively investigate the correlation between human judges and human preferences in measuring the helpfulness of responses  (Zheng et al.,  2023 ) . In this work, we extend this approach by evaluating the correlation between our safety judge with human safety preference. To achieve this, we asked four annotators, including two legal experts to rank the responses generated by our model against three baseline modelsllama3-8b, llama3-70b, and GPT-4over our proposed safety benchmark. We measured a high correlation of 95.56% between human annotators and our safety judges with an average Cohens Kappa of 0.81 between pairs of annotators. More details about the process can be found in appendix  B.3 .",
            "In order to compare the helpfulness and safety of the two models given a judge LLM, we use the prompts given in  13  and  14  respectively. These prompts are designed to evaluate the performance of the models throughout the full multi-turn interaction with the user. Given the same set of queries from a user we run those queries through two separate models and record the full conversation. Then we will feed the conversations into the given prompts in  assistant-a-conv  and  assistant-b-conv  place holders. In order to mitigate position bias and make sure the judge LLM would not get biased towards which model comes first or last we switch the two conversations and run the judge LLM again. If the judgements among the two runs contradict each other, we call it a tie. A model is only the winner for an example test case when the judge elects it as the winner in both of the runs."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  List of all topics used for data generation.",
        "table": "A1.T4.1.1",
        "footnotes": [],
        "references": [
            "Our results show that by focusing on compliance-specific data and tuning, we can significantly improve both the safety and helpfulness of LLMs in real estate applications. Section  3  will go over the process of generating the synthetic dataset. In section  4  we discuss our fine-tuning approach and section  5  will go over our evaluation setup and results.",
            "To ensure a dataset of diverse instructions and responses while avoiding semantically and lexically duplicate instructions, we aim to prune the data. This is particularly important when holding out a set of examples for evaluating our final tuned models, as we want to avoid having leaked examples from the training set in the evaluation set. We iterate over all the examples in each split of the data and remove those with a similarity above a certain threshold. Algorithm  1  outlines the procedure for pruning the data. (More details of the model and configurations we use for pruning can be found in appendix  A.4 .)",
            "Figure  3  depicts the range of scores that each of the models get on each of our proposed G-Eval metrics. It can be seen that the metrics with references better capture the nuances in the answers as they are able to compare with a ground truth. This is while there is a low variance in the scores given by reference less responses. Therefore, we also compare the head-to-head win rate of the models according to their metric scores for each test case. We set a threshold of 1% to highlight more significant win/lose rates. That is, if two models scores fall within one percent of each other, we call it a tie. Figure  4  illustrates this comparison. Each cell represents the win rate of the left hand model versus the top model. Note that the scores wouldnt sum up to 100 since there are also ties. On helpfulness with reference, our model beats all of the baselines except GPT-4o which there is a win rate of 34%, lose rate of 38% and 28% ties. This is intuitive as the ground truth responses are also given by GPT-4o. On the safety with reference, our model significantly outperforms the baselines but you can see that when there are no references and the responses are solely evaluated based on evaluator models knowledge, most of the scores are fairly close to each other. However, we can see that our proposed model outperforms the base llama3-8b model by a significant margin and wins 42% of the times while loosing 8% and getting ties 50% of the times. In appendix  B.1.2  you can find some example evaluations.",
            "We use GPT-3.5-turbo and generate 10,000 responses for expert analysis. After post-processing the responses and analyzing the topics and sub-topics, we end up with around 500 topics. We manually clean the list of topics, removing redundant ones and in some cases adding some that are not covered which results in a compiled list of 90 topics. Table  4  shows the final list of topics for both dialog and the general instructions split. You can see a diagram of top-15 topics along with their top-5 sub-topics in figure  5 .",
            "In order to compare the helpfulness and safety of the two models given a judge LLM, we use the prompts given in  13  and  14  respectively. These prompts are designed to evaluate the performance of the models throughout the full multi-turn interaction with the user. Given the same set of queries from a user we run those queries through two separate models and record the full conversation. Then we will feed the conversations into the given prompts in  assistant-a-conv  and  assistant-b-conv  place holders. In order to mitigate position bias and make sure the judge LLM would not get biased towards which model comes first or last we switch the two conversations and run the judge LLM again. If the judgements among the two runs contradict each other, we call it a tie. A model is only the winner for an example test case when the judge elects it as the winner in both of the runs.",
            "We observe that the best performing models with and without dialog data (llama3-8b-25p and llama3-8b-50p-single) achieve around the same helpfulness scores while the model trained without dialog data performs slightly better. This was expected since the test data only consists of single turn instruction following and the presence of dialog data can deteriorate the helpfulness of the model while improving the multi-turn functionality and conversationality of the model. To test this hypothesis, we also perform the head-to-head comparison of these two selected models as outlined in section  5.4 . We observe that the model trained with dialog data wins 37.07% of the times over the model without dilaog data on helpfulness dimension while loosing only 15.95% times. However, we also noticed that on safety dimension, it wins 6% and looses 14% of the times while most of the times (80%) they tie. This led us to choose the llama3-8b-25p model as our final model as it had a good balance between safety and helpfulness in multi-turn interactions."
        ]
    }
}