{
    "PAPER'S NUMBER OF TABLES": 1,
    "S3.T1": {
        "caption": "Table 1: Types of gradient inversion attacks employed to evaluate our proposed defense strategy.",
        "table": "<table id=\"S3.T1.3.3\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.3.3.4.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.3.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Attack Name</th>\n<th id=\"S3.T1.3.3.4.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Main Objective Function</th>\n<th id=\"S3.T1.3.3.4.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Description</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">2-norm</td>\n<td id=\"S3.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<math id=\"S3.T1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"g^{l2}\" display=\"inline\"><semantics id=\"S3.T1.1.1.1.1.m1.1a\"><msup id=\"S3.T1.1.1.1.1.m1.1.1\" xref=\"S3.T1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T1.1.1.1.1.m1.1.1.2\" xref=\"S3.T1.1.1.1.1.m1.1.1.2.cmml\">g</mi><mrow id=\"S3.T1.1.1.1.1.m1.1.1.3\" xref=\"S3.T1.1.1.1.1.m1.1.1.3.cmml\"><mi id=\"S3.T1.1.1.1.1.m1.1.1.3.2\" xref=\"S3.T1.1.1.1.1.m1.1.1.3.2.cmml\">l</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.T1.1.1.1.1.m1.1.1.3.1\" xref=\"S3.T1.1.1.1.1.m1.1.1.3.1.cmml\">​</mo><mn id=\"S3.T1.1.1.1.1.m1.1.1.3.3\" xref=\"S3.T1.1.1.1.1.m1.1.1.3.3.cmml\">2</mn></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.1.1.1.1.m1.1b\"><apply id=\"S3.T1.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T1.1.1.1.1.m1.1.1\">superscript</csymbol><ci id=\"S3.T1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T1.1.1.1.1.m1.1.1.2\">𝑔</ci><apply id=\"S3.T1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S3.T1.1.1.1.1.m1.1.1.3\"><times id=\"S3.T1.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"S3.T1.1.1.1.1.m1.1.1.3.1\"></times><ci id=\"S3.T1.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"S3.T1.1.1.1.1.m1.1.1.3.2\">𝑙</ci><cn type=\"integer\" id=\"S3.T1.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"S3.T1.1.1.1.1.m1.1.1.3.3\">2</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.1.1.1.1.m1.1c\">g^{l2}</annotation></semantics></math> (<a href=\"#S4.E11\" title=\"In 4.1 Inversion Attack Optimisation Algorithms ‣ 4 Experimental framework ‣ Enhancing Privacy against Inversion Attacks in Federated Learning by using Mixing Gradients Strategies\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>)</td>\n<td id=\"S3.T1.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">Euclidean distance and initial label determination.</td>\n</tr>\n<tr id=\"S3.T1.2.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.2.2.2\" class=\"ltx_td ltx_align_left\">Angle &amp; var</td>\n<td id=\"S3.T1.2.2.2.1\" class=\"ltx_td ltx_align_left\">\n<math id=\"S3.T1.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"g^{ang}\" display=\"inline\"><semantics id=\"S3.T1.2.2.2.1.m1.1a\"><msup id=\"S3.T1.2.2.2.1.m1.1.1\" xref=\"S3.T1.2.2.2.1.m1.1.1.cmml\"><mi id=\"S3.T1.2.2.2.1.m1.1.1.2\" xref=\"S3.T1.2.2.2.1.m1.1.1.2.cmml\">g</mi><mrow id=\"S3.T1.2.2.2.1.m1.1.1.3\" xref=\"S3.T1.2.2.2.1.m1.1.1.3.cmml\"><mi id=\"S3.T1.2.2.2.1.m1.1.1.3.2\" xref=\"S3.T1.2.2.2.1.m1.1.1.3.2.cmml\">a</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.T1.2.2.2.1.m1.1.1.3.1\" xref=\"S3.T1.2.2.2.1.m1.1.1.3.1.cmml\">​</mo><mi id=\"S3.T1.2.2.2.1.m1.1.1.3.3\" xref=\"S3.T1.2.2.2.1.m1.1.1.3.3.cmml\">n</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.T1.2.2.2.1.m1.1.1.3.1a\" xref=\"S3.T1.2.2.2.1.m1.1.1.3.1.cmml\">​</mo><mi id=\"S3.T1.2.2.2.1.m1.1.1.3.4\" xref=\"S3.T1.2.2.2.1.m1.1.1.3.4.cmml\">g</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.2.2.2.1.m1.1b\"><apply id=\"S3.T1.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T1.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T1.2.2.2.1.m1.1.1\">superscript</csymbol><ci id=\"S3.T1.2.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T1.2.2.2.1.m1.1.1.2\">𝑔</ci><apply id=\"S3.T1.2.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T1.2.2.2.1.m1.1.1.3\"><times id=\"S3.T1.2.2.2.1.m1.1.1.3.1.cmml\" xref=\"S3.T1.2.2.2.1.m1.1.1.3.1\"></times><ci id=\"S3.T1.2.2.2.1.m1.1.1.3.2.cmml\" xref=\"S3.T1.2.2.2.1.m1.1.1.3.2\">𝑎</ci><ci id=\"S3.T1.2.2.2.1.m1.1.1.3.3.cmml\" xref=\"S3.T1.2.2.2.1.m1.1.1.3.3\">𝑛</ci><ci id=\"S3.T1.2.2.2.1.m1.1.1.3.4.cmml\" xref=\"S3.T1.2.2.2.1.m1.1.1.3.4\">𝑔</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.2.2.2.1.m1.1c\">g^{ang}</annotation></semantics></math> + TV (<a href=\"#S4.E12\" title=\"In 4.1 Inversion Attack Optimisation Algorithms ‣ 4 Experimental framework ‣ Enhancing Privacy against Inversion Attacks in Federated Learning by using Mixing Gradients Strategies\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>)</td>\n<td id=\"S3.T1.2.2.2.3\" class=\"ltx_td ltx_align_left\">\n<cite class=\"ltx_cite ltx_citemacro_citet\">Geiping et al. (<a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">2020</a>)</cite> proposed to leverage cosine\nsimilarity, total variation (TV) and initial label determination.</td>\n</tr>\n<tr id=\"S3.T1.3.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.3.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_b\">Angle &amp; var &amp; Orth_regulators</td>\n<td id=\"S3.T1.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_b\">\n<math id=\"S3.T1.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"g^{ang}\" display=\"inline\"><semantics id=\"S3.T1.3.3.3.1.m1.1a\"><msup id=\"S3.T1.3.3.3.1.m1.1.1\" xref=\"S3.T1.3.3.3.1.m1.1.1.cmml\"><mi id=\"S3.T1.3.3.3.1.m1.1.1.2\" xref=\"S3.T1.3.3.3.1.m1.1.1.2.cmml\">g</mi><mrow id=\"S3.T1.3.3.3.1.m1.1.1.3\" xref=\"S3.T1.3.3.3.1.m1.1.1.3.cmml\"><mi id=\"S3.T1.3.3.3.1.m1.1.1.3.2\" xref=\"S3.T1.3.3.3.1.m1.1.1.3.2.cmml\">a</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.T1.3.3.3.1.m1.1.1.3.1\" xref=\"S3.T1.3.3.3.1.m1.1.1.3.1.cmml\">​</mo><mi id=\"S3.T1.3.3.3.1.m1.1.1.3.3\" xref=\"S3.T1.3.3.3.1.m1.1.1.3.3.cmml\">n</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.T1.3.3.3.1.m1.1.1.3.1a\" xref=\"S3.T1.3.3.3.1.m1.1.1.3.1.cmml\">​</mo><mi id=\"S3.T1.3.3.3.1.m1.1.1.3.4\" xref=\"S3.T1.3.3.3.1.m1.1.1.3.4.cmml\">g</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.3.3.3.1.m1.1b\"><apply id=\"S3.T1.3.3.3.1.m1.1.1.cmml\" xref=\"S3.T1.3.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.3.3.3.1.m1.1.1.1.cmml\" xref=\"S3.T1.3.3.3.1.m1.1.1\">superscript</csymbol><ci id=\"S3.T1.3.3.3.1.m1.1.1.2.cmml\" xref=\"S3.T1.3.3.3.1.m1.1.1.2\">𝑔</ci><apply id=\"S3.T1.3.3.3.1.m1.1.1.3.cmml\" xref=\"S3.T1.3.3.3.1.m1.1.1.3\"><times id=\"S3.T1.3.3.3.1.m1.1.1.3.1.cmml\" xref=\"S3.T1.3.3.3.1.m1.1.1.3.1\"></times><ci id=\"S3.T1.3.3.3.1.m1.1.1.3.2.cmml\" xref=\"S3.T1.3.3.3.1.m1.1.1.3.2\">𝑎</ci><ci id=\"S3.T1.3.3.3.1.m1.1.1.3.3.cmml\" xref=\"S3.T1.3.3.3.1.m1.1.1.3.3\">𝑛</ci><ci id=\"S3.T1.3.3.3.1.m1.1.1.3.4.cmml\" xref=\"S3.T1.3.3.3.1.m1.1.1.3.4\">𝑔</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.3.3.3.1.m1.1c\">g^{ang}</annotation></semantics></math> + TV + Orth</td>\n<td id=\"S3.T1.3.3.3.3\" class=\"ltx_td ltx_align_left ltx_border_b\">Cosine distance with orthogonal regulator for the input + initial label determination. <cite class=\"ltx_cite ltx_citemacro_citep\">(Qian et al., <a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "A typical classification architecture uses softmax, ",
                "p",
                "k",
                "=",
                "e",
                "o",
                "k",
                "∑",
                "j",
                "e",
                "o",
                "j",
                "subscript",
                "𝑝",
                "𝑘",
                "superscript",
                "𝑒",
                "subscript",
                "𝑜",
                "𝑘",
                "subscript",
                "𝑗",
                "superscript",
                "𝑒",
                "subscript",
                "𝑜",
                "𝑗",
                "p_{k}=\\frac{e^{o_{k}}}{\\sum_{j}{e^{o_{j}}}}",
                ", followed by cross-entropy to obtain the loss:",
                "where, ",
                "C",
                "𝐶",
                "C",
                " is the number of classes/categories. The derivative of ",
                "p",
                "k",
                "subscript",
                "𝑝",
                "𝑘",
                "p_{k}",
                " with respect to each ",
                "o",
                "j",
                "subscript",
                "𝑜",
                "𝑗",
                "o_{j}",
                ":",
                "The loss set of equations is then obtained:",
                "and:",
                "Specifically, the gradients are shared accurately for a single input batch, ",
                "B",
                "=",
                "1",
                "𝐵",
                "1",
                "B=1",
                ", as also observed by ",
                "Qian et al. (",
                "2021",
                ")",
                ".\nThe number of gradient equations are ",
                "n",
                "​",
                "C",
                "+",
                "C",
                "𝑛",
                "𝐶",
                "𝐶",
                "nC+C",
                " with an extra ",
                "C",
                "𝐶",
                "C",
                " equations for weights (for each ",
                "j",
                "𝑗",
                "j",
                ") whilst the unknowns are ",
                "n",
                "+",
                "C",
                "𝑛",
                "𝐶",
                "n+C",
                ". For example ",
                "x",
                "i",
                "subscript",
                "𝑥",
                "𝑖",
                "x_{i}",
                " can be found from any ",
                "j",
                "𝑗",
                "j",
                ", using ",
                "∂",
                "l",
                "∂",
                "w",
                "i",
                "​",
                "j",
                "=",
                "k",
                "/",
                "∂",
                "l",
                "∂",
                "b",
                "j",
                "=",
                "k",
                "=",
                "x",
                "i",
                "𝑙",
                "subscript",
                "𝑤",
                "𝑖",
                "𝑗",
                "𝑘",
                "𝑙",
                "subscript",
                "𝑏",
                "𝑗",
                "𝑘",
                "subscript",
                "𝑥",
                "𝑖",
                "\\frac{\\partial{l}}{\\partial w_{ij=k}}/\\frac{\\partial{l}}{\\partial b_{j=k}}=x_{i}",
                ".\nAlthough this shows the vulnerability of a linear layers in an FL model, when the mini-batch size is larger than one, the client would only share the averaged information:",
                "As no additional equations are shared, the number of unknown, ",
                "B",
                "​",
                "(",
                "n",
                "+",
                "C",
                ")",
                "𝐵",
                "𝑛",
                "𝐶",
                "B(n+C)",
                " can exceed the gradients equations number ",
                "n",
                "​",
                "C",
                "+",
                "C",
                "𝑛",
                "𝐶",
                "𝐶",
                "nC+C",
                ", and there will be no unique solution to solve the set of equations. Even in the case that a unique solution exists, numerical optimisation can be challenging. However, in the scenario in which softmax is followed by cross entropy, we show that an accurate direct solution can be found in many cases even for ",
                "B",
                "≫",
                "1",
                "much-greater-than",
                "𝐵",
                "1",
                "B\\gg 1",
                ", due to the de-mixing property across the batch.\nIn an untrained, randomised weights model, the first order expected value of ",
                "⟨",
                "p",
                "j",
                "m",
                "−",
                "y",
                "j",
                "m",
                "⟩",
                "delimited-⟨⟩",
                "subscript",
                "superscript",
                "𝑝",
                "𝑚",
                "𝑗",
                "subscript",
                "superscript",
                "𝑦",
                "𝑚",
                "𝑗",
                "\\langle p^{m}_{j}-y^{m}_{j}\\rangle",
                ", is positive but close to zero for a non-target instance (",
                "j",
                "≠",
                "c",
                "𝑗",
                "𝑐",
                "j\\neq c",
                ") and close to ",
                "−",
                "1",
                "1",
                "-1",
                " for the instance target (",
                "j",
                "=",
                "c",
                "𝑗",
                "𝑐",
                "j=c",
                "). This is due to the fact that expected value, ",
                "⟨",
                "p",
                "j",
                "​",
                "(",
                "o",
                "j",
                ")",
                "⟩",
                "delimited-⟨⟩",
                "subscript",
                "𝑝",
                "𝑗",
                "subscript",
                "𝑜",
                "𝑗",
                "\\langle p_{j}(o_{j})\\rangle",
                " can be grossly estimated as ",
                "p",
                "j",
                "​",
                "(",
                "E",
                "​",
                "(",
                "o",
                "j",
                ")",
                ")",
                "subscript",
                "𝑝",
                "𝑗",
                "𝐸",
                "subscript",
                "𝑜",
                "𝑗",
                "p_{j}(E(o_{j}))",
                " for first order Taylor expansion (see also ",
                "(Daunizeau, ",
                "2017",
                ")",
                "). Therefore, this results in ",
                "⟨",
                "p",
                "j",
                "⟩",
                "delimited-⟨⟩",
                "subscript",
                "𝑝",
                "𝑗",
                "\\langle p_{j}\\rangle",
                " to be inversely proportional to the number of classes ",
                "C",
                "𝐶",
                "C",
                ".\nSubsequently, a batch that contains non-similar labels, ",
                "c",
                "m",
                "≠",
                "c",
                "1",
                "​",
                "…",
                "​",
                "B",
                "superscript",
                "𝑐",
                "𝑚",
                "superscript",
                "𝑐",
                "1",
                "…",
                "𝐵",
                "c^{m}\\neq c^{1...B}",
                ", Eq. (",
                "6",
                ")-(",
                "7",
                ") can be estimated as:",
                "This approximation shows that the de-mixing of the gradients for each vector enables a direct estimate of the input layer for any vector ",
                "m",
                "𝑚",
                "m",
                " in the batch when we pick ",
                "j",
                "=",
                "c",
                "𝑗",
                "𝑐",
                "j=c",
                ". The error of this estimation can be very low for large ",
                "C",
                "𝐶",
                "C",
                ". It is clearly seen empirically with the two most popular data-sets used for studying such inversion attacks, the MNIST with small number of equations ",
                "C",
                "𝐶",
                "C",
                " and LFW with large ",
                "C",
                "𝐶",
                "C",
                " (",
                "C",
                "≫",
                "B",
                "much-greater-than",
                "𝐶",
                "𝐵",
                "C\\gg B",
                " ).",
                "Figure ",
                "1",
                "(a)-(c) shows our estimates to direct invert the input of a dense layer from a vector batch, and the ability to infer all inputs of a batch in a dense layer. It supports that inverting the 2-8 vectors is likely to be possible with a very small error, as long as labels are unique. In the case of the LFW dataset, due to the large ",
                "C",
                "𝐶",
                "C",
                ", the error is inversely proportional to ",
                "C",
                "𝐶",
                "C",
                ", so the two vectors in the batch are recovered for all random samples tested with a very low error. This low error is obtained so long as the vectors have a unique class, given ",
                "C",
                "≫",
                "B",
                "much-greater-than",
                "𝐶",
                "𝐵",
                "C\\gg B",
                ", which is the case for the LFW classification network with much larger batch sizes (e.g. ",
                "B",
                "≫",
                "8",
                ")",
                "B\\gg 8)",
                ".",
                "The de-mixing property of cross-entropy is not only helpful for estimating input without numerical optimisation, but also allows simple numerical convergence as the dimensionality for searching a solution is effectively reduced. This can be inferred from Eq. (",
                "8",
                "). If for a given ",
                "j",
                "=",
                "c",
                "𝑗",
                "𝑐",
                "j=c",
                ", ",
                "x",
                "i",
                "subscript",
                "𝑥",
                "𝑖",
                "x_{i}",
                " only belongs to a single ",
                "m",
                "=",
                "M",
                "𝑚",
                "𝑀",
                "m=M",
                ", the search is constrained to a one dimension parabola (in the case of a squared distance objective function), as other ",
                "m",
                "≠",
                "M",
                "𝑚",
                "𝑀",
                "m\\neq M",
                " will not influence the right term in Eq. (",
                "8",
                "). Yet, once we draw similar labels in the batch the recovery of data exhibits a large error, and the error is within the order of magnitude of the information (",
                "∝",
                "x",
                "i",
                "proportional-to",
                "absent",
                "subscript",
                "𝑥",
                "𝑖",
                "\\propto x_{i}",
                "). This mixing of gradients can serve as a strategy to increase privacy in FL against direct inversion and our results in section ",
                "4.1",
                " show that the strategy is also effective against numerical optimisation attacks.",
                "Following this insight, we can also consider changing the objective function to mix the gradients. Instead of using cross-entropy loss (CEL), it is possible to use the mean squared loss (MSE), ",
                "l",
                "2",
                "​",
                "(",
                "o",
                ",",
                "y",
                ")",
                "=",
                "−",
                "∑",
                "k",
                "C",
                "(",
                "y",
                "k",
                "−",
                "o",
                "k",
                ")",
                "2",
                "subscript",
                "𝑙",
                "2",
                "𝑜",
                "𝑦",
                "superscript",
                "subscript",
                "𝑘",
                "𝐶",
                "superscript",
                "subscript",
                "𝑦",
                "𝑘",
                "subscript",
                "𝑜",
                "𝑘",
                "2",
                "l_{2}(o,y)=-\\sum_{k}^{C}{(y_{k}-o_{k})}^{2}",
                ". This is not a typical choice for a classification task, but performance results show later that there is little to an unnoticeable degradation in classification performance when using MSE instead of CEL. On the contrary, there is a large gain in privacy by the high mixing of gradients on the dense layer. The gradients are calculated on a dense layer using mean square estimation (for simplicity with no softmax):",
                "and",
                "Here ",
                "o",
                "j",
                "subscript",
                "𝑜",
                "𝑗",
                "o_{j}",
                " can take positive or negative values and generally within the order of ",
                "y",
                "𝑦",
                "y",
                ", for ",
                "j",
                "=",
                "c",
                "𝑗",
                "𝑐",
                "j=c",
                " or ",
                "j",
                "≠",
                "c",
                "𝑗",
                "𝑐",
                "j\\neq c",
                ", hence the average of a batch will not de-mix the gradients. Note that we are also using softmax followed by MSE for comparison, and the gradients will also obtain a strong gradient mixing (but a less trivial gradient expression).\nWe show in figure ",
                "2",
                " a sample of results for the error on the direct inversion of a batch. We observe that the error on estimation of any vector using ",
                "∂",
                "w",
                "∂",
                "b",
                "𝑤",
                "𝑏",
                "\\frac{\\partial w}{\\partial b}",
                " is not negligible, and sufficiently large even in batch size of 2 and more distinctive in batch size of 8.",
                "We see here that potentially drawing input with similar labels and adjusting the loss function to increase mixing of gradients are strategies that counter the recovery of input from dense layers. In fact, we can compare our result to the effectiveness of adding noise as a defensive strategy given its wide application. We add a Gaussian noise term for all gradients in a linear layer, ",
                "∂",
                "l",
                "∂",
                "w",
                "i",
                ",",
                "j",
                "+",
                "ζ",
                "i",
                ",",
                "j",
                ",",
                "∂",
                "l",
                "∂",
                "b",
                "j",
                "+",
                "ζ",
                "j",
                "𝑙",
                "subscript",
                "𝑤",
                "𝑖",
                "𝑗",
                "subscript",
                "𝜁",
                "𝑖",
                "𝑗",
                "𝑙",
                "subscript",
                "𝑏",
                "𝑗",
                "subscript",
                "𝜁",
                "𝑗",
                "\\frac{\\partial{l}}{\\partial w_{i,j}}+\\zeta_{i,j},\\frac{\\partial{l}}{\\partial b_{j}}+\\zeta_{j}",
                ".",
                "Figure ",
                "3",
                " shows the results from addition of noise at various standard deviations. We find that small contamination of noise does not protect against inversion and the error is in the order of the noise, and given that, we are required the noise to be in the order of the weights (",
                "s",
                "​",
                "t",
                "​",
                "d",
                ">",
                "0.01",
                "𝑠",
                "𝑡",
                "𝑑",
                "0.01",
                "std>0.01",
                ", with weights initialised uniformly between ",
                "(",
                "−",
                "0.5",
                ",",
                "0.5",
                ")",
                "0.5",
                "0.5",
                "(-0.5,0.5)",
                "). The addition of such a noise will affect the training drastically. In fact, this exercise shows that our gradient mixing strategies can be as effective as the addition of a large noise term, without the loss of training performance as we show in section ",
                "5",
                ".",
                "To further support our gradient mixing strategies, in section ",
                "5",
                " we carry out an analysis of widely used inversion attacks that use numerical optimisation. We do this by firstly analysing inversion attack success in a dense layer model for simplicity, and then show its validity for a typically explored convolutional network in an inversion attack context."
            ]
        ]
    }
}