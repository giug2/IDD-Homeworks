{
    "id_table_1": {
        "caption": "Table 1:  Quantitative comparison with baselines on image-to-3D synthesis.",
        "table": "S4.T1.7",
        "footnotes": [],
        "references": [
            "The goal of 3D generative models is to empower artists and even beginners to effortlessly convert their design concepts into 3D models.  Consider the input image in Fig.  1 . A skilled craftsman can, through a blend of skills and creativity, convert a 2D concept image into an exquisite 3D model. This creative process can originate from artists pure imagination or, more commonly, through examining one or more existing 3D models as a source of inspiration  (Bob,  2022 ; Carvajal,  2023 ) . Artists often refer to these pre-existing 3D models to improve the modeling quality. The question then arises: could we develop a reference-based 3D generative model that can replicate this capability?",
            "To address these challenges, we propose to take 3D models as additional inputs to guide the generation, inspired by the success in retrieval augmented generation (RAG) for language  (Lewis et al.,  2020 )  and image  (Sheynin et al.,  2022 ) .  Given an input image and a reference 3D model, we present  Phidias , a novel reference-augmented diffusion model that unifies 3D generation from text, image, and 3D conditions. As shown in Fig.  1 , the reference 3D model would help 1)  improve quality  by alleviating ambiguity with richer information for unseen views, 2)  enhance generalization capacity  by serving as a shape template or an external memory for generative models, and 3)  provide controllability  by indicating desired shape patterns and geometric styles.",
            "Quantitative Results.   Following previous works, we conduct quantitative evaluation on google scanned objects (GSO)  (Downs et al.,  2022 ) . We remove duplicated objects with the same shape and randomly select 200 objects for evaluation.   For visual quality , we report reconstruction metrics (PSNR, SSIM and LPIPS) on 20 novel views. We also report novel views CLIP similarity with paired GT (CLIP-P) and input image (CLIP-I).  For geometry quality , we sample 50K points from mesh surface and compute Chamfer Distance (CD) and F-Score (with a threshold of 0.05). To align the generated mesh and GT, we unify their coordinate systems and re-scale them into a unit box.  We report our results with the retrieved reference,  i.e., Ours (Retrieved Ref.) , and GT mesh as reference,  i.e., Ours (GT Ref.) , respectively. As shown in  Tab.  1 , ours, with either retrieved or GT reference, outperforms all baselines, benefiting from the proposed retrieval-augmented method. While the CD is slightly larger, we argue that our approach produces plausible 3D models given different references (Fig.  7 ), though they can differ from GT mesh when computing chamfer distance.",
            "User Study.   We further conduct a user study to evaluate human preferences among different methods. We publicly invite 30 users to complete a questionnaire for pairwise comparisons. We show the preference rate ( i.e.,  the percentage of users prefer ours compared to a baseline method) in  Tab.  4.1 , which suggests that our approach significantly outperforms existing methods in the image-to-3D task based on human preferences.",
            "Ablation Studies.   We conduct ablation studies across four settings: a base model employing a standard ControlNet trained with self-reference, and three variants (each integrating one proposed component into the base model). The quantitative results in  Tab.  4.1  demonstrate clear improvements in both visual and geometric metrics with our proposed components.",
            "Interactive 3D Generation with Coarse Guidance.   Interactive generation gives users more control over the outputs, empowering them to make quick edits and receive rapid feedback.  Phidias  also provides this functionality, allowing users to continually adjust the geometry of generated 3D models using manually created coarse 3D shapes as reference models, as shown in  Fig.  10 .",
            "High-Fidelity 3D Completion.   Given incomplete 3D models, as shown in  Fig.  11 ,  Phidias  can be used to restore the missing components. Specially, by generating a complete front view through image inpainting and referencing to the original 3D model,  Phidias  can precisely predict and fill in the missing parts in novel views while maintaining the integrity and details of the origin, resulting in a seamlessly and coherently structured 3D model.",
            "Reference-augmented multi-view diffusion model.    White-Background Zero123++.  As discussed in  Sec.  3.1 , we select Zero123++ as our initial multi-view diffusion model. Upon receiving an input image, Zero123++ generates a tailored multi-view image at a resolution of  960  640 960 640 960\\times 640 960  640 , comprising six  320  320 320 320 320\\times 320 320  320  views arranged in a  3  2 3 2 3\\times 2 3  2  grid. The original Zero123++ produces images with a gray background, which can result in floaters and cloud-like artifacts during the subsequent sparse-view 3D reconstruction phase. To mitigate this issue, we initialize our model with a variant of Zero123++  (Xu et al.,  2024 ) , which is finetuned to generate multi-view images with a white background.",
            "A detailed figure of the proposed meta-ControlNet in the style of vanilla ControlNet is shown in  Fig.  12 , where  c p  a  i  r subscript c p a i r \\bm{c}_{pair} bold_italic_c start_POSTSUBSCRIPT italic_p italic_a italic_i italic_r end_POSTSUBSCRIPT  is a pair of the concept image and the front-view reference CCM.",
            "Despite promising results,  Phidias  still has several limitations for further improvement. As a retrieval-augmented generation model, the performance can be affected by the retrieval method and the scale and quality of 3D reference database. Currently, the 3D database we used for retrieval only consists of 40K objects, making it difficult to find a very similar match. Also, mainstream 3D retrieval methods rely on semantic similarity, which may not always yield the best match. For example, retrieved reference models with misaligned poses or structures can lead to undesired outcomes, as shown in  Fig.  14 . Future works that improve the retrieval accuracy and expand the 3D reference database could mitigate these issues. Additionally, the limited resolution of the backbone multi-view diffusion model ( 320  320 320 320 320\\times 320 320  320 ) restricts the handling of high-resolution images. Enhancing the resolution of the diffusion model could further improve the quality of the generated 3D models.",
            "Phidias  takes an additional 3D reference as input to improve generative quality ( Fig.  5 ) and provide greater controllability ( Fig.  4 ) for 3D generation. We argue that  Phidias  can also enhance generalization ability when given input images from atypical viewpoints. When reconstructing 3D objects from video frames with varying views ( Fig.  13 ), we observe that the baseline methods perform well with typical view angles ( i.e.,  frame 1) but struggle with atypical input view angles ( e.g.,  frame 3 and 4). Conversely,  Phidias  produces plausible results given all four input views, demonstrating robust generalization ability across both typical and atypical viewpoints.",
            "More results on theme-aware 3D-to-3D generation are shown in Fig.  15 . More results on text-to-3D and image-to-3D generation are shown in Fig.  16  and Fig.  17 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 4:  Quantitative analysis on similarity levels of 3D reference.",
        "table": "S4.SS1.1.1.fig1.1",
        "footnotes": [],
        "references": [
            "Given one  concept image , we aim at leveraging an additional  3D reference model  to alleviate 3D inconsistency issues and geometric ambiguity that exist in 3D generation.  The 3D reference model can be either provided by the user or retrieved from a large 3D database for different applications.  The overall pipeline of  Phidias  is shown in  Fig.  2 , which involves two stages: reference-augmented multi-view generation and sparse-view 3D reconstruction.",
            "To leverage the powerful pertaining capability, only the additional conditioner for reference CCMs is trainable while the base multi-view diffusion is frozen.  However, a challenge in our task is that the 3D reference may not strictly align with the concept image or, more commonly, vary in most local parts. We found naive conditioner designs such as ControlNet  (Zhang et al.,  2023 )  tend to produce undesirable artifacts, as they were originally designed for image-to-image translation where the generated images strictly align with the condition images. To mitigate this problem, we introduce three key designs for our reference-augmented diffusion model: (1)  Meta-ControlNet  for adaptive control of the conditioning strength ( Sec.  3.2 ); (2)  Dynamic Reference Routing  for dynamic adjustment of the 3D reference ( Sec.  3.3 ); (3)  Self-Reference Augmentation  for self-supervised training ( Sec.  3.4 ).",
            "Meta-controller shares a similar architecture but has different parameters    superscript   \\Theta^{\\prime} roman_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . It works as a knob that dynamically modulates base ControlNet to generate adaptive control signals. Meta-controller takes a pair  c pair subscript c pair \\bm{c}_{\\mathrm{pair}} bold_italic_c start_POSTSUBSCRIPT roman_pair end_POSTSUBSCRIPT  of the concept image and the front-view reference CCM as input to produce meta-control signals based on their similarities. The meta-control signals are injected into diffusion models in two ways. On the one hand, meta-controller produces multi-scale alignment features  y meta1 = Z   meta1  ( F   meta  ( z pair ) ) subscript y meta1 subscript superscript Z meta1 superscript   subscript superscript F meta superscript   subscript z pair \\bm{y}_{\\mathrm{meta1}}=\\mathcal{Z}^{{\\mathrm{meta1}}}_{\\Theta^{\\prime}}\\left(%  \\mathcal{F}^{{\\mathrm{meta}}}_{\\Theta^{\\prime}}\\left(\\bm{z}_{\\mathrm{pair}}%  \\right)\\right) bold_italic_y start_POSTSUBSCRIPT meta1 end_POSTSUBSCRIPT = caligraphic_Z start_POSTSUPERSCRIPT meta1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( caligraphic_F start_POSTSUPERSCRIPT roman_meta end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT roman_pair end_POSTSUBSCRIPT ) )  to be injected into base ControlNet. These features are applied to the down-sampling blocks of base ControlNet ( Eq.  2 ) at each scale to guide the encoding of reference and help produce base-signals as:",
            "A detailed figure of the proposed meta-ControlNet in the style of vanilla ControlNet is shown in  Fig.  12 , where  c p  a  i  r subscript c p a i r \\bm{c}_{pair} bold_italic_c start_POSTSUBSCRIPT italic_p italic_a italic_i italic_r end_POSTSUBSCRIPT  is a pair of the concept image and the front-view reference CCM."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "S4.SS1.8.8.7.7",
        "footnotes": [],
        "references": [
            "To leverage the powerful pertaining capability, only the additional conditioner for reference CCMs is trainable while the base multi-view diffusion is frozen.  However, a challenge in our task is that the 3D reference may not strictly align with the concept image or, more commonly, vary in most local parts. We found naive conditioner designs such as ControlNet  (Zhang et al.,  2023 )  tend to produce undesirable artifacts, as they were originally designed for image-to-image translation where the generated images strictly align with the condition images. To mitigate this problem, we introduce three key designs for our reference-augmented diffusion model: (1)  Meta-ControlNet  for adaptive control of the conditioning strength ( Sec.  3.2 ); (2)  Dynamic Reference Routing  for dynamic adjustment of the 3D reference ( Sec.  3.3 ); (3)  Self-Reference Augmentation  for self-supervised training ( Sec.  3.4 ).",
            "ControlNet is designed to add additional controls to pre-trained diffusion models for image-to-image translation. The conditions are derived from the ground-truth images for self-supervised learning, and thus the generated images are expected to follow the conditions. However, in our settings, the conditions are from the reference model, which often misaligns with the target 3D models we want to generate. The vanilla ControlNet fails to handle such cases.  This necessitates further architecture advancement to accordingly adjust conditioning strength when the reference conflicts with the concept image. To this end, we propose meta-ControlNet, as shown in  Fig.  3  (a). Meta-ControlNet is comprised of two collaborative subnets, a base ControlNet and an additional meta-controller.",
            "where  z ref subscript z ref \\bm{z}_{\\mathrm{ref}} bold_italic_z start_POSTSUBSCRIPT roman_ref end_POSTSUBSCRIPT  and  z pair subscript z pair \\bm{z}_{\\mathrm{pair}} bold_italic_z start_POSTSUBSCRIPT roman_pair end_POSTSUBSCRIPT  are the feature maps of  c ref subscript c ref \\bm{c}_{\\mathrm{ref}} bold_italic_c start_POSTSUBSCRIPT roman_ref end_POSTSUBSCRIPT  and  c pair subscript c pair \\bm{c}_{\\mathrm{pair}} bold_italic_c start_POSTSUBSCRIPT roman_pair end_POSTSUBSCRIPT  via the trainable encoders in  Fig.  3  (a).",
            "Reference models typically align roughly with the concept image in terms of coarse shape, but diverge significantly in local details. This misalignment can cause confusion and conflicts, as the generation process relies on both the image and reference model. To address this issue, we propose a dynamic reference routing strategy that adjusts the reference resolution across denoise timesteps, as shown in  Fig.  3  (b).  As widely observed during the reverse diffusion process, the coarse structure of a target image is determined in high-noised timesteps and fine details emerge later as the timestep goes on. This motivates us to start with low-resolution reference CCMs at high noise levels  t h subscript t h t_{h} italic_t start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT . By lowering the resolution, reference models provide fewer details but exhibit smaller misalignment with the concept image. This enables reference models to assist in generating the global structure of 3D objects without significant conflicts. We then gradually increase the resolution of reference CCMs as the reverse diffusion process goes into middle noise levels  t m subscript t m t_{m} italic_t start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT  and low noise levels  t l subscript t l t_{l} italic_t start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT  to help refine local structures,  e.g.,  progressively generating a curly tail from a straight one ( Fig.  3  (b)).  This design choice would ensure effective usage of both concept image and 3D reference during the multi-view image generation process while avoiding degraded generation caused by misalignment.",
            "Effectiveness of Meta-ControlNet.   To evaluate meta-ControlNet, we use both self-reference and retrieved reference for training, as the learning of Meta-Controller ( Fig.  3  (a) top) requires reference models with varying levels of similarity. As shown in  Fig.  6  (a), the base model trained with retrieved reference often ignores the reference, failing to follow the shape pattern (disconnected boat). This phenomenon stems from the considerable similarity variation among retrieved references, which confuses the diffusion model. The base model thereby struggles to determine when and how to use the reference as it lacks the ability to adjust to different levels of similarity. Consequently, they often end up with ignoring the reference models entirely. In contrast, meta-ControlNet equips the model with the capability to dynamically modulate the conditioning strength of the reference model, thereby effectively utilizing available references for improving or controlling the generation process.",
            "Reference-augmented multi-view diffusion model.    White-Background Zero123++.  As discussed in  Sec.  3.1 , we select Zero123++ as our initial multi-view diffusion model. Upon receiving an input image, Zero123++ generates a tailored multi-view image at a resolution of  960  640 960 640 960\\times 640 960  640 , comprising six  320  320 320 320 320\\times 320 320  320  views arranged in a  3  2 3 2 3\\times 2 3  2  grid. The original Zero123++ produces images with a gray background, which can result in floaters and cloud-like artifacts during the subsequent sparse-view 3D reconstruction phase. To mitigate this issue, we initialize our model with a variant of Zero123++  (Xu et al.,  2024 ) , which is finetuned to generate multi-view images with a white background.",
            "Sparse-view 3D reconstruction model.   As discussed in  Sec.  3.5 , we employ LGM to convert the synthesized multi-view images into a 3D model. The original LGM is designed to reconstruct a 3D model from four input views at a resolution of  256  256 256 256 256\\times 256 256  256 . However, this does not align with the multi-view images generated in our first stage, which consist of six views at a resolution of  320  320 320 320 320\\times 320 320  320 . To adapt LGM to our specific inputs, we take its pretrained weights as initialization and finetune it to support six input images at  320  320 320 320 320\\times 320 320  320 .  Simultaneously changing the number of input views and image resolutions can destabilize the training process. We therefore separate the finetuning of number of input views and input resolution. Specifically, we first finetune the model with six input views at the original resolution for 60 epochs and then further finetune the model at a higher resolution of  320  320 320 320 320\\times 320 320  320  for another 60 epochs. The finetuning process is conducted on 32 NVIDIA A100 (80G) GPUs using the AdamW optimizer with a learning rate of  2.0  10  4 2.0 superscript 10 4 2.0\\times 10^{-4} 2.0  10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT  and a total batch size of 192. The whole finetuning process takes around four days.",
            "Retrieved Reference.  Although the retrieved 3D reference alone is insufficient for model training, as discussed in  Sec.  3.4 , it can still serve as a strong augmentation to simulate significant misalignment. Therefore, we assign a small probability of using the retrieved model as the reference during training.",
            "Phidias  takes an additional 3D reference as input to improve generative quality ( Fig.  5 ) and provide greater controllability ( Fig.  4 ) for 3D generation. We argue that  Phidias  can also enhance generalization ability when given input images from atypical viewpoints. When reconstructing 3D objects from video frames with varying views ( Fig.  13 ), we observe that the baseline methods perform well with typical view angles ( i.e.,  frame 1) but struggle with atypical input view angles ( e.g.,  frame 3 and 4). Conversely,  Phidias  produces plausible results given all four input views, demonstrating robust generalization ability across both typical and atypical viewpoints."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "S4.T4.7",
        "footnotes": [],
        "references": [
            "To leverage the powerful pertaining capability, only the additional conditioner for reference CCMs is trainable while the base multi-view diffusion is frozen.  However, a challenge in our task is that the 3D reference may not strictly align with the concept image or, more commonly, vary in most local parts. We found naive conditioner designs such as ControlNet  (Zhang et al.,  2023 )  tend to produce undesirable artifacts, as they were originally designed for image-to-image translation where the generated images strictly align with the condition images. To mitigate this problem, we introduce three key designs for our reference-augmented diffusion model: (1)  Meta-ControlNet  for adaptive control of the conditioning strength ( Sec.  3.2 ); (2)  Dynamic Reference Routing  for dynamic adjustment of the 3D reference ( Sec.  3.3 ); (3)  Self-Reference Augmentation  for self-supervised training ( Sec.  3.4 ).",
            "A good reference model should resemble the target 3D model (with varied details) to provide additional geometric cues, but it is impractical to collect sufficient target-reference pairs for training. An intuitive solution is to retrieve a similar model from a large 3D database as the training reference. However, due to the limited variety in current databases, finding a perfect match is challenging. The retrieved reference can vary greatly in orientation, size and semantics. While this is a common situation in inference scenarios, where a very similar reference is often unavailable, we found training with these challenging pairs fails to effectively use the 3D reference. We conjecture that the learning process struggles due to the significant differences between the reference and target 3D, leading the diffusion model to disregard the references. To avoid the idleness of reference, we developed a self-reference scheme that uses the target model as its own reference by applying various augmentations to mimic misalignment (refer to  Appendix   A.4 ). This approach ensures that the reference models are somewhat aligned with the target and more compatible, alleviating the learning difficulty.  We further design a curriculum training strategy, which begins with minimal augmentations (very similar references) to force the diffusion model to rely on the reference for enhancement. Over time, we gradually increase augmentation strength and incorporate retrieved references, challenging the diffusion model to learn from references that do not closely match the target. Once trained, our model performs well with a variety of references, even those retrieved ones that are not very similar.",
            "Qualitative Results.    For visual diversity  ( Fig.  4 ), given the same concept image,  Phidias  can generate diverse 3D assets that are both faithful to the concept image and conforming to a specific retrieved 3D reference in geometry.  For visual comparisons  ( Fig.  5 ), while the baseline methods can generate plausible results, they suffer from geometry distortion ( e.g.,  horse legs). Besides, none of the existing methods can benefit from the 3D reference for improved generalization ability ( e.g.,  excavators dipper) and controllability ( e.g.,  cats tail) as ours.",
            "User Study.   We further conduct a user study to evaluate human preferences among different methods. We publicly invite 30 users to complete a questionnaire for pairwise comparisons. We show the preference rate ( i.e.,  the percentage of users prefer ours compared to a baseline method) in  Tab.  4.1 , which suggests that our approach significantly outperforms existing methods in the image-to-3D task based on human preferences.",
            "Ablation Studies.   We conduct ablation studies across four settings: a base model employing a standard ControlNet trained with self-reference, and three variants (each integrating one proposed component into the base model). The quantitative results in  Tab.  4.1  demonstrate clear improvements in both visual and geometric metrics with our proposed components.",
            "Analysis on Similarity Levels of 3D Reference.   We analyze how similarity levels of 3D references would affect the performance. For each input, we retrieve three models ranked first (top-1), third (top-3), and fifth (top-5) in similarity scores, and randomly choose one model, to serve as 3D references. Quantitative results in  Tab.  4  indicate that  Phidias  performs better with more similar references.  Fig.  7  shows  Phidias  generates diverse plausible results with different references. All results remain faithful to the input image in the front view, but show variations in shapes influenced by the specific reference used. Also, we found  Phidias  can still generate plausible results even with a random 3D reference, indicating robustness to reference with different similarity levels.",
            "Training Details.  During the training of our reference-augmented multi-view diffusion model, we use the rendered concept image and six CCMs of a 3D object as conditions, and six corresponding target images tailored to a  960  640 960 640 960\\times 640 960  640  image as ground truth image for denoising. All images and CCMs have a white background. We concatenate the concept image and the front-view CCM along the RGB channel as the input for meta-ControlNet. For the proposed dynamic reference routing, we dynamically downsample the original CCMs to lower resolutions and then upsample them to  320  320 320 320 320\\times 320 320  320 , using the nearest neighbor. Specifically, we start with a resolution of 16 at noise levels of  [ 0 , 0.05 ) 0 0.05 [0,0.05) [ 0 , 0.05 )  and gradually increase the resolution to 32 and 64 at noise levels of  [ 0.05 , 0.4 ) 0.05 0.4 [0.05,0.4) [ 0.05 , 0.4 )  and  [ 0.4 , 1.0 ] 0.4 1.0 [0.4,1.0] [ 0.4 , 1.0 ] , respectively. For self-reference augmentations ( Sec.  A.4 ), the probabilities of applying random resize, flip horizontal, grid distortion, shift, and retrieved reference are set to 0.4, 0.5, 0.1, 0.5, and 0.2, respectively. We train the model for 10,000 steps, beginning with 1000 warm-up steps with minimal augmentations. We use the AdamW optimizer with a learning rate of  1.0  10  5 1.0 superscript 10 5 1.0\\times 10^{-5} 1.0  10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT  and a total batch size of 48. The whole training process takes around 10 hours on 8 NVIDIA A100 (80G) GPUs.",
            "Retrieved Reference.  Although the retrieved 3D reference alone is insufficient for model training, as discussed in  Sec.  3.4 , it can still serve as a strong augmentation to simulate significant misalignment. Therefore, we assign a small probability of using the retrieved model as the reference during training.",
            "Despite promising results,  Phidias  still has several limitations for further improvement. As a retrieval-augmented generation model, the performance can be affected by the retrieval method and the scale and quality of 3D reference database. Currently, the 3D database we used for retrieval only consists of 40K objects, making it difficult to find a very similar match. Also, mainstream 3D retrieval methods rely on semantic similarity, which may not always yield the best match. For example, retrieved reference models with misaligned poses or structures can lead to undesired outcomes, as shown in  Fig.  14 . Future works that improve the retrieval accuracy and expand the 3D reference database could mitigate these issues. Additionally, the limited resolution of the backbone multi-view diffusion model ( 320  320 320 320 320\\times 320 320  320 ) restricts the handling of high-resolution images. Enhancing the resolution of the diffusion model could further improve the quality of the generated 3D models.",
            "Phidias  takes an additional 3D reference as input to improve generative quality ( Fig.  5 ) and provide greater controllability ( Fig.  4 ) for 3D generation. We argue that  Phidias  can also enhance generalization ability when given input images from atypical viewpoints. When reconstructing 3D objects from video frames with varying views ( Fig.  13 ), we observe that the baseline methods perform well with typical view angles ( i.e.,  frame 1) but struggle with atypical input view angles ( e.g.,  frame 3 and 4). Conversely,  Phidias  produces plausible results given all four input views, demonstrating robust generalization ability across both typical and atypical viewpoints."
        ]
    },
    "global_footnotes": [
        "Intern at Shanghai AI Lab.",
        "Equal Contribution.",
        "Intern at Shanghai AI Lab.",
        "Equal Contribution."
    ]
}