{
    "S4.T1": {
        "caption": "Table 1: Question classifier: accuracy of our question classifier on questions from both the OK-VQA and VQA v2 datasets and from Artpedia.",
        "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r\">OK-VQA/VQA v2</td>\n<td id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center\">Artpedia</td>\n</tr>\n<tr id=\"S4.T1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Question Classifier</td>\n<td id=\"S4.T1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.868</td>\n<td id=\"S4.T1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.938</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We train the question classifier module with questions of both the OK-VQA and VQA v2 datasets. We take from VQA v2 a number of visual questions equal to the number of questions that require external knowledge from OK-VQA. The obtained dataset is then split into train and test sets.\nThe question classifier is supposed to understand from the structure of the question whether the answer concerns the visual content or not. This is a generic classifier, agnostic from the domain of the task. In fact, VQA v2 and OK-VQA contain generic images, while we are interested in applications in the cultural heritage domain. We demonstrate the effectiveness of our approach and its ability to transfer to the cultural heritage domain by evaluating it both on the VQA/OK-VQA dataset and on a new dataset comprised of a subset of Artpedia [20]. Since this dataset does not contain questions but only images and descriptions, we took 30 images from this dataset and annotated them with a variable number of both visual and contextual questions (from 3 to 5 for both categories).\nThe accuracy of our question classifier module is shown in Tab. 1. We can observe that it is able to predict the type of the question correctly in most cases."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Results of the two answering models on contextual questions, visual questions and both visual and contextual questions from Artpedia. Note that the VQA model does not have access to the external information required to answer the contextual questions, making it unable to answer correctly. See section 4.3.4 for analysis of the performance of our full model on combined Visual/Contextual Question Answering",
        "table": "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span id=\"S4.T2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">QA model</span></th>\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span id=\"S4.T2.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">VQA model</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_center\">\n<table id=\"S4.T2.1.2.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T2.1.2.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Contextual</td>\n<td id=\"S4.T2.1.2.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_rr\">Visual</td>\n<td id=\"S4.T2.1.2.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r\">Accuracy</td>\n<td id=\"S4.T2.1.2.1.1.1.1.4\" class=\"ltx_td ltx_align_center\">F1-score</td>\n</tr>\n<tr id=\"S4.T2.1.2.1.1.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">✓</td>\n<td id=\"S4.T2.1.2.1.1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">✗</td>\n<td id=\"S4.T2.1.2.1.1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.684</td>\n<td id=\"S4.T2.1.2.1.1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.832</td>\n</tr>\n<tr id=\"S4.T2.1.2.1.1.1.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">✗</td>\n<td id=\"S4.T2.1.2.1.1.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_rr\">✓</td>\n<td id=\"S4.T2.1.2.1.1.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.176</td>\n<td id=\"S4.T2.1.2.1.1.1.3.4\" class=\"ltx_td ltx_align_center\">0.150</td>\n</tr>\n<tr id=\"S4.T2.1.2.1.1.1.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">✓</td>\n<td id=\"S4.T2.1.2.1.1.1.4.2\" class=\"ltx_td ltx_align_center ltx_border_rr\">✓</td>\n<td id=\"S4.T2.1.2.1.1.1.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.504</td>\n<td id=\"S4.T2.1.2.1.1.1.4.4\" class=\"ltx_td ltx_align_center\">0.417</td>\n</tr>\n</table>\n</td>\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_center\">\n<table id=\"S4.T2.1.2.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T2.1.2.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Contextual</td>\n<td id=\"S4.T2.1.2.1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_rr\">Visual</td>\n<td id=\"S4.T2.1.2.1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r\">Accuracy</td>\n</tr>\n<tr id=\"S4.T2.1.2.1.2.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.2.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">✓</td>\n<td id=\"S4.T2.1.2.1.2.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">✗</td>\n<td id=\"S4.T2.1.2.1.2.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.000</td>\n</tr>\n<tr id=\"S4.T2.1.2.1.2.1.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.2.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">✗</td>\n<td id=\"S4.T2.1.2.1.2.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_rr\">✓</td>\n<td id=\"S4.T2.1.2.1.2.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.524</td>\n</tr>\n<tr id=\"S4.T2.1.2.1.2.1.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.2.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">✓</td>\n<td id=\"S4.T2.1.2.1.2.1.4.2\" class=\"ltx_td ltx_align_center ltx_border_rr\">✓</td>\n<td id=\"S4.T2.1.2.1.2.1.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.251</td>\n</tr>\n</table>\n</td>\n</tr>\n</tbody>\n</table>\n<table id=\"S4.T2.1.2.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T2.1.2.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Contextual</td>\n<td id=\"S4.T2.1.2.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_rr\">Visual</td>\n<td id=\"S4.T2.1.2.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r\">Accuracy</td>\n<td id=\"S4.T2.1.2.1.1.1.1.4\" class=\"ltx_td ltx_align_center\">F1-score</td>\n</tr>\n<tr id=\"S4.T2.1.2.1.1.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">✓</td>\n<td id=\"S4.T2.1.2.1.1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">✗</td>\n<td id=\"S4.T2.1.2.1.1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.684</td>\n<td id=\"S4.T2.1.2.1.1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.832</td>\n</tr>\n<tr id=\"S4.T2.1.2.1.1.1.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">✗</td>\n<td id=\"S4.T2.1.2.1.1.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_rr\">✓</td>\n<td id=\"S4.T2.1.2.1.1.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.176</td>\n<td id=\"S4.T2.1.2.1.1.1.3.4\" class=\"ltx_td ltx_align_center\">0.150</td>\n</tr>\n<tr id=\"S4.T2.1.2.1.1.1.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">✓</td>\n<td id=\"S4.T2.1.2.1.1.1.4.2\" class=\"ltx_td ltx_align_center ltx_border_rr\">✓</td>\n<td id=\"S4.T2.1.2.1.1.1.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.504</td>\n<td id=\"S4.T2.1.2.1.1.1.4.4\" class=\"ltx_td ltx_align_center\">0.417</td>\n</tr>\n</table>\n<table id=\"S4.T2.1.2.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T2.1.2.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Contextual</td>\n<td id=\"S4.T2.1.2.1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_rr\">Visual</td>\n<td id=\"S4.T2.1.2.1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r\">Accuracy</td>\n</tr>\n<tr id=\"S4.T2.1.2.1.2.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.2.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">✓</td>\n<td id=\"S4.T2.1.2.1.2.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">✗</td>\n<td id=\"S4.T2.1.2.1.2.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.000</td>\n</tr>\n<tr id=\"S4.T2.1.2.1.2.1.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.2.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">✗</td>\n<td id=\"S4.T2.1.2.1.2.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_rr\">✓</td>\n<td id=\"S4.T2.1.2.1.2.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.524</td>\n</tr>\n<tr id=\"S4.T2.1.2.1.2.1.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.2.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">✓</td>\n<td id=\"S4.T2.1.2.1.2.1.4.2\" class=\"ltx_td ltx_align_center ltx_border_rr\">✓</td>\n<td id=\"S4.T2.1.2.1.2.1.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.251</td>\n</tr>\n</table>\n",
        "footnotes": "",
        "references": [
            "We test our question answering module on the subset of Artpedia containing 30 images that we annotated. In particular, we test the accuracy of our module in three different experiments: test on contextual questions, test on visual questions and test with both visual and contextual questions. Note that the outputs of the visual and contextual modules are different, since VQA is treated as a classification problem, while for QA\nFrom the results shown in Tab. 2 we can deduce that our question answering module works very well with contextual questions and obtains worse results with visual questions. This can be justified from the fact that visual questions refer to visible details of paintings that cannot be described in visual sentences of ArtPedia.",
            "Similarly to the tests conducted for the question answering module, we evaluate the visual question answering module on both visual and contextual questions. In Table 2 results of our visual question answering model are shown. We can observe that conversely from the question answering module this model performs well on visual questions and is not able to answer correctly to contextual questions. This is motivated by the fact that contextual questions require external knowledge (e.g. author, year) that a purely visual question answering engine does not have access to."
        ]
    }
}