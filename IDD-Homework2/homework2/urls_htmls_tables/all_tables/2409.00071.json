{
    "S5.T1.1": {
        "caption": "\nSample Sentences Generated by the GAN\n",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S5.T1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.1.1.1.1.1\">Sample Generated Sentences</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S5.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.1.1.1.2.1\">Qualitative Evaluation</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T1.1.2.1.1\">my grandfather work harder than your grandfather before</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T1.1.2.1.2\">good</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.3.2.1\">to consider quit job is this dream man</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.3.2.2\">good</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.4.3.1\">ask me that healthy lunch im cooking up</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.4.3.2\">good</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.5.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.5.4.1\">maryam discovered hes hes am am are are</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.5.4.2\">repetition</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.6.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.6.5.1\">home actually was everything everything listen actually everything</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.6.5.2\">repetition</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.7.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.7.6.1\">cheerful weird yourself punished music alone everybody everybody</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.7.6.2\">nonsensical</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.8.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.8.7.1\">those in so friends so complicated english comes</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.8.7.2\">nonsensical</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.9.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S5.T1.1.9.8.1\">stressed gloves eating eating worried online online online</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S5.T1.1.9.8.2\">unrelated</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "The generator was able to generate successful, coherent sentences, shown in Table I. Upon manual inspection, we found that the model was able to reproduce both syntactic and semantic meaning in some cases, although the model displayed significant errors in many other cases. Analysis of the errors made and limitations is discussed further below. Syntactically, the model placed words in the correct places based on their part of speech. A majority of the sentences also centered around a cohesive theme, indicating the model’s successful understanding of word meanings, and communicated a specific message. From random noise, the generator was able to create its own completely new and logical sentences, a significant feat considering the lack of training data.\n",
            "Frequently, the GAN generated sentences with repeating words, shown in Table I. We hypothesize that this issue, which occurs in most NMT models [29], is due to the fact that the model is trying to generate words that are close in context to each other, which is necessary in order for a sentence to make sense. However, the model may not know or weigh in if it already used a word or not and ends up repeating that same word. It likely tries to find a word that is close in context to the prior word, and because related words have closer probabilities, it generates a probability very close to the previous word’s probability. Then, after the decoder translates the latent space representations into probabilities, the close probabilities may be reduced to the same word because the words with the highest close probabilities are chosen to represent the probabilities the GAN returned. There may not be enough words with intricate probabilities close to a given one, so that one is chosen for all words with probabilities in a certain range around it. [29] theorize that the issue is in the nature of languages themselves, as some words tend to predict themselves as the next word in context. Some incoherencies in the generated sentences could also be due to the fact the model may not fully understand the grammatical structure of sentences and resorts to repeating words it does know how to represent in order to fill up space. Potential solutions would be to train the model to remember the previous probabilities it generated and to vary its generated probabilities more to ensure that it does not repeat very similar ones.\n",
            "Other sentences contained minimal repetition but were still grammatically incorrect or nonsensical, shown in Table I. We believe that these sentences contain relatively randomly-placed words because the model has not learned about these words with enough context to know where to place them grammatically. Because it has seen these well-known yet complex words, it can generate them but is unsure of how many to generate, where to place them, or which words to surround them with. The model also may have mistakenly generated words with similar probabilities in search of words with close context. For example, “cheerful” and “weird” are both adjectives that describe “yourself,” so their probabilities may be similar enough for the model to generate them together. However, the model does not understand that these words have parallel meanings, and that only one should be used. A possible avenue for future work is to explore training the model on the difference between probabilities of words parallel in meaning and probabilities of related words that are required to be together in order to form a sentence.\n",
            "Although the model generally uses related words like “studies” and “novels” together, it occasionally groups unrelated words together, shown in Table I. We hypothesize that this is because it has not seen a word (like “gloves”) enough to understand its usual context (being put on people’s hands). However, it does understand that gloves is a noun and has previously seen nouns (such as people) being stressed, eating, worrying, and going online. A potential future path to explore would be incorporating a dictionary of words into the model’s training so that it better understands the words’ meanings.\n"
        ]
    },
    "A1.T2.1": {
        "caption": "Characteristics of Training Data",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A1.T2.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A1.T2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T2.1.1.1.1.1\">Characteristic</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T2.1.1.1.2.1\">English</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T2.1.1.1.3.1\">Spanish</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A1.T2.1.2.2.1\">Average Sentence Length</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.2.2.2\">4.72</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T2.1.2.2.3\">4.52</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T2.1.3.3.1\">Max Sentence length</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.3.3.2\">8</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.3.3.3\">11</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T2.1.4.4.1\">Mean</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.4.4.2\">204.38</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T2.1.4.4.3\">300.87</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"A1.T2.1.5.5.1\">Standard Deviation</th>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"A1.T2.1.5.5.2\">595.44</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"A1.T2.1.5.5.3\">1055.53</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "Table II and Table III capture key statistical characteristics of the training and test data, respectively.\n"
        ]
    },
    "A1.T3.1": {
        "caption": "Characteristics of Test Data",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A1.T3.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A1.T3.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T3.1.1.1.1.1\">Characteristic</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T3.1.1.1.2.1\">English</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T3.1.1.1.3.1\">Spanish</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T3.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A1.T3.1.2.2.1\">Average Sentence Length</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.2.2.2\">4.71</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A1.T3.1.2.2.3\">4.53</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T3.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T3.1.3.3.1\">Max Sentence length</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.3.3.2\">7</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.3.3.3\">9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T3.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A1.T3.1.4.4.1\">Mean</th>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.4.4.2\">213.62</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A1.T3.1.4.4.3\">337.18</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T3.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"A1.T3.1.5.5.1\">Standard Deviation</th>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"A1.T3.1.5.5.2\">662.48</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"A1.T3.1.5.5.3\">1285.73</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "Table II and Table III capture key statistical characteristics of the training and test data, respectively.\n"
        ]
    },
    "A2.T4.1": {
        "caption": "Hyperparameter Values",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A2.T4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"A2.T4.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.1.1.1.1.1\">Hyperparameter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"A2.T4.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.1.1.1.2.1\">Value</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T4.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T4.1.2.1.1\">Epochs (Encoder-Decoder)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A2.T4.1.2.1.2\">400</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.3.2.1\">Batch Size (Encoder-Decoder)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.3.2.2\">30</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.4.3.1\">LSTM Units (Encoder-Decoder)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.4.3.2\">256</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.5.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.5.4.1\">LSTM Dropout (Encoder)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.5.4.2\">0.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.6.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.6.5.1\">LSTM Dropout (Decoder)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.6.5.2\">0.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.7.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.7.6.1\">Logits Dropout (Encoder-Decoder)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.7.6.2\">0.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.8.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.8.7.1\">L2 Regularizer (Encoder)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.8.7.2\">5e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.9.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.9.8.1\">L2 Regularizer (Decoder)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.9.8.2\">1e-5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.10.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.10.9.1\">Learning Rate (Encoder-Decoder)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.10.9.2\">2e-3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.11.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.11.10.1\">Beta1 Decay (Encoder-Decoder)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.11.10.2\">0.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.12.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.12.11.1\">Beta2 Decay (Encoder-Decoder)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.12.11.2\">0.97</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.13.12\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.13.12.1\">Epochs (GAN)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.13.12.2\">8000</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.14.13\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.14.13.1\">Batch Size (GAN)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.14.13.2\">1900</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.15.14\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.15.14.1\">Learning Rate (GAN)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.15.14.2\">1e-4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.16.15\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.16.15.1\">Dense Units (Generator)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.16.15.2\">256</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.17.16\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.17.16.1\">Learning Rate (Generator)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.17.16.2\">4e-4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.18.17\">\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.18.17.1\">Dense Units (Discriminator)</td>\n<td class=\"ltx_td ltx_align_left\" id=\"A2.T4.1.18.17.2\">1024</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.19.18\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"A2.T4.1.19.18.1\">Learning Rate (Discriminator)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"A2.T4.1.19.18.2\">1e-4</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "We use layers imported from the Keras Python library [31]. The hyperparameters we used are listed in Table IV.\n"
        ]
    }
}