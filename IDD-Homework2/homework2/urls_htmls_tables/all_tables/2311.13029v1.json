{
    "S3.T1": {
        "caption": "Table 1: Sample entries of the SWORME dataset. Target words (lexical instantiations of meta-alternations) in usage sentences are shown in bold italic, and noun objects that decide meta-sense labels of verb and adjective lexical instantiations are underlined.",
        "table": null,
        "footnotes": [],
        "references": [
            "We construct our SWORME usage dataset based on the sense-annotated text corpus made by Yu and Xu (2023), which consists of 1.47M sentences taken from the Wikitext-103 corpus Merity et¬†al. (2016) and contains usages of over 7,500 English polysemous words labeled with their associated WordNet synset IDs. We obtain the CoreLex meta-sense label for each polysemous word usage via the mapping method introduced in section 3.1. For each word, we only keep usages of its top-2 most frequent meta-senses in the corpus, so that there is no overlap between the lexical instantiation sets of any two meta-alternation classes. To decide a set of systematic meta-alternations, we then take all meta-sense pairs (m,m‚Ä≤)ùëösuperscriptùëö‚Ä≤(m,m^{\\prime}) with at least 50 lexical instantiations of more than 10 usage examples under each meta-sense (i.e. with at least 20 mentions in total). This gives us a total of 50 meta-sense alternation pairs that covers a variety of widely studied types of regular meaning alternation including logical metonymy, weak metaphor and strong metaphor 444See Appendix B for the full list of systematic meta-alternations in our dataset.. For each systematic meta-alternation, we take the top-100 lexical instantiations with highest numbers of usage examples in the corpus. This pipeline finally yields approximately 880,000 usage sentences for 7,346 English words (3,155 nouns and 2576 verbs and 1,615 adjectives). See Table 1 for sample entries of the resulting dataset."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Top-3 meta-alternation classes with most improved model accuracy by analogical chaining (Analog.) over associative chaining (Assoc.).",
        "table": null,
        "footnotes": [],
        "references": [
            "We further examine model sensitivity to the conceptual relatedness between existing and extended meta-senses. We quantify the degree of conceptual relatedness as the mean Wu-Palmer similarity Wu and Palmer (1994) between the anchored WordNet synsets of two meta-senses, and we then compute the mean model precision of predicting substituted partitioned tokens from each meta-sense alternation pair (averaged over both extensional directions), as shown in Figure 4 for three experiment setups with increasing amount of training words per meta-alternation (Œ±=[0,0.2,0.8]ùõº00.20.8\\alpha=[0,0.2,0.8]). We found that all models generally make better predictions on meta-alternations that are conceptually more contiguous (e.g., metonymy), and perform less well on examples where the novel meta-sense is conceptually very different to the existing one (e.g., strong metaphor). Moreover, analogical chaining model exhibits less sensitivity to semantic proximity and generally does better at predicting radical meta-sense extensions than its associative chaining counterpart. Table 2 shows the top-3 meta-alternation classes on which analogical chaining improves model performance most significantly over associative chaining. We found that all these meta-alternations are typical examples of ‚Äúmetaphorical‚Äù extensions consisting of a concrete meta-sense and a semantically very different abstract meta-sense. These results again suggest that the literal similarity between conventional and novel meaning is insufficient to account for various types of lexical creativity."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Model classification accuracy on two figurative language understanding datasets.",
        "table": null,
        "footnotes": [],
        "references": [
            "Results. Table 3 summarizes model classification accuracy on the official evaluation sets of the two FLU datasets. We found that language models fine-tuned on SWORME through analogical chaining yield best overall classification accuracy, as well as on most sub-categories of figurative language use. Fine-tuning via associative chaining, on the other hand, is much less helpful or can sometimes even be harmful for FLU. We hypothesize that associative chaining pushes usage embeddings of related meta-senses too close to each other, so that some important sentence-level semantic features in the sentence embedding become degenerated. These results together suggest that learning relational similarity between systematic word meta-senses can serve as a simple yet effective method to drive language models toward human-level understanding of figurative language."
        ]
    },
    "S6.T4": {
        "caption": "Table 4: Example FLU questions and model outputs. Entailment labels and model predicted entailment probabilities are marked in blue, and non-entailment labels/probabilities are marked in red.",
        "table": null,
        "footnotes": [],
        "references": [
            "Table 4 shows model predictions on sample FLU questions. We found that many idiomatic expressions in IMPLI can also be interpreted as systematic meaning extensions from more ‚Äúliteral‚Äù meta-senses of common polysemous words (e.g. ‚Äústorm‚Äù referring to ‚Äúdifficult situation‚Äù, which signifies a systematic extension from (hostile) NATURAL PHENOMENON to (poor) COGNITIVE STATE), so learning analogical chaining helps model better distinguish such usages against the adversarial hypothesis with high lexical overlap. We also observe that even the largest LLaMA-7B model still makes errors on metaphorical expressions whose interpretations are obvious to humans (e.g. broad imagination), while learning SWORME through analogical chaining helps correct many of these mistakes. Meanwhile, analogical chaining helps little on understanding ironic expressions such as ‚Äúas joyful as funeral‚Äù, which can also be considered as a systematic semantic extension toward the opposite word meaning. Future work can explore how antonymic meaning change can be incorporated into the SWORME framework."
        ]
    },
    "A1.T5": {
        "caption": "Table 5: CoreLex‚Äôs meta-senses (names in lowercase) with their corresponding WordNet anchor synsets (names in uppercase).",
        "table": null,
        "footnotes": [],
        "references": [
            "See Table 5 and Table 6."
        ]
    },
    "A2.T6": {
        "caption": "Table 6: Top-50 systematic CoreLex meta alternations with highest corpus frequency.",
        "table": null,
        "footnotes": [],
        "references": [
            "See Table 5 and Table 6."
        ]
    }
}