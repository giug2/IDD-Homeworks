{
    "PAPER'S NUMBER OF TABLES": 4,
    "S4.T1": {
        "caption": "Table 1: Test accuracy (%) of the considered schemes as the concentration parameter α𝛼\\alpha takes values from {0.1,0.5,1}0.10.51\\{0.1,0.5,1\\}.\nThe number of clients in these experiments is 101010, while their average bit-width budgets are 𝐯={2,2,4,4,4,6,6,6,8,8}𝐯2244466688\\mathbf{v}=\\{2,2,4,4,4,6,6,6,8,8\\} (vnsubscript𝑣𝑛v_{n} denotes the budget of client n𝑛n). The numbers in the column “Update”, “Weight” and “Activation” indicate the bits used to store the local update values, model weights and the activation signals, respectively. The last column indicates whether the scheme needs to train a full-precision model or not.",
        "table": "<table id=\"S4.T1.28\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.28.17.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.28.17.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt\"></th>\n<th id=\"S4.T1.28.17.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span id=\"S4.T1.28.17.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CIFAR10</span></th>\n<th id=\"S4.T1.28.17.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span id=\"S4.T1.28.17.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CIFAR100</span></th>\n<th id=\"S4.T1.28.17.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span id=\"S4.T1.28.17.1.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Tiny-ImageNet</span></th>\n<th id=\"S4.T1.28.17.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T1.28.17.1.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Update</span></th>\n<th id=\"S4.T1.28.17.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T1.28.17.1.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Weight</span></th>\n<th id=\"S4.T1.28.17.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T1.28.17.1.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Activation</span></th>\n<th id=\"S4.T1.28.17.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T1.28.17.1.8.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Full-Precision?</span></th>\n</tr>\n<tr id=\"S4.T1.22.10\" class=\"ltx_tr\">\n<th id=\"S4.T1.13.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\"><math id=\"S4.T1.13.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha\" display=\"inline\"><semantics id=\"S4.T1.13.1.1.m1.1a\"><mi mathsize=\"90%\" id=\"S4.T1.13.1.1.m1.1.1\" xref=\"S4.T1.13.1.1.m1.1.1.cmml\">α</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.13.1.1.m1.1b\"><ci id=\"S4.T1.13.1.1.m1.1.1.cmml\" xref=\"S4.T1.13.1.1.m1.1.1\">𝛼</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.13.1.1.m1.1c\">\\alpha</annotation></semantics></math></th>\n<th id=\"S4.T1.14.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math id=\"S4.T1.14.2.2.m1.1\" class=\"ltx_Math\" alttext=\"0.1\" display=\"inline\"><semantics id=\"S4.T1.14.2.2.m1.1a\"><mn mathsize=\"90%\" id=\"S4.T1.14.2.2.m1.1.1\" xref=\"S4.T1.14.2.2.m1.1.1.cmml\">0.1</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.14.2.2.m1.1b\"><cn type=\"float\" id=\"S4.T1.14.2.2.m1.1.1.cmml\" xref=\"S4.T1.14.2.2.m1.1.1\">0.1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.14.2.2.m1.1c\">0.1</annotation></semantics></math></th>\n<th id=\"S4.T1.15.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math id=\"S4.T1.15.3.3.m1.1\" class=\"ltx_Math\" alttext=\"0.5\" display=\"inline\"><semantics id=\"S4.T1.15.3.3.m1.1a\"><mn mathsize=\"90%\" id=\"S4.T1.15.3.3.m1.1.1\" xref=\"S4.T1.15.3.3.m1.1.1.cmml\">0.5</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.15.3.3.m1.1b\"><cn type=\"float\" id=\"S4.T1.15.3.3.m1.1.1.cmml\" xref=\"S4.T1.15.3.3.m1.1.1\">0.5</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.15.3.3.m1.1c\">0.5</annotation></semantics></math></th>\n<th id=\"S4.T1.16.4.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math id=\"S4.T1.16.4.4.m1.1\" class=\"ltx_Math\" alttext=\"1\" display=\"inline\"><semantics id=\"S4.T1.16.4.4.m1.1a\"><mn mathsize=\"90%\" id=\"S4.T1.16.4.4.m1.1.1\" xref=\"S4.T1.16.4.4.m1.1.1.cmml\">1</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.16.4.4.m1.1b\"><cn type=\"integer\" id=\"S4.T1.16.4.4.m1.1.1.cmml\" xref=\"S4.T1.16.4.4.m1.1.1\">1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.16.4.4.m1.1c\">1</annotation></semantics></math></th>\n<th id=\"S4.T1.17.5.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math id=\"S4.T1.17.5.5.m1.1\" class=\"ltx_Math\" alttext=\"0.1\" display=\"inline\"><semantics id=\"S4.T1.17.5.5.m1.1a\"><mn mathsize=\"90%\" id=\"S4.T1.17.5.5.m1.1.1\" xref=\"S4.T1.17.5.5.m1.1.1.cmml\">0.1</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.17.5.5.m1.1b\"><cn type=\"float\" id=\"S4.T1.17.5.5.m1.1.1.cmml\" xref=\"S4.T1.17.5.5.m1.1.1\">0.1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.17.5.5.m1.1c\">0.1</annotation></semantics></math></th>\n<th id=\"S4.T1.18.6.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math id=\"S4.T1.18.6.6.m1.1\" class=\"ltx_Math\" alttext=\"0.5\" display=\"inline\"><semantics id=\"S4.T1.18.6.6.m1.1a\"><mn mathsize=\"90%\" id=\"S4.T1.18.6.6.m1.1.1\" xref=\"S4.T1.18.6.6.m1.1.1.cmml\">0.5</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.18.6.6.m1.1b\"><cn type=\"float\" id=\"S4.T1.18.6.6.m1.1.1.cmml\" xref=\"S4.T1.18.6.6.m1.1.1\">0.5</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.18.6.6.m1.1c\">0.5</annotation></semantics></math></th>\n<th id=\"S4.T1.19.7.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math id=\"S4.T1.19.7.7.m1.1\" class=\"ltx_Math\" alttext=\"1\" display=\"inline\"><semantics id=\"S4.T1.19.7.7.m1.1a\"><mn mathsize=\"90%\" id=\"S4.T1.19.7.7.m1.1.1\" xref=\"S4.T1.19.7.7.m1.1.1.cmml\">1</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.19.7.7.m1.1b\"><cn type=\"integer\" id=\"S4.T1.19.7.7.m1.1.1.cmml\" xref=\"S4.T1.19.7.7.m1.1.1\">1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.19.7.7.m1.1c\">1</annotation></semantics></math></th>\n<th id=\"S4.T1.20.8.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math id=\"S4.T1.20.8.8.m1.1\" class=\"ltx_Math\" alttext=\"0.1\" display=\"inline\"><semantics id=\"S4.T1.20.8.8.m1.1a\"><mn mathsize=\"90%\" id=\"S4.T1.20.8.8.m1.1.1\" xref=\"S4.T1.20.8.8.m1.1.1.cmml\">0.1</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.20.8.8.m1.1b\"><cn type=\"float\" id=\"S4.T1.20.8.8.m1.1.1.cmml\" xref=\"S4.T1.20.8.8.m1.1.1\">0.1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.20.8.8.m1.1c\">0.1</annotation></semantics></math></th>\n<th id=\"S4.T1.21.9.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math id=\"S4.T1.21.9.9.m1.1\" class=\"ltx_Math\" alttext=\"0.5\" display=\"inline\"><semantics id=\"S4.T1.21.9.9.m1.1a\"><mn mathsize=\"90%\" id=\"S4.T1.21.9.9.m1.1.1\" xref=\"S4.T1.21.9.9.m1.1.1.cmml\">0.5</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.21.9.9.m1.1b\"><cn type=\"float\" id=\"S4.T1.21.9.9.m1.1.1.cmml\" xref=\"S4.T1.21.9.9.m1.1.1\">0.5</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.21.9.9.m1.1c\">0.5</annotation></semantics></math></th>\n<th id=\"S4.T1.22.10.10\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\"><math id=\"S4.T1.22.10.10.m1.1\" class=\"ltx_Math\" alttext=\"1\" display=\"inline\"><semantics id=\"S4.T1.22.10.10.m1.1a\"><mn mathsize=\"90%\" id=\"S4.T1.22.10.10.m1.1.1\" xref=\"S4.T1.22.10.10.m1.1.1.cmml\">1</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.22.10.10.m1.1b\"><cn type=\"integer\" id=\"S4.T1.22.10.10.m1.1.1.cmml\" xref=\"S4.T1.22.10.10.m1.1.1\">1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.22.10.10.m1.1c\">1</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.28.18.1\" class=\"ltx_tr\" style=\"background-color:#EFEFEF;\">\n<td id=\"S4.T1.28.18.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S4.T1.28.18.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">FP32</span></td>\n<td id=\"S4.T1.28.18.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.18.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">60.3</span></td>\n<td id=\"S4.T1.28.18.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.18.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">77.5</span></td>\n<td id=\"S4.T1.28.18.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.18.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">82.1</span></td>\n<td id=\"S4.T1.28.18.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.18.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">40.3</span></td>\n<td id=\"S4.T1.28.18.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.18.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">47.0</span></td>\n<td id=\"S4.T1.28.18.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.18.1.7.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">49.6</span></td>\n<td id=\"S4.T1.28.18.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.18.1.8.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">24.6</span></td>\n<td id=\"S4.T1.28.18.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.18.1.9.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">35.1</span></td>\n<td id=\"S4.T1.28.18.1.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.28.18.1.10.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">38.1</span></td>\n<td id=\"S4.T1.28.18.1.11\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.18.1.11.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">32</span></td>\n<td id=\"S4.T1.28.18.1.12\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.18.1.12.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">32</span></td>\n<td id=\"S4.T1.28.18.1.13\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.18.1.13.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">32</span></td>\n<td id=\"S4.T1.28.18.1.14\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.18.1.14.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">✔</span></td>\n</tr>\n<tr id=\"S4.T1.23.11\" class=\"ltx_tr\">\n<td id=\"S4.T1.23.11.2\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T1.23.11.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">FedPAQ</span></td>\n<td id=\"S4.T1.23.11.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.23.11.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">56.3</span></td>\n<td id=\"S4.T1.23.11.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.23.11.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">77.0</span></td>\n<td id=\"S4.T1.23.11.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.23.11.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">81.2</span></td>\n<td id=\"S4.T1.23.11.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.23.11.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">39.7</span></td>\n<td id=\"S4.T1.23.11.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.23.11.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">46.8</span></td>\n<td id=\"S4.T1.23.11.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.23.11.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">48.4</span></td>\n<td id=\"S4.T1.23.11.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.23.11.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">22.87</span></td>\n<td id=\"S4.T1.23.11.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.23.11.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">34.6</span></td>\n<td id=\"S4.T1.23.11.11\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.23.11.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">37.5</span></td>\n<td id=\"S4.T1.23.11.1\" class=\"ltx_td ltx_align_center\"><math id=\"S4.T1.23.11.1.m1.1\" class=\"ltx_Math\" alttext=\"v_{n}\" display=\"inline\"><semantics id=\"S4.T1.23.11.1.m1.1a\"><msub id=\"S4.T1.23.11.1.m1.1.1\" xref=\"S4.T1.23.11.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"S4.T1.23.11.1.m1.1.1.2\" xref=\"S4.T1.23.11.1.m1.1.1.2.cmml\">v</mi><mi mathsize=\"90%\" id=\"S4.T1.23.11.1.m1.1.1.3\" xref=\"S4.T1.23.11.1.m1.1.1.3.cmml\">n</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.23.11.1.m1.1b\"><apply id=\"S4.T1.23.11.1.m1.1.1.cmml\" xref=\"S4.T1.23.11.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.T1.23.11.1.m1.1.1.1.cmml\" xref=\"S4.T1.23.11.1.m1.1.1\">subscript</csymbol><ci id=\"S4.T1.23.11.1.m1.1.1.2.cmml\" xref=\"S4.T1.23.11.1.m1.1.1.2\">𝑣</ci><ci id=\"S4.T1.23.11.1.m1.1.1.3.cmml\" xref=\"S4.T1.23.11.1.m1.1.1.3\">𝑛</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.23.11.1.m1.1c\">v_{n}</annotation></semantics></math></td>\n<td id=\"S4.T1.23.11.12\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.23.11.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">32</span></td>\n<td id=\"S4.T1.23.11.13\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.23.11.13.1\" class=\"ltx_text\" style=\"font-size:90%;\">32</span></td>\n<td id=\"S4.T1.23.11.14\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.23.11.14.1\" class=\"ltx_text\" style=\"font-size:90%;\">✔</span></td>\n</tr>\n<tr id=\"S4.T1.24.12\" class=\"ltx_tr\">\n<td id=\"S4.T1.24.12.2\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T1.24.12.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">UVeQFed</span></td>\n<td id=\"S4.T1.24.12.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.24.12.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">56.8</span></td>\n<td id=\"S4.T1.24.12.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.24.12.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">76.7</span></td>\n<td id=\"S4.T1.24.12.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.24.12.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">81.5</span></td>\n<td id=\"S4.T1.24.12.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.24.12.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">38.6</span></td>\n<td id=\"S4.T1.24.12.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.24.12.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">46.5</span></td>\n<td id=\"S4.T1.24.12.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.24.12.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">48.8</span></td>\n<td id=\"S4.T1.24.12.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.24.12.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">21.3</span></td>\n<td id=\"S4.T1.24.12.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.24.12.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">34.3</span></td>\n<td id=\"S4.T1.24.12.11\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.24.12.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">37.4</span></td>\n<td id=\"S4.T1.24.12.1\" class=\"ltx_td ltx_align_center\"><math id=\"S4.T1.24.12.1.m1.1\" class=\"ltx_Math\" alttext=\"v_{n}\" display=\"inline\"><semantics id=\"S4.T1.24.12.1.m1.1a\"><msub id=\"S4.T1.24.12.1.m1.1.1\" xref=\"S4.T1.24.12.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"S4.T1.24.12.1.m1.1.1.2\" xref=\"S4.T1.24.12.1.m1.1.1.2.cmml\">v</mi><mi mathsize=\"90%\" id=\"S4.T1.24.12.1.m1.1.1.3\" xref=\"S4.T1.24.12.1.m1.1.1.3.cmml\">n</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.24.12.1.m1.1b\"><apply id=\"S4.T1.24.12.1.m1.1.1.cmml\" xref=\"S4.T1.24.12.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.T1.24.12.1.m1.1.1.1.cmml\" xref=\"S4.T1.24.12.1.m1.1.1\">subscript</csymbol><ci id=\"S4.T1.24.12.1.m1.1.1.2.cmml\" xref=\"S4.T1.24.12.1.m1.1.1.2\">𝑣</ci><ci id=\"S4.T1.24.12.1.m1.1.1.3.cmml\" xref=\"S4.T1.24.12.1.m1.1.1.3\">𝑛</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.24.12.1.m1.1c\">v_{n}</annotation></semantics></math></td>\n<td id=\"S4.T1.24.12.12\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.24.12.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">32</span></td>\n<td id=\"S4.T1.24.12.13\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.24.12.13.1\" class=\"ltx_text\" style=\"font-size:90%;\">32</span></td>\n<td id=\"S4.T1.24.12.14\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.24.12.14.1\" class=\"ltx_text\" style=\"font-size:90%;\">✔</span></td>\n</tr>\n<tr id=\"S4.T1.28.19.2\" class=\"ltx_tr\" style=\"background-color:#EFEFEF;\">\n<td id=\"S4.T1.28.19.2.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S4.T1.28.19.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">FPQ8</span></td>\n<td id=\"S4.T1.28.19.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.19.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">54.5</span></td>\n<td id=\"S4.T1.28.19.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.19.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">68.4</span></td>\n<td id=\"S4.T1.28.19.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.19.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">70.8</span></td>\n<td id=\"S4.T1.28.19.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.19.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">35.4</span></td>\n<td id=\"S4.T1.28.19.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.19.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">41.3</span></td>\n<td id=\"S4.T1.28.19.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.19.2.7.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">42.3</span></td>\n<td id=\"S4.T1.28.19.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.19.2.8.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">23.8</span></td>\n<td id=\"S4.T1.28.19.2.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.19.2.9.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">33.4</span></td>\n<td id=\"S4.T1.28.19.2.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.28.19.2.10.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">35.6</span></td>\n<td id=\"S4.T1.28.19.2.11\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.19.2.11.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">8</span></td>\n<td id=\"S4.T1.28.19.2.12\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.19.2.12.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">8</span></td>\n<td id=\"S4.T1.28.19.2.13\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.19.2.13.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">8</span></td>\n<td id=\"S4.T1.28.19.2.14\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.28.19.2.14.1\" class=\"ltx_text\" style=\"font-size:90%;background-color:#EFEFEF;\">✗</span></td>\n</tr>\n<tr id=\"S4.T1.26.14\" class=\"ltx_tr\">\n<td id=\"S4.T1.26.14.3\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T1.26.14.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">AQFL</span></td>\n<td id=\"S4.T1.26.14.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.26.14.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">44.3</span></td>\n<td id=\"S4.T1.26.14.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.26.14.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">58.0</span></td>\n<td id=\"S4.T1.26.14.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.26.14.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">62.1</span></td>\n<td id=\"S4.T1.26.14.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.26.14.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">23.8</span></td>\n<td id=\"S4.T1.26.14.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.26.14.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">32.9</span></td>\n<td id=\"S4.T1.26.14.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.26.14.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">36.1</span></td>\n<td id=\"S4.T1.26.14.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.26.14.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">17.1</span></td>\n<td id=\"S4.T1.26.14.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.26.14.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">23.5</span></td>\n<td id=\"S4.T1.26.14.12\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.26.14.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">25.3</span></td>\n<td id=\"S4.T1.25.13.1\" class=\"ltx_td ltx_align_center\"><math id=\"S4.T1.25.13.1.m1.1\" class=\"ltx_Math\" alttext=\"v_{n}\" display=\"inline\"><semantics id=\"S4.T1.25.13.1.m1.1a\"><msub id=\"S4.T1.25.13.1.m1.1.1\" xref=\"S4.T1.25.13.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"S4.T1.25.13.1.m1.1.1.2\" xref=\"S4.T1.25.13.1.m1.1.1.2.cmml\">v</mi><mi mathsize=\"90%\" id=\"S4.T1.25.13.1.m1.1.1.3\" xref=\"S4.T1.25.13.1.m1.1.1.3.cmml\">n</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.25.13.1.m1.1b\"><apply id=\"S4.T1.25.13.1.m1.1.1.cmml\" xref=\"S4.T1.25.13.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.T1.25.13.1.m1.1.1.1.cmml\" xref=\"S4.T1.25.13.1.m1.1.1\">subscript</csymbol><ci id=\"S4.T1.25.13.1.m1.1.1.2.cmml\" xref=\"S4.T1.25.13.1.m1.1.1.2\">𝑣</ci><ci id=\"S4.T1.25.13.1.m1.1.1.3.cmml\" xref=\"S4.T1.25.13.1.m1.1.1.3\">𝑛</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.25.13.1.m1.1c\">v_{n}</annotation></semantics></math></td>\n<td id=\"S4.T1.26.14.2\" class=\"ltx_td ltx_align_center\"><math id=\"S4.T1.26.14.2.m1.1\" class=\"ltx_Math\" alttext=\"v_{n}\" display=\"inline\"><semantics id=\"S4.T1.26.14.2.m1.1a\"><msub id=\"S4.T1.26.14.2.m1.1.1\" xref=\"S4.T1.26.14.2.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"S4.T1.26.14.2.m1.1.1.2\" xref=\"S4.T1.26.14.2.m1.1.1.2.cmml\">v</mi><mi mathsize=\"90%\" id=\"S4.T1.26.14.2.m1.1.1.3\" xref=\"S4.T1.26.14.2.m1.1.1.3.cmml\">n</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.26.14.2.m1.1b\"><apply id=\"S4.T1.26.14.2.m1.1.1.cmml\" xref=\"S4.T1.26.14.2.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.T1.26.14.2.m1.1.1.1.cmml\" xref=\"S4.T1.26.14.2.m1.1.1\">subscript</csymbol><ci id=\"S4.T1.26.14.2.m1.1.1.2.cmml\" xref=\"S4.T1.26.14.2.m1.1.1.2\">𝑣</ci><ci id=\"S4.T1.26.14.2.m1.1.1.3.cmml\" xref=\"S4.T1.26.14.2.m1.1.1.3\">𝑛</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.26.14.2.m1.1c\">v_{n}</annotation></semantics></math></td>\n<td id=\"S4.T1.26.14.13\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.26.14.13.1\" class=\"ltx_text\" style=\"font-size:90%;\">4</span></td>\n<td id=\"S4.T1.26.14.14\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.26.14.14.1\" class=\"ltx_text\" style=\"font-size:90%;\">✗</span></td>\n</tr>\n<tr id=\"S4.T1.28.16\" class=\"ltx_tr\">\n<td id=\"S4.T1.28.16.3\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span id=\"S4.T1.28.16.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FedMPQ</span></td>\n<td id=\"S4.T1.28.16.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.28.16.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">49.1</span></td>\n<td id=\"S4.T1.28.16.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.28.16.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">67.1</span></td>\n<td id=\"S4.T1.28.16.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.28.16.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">69.3</span></td>\n<td id=\"S4.T1.28.16.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.28.16.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">31.7</span></td>\n<td id=\"S4.T1.28.16.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.28.16.8.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">41.1</span></td>\n<td id=\"S4.T1.28.16.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.28.16.9.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">43.6</span></td>\n<td id=\"S4.T1.28.16.10\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.28.16.10.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">20.3</span></td>\n<td id=\"S4.T1.28.16.11\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.28.16.11.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">27.0</span></td>\n<td id=\"S4.T1.28.16.12\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T1.28.16.12.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">28.2</span></td>\n<td id=\"S4.T1.27.15.1\" class=\"ltx_td ltx_align_center ltx_border_bb\"><math id=\"S4.T1.27.15.1.m1.1\" class=\"ltx_Math\" alttext=\"v_{n}\" display=\"inline\"><semantics id=\"S4.T1.27.15.1.m1.1a\"><msub id=\"S4.T1.27.15.1.m1.1.1\" xref=\"S4.T1.27.15.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"S4.T1.27.15.1.m1.1.1.2\" xref=\"S4.T1.27.15.1.m1.1.1.2.cmml\">v</mi><mi mathsize=\"90%\" id=\"S4.T1.27.15.1.m1.1.1.3\" xref=\"S4.T1.27.15.1.m1.1.1.3.cmml\">n</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.27.15.1.m1.1b\"><apply id=\"S4.T1.27.15.1.m1.1.1.cmml\" xref=\"S4.T1.27.15.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.T1.27.15.1.m1.1.1.1.cmml\" xref=\"S4.T1.27.15.1.m1.1.1\">subscript</csymbol><ci id=\"S4.T1.27.15.1.m1.1.1.2.cmml\" xref=\"S4.T1.27.15.1.m1.1.1.2\">𝑣</ci><ci id=\"S4.T1.27.15.1.m1.1.1.3.cmml\" xref=\"S4.T1.27.15.1.m1.1.1.3\">𝑛</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.27.15.1.m1.1c\">v_{n}</annotation></semantics></math></td>\n<td id=\"S4.T1.28.16.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><math id=\"S4.T1.28.16.2.m1.1\" class=\"ltx_Math\" alttext=\"v_{n}\" display=\"inline\"><semantics id=\"S4.T1.28.16.2.m1.1a\"><msub id=\"S4.T1.28.16.2.m1.1.1\" xref=\"S4.T1.28.16.2.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"S4.T1.28.16.2.m1.1.1.2\" xref=\"S4.T1.28.16.2.m1.1.1.2.cmml\">v</mi><mi mathsize=\"90%\" id=\"S4.T1.28.16.2.m1.1.1.3\" xref=\"S4.T1.28.16.2.m1.1.1.3.cmml\">n</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.28.16.2.m1.1b\"><apply id=\"S4.T1.28.16.2.m1.1.1.cmml\" xref=\"S4.T1.28.16.2.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.T1.28.16.2.m1.1.1.1.cmml\" xref=\"S4.T1.28.16.2.m1.1.1\">subscript</csymbol><ci id=\"S4.T1.28.16.2.m1.1.1.2.cmml\" xref=\"S4.T1.28.16.2.m1.1.1.2\">𝑣</ci><ci id=\"S4.T1.28.16.2.m1.1.1.3.cmml\" xref=\"S4.T1.28.16.2.m1.1.1.3\">𝑛</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.28.16.2.m1.1c\">v_{n}</annotation></semantics></math></td>\n<td id=\"S4.T1.28.16.13\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.28.16.13.1\" class=\"ltx_text\" style=\"font-size:90%;\">4</span></td>\n<td id=\"S4.T1.28.16.14\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.28.16.14.1\" class=\"ltx_text\" style=\"font-size:90%;\">✗</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "To evaluate our method in the scenarios characterized by varied levels of data heterogeneity, we conduct ",
                "3",
                "3",
                "3",
                " sets of experiments where ",
                "α",
                "𝛼",
                "\\alpha",
                " takes on values from\n",
                "{",
                "0.1",
                ",",
                "0.5",
                ",",
                "1",
                "}",
                "0.1",
                "0.5",
                "1",
                "\\{0.1,0.5,1\\}",
                "; these correspond to severely imbalanced, moderately imbalanced and mildly imbalanced data, respectively. Results of the experiments are reported in Table ",
                "LABEL:table1",
                ". As can be seen there, performance of the global model of all the considered methods deteriorates as the data heterogeneity increases. FedPAQ and UVeQFed, the two communication-efficient FL approaches, achieve performance comparable to the FP32 baseline when ",
                "α",
                "=",
                "0.5",
                "𝛼",
                "0.5",
                "\\alpha=0.5",
                " and ",
                "1",
                "1",
                "1",
                " but experience performance degradation when ",
                "α",
                "=",
                "0.1",
                "𝛼",
                "0.1",
                "\\alpha=0.1",
                ". A significant performance decline is experienced in the experiments with FPQ8 due to the low capacity of the low-precision models. However, the performance gap between FP32 and FPQ8 narrows as the level of data heterogeneity increases. For instance, when ",
                "α",
                "=",
                "1",
                "𝛼",
                "1",
                "\\alpha=1",
                ", the test accuracy of FP32 is ",
                "11.3",
                "%",
                "percent",
                "11.3",
                "11.3\\%",
                " higher than that of FPQ8; when ",
                "α",
                "=",
                "0.1",
                "𝛼",
                "0.1",
                "\\alpha=0.1",
                ", the test accuracy difference is ",
                "5.8",
                "%",
                "percent",
                "5.8",
                "5.8\\%",
                " (experiments on CIFAR10). When the data is extremely imbalanced, FedAvg suffers from the so-called “client-drift” problem ",
                "[",
                "27",
                "]",
                " caused by overfitting on the local data. As a result, the model capacity advantage of FP32 (due to having higher precision) might not make as much of a positive impact on the accuracy, explaining the narrowing gap between FP32 and FPQ8.",
                "Since local models with ultra-low precision are aggregated into the global model, performance of AQFL shows further deterioration. The proposed method, FedMPQ outperforms AQFL (implementing fixed-precision quantization) in all scenarios even though training models under the same resource constraints. As shown in Table.",
                "LABEL:table1",
                ", FedMPQ outperforms AFQL at most ",
                "9.1",
                "%",
                "percent",
                "9.1",
                "9.1\\%",
                ", ",
                "8.2",
                "%",
                "percent",
                "8.2",
                "8.2\\%",
                " and ",
                "2.9",
                "%",
                "percent",
                "2.9",
                "2.9\\%",
                " test accuracy on CIFAR10, CIFAR100 and Tiny-ImageNet respectively. On CIFAR10/100 datasets, FedMPQ nearly preserves performance of FPQ8 baseline in the settings ",
                "α",
                "=",
                "0.5",
                "𝛼",
                "0.5",
                "\\alpha=0.5",
                " and ",
                "1",
                "1",
                "1",
                " by efficiently allocating precision to different layers, even though training the global model on resource-constrained heterogeneous devices."
            ]
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Test accuracy (%) as the number of clients N𝑁N varies over 10,20102010,20 and 404040. Here, 20%percent2020\\%, 30%percent3030\\%, 30%percent3030\\% and 20%percent2020\\% clients have average bit-width budget of 222 bits, 444 bits, 666 bits and 888 bits, respectively. The concentration parameter α𝛼\\alpha is set to 0.50.50.5. ",
        "table": "<table id=\"S4.T2.27\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.27.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.27.2.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"></th>\n<th id=\"S4.T2.27.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span id=\"S4.T2.27.2.1.2.1\" class=\"ltx_text ltx_font_bold\">CIFAR10</span></th>\n<th id=\"S4.T2.27.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span id=\"S4.T2.27.2.1.3.1\" class=\"ltx_text ltx_font_bold\">CIFAR100</span></th>\n</tr>\n<tr id=\"S4.T2.27.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.27.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><math id=\"S4.T2.27.1.1.m1.1\" class=\"ltx_Math\" alttext=\"N\" display=\"inline\"><semantics id=\"S4.T2.27.1.1.m1.1a\"><mi id=\"S4.T2.27.1.1.m1.1.1\" xref=\"S4.T2.27.1.1.m1.1.1.cmml\">N</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.27.1.1.m1.1b\"><ci id=\"S4.T2.27.1.1.m1.1.1.cmml\" xref=\"S4.T2.27.1.1.m1.1.1\">𝑁</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.27.1.1.m1.1c\">N</annotation></semantics></math></th>\n<th id=\"S4.T2.27.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">10</th>\n<th id=\"S4.T2.27.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">20</th>\n<th id=\"S4.T2.27.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">40</th>\n<th id=\"S4.T2.27.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">10</th>\n<th id=\"S4.T2.27.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">20</th>\n<th id=\"S4.T2.27.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">40</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.27.3.1\" class=\"ltx_tr\" style=\"background-color:#EFEFEF;\">\n<th id=\"S4.T2.27.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T2.27.3.1.1.1\" class=\"ltx_text\" style=\"background-color:#EFEFEF;\">FP32</span></th>\n<td id=\"S4.T2.27.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.27.3.1.2.1\" class=\"ltx_text\" style=\"background-color:#EFEFEF;\">77.5</span></td>\n<td id=\"S4.T2.27.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.27.3.1.3.1\" class=\"ltx_text\" style=\"background-color:#EFEFEF;\">68.1</span></td>\n<td id=\"S4.T2.27.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.27.3.1.4.1\" class=\"ltx_text\" style=\"background-color:#EFEFEF;\">64.5</span></td>\n<td id=\"S4.T2.27.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.27.3.1.5.1\" class=\"ltx_text\" style=\"background-color:#EFEFEF;\">47.0</span></td>\n<td id=\"S4.T2.27.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.27.3.1.6.1\" class=\"ltx_text\" style=\"background-color:#EFEFEF;\">40.1</span></td>\n<td id=\"S4.T2.27.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.27.3.1.7.1\" class=\"ltx_text\" style=\"background-color:#EFEFEF;\">35.0</span></td>\n</tr>\n<tr id=\"S4.T2.27.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.27.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedPAQ</th>\n<td id=\"S4.T2.27.4.2.2\" class=\"ltx_td ltx_align_center\">77.0</td>\n<td id=\"S4.T2.27.4.2.3\" class=\"ltx_td ltx_align_center\">61.5</td>\n<td id=\"S4.T2.27.4.2.4\" class=\"ltx_td ltx_align_center\">59.9</td>\n<td id=\"S4.T2.27.4.2.5\" class=\"ltx_td ltx_align_center\">46.8</td>\n<td id=\"S4.T2.27.4.2.6\" class=\"ltx_td ltx_align_center\">38.9</td>\n<td id=\"S4.T2.27.4.2.7\" class=\"ltx_td ltx_align_center\">33.1</td>\n</tr>\n<tr id=\"S4.T2.27.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.27.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">UVeQFed</th>\n<td id=\"S4.T2.27.5.3.2\" class=\"ltx_td ltx_align_center\">76.7</td>\n<td id=\"S4.T2.27.5.3.3\" class=\"ltx_td ltx_align_center\">66.4</td>\n<td id=\"S4.T2.27.5.3.4\" class=\"ltx_td ltx_align_center\">63.2</td>\n<td id=\"S4.T2.27.5.3.5\" class=\"ltx_td ltx_align_center\">46.5</td>\n<td id=\"S4.T2.27.5.3.6\" class=\"ltx_td ltx_align_center\">39.5</td>\n<td id=\"S4.T2.27.5.3.7\" class=\"ltx_td ltx_align_center\">34.1</td>\n</tr>\n<tr id=\"S4.T2.27.6.4\" class=\"ltx_tr\" style=\"background-color:#EFEFEF;\">\n<th id=\"S4.T2.27.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T2.27.6.4.1.1\" class=\"ltx_text\" style=\"background-color:#EFEFEF;\">FPQ8</span></th>\n<td id=\"S4.T2.27.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.27.6.4.2.1\" class=\"ltx_text\" style=\"background-color:#EFEFEF;\">68.4</span></td>\n<td id=\"S4.T2.27.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.27.6.4.3.1\" class=\"ltx_text\" style=\"background-color:#EFEFEF;\">56.3</span></td>\n<td id=\"S4.T2.27.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.27.6.4.4.1\" class=\"ltx_text\" style=\"background-color:#EFEFEF;\">48.4</span></td>\n<td id=\"S4.T2.27.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.27.6.4.5.1\" class=\"ltx_text\" style=\"background-color:#EFEFEF;\">41.3</span></td>\n<td id=\"S4.T2.27.6.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.27.6.4.6.1\" class=\"ltx_text\" style=\"background-color:#EFEFEF;\">36.2</span></td>\n<td id=\"S4.T2.27.6.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.27.6.4.7.1\" class=\"ltx_text\" style=\"background-color:#EFEFEF;\">31.9</span></td>\n</tr>\n<tr id=\"S4.T2.27.7.5\" class=\"ltx_tr\">\n<th id=\"S4.T2.27.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">AQFL</th>\n<td id=\"S4.T2.27.7.5.2\" class=\"ltx_td ltx_align_center\">58.0</td>\n<td id=\"S4.T2.27.7.5.3\" class=\"ltx_td ltx_align_center\">49.7</td>\n<td id=\"S4.T2.27.7.5.4\" class=\"ltx_td ltx_align_center\">37.3</td>\n<td id=\"S4.T2.27.7.5.5\" class=\"ltx_td ltx_align_center\">32.9</td>\n<td id=\"S4.T2.27.7.5.6\" class=\"ltx_td ltx_align_center\">20.4</td>\n<td id=\"S4.T2.27.7.5.7\" class=\"ltx_td ltx_align_center\">13.9</td>\n</tr>\n<tr id=\"S4.T2.27.8.6\" class=\"ltx_tr\">\n<th id=\"S4.T2.27.8.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"S4.T2.27.8.6.1.1\" class=\"ltx_text ltx_font_bold\">FedMPQ</span></th>\n<td id=\"S4.T2.27.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.27.8.6.2.1\" class=\"ltx_text ltx_font_bold\">67.1</span></td>\n<td id=\"S4.T2.27.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.27.8.6.3.1\" class=\"ltx_text ltx_font_bold\">56.8</span></td>\n<td id=\"S4.T2.27.8.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.27.8.6.4.1\" class=\"ltx_text ltx_font_bold\">45.4</span></td>\n<td id=\"S4.T2.27.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.27.8.6.5.1\" class=\"ltx_text ltx_font_bold\">41.1</span></td>\n<td id=\"S4.T2.27.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.27.8.6.6.1\" class=\"ltx_text ltx_font_bold\">26.1</span></td>\n<td id=\"S4.T2.27.8.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.27.8.6.7.1\" class=\"ltx_text ltx_font_bold\">19.9</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "To evaluate our method in the scenarios characterized by varied levels of data heterogeneity, we conduct ",
                "3",
                "3",
                "3",
                " sets of experiments where ",
                "α",
                "𝛼",
                "\\alpha",
                " takes on values from\n",
                "{",
                "0.1",
                ",",
                "0.5",
                ",",
                "1",
                "}",
                "0.1",
                "0.5",
                "1",
                "\\{0.1,0.5,1\\}",
                "; these correspond to severely imbalanced, moderately imbalanced and mildly imbalanced data, respectively. Results of the experiments are reported in Table ",
                "LABEL:table1",
                ". As can be seen there, performance of the global model of all the considered methods deteriorates as the data heterogeneity increases. FedPAQ and UVeQFed, the two communication-efficient FL approaches, achieve performance comparable to the FP32 baseline when ",
                "α",
                "=",
                "0.5",
                "𝛼",
                "0.5",
                "\\alpha=0.5",
                " and ",
                "1",
                "1",
                "1",
                " but experience performance degradation when ",
                "α",
                "=",
                "0.1",
                "𝛼",
                "0.1",
                "\\alpha=0.1",
                ". A significant performance decline is experienced in the experiments with FPQ8 due to the low capacity of the low-precision models. However, the performance gap between FP32 and FPQ8 narrows as the level of data heterogeneity increases. For instance, when ",
                "α",
                "=",
                "1",
                "𝛼",
                "1",
                "\\alpha=1",
                ", the test accuracy of FP32 is ",
                "11.3",
                "%",
                "percent",
                "11.3",
                "11.3\\%",
                " higher than that of FPQ8; when ",
                "α",
                "=",
                "0.1",
                "𝛼",
                "0.1",
                "\\alpha=0.1",
                ", the test accuracy difference is ",
                "5.8",
                "%",
                "percent",
                "5.8",
                "5.8\\%",
                " (experiments on CIFAR10). When the data is extremely imbalanced, FedAvg suffers from the so-called “client-drift” problem ",
                "[",
                "27",
                "]",
                " caused by overfitting on the local data. As a result, the model capacity advantage of FP32 (due to having higher precision) might not make as much of a positive impact on the accuracy, explaining the narrowing gap between FP32 and FPQ8.",
                "Since local models with ultra-low precision are aggregated into the global model, performance of AQFL shows further deterioration. The proposed method, FedMPQ outperforms AQFL (implementing fixed-precision quantization) in all scenarios even though training models under the same resource constraints. As shown in Table.",
                "LABEL:table1",
                ", FedMPQ outperforms AFQL at most ",
                "9.1",
                "%",
                "percent",
                "9.1",
                "9.1\\%",
                ", ",
                "8.2",
                "%",
                "percent",
                "8.2",
                "8.2\\%",
                " and ",
                "2.9",
                "%",
                "percent",
                "2.9",
                "2.9\\%",
                " test accuracy on CIFAR10, CIFAR100 and Tiny-ImageNet respectively. On CIFAR10/100 datasets, FedMPQ nearly preserves performance of FPQ8 baseline in the settings ",
                "α",
                "=",
                "0.5",
                "𝛼",
                "0.5",
                "\\alpha=0.5",
                " and ",
                "1",
                "1",
                "1",
                " by efficiently allocating precision to different layers, even though training the global model on resource-constrained heterogeneous devices."
            ]
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Test accuracy (%percent\\%) of FedMPQ running with different combinations of threshold value ϵitalic-ϵ\\epsilon and regularization weights λ𝜆\\lambda. All experiments are on CIFAR10 with 101010 clients and α=0.5𝛼0.5\\alpha=0.5.",
        "table": "<table id=\"S4.T3.12\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.11.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.11.1.2\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"></th>\n<th id=\"S4.T3.11.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\">threshold value <math id=\"S4.T3.11.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\epsilon\" display=\"inline\"><semantics id=\"S4.T3.11.1.1.m1.1a\"><mi id=\"S4.T3.11.1.1.m1.1.1\" xref=\"S4.T3.11.1.1.m1.1.1.cmml\">ϵ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.11.1.1.m1.1b\"><ci id=\"S4.T3.11.1.1.m1.1.1.cmml\" xref=\"S4.T3.11.1.1.m1.1.1\">italic-ϵ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.11.1.1.m1.1c\">\\epsilon</annotation></semantics></math>\n</th>\n</tr>\n<tr id=\"S4.T3.12.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.12.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><math id=\"S4.T3.12.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda\" display=\"inline\"><semantics id=\"S4.T3.12.2.1.m1.1a\"><mi id=\"S4.T3.12.2.1.m1.1.1\" xref=\"S4.T3.12.2.1.m1.1.1.cmml\">λ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.12.2.1.m1.1b\"><ci id=\"S4.T3.12.2.1.m1.1.1.cmml\" xref=\"S4.T3.12.2.1.m1.1.1\">𝜆</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.12.2.1.m1.1c\">\\lambda</annotation></semantics></math></th>\n<th id=\"S4.T3.12.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.01</th>\n<th id=\"S4.T3.12.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.02</th>\n<th id=\"S4.T3.12.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.03</th>\n<th id=\"S4.T3.12.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.04</th>\n<th id=\"S4.T3.12.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.05</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.12.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.12.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.1</th>\n<td id=\"S4.T3.12.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">64.1</td>\n<td id=\"S4.T3.12.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">63.4</td>\n<td id=\"S4.T3.12.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">65.9</td>\n<td id=\"S4.T3.12.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">63.9</td>\n<td id=\"S4.T3.12.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">64.8</td>\n</tr>\n<tr id=\"S4.T3.12.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.12.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">0.01</th>\n<td id=\"S4.T3.12.4.2.2\" class=\"ltx_td ltx_align_center\">66.4</td>\n<td id=\"S4.T3.12.4.2.3\" class=\"ltx_td ltx_align_center\">68.0</td>\n<td id=\"S4.T3.12.4.2.4\" class=\"ltx_td ltx_align_center\" style=\"background-color:#C0C0C0;\"><span id=\"S4.T3.12.4.2.4.1\" class=\"ltx_text\" style=\"background-color:#C0C0C0;\">67.1</span></td>\n<td id=\"S4.T3.12.4.2.5\" class=\"ltx_td ltx_align_center\">64.7</td>\n<td id=\"S4.T3.12.4.2.6\" class=\"ltx_td ltx_align_center\">64.3</td>\n</tr>\n<tr id=\"S4.T3.12.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.12.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">0.001</th>\n<td id=\"S4.T3.12.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">67.1</td>\n<td id=\"S4.T3.12.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">70.0</td>\n<td id=\"S4.T3.12.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">65.5</td>\n<td id=\"S4.T3.12.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">64.7</td>\n<td id=\"S4.T3.12.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">64.4</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "The experiments discussed in the previous section are conducted with hyperparameters ",
                "ϵ",
                "=",
                "0.03",
                "italic-ϵ",
                "0.03",
                "\\epsilon=0.03",
                " and ",
                "λ",
                "=",
                "0.01",
                "𝜆",
                "0.01",
                "\\lambda=0.01",
                ". Note that ",
                "ϵ",
                "italic-ϵ",
                "\\epsilon",
                " and ",
                "λ",
                "𝜆",
                "\\lambda",
                " jointly affect the training: ",
                "λ",
                "𝜆",
                "\\lambda",
                " controls the weight of the group Lasso regularization used in local training while ",
                "ϵ",
                "italic-ϵ",
                "\\epsilon",
                " controls the threshold for pruning the MSBs after local training. To explore the space of hyper-parameters, we consider a number of configurations with varied values of ",
                "ϵ",
                "∈",
                "{",
                "0.01",
                ",",
                "0.02",
                ",",
                "0.03",
                ",",
                "0.04",
                ",",
                "0.05",
                "}",
                "italic-ϵ",
                "0.01",
                "0.02",
                "0.03",
                "0.04",
                "0.05",
                "\\epsilon\\in\\{0.01,0.02,0.03,0.04,0.05\\}",
                " and ",
                "λ",
                "∈",
                "{",
                "0.1",
                ",",
                "0.01",
                ",",
                "0.001",
                "}",
                "𝜆",
                "0.1",
                "0.01",
                "0.001",
                "\\lambda\\in\\{0.1,0.01,0.001\\}",
                ". As the results shown in Table ",
                "LABEL:table3",
                " indicate, when the threshold ",
                "ϵ",
                "italic-ϵ",
                "\\epsilon",
                " is too large, the accuracy considerably deteriorates since a larger fraction of model parameters gets compressed. Selecting large regularization weights, e.g. ",
                "λ",
                "=",
                "0.1",
                "𝜆",
                "0.1",
                "\\lambda=0.1",
                ", leads to the performance drop since FedMPQ focuses on pursuing higher bit-level sparsity. Our experiments suggest that to achieve satisfactory performance, one should select ",
                "ϵ",
                "≤",
                "0.03",
                "italic-ϵ",
                "0.03",
                "\\epsilon\\leq 0.03",
                " and ",
                "λ",
                "≤",
                "0.01",
                "𝜆",
                "0.01",
                "\\lambda\\leq 0.01",
                "."
            ]
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Test accuracy (%percent\\%) of the global model trained using different combinations of the FedMPQ subroutines. “Lasso” refers to the group Lasso regularization; “MSBs” denotes bit-level pruning. All experiments involve 101010 clients. α,ϵ𝛼italic-ϵ\\alpha,\\epsilon and λ𝜆\\lambda are set to 0.50.50.5, 0.030.030.03 and 0.010.010.01, respectively (the same setting as in Section 4.1). ",
        "table": "<table id=\"S4.T4.17\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T4.17.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.17.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"></th>\n<th id=\"S4.T4.17.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T4.17.1.1.2.1\" class=\"ltx_text ltx_font_bold\">CIFAR10</span></th>\n<th id=\"S4.T4.17.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T4.17.1.1.3.1\" class=\"ltx_text ltx_font_bold\">CIFAR100</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.17.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.17.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">AQFL (baseline)</th>\n<td id=\"S4.T4.17.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">58.0</td>\n<td id=\"S4.T4.17.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">32.9</td>\n</tr>\n<tr id=\"S4.T4.17.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.17.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">(1) Lasso</th>\n<td id=\"S4.T4.17.3.2.2\" class=\"ltx_td ltx_align_center\">56.8</td>\n<td id=\"S4.T4.17.3.2.3\" class=\"ltx_td ltx_align_center\">32.6</td>\n</tr>\n<tr id=\"S4.T4.17.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T4.17.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">(2) MSBs</th>\n<td id=\"S4.T4.17.4.3.2\" class=\"ltx_td ltx_align_center\">55.1</td>\n<td id=\"S4.T4.17.4.3.3\" class=\"ltx_td ltx_align_center\">28.7</td>\n</tr>\n<tr id=\"S4.T4.17.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T4.17.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">(3) MSBs + Lasso</th>\n<td id=\"S4.T4.17.5.4.2\" class=\"ltx_td ltx_align_center\">56.2</td>\n<td id=\"S4.T4.17.5.4.3\" class=\"ltx_td ltx_align_center\">31.8</td>\n</tr>\n<tr id=\"S4.T4.17.6.5\" class=\"ltx_tr\">\n<th id=\"S4.T4.17.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">(4) MSBs + Alg.<a href=\"#algorithm2\" title=\"Algorithm 2 ‣ 3.4.2 Models Aggregation ‣ 3.4 The End-to-End Training Procedure ‣ 3 Methodology ‣ Mixed-Precision Quantization for Federated Learning on Resource-Constrained Heterogeneous Devices\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>\n</th>\n<td id=\"S4.T4.17.6.5.2\" class=\"ltx_td ltx_align_center\">65.4</td>\n<td id=\"S4.T4.17.6.5.3\" class=\"ltx_td ltx_align_center\">40.1</td>\n</tr>\n<tr id=\"S4.T4.17.7.6\" class=\"ltx_tr\">\n<th id=\"S4.T4.17.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">(5) MSBs + Lasso + Alg.<a href=\"#algorithm2\" title=\"Algorithm 2 ‣ 3.4.2 Models Aggregation ‣ 3.4 The End-to-End Training Procedure ‣ 3 Methodology ‣ Mixed-Precision Quantization for Federated Learning on Resource-Constrained Heterogeneous Devices\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>\n</th>\n<td id=\"S4.T4.17.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.17.7.6.2.1\" class=\"ltx_text ltx_font_bold\">67.1</span></td>\n<td id=\"S4.T4.17.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.17.7.6.3.1\" class=\"ltx_text ltx_font_bold\">41.1</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we empirically analyze the effect of each procedure in FedMPQ by a comparison to the AQFL baseline. The three procedures that distinguish FedMPQ from AQFL include: (1) the group Lasso regularization in the objective function as described in Section ",
                "3.3",
                "; (2) bit-level pruning in the most significant bits (MSBs); and (3) Alg. ",
                "2",
                " which restores the precision of local models. We refer to different combinations of these procedures, shown in Table ",
                "LABEL:table4",
                ", as settings (1)-(5).",
                "According to the results for setting (1) in Table ",
                "LABEL:table4",
                ", the group Lasso regularization achieves high performance while promoting bit-level sparsity, with small ",
                "1.2",
                "%",
                "percent",
                "1.2",
                "1.2\\%",
                " and ",
                "0.3",
                "%",
                "percent",
                "0.3",
                "0.3\\%",
                " drops in accuracy on CIFAR10 and CIFAR100, respectively. As shown in the results for setting (2), MSBs pruning without sparsity-promoting training causes more severe performance degradation, with ",
                "2.9",
                "%",
                "percent",
                "2.9",
                "2.9\\%",
                " and ",
                "4.2",
                "%",
                "percent",
                "4.2",
                "4.2\\%",
                " accuracy drop on these two datasets. Finally, by combining the group Lasso regularization with MSBs pruning (setting (3)) enables the global model to achieve performance close to the baseline, even though the precision is reduced after MSBs pruning. Interestingly, the group Lasso regularization forces the clients to learn local models with highly sparse binary weight representations, implying no degradation due to bit-pruning.",
                "Algorithm ",
                "2",
                " enables clients to recover layer-wise precision budget allocated to their local models, resulting in significant performance improvements. For instance, test accuracy in setting (4) is ",
                "10.3",
                "%",
                "percent",
                "10.3",
                "10.3\\%",
                " and ",
                "11.4",
                "%",
                "percent",
                "11.4",
                "11.4\\%",
                " higher than in setting (2); setting (5) achieves ",
                "10.9",
                "%",
                "percent",
                "10.9",
                "10.9\\%",
                " and ",
                "9.3",
                "%",
                "percent",
                "9.3",
                "9.3\\%",
                " higher accuracy than setting (3) on CIFAR10 and CIFAR100, respectively.",
                "This ablation study provides an insight in how FedMPQ operates: the group Lasso regularization forces clients to learn a local model with bit-level sparsity while preserving performance; MSBs pruning allows reduction of the precision of local models without major performance degradation; Algorithm ",
                "2",
                ", implemented at the server, conducts ",
                "pruning-growing",
                " to restore the precision of local models so they fully exploit allocated bit-width budgets while seeking effective bit-width allocations to layers. The results in Table ",
                "LABEL:table4",
                " suggest that Algorithm ",
                "2",
                " plays a major role in helping FedMPQ achieve high accuracy."
            ]
        ]
    }
}