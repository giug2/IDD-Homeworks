{
    "id_table_1": {
        "caption": "Table 1:  Quantitative Description of the Genuine and Generated CSW Datasets Using Various CSW Metrics.",
        "table": "S2.T1.1",
        "footnotes": [],
        "references": [
            "Code-switching (CSW), the practice of fluidly alternating between two or more languages in conversation, has become commonplace in recent years. This linguistic phenomenon, emerging as a natural consequence of multilingualism, is now widely accepted in social and professional settings  Yow et al. ( 2018 ) . Many works have highlighted the utility and cultural importance of CSW in general conversation  Beatty-Martinez et al. ( 2020 ); Falbo and LaCroix ( 2021 ) . Further research indicates that these advantages extend to language learning, with CSW offering many pedagogical benefits. These include increasing students access to content and improving their confidence.  Nguyen et al. ( 2022 )  discuss the mechanisms for this, where students use a familiar language to grasp foreign, complex concepts. CSW can also serve as a scaffolding tool, helping to bridge gaps in a students comprehension of a language and enabling them to build upon existing knowledge. These benefits reduce the barriers between a student and their target language and help promote a learning environment conducive with active exploration and deeper understanding. Therefore, it is essential that English as a Second Language (ESL) learners are not penalised for expressing their cultural identity through CSW. Grammatical error correction (GEC) is the task of automatically detecting and correcting errors in text. Research on GEC for CSW text remained largely unexplored.  Chan et al. ( 2024 )  were the first to demonstrate that exposing a sequence-tagging GEC model to CSW text during the training process improves performance compared to a monolingual system. However, further work is essential to ensure language technology is inclusive and reflective of real-world linguistic practices. Figure  1  shows two examples of CSW from our target population with their grammatical corrections. 1 1 1 The definition of CSW is a subject of ongoing debate. Throughout this work, we use the term CSW to refer specifically to the type of language mixing exhibited by ESL learners.",
            "We used several CSW metrics to quantify the qualities of CSW texts: Code Mixing Index (CMI)  Gamback and Das ( 2016 ) , Multilingual Index (M-Index)  Barnett et al. ( 2000 ) , Probability of Switching (I-Index)  Guzman et al. ( 2017 ) , Burstiness  Goh and Barabasi ( 2008 ) , and Complexity Factor (CF1-3)  Ghosh et al. ( 2017 ) . Table  1  shows the value of each metric for our genuine CSW dataset, as well as for these 3 synthetic CSW datasets. We can see that the LLM prompting-based dataset was superior in its similarity to the authentic CSW data. Using this method, we generated a corpus of 73,293 utterances covering over 20 English language pairs, including the 6 language pairs in the original dataset. 7 7 7 The LLM does not always generate the language pairs we ask for. However, these sentences are still included in the dataset categorised under their actual language pair.",
            "The synthetic CSW text and error injection methods were central to this project. The resemblance of our synthetic text to real ESL learner data, as shown by the similarity metrics in Table  1 , is a testament to the effectiveness of our chosen generation method. The improvements in  F 0.5 subscript F 0.5 F_{0.5} italic_F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT  scores provide further evidence of this."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  ERRANT-based Precision, Recall and  F 0.5 subscript F 0.5 F_{0.5} italic_F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT  Scores of Baselines and Our Model Throughout Training",
        "table": "S2.T2.2.2",
        "footnotes": [],
        "references": [
            "We compared our model against two well-established systems: a RoBERTa-base GECToR model  Omelianchuk et al. ( 2020 ) , with near SOTA performance on the BEA-2019 test set  Bryant et al. ( 2019 ) , and a seq2seq T5 model  Rothe et al. ( 2021 ) . To assess these models, we evaluated their performance on the BEA-2019 test set and the remaining 10% of our authentic CSW data. The ERRANT  Bryant et al. ( 2017 )  GEC evaluation results, as outlined in Table  2 , demonstrate a clear degradation in performance when these two systems are applied to CSW texts. The ERRANT toolkit detects and classifies edits between source and target sentence pairs into predefined error categories. It enables the comparison of a proposed set of edits with a reference set, providing a way of calculating metrics, such as precision and recall, across these categories.",
            "The progression of our model throughout training provided insights into its evolving capabilities and effectiveness of our synthetic data. We monitored several metrics, including the ERRANT precision, recall and  F 0.5 subscript F 0.5 F_{0.5} italic_F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT  score, for the BEA-2019 test set and the remaining unused 10% of our genuine CSW dataset. These metrics, as displayed in Table  2 , indicate a steady improvement in the ability to handle CSW texts. Notably, the performance on the CSW dataset shows a significant leap in the final stages, where the contribution of our synthetic dataset is largest. This improvement in CSW text handling did slightly compromise the models performance on monolingual GEC tasks, as seen on the BEA-2019 test set. This suggests a trade-off inherent in specialising the model for CSW contexts. However, our model remains competitive amongst SOTA monolingual GEC systems of its size.",
            "Three illustrative examples of our models corrections, taken from the CSW test set, can be seen in Figure  2 . The first example demonstrates a case where the model has correctly identified all of the changes required, including the incorrect capitalisation of a word, a missing word, and some missing punctuation. The second example shows a near miss; here, the model has correctly identified the majority of the changes required but dropped the I whilst rearranging the start of the sentence. Finally, the third example presents a scenario where the model has fallen slightly short, failing to recognise the need for were instead of was in this hypothetical context."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  F 0.5 subscript F 0.5 F_{0.5} italic_F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT  Scores, TP, FP, FN, and Differences in  F 0.5 subscript F 0.5 F_{0.5} italic_F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT  Scores (BEA - CSW) for Different Categories in the BEA-2019 Test Split and our Genuine CSW Dataset.",
        "table": "A2.T3.3.3",
        "footnotes": [],
        "references": [
            "Figure  3  shows an example LLM prompt used to generate synthetic CSW sentences from genuine examples. As we are using a private subset of the Lang-8 dataset, we are not permitted to share any of the CSW texts.",
            "Table  3  shows a breakdown of the performance of a single RoBERTa Large-based GECToR system trained purely on monolingual GEC data when applied to two datasets, our genuine CSW dataset and the BEA-2019  Bryant et al. ( 2019 )  test set. These datasets are approximately the same size. The model used represents a current near-SOTA single model sequence tagging-based GEC system measured using  F 0.5 subscript F 0.5 F_{0.5} italic_F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT  on the BEA-2019 test set. For brevity, we have removed categories with a low number of examples in either dataset or where performance is not significantly different."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Sentence Count and Contribution of Stage 2 Datasets",
        "table": "A3.T4.1",
        "footnotes": [],
        "references": [
            "For the second stage, we shuffled several GEC datasets. These are NUCLE  Dahlmeier et al. ( 2023 ) , FCE  Yannakoudakis et al. ( 2011 ) , W&I Locness  Bryant et al. ( 2019 ) , Lang-8  Mizumoto et al. ( 2013 )  and our 2 newly created CSW datasets. As our genuine CSW dataset is a subset of the private Lang-8 corpus, we checked and removed any duplicates. Table  4  shows the overall contributions of each corpus towards the stage 2 dataset. Similar to the previous stage, the data was split into train and validation sets."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Sentence Count and Contribution of Stage 3 Datasets",
        "table": "A3.T5.1",
        "footnotes": [],
        "references": [
            "For the final stage, we combined the high quality W&I Locness dataset with a sampled subset of the genuine CSW data and a sampled subset of the synthetic CSW texts. Again, the stage 3 dataset is split into train and validation sets. The remaining unused subset of the genuine CSW dataset was retained for testing purposes. Table  5  details the contributions to this stage from each dataset."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Precision (P), Recall (R) and  F 0.5 subscript F 0.5 F_{0.5} italic_F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT  Score of Our Proposed Model for Targeted Error Types in the Genuine CSW Test Dataset",
        "table": "A5.T6.1",
        "footnotes": [],
        "references": [
            "By exploring the ERRANT error classifications of our proposed model when applied to the CSW test dataset, we can further explore the effectiveness of our synthetic data in addressing the problematic areas identified in Appendix  B . A breakdown of the precision, recall and  F 0.5 subscript F 0.5 \\text{F}_{0.5} F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT  score for each of the previously identified categories is shown in Table  6 ."
        ]
    }
}