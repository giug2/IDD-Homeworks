{
    "id_table_1": {
        "caption": "Table 1:  An example of how the KnowPO dataset is formulated. LLMs parameter knowledge are marked in  orange , while conflicting knowledge in context are marked in  green , and noisy information is presented in  red .",
        "table": "Sx3.T1.1",
        "footnotes": [],
        "references": [
            "In response to the aforementioned issue, a mainstream approach is to construct specific instruction-tuning datasets to optimize the knowledge prioritization of LLMs in contexts with varying degrees of relevance  (Li et al.  2022 ; Xue et al.  2023 ) . However, as shown in Figure  1 , achieving a balance between  adherence capability  and  noise robustness  is highly challenging. On one hand, when the LLM heavily relies on external knowledge, it risks over-focusing on irrelevant retrieval contexts, struggling to effectively discern noise. On the other hand, an excessive emphasis on enhancing the LLMs noise resistance can inadvertently filter out useful contextual information  (Wu, Wu, and Zou  2024 ) . Moreover, the manifestation of these capabilities is closely related to the complexity of the context in real-world RAG scenarios  (Longpre et al.  2022 ; Xie et al.  2024 ) . Therefore, it is crucial to address the balance between adherence capability and noise robustness in real RAG scenarios.",
            "Specifically, we first extract world knowledge acquired during the pretraining phase of the large model, marked as parameter answer    \\alpha italic_ . We encourage LLM to abstain from answering when uncertain. Additionally, we refine the response formats for other parametric knowledge. The revised results are presented in Table  1 .",
            "As previously mentioned, we expect LLMs to utilize contextual knowledge when encountering conflicting context, while relying on parameter knowledge when faced with irrelevant context. These two modes of handling context reflect the models adherence capability and noise robustness, respectively. In practical RAG scenarios, deficiencies in these capabilities manifest as two distinct error types: one in which the LLM incorrectly uses irrelevant contextual information to construct answers, termed  Contextual Overinclusion ; and another where the LLM disregards the context entirely and relies exclusively on its parameter knowledge, termed  Contextual Ignorance . These errors can occur with both types of contexts as illustrated in Table  1 . To address these issues, we have meticulously designed a dataset comprising positive and negative sample pairs to specifically target and mitigate these errors.",
            "In situations with conflicting contexts, the ideal behavior of the LLM demonstrating adherence capability, as shown by positive samples in Table  1 , is to answer using the conflicting knowledge present in the context. However, when contextual overinclusion occurs, LLM often utilizes inappropriate information from the context due to insufficient noise robustness and contextual understanding capability. For instance, in the example presented in Table  1 , LLM chooses noisy information marked in red. To address this error, we constructed negative samples by using a prompt mechanism to guide GPT-4 to generate incorrect answers from conflicting contexts. To ensure the quality of the generated data, we adhered to stringent validation criteria: (1) The generated answers must be derived from the context, ensuring that the error is unequivocally attributable to contextual overinclusion; (2) The generated answers should be as plausible as possible and distinctly different from the conflicting answers, thereby ensuring the high quality of the data.",
            "Contextual Ignorance occurs when the LLM disregards the context in its response, a behavior deemed erroneous solely in conflicting contexts. During such episodes, LLM may either fail to recognize the utility of the context or, even upon recognizing it, may opt to disregard the conflicting answer in favor of relying on its parameter knowledge. For instance, in the example shown in Table  1 , LLM answers the question without utilizing supplemental knowledge. To simulate this error, we constructed negative samples by extracting LLMs response to the query in the absence of any contextual support, ensuring that the answer aligns with an inappropriate erroneous response. Prompt used can be found in Appendix A.",
            "Length Imbalance.   Some studies suggest that reward hacking observed in RLHF can also negatively impact DPO training  (Gao, Schulman, and Hilton  2022 ; Park et al.  2024 ) . We observed that in our previously constructed dataset, for the same preference pair, the positive sample was often the better-formatted and longer response, while the negative sample was a shorter conflicting answer. Due to the tendency of LLMs to be influenced by length bias during DPO  (Singhal et al.  2024 ) , they might prefer generating longer responses, which overall manifests as a greater tendency to refuse answering rather than providing a conflicting answer. To mitigate this issue, we standardized the format for all positive and negative samples in Table  1 , aligning their lengths to ensure that the average length  l  e  n w  i  n l e subscript n w i n len_{win} italic_l italic_e italic_n start_POSTSUBSCRIPT italic_w italic_i italic_n end_POSTSUBSCRIPT  approximately equals  l  e  n l  o  s  s l e subscript n l o s s len_{loss} italic_l italic_e italic_n start_POSTSUBSCRIPT italic_l italic_o italic_s italic_s end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Performance comparison (in percent) on Squad2.0-Eval, RGB and KNOT.  Red shading  indicates the best-performing model, while   blue  signifies the second-best in the ablation study, and  green  signifies the second-best in baselines.",
        "table": "Sx4.T2.145.145",
        "footnotes": [],
        "references": [
            "To answer RQ1, we conduct experiments and report results of the two metrics on Squad2.0-Eval, RGB and KNOT with two LLM turbos, as illustrated in Table  2 . From the reported results, we can find the following observations:",
            "To answer RQ2, we perform ablation studies to verify the effectiveness of KnowPO, as illustrated in Table  2 . Our observation can be summarized as follows:",
            "The LLMs confidence in its responses is one of the factors influencing whether it prefers internal or external knowledge  (Wu, Wu, and Zou  2024 ) . We recorded the LLMs prior probability for parameter knowledge on the RGB dataset and measured the proportion of instances in each prior probability interval where the LLM followed external knowledge when encountering conflicting context. The models prior response probability is computed from the average log probability of the response tokens without external knowledge. The results in Figure  2  show that, for base LLM, there is a general negative correlation between the prior probability of an answer and the proportion of following external knowledge; that is, the higher the prior probability, the less likely the answer is to be altered. However, after fine-tuning with KnowPO, although the overall trend remains negatively correlated, the trend is significantly mitigated, indicating that our method effectively enhances the LLMs adherence to external knowledge.",
            "As analyzed in Section III.E, two types of preference pairs exhibit distinctly opposite behavioral tendencies: those simulating error contextual ignorance in conflicting contexts and those simulating error contextual overinclusion in irrelevant contexts. We conducted a series of analyses by adjusting the ratio  R e  r  r  o  r subscript R e r r o r R_{error} italic_R start_POSTSUBSCRIPT italic_e italic_r italic_r italic_o italic_r end_POSTSUBSCRIPT  between these two types of preference pairs from the list  [ 0.2 , 0.3 , 0.5 , 1 , 2 , 3 , 5 ] 0.2 0.3 0.5 1 2 3 5 [0.2,0.3,0.5,1,2,3,5] [ 0.2 , 0.3 , 0.5 , 1 , 2 , 3 , 5 ] . The results in Figure  2  indicate that as the proportion of the first type of preference pairs increases, the LLM becomes more inclined to utilize contextual knowledge, enhancing its adherence capability but also becoming more susceptible to noise, which in turn reduces its noise robustness. Conversely, as the proportion of the second type increases, the LLM tends to disregard contextual information and respond directly, resulting in reduced  R A  d subscript R A d R_{Ad} italic_R start_POSTSUBSCRIPT italic_A italic_d end_POSTSUBSCRIPT  but improved  R R  o subscript R R o R_{Ro} italic_R start_POSTSUBSCRIPT italic_R italic_o end_POSTSUBSCRIPT . Notably, as the ratio  R e  r  r  o  r subscript R e r r o r R_{error} italic_R start_POSTSUBSCRIPT italic_e italic_r italic_r italic_o italic_r end_POSTSUBSCRIPT  increases from 1, the rate of improvement in adherence slows, while the decline in robustness becomes more pronounced. When the ratio  R e  r  r  o  r subscript R e r r o r R_{error} italic_R start_POSTSUBSCRIPT italic_e italic_r italic_r italic_o italic_r end_POSTSUBSCRIPT  decreases from 1, the curvature of  R A  d subscript R A d R_{Ad} italic_R start_POSTSUBSCRIPT italic_A italic_d end_POSTSUBSCRIPT  and  R R  o subscript R R o R_{Ro} italic_R start_POSTSUBSCRIPT italic_R italic_o end_POSTSUBSCRIPT  also shows the opposite trend. Based on these findings, we ultimately selected  R e  r  r  o  r = 1 subscript R e r r o r 1 R_{error}=1 italic_R start_POSTSUBSCRIPT italic_e italic_r italic_r italic_o italic_r end_POSTSUBSCRIPT = 1  as the optimal ratio, ensuring balanced improvements in both capabilities compared to SFT training.",
            "To demonstrate the robust generalization capability of our method beyond the general domain training set, we conducted experiments on the CMB medical test set. By using medical triplets and documents to construct supplementary contexts in the medical domain, we created a conflict dataset for 4,000 CMB questions. The experimental results, as shown in Table  3 , confirm that the model trained with KnowPO in the general domain also effectively enhances the LLMs adherence capability and noise robustness when transferred to the domain-specific context. The higher scores on CMB compared to those in Table  2  can be attributed to the fact that the contexts we constructed were less challenging than the real-world RAG knowledge."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Performance comparison (in percent) on CMB",
        "table": "Sx4.T3.16.16",
        "footnotes": [],
        "references": [
            "To demonstrate the robust generalization capability of our method beyond the general domain training set, we conducted experiments on the CMB medical test set. By using medical triplets and documents to construct supplementary contexts in the medical domain, we created a conflict dataset for 4,000 CMB questions. The experimental results, as shown in Table  3 , confirm that the model trained with KnowPO in the general domain also effectively enhances the LLMs adherence capability and noise robustness when transferred to the domain-specific context. The higher scores on CMB compared to those in Table  2  can be attributed to the fact that the contexts we constructed were less challenging than the real-world RAG knowledge."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  The match rate between LLMs parameter answers and conflicting answers after training.",
        "table": "Sx4.T4.1",
        "footnotes": [],
        "references": [
            "A potential risk of incorporating QA pairs and conflicting knowledge contexts into the training data is the inadvertent introduction of harmful information to the model. To assess whether the KnowPO-trained model retained any conflicting knowledge from the training set, we utilized prompts designed to extract LLMs parameter knowledge without providing supplemental knowledge. The results, presented in Table  4 , indicate that the model exhibited virtually no retention of conflicting knowledge after the SFT and DPO phases of KnowPO. This finding corroborates that our method enhances the LLMs ability to leverage external knowledge rather than injecting specific knowledge into the model."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  The statistics of datasets.",
        "table": "A2.T5.3",
        "footnotes": [],
        "references": [
            "Details statistics of datasets can be found in Table  5 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Baseline Code URLs of Github Repository",
        "table": "A2.T6.1",
        "footnotes": [],
        "references": [
            "For CFP, we tested the performance of the method under the knowledge conflict setting and adopted the strategy of opinion-based prompts, utilizing a zero-shot prompting paradigm in context learning.For CoT, we used a few-shot prompting paradigm to teach the LLM to generate reasoning chains. Since the context constructed in our dataset closely simulates the context retrieved in real RAG scenarios, for CoT-VE, after generating verification questions for uncertain reasoning instances, we directly used the constructed context as the retrieval result to prompt the LLM to generate answers to the verification questions, thereby editing the current reasoning instance. For KAFT, we constructed a conflicting dataset on Squad2.0 based on the descriptions in its paper, and conducted instruction fine-tuning using the templates employed in the original method with LoRA tuning. We set the learning rate to 1e-5 and training epoch to 1. For CAD, we strictly followed the original methods settings.  For KnowPO, we use lora in DPO stage and set the learning rate to 5e-6 and training epoch to 1. Besides, we replicated CFP, CoT-VE and CAD by referencing the officially released code in Table  6 ."
        ]
    },
    "global_footnotes": [
        "Corresponding authors.",
        "Junfeng Zhao is also at the Big Data Technology Research Center, Nanhu Laboratory, 314002, Jiaxing."
    ]
}