{
    "id_table_1": {
        "caption": "Table 1:  Manual template search by adding additional text and [MASK] ([M]) tokens. Target sentences are inserted into the place of [X]. Only the average of 12th layer [M] representations are used.",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "Some methods are proposed to reduce anisotropy during the training process. In particular, the authors of  [ 120 ]  added the specific regularization. According to them, the aperture of the narrow cone of representations can be improved by minimizing the cosine similarities between  any two word embeddings. Such regularization encourages the vectors to be more evenly spread and expressive. This method is highly related to the now-popular contrastive learning approach (see Section  2.3.1 ). Other authors of  [ 151 ]  improve the isotropy of the output embedding matrix using the spectrum control method. They guide the singular value distribution of the embedding matrix throughout the training process and control the decay rate of these singular values.",
            "During initial experiments, we manually searched for other more complicated templates than T0, presented in Table  1 . We will also use the T4 template, which is much longer and has 3  [MASK]  tokens that have to be averaged.",
            "If we were to look at the detailed performance on individual tasks in Appendix  A.1 , we would find several where random embeddings with the help of the two shaping techniques score higher than also-shaped BERT representations. For STS14 (Table  8 ) it is 68.8 versus 68.3 Spearman correlation, for googleTS (Table  16 ), stackoverflow (Table  18 ), and tweet (Table  19 ) datasets the clustering accuracy is correspondingly 69.5 vs 68.5, 70.6 vs 59.9, and 58.5 vs 55.1. This may indicate a smaller complexity of these particular datasets, where the task can largely be solved based on a small set of keywords. Yet it is important to note that it is achieved only with the help of aggregation and post-processing methods on top of the random embeddings.",
            "We calculate the IsoScore isotropy metric on the Wikitext-2 dataset after applying various post-processing and token aggregation methods for multiple models. We exclude whitening post-processing, as it always produces representations with the IsoScore at the maximum of 100%, just due to its working principle. Then we compare the isotropy score to the STS, clustering, and classification task performance by calculating the Pearson correlation coefficient. For semantic textual similarity tasks, results are shown in Figure  1  and for clustering tasks in Figure  2 .",
            "Six out of eight models have a moderate correlation (more than 54%) between Isoscore and the average Spearman correlation of semantic textual similarity tasks (see Figure  1 ). For clustering (see Figure  2 ) it is less apparent, with only 4 models reaching moderate Pearson correlation. However, for both STS and clustering tasks, the best score for each model is always reached by some improvement of IsoScore, compared to the isotropy score of the plain averaging setup.",
            "We have varied the  w w w italic_w  parameter to also the negative values in ( 10 ) to see if subtracting (instead of adding) the context-average representations from the context-aware ones helps.",
            "We present a strong and very simple baseline model of Random Embeddings (RE), where every token is assigned a random vector as its embedding. Combined with token aggregation and post-processing techniques, it almost matches the average STS performance of the BERT model, also with the techniques applied, with 66.4% versus 69.8% average Spearman correlation. It also shows very high performance for some tasks like stackoverflow classification (see Section  4.1.2  for more details). We encourage future work to use RE as a baseline, due to its mid-level performance, simple implementation, and ability to separate the contribution to the performance of the learned contexts from the aggregation and post-processing techniques.",
            "We found that the aggregation and post-processing techniques tried typically increase the isotropy of the representations, and the isotropy for most models is positively correlated (up to 69% Pearson correlation) with the Spearman correlation of STS tasks (Section  4.1.3 ). The highest isotropy improvement is observed for our Random Embeddings model since its token representations have the maximal isotropy to start with. We did not find the token aggregation and post-processing techniques to improve the alignment and uniformity properties of representations."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Dataset statistics for the short text clustering datasets. N is the number of text samples, C is the number of clusters, L/S is the imbalance number defined as the size of the largest class divided by that of the smallest class,   V  norm V \\|V\\|  italic_V   is vocabulary size, Len is the average number of tokens in each text sample, Alpha is the percent of tokens that are alphabetic (all token characters must be defined in the Unicode character database as Letter, while tokens with numbers, punctuation, or BERT continuation tokens such as ##ing are excluded). Statistics with plain word tokenization are marked with W and with  bert-base-uncased  model tokenizer as B.",
        "table": "S3.T2.3",
        "footnotes": [],
        "references": [
            "The rest of this paper is organized as follows. We provide a review of related work in the literature on composing word vectors and representation reshaping to get sentence- or text-level embeddings in Section  2 . In Section  3 , we outline the experimental setting and give a detailed background on our chosen approach, models, and datasets. In Section  4 , we present the results. Finally, we summarize the findings of this work in Section  5 .",
            "Thousands of works are being done on word-level vectors. Now, you can easily download popular publicly accessible word2vec  [ 16 ]  and GloVe  [ 17 ]  embeddings. They are lightweight and usually fit perfectly various word-level tasks. On the other hand, modeling sequences of words is considered much more challenging. Count-based approaches lose information on word order and are sparse. Learning higher-order n-gram vector space models, not just words but phrases or sentences, leads to sparsity, as frequencies vanish for target n-grams and contexts. The latter, in particular, is the main driving force for distributed representations. We will cover special methods dedicated to sentence level in Subsection  2.3 . Nevertheless, the easiest solution is to reuse individual word vectors of the sequence.",
            "Some methods are proposed to reduce anisotropy during the training process. In particular, the authors of  [ 120 ]  added the specific regularization. According to them, the aperture of the narrow cone of representations can be improved by minimizing the cosine similarities between  any two word embeddings. Such regularization encourages the vectors to be more evenly spread and expressive. This method is highly related to the now-popular contrastive learning approach (see Section  2.3.1 ). Other authors of  [ 151 ]  improve the isotropy of the output embedding matrix using the spectrum control method. They guide the singular value distribution of the embedding matrix throughout the training process and control the decay rate of these singular values.",
            "It is common in machine learning to standardize datasets as most methods are designed to work best with normally distributed data: Gaussian with zero mean and unit variance. Yet it is not that trivial, as data may not follow a smooth distribution and may contain disturbing outliers. In the context of the best practices reviewed in Section  2.2 , we investigate the following processing methods of embeddings.",
            "We assess the performance on 6 benchmark datasets for short text clustering. Compared to the usual ones, short datasets impose a challenge due to the weak signal caused by sparsity, which is a big problem for classic count-based approaches such as bag-of-words or tf-idf. Table  2  provides an overview of the main statistics, and the details of each dataset are as follows.",
            "An interesting result, as seen in Table  4 , is the comparison of simply averaged BERT tokens from many contexts (the Avg. model) and word vectors from a specially-trained word2vec-style model BERT2Static (the B2S model) using the same BERT contexts, as described in Section  3.2.4 . If no token aggregation and post-processing techniques are used, B2S is of similar performance at STS tasks (with average Spearman correlation of 56.7 versus 56.4), worse at clustering (with average clustering accuracy of 53.8 versus 55.2) and comparable at classification tasks (with average B2S accuracy of 76.9 versus 77.3) to the Avg. model. As we can see, the task performance differences are very small.",
            "The largest clustering accuracy difference between BERT and random embeddings is for agnews and searchsnippets datasets, with the best scores of 86.8 versus 43.4 and 82.9 versus 36.6 respectively. We think that the observed performance gap may be related to the vocabulary and text sizes of the datasets. Stackoverflow and tweet datasets, with random embeddings ahead of the BERT, has the smallest vocabulary sizes of 7 332 and 8 091 (see Table  2 ), while for agnews and searchsnippets has the largest vocabulary of 16 140 and 16 334 unique tokens respectively. Because random embeddings rely only on distinctness of tokens, with the increased amount of them, the probability of having exactly the same keywords in two texts drops. BERT embeddings having non-identical but semantically similar tokens similar representations helps in this case.",
            "We calculate the IsoScore isotropy metric on the Wikitext-2 dataset after applying various post-processing and token aggregation methods for multiple models. We exclude whitening post-processing, as it always produces representations with the IsoScore at the maximum of 100%, just due to its working principle. Then we compare the isotropy score to the STS, clustering, and classification task performance by calculating the Pearson correlation coefficient. For semantic textual similarity tasks, results are shown in Figure  1  and for clustering tasks in Figure  2 .",
            "Six out of eight models have a moderate correlation (more than 54%) between Isoscore and the average Spearman correlation of semantic textual similarity tasks (see Figure  1 ). For clustering (see Figure  2 ) it is less apparent, with only 4 models reaching moderate Pearson correlation. However, for both STS and clustering tasks, the best score for each model is always reached by some improvement of IsoScore, compared to the isotropy score of the plain averaging setup.",
            "As we already mentioned, STS tasks preferred the regular BERT ( w = 0 w 0 w=0 italic_w = 0 ) to the BERT+Avg. on average. However, for several individual semantic textual similarity datasets, the best weights turned out to be even negative. This is the case for STS15 ( w =  0.25 w 0.25 w=-0.25 italic_w = - 0.25 ), and STS16 ( w =  0.75 w 0.75 w=-0.75 italic_w = - 0.75 ). The same negative weight of  w =  0.5 w 0.5 w=-0.5 italic_w = - 0.5  preference was also observed for the searchsnippets clustering dataset. Although the Spearman correlation for three STS tasks was only up to several percent higher, for searchsnippets the clustering accuracy increased to a staggering 80.2% from 72.2% of  w = 0.0 w 0.0 w=0.0 italic_w = 0.0 . This shows that the determined values of  w = 0.0 w 0.0 w=0.0 italic_w = 0.0  for STS and  w = 0.5 w 0.5 w=0.5 italic_w = 0.5  for clustering and classification are not universal and for a small percentage of tasks can differ. For more BERT+Avg. task-wise details, see Appendix  A.2  figures for STS (Figure  6 ), clustering (Figure  7 ), and classification (Figure  8 ) performance.",
            "Figure  5  also depicts the task performance versus BERT layers. Our first observation is that for STS and clustering tasks, there is a strong preference for the first layers. As a result, the best combination of layers also involves the first ones: for semantic textual similarity tasks, it is the average of representations from 1, 2, and 12 layers; and for clustering, 0, 1, 2, and 12 layers. Even individual SICK-E, SICK-R, and STS-B classification tasks, which originate from semantic textual similarity ones, have strong first layers (see Figure  8  in Appendix  A.2 ), although on average the best layer for classification tasks is the 10th. This shows that a lot of performance for tasks that work on similarities between texts (STS and clustering) depends on the first layers, which according to  [ 234 ]  have more generalized token representations. However, the last layer, which gets recreated token identities  [ 234 ]  is also important, as the best combinations (such as 1+2+12) also involve it. This first+last cooperation of layers can be distinctly observed as a U shape for clustering performance in layers (see Figure  5  and also Figure  4 ).",
            "For most classification tasks (excluding those originating from STS), the U shape is inversed. This is very clearly seen for binary classification tasks (see Figure  8  in Appendix  A.2 ), where the highest classification accuracy is concentrated between the 9th and the 11th layers. This aligns with the explanations of the previous work  [ 100 ]  which argues that the last layer is over-specialized for the training objective.",
            "We present a strong and very simple baseline model of Random Embeddings (RE), where every token is assigned a random vector as its embedding. Combined with token aggregation and post-processing techniques, it almost matches the average STS performance of the BERT model, also with the techniques applied, with 66.4% versus 69.8% average Spearman correlation. It also shows very high performance for some tasks like stackoverflow classification (see Section  4.1.2  for more details). We encourage future work to use RE as a baseline, due to its mid-level performance, simple implementation, and ability to separate the contribution to the performance of the learned contexts from the aggregation and post-processing techniques.",
            "We question the use of prompts (adding a sentence into a certain text template) for retrieving the representation of the sentence from only the  [MASK]  token. Our experiments show that averaging all the templated text tokens, with idf weighting and post-processing for the unsupervised tasks, is better. Meanwhile, the average increase in performance due to the added template is only obvious for the STS tasks, gives no improvement in clustering, and is very slight in classification (see Table  4 , Section  4.2 )."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Dataset statistics for STS and classification tasks. N is the number of text samples, C is the number of clusters, L/S is the imbalance number defined as the size of the largest class divided by that of the smallest class (for STS tasks the two classes are binned to 1 point label value length ones from the highest and lowest sides),   V  norm V \\|V\\|  italic_V   is vocabulary size, Len is the average number of tokens in each text sample, Alpha is the percent of tokens that are alphabetic. Statistics with plain word tokenization are marked with W and with  bert-base-uncased  model tokenizer as B. For MRPC, SICK-R/E, STS-B, and STS tasks, statistics are calculated for tokenized and then concatenated sentences in each pair.",
        "table": "S3.T3.3",
        "footnotes": [],
        "references": [
            "The rest of this paper is organized as follows. We provide a review of related work in the literature on composing word vectors and representation reshaping to get sentence- or text-level embeddings in Section  2 . In Section  3 , we outline the experimental setting and give a detailed background on our chosen approach, models, and datasets. In Section  4 , we present the results. Finally, we summarize the findings of this work in Section  5 .",
            "Thousands of works are being done on word-level vectors. Now, you can easily download popular publicly accessible word2vec  [ 16 ]  and GloVe  [ 17 ]  embeddings. They are lightweight and usually fit perfectly various word-level tasks. On the other hand, modeling sequences of words is considered much more challenging. Count-based approaches lose information on word order and are sparse. Learning higher-order n-gram vector space models, not just words but phrases or sentences, leads to sparsity, as frequencies vanish for target n-grams and contexts. The latter, in particular, is the main driving force for distributed representations. We will cover special methods dedicated to sentence level in Subsection  2.3 . Nevertheless, the easiest solution is to reuse individual word vectors of the sequence.",
            "Some methods are proposed to reduce anisotropy during the training process. In particular, the authors of  [ 120 ]  added the specific regularization. According to them, the aperture of the narrow cone of representations can be improved by minimizing the cosine similarities between  any two word embeddings. Such regularization encourages the vectors to be more evenly spread and expressive. This method is highly related to the now-popular contrastive learning approach (see Section  2.3.1 ). Other authors of  [ 151 ]  improve the isotropy of the output embedding matrix using the spectrum control method. They guide the singular value distribution of the embedding matrix throughout the training process and control the decay rate of these singular values.",
            "STS assesses the degree to which two sentences are semantically equivalent to each other. A single sample consists of two sentences and a score ranging from 0 for no meaning overlap to 5 for meaning equivalence. The semantic textual similarity shared task has been held annually since 2012 up to 2017  [ 218 ,  219 ,  220 ,  221 ,  222 ,  223 ]  and formed STS12-STS17 datasets. Carefully selected 8 628 samples from these contests formed the STS benchmark  [ 223 ] . Table  3  shows details of the STS datasets, including SICK-Relatedness  [ 224 ]  and STS-B test sets, which we also use. Similarly to the work of  [ 145 ] , we also add (STR)  [ 225 ] , a recent semantic relatedness dataset created by comparative annotations.",
            "Differently from STS, these tasks are evaluated in a supervised way. Following the SentEval  [ 194 ]  benchmark suite, the commonly used evaluation protocol is to train a logistic regression or an MLP classifier with a cross-validation setup on top of the frozen representations, and the testing accuracy is used as a measure of the representation quality. We went after the logistic regression classifier and the 10-fold cross-validation scheme, the setting which is the most commonly reported in the literature. We evaluated various binary, ternary, and fine-grained classification as well as regression tasks. A more detailed description of each is presented below, while statistics are presented in Table  3 .",
            "Here we present the results of our various experiments outlined in Section  3 .",
            "An interesting result, as seen in Table  4 , is the comparison of simply averaged BERT tokens from many contexts (the Avg. model) and word vectors from a specially-trained word2vec-style model BERT2Static (the B2S model) using the same BERT contexts, as described in Section  3.2.4 . If no token aggregation and post-processing techniques are used, B2S is of similar performance at STS tasks (with average Spearman correlation of 56.7 versus 56.4), worse at clustering (with average clustering accuracy of 53.8 versus 55.2) and comparable at classification tasks (with average B2S accuracy of 76.9 versus 77.3) to the Avg. model. As we can see, the task performance differences are very small.",
            "The success of such a simple Avg. model inspired us to seek further improvement by combining it with the parent BERT model. For the combined BERT+Avg. model results, please see Section  4.3 .",
            "We observed that token pooling and post-processing techniques do not improve the alignment and uniformity properties of the representations. Let us remind the reader that alignment is calculated with only those STS-B pairs with a similarity score of 5, while uniformity with all pairs, as defined in Section  3.5.5 , and smaller values are better.",
            "We can observe in Figure  3  that the zscore post-processing always makes alignment worse, while normalization of embeddings almost always does the same for the uniformity of the representations. Excluding the random embeddings model, the best alignment and uniformity properties are almost always with plain averaging and no post-processing applied.",
            "We found that the aggregation and post-processing techniques tried typically increase the isotropy of the representations, and the isotropy for most models is positively correlated (up to 69% Pearson correlation) with the Spearman correlation of STS tasks (Section  4.1.3 ). The highest isotropy improvement is observed for our Random Embeddings model since its token representations have the maximal isotropy to start with. We did not find the token aggregation and post-processing techniques to improve the alignment and uniformity properties of representations.",
            "We presented a static vectors model Avg., which simply contains BERTs tokens averaged over multiple different contexts. Our experiments show that it outperforms a more complex BERT2Static  [ 49 ] , also a static word-level model yet specially trained on BERT contexts. With the best post-processing and token aggregation techniques, the advantage for unsupervised STS tasks is 69.5 versus 66.4, and for clustering it is 62.2 versus 57.2., with a negligible difference for classification tasks. Moreover, we show that combining Avg. with the parent BERT model can bring even further improvements. In particular, BERT+Avg. reached the highest average clustering accuracy of 64.8 out of all our considered models, as well as the classification accuracy of 80.5 (Section  4.3 ). We encourage future work to also use Avg. as a baseline, both due to its upper-level performance and simple implementation."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Effect of different token aggregation and post-processing methods on multiple text embedding models. Both correlation and accuracy scores range form 0 to 100 (from the worst to the best).",
        "table": "S4.T4.7",
        "footnotes": [],
        "references": [
            "The rest of this paper is organized as follows. We provide a review of related work in the literature on composing word vectors and representation reshaping to get sentence- or text-level embeddings in Section  2 . In Section  3 , we outline the experimental setting and give a detailed background on our chosen approach, models, and datasets. In Section  4 , we present the results. Finally, we summarize the findings of this work in Section  5 .",
            "The results of best-performing token aggregation and embedding post-processing techniques are presented in Table  4 . The results here are averaged over all the different tasks in the class. See Appendix  A  for the individual results.",
            "An interesting result, as seen in Table  4 , is the comparison of simply averaged BERT tokens from many contexts (the Avg. model) and word vectors from a specially-trained word2vec-style model BERT2Static (the B2S model) using the same BERT contexts, as described in Section  3.2.4 . If no token aggregation and post-processing techniques are used, B2S is of similar performance at STS tasks (with average Spearman correlation of 56.7 versus 56.4), worse at clustering (with average clustering accuracy of 53.8 versus 55.2) and comparable at classification tasks (with average B2S accuracy of 76.9 versus 77.3) to the Avg. model. As we can see, the task performance differences are very small.",
            "Despite the additional option for B2S to remove the 100 most frequent tokens, using additional token aggregation and post-processing techniques allows our simple Avg. model to surpass both B2S and B2S-100 (see Table  4 ). Note that the removal of the most frequent words for B2S works in a similar way to weighted token aggregations or post-processing, the gains are not additive. Now the best average STS Spearman correlation score becomes 69.5 for Avg. versus 66.4 of B2S and the best average clustering accuracy of 62.2 for Avg. versus 57.2 of B2S-100. Additional techniques here helped a much simpler Avg. model outperform a much more complex learned B2S.",
            "The success of such a simple Avg. model inspired us to seek further improvement by combining it with the parent BERT model. For the combined BERT+Avg. model results, please see Section  4.3 .",
            "First, extracting only the vector of  [MASK]  is not necessarily the best. We found that a simple average of all tokens, including the ones from the prompt template, and also the  [MASK]  token, is still a valid approach. Even more, it outperforms only the  [MASK]  token approach for clustering and classification tasks, with corresponding 55.0 and 79.9 average accuracies versus 54.2 and 76.8 for using only the  [MASK]  token (see T4 model results in Table  4 ).",
            "The use of the additional text template around the text enriches the target text representation. Let us compare the performance of 12th layer averaged token representations with and without a template (see Figure  4 , T0 avg. and T4 avg. with a template versus BERT avg. without). For all 3 groups of tasks  semantic textual similarity, clustering, and classification  the T0 template achieves 61.3 average Spearman correlation, 53.5 clustering accuracy, and 80.5 classification accuracy; the T4 template reaches 62.3, 55.0, and 79.9, while a regular, non-templated text gets just 53.2, 50.6, and 79.0 correspondingly. This shows that the use of the templates allows the model to enrich target text representation. However, this effect peaks at the last 12th layer and the achieved performance is similar to the first+last layer combination of the regular non-templated vectors.",
            "Performance of using only the  [MASK]  token also peaks at the 12th layer, so we investigated what influence it has in the enrichment of templated target text representation. Could it be that this token is the main culprit for better performance of averaged templated text representations? To answer this question we tried to omit the  [MASK]  token from averaging (see Figure  4  T0 avg. no  [MASK]  and T4 avg. no  [MASK] ). For the T4 template and 12th layer representations, averaging all tokens yields 62.3 average Spearman correlation, 55.0 clustering accuracy, and 79.9 classification accuracy; dropping the  [MASK]  token from averaging: 58.7, 53.3, and 79.8; while non-templated performance is 53.2, 50.6, and 79.0 correspondingly. We see that omitting the  [MASK]  token in averaging indeed hurts the performance. On the other hand, the results show that it is responsible only for approximately half of the improvements, with the other half coming from the other tokens in the templated text.",
            "One good reason to use averaging instead of only the  [MASK]  token is that then token weighting can be also applied. As we already showed in Table  4 ), the T4 template together with  i  d  f t T i d superscript subscript f t T idf_{t}^{T} italic_i italic_d italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  token weighting and uniform quantile post-processing allowed to reach the average Spearman correlation of 71.6, which was the best among the tried methods. Also, we observed that post-processing on text representations from  [MASK]  token was not as effective as from averaged ones. This also suggests that all tokens in templated texts have richer representations.",
            "One can see that for clustering and classification tasks, the combination of both models in equal portions of  w = 0.5 w 0.5 w=0.5 italic_w = 0.5  is better than these models alone ( w = 0.0 w 0.0 w=0.0 italic_w = 0.0  corresponds to BERT,  w = 1.0 w 1.0 w=1.0 italic_w = 1.0  is a single Avg. model). If we look at the best scores with the two techniques applied in Table  4 , for the average clustering accuracy this translates to an improvement of 1.4% from the 63.4% of BERT to 64.8% of BERT+Avg.",
            "Figure  5  also depicts the task performance versus BERT layers. Our first observation is that for STS and clustering tasks, there is a strong preference for the first layers. As a result, the best combination of layers also involves the first ones: for semantic textual similarity tasks, it is the average of representations from 1, 2, and 12 layers; and for clustering, 0, 1, 2, and 12 layers. Even individual SICK-E, SICK-R, and STS-B classification tasks, which originate from semantic textual similarity ones, have strong first layers (see Figure  8  in Appendix  A.2 ), although on average the best layer for classification tasks is the 10th. This shows that a lot of performance for tasks that work on similarities between texts (STS and clustering) depends on the first layers, which according to  [ 234 ]  have more generalized token representations. However, the last layer, which gets recreated token identities  [ 234 ]  is also important, as the best combinations (such as 1+2+12) also involve it. This first+last cooperation of layers can be distinctly observed as a U shape for clustering performance in layers (see Figure  5  and also Figure  4 ).",
            "We found the techniques to benefit all models studied for the unsupervised STS (the best model average Spearman correlation increased from 62.3% to 71.6%) and clustering (the best model average clustering accuracy increased from 59.2% to 64.8%), while it had no positive effect on the supervised classification tasks (see Table  4 ).",
            "We present a strong and very simple baseline model of Random Embeddings (RE), where every token is assigned a random vector as its embedding. Combined with token aggregation and post-processing techniques, it almost matches the average STS performance of the BERT model, also with the techniques applied, with 66.4% versus 69.8% average Spearman correlation. It also shows very high performance for some tasks like stackoverflow classification (see Section  4.1.2  for more details). We encourage future work to use RE as a baseline, due to its mid-level performance, simple implementation, and ability to separate the contribution to the performance of the learned contexts from the aggregation and post-processing techniques.",
            "We found that the aggregation and post-processing techniques tried typically increase the isotropy of the representations, and the isotropy for most models is positively correlated (up to 69% Pearson correlation) with the Spearman correlation of STS tasks (Section  4.1.3 ). The highest isotropy improvement is observed for our Random Embeddings model since its token representations have the maximal isotropy to start with. We did not find the token aggregation and post-processing techniques to improve the alignment and uniformity properties of representations.",
            "We question the use of prompts (adding a sentence into a certain text template) for retrieving the representation of the sentence from only the  [MASK]  token. Our experiments show that averaging all the templated text tokens, with idf weighting and post-processing for the unsupervised tasks, is better. Meanwhile, the average increase in performance due to the added template is only obvious for the STS tasks, gives no improvement in clustering, and is very slight in classification (see Table  4 , Section  4.2 ).",
            "We presented a static vectors model Avg., which simply contains BERTs tokens averaged over multiple different contexts. Our experiments show that it outperforms a more complex BERT2Static  [ 49 ] , also a static word-level model yet specially trained on BERT contexts. With the best post-processing and token aggregation techniques, the advantage for unsupervised STS tasks is 69.5 versus 66.4, and for clustering it is 62.2 versus 57.2., with a negligible difference for classification tasks. Moreover, we show that combining Avg. with the parent BERT model can bring even further improvements. In particular, BERT+Avg. reached the highest average clustering accuracy of 64.8 out of all our considered models, as well as the classification accuracy of 80.5 (Section  4.3 ). We encourage future work to also use Avg. as a baseline, both due to its upper-level performance and simple implementation."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Performance of post-processing with plain token averaging.",
        "table": "S4.T5.4",
        "footnotes": [],
        "references": [
            "The rest of this paper is organized as follows. We provide a review of related work in the literature on composing word vectors and representation reshaping to get sentence- or text-level embeddings in Section  2 . In Section  3 , we outline the experimental setting and give a detailed background on our chosen approach, models, and datasets. In Section  4 , we present the results. Finally, we summarize the findings of this work in Section  5 .",
            "For maximum performance, usually, both aggregation and post-processing techniques must be used. As an example, sentences placed within the T4 template with no techniques applied have a 62.3 average Spearman correlation. With a better aggregation method of  i  d  f t T i d superscript subscript f t T idf_{t}^{T} italic_i italic_d italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , it is pushed up to 69.2, while only using quantile-u post-processing gives 67.3 (see Table  5  with only post-processing techniques applied). However, a combined effort of both  i  d  f t T i d superscript subscript f t T idf_{t}^{T} italic_i italic_d italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  and quantile-u gives the average Spearman correlation score of 71.6.",
            "We considered many post-processing techniques for sentence vectors and show the performance of the most popular and best-performing ones in Table  5  separately. Indeed, almost every method somehow improves the average Spearman correlation for STS tasks. For all-but-the-top (abtt), we varied the number of components to remove up to a hundred but settled on removing only the first two, as it was slightly better than the rest. Although highly credited in the literature, abtt-2 still got smaller scores than the others. Plainly averaging the token representations, the highest scores for both STS and clustering tasks were achieved using quantile-uniform normalization. We would also like to mention the simple normalization of vectors to the unit length, which was often beneficial for clustering tasks; and whitening normalization, which was learned on Wikitext-2 dataset, and was favorable with the random embeddings model. We trained other post-processing methods on Wikitext-2 too, yet they resulted in similar or slightly smaller scores.",
            "We observed that token pooling and post-processing techniques do not improve the alignment and uniformity properties of the representations. Let us remind the reader that alignment is calculated with only those STS-B pairs with a similarity score of 5, while uniformity with all pairs, as defined in Section  3.5.5 , and smaller values are better.",
            "The impact of the  w w w italic_w  parameter and the choice of the layer to source the representations (same layer for both BERT and Avg.) is presented in Figure  5 .",
            "Meanwhile, as we see in the same table, the average accuracy of classification tasks without additional techniques applied to the combination of the first and last layers is similar between only BERT (79.9%) and BERT+Avg. (79.7%). However, as we see in Figure  5 , differently from the first+last optimal layer setting for BERT, BERT+Avg. has a sweet spot in the 10th layer with an average classification accuracy of 80.5%, surpassing the one of BERT by 0.6%.",
            "Figure  5  also depicts the task performance versus BERT layers. Our first observation is that for STS and clustering tasks, there is a strong preference for the first layers. As a result, the best combination of layers also involves the first ones: for semantic textual similarity tasks, it is the average of representations from 1, 2, and 12 layers; and for clustering, 0, 1, 2, and 12 layers. Even individual SICK-E, SICK-R, and STS-B classification tasks, which originate from semantic textual similarity ones, have strong first layers (see Figure  8  in Appendix  A.2 ), although on average the best layer for classification tasks is the 10th. This shows that a lot of performance for tasks that work on similarities between texts (STS and clustering) depends on the first layers, which according to  [ 234 ]  have more generalized token representations. However, the last layer, which gets recreated token identities  [ 234 ]  is also important, as the best combinations (such as 1+2+12) also involve it. This first+last cooperation of layers can be distinctly observed as a U shape for clustering performance in layers (see Figure  5  and also Figure  4 ).",
            "In our work, we also analyzed prompt and BERT+Avg. models layer-wise. We found that for the STS tasks taking the representations from the first layers is performing better, and for the clustering task, the performance profile forms a U shape with tops at the first and last layers. Therefore, for these two task groups we mostly use the average of first+last layers, harnessing both of the tops for the best performance. On the other hand, classification tasks have an inverted U shape with the top in the 10th layer (Figure  5 ). We did not find token aggregation and post-processing techniques to change such profile curvature."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Spearman correlation dependence on token aggregation and post-processing techniques for semantic textual similarity STS12 task.",
        "table": "A1.T6.12",
        "footnotes": [],
        "references": [
            "If we were to look at the detailed performance on individual tasks in Appendix  A.1 , we would find several where random embeddings with the help of the two shaping techniques score higher than also-shaped BERT representations. For STS14 (Table  8 ) it is 68.8 versus 68.3 Spearman correlation, for googleTS (Table  16 ), stackoverflow (Table  18 ), and tweet (Table  19 ) datasets the clustering accuracy is correspondingly 69.5 vs 68.5, 70.6 vs 59.9, and 58.5 vs 55.1. This may indicate a smaller complexity of these particular datasets, where the task can largely be solved based on a small set of keywords. Yet it is important to note that it is achieved only with the help of aggregation and post-processing methods on top of the random embeddings.",
            "As we already mentioned, STS tasks preferred the regular BERT ( w = 0 w 0 w=0 italic_w = 0 ) to the BERT+Avg. on average. However, for several individual semantic textual similarity datasets, the best weights turned out to be even negative. This is the case for STS15 ( w =  0.25 w 0.25 w=-0.25 italic_w = - 0.25 ), and STS16 ( w =  0.75 w 0.75 w=-0.75 italic_w = - 0.75 ). The same negative weight of  w =  0.5 w 0.5 w=-0.5 italic_w = - 0.5  preference was also observed for the searchsnippets clustering dataset. Although the Spearman correlation for three STS tasks was only up to several percent higher, for searchsnippets the clustering accuracy increased to a staggering 80.2% from 72.2% of  w = 0.0 w 0.0 w=0.0 italic_w = 0.0 . This shows that the determined values of  w = 0.0 w 0.0 w=0.0 italic_w = 0.0  for STS and  w = 0.5 w 0.5 w=0.5 italic_w = 0.5  for clustering and classification are not universal and for a small percentage of tasks can differ. For more BERT+Avg. task-wise details, see Appendix  A.2  figures for STS (Figure  6 ), clustering (Figure  7 ), and classification (Figure  8 ) performance."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Spearman correlation dependence on token aggregation and post-processing techniques for semantic textual similarity STS13 task.",
        "table": "A1.T7.12",
        "footnotes": [],
        "references": [
            "As we already mentioned, STS tasks preferred the regular BERT ( w = 0 w 0 w=0 italic_w = 0 ) to the BERT+Avg. on average. However, for several individual semantic textual similarity datasets, the best weights turned out to be even negative. This is the case for STS15 ( w =  0.25 w 0.25 w=-0.25 italic_w = - 0.25 ), and STS16 ( w =  0.75 w 0.75 w=-0.75 italic_w = - 0.75 ). The same negative weight of  w =  0.5 w 0.5 w=-0.5 italic_w = - 0.5  preference was also observed for the searchsnippets clustering dataset. Although the Spearman correlation for three STS tasks was only up to several percent higher, for searchsnippets the clustering accuracy increased to a staggering 80.2% from 72.2% of  w = 0.0 w 0.0 w=0.0 italic_w = 0.0 . This shows that the determined values of  w = 0.0 w 0.0 w=0.0 italic_w = 0.0  for STS and  w = 0.5 w 0.5 w=0.5 italic_w = 0.5  for clustering and classification are not universal and for a small percentage of tasks can differ. For more BERT+Avg. task-wise details, see Appendix  A.2  figures for STS (Figure  6 ), clustering (Figure  7 ), and classification (Figure  8 ) performance."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Spearman correlation dependence on token aggregation and post-processing techniques for semantic textual similarity STS14 task.",
        "table": "A1.T8.12",
        "footnotes": [],
        "references": [
            "If we were to look at the detailed performance on individual tasks in Appendix  A.1 , we would find several where random embeddings with the help of the two shaping techniques score higher than also-shaped BERT representations. For STS14 (Table  8 ) it is 68.8 versus 68.3 Spearman correlation, for googleTS (Table  16 ), stackoverflow (Table  18 ), and tweet (Table  19 ) datasets the clustering accuracy is correspondingly 69.5 vs 68.5, 70.6 vs 59.9, and 58.5 vs 55.1. This may indicate a smaller complexity of these particular datasets, where the task can largely be solved based on a small set of keywords. Yet it is important to note that it is achieved only with the help of aggregation and post-processing methods on top of the random embeddings.",
            "As we already mentioned, STS tasks preferred the regular BERT ( w = 0 w 0 w=0 italic_w = 0 ) to the BERT+Avg. on average. However, for several individual semantic textual similarity datasets, the best weights turned out to be even negative. This is the case for STS15 ( w =  0.25 w 0.25 w=-0.25 italic_w = - 0.25 ), and STS16 ( w =  0.75 w 0.75 w=-0.75 italic_w = - 0.75 ). The same negative weight of  w =  0.5 w 0.5 w=-0.5 italic_w = - 0.5  preference was also observed for the searchsnippets clustering dataset. Although the Spearman correlation for three STS tasks was only up to several percent higher, for searchsnippets the clustering accuracy increased to a staggering 80.2% from 72.2% of  w = 0.0 w 0.0 w=0.0 italic_w = 0.0 . This shows that the determined values of  w = 0.0 w 0.0 w=0.0 italic_w = 0.0  for STS and  w = 0.5 w 0.5 w=0.5 italic_w = 0.5  for clustering and classification are not universal and for a small percentage of tasks can differ. For more BERT+Avg. task-wise details, see Appendix  A.2  figures for STS (Figure  6 ), clustering (Figure  7 ), and classification (Figure  8 ) performance.",
            "Figure  5  also depicts the task performance versus BERT layers. Our first observation is that for STS and clustering tasks, there is a strong preference for the first layers. As a result, the best combination of layers also involves the first ones: for semantic textual similarity tasks, it is the average of representations from 1, 2, and 12 layers; and for clustering, 0, 1, 2, and 12 layers. Even individual SICK-E, SICK-R, and STS-B classification tasks, which originate from semantic textual similarity ones, have strong first layers (see Figure  8  in Appendix  A.2 ), although on average the best layer for classification tasks is the 10th. This shows that a lot of performance for tasks that work on similarities between texts (STS and clustering) depends on the first layers, which according to  [ 234 ]  have more generalized token representations. However, the last layer, which gets recreated token identities  [ 234 ]  is also important, as the best combinations (such as 1+2+12) also involve it. This first+last cooperation of layers can be distinctly observed as a U shape for clustering performance in layers (see Figure  5  and also Figure  4 ).",
            "For most classification tasks (excluding those originating from STS), the U shape is inversed. This is very clearly seen for binary classification tasks (see Figure  8  in Appendix  A.2 ), where the highest classification accuracy is concentrated between the 9th and the 11th layers. This aligns with the explanations of the previous work  [ 100 ]  which argues that the last layer is over-specialized for the training objective."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Spearman correlation dependence on token aggregation and post-processing techniques for semantic textual similarity STS15 task.",
        "table": "A1.T9.12",
        "footnotes": [],
        "references": [
            "If we were to look at the detailed performance on individual tasks in Appendix  A.1 , we would find several where random embeddings with the help of the two shaping techniques score higher than also-shaped BERT representations. For STS14 (Table  8 ) it is 68.8 versus 68.3 Spearman correlation, for googleTS (Table  16 ), stackoverflow (Table  18 ), and tweet (Table  19 ) datasets the clustering accuracy is correspondingly 69.5 vs 68.5, 70.6 vs 59.9, and 58.5 vs 55.1. This may indicate a smaller complexity of these particular datasets, where the task can largely be solved based on a small set of keywords. Yet it is important to note that it is achieved only with the help of aggregation and post-processing methods on top of the random embeddings."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Spearman correlation dependence on token aggregation and post-processing techniques for semantic textual similarity STS16 task.",
        "table": "A1.T10.12",
        "footnotes": [],
        "references": [
            "We have varied the  w w w italic_w  parameter to also the negative values in ( 10 ) to see if subtracting (instead of adding) the context-average representations from the context-aware ones helps."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Spearman correlation dependence on token aggregation and post-processing techniques for semantic textual similarity STS-B task.",
        "table": "A1.T11.12",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "Table 12:  Spearman correlation dependence on token aggregation and post-processing techniques for semantic textual similarity SICK-R task.",
        "table": "A1.T12.12",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "Table 13:  Spearman correlation dependence on token aggregation and post-processing techniques for semantic textual similarity STR task.",
        "table": "A1.T13.12",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "Table 14:  Clustering accuracy dependence on token aggregation and post-processing techniques for the agnews dataset.",
        "table": "A1.T14.12",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "Table 15:  Clustering accuracy dependence on token aggregation and post-processing techniques for the biomedical dataset.",
        "table": "A1.T15.12",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "Table 16:  Clustering accuracy dependence on token aggregation and post-processing techniques for the googleTS dataset.",
        "table": "A1.T16.12",
        "footnotes": [],
        "references": [
            "If we were to look at the detailed performance on individual tasks in Appendix  A.1 , we would find several where random embeddings with the help of the two shaping techniques score higher than also-shaped BERT representations. For STS14 (Table  8 ) it is 68.8 versus 68.3 Spearman correlation, for googleTS (Table  16 ), stackoverflow (Table  18 ), and tweet (Table  19 ) datasets the clustering accuracy is correspondingly 69.5 vs 68.5, 70.6 vs 59.9, and 58.5 vs 55.1. This may indicate a smaller complexity of these particular datasets, where the task can largely be solved based on a small set of keywords. Yet it is important to note that it is achieved only with the help of aggregation and post-processing methods on top of the random embeddings."
        ]
    },
    "id_table_17": {
        "caption": "Table 17:  Clustering accuracy dependence on token aggregation and post-processing techniques for the searchsnippets dataset.",
        "table": "A1.T17.12",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "Table 18:  Clustering accuracy dependence on token aggregation and post-processing techniques for the stackoverflow dataset.",
        "table": "A1.T18.12",
        "footnotes": [],
        "references": [
            "If we were to look at the detailed performance on individual tasks in Appendix  A.1 , we would find several where random embeddings with the help of the two shaping techniques score higher than also-shaped BERT representations. For STS14 (Table  8 ) it is 68.8 versus 68.3 Spearman correlation, for googleTS (Table  16 ), stackoverflow (Table  18 ), and tweet (Table  19 ) datasets the clustering accuracy is correspondingly 69.5 vs 68.5, 70.6 vs 59.9, and 58.5 vs 55.1. This may indicate a smaller complexity of these particular datasets, where the task can largely be solved based on a small set of keywords. Yet it is important to note that it is achieved only with the help of aggregation and post-processing methods on top of the random embeddings."
        ]
    },
    "id_table_19": {
        "caption": "Table 19:  Clustering accuracy dependence on token aggregation and post-processing techniques for the tweet dataset.",
        "table": "A1.T19.12",
        "footnotes": [],
        "references": [
            "If we were to look at the detailed performance on individual tasks in Appendix  A.1 , we would find several where random embeddings with the help of the two shaping techniques score higher than also-shaped BERT representations. For STS14 (Table  8 ) it is 68.8 versus 68.3 Spearman correlation, for googleTS (Table  16 ), stackoverflow (Table  18 ), and tweet (Table  19 ) datasets the clustering accuracy is correspondingly 69.5 vs 68.5, 70.6 vs 59.9, and 58.5 vs 55.1. This may indicate a smaller complexity of these particular datasets, where the task can largely be solved based on a small set of keywords. Yet it is important to note that it is achieved only with the help of aggregation and post-processing methods on top of the random embeddings."
        ]
    },
    "global_footnotes": [
        "Downloaded from",
        "provided in",
        ".",
        "We used the codebase from",
        "and downloaded clustering datasets from"
    ]
}