{
    "id_table_1": {
        "caption": "Table 1:  Experiments on ASQA dataset. Baselines are either fully-supervised or 5-shot prompted. The metric Time indicates inference time (sec) per query.  indicates numbers that are not reported by the original papers or are not applicable.  We emphasize our results in bold, for easy comparisons.",
        "table": "S5.T1.7.7",
        "footnotes": [],
        "references": [
            "To address this issue, the iterative RAG approach, ToC  Kim et al. ( 2023 ) , has been introduced (c.f. Fig  2 (b)) to further explore other interpretations that can not be covered by the single retrieval process. Specifically,  to further explore missing interpretations, the interpretations extracted in the previous iteration are utilized as queries to retrieve new passages, additional interpretations are then extracted. This exploration process is repeated in multiple times, leading to encompassing more diverse interpretations and corresponding answers. However, we argue that this effectiveness comes with a significant increase in computational overheads due to the iterative passage retrieval and LLM reasoning. In our experiments, this method requires an average of 5.5 exploration steps per query. As shown in Figure  1 , Iterative RAG (i.e., ToC) significantly outperforms the vanilla RAG approach in terms of factual accuracy but at the cost of greatly reduced efficiency, with notable increases in both inference time and API call costs.",
            "Experimental Details.    We utilize the most recent ambiguous QA dataset, ASQA  Stelmakh et al. ( 2022 ) . We classify the quality of retrieved passages into three labels:  1)  Fully Cover,  2)  Partially Cover, and  3)  Not Cover. Fully Cover indicates that the retrieved passages encompass all plausible interpretations, Not Cover does that the retrieved passages do not contain any of them, and otherwise Partially Cover. We obtain these labels for each question by computing a string exact match between a set of retrieved passages and all plausible answers provided in ASQA as ground-truth answers. For implementation details of retrieving passages, see Appendix  A.4.1 .",
            "Based on the findings, we propose an efficient and robust RAG framework for ambiguous QA,  di versify- v erify- a dapt ( Diva ).  This framework comprises two key components: Retrieval Diversification  (Sec  3.2 )  and Adaptive Generation  (Sec  3.3 ) . The retrieval diversification method aims to efficiently  diversify  the retrieved passages to encompass diverse interpretations. Subsequently, the adaptive generation method aims to  verify  the quality of the passages and  adapt  the most suitable approach tailored to their quality. Fig  2 (c) and Algorithm  1  show the overview and inference algorithm of  Diva , respectively.",
            "Adaptive Generation   Once we get the verification results from Eq  7 , if the  P i subscript P i \\mathcal{P}_{i} caligraphic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is classified to  Useful  or  PartialUseful , we decide to utilize the retrieved passages  P i subscript P i \\mathcal{P}_{i} caligraphic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  to generate a response by Eq  1  and  2 . If  P i subscript P i \\mathcal{P}_{i} caligraphic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is classified to  Useless , we decide to only utilize the LLMs internal knowledge to generate a response:  LLM  ( q i , I l ) LLM subscript q i subscript I l \\texttt{LLM}(q_{i},I_{\\text{l}}) LLM ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT l end_POSTSUBSCRIPT ) . The full prompt of  I l subscript I l I_{\\text{l}} italic_I start_POSTSUBSCRIPT l end_POSTSUBSCRIPT  is presented in Table  C  in Appendix  C . This process enables the utilization of the most suitable approach tailored to each retrieval quality, which is beneficial to both accuracy and efficiency.",
            "Our proposed method and all baseline models are assessed using the ASQA  Stelmakh et al. ( 2022 )  and SituatedQA  Zhang and Choi ( 2021 )  datasets. ASQA is a long-form QA dataset featuring ambiguous questions. SituatedQA is a short-form QA dataset featuring questions that specifically highlight ambiguities related to temporal and geographical contexts. We give these questions to the QA systems and assess how comprehensively the responses cover the provided possible interpretations of questions. Further details about the datasets are provided in the Appendix  A.1 .",
            "In  Diva , the LLM is employed across three modules: retrieval diversification (Eqn  3 ), retrieval verification (Eqn  7 ), and adaptive response generation (Eqn  1 ,  2 , and closed-book LLM). For adaptive response generation, we use the same LLM backbones as the other baselines. For the retrieval diversification and verification modules, we assess the performance of GPT-3.5 (gpt-3.5-turbo) and GPT-4 (gpt-4) across them, ultimately opting to use GPT-4 for both modules in the ASQA dataset and GPT-3.5 for both modules in the SituatedQA dataset in all experiments. However, as demonstrated in Section  5.4 , other LLMs also perform effectively in these modules. For other implementation details, please refer to Appendix  A.4 .",
            "Table  1  presents the long-form ambiguous QA performance of baselines and  Diva  on the development set of ASQA.",
            "Additionally, Fig  6  shows the performance and efficiency of baselines and  Diva  on the SituatedQA test set for short-form ambiguous QA tasks. All experimental results align with those seen in Table  1 , demonstrating strong generalizability of  Diva  across different types of ambiguous questions.",
            "We conduct a case study to qualitatively compare the reasoning chains of Iterative RAG, ToC  Kim et al. ( 2023 ) , and  Diva . Due to space limits, please refer to Appendix  B.1  for detailed case studies.",
            "Table  C  and Table  C  show an example of text prompt for inferring  pseudo-interpretations  (i.e.,  I a subscript I a I_{\\text{a}} italic_I start_POSTSUBSCRIPT a end_POSTSUBSCRIPT  and  I p subscript I p I_{\\text{p}} italic_I start_POSTSUBSCRIPT p end_POSTSUBSCRIPT  in Eqn  3 ). Table  C  shows an example of text prompt for verifying the retrieved passages (i.e.,  I v subscript I v I_{\\text{v}} italic_I start_POSTSUBSCRIPT v end_POSTSUBSCRIPT  in Eqn  7 ). Table  C  and  C  show an exmple of text prompt for response generation in vanilla RAG framework (i.e.,  I e subscript I e I_{\\text{e}} italic_I start_POSTSUBSCRIPT e end_POSTSUBSCRIPT  in Eqn  1  and  I g subscript I g I_{\\text{g}} italic_I start_POSTSUBSCRIPT g end_POSTSUBSCRIPT  in Eqn  2 )"
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Ablation studies on ASQA dataset.",
        "table": "S5.T2.1.1",
        "footnotes": [],
        "references": [
            "Retrieval-augmented generation (RAG) framework has made significant progress in open-domain QA tasks  Izacard and Grave ( 2021 ); Lazaridou et al. ( 2022 ); Shi et al. ( 2023 ); Ram et al. ( 2023 )  and also proven to be an effective solution for addressing ambiguous questions  Min et al. ( 2020 ,  2021 ); Kim et al. ( 2023 ); Sun et al. ( 2023 ) . Specifically, these approaches first retrieve passages on the given question and prompt the LLM to extract plausible interpretations and answers relying on the passages (c.f. Fig  2 (a)).",
            "Despite the success of the RAG framework on the ambiguous QA task, we should rethink:  Is a single retrieval process sufficient to retrieve passages encompassing all plausible interpretations? To answer this question, we conduct preliminary experiments (c.f. Sec  2 ) about the quality of the retrieved passages used in the RAG framework. We observe that the passages obtained from the single retrieval process often pose a low quality issue with respect to addressing ambiguous questions. In other words, the retrieved passages often partially or completely failed to cover all plausible interpretations, leading to significant performance degradation in terms of factual accuracy.",
            "To address this issue, the iterative RAG approach, ToC  Kim et al. ( 2023 ) , has been introduced (c.f. Fig  2 (b)) to further explore other interpretations that can not be covered by the single retrieval process. Specifically,  to further explore missing interpretations, the interpretations extracted in the previous iteration are utilized as queries to retrieve new passages, additional interpretations are then extracted. This exploration process is repeated in multiple times, leading to encompassing more diverse interpretations and corresponding answers. However, we argue that this effectiveness comes with a significant increase in computational overheads due to the iterative passage retrieval and LLM reasoning. In our experiments, this method requires an average of 5.5 exploration steps per query. As shown in Figure  1 , Iterative RAG (i.e., ToC) significantly outperforms the vanilla RAG approach in terms of factual accuracy but at the cost of greatly reduced efficiency, with notable increases in both inference time and API call costs.",
            "We investigate the quality of retrieved passages and their impact on the performance of the RAG framework (as in Fig  2 (a)) in ambiguous QA task.",
            "Based on the findings, we propose an efficient and robust RAG framework for ambiguous QA,  di versify- v erify- a dapt ( Diva ).  This framework comprises two key components: Retrieval Diversification  (Sec  3.2 )  and Adaptive Generation  (Sec  3.3 ) . The retrieval diversification method aims to efficiently  diversify  the retrieved passages to encompass diverse interpretations. Subsequently, the adaptive generation method aims to  verify  the quality of the passages and  adapt  the most suitable approach tailored to their quality. Fig  2 (c) and Algorithm  1  show the overview and inference algorithm of  Diva , respectively.",
            "From the findings in Section  2 , if  P i subscript P i \\mathcal{P}_{i} caligraphic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  does not encompass all plausible interpretations,  Q i subscript Q i \\mathcal{Q}_{i} caligraphic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  A i subscript A i \\mathcal{A}_{i} caligraphic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , the response generated by the RAG framework is highly likely to be inaccurate. To this end, we introduce an adaptive generation (AG) method that dynamically adjust the response generation strategy among the RAG framework and closed-book LLM, which is achieved by verifying the quality of  P i subscript P i \\mathcal{P}_{i} caligraphic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  before attempting a solution.",
            "Adaptive Generation   Once we get the verification results from Eq  7 , if the  P i subscript P i \\mathcal{P}_{i} caligraphic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is classified to  Useful  or  PartialUseful , we decide to utilize the retrieved passages  P i subscript P i \\mathcal{P}_{i} caligraphic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  to generate a response by Eq  1  and  2 . If  P i subscript P i \\mathcal{P}_{i} caligraphic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is classified to  Useless , we decide to only utilize the LLMs internal knowledge to generate a response:  LLM  ( q i , I l ) LLM subscript q i subscript I l \\texttt{LLM}(q_{i},I_{\\text{l}}) LLM ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT l end_POSTSUBSCRIPT ) . The full prompt of  I l subscript I l I_{\\text{l}} italic_I start_POSTSUBSCRIPT l end_POSTSUBSCRIPT  is presented in Table  C  in Appendix  C . This process enables the utilization of the most suitable approach tailored to each retrieval quality, which is beneficial to both accuracy and efficiency.",
            "For more details of the evaluation metrics, please refer to Appendix  A.2 .",
            "In  Diva , the LLM is employed across three modules: retrieval diversification (Eqn  3 ), retrieval verification (Eqn  7 ), and adaptive response generation (Eqn  1 ,  2 , and closed-book LLM). For adaptive response generation, we use the same LLM backbones as the other baselines. For the retrieval diversification and verification modules, we assess the performance of GPT-3.5 (gpt-3.5-turbo) and GPT-4 (gpt-4) across them, ultimately opting to use GPT-4 for both modules in the ASQA dataset and GPT-3.5 for both modules in the SituatedQA dataset in all experiments. However, as demonstrated in Section  5.4 , other LLMs also perform effectively in these modules. For other implementation details, please refer to Appendix  A.4 .",
            "To evaluate the importance of each component of  Diva , namely retrieval diversification (RD) and adaptive generation (AG), we incrementally add them to Vanilla RAG (row 2 in Table  2 ). Table  2  reveals the following insights:   1)  RAG (row 2) with the closed-book LLM (row 1) significantly enhances the ability to handle ambiguity in questions.   2)  Implementing the RD module (row 3) enhances all performance metrics, demonstrating that RD effectively diversifies and improves the quality of retrieved passages, thereby enhancing the RAG framework.  3)  Incorporating the AG module (row 4) also boosts all metrics, showing that the retrieval verification method accurately identifies  Useless  passages. Additionally, this supports our finding in Sec  2  that when retrieved passages are of extremely low quality, the internal knowledge of LLMs proves more advantageous than RAG.",
            "Table  C  and Table  C  show an example of text prompt for inferring  pseudo-interpretations  (i.e.,  I a subscript I a I_{\\text{a}} italic_I start_POSTSUBSCRIPT a end_POSTSUBSCRIPT  and  I p subscript I p I_{\\text{p}} italic_I start_POSTSUBSCRIPT p end_POSTSUBSCRIPT  in Eqn  3 ). Table  C  shows an example of text prompt for verifying the retrieved passages (i.e.,  I v subscript I v I_{\\text{v}} italic_I start_POSTSUBSCRIPT v end_POSTSUBSCRIPT  in Eqn  7 ). Table  C  and  C  show an exmple of text prompt for response generation in vanilla RAG framework (i.e.,  I e subscript I e I_{\\text{e}} italic_I start_POSTSUBSCRIPT e end_POSTSUBSCRIPT  in Eqn  1  and  I g subscript I g I_{\\text{g}} italic_I start_POSTSUBSCRIPT g end_POSTSUBSCRIPT  in Eqn  2 )"
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Retrieval accuracy and corresponding QA performance on ASQA dataset.",
        "table": "S5.T3.5.5",
        "footnotes": [],
        "references": [
            "Results.    In Fig  3 (a), we observe that for only 34.6% of questions (i.e., Fully Cover) the retriever successfully retrieves passages that cover all plausible interpretations. Additionally, for 15.7% of questions (i.e., Not Cover) the retriever fails to retrieve any relevant passages.  More critically, as shown in Fig  3 (b), the performance of the RAG framework (i.e., RAG in the figure)  significantly deteriorates in terms of the factual accuracy (i.e., D-F1) when the retrieved passages pose a low quality issue (i.e., Partial Cover and Not Cover), indicating that it is highly susceptible to noise and irrelevant information in the ambiguous QA.",
            "Based on the findings, we propose an efficient and robust RAG framework for ambiguous QA,  di versify- v erify- a dapt ( Diva ).  This framework comprises two key components: Retrieval Diversification  (Sec  3.2 )  and Adaptive Generation  (Sec  3.3 ) . The retrieval diversification method aims to efficiently  diversify  the retrieved passages to encompass diverse interpretations. Subsequently, the adaptive generation method aims to  verify  the quality of the passages and  adapt  the most suitable approach tailored to their quality. Fig  2 (c) and Algorithm  1  show the overview and inference algorithm of  Diva , respectively.",
            "We compare our  Diva  against relevant models, including fully-supervised LMs, few-shot closed book LLMs, LLMs w/ RAG, and the adaptive generation. Specifically, fully-supervised LMs include the  1) T5 closed-book   Raffel et al. ( 2020 ) ,  2) T5 w/ JPR   Min et al. ( 2021 ) , and  3)   PaLM   Chowdhery et al. ( 2023 )   w/ Soft Prompt Tuning .  Few-shot closed book LLMs include  3) Vanilla LLAMA3, GPT-3.5-turbo, and GPT-4  and  4) Query refinement   Amplayo et al. ( 2022 ) . Few-shot LLMs w/ RAG include  5) Vanilla RAG  where we use RAC prompt in  Kim et al. ( 2023 ) , for  6) Iterative RAG  we use the sota method ToC  Kim et al. ( 2023 ) , and for adaptive generation  7) Self-RAG   Asai et al. ( 2023 ) . For more details of the baselines, please refer to Appendix  A.3 .",
            "In  Diva , the LLM is employed across three modules: retrieval diversification (Eqn  3 ), retrieval verification (Eqn  7 ), and adaptive response generation (Eqn  1 ,  2 , and closed-book LLM). For adaptive response generation, we use the same LLM backbones as the other baselines. For the retrieval diversification and verification modules, we assess the performance of GPT-3.5 (gpt-3.5-turbo) and GPT-4 (gpt-4) across them, ultimately opting to use GPT-4 for both modules in the ASQA dataset and GPT-3.5 for both modules in the SituatedQA dataset in all experiments. However, as demonstrated in Section  5.4 , other LLMs also perform effectively in these modules. For other implementation details, please refer to Appendix  A.4 .",
            "We evaluate the effectiveness of our proposed RD method in Table  3  using MRecall@ k k k italic_k   Min et al. ( 2021 ) . Vanilla RAG (row 1) involves basic retrieval of passages using a given question  q i subscript q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . \" + + +  RD\" (row 3) applies the RD method to row 1, using  pseudo-interpretations  generated by our proposed instructions (i.e.,  I p subscript I p I_{\\text{p}} italic_I start_POSTSUBSCRIPT p end_POSTSUBSCRIPT  and  I a subscript I a I_{\\text{a}} italic_I start_POSTSUBSCRIPT a end_POSTSUBSCRIPT ). Row 2 uses the RD method with  pseudo-interpretations  generated by the LLM query rewriter as described in  Ma et al. ( 2023 )  using simple instructions. \" + + +  Oracle\" (row 4) applies RD to Vanilla RAG using ground-truth interpretations from the ASQA dataset.",
            "Table  C  and Table  C  show an example of text prompt for inferring  pseudo-interpretations  (i.e.,  I a subscript I a I_{\\text{a}} italic_I start_POSTSUBSCRIPT a end_POSTSUBSCRIPT  and  I p subscript I p I_{\\text{p}} italic_I start_POSTSUBSCRIPT p end_POSTSUBSCRIPT  in Eqn  3 ). Table  C  shows an example of text prompt for verifying the retrieved passages (i.e.,  I v subscript I v I_{\\text{v}} italic_I start_POSTSUBSCRIPT v end_POSTSUBSCRIPT  in Eqn  7 ). Table  C  and  C  show an exmple of text prompt for response generation in vanilla RAG framework (i.e.,  I e subscript I e I_{\\text{e}} italic_I start_POSTSUBSCRIPT e end_POSTSUBSCRIPT  in Eqn  1  and  I g subscript I g I_{\\text{g}} italic_I start_POSTSUBSCRIPT g end_POSTSUBSCRIPT  in Eqn  2 )"
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "A3.T4.3",
        "footnotes": [],
        "references": [
            "Experimental Details.    We utilize the most recent ambiguous QA dataset, ASQA  Stelmakh et al. ( 2022 ) . We classify the quality of retrieved passages into three labels:  1)  Fully Cover,  2)  Partially Cover, and  3)  Not Cover. Fully Cover indicates that the retrieved passages encompass all plausible interpretations, Not Cover does that the retrieved passages do not contain any of them, and otherwise Partially Cover. We obtain these labels for each question by computing a string exact match between a set of retrieved passages and all plausible answers provided in ASQA as ground-truth answers. For implementation details of retrieving passages, see Appendix  A.4.1 .",
            "where  I p subscript I p I_{\\text{p}} italic_I start_POSTSUBSCRIPT p end_POSTSUBSCRIPT  and  I a subscript I a I_{\\text{a}} italic_I start_POSTSUBSCRIPT a end_POSTSUBSCRIPT  are carefully designed instructions for each step, respectively. We present the conceptual example of  I a subscript I a I_{\\text{a}} italic_I start_POSTSUBSCRIPT a end_POSTSUBSCRIPT  and  I p subscript I p I_{\\text{p}} italic_I start_POSTSUBSCRIPT p end_POSTSUBSCRIPT  in Fig  4  and full instructions in Table  C  and  C  of Appendix  C . For  LLM (  )  (\\cdot) (  ) , we consider GPT-3.5  Brown ( 2020 )  and GPT-4  Achiam et al. ( 2023 ) .",
            "In  Diva , the LLM is employed across three modules: retrieval diversification (Eqn  3 ), retrieval verification (Eqn  7 ), and adaptive response generation (Eqn  1 ,  2 , and closed-book LLM). For adaptive response generation, we use the same LLM backbones as the other baselines. For the retrieval diversification and verification modules, we assess the performance of GPT-3.5 (gpt-3.5-turbo) and GPT-4 (gpt-4) across them, ultimately opting to use GPT-4 for both modules in the ASQA dataset and GPT-3.5 for both modules in the SituatedQA dataset in all experiments. However, as demonstrated in Section  5.4 , other LLMs also perform effectively in these modules. For other implementation details, please refer to Appendix  A.4 ."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A3.T5.3",
        "footnotes": [],
        "references": [
            "We examine the factors contributing to  Diva s strong efficiency in Figure  5 , which illustrates the average number of input and output tokens per query when using the GPT-4 backbone.  First,  Diva s strong efficiency is largely due to the RD method.  Unlike Iterative RAG, which involves an average of 5.5 exploration steps per query and requires more than 12,000 tokens for input and 1,200 tokens for output, the RD method significantly reduces the number of tokens needed. This reduction leads to improved efficiency in inference time and API costs.  Second, although the RV method introduces some additional costs, these are acceptable compared to the complexity of Iterative RAG.  Moreover, RV enables the adaptive generation (AG) strategy, where the faster closed-book LLM is selectively used instead of RAG, further enhancing efficiency.  As a result,  Diva , which combines RD, RV, and AG, requires substantially less inference time and API costs.",
            "In  Diva , the LLM is employed across three modules: retrieval diversification (Eqn  3 ), retrieval verification (Eqn  7 ), and adaptive response generation (Eqn  1 ,  2 , and closed-book LLM). For adaptive response generation, we use the same LLM backbones as the other baselines. For the retrieval diversification and verification modules, we assess the performance of GPT-3.5 (gpt-3.5-turbo) and GPT-4 (gpt-4) across them, ultimately opting to use GPT-4 for both modules in the ASQA dataset and GPT-3.5 for both modules in the SituatedQA dataset in all experiments. However, as demonstrated in Section  5.4 , other LLMs also perform effectively in these modules. For other implementation details, please refer to Appendix  A.4 ."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A3.2.2",
        "footnotes": [],
        "references": [
            "Additionally, Fig  6  shows the performance and efficiency of baselines and  Diva  on the SituatedQA test set for short-form ambiguous QA tasks. All experimental results align with those seen in Table  1 , demonstrating strong generalizability of  Diva  across different types of ambiguous questions."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A3.4.2",
        "footnotes": [],
        "references": [
            "Adaptive Generation   Once we get the verification results from Eq  7 , if the  P i subscript P i \\mathcal{P}_{i} caligraphic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is classified to  Useful  or  PartialUseful , we decide to utilize the retrieved passages  P i subscript P i \\mathcal{P}_{i} caligraphic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  to generate a response by Eq  1  and  2 . If  P i subscript P i \\mathcal{P}_{i} caligraphic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is classified to  Useless , we decide to only utilize the LLMs internal knowledge to generate a response:  LLM  ( q i , I l ) LLM subscript q i subscript I l \\texttt{LLM}(q_{i},I_{\\text{l}}) LLM ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT l end_POSTSUBSCRIPT ) . The full prompt of  I l subscript I l I_{\\text{l}} italic_I start_POSTSUBSCRIPT l end_POSTSUBSCRIPT  is presented in Table  C  in Appendix  C . This process enables the utilization of the most suitable approach tailored to each retrieval quality, which is beneficial to both accuracy and efficiency.",
            "In  Diva , the LLM is employed across three modules: retrieval diversification (Eqn  3 ), retrieval verification (Eqn  7 ), and adaptive response generation (Eqn  1 ,  2 , and closed-book LLM). For adaptive response generation, we use the same LLM backbones as the other baselines. For the retrieval diversification and verification modules, we assess the performance of GPT-3.5 (gpt-3.5-turbo) and GPT-4 (gpt-4) across them, ultimately opting to use GPT-4 for both modules in the ASQA dataset and GPT-3.5 for both modules in the SituatedQA dataset in all experiments. However, as demonstrated in Section  5.4 , other LLMs also perform effectively in these modules. For other implementation details, please refer to Appendix  A.4 .",
            "For the retrieval diversification (RD) and retrieval verification (RV) modules, we explore how their performance is affected by the choice of LLM. We evaluate the impact of using GPT-3.5 and GPT-4 across both modules, comparing the overall QA performance against the sota baseline, ToC  Kim et al. ( 2023 ) , on the ASQA and SituatedQA datasets. Fig  7 (a) and (b) represent using GPT-3.5 and GPT-4 as the response generation models on the ASQA dataset, respectively. Fig  7 (c) represents using GPT-3.5 as the response generation model on the SituatedQA dataset.",
            "In Fig  7 , we observe the following:  1)   Diva  consistently outperforms ToC, regardless of the LLM model used in each module.  2)  While the RD module shows very stable results, the RV module appears relatively sensitive to the choice of LLM. This highlights that verifying the quality of retrieved passages for ambiguous questions requires more powerful natural language understanding ability, underscoring the need for future work to alleviate the dependency on the choice of LLM. Based on these results, we argue that  Diva  is a general framework that is robust across different LLM models.",
            "Table  C  and Table  C  show an example of text prompt for inferring  pseudo-interpretations  (i.e.,  I a subscript I a I_{\\text{a}} italic_I start_POSTSUBSCRIPT a end_POSTSUBSCRIPT  and  I p subscript I p I_{\\text{p}} italic_I start_POSTSUBSCRIPT p end_POSTSUBSCRIPT  in Eqn  3 ). Table  C  shows an example of text prompt for verifying the retrieved passages (i.e.,  I v subscript I v I_{\\text{v}} italic_I start_POSTSUBSCRIPT v end_POSTSUBSCRIPT  in Eqn  7 ). Table  C  and  C  show an exmple of text prompt for response generation in vanilla RAG framework (i.e.,  I e subscript I e I_{\\text{e}} italic_I start_POSTSUBSCRIPT e end_POSTSUBSCRIPT  in Eqn  1  and  I g subscript I g I_{\\text{g}} italic_I start_POSTSUBSCRIPT g end_POSTSUBSCRIPT  in Eqn  2 )"
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A3.6.2",
        "footnotes": [],
        "references": [
            "Figure  8  illustrates the reasoning chains of Iterative RAG, ToC  Kim et al. ( 2023 ) , and  Diva  using the ASQA question, \"The movement of food in the food pipe is called?\". In panel (a), the answer \"Peristalsis\" is easily covered during the first exploration, whereas \"Swallowing\" requires six steps of passage retrieval and LLM reasoning for exploration. In contrast, panel (b) shows that our  pseudo-interpretations  include both interpretations, with the RD retrieving passages that encompass all necessary information. Consequently, the LLM efficiently extracts all plausible interpretations from the retrieved passages without the need for the cumbersome iterative exploration process."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A3.T9.3",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "We use ColBERT",
        "and Bing search API as retrievers.",
        "https://openai.com/index/openai-api/",
        "https://www.microsoft.com/bing"
    ]
}