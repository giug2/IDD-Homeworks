{
    "PAPER'S NUMBER OF TABLES": 1,
    "S5.T1": {
        "caption": "Table 1: Number of rounds to reach accuracy Ax=A⋅xsubscript𝐴𝑥⋅𝐴𝑥A_{x}=A\\cdot x where A𝐴A is the target accuracy and x𝑥x is a fraction of it.",
        "table": "<table id=\"S5.T1.18\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T1.9.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.9.3.4\" class=\"ltx_td ltx_border_r ltx_border_tt\"></td>\n<td id=\"S5.T1.7.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\">CIFAR10, <math id=\"S5.T1.7.1.1.m1.1\" class=\"ltx_Math\" alttext=\"A=48.2\" display=\"inline\"><semantics id=\"S5.T1.7.1.1.m1.1a\"><mrow id=\"S5.T1.7.1.1.m1.1.1\" xref=\"S5.T1.7.1.1.m1.1.1.cmml\"><mi id=\"S5.T1.7.1.1.m1.1.1.2\" xref=\"S5.T1.7.1.1.m1.1.1.2.cmml\">A</mi><mo id=\"S5.T1.7.1.1.m1.1.1.1\" xref=\"S5.T1.7.1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S5.T1.7.1.1.m1.1.1.3\" xref=\"S5.T1.7.1.1.m1.1.1.3.cmml\">48.2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.7.1.1.m1.1b\"><apply id=\"S5.T1.7.1.1.m1.1.1.cmml\" xref=\"S5.T1.7.1.1.m1.1.1\"><eq id=\"S5.T1.7.1.1.m1.1.1.1.cmml\" xref=\"S5.T1.7.1.1.m1.1.1.1\"></eq><ci id=\"S5.T1.7.1.1.m1.1.1.2.cmml\" xref=\"S5.T1.7.1.1.m1.1.1.2\">𝐴</ci><cn type=\"float\" id=\"S5.T1.7.1.1.m1.1.1.3.cmml\" xref=\"S5.T1.7.1.1.m1.1.1.3\">48.2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.7.1.1.m1.1c\">A=48.2</annotation></semantics></math>%</td>\n<td id=\"S5.T1.8.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\">CINIC10, <math id=\"S5.T1.8.2.2.m1.1\" class=\"ltx_Math\" alttext=\"A=43.5\" display=\"inline\"><semantics id=\"S5.T1.8.2.2.m1.1a\"><mrow id=\"S5.T1.8.2.2.m1.1.1\" xref=\"S5.T1.8.2.2.m1.1.1.cmml\"><mi id=\"S5.T1.8.2.2.m1.1.1.2\" xref=\"S5.T1.8.2.2.m1.1.1.2.cmml\">A</mi><mo id=\"S5.T1.8.2.2.m1.1.1.1\" xref=\"S5.T1.8.2.2.m1.1.1.1.cmml\">=</mo><mn id=\"S5.T1.8.2.2.m1.1.1.3\" xref=\"S5.T1.8.2.2.m1.1.1.3.cmml\">43.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.8.2.2.m1.1b\"><apply id=\"S5.T1.8.2.2.m1.1.1.cmml\" xref=\"S5.T1.8.2.2.m1.1.1\"><eq id=\"S5.T1.8.2.2.m1.1.1.1.cmml\" xref=\"S5.T1.8.2.2.m1.1.1.1\"></eq><ci id=\"S5.T1.8.2.2.m1.1.1.2.cmml\" xref=\"S5.T1.8.2.2.m1.1.1.2\">𝐴</ci><cn type=\"float\" id=\"S5.T1.8.2.2.m1.1.1.3.cmml\" xref=\"S5.T1.8.2.2.m1.1.1.3\">43.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.8.2.2.m1.1c\">A=43.5</annotation></semantics></math>%</td>\n<td id=\"S5.T1.9.3.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">FEMNIST, <math id=\"S5.T1.9.3.3.m1.1\" class=\"ltx_Math\" alttext=\"A=69.5\" display=\"inline\"><semantics id=\"S5.T1.9.3.3.m1.1a\"><mrow id=\"S5.T1.9.3.3.m1.1.1\" xref=\"S5.T1.9.3.3.m1.1.1.cmml\"><mi id=\"S5.T1.9.3.3.m1.1.1.2\" xref=\"S5.T1.9.3.3.m1.1.1.2.cmml\">A</mi><mo id=\"S5.T1.9.3.3.m1.1.1.1\" xref=\"S5.T1.9.3.3.m1.1.1.1.cmml\">=</mo><mn id=\"S5.T1.9.3.3.m1.1.1.3\" xref=\"S5.T1.9.3.3.m1.1.1.3.cmml\">69.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.9.3.3.m1.1b\"><apply id=\"S5.T1.9.3.3.m1.1.1.cmml\" xref=\"S5.T1.9.3.3.m1.1.1\"><eq id=\"S5.T1.9.3.3.m1.1.1.1.cmml\" xref=\"S5.T1.9.3.3.m1.1.1.1\"></eq><ci id=\"S5.T1.9.3.3.m1.1.1.2.cmml\" xref=\"S5.T1.9.3.3.m1.1.1.2\">𝐴</ci><cn type=\"float\" id=\"S5.T1.9.3.3.m1.1.1.3.cmml\" xref=\"S5.T1.9.3.3.m1.1.1.3\">69.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.9.3.3.m1.1c\">A=69.5</annotation></semantics></math>%</td>\n</tr>\n<tr id=\"S5.T1.18.12\" class=\"ltx_tr\">\n<td id=\"S5.T1.18.12.10\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S5.T1.10.4.1\" class=\"ltx_td ltx_align_right\"><math id=\"S5.T1.10.4.1.m1.1\" class=\"ltx_Math\" alttext=\"A_{0.5}\" display=\"inline\"><semantics id=\"S5.T1.10.4.1.m1.1a\"><msub id=\"S5.T1.10.4.1.m1.1.1\" xref=\"S5.T1.10.4.1.m1.1.1.cmml\"><mi id=\"S5.T1.10.4.1.m1.1.1.2\" xref=\"S5.T1.10.4.1.m1.1.1.2.cmml\">A</mi><mn id=\"S5.T1.10.4.1.m1.1.1.3\" xref=\"S5.T1.10.4.1.m1.1.1.3.cmml\">0.5</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.10.4.1.m1.1b\"><apply id=\"S5.T1.10.4.1.m1.1.1.cmml\" xref=\"S5.T1.10.4.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T1.10.4.1.m1.1.1.1.cmml\" xref=\"S5.T1.10.4.1.m1.1.1\">subscript</csymbol><ci id=\"S5.T1.10.4.1.m1.1.1.2.cmml\" xref=\"S5.T1.10.4.1.m1.1.1.2\">𝐴</ci><cn type=\"float\" id=\"S5.T1.10.4.1.m1.1.1.3.cmml\" xref=\"S5.T1.10.4.1.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.10.4.1.m1.1c\">A_{0.5}</annotation></semantics></math></td>\n<td id=\"S5.T1.11.5.2\" class=\"ltx_td ltx_align_right\"><math id=\"S5.T1.11.5.2.m1.1\" class=\"ltx_Math\" alttext=\"A_{0.75}\" display=\"inline\"><semantics id=\"S5.T1.11.5.2.m1.1a\"><msub id=\"S5.T1.11.5.2.m1.1.1\" xref=\"S5.T1.11.5.2.m1.1.1.cmml\"><mi id=\"S5.T1.11.5.2.m1.1.1.2\" xref=\"S5.T1.11.5.2.m1.1.1.2.cmml\">A</mi><mn id=\"S5.T1.11.5.2.m1.1.1.3\" xref=\"S5.T1.11.5.2.m1.1.1.3.cmml\">0.75</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.11.5.2.m1.1b\"><apply id=\"S5.T1.11.5.2.m1.1.1.cmml\" xref=\"S5.T1.11.5.2.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T1.11.5.2.m1.1.1.1.cmml\" xref=\"S5.T1.11.5.2.m1.1.1\">subscript</csymbol><ci id=\"S5.T1.11.5.2.m1.1.1.2.cmml\" xref=\"S5.T1.11.5.2.m1.1.1.2\">𝐴</ci><cn type=\"float\" id=\"S5.T1.11.5.2.m1.1.1.3.cmml\" xref=\"S5.T1.11.5.2.m1.1.1.3\">0.75</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.11.5.2.m1.1c\">A_{0.75}</annotation></semantics></math></td>\n<td id=\"S5.T1.12.6.3\" class=\"ltx_td ltx_align_right ltx_border_r\"><math id=\"S5.T1.12.6.3.m1.1\" class=\"ltx_Math\" alttext=\"A_{0.95}\" display=\"inline\"><semantics id=\"S5.T1.12.6.3.m1.1a\"><msub id=\"S5.T1.12.6.3.m1.1.1\" xref=\"S5.T1.12.6.3.m1.1.1.cmml\"><mi id=\"S5.T1.12.6.3.m1.1.1.2\" xref=\"S5.T1.12.6.3.m1.1.1.2.cmml\">A</mi><mn id=\"S5.T1.12.6.3.m1.1.1.3\" xref=\"S5.T1.12.6.3.m1.1.1.3.cmml\">0.95</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.12.6.3.m1.1b\"><apply id=\"S5.T1.12.6.3.m1.1.1.cmml\" xref=\"S5.T1.12.6.3.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T1.12.6.3.m1.1.1.1.cmml\" xref=\"S5.T1.12.6.3.m1.1.1\">subscript</csymbol><ci id=\"S5.T1.12.6.3.m1.1.1.2.cmml\" xref=\"S5.T1.12.6.3.m1.1.1.2\">𝐴</ci><cn type=\"float\" id=\"S5.T1.12.6.3.m1.1.1.3.cmml\" xref=\"S5.T1.12.6.3.m1.1.1.3\">0.95</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.12.6.3.m1.1c\">A_{0.95}</annotation></semantics></math></td>\n<td id=\"S5.T1.13.7.4\" class=\"ltx_td ltx_align_right\"><math id=\"S5.T1.13.7.4.m1.1\" class=\"ltx_Math\" alttext=\"A_{0.5}\" display=\"inline\"><semantics id=\"S5.T1.13.7.4.m1.1a\"><msub id=\"S5.T1.13.7.4.m1.1.1\" xref=\"S5.T1.13.7.4.m1.1.1.cmml\"><mi id=\"S5.T1.13.7.4.m1.1.1.2\" xref=\"S5.T1.13.7.4.m1.1.1.2.cmml\">A</mi><mn id=\"S5.T1.13.7.4.m1.1.1.3\" xref=\"S5.T1.13.7.4.m1.1.1.3.cmml\">0.5</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.13.7.4.m1.1b\"><apply id=\"S5.T1.13.7.4.m1.1.1.cmml\" xref=\"S5.T1.13.7.4.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T1.13.7.4.m1.1.1.1.cmml\" xref=\"S5.T1.13.7.4.m1.1.1\">subscript</csymbol><ci id=\"S5.T1.13.7.4.m1.1.1.2.cmml\" xref=\"S5.T1.13.7.4.m1.1.1.2\">𝐴</ci><cn type=\"float\" id=\"S5.T1.13.7.4.m1.1.1.3.cmml\" xref=\"S5.T1.13.7.4.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.13.7.4.m1.1c\">A_{0.5}</annotation></semantics></math></td>\n<td id=\"S5.T1.14.8.5\" class=\"ltx_td ltx_align_right\"><math id=\"S5.T1.14.8.5.m1.1\" class=\"ltx_Math\" alttext=\"A_{0.75}\" display=\"inline\"><semantics id=\"S5.T1.14.8.5.m1.1a\"><msub id=\"S5.T1.14.8.5.m1.1.1\" xref=\"S5.T1.14.8.5.m1.1.1.cmml\"><mi id=\"S5.T1.14.8.5.m1.1.1.2\" xref=\"S5.T1.14.8.5.m1.1.1.2.cmml\">A</mi><mn id=\"S5.T1.14.8.5.m1.1.1.3\" xref=\"S5.T1.14.8.5.m1.1.1.3.cmml\">0.75</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.14.8.5.m1.1b\"><apply id=\"S5.T1.14.8.5.m1.1.1.cmml\" xref=\"S5.T1.14.8.5.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T1.14.8.5.m1.1.1.1.cmml\" xref=\"S5.T1.14.8.5.m1.1.1\">subscript</csymbol><ci id=\"S5.T1.14.8.5.m1.1.1.2.cmml\" xref=\"S5.T1.14.8.5.m1.1.1.2\">𝐴</ci><cn type=\"float\" id=\"S5.T1.14.8.5.m1.1.1.3.cmml\" xref=\"S5.T1.14.8.5.m1.1.1.3\">0.75</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.14.8.5.m1.1c\">A_{0.75}</annotation></semantics></math></td>\n<td id=\"S5.T1.15.9.6\" class=\"ltx_td ltx_align_right ltx_border_r\"><math id=\"S5.T1.15.9.6.m1.1\" class=\"ltx_Math\" alttext=\"A_{0.95}\" display=\"inline\"><semantics id=\"S5.T1.15.9.6.m1.1a\"><msub id=\"S5.T1.15.9.6.m1.1.1\" xref=\"S5.T1.15.9.6.m1.1.1.cmml\"><mi id=\"S5.T1.15.9.6.m1.1.1.2\" xref=\"S5.T1.15.9.6.m1.1.1.2.cmml\">A</mi><mn id=\"S5.T1.15.9.6.m1.1.1.3\" xref=\"S5.T1.15.9.6.m1.1.1.3.cmml\">0.95</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.15.9.6.m1.1b\"><apply id=\"S5.T1.15.9.6.m1.1.1.cmml\" xref=\"S5.T1.15.9.6.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T1.15.9.6.m1.1.1.1.cmml\" xref=\"S5.T1.15.9.6.m1.1.1\">subscript</csymbol><ci id=\"S5.T1.15.9.6.m1.1.1.2.cmml\" xref=\"S5.T1.15.9.6.m1.1.1.2\">𝐴</ci><cn type=\"float\" id=\"S5.T1.15.9.6.m1.1.1.3.cmml\" xref=\"S5.T1.15.9.6.m1.1.1.3\">0.95</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.15.9.6.m1.1c\">A_{0.95}</annotation></semantics></math></td>\n<td id=\"S5.T1.16.10.7\" class=\"ltx_td ltx_align_right\"><math id=\"S5.T1.16.10.7.m1.1\" class=\"ltx_Math\" alttext=\"A_{0.5}\" display=\"inline\"><semantics id=\"S5.T1.16.10.7.m1.1a\"><msub id=\"S5.T1.16.10.7.m1.1.1\" xref=\"S5.T1.16.10.7.m1.1.1.cmml\"><mi id=\"S5.T1.16.10.7.m1.1.1.2\" xref=\"S5.T1.16.10.7.m1.1.1.2.cmml\">A</mi><mn id=\"S5.T1.16.10.7.m1.1.1.3\" xref=\"S5.T1.16.10.7.m1.1.1.3.cmml\">0.5</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.16.10.7.m1.1b\"><apply id=\"S5.T1.16.10.7.m1.1.1.cmml\" xref=\"S5.T1.16.10.7.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T1.16.10.7.m1.1.1.1.cmml\" xref=\"S5.T1.16.10.7.m1.1.1\">subscript</csymbol><ci id=\"S5.T1.16.10.7.m1.1.1.2.cmml\" xref=\"S5.T1.16.10.7.m1.1.1.2\">𝐴</ci><cn type=\"float\" id=\"S5.T1.16.10.7.m1.1.1.3.cmml\" xref=\"S5.T1.16.10.7.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.16.10.7.m1.1c\">A_{0.5}</annotation></semantics></math></td>\n<td id=\"S5.T1.17.11.8\" class=\"ltx_td ltx_align_right\"><math id=\"S5.T1.17.11.8.m1.1\" class=\"ltx_Math\" alttext=\"A_{0.75}\" display=\"inline\"><semantics id=\"S5.T1.17.11.8.m1.1a\"><msub id=\"S5.T1.17.11.8.m1.1.1\" xref=\"S5.T1.17.11.8.m1.1.1.cmml\"><mi id=\"S5.T1.17.11.8.m1.1.1.2\" xref=\"S5.T1.17.11.8.m1.1.1.2.cmml\">A</mi><mn id=\"S5.T1.17.11.8.m1.1.1.3\" xref=\"S5.T1.17.11.8.m1.1.1.3.cmml\">0.75</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.17.11.8.m1.1b\"><apply id=\"S5.T1.17.11.8.m1.1.1.cmml\" xref=\"S5.T1.17.11.8.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T1.17.11.8.m1.1.1.1.cmml\" xref=\"S5.T1.17.11.8.m1.1.1\">subscript</csymbol><ci id=\"S5.T1.17.11.8.m1.1.1.2.cmml\" xref=\"S5.T1.17.11.8.m1.1.1.2\">𝐴</ci><cn type=\"float\" id=\"S5.T1.17.11.8.m1.1.1.3.cmml\" xref=\"S5.T1.17.11.8.m1.1.1.3\">0.75</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.17.11.8.m1.1c\">A_{0.75}</annotation></semantics></math></td>\n<td id=\"S5.T1.18.12.9\" class=\"ltx_td ltx_align_right\"><math id=\"S5.T1.18.12.9.m1.1\" class=\"ltx_Math\" alttext=\"A_{0.95}\" display=\"inline\"><semantics id=\"S5.T1.18.12.9.m1.1a\"><msub id=\"S5.T1.18.12.9.m1.1.1\" xref=\"S5.T1.18.12.9.m1.1.1.cmml\"><mi id=\"S5.T1.18.12.9.m1.1.1.2\" xref=\"S5.T1.18.12.9.m1.1.1.2.cmml\">A</mi><mn id=\"S5.T1.18.12.9.m1.1.1.3\" xref=\"S5.T1.18.12.9.m1.1.1.3.cmml\">0.95</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.18.12.9.m1.1b\"><apply id=\"S5.T1.18.12.9.m1.1.1.cmml\" xref=\"S5.T1.18.12.9.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T1.18.12.9.m1.1.1.1.cmml\" xref=\"S5.T1.18.12.9.m1.1.1\">subscript</csymbol><ci id=\"S5.T1.18.12.9.m1.1.1.2.cmml\" xref=\"S5.T1.18.12.9.m1.1.1.2\">𝐴</ci><cn type=\"float\" id=\"S5.T1.18.12.9.m1.1.1.3.cmml\" xref=\"S5.T1.18.12.9.m1.1.1.3\">0.95</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.18.12.9.m1.1c\">A_{0.95}</annotation></semantics></math></td>\n</tr>\n<tr id=\"S5.T1.18.13\" class=\"ltx_tr\">\n<td id=\"S5.T1.18.13.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">FedAvg</td>\n<td id=\"S5.T1.18.13.2\" class=\"ltx_td ltx_align_right ltx_border_t\">12</td>\n<td id=\"S5.T1.18.13.3\" class=\"ltx_td ltx_align_right ltx_border_t\">82</td>\n<td id=\"S5.T1.18.13.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">-</td>\n<td id=\"S5.T1.18.13.5\" class=\"ltx_td ltx_align_right ltx_border_t\">13</td>\n<td id=\"S5.T1.18.13.6\" class=\"ltx_td ltx_align_right ltx_border_t\">-</td>\n<td id=\"S5.T1.18.13.7\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">-</td>\n<td id=\"S5.T1.18.13.8\" class=\"ltx_td ltx_align_right ltx_border_t\">49</td>\n<td id=\"S5.T1.18.13.9\" class=\"ltx_td ltx_align_right ltx_border_t\">75</td>\n<td id=\"S5.T1.18.13.10\" class=\"ltx_td ltx_align_right ltx_border_t\">138</td>\n</tr>\n<tr id=\"S5.T1.18.14\" class=\"ltx_tr\">\n<td id=\"S5.T1.18.14.1\" class=\"ltx_td ltx_align_left ltx_border_r\">FedDF</td>\n<td id=\"S5.T1.18.14.2\" class=\"ltx_td ltx_align_right\">7</td>\n<td id=\"S5.T1.18.14.3\" class=\"ltx_td ltx_align_right\">40</td>\n<td id=\"S5.T1.18.14.4\" class=\"ltx_td ltx_align_right ltx_border_r\">112</td>\n<td id=\"S5.T1.18.14.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T1.18.14.5.1\" class=\"ltx_text ltx_font_bold\">2</span></td>\n<td id=\"S5.T1.18.14.6\" class=\"ltx_td ltx_align_right\">30</td>\n<td id=\"S5.T1.18.14.7\" class=\"ltx_td ltx_align_right ltx_border_r\">-</td>\n<td id=\"S5.T1.18.14.8\" class=\"ltx_td ltx_align_right\">-</td>\n<td id=\"S5.T1.18.14.9\" class=\"ltx_td ltx_align_right\">-</td>\n<td id=\"S5.T1.18.14.10\" class=\"ltx_td ltx_align_right\">-</td>\n</tr>\n<tr id=\"S5.T1.18.15\" class=\"ltx_tr\">\n<td id=\"S5.T1.18.15.1\" class=\"ltx_td ltx_align_left ltx_border_r\">FedNTD</td>\n<td id=\"S5.T1.18.15.2\" class=\"ltx_td ltx_align_right\">12</td>\n<td id=\"S5.T1.18.15.3\" class=\"ltx_td ltx_align_right\">41</td>\n<td id=\"S5.T1.18.15.4\" class=\"ltx_td ltx_align_right ltx_border_r\">112</td>\n<td id=\"S5.T1.18.15.5\" class=\"ltx_td ltx_align_right\">13</td>\n<td id=\"S5.T1.18.15.6\" class=\"ltx_td ltx_align_right\">46</td>\n<td id=\"S5.T1.18.15.7\" class=\"ltx_td ltx_align_right ltx_border_r\">-</td>\n<td id=\"S5.T1.18.15.8\" class=\"ltx_td ltx_align_right\">-</td>\n<td id=\"S5.T1.18.15.9\" class=\"ltx_td ltx_align_right\">-</td>\n<td id=\"S5.T1.18.15.10\" class=\"ltx_td ltx_align_right\">-</td>\n</tr>\n<tr id=\"S5.T1.18.16\" class=\"ltx_tr\">\n<td id=\"S5.T1.18.16.1\" class=\"ltx_td ltx_align_left ltx_border_r\">FedProx</td>\n<td id=\"S5.T1.18.16.2\" class=\"ltx_td ltx_align_right\">35</td>\n<td id=\"S5.T1.18.16.3\" class=\"ltx_td ltx_align_right\">93</td>\n<td id=\"S5.T1.18.16.4\" class=\"ltx_td ltx_align_right ltx_border_r\">-</td>\n<td id=\"S5.T1.18.16.5\" class=\"ltx_td ltx_align_right\">13</td>\n<td id=\"S5.T1.18.16.6\" class=\"ltx_td ltx_align_right\">-</td>\n<td id=\"S5.T1.18.16.7\" class=\"ltx_td ltx_align_right ltx_border_r\">-</td>\n<td id=\"S5.T1.18.16.8\" class=\"ltx_td ltx_align_right\">142</td>\n<td id=\"S5.T1.18.16.9\" class=\"ltx_td ltx_align_right\">-</td>\n<td id=\"S5.T1.18.16.10\" class=\"ltx_td ltx_align_right\">-</td>\n</tr>\n<tr id=\"S5.T1.18.17\" class=\"ltx_tr\">\n<td id=\"S5.T1.18.17.1\" class=\"ltx_td ltx_align_left ltx_border_r\">FedReg</td>\n<td id=\"S5.T1.18.17.2\" class=\"ltx_td ltx_align_right\">35</td>\n<td id=\"S5.T1.18.17.3\" class=\"ltx_td ltx_align_right\">108</td>\n<td id=\"S5.T1.18.17.4\" class=\"ltx_td ltx_align_right ltx_border_r\">-</td>\n<td id=\"S5.T1.18.17.5\" class=\"ltx_td ltx_align_right\">16</td>\n<td id=\"S5.T1.18.17.6\" class=\"ltx_td ltx_align_right\">-</td>\n<td id=\"S5.T1.18.17.7\" class=\"ltx_td ltx_align_right ltx_border_r\">-</td>\n<td id=\"S5.T1.18.17.8\" class=\"ltx_td ltx_align_right\">-</td>\n<td id=\"S5.T1.18.17.9\" class=\"ltx_td ltx_align_right\">-</td>\n<td id=\"S5.T1.18.17.10\" class=\"ltx_td ltx_align_right\">-</td>\n</tr>\n<tr id=\"S5.T1.18.18\" class=\"ltx_tr\">\n<td id=\"S5.T1.18.18.1\" class=\"ltx_td ltx_align_left ltx_border_r\">MOON</td>\n<td id=\"S5.T1.18.18.2\" class=\"ltx_td ltx_align_right\">82</td>\n<td id=\"S5.T1.18.18.3\" class=\"ltx_td ltx_align_right\">-</td>\n<td id=\"S5.T1.18.18.4\" class=\"ltx_td ltx_align_right ltx_border_r\">-</td>\n<td id=\"S5.T1.18.18.5\" class=\"ltx_td ltx_align_right\">124</td>\n<td id=\"S5.T1.18.18.6\" class=\"ltx_td ltx_align_right\">-</td>\n<td id=\"S5.T1.18.18.7\" class=\"ltx_td ltx_align_right ltx_border_r\">-</td>\n<td id=\"S5.T1.18.18.8\" class=\"ltx_td ltx_align_right\">-</td>\n<td id=\"S5.T1.18.18.9\" class=\"ltx_td ltx_align_right\">-</td>\n<td id=\"S5.T1.18.18.10\" class=\"ltx_td ltx_align_right\">-</td>\n</tr>\n<tr id=\"S5.T1.18.19\" class=\"ltx_tr\">\n<td id=\"S5.T1.18.19.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\">Flashback</td>\n<td id=\"S5.T1.18.19.2\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span id=\"S5.T1.18.19.2.1\" class=\"ltx_text ltx_font_bold\">2</span></td>\n<td id=\"S5.T1.18.19.3\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span id=\"S5.T1.18.19.3.1\" class=\"ltx_text ltx_font_bold\">4</span></td>\n<td id=\"S5.T1.18.19.4\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S5.T1.18.19.4.1\" class=\"ltx_text ltx_font_bold\">10</span></td>\n<td id=\"S5.T1.18.19.5\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">4</td>\n<td id=\"S5.T1.18.19.6\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span id=\"S5.T1.18.19.6.1\" class=\"ltx_text ltx_font_bold\">5</span></td>\n<td id=\"S5.T1.18.19.7\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S5.T1.18.19.7.1\" class=\"ltx_text ltx_font_bold\">6</span></td>\n<td id=\"S5.T1.18.19.8\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span id=\"S5.T1.18.19.8.1\" class=\"ltx_text ltx_font_bold\">3</span></td>\n<td id=\"S5.T1.18.19.9\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span id=\"S5.T1.18.19.9.1\" class=\"ltx_text ltx_font_bold\">5</span></td>\n<td id=\"S5.T1.18.19.10\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span id=\"S5.T1.18.19.10.1\" class=\"ltx_text ltx_font_bold\">16</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We outline and analyze our experimental findings to investigate whether mitigating forgetting successfully addresses the issues of slow and unstable convergence observed in the initial problem laid out in ",
                "§",
                " ",
                "1",
                ". The experimental results stem from three settings: CIFAR10 and CINIC10, where heterogeneous data partitions are created using Dirichlet distribution with ",
                "β",
                "=",
                "0.1",
                "𝛽",
                "0.1",
                "\\beta=0.1",
                " and FEMNIST with 3,432 clients, following the natural heterogeneity of the dataset.\nFurthermore, we do an ablation study on the different components of the algorithm.\nWe use the same neural network architecture that is used in  ",
                "Lee et al. (",
                "2021",
                "); McMahan et al. (",
                "2017",
                ")",
                ", which is a 2-layer ",
                "Convolutional Neural Network (CNN)",
                ".\nSummaries of the datasets, partitions, and more details on the experimental setup, as well as additional results, are reported in ",
                "Appendix",
                " ",
                "A",
                ".",
                "We compare Flashback against several baseline methods, namely:\n\n",
                "\n",
                "1)",
                " ",
                "FedAvg ",
                "(McMahan et al., ",
                "2017",
                ")",
                ",\n",
                "\n",
                "2)",
                " ",
                "FedDF ",
                "(Lin et al., ",
                "2020",
                ")",
                ",\n",
                "\n",
                "3)",
                " ",
                "FedNTD ",
                "(Lee et al., ",
                "2021",
                ")",
                ",\n",
                "\n",
                "4)",
                " ",
                "FedProx ",
                "(Li et al., ",
                "2020",
                ")",
                ",\n",
                "\n",
                "5)",
                " ",
                "FedReg ",
                "(Xu et al., ",
                "2022",
                ")",
                ",\n",
                "\n",
                "6)",
                " ",
                "MOON ",
                "(Li et al., ",
                "2021b",
                ")",
                ".\n",
                "\n",
                "\nIt is noteworthy that both FedNTD and FedReg target forgetting in ",
                "FL",
                " (discussed further in ",
                "§",
                " ",
                "6",
                ").\nFlashback server distillation is performed until early stopping is triggered on the public validation set (details in ",
                "Appendix",
                " ",
                "A",
                "). Moreover, Flashback only introduces one additional hyperparameters ",
                "γ",
                "𝛾",
                "\\gamma",
                ", which represents how fast trust is built in the global model. We analyze the effect of ",
                "γ",
                "𝛾",
                "\\gamma",
                " later in the section.\nWe start by evaluating Flashback performance by showing round-to-accuracy, round forgetting, and the local-global loss over the rounds.",
                "Improved round-to-accuracy.",
                "\nWe evaluate the learning efficiency of Flashback and other baselines by showing the accuracy over rounds in ",
                "Fig.",
                " ",
                "3",
                ". Flashback consistently shows a faster convergence to a high accuracy. Furthermore, we show the number of rounds it takes to reach a target accuracy and fractions of that target accuracy in ",
                "Table",
                " ",
                "1",
                "; we include the result of a central training on the public dataset. Flashback shows a much faster convergence than other baselines.\nThis indicates that addressing forgetting on the clients’ local update and at the aggregation step does provide training stability and indeed a faster convergence.",
                "Less round forgetting.",
                "\nWe show the empirical cumulative distribution function (ECDF) of the round forgetting in ",
                "Fig.",
                " ",
                "6",
                ". We see that Flashback successfully reduces round forgetting. Also, FedNTD has less round forgetting than the remaining baselines. In the appendix, we show the round forgetting over the rounds (",
                "Fig.",
                " ",
                "12",
                ").",
                "Minimizing local models divergence.",
                "\nTo further understand the effect of Flashback on the training behavior, we show the transition of the mean loss of the local models to the loss of the global model over the rounds in ",
                "Fig.",
                " ",
                "4",
                ". This gives us an insight into the effect of the regularization made by our dynamic distillation.\nWe see that the mean loss of the local models of the other baselines always spikes, signifying a divergence of these models from the global training objective, while Flashback has a much more stable loss. This shows that ",
                "Eq.",
                " ",
                "3",
                " regularizes the local models well so that they do not diverge too much from the global training objective.",
                "In the rest of the section, we delve deeper into Flashback to understand its behavior and validate its performance gains.",
                "Dissecting the distillation.",
                "\nTo show the importance of performing dynamic distillation during clients’ updates and at the server’s aggregation step, we conduct an experiment where we run Flashback with local distillation only and with server distillation only (c.f. ",
                "Fig.",
                " ",
                "7",
                "). We observe that doing dynamic distillation at either side of the algorithm – client update and aggregation step – doesn’t address forgetting or gets a similar performance to Flashback.",
                "To validate the ",
                "importance of dynamic distillation at client- and server sides",
                " towards Flashback’s performance gains, we create a baseline where we replace the local distillation loss with not-true distillation (NTD) loss ",
                "Lee et al. (",
                "2021",
                ")",
                ".\nFrom ",
                "Fig.",
                " ",
                "8",
                ", we observe that this baseline doesn’t perform as well as Flashback, and performs similarly to FedNTD.\nThis indicates that performing the distillation at the server needs well-regularized local models (teachers), which is further supported by our previous experiment contrasting Flashback to single-side distillation (c.f. ",
                "Fig.",
                " ",
                "7",
                ").",
                "Effect of public dataset size.",
                "\nFlashback requires the availability of a public labeled dataset. This may be a limiting assumption in some cases. To study this limitation, we explore a few scenarios for the size of the public dataset in ",
                "Fig.",
                " ",
                "9",
                ":\n\n",
                "\n",
                "1)",
                " ",
                "9000 samples (",
                "15",
                "15",
                "15",
                "% of CIFAR10),\n",
                "\n",
                "2)",
                " ",
                "1125 samples (",
                "1.88",
                "1.88",
                "1.88",
                "% of CIFAR10),\n",
                "\n",
                "3)",
                " ",
                "1283 samples that have unbalanced class distribution (",
                "2.14",
                "2.14",
                "2.14",
                "% of CIFAR10),\n",
                "\n",
                "4)",
                " ",
                "450 samples (",
                "0.75",
                "0.75",
                "0.75",
                "% of CIFAR10).\n",
                "\n",
                "\nFor all of these scenarios, we train a model centrally on the public dataset.\nWe find that Flashback can benefit from a large balanced public dataset. Most importantly, Flashback can work well with a small public dataset (1125 samples is the default in all experiments). Furthermore, even if the public dataset has a class imbalance Flashback still performs relatively well. In all of the cases, Flashback always outperforms central training on the public dataset.\nOverall, Flashback requires the availability of a public dataset, however, it does not require a huge amount of data or hard requirements for the class distribution to be very balanced.",
                "Training on the public dataset.",
                "\nWe also experiment to answer the following question: ",
                "does the performance improvement of Flashback come from the fact that we train the global model on a public labeled dataset?",
                " To answer this question we create a naive baseline, where we extend FedAvg to fine-tune the global model after the aggregation step at every communication round. From ",
                "Fig.",
                " ",
                "10",
                ", we see that ",
                "FedAvg",
                " with fine-tuning quickly reaches a stale model and eventually collapses. We believe this collapse happens due to the local models diverging too much after the local update such that aggregating those models fails. This is evident by the spike of the local model’s loss.",
                "The importance of ",
                " ",
                "γ",
                "𝛾",
                "\\gamma",
                ".\nAs mentioned, Flashback has a single hyperparameter ",
                "γ",
                "𝛾",
                "\\gamma",
                ", which dictates how fast models will trust the global model as a competent teacher. We explore the effect of this hyperparameter in ",
                "Fig.",
                " ",
                "11",
                ". We find that setting this parameter to a larger value leads the learning process to get to a stale solution quickly. This is intuitive since large ",
                "γ",
                "𝛾",
                "\\gamma",
                " leads the global model label count to grow faster, therefore, this dominates the loss term in ",
                "Eq.",
                " ",
                "3",
                " during both the client- and the server update.\nSmaller ",
                "γ",
                "𝛾",
                "\\gamma",
                " gives the best performance because it gives the local models time to learn from their own private data by having a small weight to the distillation term in ",
                "Eq.",
                " ",
                "3",
                ". However, too small value for ",
                "γ",
                "𝛾",
                "\\gamma",
                " such as ",
                "0.001",
                "0.001",
                "0.001",
                " slows the training process, since during the local training the distillation term in ",
                "Eq.",
                " ",
                "3",
                " will be very small in the early rounds."
            ]
        ]
    }
}