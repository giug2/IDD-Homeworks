{
    "id_table_1": {
        "caption": "Table 1 :  Character Error Rate (CER, in %) on standard HTR datasets.",
        "table": "S3.T1.25",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Given an input text-line image, our goal is to predict its transcription, i.e., a sequence of characters. We tackle this problem as a character detection task and build on the DINO-DETR architecture  [ 57 ] , shown in Figure  2 , to simultaneously detect all characters. The rest of this section is organized as follows. First, in Section  3.1 , we discuss how we pretrain a character-detection model using synthetic data with character-level supervision. Then, in Section  3.2 , we explain how we finetune our model over real images from the target dataset with only line-level supervision and make final predictions.",
            "We evaluate our approach on text line HTR in various languages with latin script: IAM (English)  [ 29 ] , RIMES  [ 39 ]  (French), and READ  [ 44 ]  (Old German). For IAM, we follow common practice  [ 13 ,  25 ,  19 ]  and use the unofficial Aachen partition  3 3 3 https://www.openslr.org/56/ . It includes 6,161 training lines, 966 validation lines, and 2,915 testing lines. The READ 2016 dataset consists of Early Modern German handwritten pages from the Ratsprotokolle collection. It includes 8,367 training lines, 1,043 validation lines, and 1,140 test lines. The RIMES dataset is composed of administrative documents written in French. We use the RIMES-2011 version which includes only text lines from the letters body  4 4 4 https://zenodo.org/records/10805048 . The training set has 10,188 lines, which we split into a training set (80%) and a validation set (20%). The test set includes 778 lines. To assess the quality of our prediction, we use the Character Error Rate (CER) as defined by  [ 13 ] . In Table  1 , we present the results of our experiments across all three datasets.",
            "Most of our results are visually meaningful, as can be seen in Figure  1 . In Figure  4 , we show the worst results for each dataset, which outlines the limitations of our model and the difficulty or problems in the datasets. On the Google1000 sample, the annotation is inaccurate since it is provided by an OCR model. On the IAM dataset, our model struggles to distinguish lowercase from uppercase letters in a specific portion of the test set where the test set letters are mostly uppercase, because the training set does not include such lines. On the RIMES sample, the input image is an example of a badly cropped line, a recurring problem in this dataset. The model misses some of the letters in  couvert  and relies on a misleading visual cue for  e  instead of  e . On both READ and CASIA v2, the model misses a few characters with high variation in writing style such as the  D  in READ or digits in CASIA v2 which are sparse in the training set of the model. On CASIA v2 where the fine-tuning process is much longer than for other datasets, we observed that the bounding boxes degenerate while the Character Error Rate (CER) continues to improve. This can be understood by the fact that no specific loss is used to constrain bounding boxes, and that features have large receptive fields. Thus, in Figure  1 , we show the prediction of our model in middle of the training process, before the bounding boxes degenerate. On the Copiale dataset, the error comes from the labels, the last dozen characters are present in the image but not in the annotation."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Accurate Rate (AR) and Correct Rate (CR)  [ 56 ]  for Chinese HTR on CASIA  [ 27 ] .",
        "table": "S3.T2.14",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "Given an input text-line image, our goal is to predict its transcription, i.e., a sequence of characters. We tackle this problem as a character detection task and build on the DINO-DETR architecture  [ 57 ] , shown in Figure  2 , to simultaneously detect all characters. The rest of this section is organized as follows. First, in Section  3.1 , we discuss how we pretrain a character-detection model using synthetic data with character-level supervision. Then, in Section  3.2 , we explain how we finetune our model over real images from the target dataset with only line-level supervision and make final predictions.",
            "Since character labels and bounding boxes are known in synthetic data, we can simply leverage a standard DINO-DETR architecture  [ 57 ] , depicted in Figure  2  and the associated loss. Multi-scale image features are extracted through a CNN backbone and are then further refined in the  N e subscript N e N_{e} italic_N start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  transformer encoder layers through deformable attention  [ 60 ] . The transformer decoder is composed of  N d subscript N d N_{d} italic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  decoder layers which are fed a set of  Q Q Q italic_Q  character queries. Each character query  q q q italic_q  is the concatenation of a content query and a positional query, initialized by the output of the encoder. In each decoder layer, the character queries first interact through self-attention and then attend to the encoder image features via deformable cross-attention. The decoder outputs  Q Q Q italic_Q  bounding boxes  b ^ q  R 4 subscript ^ b q superscript R 4 \\hat{\\mathbf{b}}_{q}\\in\\mathbb{R}^{4} over^ start_ARG bold_b end_ARG start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT  and for each one a probability vector  c ^ q  R | A 0 | subscript ^ c q superscript R subscript A 0 \\hat{\\mathbf{c}}_{q}\\in\\mathbb{R}^{|\\mathcal{A}_{0}|} over^ start_ARG bold_c end_ARG start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT | caligraphic_A start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | end_POSTSUPERSCRIPT , where  | A 0 | subscript A 0 |\\mathcal{A}_{0}| | caligraphic_A start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT |  is the number of characters in the alphabet  A 0 subscript A 0 \\mathcal{A}_{0} caligraphic_A start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  and  q q q italic_q  is the query index.",
            "To showcase the versatility of our approach, we also evaluate our method on CASIA v2  [ 27 ]  a benchmark for handwritten Chinese text-line recognition, using Accurate Rate (AR) and Correct Rate (CR) as defined in  [ 56 ] . The alphabet for this database has 2,703 characters, significantly more than Latin script. The training set consists of 41,781 text lines, which we further divide into a new training set (80%) and a validation set (20%) while the test set has 10,449 text lines. We report our results in Table  2 . Our method outperforms the current state of the art by  Yu et al. [ 56 ]  in both accurate and correct rate."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Symbol Error Rates (SER) and Word Accuracy (WA) for cipher recognition  [ 41 ]",
        "table": "S3.T3.24",
        "footnotes": [
            ""
        ],
        "references": [
            "Given an input text-line image, our goal is to predict its transcription, i.e., a sequence of characters. We tackle this problem as a character detection task and build on the DINO-DETR architecture  [ 57 ] , shown in Figure  2 , to simultaneously detect all characters. The rest of this section is organized as follows. First, in Section  3.1 , we discuss how we pretrain a character-detection model using synthetic data with character-level supervision. Then, in Section  3.2 , we explain how we finetune our model over real images from the target dataset with only line-level supervision and make final predictions.",
            "To simulate text line images, we build on the synthetic generation pipeline of  Monnier and Aubry [ 32 ] . For the Latin alphabet, we sample a font from a set of publicly available fonts 1 1 1 https://github.com/google/fonts , using a font labeled as \"handwriting\" with a 50% probability, and a random background from a set of empty page photographs. We use the font to render the text and blend it with the background by using a random color and adding structured noise to both the background and font layers. For the Chinese alphabet, we use the CASIA v1 dataset  [ 27 ] , which contains various handwritten Chinese characters from different writers, to create synthetic sentences. We also apply significant blur data augmentation. This process results in challenging samples, as shown in the left column of Figure  3 .",
            "In addition, we use random erasing, masking complete vertical blocks, and small horizontal blocks completely, similar to  Zhong et al. [ 59 ], Chaudhary and Bali [ 10 ] . This results in the text lines shown in the right column of Figure  3 . Masking increases the robustness of the model, but also has additional motivations. First, masking vertical blocks can make a character unreadable or even completely hide it. In cases where the text is sampled from natural language, our network is thus encouraged to learn an implicit language model to predict detections for unreadable or even non-visible characters. Since our training time is modest, this model is far simpler than Large Language Models, but we still found it significantly improved predictions.",
            "We evaluate our approach on the Borg and Copiale cipher  5 5 5 https://pages.cvc.uab.es/abaro/datasets.html , using the line segmentation and splits outlined in  [ 3 ] . The Borg cipher dataset  6 6 6 https://digi.vatlib.it/view/MSS_Borg.lat.898 , derived from a 17th-century manuscript, has 34 unique characters. The dataset is divided into a training set of 195 lines, a validation set of 31 lines, and a test set of 273 lines. The Copiale cipher, from 1730-1760, has 99 symbols and includes 711 training lines, 156 validation lines, and 908 test lines. As reported in Table  3 , we achieve better results than the state of the art on both data sets. Figure  7  in the appendix presents challenging examples from the Copiale Cipher, where our method performs well."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Ablation studies on the IAM and Borg dataset",
        "table": "S4.T4.11",
        "footnotes": [],
        "references": [
            "We performed OCR on the Google1000 dataset  [ 22 ] , which contains scanned historical printed books, using our pre-trained model. We use the English Volume 0002, with 5,097 training lines, 567 validation lines, and 630 testing lines. Prior to fine-tuning on real data, our model has only  3.61 % percent 3.61 3.61\\% 3.61 %  in Character Error Rate (CER) and this rate is further reduced to  2.04 % percent 2.04 2.04\\% 2.04 %  after fine-tuning. Most of the \"errors\" are actually due to mislabeled samples. An example is shown in Figure  4 . In Appendix, we also show qualitative results on the ICDAR 2024 Competition on Multi Font Group Recognition (Figure  8 ), a dataset that includes multiple fonts and languages, showcasing our methods ability to handle various languages and printing styles.",
            "Most of our results are visually meaningful, as can be seen in Figure  1 . In Figure  4 , we show the worst results for each dataset, which outlines the limitations of our model and the difficulty or problems in the datasets. On the Google1000 sample, the annotation is inaccurate since it is provided by an OCR model. On the IAM dataset, our model struggles to distinguish lowercase from uppercase letters in a specific portion of the test set where the test set letters are mostly uppercase, because the training set does not include such lines. On the RIMES sample, the input image is an example of a badly cropped line, a recurring problem in this dataset. The model misses some of the letters in  couvert  and relies on a misleading visual cue for  e  instead of  e . On both READ and CASIA v2, the model misses a few characters with high variation in writing style such as the  D  in READ or digits in CASIA v2 which are sparse in the training set of the model. On CASIA v2 where the fine-tuning process is much longer than for other datasets, we observed that the bounding boxes degenerate while the Character Error Rate (CER) continues to improve. This can be understood by the fact that no specific loss is used to constrain bounding boxes, and that features have large receptive fields. Thus, in Figure  1 , we show the prediction of our model in middle of the training process, before the bounding boxes degenerate. On the Copiale dataset, the error comes from the labels, the last dozen characters are present in the image but not in the annotation.",
            "We conduct an ablation study in Table  4 . We first examine the impact of the pretraining dataset on IAM, which uses latin script. Using a model trained on English and random erasing both improve performances. We then study the effects of the finetuning strategy as well as the presence of random erasing during finetuning on the IAM and Borg datasets."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Symbol Error Rate (SER, in %) on ICDAR 2024 Competition on Handwriting Recognition of Historical Ciphers.",
        "table": "Sx1.T5.25",
        "footnotes": [],
        "references": [
            "DTLR also ranked first in the ICDAR 2024 Competition on Handwriting Recognition of Historical Ciphers  7 7 7 https://rrc.cvc.uab.es/?ch=27  for the Borg and Ramanacoil ciphers. The competition featured five ciphers: a digit-based cipher, updated versions of the Borg and Copiale ciphers, enciphered documents from the Bibliotheque Nationale de France (BNF), and the Ramanacoil manuscript. The digit cipher uses 76 symbols, mostly digits, with various diacritics. The BNF cipher contains 37 unique graphical symbols. The Ramanacoil cipher uses 24 Latin-based symbols and special characters for seven key words. In Appendix, we report the competition results in Table  5 , and show visual examples of our results on the Ramanacoil dataset in Figure  7 ."
        ]
    }
}