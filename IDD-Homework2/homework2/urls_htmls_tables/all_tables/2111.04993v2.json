{
    "Sx4.T1": {
        "caption": "Table 1: Our proposed 16-task split of Mini-ImageNet and CIFAR100 datasets for incremental few-shot learning.",
        "table": null,
        "footnotes": [],
        "references": [
            "Datasets.  We evaluate performance on four datasets:\nMini-ImageNet (Vinyals et al. 2016), CIFAR100 (Krizhevsky 2009), CUB-200-2011 (Wah et al. 2011) and Tiered-ImageNet (Ren et al. 2018). Mini-ImageNet consists of 600\n84×\\times84 images from 100 classes. We propose a split with 20 of these\nclasses as meta-test set unseen in all training sessions. The other 80\nclasses are used to form the incremental meta-training set which is split into 4\nor 16 tasks with equal numbers of classes for incremental meta-learning. Each\nclass in each task is then divided into a meta-training split with 500 images,\nfrom which support and query sets are sampled for each episode, and a test split\nwith 100 images that is set aside for task-specific evaluation. We select\nNe​x=20subscript𝑁𝑒𝑥20N_{ex}=20 exemplars per class before proceeding to the next task. Table 1 is an illustration of the 16-task setting data split."
        ]
    },
    "Sx4.T2": {
        "caption": "Table 2: Meta-test accuracy by training session in the 4-task setting. We evaluate 1-shot and 5-way few-shot recognition on three datasets using two different backbones.",
        "table": null,
        "footnotes": [],
        "references": [
            "We also compare our method with others on short sequences.\nIn table 2, we first evaluate our model with a\n4-Conv backbone on 1-shot/5-way few-shot on three datasets (5-shot results in supplementary). We see that FT suffers from\ncatastrophic forgetting, and meta-test accuracy drops dramatically and exhibits\noverfitting to the current task. IDA is not able to improve meta-test accuracy\non Mini-ImageNet, but improves performance on CIFAR100 and CUB. As for EIML,\nwith exemplars it shows large improvement compared to IDA. However, our method\nERD outperforms EIML by a large margin after learning all four tasks. These\nresults further confirm the observations on the 16-task setting. ERD not\nonly achieves the best performance with less forgetting, but also gets\ncloser to the upper bound after the last task. Note also that CIFAR100\nand Mini-ImageNet are coarse-grained datasets, compared to CUB, which makes few\nshot classification much harder due to intra-class variability."
        ]
    },
    "Sx4.T3": {
        "caption": "Table 3: Meta-test accuracy and mean accuracy on seen classes by training session on the 16-task setting. We evaluate 1-shot/5-way few-shot recognition to compare with continual learning methods (5-shot results in supplementary).",
        "table": null,
        "footnotes": [],
        "references": [
            "Comparison with standard CL methods. \n\nAs shown in Table 3, we compare our method with three state-of-the-art CL methods: iCaRL (Rebuffi et al. 2017), PODNet (Douillard et al. 2020) and UCIR (Hou et al. 2019). For the evaluation on seen classes, we followed the same protocol as IDA, where the average classification accuracy is calculated over Ne​psubscript𝑁𝑒𝑝N_{ep} episodes. Note that this is different from the evaluation in regular CL. Thus, these methods cannot be directly applied in this scenario. Instead, we use them to continually learn representations and then evaluate them with a nearest centroid classifier for few-shot learning. Observe how, on seen classes evaluation, UCIR works better than iCaRL and PODNet. But under meta-test evaluation, iCaRL works the best among them. PODNet performs similar to the FT baseline in both cases. Clearly, without episodic training, regular CL methods are inferior to our proposed ERD."
        ]
    },
    "Sx4.T4": {
        "caption": "Table 4: Meta-test accuracy by training sessions on the 4-task and 16-task settings. We evaluate 1-shot/5-way few-shot recognition on Mini-ImageNet, CIFAR-100 and CUB.",
        "table": null,
        "footnotes": [],
        "references": [
            "Since in Relation Networks there is no embedding to\nexploit for computing prototypes as in ProtoNets,\nIDA and EIML cannot be directly applied. Therefore we only compare with FT in\nthis experiment. As the experimental results shown in\nTable 4, our model not only surpasses the FT baseline significantly, but\nalso gets close to the joint training upper bounds after the last task,\nespecially on the CUB dataset."
        ]
    }
}