{
    "id_table_1": {
        "caption": "Table 1:  Common quantitative metrics used for evaluation of different models during hyperparameter search. Reconstruction is evaluated for the test set while novelty and uniqueness are evaluated for a sampled set of 16000 polymers sampled from Gaussian noise.",
        "table": "S2.T1.1",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "An inverse design model requires a machine-readable representation that accurately captures the molecular structure and further can be generated by a generative model. This is especially challenging for polymers. Polymers possess a hierarchical structure spanning from the monomer chemistry to the polymer morphology. To a certain extent, all levels indicated in Figure  1 (A) impact the materials properties. At the smallest scale, polymers consist of monomer units with varying stoichiometries. Depending on the polymerization type and the reaction/processing conditions, the monomers can be arranged in diverse chain architectures, which are often described as polymer topologies. For instance, simply linear topologies with alternating, block, random, or other monomer arrangements exist. Other examples are lightly branched chains  (Zhu et al.,  2011 ) , hyperbranched  (Hult et al.,  1999 ) , cyclic  (Hadjichristidis et al.,  2001 )  or star-shaped  (Bazan and Schrock,  1991 )  polymer topologies. Further, the processing conditions influence morphological traits such as crystallinity and therewith also the properties  (Hatakeyama-Sato,  2023 ) . Besides, many polymers are stochastic. That means that they are ensembles of macromolecules of differing sizes and weights (see Figure  1 (B)), which complicates the use of just one single polymer representation.",
            "The string representation encapsulates stoichiometry and connection probabilities as numerical values next to the monomer SMILES. A polymer string  x S ( i ) superscript subscript x S i \\boldsymbol{x_{S}}^{(i)} bold_italic_x start_POSTSUBSCRIPT bold_italic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT  can be formally described as a sequence of tokens  x S ( i ) = { x S 1 ( i ) , x S 2 ( i ) , ... , x S N ( i ) } superscript subscript x S i superscript subscript subscript x S 1 i superscript subscript subscript x S 2 i ... superscript subscript subscript x S N i \\boldsymbol{x_{S}}^{(i)}=\\{\\boldsymbol{x_{S}}_{1}^{(i)},\\boldsymbol{x_{S}}_{2}% ^{(i)},...,\\boldsymbol{x_{S}}_{N}^{(i)}\\} bold_italic_x start_POSTSUBSCRIPT bold_italic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT = { bold_italic_x start_POSTSUBSCRIPT bold_italic_S end_POSTSUBSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , bold_italic_x start_POSTSUBSCRIPT bold_italic_S end_POSTSUBSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , ... , bold_italic_x start_POSTSUBSCRIPT bold_italic_S end_POSTSUBSCRIPT start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT } , where  x S i ( i ) superscript subscript subscript x S i i \\boldsymbol{x_{S}}_{i}^{(i)} bold_italic_x start_POSTSUBSCRIPT bold_italic_S end_POSTSUBSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT  can be a SMILES token describing the monomer chemistry or a numerical token as part of the stochiometry or chain architecture. We use a tokenization scheme that combines a SMILES tokenizer and numerical number tokenizer adapted from the Regression Transformer  Born and Manica ( 2022 ) . In Appendix  A.1  we show an example tokenization of a polymer string.",
            "The loss in Equation ( 1 ) to train the graph-2-string VAE architecture consists of a reconstruction loss term  L R  e  c subscript L R e c \\mathcal{L}_{Rec} caligraphic_L start_POSTSUBSCRIPT italic_R italic_e italic_c end_POSTSUBSCRIPT , Kullback-Leibler divergence loss  L K  L  D subscript L K L D \\mathcal{L}_{KLD} caligraphic_L start_POSTSUBSCRIPT italic_K italic_L italic_D end_POSTSUBSCRIPT  and an additional property loss term  L y subscript L y \\mathcal{L}_{y} caligraphic_L start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT . For additional details on the deriviation of the loss see Appendix  A.2 . We further use two hyperparameters    \\beta italic_  and    \\alpha italic_  to balance the loss terms.",
            "During the hyperparameter search and to assess the final model performance, we calculate quantitative metrics that have been established in the small molecule domain: reconstruction, validity, novelty, and uniqueness, defined in Table  1 . Reconstruction is evaluated on the test set, encoding each sample to a latent point  z =  z  \\boldsymbol{z}=\\boldsymbol{\\mu} bold_italic_z = bold_italic_  and passing it to the decoder to produce the polymer string representation. Novelty and Uniqueness are evaluated for the sampled set which consists of 16000 polymers randomly sampled from Gaussian noise. Validity is calculated both for the reconstructed test set as well as the sampled set. In the results in Section  3  we use the best performing model according to these model metrics and qualitative assessments of the latent space structure. In Section  3.3  we explain in more detail how we combine the quantitative and qualitative assessment.",
            "Figure  6  visualizes the first two principal components of the training dataset encoded to the LS. We observe that the LS of the best model primarily organizes according to monomer chemistry. Figure  14  in the Appendix shows that the LS is more locally structured according to stoichiometry and chain architecture. This organization aligns with the intuitive understanding that polymers sharing the same monomer types are inherently more similar, regardless of differences in chain architecture. Further, we verify that the LS structures according to the ionization potential (IP) and electron affinity (EA), which facilitates successful optimization in the LS (see Section  3.4 ). Figure  7  illustrates distribution of property values and gradients in the PCA plot of the latent space, demonstrating the models ability to capture property-structure relationships.",
            "As detailed in Section  2.4 , to select the best model we conducted a hyperparameter search over the weights    \\beta italic_  and    \\alpha italic_  in the loss terms in Equation ( 2 ) and evaluated the model using the performance metrics from Table  1 . We observed that lower values of    \\beta italic_  and    \\alpha italic_  enhance reconstruction accuracy by prioritizing the reconstruction term in the loss function. However, achieving high novelty and validity in randomly generated samples requires larger    \\beta italic_  values. If    \\beta italic_  is too small, the latent space becomes discontinuous with \"holes\" or regions of low confidence, leading to invalid molecules when sampled. We found that for this dataset,    \\beta italic_  values between 0.0003 and 0.0004 work best to optimize the trade-off between reconstruction, validity and novelty. Additionally, we found that a well-structured latent space (LS) is crucial for successful inverse design using optimization algorithms. Ideally, the LS should organize polymers by their properties, as demonstrated in Section  3.2.1 . Increasing the weighting factor    \\alpha italic_  improved the organization of molecules regarding their properties, which was visually confirmed by examining PCA plots of the latent space showing clear property gradients (Figure  7 ). A sufficiently high    \\alpha italic_  also leads to accurate property predictions, as verified in Appendix  B.2 . Contrary, increasing    \\alpha italic_  too much harms the reconstruction performance. As a result, we select the best model (  = 0.2  0.2 \\alpha=0.2 italic_ = 0.2  and   = 0.0004  0.0004 \\beta=0.0004 italic_ = 0.0004 ) according to both the quantitative metrics and visual inspection of gradients in the latent space. In the subsequent section, we use this model for the inverse design approach. Note, that the hyperparameter tuning does not guarantee reaching a globally optimal configuration for the inverse design problem and may vary for new datasets.",
            "With the objective function defined in Equation ( 4 ), we can directly apply optimization techniques to decode novel polymer photocatalysts that best align with our targets. We compare Bayesian optimization (BO) and genetic algorithms (GA), as explained in Section  2.5 . Table  2  shows a comparison of the best objective values of polymers in the training dataset with the decoded polymers using BO and GA. Notably, both BO and GA produce top 3 candidates that better satisfy the objective than the best candidate from the dataset. Inspecting the average objective of the top 10 polymers in Table  2  shows that both optimization strategies outperform selecting the ten best candidates from the dataset, while we observe that the GA outperforms the BO. One possible explanation is the computational efficiency of the GA which allows it to evaluate a greater number of molecules than the BO within the same runtime, giving it an advantage in identifying more candidates with high objective values. Thus, we conclude that with the aim to obtain a large variety of good candidate polymers, the GA, in combination with our model, is the most effective approach for inverse design.   The result is a library of polymer structures that adhere to the specified target objectives. Figures  11(a)  and   11(b)  show the effective distribution shift towards polymers with targeted EA and IP, compared to the wide distribution of properties in the training data.",
            "Analysis of the best polymer candidates in Figures  9  and  10  suggests that the properties EA and IP, are primarily influenced by the monomer chemistries. Both optimization algorithms converge to a specific region in the latent space characterized by monomers of similar sizes, typically featuring one aromatic ring and at least one nitrogen atom, with rare occurences of other heteroatoms such as oxygen, fluorine and chlorine. Sulfur and bromine, which are present in the dataset, do not appear in the optimal polymers identified by our model. The chemical region that the optimization algorithms converge to is strongly influenced by the property targets defined in the objective function, which can be easily adjusted to investigate different scenarios (see Appendix  B.5 ).",
            "Stoichiometry and chain architecture play a less significant role. For instance, candidates two and three of the BO consist of the same monomers and block chain architecture and have very similar properties, despite the different stoichiometry. Similarly, candidate 5 and 6 only differ in the chain architecture but possess very similar properties. The importance of monomer chemistry compared to stoichiometry and chain architecture aligns with our observations of the latent space structure (Figures  6  and  14 ) and the analysis in prior studies  (Aldeghi and Coley,  2022 ) . Nonetheless, its important to acknowledge that for other datasets or properties, stoichiometry and chain architecture might play a more critical role in the optimization process.",
            "Figure  12  shows an example monomer combination found in the dataset. Changing the stoichiometry from 3:1 to 1:3 leads to a significant drop of the EA compared to the data set range. Further, for a fixed stoichiometry, changing the chain architecture from alternating to block structure leads to a non negligible drop in the EA of the polymer material. This example illustrates that the higher order structure matters to learn accurate structure-property relationships.",
            "To assess how well properties are predicted for unseen data, we encode the test set to the latent representations and use the trained neural network on top of the latent space to predict the properties. Figure  13  shows a high performance, comparable to the values reported in  [Aldeghi and Coley,  2022 ] . This is important to ensure accurate property values as feedback for the optimization algorithms in the inverse design approach.",
            "This analysis focuses on generating polymers that are similar to the best performing photocatalyst in  [Bai et al.,  2019 ] . We make use of two common techniques. First, we can sample the latent neighborhood of good polymer candidates that showed high HERs in literature. Second, we can perform interpolation in latent space between interesting candidates. Given our well structured latent space, as demonstrated in Section  3.2.1 , these approaches should help to yield a higher fraction of high-performing candidate polymers than just randomly sampled ones.",
            "Sampling around polymers that are known to be high-performing photocatalysts can prove as a powerful approach to generate novel polymers with similarly good or better performance. To do so, we take one high-performing photocatalysts as a starting point and encode the respective graph to a continuous latent vector  z s  e  e  d subscript z s e e d \\boldsymbol{z}_{seed} bold_italic_z start_POSTSUBSCRIPT italic_s italic_e italic_e italic_d end_POSTSUBSCRIPT . Next, we repeatedly add small noise (drawn from  N  ( 0 , 0.25 ) N 0 0.25 \\mathcal{N}(0,0.25) caligraphic_N ( 0 , 0.25 ) ) to obtain new latent vectors. We can then decode these new latent vectors to structures. Figure  15  shows the results of sampling around the polymer with the best experimental HER in  Bai et al. [ 2019 ] s study, highlighted in black dashed lines. The novel molecules we obtained by sampling around the latent vector of the encoded seed molecule possess a variety of changes from the encoded polymer, where monomer B is varied most. We see that monomer A is altered less, attributable to the limited monomer A variety in the dataset. Same holds for the stoichiometry while the chain architecture is varied more than stoichiometry, meaning that it has less strong signal in the latent space than the stoichiometry (less robust to changes in the latent vector).",
            "The second strategy to sample novel polymers without property optimization is interpolation in the latent space. With the aim to find promising novel photocatalysts we can for instance interpolate between the two best polymers found in  [Bai et al.,  2019 ] . For this we assume that both polymers possess a 1:1 monomer stoichiometry and alternating chain architecture. However, the two best candidates are structurally very similar: Monomer A is the same and monomer B is only slightly different. As a result, the interpolation only leads to one additional candidate that is found on the interpolation path (see Figure  16 ). While we expect the polymers to be close in the latent space, we could think of more candidates that lie on the interpolation path. Generally, interpolation is more interesting in scenarios where we want to know the path in the chemical space between two structurally substantially different molecules. For instance, we can interpolate between the best molecule and a random molecule in the dataset, leading to a more interesting path as visualized in Figure  17 .",
            "Let us consider the best-performing molecule in  Bai et al. [ 2019 ] s study which has an EA of approximately -2.64 eV and an IP of 1.61 eV (as reported in the used training dataset  Aldeghi and Coley [ 2022 ] . These values differ markedly from the targets in the objective function in Section  3.4 , which aimed for minimal EAs (converging to around -4 eV) and an IP of 1 eV. Hence, we observe that the results in Figure  18  differ significantly from those in Figure  9 . Interestingly, the sixth-best candidate (indicated by black dashed lines) is identical to the molecule from the experimental study we used to specify the property targets. It should be the best candidate, matching the objective values exactly. However, despite the very high accuracy of the property predictor (see Section  B.2 ), there is a small prediction error. We also see a variety of structures that satisfy the objective function well but differ structurally from the best polymer reported in the literature."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparison of polymers from the dataset and the generated data after optimization in the latent space. The table shows the average objective values according to Equation ( 4 ) of the ten best polymers and the objective values of the top three polymers.",
        "table": "S3.T2.2",
        "footnotes": [
            ""
        ],
        "references": [
            "Our approach integrates graph- and string-based representations, as depicted in Figure  2 . Both represent the polymers as stochastic ensembles, including stoichiometry and chain architecture information.",
            "Each polymer graph  G ( i ) = { V ( i ) , E ( i ) } superscript G i superscript V i superscript E i G^{(i)}=\\{V^{(i)},E^{(i)}\\} italic_G start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT = { italic_V start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_E start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT }  consists of nodes (or vertices)  V i subscript V i V_{i} italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  representing atoms and edges  E i subscript E i E_{i} italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  representing bonds. In the graph representation by  Aldeghi and Coley ( 2022 ) , the edges are weighted according to the probability that they occur in the polymer. Bonds within the monomer structure have the weight  w = 1.0 w 1.0 w=1.0 italic_w = 1.0  and bonds between monomers have a weight  w  ( 0 ,  1.0 ] w 0 1.0 w\\in(0,\\,1.0] italic_w  ( 0 , 1.0 ] . The weights reflect how monomers are connected, essentially depicting the chain architecture of the polymer. Furthermore, the stoichiometry is incorporated as node weights, where nodes of the same monomer have the same weight. The node weights are used during pooling in the graph neural network (GNN) introduced by  Aldeghi and Coley ( 2022 ) . We make use of their GNN architecture as encoder block for the VAE, see Section  2.2 .",
            "The loss in Equation ( 1 ) to train the graph-2-string VAE architecture consists of a reconstruction loss term  L R  e  c subscript L R e c \\mathcal{L}_{Rec} caligraphic_L start_POSTSUBSCRIPT italic_R italic_e italic_c end_POSTSUBSCRIPT , Kullback-Leibler divergence loss  L K  L  D subscript L K L D \\mathcal{L}_{KLD} caligraphic_L start_POSTSUBSCRIPT italic_K italic_L italic_D end_POSTSUBSCRIPT  and an additional property loss term  L y subscript L y \\mathcal{L}_{y} caligraphic_L start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT . For additional details on the deriviation of the loss see Appendix  A.2 . We further use two hyperparameters    \\beta italic_  and    \\alpha italic_  to balance the loss terms.",
            "The reconstruction loss  L R  e  c subscript L R e c \\mathcal{L}_{Rec} caligraphic_L start_POSTSUBSCRIPT italic_R italic_e italic_c end_POSTSUBSCRIPT  is calculated as the weighted cross entropy loss between the ground truth and predicted polymer string given the latent representation  z z \\boldsymbol{z} bold_italic_z . Further, as will be outlined in Section  2.3 , we have a combination of labelled and unlabelled data. To handle partially labelled data, we introduce a mask  m m m italic_m  for the property loss",
            "ensuring it is calculated solely for labelled data. This approach limits gradient computation to labelled instances, thereby updating the property prediction network based exclusively on labelled data in a batch with  N N N italic_N  samples. Given two ( M = 2 M 2 M=2 italic_M = 2 ) continuous properties  P = { p 1 , p 2 } P subscript p 1 subscript p 2 P=\\{p_{1},p_{2}\\} italic_P = { italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT }  of interest (see Section  2.3 ), we design the neural network to output two property values and calculate the masked property prediction loss as the mean squared error averaged over the two properties and the number of samples in the batch.",
            "We test our model in a case study to design novel conjugated copolymers which are emerging as promising organic photocatalysts for green hydrogen production through photocatalytic water splitting. The process involves a photocatalyst that absorbs light to generate charge carriers which reduce protons to hydrogen while oxidizing water or an electron donor. Addressing the vast synthetic diversity of conjugated polymers,  Bai et al. ( 2019 )  conducted a high-throughput screening study to explore various copolymers, analyzing their hydrogen evolution rate (HER) as a measure of photocatalytic activity. Ultimately, we aim to find candidate polymers with improved catalytic activity (see Sections  2.5  and  3.4 ).",
            "We split the dataset in 80% training data, and 10% validation and 10% test data. Since we have multiple data points per monomer combination (varying stoichiomtry and chain architecture), we split the data by monomer combinations to prevent data leakage, i.e. ensuring that there are no monomer combinations in the test- or validation set that also occur in the training set. During training we apply the default teacher forcing in the Transformer decoder and use early stopping based on the evaluation on the held out validation set. During inference, i.e. the novel generation of molecules from latent embeddings  z z \\boldsymbol{z} bold_italic_z , we use beam search with a beam size of five to decode the polymer strings. This maximizes the final probability of the sequence and helps with generating valid SMILES. Where applicable, we adopted the hyperparameters of  (Vogel et al.,  2023 ) , which is the unsupervised version (only encoder and decoder) of the model used in this work. To ensure a more stable training we use a cyclical schedule for    \\beta italic_   (Fu et al.,  2019 ) . Additionally, we performed a hyperparameter search over    \\beta italic_  and    \\alpha italic_  to balance the contribution of the loss terms in Equation ( 2 ). Note, that we limited the hyperparameter search of these parameters to a fixed number of evaluations in a random search approach and subsequent fine-grained grid search.",
            "where    \\kappa italic_  balances exploration and exploitation (higher    \\kappa italic_  favors more exploration). We use the default value for    \\kappa italic_  of 2.576.   A substantial number of works discuss good practices  (Siivola et al.,  2021 )  or propose strategies to overcome common challenges of Bayesian optimization particularly in the relatively high-dimensional latent spaces of VAEs, e.g.,  (Griffiths and Hernandez-Lobato,  2020 ; Maus et al.,  2022 ; Tripp et al.,  2020 ) . A general issue encountered while performing any optimization in the latent space of VAEs is the presence of holes or low-confidence regions, which do not align with the high-confidence regions produced by the encoder. The optimization algorithm often tends to exploit these regions to optimize the objective, leading to a mismatch of the predicted and the real properties of the decoded polymer. To address this, we designed an approach aimed at ensuring high confidence in property predictions  p  p I  P  ( z ) p subscript p I P z pp_{IP}(\\boldsymbol{z}) italic_p italic_p start_POSTSUBSCRIPT italic_I italic_P end_POSTSUBSCRIPT ( bold_italic_z )  and  p  p E  A  ( z ) p subscript p E A z pp_{EA}(\\boldsymbol{z}) italic_p italic_p start_POSTSUBSCRIPT italic_E italic_A end_POSTSUBSCRIPT ( bold_italic_z ) . This involves correcting points sampled by BO, denoted as  z BO subscript z BO \\boldsymbol{z_{\\text{BO}}} bold_italic_z start_POSTSUBSCRIPT BO end_POSTSUBSCRIPT , by decoding and re-encoding them to obtain  z   z BO superscript z bold- subscript z BO \\boldsymbol{z^{\\prime}}\\approx\\boldsymbol{z_{\\text{BO}}} bold_italic_z start_POSTSUPERSCRIPT bold_ end_POSTSUPERSCRIPT  bold_italic_z start_POSTSUBSCRIPT BO end_POSTSUBSCRIPT . Consequently, we ensure that the point lies within a high-confidence region of the latent space, suitable as input to the property predictor network. Thus, we achieve more accurate property predictions (see also Appendix  B.2 ), which mitigates discrepancies between the predicted properties used for objective function evaluations and the corresponding decoded molecules.",
            "As detailed in Section  2.4 , to select the best model we conducted a hyperparameter search over the weights    \\beta italic_  and    \\alpha italic_  in the loss terms in Equation ( 2 ) and evaluated the model using the performance metrics from Table  1 . We observed that lower values of    \\beta italic_  and    \\alpha italic_  enhance reconstruction accuracy by prioritizing the reconstruction term in the loss function. However, achieving high novelty and validity in randomly generated samples requires larger    \\beta italic_  values. If    \\beta italic_  is too small, the latent space becomes discontinuous with \"holes\" or regions of low confidence, leading to invalid molecules when sampled. We found that for this dataset,    \\beta italic_  values between 0.0003 and 0.0004 work best to optimize the trade-off between reconstruction, validity and novelty. Additionally, we found that a well-structured latent space (LS) is crucial for successful inverse design using optimization algorithms. Ideally, the LS should organize polymers by their properties, as demonstrated in Section  3.2.1 . Increasing the weighting factor    \\alpha italic_  improved the organization of molecules regarding their properties, which was visually confirmed by examining PCA plots of the latent space showing clear property gradients (Figure  7 ). A sufficiently high    \\alpha italic_  also leads to accurate property predictions, as verified in Appendix  B.2 . Contrary, increasing    \\alpha italic_  too much harms the reconstruction performance. As a result, we select the best model (  = 0.2  0.2 \\alpha=0.2 italic_ = 0.2  and   = 0.0004  0.0004 \\beta=0.0004 italic_ = 0.0004 ) according to both the quantitative metrics and visual inspection of gradients in the latent space. In the subsequent section, we use this model for the inverse design approach. Note, that the hyperparameter tuning does not guarantee reaching a globally optimal configuration for the inverse design problem and may vary for new datasets.",
            "With the objective function defined in Equation ( 4 ), we can directly apply optimization techniques to decode novel polymer photocatalysts that best align with our targets. We compare Bayesian optimization (BO) and genetic algorithms (GA), as explained in Section  2.5 . Table  2  shows a comparison of the best objective values of polymers in the training dataset with the decoded polymers using BO and GA. Notably, both BO and GA produce top 3 candidates that better satisfy the objective than the best candidate from the dataset. Inspecting the average objective of the top 10 polymers in Table  2  shows that both optimization strategies outperform selecting the ten best candidates from the dataset, while we observe that the GA outperforms the BO. One possible explanation is the computational efficiency of the GA which allows it to evaluate a greater number of molecules than the BO within the same runtime, giving it an advantage in identifying more candidates with high objective values. Thus, we conclude that with the aim to obtain a large variety of good candidate polymers, the GA, in combination with our model, is the most effective approach for inverse design.   The result is a library of polymer structures that adhere to the specified target objectives. Figures  11(a)  and   11(b)  show the effective distribution shift towards polymers with targeted EA and IP, compared to the wide distribution of properties in the training data.",
            "The goal in our study is to generate novel polymers with targeted property values indicating a high HER. To do so, we leverage the continuous latent space of our model, allowing for optimization of the latent variables to decode candidates with desired properties. In our case study the ideal property to optimize would be the HER, which is not given for the dataset we used to train our model. However,  Bai et al. [ 2019 ]  found that materials with more negative electron affinity (EA) and more positive ioinization potential (IP) showed better HERs. This is likely related to the optical gap, approximated as |EA - IP|; larger optical gaps often correlate with higher HER. Thus, maximizing the optical gap by minimizing EA and maximizing IP could help identify high-performance materials. Looking at the studys results more closely, it showed that the HER was nearly zero for polymers with positive EA values on the standard hydrogen electrode (SHE) scale and peaked at an EA of around -2 eV. This peak was at the lower end of the EAs observed, suggesting optimal driving force for the proton reduction when minimizing this property. The number of materials with high observed HERs increased with rising IP, peaking around 1 eV before declining. This indicates the importance of balancing the driving force for oxidation. We conclude that, to optimize copolymers for photocatalytic activity, one could target minimal EA values and IP values near 1 eV (see Section  2.5  and the objective function in Equation ( 4 ).",
            "Figure  12  shows an example monomer combination found in the dataset. Changing the stoichiometry from 3:1 to 1:3 leads to a significant drop of the EA compared to the data set range. Further, for a fixed stoichiometry, changing the chain architecture from alternating to block structure leads to a non negligible drop in the EA of the polymer material. This example illustrates that the higher order structure matters to learn accurate structure-property relationships.",
            "This analysis focuses on generating polymers that are similar to the best performing photocatalyst in  [Bai et al.,  2019 ] . We make use of two common techniques. First, we can sample the latent neighborhood of good polymer candidates that showed high HERs in literature. Second, we can perform interpolation in latent space between interesting candidates. Given our well structured latent space, as demonstrated in Section  3.2.1 , these approaches should help to yield a higher fraction of high-performing candidate polymers than just randomly sampled ones.",
            "Let us consider the best-performing molecule in  Bai et al. [ 2019 ] s study which has an EA of approximately -2.64 eV and an IP of 1.61 eV (as reported in the used training dataset  Aldeghi and Coley [ 2022 ] . These values differ markedly from the targets in the objective function in Section  3.4 , which aimed for minimal EAs (converging to around -4 eV) and an IP of 1 eV. Hence, we observe that the results in Figure  18  differ significantly from those in Figure  9 . Interestingly, the sixth-best candidate (indicated by black dashed lines) is identical to the molecule from the experimental study we used to specify the property targets. It should be the best candidate, matching the objective values exactly. However, despite the very high accuracy of the property predictor (see Section  B.2 ), there is a small prediction error. We also see a variety of structures that satisfy the objective function well but differ structurally from the best polymer reported in the literature."
        ]
    }
}