{
    "id_table_1": {
        "caption": "Table 1:  Performance of  SurVerify  on ACS_Income dataset with tolerance parameter   = 0.1 italic- 0.1 \\epsilon=0.1 italic_ = 0.1 .",
        "table": "A5.EGx1",
        "footnotes": [],
        "references": [
            "In this paper, we focus on the bounded linear models learned through a linear regression procedure, which is still one of the widely used models for socio-economic data  (Stanley et al.,,  2008 ) . Please refer to Section  2.1  for a more detailed discussion on linear regression models. In this paper, we only consider the linear regression models with bounded coefficients.",
            "1. Task-specific Credibility Testing.  First, we introduce the formulation of task-specific credibility testing of datasets (Section  1.1 ). When survey data is collected with the intended purpose of downstream analysis (e.g. regression, classification), this formulation offers a more appropriate testing criterion to compare the credibility of survey data with respect to validation data. This provides a novel task-specific perspective compared to classical distribution testing problems. It is important to note that the proposed test is not a strictly weaker problem than identity testing as two different distributions can have the same optimal model, e.g. two far-apart distributions can yield the same linear model.",
            "We now discuss the primary contribution of this paper: an efficient algorithmic framework  SurVerify , that verifies whether the linear regression model learned from a survey sample  S S S italic_S  is not significantly far in statistical distance from the optimal model learned from a true distribution  D  superscript D \\mathcal{D}^{*} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  by utilizing sampling access to  D  superscript D \\mathcal{D}^{*} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . The pseudo-code of  SurVerify  is presented in Algorithm  1 .",
            "LASSO-SEN  takes as input a noisy survey data  S D  P subscript S D P S_{DP} italic_S start_POSTSUBSCRIPT italic_D italic_P end_POSTSUBSCRIPT , the corresponding noise covariance matrix   q subscript  q \\mathbf{\\Sigma}_{\\mathbf{q}} bold_ start_POSTSUBSCRIPT bold_q end_POSTSUBSCRIPT , and a norm bound  R R R italic_R  of the regression coefficients. First, it uses the covariates of the noisy survey to compute the design matrix denoted by  1 m   i = 1 m z i  z i   1 m  Z T  Z  1 m superscript subscript i 1 m subscript z i superscript subscript z i top 1 m superscript Z T Z \\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{z}_{i}\\mathbf{z}_{i}^{\\top}\\triangleq\\frac{1}% {m}\\mathbf{Z}^{T}\\mathbf{Z} divide start_ARG 1 end_ARG start_ARG italic_m end_ARG  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT bold_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  divide start_ARG 1 end_ARG start_ARG italic_m end_ARG bold_Z start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_Z , where  Z  R m  d Z superscript R m d \\mathbf{Z}\\in\\mathbb{R}^{m\\times d} bold_Z  blackboard_R start_POSTSUPERSCRIPT italic_m  italic_d end_POSTSUPERSCRIPT  is called the covariate matrix (or data matrix). Due to the noise in the covariates, we calibrate the design matrix further to compute the noisy design matrix   ^  1 m  Z T  Z   q  ^  1 m superscript Z T Z subscript  q \\mathbf{\\hat{\\Gamma}}\\triangleq\\frac{1}{m}\\mathbf{Z}^{T}\\mathbf{Z}-\\mathbf{% \\Sigma}_{\\mathbf{q}} over^ start_ARG bold_ end_ARG  divide start_ARG 1 end_ARG start_ARG italic_m end_ARG bold_Z start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_Z - bold_ start_POSTSUBSCRIPT bold_q end_POSTSUBSCRIPT  (Line  1 ).  This is the main deviation from the classical Lasso . Then, we use the noise covariates and the response variable to compute   ^  1 m   i = 1 m y i  z i  1 m  Z T  Y  bold-^  1 m superscript subscript i 1 m subscript y i subscript z i  1 m superscript Z T Y \\boldsymbol{\\hat{\\gamma}}\\triangleq\\frac{1}{m}\\sum_{i=1}^{m}y_{i}\\mathbf{z}_{i% }\\triangleq\\frac{1}{m}\\mathbf{Z}^{T}\\mathbf{Y} overbold_^ start_ARG bold_italic_ end_ARG  divide start_ARG 1 end_ARG start_ARG italic_m end_ARG  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  divide start_ARG 1 end_ARG start_ARG italic_m end_ARG bold_Z start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_Y  (Line  2 ).   ^ ^  \\mathbf{\\hat{\\Gamma}} over^ start_ARG bold_ end_ARG  and   ^ bold-^  \\boldsymbol{\\hat{\\gamma}} overbold_^ start_ARG bold_italic_ end_ARG  are the unbiased empirical estimates of   x subscript  x \\mathbf{\\Sigma}_{\\mathbf{x}} bold_ start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  and   x    subscript  x superscript  \\mathbf{\\Sigma}_{\\mathbf{x}}\\boldsymbol{\\theta}^{*} bold_ start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , respectively. Now, we plug in these matrices in the Lasso optimization problem and further estimate the regression coefficients as   ^  argmin    1  R 1 2   T   ^      ^ ,    ^  subscript argmin subscript norm  1 R 1 2 superscript  T ^   bold-^   \\hat{\\boldsymbol{\\theta}}\\triangleq\\mathop{\\mathrm{argmin}}_{\\left\\|{% \\boldsymbol{\\theta}}\\right\\|_{1}\\leq R}\\frac{1}{2}\\boldsymbol{\\theta}^{T}% \\mathbf{\\hat{\\Gamma}}\\boldsymbol{\\theta}-\\left\\langle{\\boldsymbol{\\hat{\\gamma}% }},{\\boldsymbol{\\theta}}\\right\\rangle over^ start_ARG bold_italic_ end_ARG  roman_argmin start_POSTSUBSCRIPT  bold_italic_  start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  italic_R end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG bold_italic_ start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT over^ start_ARG bold_ end_ARG bold_italic_ -  overbold_^ start_ARG bold_italic_ end_ARG , bold_italic_  . Note that though we present  LASSO-SEN  in terms of LDP survey data, it works without modification for any setup with noisy covariates.",
            "Under the same premises of Theorem  10 , let us consider that the noise    \\eta italic_  to be generated from a sub-exponential distribution such that  P  [   t ]  exp  (  t c  ) P delimited-[]  t t subscript c  \\mathbb{P}[\\eta\\geq t]\\leq\\exp\\left(-\\frac{t}{c_{\\eta}}\\right) blackboard_P [ italic_  italic_t ]  roman_exp ( - divide start_ARG italic_t end_ARG start_ARG italic_c start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT end_ARG ) , and the survey size  m  max  ( max  ( max  (   ,  2 , c  )  min  (  x ) , 1 )  d  log  d , max  (   ,  2 , c  )  log 3  ( d ) ) m   superscript  2 subscript c  subscript  subscript  x 1 d d   superscript  2 subscript c  superscript 3 d m\\geq\\max\\bigg{(}\\max\\left(\\frac{\\max\\left(\\frac{\\zeta}{\\alpha},\\zeta^{2},c_{% \\eta}\\right)}{\\lambda_{\\min}(\\mathbf{\\Sigma}_{\\mathbf{x}})},1\\right)d\\log{d},% \\max\\left(\\frac{\\zeta}{\\alpha},\\zeta^{2},c_{\\eta}\\right)\\log^{3}(d)\\bigg{)} italic_m  roman_max ( roman_max ( divide start_ARG roman_max ( divide start_ARG italic_ end_ARG start_ARG italic_ end_ARG , italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , italic_c start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ) end_ARG start_ARG italic_ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT ( bold_ start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT ) end_ARG , 1 ) italic_d roman_log italic_d , roman_max ( divide start_ARG italic_ end_ARG start_ARG italic_ end_ARG , italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , italic_c start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ) roman_log start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ( italic_d ) )  Then, if we apply  LASSO-SEN  on an    \\alpha italic_ -LDP version of  S S S italic_S  published by  Priv-n-Pub  to get   ^ ^  \\hat{\\boldsymbol{\\theta}} over^ start_ARG bold_italic_ end_ARG , for some constants  c 1 , c 2 > 0 subscript c 1 subscript c 2 0 c_{1},c_{2}>0 italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT > 0 , we obtain with probability at least  1  d  c 1 1 superscript d subscript c 1 1-d^{-c_{1}} 1 - italic_d start_POSTSUPERSCRIPT - italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ,       ^  2 subscript norm superscript  ^  2 \\left\\|{\\boldsymbol{\\theta}^{*}-\\hat{\\boldsymbol{\\theta}}}\\right\\|_{2}  bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT - over^ start_ARG bold_italic_ end_ARG  start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  is at most:",
            "It is important to note that beyond the direct use case presented here for learning from    \\alpha italic_ -LDP data, Theorem  12  ensures that  LASSO-SEN  works for the general problem of learning regression when the  x x \\mathbf{x} bold_x ,  y y y italic_y ,    \\eta italic_  and  q q \\mathbf{q} bold_q  are generated from subexponential distributions.",
            "Q1: Performance of  SurVerify  on Synthetic and Real-World Datasets.  In Figure  LABEL:Fig:_Model_Distance , we depict how  SurVerify  performs with varying values of  d  i  s  t D   ( f S , f  ) d i s subscript t superscript D subscript f S superscript f dist_{\\mathcal{D}^{*}}(f_{S},f^{*}) italic_d italic_i italic_s italic_t start_POSTSUBSCRIPT caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , italic_f start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  under different values of tolerance parameter   italic- \\epsilon italic_  and confidence   = 0.1  0.1 \\delta=0.1 italic_ = 0.1 . In Table  1 , we show how the rejection rate of  SurVerify  changes with varying number of samples, where  D S subscript D S \\mathcal{D}_{S} caligraphic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  and  D  superscript D \\mathcal{D}^{*} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  denoting the distribution of Female and Male in ACS_Income , respectively. For ACS_Income , we set the tolerance parameter to be   = 0.1 italic- 0.1 \\epsilon=0.1 italic_ = 0.1 .",
            "Observations.  (i)  LASSO-SEN  ensures that the Normalized Estimation Error for the coefficients decrease as per the specified rates of  O  ( 1 / m ) O 1 m O(1/\\sqrt{m}) italic_O ( 1 / square-root start_ARG italic_m end_ARG )  and  O  ( 1 / m ) O 1 m O(1/{m}) italic_O ( 1 / italic_m )  for both    \\alpha italic_  and  (  ,  )   (\\alpha,\\beta) ( italic_ , italic_ ) -LDP data but only after crossing the initial threshold for number of samples. (ii) Normalized Estimation Error for  LASSO-SEN  decays at a slower rate for    \\alpha italic_ -LDP compared to the case for  (  ,  )   (\\alpha,\\beta) ( italic_ , italic_ ) -LDP privacy. (iii) The initial threshold of sample complexity appears to be larger for    \\alpha italic_ -LDP compared to the case for  (  ,  )   (\\alpha,\\beta) ( italic_ , italic_ ) -LDP privacy. These results are reflective of the bounds obtained in Theorem  10  and  11 ."
        ]
    },
    "id_table_2": {
        "caption": "",
        "table": "A5.EGx2",
        "footnotes": [],
        "references": [
            "In this paper, we focus on the bounded linear models learned through a linear regression procedure, which is still one of the widely used models for socio-economic data  (Stanley et al.,,  2008 ) . Please refer to Section  2.1  for a more detailed discussion on linear regression models. In this paper, we only consider the linear regression models with bounded coefficients.",
            "If the loss function  L  ( y , y  ) L y superscript y  \\mathcal{L}(y,y^{\\prime}) caligraphic_L ( italic_y , italic_y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  chosen for the linear regression problem is bounded for all  y , y   R y superscript y  R y,y^{\\prime}\\in\\mathbb{R} italic_y , italic_y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  blackboard_R , it is known as the  bounded linear regression problem . Additionally, Assumption  2  also enforces us to only consider linear models with bounded coefficients.",
            "The core idea of  SurVerify  is that for evaluating the credibility of the survey data  S S S italic_S  we proceed in two phases. In the first phase, (in lines  2  to  4 ) we learn the linear regression model  f S subscript f S f_{S} italic_f start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  which fits the observations in  S S S italic_S . In the second phase, (in lines  5  and  6 ) we evaluate the credibility of  S S S italic_S  by drawing sufficient samples from the true distribution  D  superscript D \\mathcal{D}^{*} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  to obtain an additive estimate of the expected loss of the function  f S subscript f S f_{S} italic_f start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  for samples drawn from  D  superscript D \\mathcal{D}^{*} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . Finally, (in Lines  7  and  8 ) if this estimate differs significantly from the upper bound on the expected loss of  f S subscript f S f_{S} italic_f start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ,  SurVerify  REJECTS the survey data  S S S italic_S  implying the survey set is far from being a credible survey data with respect to the class  F R subscript F R \\mathcal{F}_{R} caligraphic_F start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT . We would also like to emphasize that by a careful treatment of generalization error bound with  l 1 subscript l 1 \\ell_{1} roman_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  geometry, we obtain an upper bound on the expected loss on  S S S italic_S ,   ^ S subscript ^  S \\hat{\\gamma}_{S} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , with  log  ( d ) d \\sqrt{\\log(d)} square-root start_ARG roman_log ( italic_d ) end_ARG  dependence rather than the classical  d d \\sqrt{d} square-root start_ARG italic_d end_ARG  term  (Mohri et al.,,  2018 , Theorem 11.8) .",
            "Now, we discuss our other contribution: a framework ( Priv-n-Pub ) to publish survey data satisfying LDP guarantees, and an algorithm ( LASSO-SEN ) to learn Lasso regression on the privately published survey data. Given survey data satisfying the bounded response variable and covariates assumption (Assumption  2 ),  Priv-n-Pub  publishes private survey data satisfying LDP guarantees and the covariance of added noise.  LASSO-SEN  takes the private survey data and noise covariance as input and outputs an estimate of the linear regression that is close to the true coefficients. The pseudocode of  Priv-n-Pub  and  LASSO-SEN  are presented in Algorithm  2  and  3 , respectively.",
            "Proof Sketch.  The results are direct consequences of Lemma  6  and  7  with the boundedness assumption (Assumption  2 ) guaranteeing that for a function  f  ( x i ) = x i f subscript x i subscript x i f(\\mathbf{x}_{i})=\\mathbf{x}_{i} italic_f ( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  the  l 1 subscript l 1 \\ell_{1} roman_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT -sensitivity is   1  ( f )  2   subscript  1 f 2  \\Delta_{1}(f)\\leq 2\\zeta roman_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_f )  2 italic_ .",
            "LASSO-SEN  takes as input a noisy survey data  S D  P subscript S D P S_{DP} italic_S start_POSTSUBSCRIPT italic_D italic_P end_POSTSUBSCRIPT , the corresponding noise covariance matrix   q subscript  q \\mathbf{\\Sigma}_{\\mathbf{q}} bold_ start_POSTSUBSCRIPT bold_q end_POSTSUBSCRIPT , and a norm bound  R R R italic_R  of the regression coefficients. First, it uses the covariates of the noisy survey to compute the design matrix denoted by  1 m   i = 1 m z i  z i   1 m  Z T  Z  1 m superscript subscript i 1 m subscript z i superscript subscript z i top 1 m superscript Z T Z \\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{z}_{i}\\mathbf{z}_{i}^{\\top}\\triangleq\\frac{1}% {m}\\mathbf{Z}^{T}\\mathbf{Z} divide start_ARG 1 end_ARG start_ARG italic_m end_ARG  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT bold_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  divide start_ARG 1 end_ARG start_ARG italic_m end_ARG bold_Z start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_Z , where  Z  R m  d Z superscript R m d \\mathbf{Z}\\in\\mathbb{R}^{m\\times d} bold_Z  blackboard_R start_POSTSUPERSCRIPT italic_m  italic_d end_POSTSUPERSCRIPT  is called the covariate matrix (or data matrix). Due to the noise in the covariates, we calibrate the design matrix further to compute the noisy design matrix   ^  1 m  Z T  Z   q  ^  1 m superscript Z T Z subscript  q \\mathbf{\\hat{\\Gamma}}\\triangleq\\frac{1}{m}\\mathbf{Z}^{T}\\mathbf{Z}-\\mathbf{% \\Sigma}_{\\mathbf{q}} over^ start_ARG bold_ end_ARG  divide start_ARG 1 end_ARG start_ARG italic_m end_ARG bold_Z start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_Z - bold_ start_POSTSUBSCRIPT bold_q end_POSTSUBSCRIPT  (Line  1 ).  This is the main deviation from the classical Lasso . Then, we use the noise covariates and the response variable to compute   ^  1 m   i = 1 m y i  z i  1 m  Z T  Y  bold-^  1 m superscript subscript i 1 m subscript y i subscript z i  1 m superscript Z T Y \\boldsymbol{\\hat{\\gamma}}\\triangleq\\frac{1}{m}\\sum_{i=1}^{m}y_{i}\\mathbf{z}_{i% }\\triangleq\\frac{1}{m}\\mathbf{Z}^{T}\\mathbf{Y} overbold_^ start_ARG bold_italic_ end_ARG  divide start_ARG 1 end_ARG start_ARG italic_m end_ARG  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  divide start_ARG 1 end_ARG start_ARG italic_m end_ARG bold_Z start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_Y  (Line  2 ).   ^ ^  \\mathbf{\\hat{\\Gamma}} over^ start_ARG bold_ end_ARG  and   ^ bold-^  \\boldsymbol{\\hat{\\gamma}} overbold_^ start_ARG bold_italic_ end_ARG  are the unbiased empirical estimates of   x subscript  x \\mathbf{\\Sigma}_{\\mathbf{x}} bold_ start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  and   x    subscript  x superscript  \\mathbf{\\Sigma}_{\\mathbf{x}}\\boldsymbol{\\theta}^{*} bold_ start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , respectively. Now, we plug in these matrices in the Lasso optimization problem and further estimate the regression coefficients as   ^  argmin    1  R 1 2   T   ^      ^ ,    ^  subscript argmin subscript norm  1 R 1 2 superscript  T ^   bold-^   \\hat{\\boldsymbol{\\theta}}\\triangleq\\mathop{\\mathrm{argmin}}_{\\left\\|{% \\boldsymbol{\\theta}}\\right\\|_{1}\\leq R}\\frac{1}{2}\\boldsymbol{\\theta}^{T}% \\mathbf{\\hat{\\Gamma}}\\boldsymbol{\\theta}-\\left\\langle{\\boldsymbol{\\hat{\\gamma}% }},{\\boldsymbol{\\theta}}\\right\\rangle over^ start_ARG bold_italic_ end_ARG  roman_argmin start_POSTSUBSCRIPT  bold_italic_  start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  italic_R end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG bold_italic_ start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT over^ start_ARG bold_ end_ARG bold_italic_ -  overbold_^ start_ARG bold_italic_ end_ARG , bold_italic_  . Note that though we present  LASSO-SEN  in terms of LDP survey data, it works without modification for any setup with noisy covariates.",
            "It is important to note that beyond the direct use case presented here for learning from    \\alpha italic_ -LDP data, Theorem  12  ensures that  LASSO-SEN  works for the general problem of learning regression when the  x x \\mathbf{x} bold_x ,  y y y italic_y ,    \\eta italic_  and  q q \\mathbf{q} bold_q  are generated from subexponential distributions."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "S5.T1.7",
        "footnotes": [],
        "references": [
            "2. A Generic Algorithm Design.  We develop a generic algorithmic framework  SurVerify  to conduct credibility testing of survey data with bounded variables, and independent, homoscedastic noise (Section  3 ). Specifically, for both public and private surveys,  SurVerify  accepts  D S subscript D S \\mathcal{D}_{S} caligraphic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  to be credible with high probability if  d  i  s  t D   ( f S , f  ) d i s subscript t superscript D subscript f S superscript f dist_{\\mathcal{D}^{*}}(f_{S},f^{*}) italic_d italic_i italic_s italic_t start_POSTSUBSCRIPT caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , italic_f start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  is smaller than a threshold, and rejects only if  d  i  s  t D   ( f S , f  ) d i s subscript t superscript D subscript f S superscript f dist_{\\mathcal{D}^{*}}(f_{S},f^{*}) italic_d italic_i italic_s italic_t start_POSTSUBSCRIPT caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , italic_f start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  is larger than a threshold. Under the stated assumptions, we prove the correctness and sample complexity of  SurVerify  for both public and  (  ,  )   (\\alpha,\\beta) ( italic_ , italic_ ) -local DP surveys (Section  3 ).",
            "Now, we discuss our other contribution: a framework ( Priv-n-Pub ) to publish survey data satisfying LDP guarantees, and an algorithm ( LASSO-SEN ) to learn Lasso regression on the privately published survey data. Given survey data satisfying the bounded response variable and covariates assumption (Assumption  2 ),  Priv-n-Pub  publishes private survey data satisfying LDP guarantees and the covariance of added noise.  LASSO-SEN  takes the private survey data and noise covariance as input and outputs an estimate of the linear regression that is close to the true coefficients. The pseudocode of  Priv-n-Pub  and  LASSO-SEN  are presented in Algorithm  2  and  3 , respectively."
        ]
    }
}