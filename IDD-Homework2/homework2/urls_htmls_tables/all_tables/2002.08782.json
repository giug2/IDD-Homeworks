{
    "PAPER'S NUMBER OF TABLES": 1,
    "S2.T1": {
        "caption": "TABLE I: List of references on the convergence analysis of federated learning under different assumptions. This work is the only one to tackle the 3 challenges of federated learning.",
        "table": "<table id=\"S2.T1.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.4.1.1\" class=\"ltx_tr\" style=\"background-color:#CCCCCC;\">\n<th id=\"S2.T1.4.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#CCCCCC;\">References</span></th>\n<td id=\"S2.T1.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#CCCCCC;\">Algorithm</span></td>\n<td id=\"S2.T1.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.1.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#CCCCCC;\">Function Type</span></td>\n<td id=\"S2.T1.4.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.1.1.4.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#CCCCCC;\">Data Heterogeneity</span></td>\n<td id=\"S2.T1.4.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.1.1.5.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#CCCCCC;\">Operation</span></td>\n<td id=\"S2.T1.4.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.1.1.6.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#CCCCCC;\">Agent Participation</span></td>\n<td id=\"S2.T1.4.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.1.1.7.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#CCCCCC;\">Other Assumptions</span></td>\n</tr>\n<tr id=\"S2.T1.4.2.2\" class=\"ltx_tr\">\n<th id=\"S2.T1.4.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a>]</cite></th>\n<td id=\"S2.T1.4.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">dist. gradient descent</td>\n<td id=\"S2.T1.4.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">convex</td>\n<td id=\"S2.T1.4.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.2.2.4.1\" class=\"ltx_text ltx_font_bold ltx_font_italic\">non-IID</span></td>\n<td id=\"S2.T1.4.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">synchronous</td>\n<td id=\"S2.T1.4.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">full</td>\n<td id=\"S2.T1.4.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">smooth</td>\n</tr>\n<tr id=\"S2.T1.4.3.3\" class=\"ltx_tr\">\n<th id=\"S2.T1.4.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">23</a>]</cite></th>\n<td id=\"S2.T1.4.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">dist. SGD</td>\n<td id=\"S2.T1.4.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">convex</td>\n<td id=\"S2.T1.4.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">IID</td>\n<td id=\"S2.T1.4.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">synchronous</td>\n<td id=\"S2.T1.4.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">full</td>\n<td id=\"S2.T1.4.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">smooth</td>\n</tr>\n<tr id=\"S2.T1.4.4.4\" class=\"ltx_tr\">\n<th id=\"S2.T1.4.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">24</a>, <a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite></th>\n<td id=\"S2.T1.4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">dist. SGD</td>\n<td id=\"S2.T1.4.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">non-convex</td>\n<td id=\"S2.T1.4.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">IID</td>\n<td id=\"S2.T1.4.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">synchronous</td>\n<td id=\"S2.T1.4.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">full</td>\n<td id=\"S2.T1.4.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">smooth</td>\n</tr>\n<tr id=\"S2.T1.4.5.5\" class=\"ltx_tr\">\n<th id=\"S2.T1.4.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a>]</cite></th>\n<td id=\"S2.T1.4.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">dist. SGD</td>\n<td id=\"S2.T1.4.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">non-convex</td>\n<td id=\"S2.T1.4.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<table id=\"S2.T1.4.5.5.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S2.T1.4.5.5.4.1.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.4.5.5.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S2.T1.4.5.5.4.1.1.1.1\" class=\"ltx_text ltx_font_bold ltx_font_italic\">non-IID</span></td>\n</tr>\n<tr id=\"S2.T1.4.5.5.4.1.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.4.5.5.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">IID</td>\n</tr>\n</table>\n</td>\n<td id=\"S2.T1.4.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<table id=\"S2.T1.4.5.5.5.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S2.T1.4.5.5.5.1.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.4.5.5.5.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">synchronous</td>\n</tr>\n<tr id=\"S2.T1.4.5.5.5.1.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.4.5.5.5.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S2.T1.4.5.5.5.1.2.1.1\" class=\"ltx_text ltx_font_bold ltx_font_italic\">asynchronous</span></td>\n</tr>\n</table>\n</td>\n<td id=\"S2.T1.4.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">full</td>\n<td id=\"S2.T1.4.5.5.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-</td>\n</tr>\n<tr id=\"S2.T1.4.6.6\" class=\"ltx_tr\">\n<th id=\"S2.T1.4.6.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">27</a>]</cite></th>\n<td id=\"S2.T1.4.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">dist. SGD</td>\n<td id=\"S2.T1.4.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">convex</td>\n<td id=\"S2.T1.4.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.6.6.4.1\" class=\"ltx_text ltx_font_bold ltx_font_italic\">non-IID</span></td>\n<td id=\"S2.T1.4.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">synchronous</td>\n<td id=\"S2.T1.4.6.6.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">full</td>\n<td id=\"S2.T1.4.6.6.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">bounded gradients</td>\n</tr>\n<tr id=\"S2.T1.4.7.7\" class=\"ltx_tr\">\n<th id=\"S2.T1.4.7.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a>]</cite></th>\n<td id=\"S2.T1.4.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">dist. momentum SGD</td>\n<td id=\"S2.T1.4.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">non-convex</td>\n<td id=\"S2.T1.4.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.7.7.4.1\" class=\"ltx_text ltx_font_bold ltx_font_italic\">non-IID</span></td>\n<td id=\"S2.T1.4.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">synchronous</td>\n<td id=\"S2.T1.4.7.7.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">full</td>\n<td id=\"S2.T1.4.7.7.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-</td>\n</tr>\n<tr id=\"S2.T1.4.8.8\" class=\"ltx_tr\">\n<th id=\"S2.T1.4.8.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">29</a>]</cite></th>\n<td id=\"S2.T1.4.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAvg</td>\n<td id=\"S2.T1.4.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<table id=\"S2.T1.4.8.8.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S2.T1.4.8.8.3.1.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.4.8.8.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">convex</td>\n</tr>\n<tr id=\"S2.T1.4.8.8.3.1.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.4.8.8.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">some non-convex</td>\n</tr>\n</table>\n</td>\n<td id=\"S2.T1.4.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.8.8.4.1\" class=\"ltx_text ltx_font_bold ltx_font_italic\">non-IID</span></td>\n<td id=\"S2.T1.4.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.8.8.5.1\" class=\"ltx_text ltx_font_bold ltx_font_italic\">asynchronous</span></td>\n<td id=\"S2.T1.4.8.8.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">full</td>\n<td id=\"S2.T1.4.8.8.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-</td>\n</tr>\n<tr id=\"S2.T1.4.9.9\" class=\"ltx_tr\">\n<th id=\"S2.T1.4.9.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">30</a>]</cite></th>\n<td id=\"S2.T1.4.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAvg</td>\n<td id=\"S2.T1.4.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">convex</td>\n<td id=\"S2.T1.4.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.9.9.4.1\" class=\"ltx_text ltx_font_bold ltx_font_italic\">non-IID</span></td>\n<td id=\"S2.T1.4.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">synchronous</td>\n<td id=\"S2.T1.4.9.9.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.9.9.6.1\" class=\"ltx_text ltx_font_bold ltx_font_italic\">partial</span></td>\n<td id=\"S2.T1.4.9.9.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">bounded gradients</td>\n</tr>\n<tr id=\"S2.T1.4.10.10\" class=\"ltx_tr\" style=\"background-color:#E0FFFF;\">\n<th id=\"S2.T1.4.10.10.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\"><em id=\"S2.T1.4.10.10.1.1\" class=\"ltx_emph ltx_font_italic\" style=\"background-color:#E0FFFF;\">This work</em></th>\n<td id=\"S2.T1.4.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.10.10.2.1\" class=\"ltx_text\" style=\"background-color:#E0FFFF;\">FedAvg</span></td>\n<td id=\"S2.T1.4.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.10.10.3.1\" class=\"ltx_text\" style=\"background-color:#E0FFFF;\">convex</span></td>\n<td id=\"S2.T1.4.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.10.10.4.1\" class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"background-color:#E0FFFF;\">non-IID</span></td>\n<td id=\"S2.T1.4.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.10.10.5.1\" class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"background-color:#E0FFFF;\">asynchronous</span></td>\n<td id=\"S2.T1.4.10.10.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.10.10.6.1\" class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"background-color:#E0FFFF;\">partial</span></td>\n<td id=\"S2.T1.4.10.10.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S2.T1.4.10.10.7.1\" class=\"ltx_text\" style=\"background-color:#E0FFFF;\">model drift</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Returning to (",
                "1",
                "), in the absence of communication or computational constraints, the centralized gradient descent step takes the form:",
                "where ",
                "i",
                "𝑖",
                "i",
                " is the iteration index. We can distribute the update by splitting the gradient descent step among the ",
                "K",
                "𝐾",
                "K",
                " agents. After introducing local parameters ",
                "𝒘",
                "k",
                ",",
                "i",
                "subscript",
                "𝒘",
                "𝑘",
                "𝑖",
                "\\bm{w}_{k,i}",
                " at each agent, recursion (",
                "3",
                ") becomes:",
                "The drawback of this formulation is that evaluation of ",
                "∇",
                "w",
                "𝖳",
                "P",
                "k",
                "​",
                "(",
                "w",
                "i",
                "−",
                "1",
                ")",
                "subscript",
                "∇",
                "superscript",
                "𝑤",
                "𝖳",
                "subscript",
                "𝑃",
                "𝑘",
                "subscript",
                "𝑤",
                "𝑖",
                "1",
                "\\nabla_{w^{\\mathsf{T}}}P_{k}(w_{i-1})",
                " at each agent ",
                "k",
                "𝑘",
                "k",
                " and for every iteration ",
                "i",
                "𝑖",
                "i",
                " requires a total of ",
                "N",
                "k",
                "subscript",
                "𝑁",
                "𝑘",
                "N_{k}",
                " evaluations of the gradients ",
                "∇",
                "Q",
                "k",
                "​",
                "(",
                "w",
                "i",
                "−",
                "1",
                ";",
                "𝒙",
                "k",
                ",",
                "n",
                ")",
                "∇",
                "subscript",
                "𝑄",
                "𝑘",
                "subscript",
                "𝑤",
                "𝑖",
                "1",
                "subscript",
                "𝒙",
                "𝑘",
                "𝑛",
                "\\nabla Q_{k}(w_{i-1};\\bm{x}_{k,n})",
                ". A popular approach for reducing the per-iteration computational cost is the utilization of stochastic gradient approximations. We will construct the gradient approximation as the average of ",
                "E",
                "k",
                "subscript",
                "𝐸",
                "𝑘",
                "E_{k}",
                " individual mini-batch approximations, each of size ",
                "B",
                "k",
                "subscript",
                "𝐵",
                "𝑘",
                "B_{k}",
                ":",
                "where ",
                "𝓑",
                "k",
                ",",
                "e",
                "subscript",
                "𝓑",
                "𝑘",
                "𝑒",
                "\\mathcal{\\bm{B}}_{k,e}",
                " denotes the ",
                "e",
                "𝑒",
                "e",
                "-th mini-batch set randomly sampled at time ",
                "i",
                "𝑖",
                "i",
                " by agent ",
                "k",
                "𝑘",
                "k",
                ". We sample the indices in ",
                "𝓑",
                "k",
                ",",
                "e",
                "subscript",
                "𝓑",
                "𝑘",
                "𝑒",
                "\\mathcal{\\bm{B}}_{k,e}",
                " from the set of integers ",
                "{",
                "1",
                ",",
                "…",
                ",",
                "N",
                "k",
                "}",
                "1",
                "…",
                "subscript",
                "𝑁",
                "𝑘",
                "\\left\\{1,\\ldots,N_{k}\\right\\}",
                " ",
                "without replacement",
                ". In the above, we are using the boldface notation ",
                "𝒘",
                "i",
                "−",
                "1",
                "subscript",
                "𝒘",
                "𝑖",
                "1",
                "\\bm{w}_{i-1}",
                " to reflect the random nature of the weight iterates. Note that this construction allows for significant heterogeneity in the agents’ computational capabilities. In particular, by choosing ",
                "E",
                "k",
                "subscript",
                "𝐸",
                "𝑘",
                "E_{k}",
                " and ",
                "B",
                "k",
                "subscript",
                "𝐵",
                "𝑘",
                "B_{k}",
                " appropriately, each agent ",
                "k",
                "𝑘",
                "k",
                " is able to contribute to varying degrees, by performing a different number of gradient calculations. Then, the resulting stochastic gradient steps in (",
                "4",
                ")-(",
                "5",
                ") become:",
                "An equivalent way of writing (",
                "7",
                ")–(",
                "8",
                ") is by introducing an inner iteration over ",
                "e",
                "=",
                "0",
                ",",
                "…",
                ",",
                "E",
                "k",
                "−",
                "1",
                "𝑒",
                "0",
                "…",
                "subscript",
                "𝐸",
                "𝑘",
                "1",
                "e=0,\\ldots,E_{k}-1",
                ", initialized at ",
                "𝒘",
                "k",
                ",",
                "−",
                "1",
                "=",
                "𝒘",
                "i",
                "−",
                "1",
                "subscript",
                "𝒘",
                "𝑘",
                "1",
                "subscript",
                "𝒘",
                "𝑖",
                "1",
                "\\bm{w}_{k,-1}=\\bm{w}_{i-1}",
                ":",
                "followed by:",
                "Examination of (",
                "9",
                ") reveals that, while ",
                "𝒘",
                "k",
                ",",
                "e",
                "subscript",
                "𝒘",
                "𝑘",
                "𝑒",
                "\\bm{w}_{k,e}",
                " evolves with ",
                "e",
                "𝑒",
                "e",
                ", all gradients ",
                "∇",
                "w",
                "𝖳",
                "Q",
                "k",
                "​",
                "(",
                "𝒘",
                "i",
                "−",
                "1",
                ";",
                "𝒙",
                "k",
                ",",
                "b",
                ")",
                "subscript",
                "∇",
                "superscript",
                "𝑤",
                "𝖳",
                "subscript",
                "𝑄",
                "𝑘",
                "subscript",
                "𝒘",
                "𝑖",
                "1",
                "subscript",
                "𝒙",
                "𝑘",
                "𝑏",
                "\\nabla_{w^{\\mathsf{T}}}Q_{k}(\\bm{w}_{i-1};\\bm{x}_{k,b})",
                " are evaluated at ",
                "𝒘",
                "k",
                ",",
                "−",
                "1",
                "=",
                "𝒘",
                "i",
                "−",
                "1",
                "subscript",
                "𝒘",
                "𝑘",
                "1",
                "subscript",
                "𝒘",
                "𝑖",
                "1",
                "\\bm{w}_{k,-1}=\\bm{w}_{i-1}",
                ", which is the starting point of the inner loop. We can instead appeal to an incremental arguement ",
                "[",
                "3",
                "]",
                " and replace (",
                "9",
                ") by:",
                "where now all gradients ",
                "∇",
                "w",
                "𝖳",
                "Q",
                "k",
                "​",
                "(",
                "𝒘",
                "k",
                ",",
                "e",
                "−",
                "1",
                ";",
                "𝒙",
                "b",
                ")",
                "subscript",
                "∇",
                "superscript",
                "𝑤",
                "𝖳",
                "subscript",
                "𝑄",
                "𝑘",
                "subscript",
                "𝒘",
                "𝑘",
                "𝑒",
                "1",
                "subscript",
                "𝒙",
                "𝑏",
                "\\nabla_{w^{\\mathsf{T}}}Q_{k}(\\bm{w}_{k,e-1};\\bm{x}_{b})",
                " are evaluated at the latest estimate ",
                "𝒘",
                "k",
                ",",
                "e",
                "−",
                "1",
                "subscript",
                "𝒘",
                "𝑘",
                "𝑒",
                "1",
                "\\bm{w}_{k,e-1}",
                " at agent ",
                "k",
                "𝑘",
                "k",
                ". Since full participation of all ",
                "K",
                "𝐾",
                "K",
                " agents at every time instant ",
                "i",
                "𝑖",
                "i",
                " is generally infeasible in a federated learning scenario ",
                "[",
                "8",
                "]",
                ", we allow for a participation of only ",
                "L",
                "𝐿",
                "L",
                " agents at every iteration, and sample the set of indices of participating agents ",
                "ℒ",
                "i",
                "subscript",
                "ℒ",
                "𝑖",
                "\\mathcal{L}_{i}",
                " from ",
                "{",
                "1",
                ",",
                "…",
                ",",
                "L",
                "}",
                "1",
                "…",
                "𝐿",
                "\\{1,\\ldots,L\\}",
                " without replacement, transforming the combination step (",
                "10",
                ") to:",
                "With the local incremental update step (",
                "11",
                ") and partial-participation combination step (",
                "12",
                ") we arrive at Algorithm ",
                "1",
                ".",
                "This algorithm bears similarity to the original FedAvg ",
                "[",
                "8",
                "]",
                " algorithm, but differs in the fact that we allow for varying local epoch sizes ",
                "E",
                "k",
                "subscript",
                "𝐸",
                "𝑘",
                "E_{k}",
                " and the normalization of gradient directions by epoch size. The advantage of this construction is in its applicability to settings where agents have varying capabilities so that some agents are able to run multiple epochs while others are not. Instead of forcing capable agents to only run a single epoch, not taking advantage of their computational resources, or forcing slow agents to perform multiple epochs, risking a straggler effect, this model allows every agent to contribute precisely as much as they are able to. The normalization of gradients by the number of local epochs ",
                "E",
                "k",
                "subscript",
                "𝐸",
                "𝑘",
                "E_{k}",
                ", as our analysis will show, is necessary to ensure an unbiased solution despite asymmetric agent contribution, essentially by ensuring that (",
                "6",
                ") is an unbiased estimate of the gradient of ",
                "P",
                "k",
                "​",
                "(",
                "⋅",
                ")",
                "subscript",
                "𝑃",
                "𝑘",
                "⋅",
                "P_{k}(\\cdot)",
                ". In the absence of step-size normalization, agents with more participation would be able to bias the limiting point of the algorithm towards their own local minimizers."
            ]
        ]
    }
}