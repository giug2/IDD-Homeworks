{
    "S4.T1": {
        "caption": "Table 1: Accuracy of the baseline and our proposed model finetuned with reasoning supervision on VCR.",
        "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">Model</th>\n<th id=\"S4.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Acc(%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.3.1.1.1\" class=\"ltx_text ltx_font_italic\">Baseline model</span></th>\n<td id=\"S4.T1.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">61.2</td>\n</tr>\n<tr id=\"S4.T1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"S4.T1.1.1.2.1\" class=\"ltx_text ltx_font_italic\">Reasoning Supervision</span></th>\n<td id=\"S4.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_bb\"><math id=\"S4.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mathbf{63.9}\" display=\"inline\"><semantics id=\"S4.T1.1.1.1.m1.1a\"><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\" id=\"S4.T1.1.1.1.m1.1.1\" xref=\"S4.T1.1.1.1.m1.1.1.cmml\">63.9</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.1.1.1.m1.1b\"><cn type=\"float\" id=\"S4.T1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.1.1.1.m1.1.1\">63.9</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.m1.1c\">\\mathbf{63.9}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Quantitative Evaluation.\nResults in terms of model accuracy are reported in Table 1. Our baseline model (only the question decoder) achieves 61.2%percent61.261.2\\% accuracy on the validation set. Finetuning by aligning question and reasoning attention distributions yields 63.9%percent63.963.9\\%, that is a 2.7%percent2.72.7\\% absolute improvement, thus demonstrating the benefit of reasoning supervision. We note that our main goal is to propose a novel training strategy for boosting a VQA model’s visual explanatory strength by exploiting reasoning as an alternative supervisory signal. Thus, we do not directly compare to methods such as [2, 3, 20] that contain a larger number of parameters, leverage large-scale VL and video pretraining or ground-truth object bounding boxes. For comparison, the best performance reported in R2C [15] was 63.8%percent63.863.8\\%."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Performance drop on the VCR validation set due to object masking.",
        "table": "<table id=\"S4.T2.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\">Model</th>\n<th id=\"S4.T2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">Acc(%)</th>\n<th id=\"S4.T2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">(<math id=\"S4.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"+\" display=\"inline\"><semantics id=\"S4.T2.1.1.1.m1.1a\"><mo id=\"S4.T2.1.1.1.m1.1.1\" xref=\"S4.T2.1.1.1.m1.1.1.cmml\">+</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.1.1.1.m1.1b\"><plus id=\"S4.T2.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.1.1.1.m1.1.1\"></plus></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.1.1.1.m1.1c\">+</annotation></semantics></math><span id=\"S4.T2.1.1.1.1\" class=\"ltx_text ltx_font_italic\">masking</span>) Acc(%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S4.T2.2.2.2.1\" class=\"ltx_text ltx_font_italic\">Baseline model</span></td>\n<td id=\"S4.T2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">61.2</td>\n<td id=\"S4.T2.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">59.3 <math id=\"S4.T2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"(-1.9)\" display=\"inline\"><semantics id=\"S4.T2.2.2.1.m1.1a\"><mrow id=\"S4.T2.2.2.1.m1.1.1.1\" xref=\"S4.T2.2.2.1.m1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S4.T2.2.2.1.m1.1.1.1.2\" xref=\"S4.T2.2.2.1.m1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.T2.2.2.1.m1.1.1.1.1\" xref=\"S4.T2.2.2.1.m1.1.1.1.1.cmml\"><mo id=\"S4.T2.2.2.1.m1.1.1.1.1a\" xref=\"S4.T2.2.2.1.m1.1.1.1.1.cmml\">−</mo><mn id=\"S4.T2.2.2.1.m1.1.1.1.1.2\" xref=\"S4.T2.2.2.1.m1.1.1.1.1.2.cmml\">1.9</mn></mrow><mo stretchy=\"false\" id=\"S4.T2.2.2.1.m1.1.1.1.3\" xref=\"S4.T2.2.2.1.m1.1.1.1.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.2.2.1.m1.1b\"><apply id=\"S4.T2.2.2.1.m1.1.1.1.1.cmml\" xref=\"S4.T2.2.2.1.m1.1.1.1\"><minus id=\"S4.T2.2.2.1.m1.1.1.1.1.1.cmml\" xref=\"S4.T2.2.2.1.m1.1.1.1\"></minus><cn type=\"float\" id=\"S4.T2.2.2.1.m1.1.1.1.1.2.cmml\" xref=\"S4.T2.2.2.1.m1.1.1.1.1.2\">1.9</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.2.2.1.m1.1c\">(-1.9)</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S4.T2.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span id=\"S4.T2.3.3.2.1\" class=\"ltx_text ltx_font_italic\">Reasoning Supervision</span></td>\n<td id=\"S4.T2.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">63.9</td>\n<td id=\"S4.T2.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">61.1 <math id=\"S4.T2.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"(-2.8)\" display=\"inline\"><semantics id=\"S4.T2.3.3.1.m1.1a\"><mrow id=\"S4.T2.3.3.1.m1.1.1.1\" xref=\"S4.T2.3.3.1.m1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S4.T2.3.3.1.m1.1.1.1.2\" xref=\"S4.T2.3.3.1.m1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.T2.3.3.1.m1.1.1.1.1\" xref=\"S4.T2.3.3.1.m1.1.1.1.1.cmml\"><mo id=\"S4.T2.3.3.1.m1.1.1.1.1a\" xref=\"S4.T2.3.3.1.m1.1.1.1.1.cmml\">−</mo><mn id=\"S4.T2.3.3.1.m1.1.1.1.1.2\" xref=\"S4.T2.3.3.1.m1.1.1.1.1.2.cmml\">2.8</mn></mrow><mo stretchy=\"false\" id=\"S4.T2.3.3.1.m1.1.1.1.3\" xref=\"S4.T2.3.3.1.m1.1.1.1.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.3.3.1.m1.1b\"><apply id=\"S4.T2.3.3.1.m1.1.1.1.1.cmml\" xref=\"S4.T2.3.3.1.m1.1.1.1\"><minus id=\"S4.T2.3.3.1.m1.1.1.1.1.1.cmml\" xref=\"S4.T2.3.3.1.m1.1.1.1\"></minus><cn type=\"float\" id=\"S4.T2.3.3.1.m1.1.1.1.1.2.cmml\" xref=\"S4.T2.3.3.1.m1.1.1.1.1.2\">2.8</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.3.3.1.m1.1c\">(-2.8)</annotation></semantics></math>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "To further investigate our model’s ability to leverage the visual modality, we perform an ablation study where we mask the visual features of the objects/people referenced by the question at test time and measure the effect on accuracy. Results are reported in Table 2. We observe that the baseline VQA model (that does not fully alleviate the lack of visual grounding) suffers a lesser performance degradation of 1.9%percent1.91.9\\% compared to 2.8%percent2.82.8\\% for our finetuned model (on reasoning supervision). This is a different manifestation of the fact, that the baseline model is over-reliant on the language modality, thus performance is penalized less when visual information is not available due to object masking."
        ]
    }
}