{
    "PAPER'S NUMBER OF TABLES": 1,
    "S4.T1": {
        "caption": "Table 1: Variational inference schemes encompassed by the PVI framework. (See next page.)\nSelected past work has been organized into four categories: global VI (PVI with M=1𝑀1M=1), fully local PVI (M=N𝑀𝑁M=N), Power EP variants, and online VI. The citation to the work is provided along with the granularity of the method (global indicates M=1𝑀1M=1, fully local M=N𝑀𝑁M=N, local implies general M𝑀M can be used).\nThe optimization used from the PVI perspective on this work is noted. Abbreviations used here are: Conjugate Gradient (CG) and Monte Carlo (MC). The model class that the scheme encompasses is noted (conjugate versus non-conjugate) along with the specific models that the scheme was tested on. Model abbreviations are: Non-linear State-space Model (NSSM), Non-linear Factor Analysis (NFA), Latent Dirichlet Allocation (LDA), Poisson Mixed Model (PMM), Heteroscedastic Linear Regression (HLR), Sparse Gaussian Processes (SGPs), Graphical Model (GM), Logistic Regression (LR), Beta-binomial (BB), Stochastic Volatility model (SV), Probit Regression (PR), Multinomial Regression (MR), Bayesian Neural Network (BNN), Gamma factor model (GFM), Poisson Gamma Matrix Factorization (PGMF), Mixture of Gaussians (MoG). Poisson Mixed Model (PMM), Heteroscedastic Linear Regression (HLR), Gaussian Latent Variable (GLV).\nIf the scheme proposed by the method has a name, this is noted in the final column. Abbreviations of the inference scheme are: Automatic Differentiation VI (ADVI), Incremental VI (IVI), Non-conjugate Variational Message Passing (NC-VMP), Simplified NC-VMP (SNC-VMP), Conjugate-Computation VI (CCVI), Power EP (PEP), Alpha-divergence PEP (ADPEP), Convergent Power EP (CPEP), Stochastic Power EP (SPEP), Variational Continual Learning (VCL), Bayesian Gradient Descent (BGD). ",
        "table": "",
        "footnotes": "",
        "references": [
            [
                "The convergent distributed Power EP approach of ",
                "Hasenclever et al. (",
                "2017",
                ")",
                " recovers a version of PVI as ",
                "α",
                "→",
                "0",
                "→",
                "𝛼",
                "0",
                "\\alpha\\rightarrow 0",
                " with convergence guarantees. The PVI approach is also similar in spirit to ",
                "Gelman et al. (",
                "2014",
                "); Hasenclever et al. (",
                "2017",
                ")",
                " who use EP to split up data sets into small parts that are amenable to MCMC. Here we are using PVI to split up data sets so that they are amenable for optimization."
            ]
        ]
    }
}