{
    "S5.T1": {
        "caption": "TABLE I: Default top-10 workflows and their frequency in the top-5 positions.",
        "table": "<table id=\"S5.T1.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.3.4.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.4.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">1</th>\n<td id=\"S5.T1.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S5.T1.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3</td>\n<td id=\"S5.T1.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4</td>\n<td id=\"S5.T1.3.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5</td>\n<td id=\"S5.T1.3.4.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">6</td>\n<td id=\"S5.T1.3.4.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">7</td>\n<td id=\"S5.T1.3.4.1.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8</td>\n<td id=\"S5.T1.3.4.1.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">9</td>\n<td id=\"S5.T1.3.4.1.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10</td>\n</tr>\n<tr id=\"S5.T1.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.3.4.1\" class=\"ltx_text ltx_font_italic\">LR</span></th>\n<td id=\"S5.T1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.3.3.5.1\" class=\"ltx_text ltx_font_italic\">IG</span>+<span id=\"S5.T1.3.3.5.2\" class=\"ltx_text ltx_font_italic\">LR</span>\n</td>\n<td id=\"S5.T1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.3.3.6.1\" class=\"ltx_text ltx_font_italic\">RF</span>+<span id=\"S5.T1.3.3.6.2\" class=\"ltx_text ltx_font_italic\">LR</span>\n</td>\n<td id=\"S5.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.1.1.1.2\" class=\"ltx_text ltx_font_italic\">SVMRFE</span>+<span id=\"S5.T1.1.1.1.1\" class=\"ltx_text ltx_font_italic\">SVM<sub id=\"S5.T1.1.1.1.1.1\" class=\"ltx_sub\">l</sub></span>\n</td>\n<td id=\"S5.T1.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.3.3.7.1\" class=\"ltx_text ltx_font_italic\">SVMRFE</span>+<span id=\"S5.T1.3.3.7.2\" class=\"ltx_text ltx_font_italic\">LR</span>\n</td>\n<td id=\"S5.T1.3.3.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.3.3.8.1\" class=\"ltx_text ltx_font_italic\">IG</span>+<span id=\"S5.T1.3.3.8.2\" class=\"ltx_text ltx_font_italic\">NBN</span>\n</td>\n<td id=\"S5.T1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.2.2.2.2\" class=\"ltx_text ltx_font_italic\">IG</span>+<span id=\"S5.T1.2.2.2.1\" class=\"ltx_text ltx_font_italic\">SVM<sub id=\"S5.T1.2.2.2.1.1\" class=\"ltx_sub\">l</sub></span>\n</td>\n<td id=\"S5.T1.3.3.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.3.3.9.1\" class=\"ltx_text ltx_font_italic\">CHI</span>+<span id=\"S5.T1.3.3.9.2\" class=\"ltx_text ltx_font_italic\">NBN</span>\n</td>\n<td id=\"S5.T1.3.3.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.3.3.10.1\" class=\"ltx_text ltx_font_italic\">SVMRFE</span>+<span id=\"S5.T1.3.3.10.2\" class=\"ltx_text ltx_font_italic\">NBN</span>\n</td>\n<td id=\"S5.T1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.3.3.3.2\" class=\"ltx_text ltx_font_italic\">RF</span>+<span id=\"S5.T1.3.3.3.1\" class=\"ltx_text ltx_font_italic\">SVM<sub id=\"S5.T1.3.3.3.1.1\" class=\"ltx_sub\">l</sub></span>\n</td>\n</tr>\n<tr id=\"S5.T1.3.5.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.5.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">25</th>\n<td id=\"S5.T1.3.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">12</td>\n<td id=\"S5.T1.3.5.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">13</td>\n<td id=\"S5.T1.3.5.2.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">16</td>\n<td id=\"S5.T1.3.5.2.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">13</td>\n<td id=\"S5.T1.3.5.2.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">17</td>\n<td id=\"S5.T1.3.5.2.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">14</td>\n<td id=\"S5.T1.3.5.2.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">10</td>\n<td id=\"S5.T1.3.5.2.9\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">13</td>\n<td id=\"S5.T1.3.5.2.10\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">8</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "In order to meta-mine we first need to perform a set of base-level experiments over which we will construct our meta-mining models. To do so we used\n65 real world cancer microarray datasets, most of them were taken from the National Center for Biotechnology InformationÂ 222http://www.ncbi.nlm.nih.gov/. \nMicroarray datasets are characterized by a high-dimensionality and a small sample size, and a relatively low number of classes, most often two.\nThese datasets have an average of 79.26 instances, 15268.57 attributes, and 2.33 classes.\n\nOn these datasets we applied a total of 35 classification data mining workflows;\n28 of them were workflows that contained one feature selection and one classification algorithm, while the seven remaining ones had only a single\nclassification algorithm. We used the four following feature selection algorithms:\nInformation Gain, IG,\nChi-square, CHI,\nReliefFÂ [20], RF,\nand recursive feature elimination with SVMÂ [21], SVMRFE,\nand fixed the number of selected features to ten. For classification we used the seven following algorithms:\none-nearest-neighbor, 1NN,\nthe C4.5Â [23] and CARTÂ [24] decision tree algorithms,\na Naive Bayes algorithm with normal probability estimation, NBN,\na logistic regression algorithm, LR,\nand SVMÂ [19] with the linear, SVMl\nand the rbf, SVMr,\nkernels. We used the implementations of these algorithms provided by the RapidMiner data mining suite with their default parameters.\nOverall we had a total of 65Ã—(28+7)=227565287227565\\times(28+7)=2275 base-level DM experiments, i.e. applications of these workflows on the datasets.\nTo construct the ğ‘ğ‘\\mathbf{R} preference matrix we estimated the performance of the workflows using 10-fold cross-validation and applied\nthe scoring McNemar based scoring schema described in sectionÂ II.\nIn tableÂ I we give for each of the ten top workflows over the full set of 65 datasets\nthe number of times that these were ranked in the top five positions."
        ]
    },
    "S5.T2": {
        "caption": "TABLE II: Evaluation results. Î´ğ›¿\\delta and Î´Eâ€‹Csubscriptğ›¿ğ¸ğ¶\\delta_{EC} denote comparison results with the default (def) and the Euclidean\nbaseline strategy (EC) respectively. ÏğœŒ\\rho is the Spearmanâ€™s rank correlation coefficient, in t5p we give\nthe average accuracy of the top five workflows proposed by each strategy, and mae is the mean average error. X/Y indicates the\nnumber of times X that a method was better overall the experiments Y than the default or the baseline\nstrategy.",
        "table": null,
        "footnotes": [],
        "references": [
            "We will now take a close look on the experimental results for the different meta-mining tasks\nand objective functions that we have presented to address them. The full results are given in\nTable II.",
            "Learning algorithm preferences is the most popular formulation in the traditional stream of meta-learning. There\ngiven a dataset description we seek to identify the algorithm that will most probably deliver the best results for\nthe given dataset. In that sense this meta-mining task is the most similar to the typical meta-learning task. We have presented\nthree different objective functions that can be used to address this problem. F1subscriptğ¹1F_{1}, optimization problemÂ (1),\nmakes use of only the dataset descriptors and learns a similarity measure in that space that best approximates\ntheir similarity with respect to their relative workflow preference vectors. In traditional meta-learning this similarity\nis computed directly in the dataset space, it is not learned, and most importantly it does not try to model the relative\nworkflow preference vector, [11, 25]. In our experimental setting the strategy that implements this\ntraditional meta-learning approach is the Euclidean distance-based dataset similarity, EC. In addition to the homogeneous metric learning\napproach we can also use the two heterogeneous metric learning variants to provide the workflow preferences. The simplest\none, corresponding to the optimization function F3subscriptğ¹3F_{3}, optimization problemÂ III-C, uses both dataset\nand workflow characteristics and tries to directly approximate the relative preference matrix. However this approach\nignores the fact that the learned metric should reflect two basic meta-mining requirements, that similar datasets should have\nsimilar workflow preferences, and that similar workflows should have similar dataset preferences. The optimization\nfunction F4subscriptğ¹4F_{4}, optimization problemÂ III-C, reflects exactly this bias by regularizing appropriately\nthe learned metrics in the dataset and workflow spaces so that they reflect well the similarities of the respective preference\nvectors. Before discussing the actual results, given in the left table of TableÂ II, we give the parameter settings for\nthe different variants. F1subscriptğ¹1F_{1}: Î¼1=0.5subscriptğœ‡10.5\\mu_{1}=0.5, Nxn=5subscriptğ‘subscriptğ‘¥ğ‘›5N_{x_{n}}=5; F3subscriptğ¹3F_{3}: Î¼1=Î¼2=0.5subscriptğœ‡1subscriptğœ‡20.5\\mu_{1}=\\mu_{2}=0.5; F4subscriptğ¹4F_{4}: Î±=1â€‹eâˆ’10ğ›¼1superscriptğ‘’10\\alpha=1e^{-10},\nÎ²=1â€‹eâˆ’3ğ›½1superscriptğ‘’3\\beta=1e^{-3}, Î³=1â€‹eâˆ’3ğ›¾1superscriptğ‘’3\\gamma=1e^{-3}, Î¼1=10subscriptğœ‡110\\mu_{1}=10, Î¼2=0subscriptğœ‡20\\mu_{2}=0.\n\nThese parameters reflect what we think are appropriate choices based on our prior knowledge\nof the meta-mining problem. Better results would have been obtained if we had tuned, at least\nsome of them, via inner cross validation.",
            "The goal of this meta-mining task is given a new workflow and a collection of\ndatasets to provide a dataset preference vector that will reflect the order of appropriateness of the datasets for the given workflow.\nAs already mentioned the default strategy provides here a vector of equal ranks thus we cannot compute its Spermanâ€™s rank correlation\ncoefficient.\n\nWe will compare the performance of the F2subscriptğ¹2F_{2} objective function that makes use of only of the workflow descriptors when\nit tries to approximate the similarity of the dataset preference vectors, and these of F3subscriptğ¹3F_{3} and F4subscriptğ¹4F_{4}.\nWe used the following parameter: F2subscriptğ¹2F_{2}: Î¼1=10subscriptğœ‡110\\mu_{1}=10, Nan=5subscriptğ‘subscriptğ‘ğ‘›5N_{a_{n}}=5; F3subscriptğ¹3F_{3}: Î¼1=Î¼2=10subscriptğœ‡1subscriptğœ‡210\\mu_{1}=\\mu_{2}=10;\nF4subscriptğ¹4F_{4}: Î±=1â€‹eâˆ’10ğ›¼1superscriptğ‘’10\\alpha=1e^{-10}, Î²=1â€‹eâˆ’3ğ›½1superscriptğ‘’3\\beta=1e^{-3}, Î³=1â€‹eâˆ’3ğ›¾1superscriptğ‘’3\\gamma=1e^{-3}, Î¼1=0.5subscriptğœ‡10.5\\mu_{1}=0.5, Î¼2=0subscriptğœ‡20\\mu_{2}=0.\nLooking at the results, middle table of TableÂ II, we see that when it comes to the mean average error,\nall methods achieve a performance that is statistically significant better than that of the default strategy, suggesting\nthat this meta-mining task is probably easier than the first one. This makes sense since it is easier to describe\na workflow similarity in terms of the concepts that these workflows use, than what it is to describe a dataset similarity\nin terms of the datasets characteristics. Neither F3subscriptğ¹3F_{3} nor F4subscriptğ¹4F_{4} have a mae performance that is statistically significant\nbetter than the Euclidean baseline. Nevertheless F4subscriptğ¹4F_{4} is statistically significant better than the Euclidean when it comes\nto the Spermanâ€™s rank correlation coefficient. Thus for this meta-mining task there is also evidence that we should take\na more global approach by accounting for all the different constraints on the dataset and workflow metrics as F4subscriptğ¹4F_{4} does."
        ]
    }
}