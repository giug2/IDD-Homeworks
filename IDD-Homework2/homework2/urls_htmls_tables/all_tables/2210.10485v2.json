{
    "S4.T1": {
        "caption": "Table 1: Results of adversarial robustness for 5-way 5-shot classification tasks on unseen and seen domains. All adversarial meta-learning methods are trained on CIFAR-FS or Mini-ImageNet. CML stands for the clean meta-learning. AML stands for adversarial meta-learning. Rob. stands for accuracy (%) calculated with PGD-20 attack (Ïµ=8./255.\\epsilon=8./255., Î³=Ïµ/10ğ›¾italic-Ïµ10\\gamma=\\epsilon/10). Bold and underline stands for the best and second.",
        "table": null,
        "footnotes": [],
        "references": [
            "Despite their successes, we find that the previous AML approaches (Figure 1) are only effective in achieving adversarial robustness from seen domain tasks (e.g., CIFAR-FS, Mini-ImageNet), while showing poor transferable robustness to unseen domains (e.g., Tiered-ImageNet, CUB, Flower, Cars) as shown in TableÂ 1. While the ultimate goal of meta-learning is obtaining transferable performance across various domainÂ (Guo etÂ al., 2020; Oh etÂ al., 2022), which is a common occurrence in real-world, to the best of our knowledge, no research has yet targeted generalizable adversarial robustness in few-shot classification on unseen domains, leaving the problem largely unexplored.",
            "Given that our main goal is to attain transferable robustness on tasks from unseen domains, we mainly validate our method on unseen domain few-shot classification tasks. We meta-train MAVRL on CIFAR-FS (or Mini-ImageNet) and meta-test it on other benchmark datasets from different domains such as Mini-ImageNet (or CIFAR-FS), Tiered-ImageNet, CUB, Flower, and Cars. As shown in TableÂ 1, MAVRL achieves impressive transferable robustness on unseen domain tasks, while previous AML methods easily break down to adversarial attacks from the unseen domains. In particular, MAVRL outperforms the baselines by more than 10% in robust accuracy even though the distribution of the unseen domains (i.e., CUB, Flower, and Cars) which are different from the distribution of the meta-trained dataset.",
            "To verify whether the model can capture distinctive visual features in any unseen domain, we visualize the representation space of an unseen domain, CIFAR-10, using t-SNEÂ (VanÂ der Maaten & Hinton, 2008). FigureÂ 3 shows that MAVRL is able to obtain a well-separated feature space for adversarial examples in this novel domain. In contrast, AQ presents a substantially overlapped feature space across adversarial instances belonging to different classes, indicated by red dots scattered on diverse clusters. This observation suggests that the superior adversarial robustness of MAVRL in unseen domains in TableÂ 1 mainly stems from its ability to extract robust visual features from the input of any domain through the proposed multi-view meta-adversarial representation learning."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Results of ablation experiments of naÃ¯ve combination of previous meta-learning, self-supervised learning (SSL), and adversarial training approaches. All adversarial meta-learning methods are trained on CIFAR-FS. Rob. stands for accuracy (%) calculated with PGD-20 attack (Ïµ=8./255.\\epsilon=8./255., Î³=Ïµ/10ğ›¾italic-Ïµ10\\gamma=\\epsilon/10).",
        "table": null,
        "footnotes": [],
        "references": [
            "Our motivation is derived from self-supervised learning (SSL) that learns visual representation wherein augmented images coexist within the same latent space, which is label-free and effective in yielding transferable representation. One of the straightforward adaptations of SSL and AML is applying SSL-based meta-learningÂ (Liu etÂ al., 2021; Zhou etÂ al., 2023) on the AML. However, simple employment of SSL could not contribute to the transferable adversarial robustness contrary to the success in achieving transferable clean performance in TableÂ 3. Another trivial combination to obtain transferable robustness is adopting the self-supervised adversarial attackÂ (Kim etÂ al., 2020) on the query set ğ’¬ğ’¬\\mathcal{Q} (FigureÂ 1 (b)) as follows:",
            "where t1â€‹(â‹…),t2â€‹(â‹…)subscriptğ‘¡1â‹…subscriptğ‘¡2â‹…t_{1}(\\cdot),t_{2}(\\cdot) are two randomly selected data augmentations to a given batch {x}ğ‘¥\\{x\\} and define xpossubscriptğ‘¥posx_{\\texttt{pos}} of t1â€‹(x)subscriptğ‘¡1ğ‘¥t_{1}(x) as t2â€‹(x)subscriptğ‘¡2ğ‘¥t_{2}(x). The remaining instances in the batch {x}ğ‘¥\\{x\\} are then defined as {xneg}subscriptğ‘¥neg\\{x_{\\texttt{neg}}\\}. z,zposğ‘§subscriptğ‘§posz,z_{\\texttt{pos}}, and {zneg}subscriptğ‘§neg\\{z_{\\texttt{neg}}\\} are latent vectors obtained from the feature encoder fÎ¸â€‹(â‹…)subscriptğ‘“ğœƒâ‹…f_{\\theta}(\\cdot). The simâ€‹(â‹…,â‹…)simâ‹…â‹…\\text{sim}(\\cdot,\\cdot) and Tğ‘‡T are cosine similarity function and a temperature term, respectively. However, the trivial combination of self-supervised adversarial attack to AML could not ensure the transferable adversarial robustness of meta-learners, as shown in TableÂ 3. We attribute the failure of simple modification to the well-known problem of the contrastive objective in a small batch, representational collapseÂ (Chen & He, 2021; Zbontar etÂ al., 2021) of adversarial examples: models trivially produce similar or even identical representations for different adversarial examples, especially when using small batch sizes in self-supervised adversarial attack. Conventional few-shot learning settings (e.g., |ğ’®|=5Ã—5,|ğ’¬|=5Ã—15formulae-sequenceğ’®55ğ’¬515|\\mathcal{S}|=5\\times 5,|\\mathcal{Q}|=5\\times 15), by their own definition, suffer from the severe representational collapse between different adversarial examples, leading adversaries to be ineffective. This hinders meta-learners from achieving both generalized adversarial robustness and clean performance in unseen domains.",
            "Our method provides a novel attack scheme that maximizes the representational discrepancy across the views, along with a consistency-based robust representation learning scheme, which is not a mere combination of adversarial learning, SSL, and meta-learning. A naÃ¯ve combination of these strategies does not yield the same level of transferable adversarial robustness, as evidenced in TableÂ 3. Combining meta-learning and adversarial trainingÂ (Madry etÂ al., 2018) fails to provide transferable performance in both clean and robust settings, as shown in previous AML approaches. While combining self-supervised learning-based meta-learningÂ (Liu etÂ al., 2021; Zhou etÂ al., 2023) with class-wise adversarial training grants transferable clean performance, it fails to obtain adversarial robustness. Additionally, employing self-supervised adversarial trainingÂ (Kim etÂ al., 2020) within meta-learning forces a compromise on clean performance for transferable adversarial robustness. Contrarily, our bootstrapped multi-view representation learning successfully delivers exceptional clean and adversarial robustness in unseen domains."
        ]
    },
    "A3.T6": {
        "caption": "Table 6: Results of adversarial robustness for 5-way 5-shot classification tasks on unseen and seen domains. All adversarial meta-learning methods are trained on CIFAR-FS. Rob. stands for accuracy (%) calculated with PGD-20 attack (Ïµ=8./255.\\epsilon=8./255., Î³=Ïµ/10ğ›¾italic-Ïµ10\\gamma=\\epsilon/10). The ablation condition is as follows: [1]: bootstrap multi-view encoders, [2]: task-agnostic adversarial attack, [3]: cosine distance loss.",
        "table": null,
        "footnotes": [],
        "references": [
            "Recent SSL demonstrates proficiency in acquiring transferable representations by learning view-invariant features by maximizing the similarity between the differently transformed instances of identical origin. This leads SSL to be able to attain strong structural recognitionÂ (Ericsson etÂ al., 2021) based on large-scale data. However, since few-shot learning operates with a limited number of examples, a simple combination of SSL and AML approaches could not achieve adversarially robust representations for unseen domains due to adversarial representational collapse (Figure 1, TableÂ 6).",
            "Many existing works aim at enhancing the adversarial robustness of models trained using supervised learningÂ (Goodfellow etÂ al., 2015; Carlini & Wagner, 2017; Papernot etÂ al., 2016), such as adversarial training (AT) and regularized Kullback-Leibler divergence (KLD) loss. AT uses project gradient descent (PGD)Â (Madry etÂ al., 2018) to maximize loss in inner-maximization loops while minimizing overall loss on adversarial samples. TRADESÂ (Zhang etÂ al., 2019) have theoretically shown that KLD loss enhances robustness by enforcing consistency in predictive distribution between clean and adversarial examples. Transfer learningÂ (Shafahi etÂ al., 2019) can also be used to transfer learned robust representations to new domains with few data. One of the most similar adversarial learning methods to ours is RoCLÂ (Kim etÂ al., 2020), which proposes to adversarially train a robust neural network without labeled data, by instance-wise adversarial attack. However, we found that the simple application of instance-wise attacks on few-shot learning is not effective (TableÂ 6).",
            "As discussed in SectionÂ 3.3, a naive combination of self-supervised learning (SSL) and adversarial meta-learning (AML) fails to achieve transferable robust representation learning. We investigate two cases of this naÃve combination. First, we incorporate task-agnostic instance-wise attacks for adversarial training with a single encoder during the outer optimization phase. We generate adversarial examples following previous worksÂ (Kim etÂ al., 2020) using a single encoder (parameters Î¸Ï„superscriptğœƒğœ\\theta^{\\tau}), as shown in Eq.Â 11. We then minimize the adversarial loss in the outer optimization, as indicated in Eq.Â 12 [1], i.e., â„’ablsubscriptâ„’abl\\mathcal{L}_{\\texttt{abl}}[1]. However, as demonstrated in TableÂ 6, without multi-view encoders, the model fails to generate strong adversarial examples, resulting in insufficient robustness even within the seen domain. Additionally, when we incorporate representation learning loss in the outer optimization using Eq.Â 12 [2], i.e., â„’ablsubscriptâ„’abl\\mathcal{L}_{\\texttt{abl}}[2]. the model exhibits slightly improved transferable robustness but still performs poorly. The difference is illustrated as blue text in Eq.Â 11, Â 12. In conclusion, a simple combination of self-supervised learning and adversarial meta-learning, as presented in AlgorithmÂ 3, leads to representation collapse due to the small batch size, rendering the task-agnostic adversarial attack ineffective in leveraging transferable robustness in unseen domains.",
            "We conducted ablation experiments on each component of our framework, as summarized in TableÂ 6. Each component significantly contributes to improving robustness in both seen and unseen domains. Notably, when we incorporate bootstrapping multi-view encoders, the model achieves substantially enhanced robustness in the unseen domains. These results highlight the crucial role of each of our novel components in achieving robustness in unseen domains."
        ]
    },
    "A3.T7": {
        "caption": "Table 7: Results of adversarial robustness for 5-way 5-shot classification tasks on unseen and seen domains. All adversarial meta-learning methods are trained on CIFAR-FS. Rob. stands for accuracy (%) calculated with PGD-20 attack (Ïµ=8./255.\\epsilon=8./255., Î³=Ïµ/10ğ›¾italic-Ïµ10\\gamma=\\epsilon/10).",
        "table": null,
        "footnotes": [],
        "references": []
    },
    "A3.T8": {
        "caption": "Table 8: Results of transferable robustness with different meta-learning framework and attack iteration in 5-shot tasks. All models are trained with 5-way 5-shot images on CIFAR-FS and Mini-ImageNet. Rob. stands for accuracy(%) that is calculated with PGD-20 attack (Ïµ=8./255.\\epsilon=8./255., step size=Ïµ/10italic-Ïµ10\\epsilon/10). Clean stands for test accuracy(%) of clean images. All models are trained on ResNet12. The number of attack iterations during training is marked in parentheses next to the meta-train dataset. Further, we denote (Î¸ğœƒ\\theta) next to the meta-learning strategies to notice that we update only the encoder parameters during inner optimization.",
        "table": null,
        "footnotes": [],
        "references": [
            "TableÂ 8 highlights that MAVRL outperforms the previous adversarial meta-learning modelÂ (Goldblum etÂ al., 2020) in terms of adversarial robustness by more than 10%, irrespective of meta-learning strategies. Furthermore, MAVRL exhibits remarkable robustness with just 3 steps of multi-view latent attacks compared to AQÂ (Goldblum etÂ al., 2020), which is trained with PGD-7 attacks (i.e., class-wise attack). To emphasize the superiority of multi-view latent attacks over class-wise attacks at the representation level, we calculate feature similarity between clean and adversarial examples using CKAÂ (Kornblith etÂ al., 2019). Notably, the latent attack yielded a lower CKA value than the class-wise attack (as seen in FigureÂ 4), which means that latent attacks produce perturbations that deviate more significantly from the original clean images, making them more challenging. Through these remarkable results, we underscore that our proposed multi-view latent attack served as a stronger attack that promotes the robust transferability of the model to unseen domains, even with fewer gradient steps of attacks and limited data.",
            "All robust accuracies reported in our paper are calculated using the strength Ïµ=8./255.\\epsilon=8./255., step size Î±=8./2550.\\alpha=8./2550., and 20 steps for the â„“âˆsubscriptâ„“\\ell_{\\infty} PGD attacks. In order to assess the presence of obfuscated gradient issues, we conduct experiments with two different settings of â„“âˆsubscriptâ„“\\ell_{\\infty} PGD attacks. Firstly, we apply PGD attacks with an extremely large strength, expecting the robust accuracy to be nearly zero. Secondly, we use the same strength but different step sizes and steps, specifically 4./2550.4./2550. and 40 respectively. In this case, we expect the robust accuracy to remain the same as the robust accuracy from our original evaluation setting. To demonstrate this, we evaluate MAVRL trained on CIFAR-FS with ResNet12 as the base encoder, and built on top of the FOMAML architecture as reported in TableÂ 8. As shown in TableÂ 12, we confirm that our models do not exhibit any obfuscated gradient issues."
        ]
    },
    "A3.T9": {
        "caption": "Table 9: Ablation results of transferable robustness with different consistency loss in MAVRL framework. Rob. stands for accuracy (%) that is calculated with PGD-20 attack (Ïµ=8./255.\\epsilon=8./255., step size=Ïµ/10italic-Ïµ10\\epsilon/10). Clean stands for test accuracy (%) of clean images. All models are meta-trained on CIFAR-FS with PGD-7 attacks on ResNet12.",
        "table": null,
        "footnotes": [],
        "references": [
            "The objective of the proposed meta-adversarial learning framework consists of three different elements, 1) cross-entropy loss, 2) multi-view instance-wise adversarial loss, and 3) cosine distance loss as in Eq.Â 10. In particular, cosine distance loss enforces the consistency between two maximally dissimilar views of adversaries, leading meta-learners to achieve transferable robustness. We further examine the effectiveness of cosine distance loss by altering it to other consistency loss including contrastive loss and KLD loss. As shown in TableÂ 9, the cosine similarity term was the most effective objective for aligning the multi-view latent spaces, demonstrating the highest unseen domain robustness on average. This is because cosine distance loss explicitly aligns the two latent vectors obtained from multi-view latent attacks while others implicitly enforce the consistency to differently generated adversarial representations."
        ]
    },
    "A4.T10": {
        "caption": "Table 10: Results of transferable robustness in 5-way 5-shot unseen domain tasks that are trained on 5-way 5-shot Tiered-ImageNet. Rob. stands for accuracy (%) that is calculated with PGD-20 attack (Ïµ=8./255.\\epsilon=8./255., step size=Ïµ/10italic-Ïµ10\\epsilon/10). Clean stands for test accuracy (%) of clean images. All models are trained with PGD-7 attacks on ResNet12.",
        "table": null,
        "footnotes": [],
        "references": [
            "To provide a more convincing comparison, we additionally conduct experiments where models are meta-trained on a larger dataset, Tiered-ImageNetÂ (Russakovsky etÂ al., 2015). Tiered-ImageNet consists of 779,165 images and 608 classes which are 351, 97, and 160 classes for meta-training, meta-validation, and meta-testing respectively. All images are resized by 3Ã—32Ã—32332323\\times 32\\times 32 resolution (channel, width, and height) to validate the modelâ€™s robust transferability to unseen domain tasks. As demonstrated in TableÂ 10, when the models are meta-trained on a larger dataset, our meta-leaner consistently outperforms the previous adversarial meta-learning method (AQ) for both clean and robust accuracy on unseen domain tasks. This indicates that MAVRL can effectively learn robust representations transferred to unseen domains, regardless of how unseen domains are different from the meta-trained dataset."
        ]
    },
    "A5.T11": {
        "caption": "Table 11: Results of transferable adversarial robustness in 5-way 15-shot non-RGB unseen domain tasks that are trained on CIFAR-FS.",
        "table": null,
        "footnotes": [],
        "references": [
            "To demonstrate the ability to learn transferable robustness on unseen domain tasks of the proposed framework MAVRL, we further employ unseen domains of non-RGB domains (i.e., ISICÂ (Codella etÂ al., 2018), CropDiseaseÂ (Mohanty etÂ al., 2016), and EuroSATÂ (Helber etÂ al., 2019)), which have much more different distributions from meta-trained RGB dataset (i.e., CIFAR-FS). This experiment can encompass variations such as color scale (RGB, Gray-scale), and distinct image type (i.e., MRI, satellite imagery), enabling more accurate evaluation of the transferable robustness across a wide range of domains. As shown in TableÂ 11, MAVRL exhibits outstanding transferable robustness of 17.47% on average even in non-RGB unseen domain tasks compared to previous adversarial meta-learning method."
        ]
    },
    "A6.T12": {
        "caption": "Table 12: Test accuracy(%) on multiple benchmark datasets for 5-shots. Robustness is calculated with PGD-20 attack (Ïµ=8./255.\\epsilon=8./255., step size=Ïµ/10italic-Ïµ10\\epsilon/10), clean is for clean images. All models are adversarially meta-trained on CIFAR-FS.",
        "table": null,
        "footnotes": [],
        "references": [
            "All robust accuracies reported in our paper are calculated using the strength Ïµ=8./255.\\epsilon=8./255., step size Î±=8./2550.\\alpha=8./2550., and 20 steps for the â„“âˆsubscriptâ„“\\ell_{\\infty} PGD attacks. In order to assess the presence of obfuscated gradient issues, we conduct experiments with two different settings of â„“âˆsubscriptâ„“\\ell_{\\infty} PGD attacks. Firstly, we apply PGD attacks with an extremely large strength, expecting the robust accuracy to be nearly zero. Secondly, we use the same strength but different step sizes and steps, specifically 4./2550.4./2550. and 40 respectively. In this case, we expect the robust accuracy to remain the same as the robust accuracy from our original evaluation setting. To demonstrate this, we evaluate MAVRL trained on CIFAR-FS with ResNet12 as the base encoder, and built on top of the FOMAML architecture as reported in TableÂ 8. As shown in TableÂ 12, we confirm that our models do not exhibit any obfuscated gradient issues."
        ]
    },
    "A8.T13": {
        "caption": "Table 13: Experiments results for self-supervised robust full-finetuning of MAVRL and the state-of-the-art adversarial self-supervised models on unseen domains. While MAVRL is trained on CIFAR-FS with bilevel attacks, adversarial self-supervised models are trained on full dataset of CIFAR-100. All models are trained on ResNet18, and evaluated against PGD-20 attacks (Ïµitalic-Ïµ\\epsilon = 8./255.8./255.) and AutoAttack (AA)Â (Croce & Hein, 2020)",
        "table": null,
        "footnotes": [],
        "references": [
            "Although our models utilize only scarce data to train and even apply latent attacks with fewer gradient steps, we show comparable clean and robust accuracy compared to self-supervised pre-trained models which are trained with larger data and stronger attacks with more steps of inner maximization (TableÂ 13). Especially, our methods show a larger gap in robustness on fine-grained datasets (i.e., CUB, Cars), which have highly different distributions from meta-trained domains (i.e., CIFAR-FS). Further, we hope that our models to be robust in real-world adversarial perturbation such as common corruptionÂ (Hendrycks & Dietterich, 2019), we evaluate our fully finetuned models with adversarial examples on CIFAR-10, with common corruption datasets on CIFAR-10."
        ]
    }
}