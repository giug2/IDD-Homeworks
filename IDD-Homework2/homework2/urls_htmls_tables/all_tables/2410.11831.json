{
    "id_table_1": {
        "caption": "Table 1:    TAP-Vid benchmarks  CoTracker3 trained on synthetic Kubric shows strong performance compared to other models, while the online version fine-tuned on 15k additional real videos (Kub+15k) outperforms all the other methods, even BootsTAPIR trained on 1,000  \\times   more real videos. Training data: (Kub) Kubric  (Greff et al.,  2022 ) , (PO) Point Odyssey  (Zheng et al.,  2023 ) .",
        "table": "Ax1.EGx1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "In order to enable such training, we collected a large-scale dataset of Internet-like videos (around 100,000 videos of 30 seconds each) featuring diverse scenes and dynamic objects, primarily humans and animals. We demonstrate that performance improves when training on increasingly larger subsets of this data, starting from as few as 100 videos (see  Figure   1 ).",
            "In this section, we describe our evaluation protocol. Then, we compare our online and offline models to state-of-the-art trackers ( Section   4.1 ), analyse their performance for occluded points ( Section   4.1 ), show how different models scale with the proposed pseudo-labeling pipeline ( Section   4.2 ), and ablate the design choices of the architecture and the scaling pipeline ( Section   4.3 ).",
            "As shown in  Table   1 , CoTracker3 is highly competitive with other trackers across various benchmarks even when only trained using synthetic data (Kub). Adding unlabelled videos utilizing the approach of  Section   4.2  (+15k) boosts the results well above the state-of-the-art for all metrics for DAVIS, RGB-S, and Kinetics, and for two out of three metrics (AJ and OA) on RoboTAP ( Table   2 ). The +15k offline version is even better than the online one on DAVIS and RGB-S, but worse on Kinetics and RoboTAP. As for data efficiency, despite being trained on just 15k additional real videos, our models outperform BootsTAPIR, which was trained using 15M videos (i.e.,  1,000  more). Slightly better performance can be obtained by increasing the data further ( Section   4.2 ). LocoTrack also benefits similarly from our training scheme but struggles during occlusions, as shown next.",
            "In  Figure   1 , we show how CoTracker3, LocoTrack, and CoTracker  (Karaev et al.,  2024 )  improve with our pseudo-labeling pipeline as the training set size increases. Starting with models pre-trained on a synthetic dataset  (Greff et al.,  2022 )  ( 0 0  at x-axis), we train them on progressively larger real data sets: 0.1k, 1k, 5k, 15k, 30k, and 100k videos. Models are trained to convergence on their respective subsets. All models improve with just 0.1k real-world videos and continue improving with more. Improvements for CoTracker3 online, offline, and LocoTrack tend to plateau after 30k videos, likely because the student surpasses the teachers. This may also explain why CoTracker, initially much weaker than two of its teachers (CoTracker3 online and offline), keeps improving up to and possibly beyond 100k videos, which is the maximum we can afford to explore. Our training strategy is effective for all these models. We analyse the effect of using a scaled CoTracker3 as a new teacher in the supplement. For comparison, BootsTAPIR  (Doersch et al.,  2024 )  uses 15 million real videos and a complex protocol involving augmentations, loss masks, and more.",
            "We find that this second round of scaling leads to slight improvements in performance metrics. This suggests that the student model has already distilled most of the knowledge from the other teachers during the initial training phase. We report the results in  Table   10 .",
            "We examine the convergence behavior of our scaling pipeline by fixing the dataset and all the hyper-parameters, varying only the number of iterations over the dataset. We show in  Table   10  that increasing the number of iterations leads to improved performance on TAP-Vid, but with diminishing returns. Specifically, we observe a saturation point beyond which further increases in the number of training iterations do not yield significant improvements in model quality. We thus use the same number of iterations as the number of training videos with a batch size of 32, iterating over each video 32 times."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:    Results on Dynamic Replica and RoboTAP.  Our approach consistently shows better results. Only   avg vis superscript subscript  avg vis \\delta_{\\text{avg}}^{\\text{vis}} italic_ start_POSTSUBSCRIPT avg end_POSTSUBSCRIPT start_POSTSUPERSCRIPT vis end_POSTSUPERSCRIPT  on RoboTAP is better for BootsTAPIR, trained on 1,000  \\times   more data. Size in number of params; speed expressed as    s  s \\mu s italic_ italic_s  per frame and per tracked point. See  Figure   3  for qualitative results on RoboTAP.",
        "table": "S4.T1.14",
        "footnotes": [
            ""
        ],
        "references": [
            "When using pseudo-labelled videos, we supervise CoTracker3 using the same loss ( 2 ) used for the synthetic data, but found it more stable not to supervise confidence and visibility. To avoid forgetting the latter predictions, we use a separate linear layer to estimate confidence and visibility and simply freeze it at this training stage.",
            "During training, we compute the same losses ( 2 ) to ( 4 ) for the online version separately for each sliding window. Then, we take the mean across all the sliding windows. Since the online version can track points only forward in time, we compute the losses only starting from the first window with the query frame  t q superscript t q \\mathbf{t}^{q} bold_t start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT  onwards. For the offline version, however, we compute the losses for every frame because it tracks points in both directions. We train the online version on videos of the same length, while the offline version needs to see videos of different lengths during training to avoid overfitting to a specific length. With this intuition in mind, for the offline version, we randomly trim a video between  T / 2 T 2 T/2 italic_T / 2  and  T T T italic_T  frames and linearly interpolate time embeddings during training.",
            "In this section, we describe our evaluation protocol. Then, we compare our online and offline models to state-of-the-art trackers ( Section   4.1 ), analyse their performance for occluded points ( Section   4.1 ), show how different models scale with the proposed pseudo-labeling pipeline ( Section   4.2 ), and ablate the design choices of the architecture and the scaling pipeline ( Section   4.3 ).",
            "As shown in  Table   1 , CoTracker3 is highly competitive with other trackers across various benchmarks even when only trained using synthetic data (Kub). Adding unlabelled videos utilizing the approach of  Section   4.2  (+15k) boosts the results well above the state-of-the-art for all metrics for DAVIS, RGB-S, and Kinetics, and for two out of three metrics (AJ and OA) on RoboTAP ( Table   2 ). The +15k offline version is even better than the online one on DAVIS and RGB-S, but worse on Kinetics and RoboTAP. As for data efficiency, despite being trained on just 15k additional real videos, our models outperform BootsTAPIR, which was trained using 15M videos (i.e.,  1,000  more). Slightly better performance can be obtained by increasing the data further ( Section   4.2 ). LocoTrack also benefits similarly from our training scheme but struggles during occlusions, as shown next.",
            "We compare CoTracker3 with other methods on Dynamic Replica in  Table   2  (  avg occ superscript subscript  avg occ \\delta_{\\text{avg}}^{\\text{occ}} italic_ start_POSTSUBSCRIPT avg end_POSTSUBSCRIPT start_POSTSUPERSCRIPT occ end_POSTSUPERSCRIPT  and OA columns). On this benchmark, CoTracker3 online is better than all the other methods even when trained solely on Kubric; in particular, it is much better than LocoTrack, which justifies the additional parameters in the cross-track attention modules. Adding the 15k real videos improves the tracking of visible points for the online and offline versions, but only the offline model shows improvement in tracking occluded points. In addition to improving more, CoTracker3 offline tracks occluded points better than the online version. This is because accessing all video frames at once helps to interpolate trajectories behind occlusions. See  Figure   4  for qualitative results."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Impact of cross-track attention on occluded tracking.  Cross-track attention improves the tracking of occluded points substantially. It also improves visible points, but the effect is smaller.",
        "table": "S4.T2.12",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "We supervise tracks predicted by the student model with the same loss used to pre-train the model on synthetic data, with only minor modifications for handling occlusion and tracking confidence. These details are given later in  Section   3.3 .",
            "In this section, we describe our evaluation protocol. Then, we compare our online and offline models to state-of-the-art trackers ( Section   4.1 ), analyse their performance for occluded points ( Section   4.1 ), show how different models scale with the proposed pseudo-labeling pipeline ( Section   4.2 ), and ablate the design choices of the architecture and the scaling pipeline ( Section   4.3 )."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Self-training.  Training CoTracker3 online on its own predictions improves the model. We use 10k real videos and train to convergence.",
        "table": "S4.T4.4.4",
        "footnotes": [],
        "references": [
            "During training, we compute the same losses ( 2 ) to ( 4 ) for the online version separately for each sliding window. Then, we take the mean across all the sliding windows. Since the online version can track points only forward in time, we compute the losses only starting from the first window with the query frame  t q superscript t q \\mathbf{t}^{q} bold_t start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT  onwards. For the offline version, however, we compute the losses for every frame because it tracks points in both directions. We train the online version on videos of the same length, while the offline version needs to see videos of different lengths during training to avoid overfitting to a specific length. With this intuition in mind, for the offline version, we randomly trim a video between  T / 2 T 2 T/2 italic_T / 2  and  T T T italic_T  frames and linearly interpolate time embeddings during training.",
            "In this section, we describe our evaluation protocol. Then, we compare our online and offline models to state-of-the-art trackers ( Section   4.1 ), analyse their performance for occluded points ( Section   4.1 ), show how different models scale with the proposed pseudo-labeling pipeline ( Section   4.2 ), and ablate the design choices of the architecture and the scaling pipeline ( Section   4.3 ).",
            "As shown in  Table   1 , CoTracker3 is highly competitive with other trackers across various benchmarks even when only trained using synthetic data (Kub). Adding unlabelled videos utilizing the approach of  Section   4.2  (+15k) boosts the results well above the state-of-the-art for all metrics for DAVIS, RGB-S, and Kinetics, and for two out of three metrics (AJ and OA) on RoboTAP ( Table   2 ). The +15k offline version is even better than the online one on DAVIS and RGB-S, but worse on Kinetics and RoboTAP. As for data efficiency, despite being trained on just 15k additional real videos, our models outperform BootsTAPIR, which was trained using 15M videos (i.e.,  1,000  more). Slightly better performance can be obtained by increasing the data further ( Section   4.2 ). LocoTrack also benefits similarly from our training scheme but struggles during occlusions, as shown next.",
            "We compare CoTracker3 with other methods on Dynamic Replica in  Table   2  (  avg occ superscript subscript  avg occ \\delta_{\\text{avg}}^{\\text{occ}} italic_ start_POSTSUBSCRIPT avg end_POSTSUBSCRIPT start_POSTSUPERSCRIPT occ end_POSTSUPERSCRIPT  and OA columns). On this benchmark, CoTracker3 online is better than all the other methods even when trained solely on Kubric; in particular, it is much better than LocoTrack, which justifies the additional parameters in the cross-track attention modules. Adding the 15k real videos improves the tracking of visible points for the online and offline versions, but only the offline model shows improvement in tracking occluded points. In addition to improving more, CoTracker3 offline tracks occluded points better than the online version. This is because accessing all video frames at once helps to interpolate trajectories behind occlusions. See  Figure   4  for qualitative results.",
            "Interestingly, we found that training CoTracker3 with its own predictions as annotations without other teachers (i.e., self-training) further improves the results on all the TAP-Vid benchmarks by  + 1.2 1.2 +1.2 + 1.2  points on average (see  Table   4 ). Presumably, fine-tuning on real data, even with its own annotations, helps the model reduce the domain gap between real and synthetic data.",
            "Table   4  shows that cross-track attention improves results, particularly for occluded points ( + 5.1 5.1 +5.1 + 5.1  occluded vs.  + 1.6 1.6 +1.6 + 1.6  visible on Dynamic Replica). This is because by using cross-track attention, the model can guess the positions of the occluded points based on the positions of the visible ones. This cannot be done if the points are tracked independently."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Models used as teachers.  We use CoTracker3 online as a student model and ablate different combinations of teacher models. The first row corresponds to the model trained only on synthetic data. The second row corresponds to self-training. Generally, the more diverse teachers we have, the better is the tracking accuracy (  avg subscript  avg \\delta_{\\text{avg}} italic_ start_POSTSUBSCRIPT avg end_POSTSUBSCRIPT ).",
        "table": "S4.T4.8.4",
        "footnotes": [],
        "references": [
            "We assess the impact of using multiple teachers for generating pseudo-labels in  Table   5 . We start by removing weaker models and always keep the student model itself as a teacher. We demonstrate that removing a teacher always leads to worse results compared to the last row, where we train with all four teacher models. This shows that every teacher is important and that the student model can always extract complementary knowledge, even from weaker teachers.",
            "We scale CoTracker3 on a dataset of Internet-like videos primarily featuring humans and animals. We visualize the scaling pipeline in  Figure   5 . To ensure the quality and relevance of our training data, we use caption-based filtering with specific keywords to select videos containing real-world content while excluding those with computer-generated imagery, animation, or natural phenomena that are challenging to track, such as fire, lights, and water.",
            "Interestingly, we found that aggregating the predictions of multiple teachers instead of using a random teacher does not improve performance, as shown in  Table   8 , whereas incorporating additional teachers into training consistently enhances the quality of our student model, demonstrated in  Table   5 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Point sampling strategies  on   avg subscript  avg \\delta_{\\text{avg}} italic_ start_POSTSUBSCRIPT avg end_POSTSUBSCRIPT  on TAP-Vid. SIFT is overall best, but the method is robust w.r.t. this choice.",
        "table": "S4.T5.4.4",
        "footnotes": [],
        "references": [
            "In  Figure   6 , we compare the speed of CoTracker3 with other point trackers. We measure the average time it takes for the method to process one frame, with the number of tracked points varying between 1 and 10,000. We average this across 20 videos of varying lengths from DAVIS. Even though CoTracker and CoTracker3 apply group attention between tracked points, the time complexity remains linear thanks to the proxy tokens introduced by  (Karaev et al.,  2024 ) . While all the trackers exhibit linear time complexity depending on the number of tracks, CoTracker3 is approximately 30% faster than LocoTrack  (Cho et al.,  2024 ) , the fastest point tracker to date."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:   Average AJ,   avg subscript  avg \\delta_{\\text{avg}} italic_ start_POSTSUBSCRIPT avg end_POSTSUBSCRIPT  and OA on TAP-Vid, where  freezing  the confidence and visibility heads improves performance,  avoiding forgetting .",
        "table": "S4.T7.2.3",
        "footnotes": [],
        "references": [
            "In  Table   7  we have explored alternative point sampling methods, including LightGlue  (Lindenberger et al.,  2023 ) , SuperPoint  (DeTone et al.,  2018 ) , and DISK  (Tyszkiewicz et al.,  2020 ) . The choice of the sampling method does not significantly affect the performance. However, SIFT sampling results are consistently high across all the TAP-Vid datasets.",
            "In  Table   7 , we show that splitting the transformer head into a separate head for tracks and a head for confidence and visibility helps to avoid forgetting when supervising only tracks while training on real data. We freeze the head for confidence and visibility at this stage. This improves AJ by  + 0.8 0.8 +0.8 + 0.8  and OA by  + 3.9 3.9 +3.9 + 3.9  on TAP-Vid on average."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Supervision.  Random sampling of teachers consistently leads to better   avg subscript  avg \\delta_{\\text{avg}} italic_ start_POSTSUBSCRIPT avg end_POSTSUBSCRIPT  on TAP-Vid compared to supervision with either the mean or the median of all teachers predictions.",
        "table": "S4.T7.6.4",
        "footnotes": [],
        "references": [
            "Interestingly, we found that aggregating the predictions of multiple teachers instead of using a random teacher does not improve performance, as shown in  Table   8 , whereas incorporating additional teachers into training consistently enhances the quality of our student model, demonstrated in  Table   5 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Repeated scaling.  We scale CoTracker3 offline, then start from a scaled model, and scale it again with the scaled model as one of the teachers. Repeated scaling slightly improves tracking accuracy.",
        "table": "Ax1.T8.3",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "Table 10:  Longer training on 15k videos.  We train CoTracker3 offline for longer to determine the optimal number of iterations for a given number of videos. As a trade-off between training costs and the results obtained, we use the same number of iterations as the number of videos.",
        "table": "Ax1.T10.4.4",
        "footnotes": [],
        "references": [
            "We find that this second round of scaling leads to slight improvements in performance metrics. This suggests that the student model has already distilled most of the knowledge from the other teachers during the initial training phase. We report the results in  Table   10 .",
            "We examine the convergence behavior of our scaling pipeline by fixing the dataset and all the hyper-parameters, varying only the number of iterations over the dataset. We show in  Table   10  that increasing the number of iterations leads to improved performance on TAP-Vid, but with diminishing returns. Specifically, we observe a saturation point beyond which further increases in the number of training iterations do not yield significant improvements in model quality. We thus use the same number of iterations as the number of training videos with a batch size of 32, iterating over each video 32 times."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "Ax1.T10.8.4",
        "footnotes": [],
        "references": []
    }
}