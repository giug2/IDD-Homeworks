{
    "id_table_1": {
        "caption": "Table 1:  An example of data collection. Step-2: GPT-4 generates memories from raw data. Step-3: GPT-4 forms QA pairs using several memories, and produces the required memories, which are utilized for training the  EMG-RAG .",
        "table": "S3.T1.12",
        "footnotes": [],
        "references": [
            "These challenges can be summarized below.   (1) Data Collection : Personal memories should encompass valuable information about a user. Extracting these memories from everyday trivial conversations presents unique challenges in data collection, especially considering that existing datasets like personalized chats sourced through crowdsourcing  Zhang et al. ( 2018 )  or psychological dialogues  Zhong et al. ( 2024 )  lack this property. Moreover, constructing annotated data, such as QA pairs, is essential for enabling effective training of personalized agents.  (2) Editability : Personal memories are dynamic and continuously evolving, requiring three types of editable operations: insertion, deletion, and replacement. For example, 1) insertion occurs when new memories are added; 2) deletion is necessary for time-sensitive memories, such as a hotel voucher that expires and needs to be removed; 3) replacement is required when an existing memory, such as a flight booking, undergoes a change in departure time and needs updating. Therefore, a carefully designed memory data structure is essential to support this editability.  (3) Selectability : To enable the memory data services for real-world applications, it often requires querying a combination of multiple memories. For example, in a QA scenario (illustrated in Table  1 ), the AI assistant answering a question about a secretarys bosss flight departure time needs several memories: the secretary booked a flight to Amsterdam for her boss ( M 1 subscript M 1 M_{1} italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ); the flights number is EK349 ( M 2 subscript M 2 M_{2} italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ); the departure time for EK349 is at 01:40 on 2024-05-12 ( M 4 subscript M 4 M_{4} italic_M start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT ). To achieve this, one intuitive approach is to use Retrieval-Augmented Generation (RAG)  Lewis et al. ( 2020 )  to find relevant memories and form a context that is fed into a LLM to generate answers. Here, we discuss two potential solutions and their limitations, which motivate the proposed solution.  1) Needles in a Haystack (NiaH)  Briakou et al. ( 2023 ) : it organizes all memories into a single context (the Haystack) and inputs this into a LLM, relying on the capability of a LLM itself to identify relevant memories (the Needles) for generating an answer. However, this method incurs significant overhead by extending the LLMs context window and introduces noise from irrelevant memories, hindering the LLMs ability to generate accurate answers.  2) Advanced RAG  Wang et al. ( 2024 ); Ma et al. ( 2023 ) : many advanced RAG techniques still rely on Top- K K K italic_K  retrieval to identify relevant memories. However, a fixed parameter  K K K italic_K  may limit the LLMs ability to uncover all relevant memories, especially for the questions requiring diverse memory combinations. Thus, an adaptive selection mechanism is essential for the personalized applications.",
            "The process entails (1) gathering raw data, such as everyday conversations or screenshots from user interactions with the smartphone AI assistants; (2) extracting crucial information from this raw data, referred to as memories (denoted by  M M M italic_M ); and (3) generating QA pairs (denoted by  < Q , A > <Q,A> < italic_Q , italic_A > ), and outputting the required memories to facilitate this pairing. For (1), we acquire data from real AI assistant products and employ text processing techniques like OCR to extract content from screenshots. Subsequently, for (2) and (3), we leverage the capabilities of LLMs, such as GPT-4  OpenAI ( 2023 ) , to extract key memories from the raw data and create QA pairs. These pairs serve the purpose of training personalized agents for the proposed  EMG-RAG . To illustrate the collection process, we provide a running example in Figure  1  and Table  1 , which involve the three primary steps. Further details are outlined in Appendix  A.1 .",
            "Training Policies of MDPs.  Training the MDP policy involves two stages: warm-start stage (WS) and policy gradient stage (PG).  In WS , we employ supervised fine-tuning to equip the agent with the basic ability to select memories given a question  Q Q Q italic_Q . Specifically, based on a state  s s \\mathbf{s} bold_s , the agent undergoes a binary classification task to predict whether the memory  M i subscript M i M_{i} italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  should be included. This prediction is supervised according to whether the memory falls into the required memories (presented in the Step-3 in Table  1 ). Thus, the objective is trained with binary cross-entropy, formulated as:",
            "As detailed in Section  4.1 , we establish the ground truth for the applications of question answering and autofill forms/user services using GPT-4 generated answers and key entities (e.g., identification number, address, and time), respectively. We provide a quality evaluation for the collected dataset in Section  5.2 .",
            "Table  9  presents the prompt for collecting memories from raw extracted data, while Table  10  provides the prompt for generating reasoning as the required memories for QA pairs. The prompt for generating QA pairs based on this reasoning is presented in Table  11 . Additionally, Table  12  offers an alternative method to synthesize memories when raw extracted data is unavailable."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Effectiveness of  EMG-RAG  in downstream applications.",
        "table": "S3.T1.1.1.1.1",
        "footnotes": [],
        "references": [
            "Editability: The responses from the agents may be editable based on the users dynamic memory data, which involves insertion, deletion, and replacement operations corresponding to different usage scenarios, as illustrated in Figure  2 (a).",
            "Selectability: The agents can select relevant memories to respond to users queries, with some queries requiring the combination of multiple memories to generate responses through a base language model, as illustrated in Figure  2 (b).",
            "The EMG Construction and Insights.  Utilizing a users memories, we establish the Editable Memory Graph with a multilayered structure, depicted in Figure  2 (a), where the user is the root node.",
            "Memory Type Layer (MTL) : Aligned with the business scope, we categorize memories into 4 predefined types: Relationship, Preference, Event, and Attribute. Details are provided in Appendix  A.2 .",
            "Memory Subclass Layer (MSL) : The MSL further outlines subclasses for each type, where the MTL and MSL are organized in a hierarchical tree structure to manage the memories. Detailed subclasses with examples are listed in Appendix  A.2 .",
            "Memory Graph Layer (MGL) : The memory graph is built by utilizing the collected memories, employing entity recognition for nodes and relation extraction for edges. In this graph, each in-degree node is associated with its corresponding memory, e.g., the in-degree node (01:40 on 2024-05-12) contains  M 4 subscript M 4 M_{4} italic_M start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT , as shown in Figure  2 (a).  Further, to establish the connection between the MSL and MGL, TransE embeddings  Bordes et al. ( 2013 )  are employed to capture semantic information of nodes in MSL (subclasses) and MGL (entities), respectively. Then, each entity is assigned to its closest classes based on these embeddings. It is noteworthy that entity nodes are categorized into different subclasses, and their connections may span across different classes, e.g., Boss and Amsterdam are linked across Colleague and Arrangement classes in Figure  2 (a). This design enables further traversal across various parts of the whole graph.",
            "The EMG Editing.  When editing a given memory within the EMG (e.g., insertion, deletion, or replacement), the process involves three steps. Initially, a model such as CPT-Text  Neelakantan et al. ( 2022 )  is employed to acquire memory representations. Then, the memory is assigned to its nearest subclass (partition), and the Top-1 retrieved memory within the partition is then returned, and editing operations are performed based on comparing the relations between the given memory and the retrieved memory. Specifically, as illustrated in Figure  2 , (1) Insertion: It introduces a new relation to be added, e.g., obtaining a new memory containing flight seat number. (2) Deletion: It introduces a new relation, but it is valid for a specific period of time. e.g., a hotel voucher will expire on May 14, 2024. (3) Replacement: It provides an existing relation, and updates the corresponding entity nodes based on this relation, e.g., changing the departure time to 01:30 on May 12, 2024.",
            "Constructing Environment (Nodes activated by Questions).  Given an EMG, which often contains numerous memories in practice. Here, we confine the movement of the RL agent to a subset of memories to facilitate more focused selection. To achieve this, we first retrieve Top- K K K italic_K  memories for a given question  Q Q Q italic_Q , and based on these memories, we activate the corresponding nodes on the EMG (e.g., the nodes highlighted in yellow in Figure  2 (b)). Subsequently, the agents traversal starts from each activated node via depth-first search.",
            "Inference Stage of  EMG-RAG .  As shown in Figure  2 , the inference involves three steps: (a) collecting newly recorded memories from users and editing their EMGs; (b) using the edited EMGs to traverse the graph and retrieve relevant memories for LLM generation; (c) integrating the generated answers to serve users across three downstream applications.",
            "Applications of the Personalized Agents.  As shown in Figure  2 (c), we explore the capabilities of personalized agents in three scenarios: (1) question answering, (2) autofill forms, and (3) user services. For (1),  EMG-RAG  can generate answers to users questions when they interact with the smartphone AI assistants. For (2), the goal is to extract personal information from users EMGs to automatically fill out various online forms, such as flight and hotel bookings.  To achieve this, we input form-related questions (e.g., What is the users mobile number?) into the LLM and use the generated entities to complete the forms. For (3), we focus on two specific domains. a) reminder service: It involves reminding users of recent events and times. To achieve this, we query a LLM for information about a users recent events and their associated times. b) travel service: We assist users with navigation by providing the address of a destination they might want to visit. Further, we integrate the generated answers (e.g., events, times, addresses) with external tools such as calendar or map apps to provide the services for users.",
            "As detailed in Section  4.1 , we establish the ground truth for the applications of question answering and autofill forms/user services using GPT-4 generated answers and key entities (e.g., identification number, address, and time), respectively. We provide a quality evaluation for the collected dataset in Section  5.2 .",
            "(1) Effectiveness evaluation (question answering).  We compare the  EMG-RAG  with other RAG methods for question answering on three LLMs. As shown in Table  2 , we observe that the performance of  EMG-RAG  consistently outperforms the baselines. For example, it improves upon the best baseline method, M-RAG, by 5.3%, 8.3%, 3.9%, and 18.4% in terms of R-1, R-2, R-L, and BLEU, respectively. This improvement is due to two main factors: 1) it captures complex relationships between memories with the EMG, and 2) it effectively selects essential memories for the RAG execution. Additionally, GPT-4 demonstrates superior performance compared to other LLMs, and  EMG-RAG  shows comparable performance to M-RAG even when deployed on the relatively smaller ChatGLM3-6B.",
            "(2) Effectiveness evaluation (autofill forms).  We further evaluate the  EMG-RAG  for autofill forms, and it shows consistent improvement, as detailed in Table  2 . For example, it surpasses M-RAG by 2.2% in terms of exact match accuracy.",
            "(3) Effectiveness evaluation (user services).  We target two specific domains of user services: 1) reminders of important events and their times, and 2) travel services involving destination addresses for navigation. We report the exact match accuracy for events and times (reminders), and addresses (travel) in Table  2 . The improvements over M-RAG for the two tasks are 2.9% and 5.5%.",
            "Moreover, using a graph enhances effectiveness by naturally capturing semantic relationships between memories, which improves reasoning during RAG. As demonstrated by the experimental results in Table  2  and Table  3 , our approach outperforms the NiaH, Naive, and M-RAG baselines, achieving approximately a 10% improvement over the best baseline M-RAG, which manages the memory instances independently.",
            "Table  9  presents the prompt for collecting memories from raw extracted data, while Table  10  provides the prompt for generating reasoning as the required memories for QA pairs. The prompt for generating QA pairs based on this reasoning is presented in Table  11 . Additionally, Table  12  offers an alternative method to synthesize memories when raw extracted data is unavailable."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Effectiveness of  EMG-RAG  for continuous edits.",
        "table": "S3.T1.11.11.1.1",
        "footnotes": [],
        "references": [
            "(4) Effectiveness evaluation (continuous edits).  We evaluate the effectiveness of  EMG-RAG  in supporting continuous edits over a period of 4 weeks. The results, in terms of R-L for question answering (QA), and exact match accuracy for autofill forms (AF) and user services (US, combining reminder and travel results), are presented in Table  3 . We observe that  EMG-RAG  consistently outperforms M-RAG, by approximately 10.6%, 9.5%, and 9.7% for QA, AF, and US, respectively. This is owing to the editability of  EMG-RAG , whereas M-RAG simply incorporates edits into a database, where many memories may become outdated for answering. Additionally, we report the total number of edits involved in the testing set for each week.",
            "Moreover, using a graph enhances effectiveness by naturally capturing semantic relationships between memories, which improves reasoning during RAG. As demonstrated by the experimental results in Table  2  and Table  3 , our approach outperforms the NiaH, Naive, and M-RAG baselines, achieving approximately a 10% improvement over the best baseline M-RAG, which manages the memory instances independently."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Ablation study.",
        "table": "S3.T1.12.12.1.1",
        "footnotes": [],
        "references": [
            "By satisfying these properties, the agents aim to enhance the user experience during interactions with their smartphone AI assistants. These agents offer essential functionalities to support personalized applications, including question answering, autofill forms, and user services like reminders for important events and times, and travel navigation (further details will be discussed in Section  4.4 ).",
            "As detailed in Section  4.1 , we establish the ground truth for the applications of question answering and autofill forms/user services using GPT-4 generated answers and key entities (e.g., identification number, address, and time), respectively. We provide a quality evaluation for the collected dataset in Section  5.2 .",
            "(7) Online A/B test.  We perform an online A/B test over one month to compare the new system with the existing one. During this period, we collect real users questions and manually written answers to fine-tune the model. The results, presented in Table  6 , show further improvements across all applications. It highlights a cold-start problem caused by distributional shifts between questions generated by GPT-4 and those posed by real users. We use GPT-4-generated questions for model training because they cover diverse scenarios and allow for the automatic collection of required memories, enabling large-scale training. Once the trained model is deployed, we fine-tune it using real user questions and manually written answers through online learning as described in Section  4.4 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Impacts of the number of  K K K italic_K  for activated nodes.",
        "table": "S5.T2.3",
        "footnotes": [],
        "references": [
            "As detailed in Section  4.1 , we establish the ground truth for the applications of question answering and autofill forms/user services using GPT-4 generated answers and key entities (e.g., identification number, address, and time), respectively. We provide a quality evaluation for the collected dataset in Section  5.2 .",
            "(6) Parameter study ( K K K italic_K  for activated nodes).  We vary the value of  K K K italic_K  from 1 to 5 and report the R-L score for the question answering task, along with the corresponding inference times. As shown in Table  5 , we observe that  K = 3 K 3 K=3 italic_K = 3  provides the best effectiveness while maintaining reasonable inference time. When  K K K italic_K  is smaller, the limited number of activated nodes for graph traversal restricts the ability to find crucial memories. Conversely, when  K K K italic_K  is larger, it activates many nodes and returns numerous memories, potentially introducing noise that hinders the LLM generation. As expected, the inference time increases as  K K K italic_K  increases."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Online A/B test.",
        "table": "S5.T3.3",
        "footnotes": [],
        "references": [
            "Handling the Cold-start Problem.   Given that  EMG-RAG  relies on generated questions for training, it may encounter a potential cold-start issue when deploying to answer real user questions. To address this issue, we utilize online learning to continuously fine-tune the agent using newly recorded questions and manually written answers, as outlined in Equation  6 . This approach aims to ensure that the models policy remains up-to-date for online usage. We validate this method through online A/B testing, and the results demonstrate improvements in user experience, highlighting the positive impact of this strategy in practice.",
            "(7) Online A/B test.  We perform an online A/B test over one month to compare the new system with the existing one. During this period, we collect real users questions and manually written answers to fine-tune the model. The results, presented in Table  6 , show further improvements across all applications. It highlights a cold-start problem caused by distributional shifts between questions generated by GPT-4 and those posed by real users. We use GPT-4-generated questions for model training because they cover diverse scenarios and allow for the automatic collection of required memories, enabling large-scale training. Once the trained model is deployed, we fine-tune it using real user questions and manually written answers through online learning as described in Section  4.4 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Data quality evaluation.",
        "table": "S5.T4.1",
        "footnotes": [],
        "references": [
            "(8) Data quality evaluation.  We evaluate data quality across three data collection steps. For Step-1, we note that OCR is a well-established technology used to extract information from app screenshots in our study. Given that the printed fonts from apps are typically standard, OCR is not expected to face significant challenges. For Step-2 and Step-3, we utilize the powerful GPT-4 model for memory and QA pair collection and assess quality from two perspectives: (1) Qualitatively: We present memory samples from our focus domains as shown in Table  8 , which generally meet the expected precision. (2) Quantitatively: We assess quality using human evaluation and LLM evaluation. The results are reported in Table  7 . For human evaluation, we randomly selected 10% of the user data and asked five participants to annotate the answers (for QA) and entities (for AF and US) based on the collected questions and memories. By comparing the human-annotated answers and entities with those generated by GPT-4, we report a R-L score of 91.1% for QA and exact match scores of 87.5% for AF and 97.4% for US. These results demonstrate the high accuracy of the collected data. For LLM evaluation, we employ a method where GPT-4 self-verifies whether it can generate answers (or entities) that are consistent with those obtained during the data collection, based on the collected questions and required memories. The evaluation reveals the scores of 93.3%, 98.7%, and 99.3% for the three applications, respectively, demonstrating a high level of consistency and effectiveness."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  The supported memory subclasses with memory examples.",
        "table": "S5.T5.3",
        "footnotes": [],
        "references": [
            "(8) Data quality evaluation.  We evaluate data quality across three data collection steps. For Step-1, we note that OCR is a well-established technology used to extract information from app screenshots in our study. Given that the printed fonts from apps are typically standard, OCR is not expected to face significant challenges. For Step-2 and Step-3, we utilize the powerful GPT-4 model for memory and QA pair collection and assess quality from two perspectives: (1) Qualitatively: We present memory samples from our focus domains as shown in Table  8 , which generally meet the expected precision. (2) Quantitatively: We assess quality using human evaluation and LLM evaluation. The results are reported in Table  7 . For human evaluation, we randomly selected 10% of the user data and asked five participants to annotate the answers (for QA) and entities (for AF and US) based on the collected questions and memories. By comparing the human-annotated answers and entities with those generated by GPT-4, we report a R-L score of 91.1% for QA and exact match scores of 87.5% for AF and 97.4% for US. These results demonstrate the high accuracy of the collected data. For LLM evaluation, we employ a method where GPT-4 self-verifies whether it can generate answers (or entities) that are consistent with those obtained during the data collection, based on the collected questions and required memories. The evaluation reveals the scores of 93.3%, 98.7%, and 99.3% for the three applications, respectively, demonstrating a high level of consistency and effectiveness.",
            "Step-1: Raw Data Collection.  We explore two approaches, termed Active Remember (AR) and Passive Remember (PR), for collecting raw data derived from users daily conversations with AI assistants and screenshots from their apps. With AR, the AI assistant is trained to actively classify data (such as conversation sentences) into supported subclasses outlined in Table  8 , and filter out noise data. With PR, users have the option to directly let the assistant to remember specific content for future use. Leveraging AR and PR, we remove a significant volume of trivial data, and then extract memories from the refined dataset.",
            "We enumerate the supported business subclasses of the EMG with memory examples in Table  8 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Prompt for collecting memories from raw extracted data.",
        "table": "S5.T6.1",
        "footnotes": [],
        "references": [
            "Table  9  presents the prompt for collecting memories from raw extracted data, while Table  10  provides the prompt for generating reasoning as the required memories for QA pairs. The prompt for generating QA pairs based on this reasoning is presented in Table  11 . Additionally, Table  12  offers an alternative method to synthesize memories when raw extracted data is unavailable."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Prompt for generating reasoning as the required memories for QA pairs.",
        "table": "S5.T7.1",
        "footnotes": [],
        "references": [
            "Table  9  presents the prompt for collecting memories from raw extracted data, while Table  10  provides the prompt for generating reasoning as the required memories for QA pairs. The prompt for generating QA pairs based on this reasoning is presented in Table  11 . Additionally, Table  12  offers an alternative method to synthesize memories when raw extracted data is unavailable."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Prompt for generating QA pairs.",
        "table": "A1.T8.1",
        "footnotes": [],
        "references": [
            "Table  9  presents the prompt for collecting memories from raw extracted data, while Table  10  provides the prompt for generating reasoning as the required memories for QA pairs. The prompt for generating QA pairs based on this reasoning is presented in Table  11 . Additionally, Table  12  offers an alternative method to synthesize memories when raw extracted data is unavailable."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  Prompt for synthesizing memories.",
        "table": "A1.T8.1.8.8.2.1",
        "footnotes": [],
        "references": [
            "Table  9  presents the prompt for collecting memories from raw extracted data, while Table  10  provides the prompt for generating reasoning as the required memories for QA pairs. The prompt for generating QA pairs based on this reasoning is presented in Table  11 . Additionally, Table  12  offers an alternative method to synthesize memories when raw extracted data is unavailable."
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A1.T8.1.10.10.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A1.T8.1.10.10.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A1.T8.1.12.12.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A1.T8.1.12.12.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "A1.T8.1.13.13.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "A1.T8.1.13.13.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "A1.T8.1.14.14.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "A1.T8.1.17.17.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "",
        "table": "A1.T9.1",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "",
        "table": "A1.T10.1",
        "footnotes": [],
        "references": []
    },
    "id_table_23": {
        "caption": "",
        "table": "A1.T11.1",
        "footnotes": [],
        "references": []
    },
    "id_table_24": {
        "caption": "",
        "table": "A1.T12.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "We remark that all reported results are statistically significant, as confirmed by a t-test with",
        ".",
        "https://github.com/facebookresearch/faiss",
        "https://github.com/zilliztech/GPTCache"
    ]
}