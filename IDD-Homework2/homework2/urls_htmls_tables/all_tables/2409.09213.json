{
    "id_table_1": {
        "caption": "TABLE I:  Performance comparison of ReCLAP with baselines on Text-to-Audio and Audio-to-Text retrieval on AudioCaps and Clotho. ReCLAP outperforms baselines by 0.4%-38.9%.",
        "table": "S3.T1.3.1",
        "footnotes": [],
        "references": [
            "We present ReCLAP, a CLAP model trained using  caption augmentation . Specifically, we prompt a Large Language Model (LLM) to generate multiple diverse rewrites of the caption associated with each audio. Each rewrite describes the sounds in a unique way. Additionally, they exhibit diversity in sentence structure and vocabulary while preserving the original key concepts and meanings (example in Fig.  1  and Section  III-A ). This simple data augmentation technique has several advantages, including  (1)  It enables the model to learn about the distinct acoustic features of sound events beyond what abstract labels alone can provide. This leads to more accurate clustering of sounds based on their actual acoustic properties rather than relying solely on predefined labels.  (2)  Text-based augmentation via LLM-generated captions provides an effective and scalable method for training-time data augmentation. Unlike traditional data augmentation techniques, which typically involve random audio perturbations, our method is more interpretable and avoids the complexities and limitations of generating synthetic audio. ReCLAP achieves state-of-the-art performance across various retrieval, and ZSAC benchmarks with standard setups."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  Performance comparison of ReCLAP with baselines on Zero-shot Audio classification benchmarks. ReCLAP outperforms baselines by 0.6%-54.8%.",
        "table": "S4.T2.3.1",
        "footnotes": [],
        "references": [
            "We train ReCLAP with a training objective similar to that in CLAP but with  caption augmentation . Specifically, we augment each training sample with  K K K italic_K  additional text captions by rewriting the original caption associated with each audio sample in the dataset in  K K K italic_K  diverse ways. During training, for each audio sample, ReCLAP chooses the original caption with a probability  p = 0.4 p 0.4 p=0.4 italic_p = 0.4  or one of the rewritten versions (with a probability  1  p 1 p 1-p 1 - italic_p ) where each rewritten caption has an equal probability of selection. Thus, Eqn.  2  can be re-written as:",
            "Fig.  2  illustrates how ReCLAP and prompt augmentation increase the number of correct predictions for 4 labels from different datasets on ZSAC. As we can see, training CLAP on caption augmentation on the same dataset (CLAP-2.3M vs ReCLAP) improves retrieval of the correct label, which is further boosted by prompt augmentation."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III:  Examples of ambiguous labels where prompt augmentations provide additional context.",
        "table": "S6.T3.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_4": {
        "caption": "TABLE IV:  Impact of  N N N italic_N  on ZSAC with ReCLAP.",
        "table": "S7.T4.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_5": {
        "caption": "TABLE V:  Impact of  p p p italic_p  on ZSAC with ReCLAP.",
        "table": "S7.T5.3.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "Code and Checkpoints:"
    ]
}