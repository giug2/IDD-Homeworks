{
    "S1.T1.12.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.12.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S1.T1.12.1.1.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">\n<span class=\"ltx_rule\" style=\"width:100%;height:0.8pt;background:black;display:inline-block;\">&#160;</span><span class=\"ltx_text\" id=\"S1.T1.12.1.1.1.1\" style=\"font-size:80%;\"> Dataset</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.2\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.2.1\" style=\"font-size:80%;\">Year</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.3\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.3.1\" style=\"font-size:80%;\">Videos</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.4\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.4.1\" style=\"font-size:80%;\">Classes</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.5\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.12.1.1.5.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.5.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.5.1.1.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.5.1.1.1.1\" style=\"font-size:80%;\">Min</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.5.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.5.1.2.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.5.1.2.1.1\" style=\"font-size:80%;\">frame</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.6\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.12.1.1.6.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.6.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.6.1.1.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.6.1.1.1.1\" style=\"font-size:80%;\">Mean</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.6.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.6.1.2.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.6.1.2.1.1\" style=\"font-size:80%;\">frame</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.7\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.12.1.1.7.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.7.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.7.1.1.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.7.1.1.1.1\" style=\"font-size:80%;\">Max</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.7.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.7.1.2.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.7.1.2.1.1\" style=\"font-size:80%;\">frame</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.8\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.12.1.1.8.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.8.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.8.1.1.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.8.1.1.1.1\" style=\"font-size:80%;\">Total</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.8.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.8.1.2.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.8.1.2.1.1\" style=\"font-size:80%;\">frames</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.9\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.12.1.1.9.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.9.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.9.1.1.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.9.1.1.1.1\" style=\"font-size:80%;\">Labeled</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.9.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.9.1.2.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.9.1.2.1.1\" style=\"font-size:80%;\">frames</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.10\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.10.1\" style=\"font-size:80%;\">Annotation</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.11\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.11.1\" style=\"font-size:80%;\">Link</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.12\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.12.1\" style=\"font-size:80%;\">Underwater</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" id=\"S1.T1.12.1.2.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.12.1.2.1.1\" style=\"font-size:80%;\">CAD</span><span class=\"ltx_text\" id=\"S1.T1.12.1.2.1.2\" style=\"font-size:80%;\">&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.16902v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">bideau2016s</span> </a></cite>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.2.2\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.2.2.1\" style=\"font-size:80%;\">2016</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.2.3\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.2.3.1\" style=\"font-size:80%;\">9</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.2.4\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.2.4.1\" style=\"font-size:80%;\">6</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.2.5\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.2.5.1\" style=\"font-size:80%;\">30</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.2.6\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.2.6.1\" style=\"font-size:80%;\">93</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.2.7\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.2.7.1\" style=\"font-size:80%;\">218</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.2.8\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.2.8.1\" style=\"font-size:80%;\">836</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.2.9\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.2.9.1\" style=\"font-size:80%;\">191</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.2.10\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.2.10.1\" style=\"font-size:80%;\">Mask</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.2.11\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><a class=\"ltx_ref ltx_href\" href=\"http://vis-www.cs.umass.edu/motionSegmentation/\" style=\"font-size:80%;color:#FF00FF;\" title=\"\">URL</a></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.2.12\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.2.12.1\" style=\"font-size:80%;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.3\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S1.T1.12.1.3.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.12.1.3.1.1\" style=\"font-size:80%;\">MoCA-Mask</span><span class=\"ltx_text\" id=\"S1.T1.12.1.3.1.2\" style=\"font-size:80%;\">&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.16902v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">cheng2022implicit</span> </a></cite>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.3.2\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.3.2.1\" style=\"font-size:80%;\">2022</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.3.3\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.3.3.1\" style=\"font-size:80%;\">87</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.3.4\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.3.4.1\" style=\"font-size:80%;\">45</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.3.5\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.3.5.1\" style=\"font-size:80%;\">23</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.3.6\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.3.6.1\" style=\"font-size:80%;\">264</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.3.7\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.3.7.1\" style=\"font-size:80%;\">1,296</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.3.8\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.3.8.1\" style=\"font-size:80%;\">23 K</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.3.9\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.3.9.1\" style=\"font-size:80%;\">23 K</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.3.10\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.3.10.1\" style=\"font-size:80%;\">Mask</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.3.11\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><a class=\"ltx_ref ltx_href\" href=\"https://xueliancheng.github.io/SLT-Net-project/\" style=\"font-size:80%;color:#FF00FF;\" title=\"\">URL</a></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.3.12\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.3.12.1\" style=\"font-size:80%;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.4\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S1.T1.12.1.4.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.12.1.4.1.1\" style=\"font-size:80%;\">COTD</span><span class=\"ltx_text\" id=\"S1.T1.12.1.4.1.2\" style=\"font-size:80%;\">&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.16902v1#bib.bib14\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">guo2024cotd</span> </a></cite>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.4.2\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.4.2.1\" style=\"font-size:80%;\">2024</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.4.3\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.4.3.1\" style=\"font-size:80%;\">200</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.4.4\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.4.4.1\" style=\"font-size:80%;\">20</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.4.5\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.4.5.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.4.6\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.4.6.1\" style=\"font-size:80%;\">400</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.4.7\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.4.7.1\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.4.8\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.4.8.1\" style=\"font-size:80%;\">80 K</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.4.9\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.4.9.1\" style=\"font-size:80%;\">80 K</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.4.10\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.4.10.1\" style=\"font-size:80%;\">BBox</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.4.11\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><a class=\"ltx_ref ltx_href\" href=\"https://github.com/openat25/HIPTrack-MLS\" style=\"font-size:80%;color:#FF00FF;\" title=\"\">URL</a></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.4.12\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.4.12.1\" style=\"font-size:80%;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.5\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" id=\"S1.T1.12.1.5.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.12.1.5.1.1\" style=\"font-size:80%;\">UW-COT (Ours)</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.5.2\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.5.2.1\" style=\"font-size:80%;\">2024</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.5.3\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.5.3.1\" style=\"font-size:80%;\">220</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.5.4\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.5.4.1\" style=\"font-size:80%;\">96</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.5.5\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.5.5.1\" style=\"font-size:80%;\">10</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.5.6\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.5.6.1\" style=\"font-size:80%;\">722</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.5.7\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.5.7.1\" style=\"font-size:80%;\">7,448</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.5.8\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.5.8.1\" style=\"font-size:80%;\">159 K</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.5.9\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.5.9.1\" style=\"font-size:80%;\">159 K</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.5.10\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.5.10.1\" style=\"font-size:80%;\">BBox+Mask</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.5.11\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><a class=\"ltx_ref ltx_href\" href=\"https://github.com/983632847/Awesome-Multimodal-Object-Tracking\" style=\"font-size:80%;color:#FF00FF;\" title=\"\">URL</a></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.12.1.5.12\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.5.12.1\" style=\"font-size:80%;\">&#10003;</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.6\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left\" id=\"S1.T1.12.1.6.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_rule\" style=\"width:100%;height:0.8pt;background:black;display:inline-block;\">&#160;</span></td>\n<td class=\"ltx_td\" id=\"S1.T1.12.1.6.2\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td\" id=\"S1.T1.12.1.6.3\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td\" id=\"S1.T1.12.1.6.4\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td\" id=\"S1.T1.12.1.6.5\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td\" id=\"S1.T1.12.1.6.6\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td\" id=\"S1.T1.12.1.6.7\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td\" id=\"S1.T1.12.1.6.8\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td\" id=\"S1.T1.12.1.6.9\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td\" id=\"S1.T1.12.1.6.10\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td\" id=\"S1.T1.12.1.6.11\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td\" id=\"S1.T1.12.1.6.12\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n</tr>\n</table>\n\n",
        "caption": "Table 1:  Comparison of UW-COT with existing camouflaged object tracking dataset (COTDÂ  guo2024cotd   ) and video camouflaged object detection datasets (CADÂ  bideau2016s    and MoCA-MaskÂ  cheng2022implicit   ). UW-COT is currently the largest underwater camouflaged object tracking dataset.",
        "footnotes": [
            "(14) \nXiaoyu Guo, Pengzhi Zhong, Hao Zhang, Ling Huang, Defeng Huang, and Shuiwang Li.\n \n Camouflaged object tracking: A benchmark.\n \n arXiv preprint arXiv:2408.13877 , 2024.\n \n",
            "(4) \nPia Bideau and Erik Learned-Miller.\n \n Itâ€™s moving! a probabilistic model for causal motion segmentation in moving camera videos.\n \n In  Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14 , pages 433â€“449. Springer, 2016.\n \n",
            "(7) \nXuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, and Zongyuan Ge.\n \n Implicit motion handling for video camouflaged object detection.\n \n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 13864â€“13873, 2022.\n \n",
            "(4) \nPia Bideau and Erik Learned-Miller.\n \n Itâ€™s moving! a probabilistic model for causal motion segmentation in moving camera videos.\n \n In  Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14 , pages 433â€“449. Springer, 2016.\n \n",
            "(7) \nXuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, and Zongyuan Ge.\n \n Implicit motion handling for video camouflaged object detection.\n \n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 13864â€“13873, 2022.\n \n",
            "(14) \nXiaoyu Guo, Pengzhi Zhong, Hao Zhang, Ling Huang, Defeng Huang, and Shuiwang Li.\n \n Camouflaged object tracking: A benchmark.\n \n arXiv preprint arXiv:2408.13877 , 2024.\n \n"
        ],
        "references": [
            "Following the latest large-scale underwater object tracking dataset (i.e., WebUOT-1MÂ zhang2024webuot ), we take a step forward and construct the first underwater camouflaged object tracking dataset, UW-COT (see Fig.Â 1 and Tab.Â 1), as a benchmark for evaluating and developing advanced underwater camouflaged object tracking methods. The dataset consists of 220 underwater video sequences, spanning 96 categories, with approximately 159,000 frames. As a single-object visual tracking task, we provide bounding box annotations for the camouflaged underwater objects in each frame. Additionally, using manually annotated bounding boxes as prompts, we generate pseudo mask annotations for the camouflaged underwater objects by leveraging advanced interactive segmentation models (e.g., HQ-SAMÂ ke2024segment  and SAM-2Â ravi2024sam ). We conduct pioneering experiments on the constructed dataset to evaluate the performance of state-of-the-art visual object tracking methods and the latest advancements in image and video segmentation. By releasing UW-COT, we aim to inspire interest in exploring various underwater vision tasks and to foster the development of advanced methods for underwater camouflaged object tracking.",
            "Our goal is to construct a large-scale underwater camouflaged object tracking dataset that involves a rich variety of categories and various real underwater scenes for evaluating and developing general underwater camouflaged object tracking methods. To achieve this, we collect underwater videos from video-sharing platforms (e.g., YouTube111https://www.youtube.com/) and existing tracking datasets (e.g., WebUOT-1MÂ zhang2024webuot  and VastTrackÂ peng2024vasttrack ), and we filter out 220 videos that contain underwater camouflaged objects. Following existing datasetsÂ huang2019got ; fan2019lasot ; zhang2022webuav ; peng2024vasttrack ; zhang2024webuot , we provide a bounding box annotation for the camouflaged object in each frame, represented as [x,y,w,h]ğ‘¥ğ‘¦ğ‘¤â„[x,y,w,h][ italic_x , italic_y , italic_w , italic_h ], where xğ‘¥xitalic_x and yğ‘¦yitalic_y denote the coordinates of the top-left corner of the object, and wğ‘¤witalic_w and hâ„hitalic_h indicate the width and height of the object. Furthermore, to enhance the accuracy of object representation, we use segmentation foundation models (e.g., HQ-SAMÂ ke2024segment  and SAM-2Â ravi2024sam ) to generate pseudo mask annotations for the camouflaged objects. Some examples and detailed statistics of the UW-COT dataset are provided in Fig.Â 1 and Tab.Â 1. To the best of our knowledge, UW-COT is the first large-scale benchmark for underwater camouflaged object tracking, featuring a diverse set of 96 categories and various challenging underwater scenes."
        ]
    },
    "S1.T1.12.1.1.5.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.12.1.1.5.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.5.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.5.1.1.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.5.1.1.1.1\" style=\"font-size:80%;\">Min</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.5.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.5.1.2.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.5.1.2.1.1\" style=\"font-size:80%;\">frame</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 1:  Comparison of UW-COT with existing camouflaged object tracking dataset (COTDÂ  guo2024cotd   ) and video camouflaged object detection datasets (CADÂ  bideau2016s    and MoCA-MaskÂ  cheng2022implicit   ). UW-COT is currently the largest underwater camouflaged object tracking dataset.",
        "footnotes": [
            "(14) \nXiaoyu Guo, Pengzhi Zhong, Hao Zhang, Ling Huang, Defeng Huang, and Shuiwang Li.\n \n Camouflaged object tracking: A benchmark.\n \n arXiv preprint arXiv:2408.13877 , 2024.\n \n",
            "(4) \nPia Bideau and Erik Learned-Miller.\n \n Itâ€™s moving! a probabilistic model for causal motion segmentation in moving camera videos.\n \n In  Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14 , pages 433â€“449. Springer, 2016.\n \n",
            "(7) \nXuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, and Zongyuan Ge.\n \n Implicit motion handling for video camouflaged object detection.\n \n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 13864â€“13873, 2022.\n \n",
            "(4) \nPia Bideau and Erik Learned-Miller.\n \n Itâ€™s moving! a probabilistic model for causal motion segmentation in moving camera videos.\n \n In  Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14 , pages 433â€“449. Springer, 2016.\n \n",
            "(7) \nXuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, and Zongyuan Ge.\n \n Implicit motion handling for video camouflaged object detection.\n \n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 13864â€“13873, 2022.\n \n",
            "(14) \nXiaoyu Guo, Pengzhi Zhong, Hao Zhang, Ling Huang, Defeng Huang, and Shuiwang Li.\n \n Camouflaged object tracking: A benchmark.\n \n arXiv preprint arXiv:2408.13877 , 2024.\n \n"
        ],
        "references": [
            "Following the latest large-scale underwater object tracking dataset (i.e., WebUOT-1MÂ zhang2024webuot ), we take a step forward and construct the first underwater camouflaged object tracking dataset, UW-COT (see Fig.Â 1 and Tab.Â 1), as a benchmark for evaluating and developing advanced underwater camouflaged object tracking methods. The dataset consists of 220 underwater video sequences, spanning 96 categories, with approximately 159,000 frames. As a single-object visual tracking task, we provide bounding box annotations for the camouflaged underwater objects in each frame. Additionally, using manually annotated bounding boxes as prompts, we generate pseudo mask annotations for the camouflaged underwater objects by leveraging advanced interactive segmentation models (e.g., HQ-SAMÂ ke2024segment  and SAM-2Â ravi2024sam ). We conduct pioneering experiments on the constructed dataset to evaluate the performance of state-of-the-art visual object tracking methods and the latest advancements in image and video segmentation. By releasing UW-COT, we aim to inspire interest in exploring various underwater vision tasks and to foster the development of advanced methods for underwater camouflaged object tracking.",
            "Our goal is to construct a large-scale underwater camouflaged object tracking dataset that involves a rich variety of categories and various real underwater scenes for evaluating and developing general underwater camouflaged object tracking methods. To achieve this, we collect underwater videos from video-sharing platforms (e.g., YouTube111https://www.youtube.com/) and existing tracking datasets (e.g., WebUOT-1MÂ zhang2024webuot  and VastTrackÂ peng2024vasttrack ), and we filter out 220 videos that contain underwater camouflaged objects. Following existing datasetsÂ huang2019got ; fan2019lasot ; zhang2022webuav ; peng2024vasttrack ; zhang2024webuot , we provide a bounding box annotation for the camouflaged object in each frame, represented as [x,y,w,h]ğ‘¥ğ‘¦ğ‘¤â„[x,y,w,h][ italic_x , italic_y , italic_w , italic_h ], where xğ‘¥xitalic_x and yğ‘¦yitalic_y denote the coordinates of the top-left corner of the object, and wğ‘¤witalic_w and hâ„hitalic_h indicate the width and height of the object. Furthermore, to enhance the accuracy of object representation, we use segmentation foundation models (e.g., HQ-SAMÂ ke2024segment  and SAM-2Â ravi2024sam ) to generate pseudo mask annotations for the camouflaged objects. Some examples and detailed statistics of the UW-COT dataset are provided in Fig.Â 1 and Tab.Â 1. To the best of our knowledge, UW-COT is the first large-scale benchmark for underwater camouflaged object tracking, featuring a diverse set of 96 categories and various challenging underwater scenes."
        ]
    },
    "S1.T1.12.1.1.6.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.12.1.1.6.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.6.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.6.1.1.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.6.1.1.1.1\" style=\"font-size:80%;\">Mean</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.6.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.6.1.2.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.6.1.2.1.1\" style=\"font-size:80%;\">frame</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 1:  Comparison of UW-COT with existing camouflaged object tracking dataset (COTDÂ  guo2024cotd   ) and video camouflaged object detection datasets (CADÂ  bideau2016s    and MoCA-MaskÂ  cheng2022implicit   ). UW-COT is currently the largest underwater camouflaged object tracking dataset.",
        "footnotes": [
            "(14) \nXiaoyu Guo, Pengzhi Zhong, Hao Zhang, Ling Huang, Defeng Huang, and Shuiwang Li.\n \n Camouflaged object tracking: A benchmark.\n \n arXiv preprint arXiv:2408.13877 , 2024.\n \n",
            "(4) \nPia Bideau and Erik Learned-Miller.\n \n Itâ€™s moving! a probabilistic model for causal motion segmentation in moving camera videos.\n \n In  Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14 , pages 433â€“449. Springer, 2016.\n \n",
            "(7) \nXuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, and Zongyuan Ge.\n \n Implicit motion handling for video camouflaged object detection.\n \n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 13864â€“13873, 2022.\n \n",
            "(4) \nPia Bideau and Erik Learned-Miller.\n \n Itâ€™s moving! a probabilistic model for causal motion segmentation in moving camera videos.\n \n In  Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14 , pages 433â€“449. Springer, 2016.\n \n",
            "(7) \nXuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, and Zongyuan Ge.\n \n Implicit motion handling for video camouflaged object detection.\n \n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 13864â€“13873, 2022.\n \n",
            "(14) \nXiaoyu Guo, Pengzhi Zhong, Hao Zhang, Ling Huang, Defeng Huang, and Shuiwang Li.\n \n Camouflaged object tracking: A benchmark.\n \n arXiv preprint arXiv:2408.13877 , 2024.\n \n"
        ],
        "references": [
            "Following the latest large-scale underwater object tracking dataset (i.e., WebUOT-1MÂ zhang2024webuot ), we take a step forward and construct the first underwater camouflaged object tracking dataset, UW-COT (see Fig.Â 1 and Tab.Â 1), as a benchmark for evaluating and developing advanced underwater camouflaged object tracking methods. The dataset consists of 220 underwater video sequences, spanning 96 categories, with approximately 159,000 frames. As a single-object visual tracking task, we provide bounding box annotations for the camouflaged underwater objects in each frame. Additionally, using manually annotated bounding boxes as prompts, we generate pseudo mask annotations for the camouflaged underwater objects by leveraging advanced interactive segmentation models (e.g., HQ-SAMÂ ke2024segment  and SAM-2Â ravi2024sam ). We conduct pioneering experiments on the constructed dataset to evaluate the performance of state-of-the-art visual object tracking methods and the latest advancements in image and video segmentation. By releasing UW-COT, we aim to inspire interest in exploring various underwater vision tasks and to foster the development of advanced methods for underwater camouflaged object tracking.",
            "Our goal is to construct a large-scale underwater camouflaged object tracking dataset that involves a rich variety of categories and various real underwater scenes for evaluating and developing general underwater camouflaged object tracking methods. To achieve this, we collect underwater videos from video-sharing platforms (e.g., YouTube111https://www.youtube.com/) and existing tracking datasets (e.g., WebUOT-1MÂ zhang2024webuot  and VastTrackÂ peng2024vasttrack ), and we filter out 220 videos that contain underwater camouflaged objects. Following existing datasetsÂ huang2019got ; fan2019lasot ; zhang2022webuav ; peng2024vasttrack ; zhang2024webuot , we provide a bounding box annotation for the camouflaged object in each frame, represented as [x,y,w,h]ğ‘¥ğ‘¦ğ‘¤â„[x,y,w,h][ italic_x , italic_y , italic_w , italic_h ], where xğ‘¥xitalic_x and yğ‘¦yitalic_y denote the coordinates of the top-left corner of the object, and wğ‘¤witalic_w and hâ„hitalic_h indicate the width and height of the object. Furthermore, to enhance the accuracy of object representation, we use segmentation foundation models (e.g., HQ-SAMÂ ke2024segment  and SAM-2Â ravi2024sam ) to generate pseudo mask annotations for the camouflaged objects. Some examples and detailed statistics of the UW-COT dataset are provided in Fig.Â 1 and Tab.Â 1. To the best of our knowledge, UW-COT is the first large-scale benchmark for underwater camouflaged object tracking, featuring a diverse set of 96 categories and various challenging underwater scenes."
        ]
    },
    "S1.T1.12.1.1.7.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.12.1.1.7.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.7.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.7.1.1.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.7.1.1.1.1\" style=\"font-size:80%;\">Max</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.7.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.7.1.2.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.7.1.2.1.1\" style=\"font-size:80%;\">frame</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 1:  Comparison of UW-COT with existing camouflaged object tracking dataset (COTDÂ  guo2024cotd   ) and video camouflaged object detection datasets (CADÂ  bideau2016s    and MoCA-MaskÂ  cheng2022implicit   ). UW-COT is currently the largest underwater camouflaged object tracking dataset.",
        "footnotes": [
            "(14) \nXiaoyu Guo, Pengzhi Zhong, Hao Zhang, Ling Huang, Defeng Huang, and Shuiwang Li.\n \n Camouflaged object tracking: A benchmark.\n \n arXiv preprint arXiv:2408.13877 , 2024.\n \n",
            "(4) \nPia Bideau and Erik Learned-Miller.\n \n Itâ€™s moving! a probabilistic model for causal motion segmentation in moving camera videos.\n \n In  Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14 , pages 433â€“449. Springer, 2016.\n \n",
            "(7) \nXuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, and Zongyuan Ge.\n \n Implicit motion handling for video camouflaged object detection.\n \n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 13864â€“13873, 2022.\n \n",
            "(4) \nPia Bideau and Erik Learned-Miller.\n \n Itâ€™s moving! a probabilistic model for causal motion segmentation in moving camera videos.\n \n In  Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14 , pages 433â€“449. Springer, 2016.\n \n",
            "(7) \nXuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, and Zongyuan Ge.\n \n Implicit motion handling for video camouflaged object detection.\n \n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 13864â€“13873, 2022.\n \n",
            "(14) \nXiaoyu Guo, Pengzhi Zhong, Hao Zhang, Ling Huang, Defeng Huang, and Shuiwang Li.\n \n Camouflaged object tracking: A benchmark.\n \n arXiv preprint arXiv:2408.13877 , 2024.\n \n"
        ],
        "references": [
            "Following the latest large-scale underwater object tracking dataset (i.e., WebUOT-1MÂ zhang2024webuot ), we take a step forward and construct the first underwater camouflaged object tracking dataset, UW-COT (see Fig.Â 1 and Tab.Â 1), as a benchmark for evaluating and developing advanced underwater camouflaged object tracking methods. The dataset consists of 220 underwater video sequences, spanning 96 categories, with approximately 159,000 frames. As a single-object visual tracking task, we provide bounding box annotations for the camouflaged underwater objects in each frame. Additionally, using manually annotated bounding boxes as prompts, we generate pseudo mask annotations for the camouflaged underwater objects by leveraging advanced interactive segmentation models (e.g., HQ-SAMÂ ke2024segment  and SAM-2Â ravi2024sam ). We conduct pioneering experiments on the constructed dataset to evaluate the performance of state-of-the-art visual object tracking methods and the latest advancements in image and video segmentation. By releasing UW-COT, we aim to inspire interest in exploring various underwater vision tasks and to foster the development of advanced methods for underwater camouflaged object tracking.",
            "Our goal is to construct a large-scale underwater camouflaged object tracking dataset that involves a rich variety of categories and various real underwater scenes for evaluating and developing general underwater camouflaged object tracking methods. To achieve this, we collect underwater videos from video-sharing platforms (e.g., YouTube111https://www.youtube.com/) and existing tracking datasets (e.g., WebUOT-1MÂ zhang2024webuot  and VastTrackÂ peng2024vasttrack ), and we filter out 220 videos that contain underwater camouflaged objects. Following existing datasetsÂ huang2019got ; fan2019lasot ; zhang2022webuav ; peng2024vasttrack ; zhang2024webuot , we provide a bounding box annotation for the camouflaged object in each frame, represented as [x,y,w,h]ğ‘¥ğ‘¦ğ‘¤â„[x,y,w,h][ italic_x , italic_y , italic_w , italic_h ], where xğ‘¥xitalic_x and yğ‘¦yitalic_y denote the coordinates of the top-left corner of the object, and wğ‘¤witalic_w and hâ„hitalic_h indicate the width and height of the object. Furthermore, to enhance the accuracy of object representation, we use segmentation foundation models (e.g., HQ-SAMÂ ke2024segment  and SAM-2Â ravi2024sam ) to generate pseudo mask annotations for the camouflaged objects. Some examples and detailed statistics of the UW-COT dataset are provided in Fig.Â 1 and Tab.Â 1. To the best of our knowledge, UW-COT is the first large-scale benchmark for underwater camouflaged object tracking, featuring a diverse set of 96 categories and various challenging underwater scenes."
        ]
    },
    "S1.T1.12.1.1.8.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.12.1.1.8.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.8.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.8.1.1.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.8.1.1.1.1\" style=\"font-size:80%;\">Total</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.8.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.8.1.2.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.8.1.2.1.1\" style=\"font-size:80%;\">frames</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 1:  Comparison of UW-COT with existing camouflaged object tracking dataset (COTDÂ  guo2024cotd   ) and video camouflaged object detection datasets (CADÂ  bideau2016s    and MoCA-MaskÂ  cheng2022implicit   ). UW-COT is currently the largest underwater camouflaged object tracking dataset.",
        "footnotes": [
            "(14) \nXiaoyu Guo, Pengzhi Zhong, Hao Zhang, Ling Huang, Defeng Huang, and Shuiwang Li.\n \n Camouflaged object tracking: A benchmark.\n \n arXiv preprint arXiv:2408.13877 , 2024.\n \n",
            "(4) \nPia Bideau and Erik Learned-Miller.\n \n Itâ€™s moving! a probabilistic model for causal motion segmentation in moving camera videos.\n \n In  Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14 , pages 433â€“449. Springer, 2016.\n \n",
            "(7) \nXuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, and Zongyuan Ge.\n \n Implicit motion handling for video camouflaged object detection.\n \n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 13864â€“13873, 2022.\n \n",
            "(4) \nPia Bideau and Erik Learned-Miller.\n \n Itâ€™s moving! a probabilistic model for causal motion segmentation in moving camera videos.\n \n In  Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14 , pages 433â€“449. Springer, 2016.\n \n",
            "(7) \nXuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, and Zongyuan Ge.\n \n Implicit motion handling for video camouflaged object detection.\n \n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 13864â€“13873, 2022.\n \n",
            "(14) \nXiaoyu Guo, Pengzhi Zhong, Hao Zhang, Ling Huang, Defeng Huang, and Shuiwang Li.\n \n Camouflaged object tracking: A benchmark.\n \n arXiv preprint arXiv:2408.13877 , 2024.\n \n"
        ],
        "references": [
            "Following the latest large-scale underwater object tracking dataset (i.e., WebUOT-1MÂ zhang2024webuot ), we take a step forward and construct the first underwater camouflaged object tracking dataset, UW-COT (see Fig.Â 1 and Tab.Â 1), as a benchmark for evaluating and developing advanced underwater camouflaged object tracking methods. The dataset consists of 220 underwater video sequences, spanning 96 categories, with approximately 159,000 frames. As a single-object visual tracking task, we provide bounding box annotations for the camouflaged underwater objects in each frame. Additionally, using manually annotated bounding boxes as prompts, we generate pseudo mask annotations for the camouflaged underwater objects by leveraging advanced interactive segmentation models (e.g., HQ-SAMÂ ke2024segment  and SAM-2Â ravi2024sam ). We conduct pioneering experiments on the constructed dataset to evaluate the performance of state-of-the-art visual object tracking methods and the latest advancements in image and video segmentation. By releasing UW-COT, we aim to inspire interest in exploring various underwater vision tasks and to foster the development of advanced methods for underwater camouflaged object tracking.",
            "Our goal is to construct a large-scale underwater camouflaged object tracking dataset that involves a rich variety of categories and various real underwater scenes for evaluating and developing general underwater camouflaged object tracking methods. To achieve this, we collect underwater videos from video-sharing platforms (e.g., YouTube111https://www.youtube.com/) and existing tracking datasets (e.g., WebUOT-1MÂ zhang2024webuot  and VastTrackÂ peng2024vasttrack ), and we filter out 220 videos that contain underwater camouflaged objects. Following existing datasetsÂ huang2019got ; fan2019lasot ; zhang2022webuav ; peng2024vasttrack ; zhang2024webuot , we provide a bounding box annotation for the camouflaged object in each frame, represented as [x,y,w,h]ğ‘¥ğ‘¦ğ‘¤â„[x,y,w,h][ italic_x , italic_y , italic_w , italic_h ], where xğ‘¥xitalic_x and yğ‘¦yitalic_y denote the coordinates of the top-left corner of the object, and wğ‘¤witalic_w and hâ„hitalic_h indicate the width and height of the object. Furthermore, to enhance the accuracy of object representation, we use segmentation foundation models (e.g., HQ-SAMÂ ke2024segment  and SAM-2Â ravi2024sam ) to generate pseudo mask annotations for the camouflaged objects. Some examples and detailed statistics of the UW-COT dataset are provided in Fig.Â 1 and Tab.Â 1. To the best of our knowledge, UW-COT is the first large-scale benchmark for underwater camouflaged object tracking, featuring a diverse set of 96 categories and various challenging underwater scenes."
        ]
    },
    "S1.T1.12.1.1.9.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.12.1.1.9.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.9.1.1\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.9.1.1.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.9.1.1.1.1\" style=\"font-size:80%;\">Labeled</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.12.1.1.9.1.2\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" id=\"S1.T1.12.1.1.9.1.2.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" id=\"S1.T1.12.1.1.9.1.2.1.1\" style=\"font-size:80%;\">frames</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 1:  Comparison of UW-COT with existing camouflaged object tracking dataset (COTDÂ  guo2024cotd   ) and video camouflaged object detection datasets (CADÂ  bideau2016s    and MoCA-MaskÂ  cheng2022implicit   ). UW-COT is currently the largest underwater camouflaged object tracking dataset.",
        "footnotes": [
            "(14) \nXiaoyu Guo, Pengzhi Zhong, Hao Zhang, Ling Huang, Defeng Huang, and Shuiwang Li.\n \n Camouflaged object tracking: A benchmark.\n \n arXiv preprint arXiv:2408.13877 , 2024.\n \n",
            "(4) \nPia Bideau and Erik Learned-Miller.\n \n Itâ€™s moving! a probabilistic model for causal motion segmentation in moving camera videos.\n \n In  Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14 , pages 433â€“449. Springer, 2016.\n \n",
            "(7) \nXuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, and Zongyuan Ge.\n \n Implicit motion handling for video camouflaged object detection.\n \n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 13864â€“13873, 2022.\n \n",
            "(4) \nPia Bideau and Erik Learned-Miller.\n \n Itâ€™s moving! a probabilistic model for causal motion segmentation in moving camera videos.\n \n In  Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14 , pages 433â€“449. Springer, 2016.\n \n",
            "(7) \nXuelian Cheng, Huan Xiong, Deng-Ping Fan, Yiran Zhong, Mehrtash Harandi, Tom Drummond, and Zongyuan Ge.\n \n Implicit motion handling for video camouflaged object detection.\n \n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 13864â€“13873, 2022.\n \n",
            "(14) \nXiaoyu Guo, Pengzhi Zhong, Hao Zhang, Ling Huang, Defeng Huang, and Shuiwang Li.\n \n Camouflaged object tracking: A benchmark.\n \n arXiv preprint arXiv:2408.13877 , 2024.\n \n"
        ],
        "references": [
            "Following the latest large-scale underwater object tracking dataset (i.e., WebUOT-1MÂ zhang2024webuot ), we take a step forward and construct the first underwater camouflaged object tracking dataset, UW-COT (see Fig.Â 1 and Tab.Â 1), as a benchmark for evaluating and developing advanced underwater camouflaged object tracking methods. The dataset consists of 220 underwater video sequences, spanning 96 categories, with approximately 159,000 frames. As a single-object visual tracking task, we provide bounding box annotations for the camouflaged underwater objects in each frame. Additionally, using manually annotated bounding boxes as prompts, we generate pseudo mask annotations for the camouflaged underwater objects by leveraging advanced interactive segmentation models (e.g., HQ-SAMÂ ke2024segment  and SAM-2Â ravi2024sam ). We conduct pioneering experiments on the constructed dataset to evaluate the performance of state-of-the-art visual object tracking methods and the latest advancements in image and video segmentation. By releasing UW-COT, we aim to inspire interest in exploring various underwater vision tasks and to foster the development of advanced methods for underwater camouflaged object tracking.",
            "Our goal is to construct a large-scale underwater camouflaged object tracking dataset that involves a rich variety of categories and various real underwater scenes for evaluating and developing general underwater camouflaged object tracking methods. To achieve this, we collect underwater videos from video-sharing platforms (e.g., YouTube111https://www.youtube.com/) and existing tracking datasets (e.g., WebUOT-1MÂ zhang2024webuot  and VastTrackÂ peng2024vasttrack ), and we filter out 220 videos that contain underwater camouflaged objects. Following existing datasetsÂ huang2019got ; fan2019lasot ; zhang2022webuav ; peng2024vasttrack ; zhang2024webuot , we provide a bounding box annotation for the camouflaged object in each frame, represented as [x,y,w,h]ğ‘¥ğ‘¦ğ‘¤â„[x,y,w,h][ italic_x , italic_y , italic_w , italic_h ], where xğ‘¥xitalic_x and yğ‘¦yitalic_y denote the coordinates of the top-left corner of the object, and wğ‘¤witalic_w and hâ„hitalic_h indicate the width and height of the object. Furthermore, to enhance the accuracy of object representation, we use segmentation foundation models (e.g., HQ-SAMÂ ke2024segment  and SAM-2Â ravi2024sam ) to generate pseudo mask annotations for the camouflaged objects. Some examples and detailed statistics of the UW-COT dataset are provided in Fig.Â 1 and Tab.Â 1. To the best of our knowledge, UW-COT is the first large-scale benchmark for underwater camouflaged object tracking, featuring a diverse set of 96 categories and various challenging underwater scenes."
        ]
    },
    "S2.T2.1.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S2.T2.1.1\">\n<tr class=\"ltx_tr\" id=\"S2.T2.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T2.1.1.1.1\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">\n<span class=\"ltx_rule\" style=\"width:100%;height:0.8pt;background:black;display:inline-block;\">&#160;</span> Method</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.1.2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Point type</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.1.3\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">AUC (%)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.1.4\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">nPre (%)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.1.5\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Pre (%)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.1.6\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">cAUC (%)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.1.7\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">mACC (%)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T2.1.1.2.1\" rowspan=\"2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" id=\"S2.T2.1.1.2.1.1\">SAM 2-tiny</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.2.2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Center point</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.2.3\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">51.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.2.4\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">58.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.2.5\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">52.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.2.6\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">50.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.2.7\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">51.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.3.1\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Random point</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.3.2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">36.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.3.3\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">40.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.3.4\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">36.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.3.5\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">35.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.3.6\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">36.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T2.1.1.4.1\" rowspan=\"2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" id=\"S2.T2.1.1.4.1.1\">SAM 2-small</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.4.2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Center point</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.4.3\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">51.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.4.4\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">58.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.4.5\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">52.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.4.6\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">50.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.4.7\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">51.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.1.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.5.1\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Random point</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.5.2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">35.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.5.3\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">39.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.5.4\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">34.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.5.5\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">34.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.5.6\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">35.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T2.1.1.6.1\" rowspan=\"2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" id=\"S2.T2.1.1.6.1.1\">SAM 2-base plus</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.6.2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Center point</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.6.3\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">53.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.6.4\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">60.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.6.5\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">54.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.6.6\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">52.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.6.7\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">53.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.1.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.7.1\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Random point</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.7.2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">39.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.7.3\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">44.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.7.4\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">39.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.7.5\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">38.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.7.6\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">39.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.1.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T2.1.1.8.1\" rowspan=\"2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" id=\"S2.T2.1.1.8.1.1\">SAM 2-large</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.8.2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Center point</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.8.3\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">58.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.8.4\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">65.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.8.5\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">61.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.8.6\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">58.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.1.8.7\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">59.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.1.9\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.9.1\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Random point</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.9.2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">43.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.9.3\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">48.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.9.4\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">44.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.9.5\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">42.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T2.1.1.9.6\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">43.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T2.1.1.10.1\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_rule\" style=\"width:100%;height:0.8pt;background:black;display:inline-block;\">&#160;</span></td>\n<td class=\"ltx_td\" id=\"S2.T2.1.1.10.2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"/>\n<td class=\"ltx_td\" id=\"S2.T2.1.1.10.3\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"/>\n<td class=\"ltx_td\" id=\"S2.T2.1.1.10.4\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"/>\n<td class=\"ltx_td\" id=\"S2.T2.1.1.10.5\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"/>\n<td class=\"ltx_td\" id=\"S2.T2.1.1.10.6\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"/>\n<td class=\"ltx_td\" id=\"S2.T2.1.1.10.7\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"/>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Impact of using different points as the point prompt for SAM 2Â  ravi2024sam   . We report AUC, nPre, Pre, cAUC, and mACC scores on the UW-COT dataset.",
        "footnotes": [
            "(24) \nNikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman RÃ¤dle, Chloe Rolland, Laura Gustafson, etÂ al.\n \n Sam 2: Segment anything in images and videos.\n \n arXiv preprint arXiv:2408.00714 , 2024.\n \n"
        ],
        "references": [
            "Ablation Studies. We take SAM 2 as an example to explore the impact of different ways of point prompts (i.e., center point and random point within the initial target box) and model sizes. From Tab.Â 2, we find that using the center point as a prompt yields significantly better results than using random points. This suggests that for the interactive segmentation model SAM 2, the quality of the prompt is very important. The results in Tab.Â 3 demonstrate that larger model sizes generally lead to better performance, but the speed of the model decreases significantly. We also discovered an interesting phenomenon: when the number of model parameters is relatively small, a smaller model (e.g., SAM 2-tiny) can even outperform a larger model (e.g., SAM 2-small). We suspect this may be due to overfitting, or that small models are more sensitive to the quality of the training data."
        ]
    },
    "S2.T3.1.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S2.T3.1.1\">\n<tr class=\"ltx_tr\" id=\"S2.T3.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T3.1.1.1.1\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">\n<span class=\"ltx_rule\" style=\"width:100%;height:0.8pt;background:black;display:inline-block;\">&#160;</span> Method</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.1.2\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">Size (M)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.1.3\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">Speed (FPS)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.1.4\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">AUC (%)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.1.5\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">nPre (%)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.1.6\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">Pre (%)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.1.7\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">cAUC (%)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.1.8\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">mACC (%)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T3.1.1.2.1\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">SAM 2-tiny</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.1.1.2.2\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">38.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.1.1.2.3\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">47.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.1.1.2.4\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">51.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.1.1.2.5\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">58.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.1.1.2.6\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">52.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.1.1.2.7\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">50.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T3.1.1.2.8\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">51.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T3.1.1.3.1\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">SAM 2-small</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.3.2\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">46</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.3.3\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">43.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.3.4\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">51.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.3.5\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">58.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.3.6\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">52.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.3.7\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">50.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.3.8\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">51.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T3.1.1.4.1\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">SAM 2-base plus</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.4.2\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">80.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.4.3\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">34.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.4.4\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">53.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.4.5\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">60.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.4.6\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">54.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.4.7\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">52.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.4.8\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">53.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T3.1.1.5.1\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">SAM 2-large</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.5.2\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">224.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.5.3\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">24.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.5.4\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">58.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.5.5\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">65.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.5.6\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">61.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.5.7\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">58.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T3.1.1.5.8\" style=\"padding-left:2.3pt;padding-right:2.3pt;\">59.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T3.1.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T3.1.1.6.1\" style=\"padding-left:2.3pt;padding-right:2.3pt;\"><span class=\"ltx_rule\" style=\"width:100%;height:0.8pt;background:black;display:inline-block;\">&#160;</span></td>\n<td class=\"ltx_td\" id=\"S2.T3.1.1.6.2\" style=\"padding-left:2.3pt;padding-right:2.3pt;\"/>\n<td class=\"ltx_td\" id=\"S2.T3.1.1.6.3\" style=\"padding-left:2.3pt;padding-right:2.3pt;\"/>\n<td class=\"ltx_td\" id=\"S2.T3.1.1.6.4\" style=\"padding-left:2.3pt;padding-right:2.3pt;\"/>\n<td class=\"ltx_td\" id=\"S2.T3.1.1.6.5\" style=\"padding-left:2.3pt;padding-right:2.3pt;\"/>\n<td class=\"ltx_td\" id=\"S2.T3.1.1.6.6\" style=\"padding-left:2.3pt;padding-right:2.3pt;\"/>\n<td class=\"ltx_td\" id=\"S2.T3.1.1.6.7\" style=\"padding-left:2.3pt;padding-right:2.3pt;\"/>\n<td class=\"ltx_td\" id=\"S2.T3.1.1.6.8\" style=\"padding-left:2.3pt;padding-right:2.3pt;\"/>\n</tr>\n</table>\n\n",
        "caption": "Table 3:  Comparison of different models for SAM 2Â  ravi2024sam   . We report AUC, nPre, Pre, cAUC, and mACC scores on the UW-COT dataset.",
        "footnotes": [
            "(24) \nNikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman RÃ¤dle, Chloe Rolland, Laura Gustafson, etÂ al.\n \n Sam 2: Segment anything in images and videos.\n \n arXiv preprint arXiv:2408.00714 , 2024.\n \n"
        ],
        "references": [
            "Ablation Studies. We take SAM 2 as an example to explore the impact of different ways of point prompts (i.e., center point and random point within the initial target box) and model sizes. From Tab.Â 2, we find that using the center point as a prompt yields significantly better results than using random points. This suggests that for the interactive segmentation model SAM 2, the quality of the prompt is very important. The results in Tab.Â 3 demonstrate that larger model sizes generally lead to better performance, but the speed of the model decreases significantly. We also discovered an interesting phenomenon: when the number of model parameters is relatively small, a smaller model (e.g., SAM 2-tiny) can even outperform a larger model (e.g., SAM 2-small). We suspect this may be due to overfitting, or that small models are more sensitive to the quality of the training data."
        ]
    }
}