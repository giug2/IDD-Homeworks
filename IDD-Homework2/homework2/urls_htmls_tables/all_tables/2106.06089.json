{
    "PAPER'S NUMBER OF TABLES": 10,
    "S4.T1": {
        "caption": "Table 1: Fraction of P𝑃P reconstructed with FedAvg model updates (users=100, rounds=200, Cifar10 LeNet, participant rate=.1, granularity=10, time limit per column=10 min). We exactly reconstruct P𝑃P in the majority of FedAvg settings.",
        "table": "<table id=\"S4.T1.3.3\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.3.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Dataset Size <math id=\"S4.T1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"D\" display=\"inline\"><semantics id=\"S4.T1.1.1.1.1.m1.1a\"><mi id=\"S4.T1.1.1.1.1.m1.1.1\" xref=\"S4.T1.1.1.1.1.m1.1.1.cmml\">D</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.1.1.1.1.m1.1b\"><ci id=\"S4.T1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.1.1.1.1.m1.1.1\">𝐷</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.1.m1.1c\">D</annotation></semantics></math>\n</th>\n<th id=\"S4.T1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">Batch Size <math id=\"S4.T1.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"b\" display=\"inline\"><semantics id=\"S4.T1.2.2.2.2.m1.1a\"><mi id=\"S4.T1.2.2.2.2.m1.1.1\" xref=\"S4.T1.2.2.2.2.m1.1.1.cmml\">b</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.2.2.2.2.m1.1b\"><ci id=\"S4.T1.2.2.2.2.m1.1.1.cmml\" xref=\"S4.T1.2.2.2.2.m1.1.1\">𝑏</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.2.2.2.2.m1.1c\">b</annotation></semantics></math>\n</th>\n<th id=\"S4.T1.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"7\">Local Epochs <math id=\"S4.T1.3.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"e\" display=\"inline\"><semantics id=\"S4.T1.3.3.3.3.m1.1a\"><mi id=\"S4.T1.3.3.3.3.m1.1.1\" xref=\"S4.T1.3.3.3.3.m1.1.1.cmml\">e</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.3.3.3.3.m1.1b\"><ci id=\"S4.T1.3.3.3.3.m1.1.1.cmml\" xref=\"S4.T1.3.3.3.3.m1.1.1\">𝑒</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.3.3.3.3.m1.1c\">e</annotation></semantics></math>\n</th>\n</tr>\n<tr id=\"S4.T1.3.3.4.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.4.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S4.T1.3.3.4.1.2\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"></th>\n<th id=\"S4.T1.3.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">1</th>\n<th id=\"S4.T1.3.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">2</th>\n<th id=\"S4.T1.3.3.4.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">4</th>\n<th id=\"S4.T1.3.3.4.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">8</th>\n<th id=\"S4.T1.3.3.4.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">16</th>\n<th id=\"S4.T1.3.3.4.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">32</th>\n<th id=\"S4.T1.3.3.4.1.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">64</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.3.3.5.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T1.3.3.5.1.1.1\" class=\"ltx_text\">64</span></th>\n<th id=\"S4.T1.3.3.5.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">8</th>\n<td id=\"S4.T1.3.3.5.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.5.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.5.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.5.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.5.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.5.1.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.5.1.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n</tr>\n<tr id=\"S4.T1.3.3.6.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.6.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">16</th>\n<td id=\"S4.T1.3.3.6.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.6.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.6.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.6.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.6.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.6.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.6.2.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n</tr>\n<tr id=\"S4.T1.3.3.7.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.7.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">32</th>\n<td id=\"S4.T1.3.3.7.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.7.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.7.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.7.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.7.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.7.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.7.3.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n</tr>\n<tr id=\"S4.T1.3.3.8.4\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.8.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T1.3.3.8.4.1.1\" class=\"ltx_text\">128</span></th>\n<th id=\"S4.T1.3.3.8.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">8</th>\n<td id=\"S4.T1.3.3.8.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.8.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.8.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.8.4.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.8.4.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.8.4.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.8.4.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n</tr>\n<tr id=\"S4.T1.3.3.9.5\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.9.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">16</th>\n<td id=\"S4.T1.3.3.9.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.9.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.9.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.9.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.9.5.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.9.5.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.9.5.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n</tr>\n<tr id=\"S4.T1.3.3.10.6\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.10.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">32</th>\n<td id=\"S4.T1.3.3.10.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.10.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.10.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.10.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.10.6.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.10.6.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.10.6.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n</tr>\n<tr id=\"S4.T1.3.3.11.7\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.11.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T1.3.3.11.7.1.1\" class=\"ltx_text\">64 (momentum=.9)</span></th>\n<th id=\"S4.T1.3.3.11.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">8</th>\n<td id=\"S4.T1.3.3.11.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n<td id=\"S4.T1.3.3.11.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.11.7.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.11.7.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.11.7.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.11.7.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.11.7.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n</tr>\n<tr id=\"S4.T1.3.3.12.8\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.12.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">16</th>\n<td id=\"S4.T1.3.3.12.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.12.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.12.8.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.12.8.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.12.8.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.12.8.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.12.8.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n</tr>\n<tr id=\"S4.T1.3.3.13.9\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.13.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">32</th>\n<td id=\"S4.T1.3.3.13.9.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.13.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.13.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.13.9.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.13.9.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.13.9.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.13.9.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n</tr>\n<tr id=\"S4.T1.3.3.14.10\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.14.10.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T1.3.3.14.10.1.1\" class=\"ltx_text\">128 (momentum=.9)</span></th>\n<th id=\"S4.T1.3.3.14.10.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">8</th>\n<td id=\"S4.T1.3.3.14.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.14.10.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.14.10.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.14.10.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.14.10.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.14.10.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.14.10.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.96</td>\n</tr>\n<tr id=\"S4.T1.3.3.15.11\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.15.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">16</th>\n<td id=\"S4.T1.3.3.15.11.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.15.11.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.15.11.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.15.11.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.15.11.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.15.11.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.15.11.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n</tr>\n<tr id=\"S4.T1.3.3.16.12\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.16.12.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">32</th>\n<td id=\"S4.T1.3.3.16.12.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.16.12.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.16.12.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.16.12.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.16.12.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.16.12.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.16.12.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n</tr>\n<tr id=\"S4.T1.3.3.17.13\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.17.13.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T1.3.3.17.13.1.1\" class=\"ltx_text\">64 (fraction=.9)</span></th>\n<th id=\"S4.T1.3.3.17.13.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">8</th>\n<td id=\"S4.T1.3.3.17.13.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.06</td>\n<td id=\"S4.T1.3.3.17.13.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.66</td>\n<td id=\"S4.T1.3.3.17.13.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.97</td>\n<td id=\"S4.T1.3.3.17.13.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n<td id=\"S4.T1.3.3.17.13.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.98</td>\n<td id=\"S4.T1.3.3.17.13.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n<td id=\"S4.T1.3.3.17.13.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.85</td>\n</tr>\n<tr id=\"S4.T1.3.3.18.14\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.18.14.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">16</th>\n<td id=\"S4.T1.3.3.18.14.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.18.14.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.18.14.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.18.14.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.18.14.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.18.14.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.18.14.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n</tr>\n<tr id=\"S4.T1.3.3.19.15\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.19.15.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">32</th>\n<td id=\"S4.T1.3.3.19.15.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.19.15.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.19.15.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.19.15.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.19.15.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.19.15.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.19.15.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n</tr>\n<tr id=\"S4.T1.3.3.20.16\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.20.16.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T1.3.3.20.16.1.1\" class=\"ltx_text\">128 (fraction=.9)</span></th>\n<th id=\"S4.T1.3.3.20.16.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">8</th>\n<td id=\"S4.T1.3.3.20.16.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.20.16.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.20.16.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.20.16.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.20.16.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.20.16.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.20.16.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.90</td>\n</tr>\n<tr id=\"S4.T1.3.3.21.17\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.21.17.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">16</th>\n<td id=\"S4.T1.3.3.21.17.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.59</td>\n<td id=\"S4.T1.3.3.21.17.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.96</td>\n<td id=\"S4.T1.3.3.21.17.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.21.17.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.21.17.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.21.17.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.21.17.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n</tr>\n<tr id=\"S4.T1.3.3.22.18\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.3.22.18.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">32</th>\n<td id=\"S4.T1.3.3.22.18.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.22.18.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.22.18.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.22.18.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.22.18.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.22.18.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S4.T1.3.3.22.18.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Additionally, we perform experiments on gradient disaggregation using model updates generated by the FedAvg algorithm (McMahan et al., 2017), on Cifar10 (Krizhevsky, 2009) with a LeNet neural network (SGD lr=.01). FedAvg performs multiple epochs of training over the participant’s dataset before sending the final model difference back to the central server. We evaluate gradient disaggregation on updates generated by FedAvg over various parameter settings: local batchsize b𝑏b, epochs e𝑒e, user dataset size D𝐷D (see (McMahan et al., 2017) for more details on these parameters); additionally, we simulate a shift in data distribution by randomly sampling a fraction f𝑓f of participants’ total data set during computation of model updates; finally we test disaggregation on updates generated with and without SGD momentum m𝑚m. Figure 6(b) shows that relative variance of model updates (D=128𝐷128D=128) increases with epochs of training, with momentum and with a shifting data distribution. However, as Table 1 shows we can reconstruct P𝑃P exactly in nearly all cases. The failure cases happen at lower (≤\\leq 1) or higher epochs (≥\\geq 64) of training. At lower epochs, we believe parameters of the update are smaller and less distinguished from each other, making reconstruction more difficult; at higher epochs, reconstruction is more difficult as updates are more noisy. With 2−322322-32 epochs, we are generally able to exactly recover P𝑃P across the settings."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Corresponding PSNR scores against ground truth for Figure 7",
        "table": "<table id=\"S4.T2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.1.1.1\" class=\"ltx_td ltx_border_l ltx_border_r ltx_border_t\"></td>\n<td id=\"S4.T2.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">users=1</td>\n<td id=\"S4.T2.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">users=2</td>\n<td id=\"S4.T2.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">users=4</td>\n<td id=\"S4.T2.1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">users=32</td>\n</tr>\n<tr id=\"S4.T2.1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">PSNR</td>\n<td id=\"S4.T2.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">36.5</td>\n<td id=\"S4.T2.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">18.8</td>\n<td id=\"S4.T2.1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">13.9</td>\n<td id=\"S4.T2.1.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">6.1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We perform the attack in (Zhu et al., 2019) on an MLP network on Cifar100 and show the effect of inversion with and without gradient disaggregation across multiple users with each user having 1 image in their dataset (submitting full gradients of that image). Figure 7 shows the closest reconstructed image to a user’s data example and Table 2 shows the corresponding PSNR achieved. With gradient disaggregation, we recover the target user’s exact gradient and hence the reconstructed image is high quality. Without disaggregation, reconstruction quality degrades significantly."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Corresponding PSNR scores against ground truth for Figure 8.",
        "table": "<table id=\"S4.T3.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S4.T3.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">users=1</th>\n<th id=\"S4.T3.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">users=10</th>\n<th id=\"S4.T3.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.1.1.1.4.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T3.1.1.1.1.4.1.1\" class=\"ltx_p\">users=100</span>\n<span id=\"S4.T3.1.1.1.1.4.1.2\" class=\"ltx_p\">(disaggregated)</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">PSNR</td>\n<td id=\"S4.T3.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">16.0</td>\n<td id=\"S4.T3.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">13.3</td>\n<td id=\"S4.T3.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">18.6</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We furthermore perform the attack in (Geiping et al., 2020) to invert noisy FedAvg updates. Figure 8 and Table 3 show the results of inverting fedavg updates with local epochs = 4, batch size = 16, user data set size = 64, with and without gradient disaggregation (100 users, 2 layer MLP). With gradient disaggregation we achieve similar quality as inverting a single model update, whereas inverting an update aggregated over multiple users (users=10) significantly degrades reconstruction quality."
        ]
    },
    "S1.T1": {
        "caption": "Table S1: ”Honest but curious” gradient disaggregation on FedAvg updates: neural network model is updated with participants’ updates via FedAvg (SGD lr=1e-3) every round. Even with extra added noise from the changing model, our gradient disaggregation attack may reconstruct P𝑃P exactly in a good portion of settings.",
        "table": "<table id=\"S1.T1.3.3\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S1.T1.3.3.3\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Dataset Size <math id=\"S1.T1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"D\" display=\"inline\"><semantics id=\"S1.T1.1.1.1.1.m1.1a\"><mi id=\"S1.T1.1.1.1.1.m1.1.1\" xref=\"S1.T1.1.1.1.1.m1.1.1.cmml\">D</mi><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.1.1.1.1.m1.1b\"><ci id=\"S1.T1.1.1.1.1.m1.1.1.cmml\" xref=\"S1.T1.1.1.1.1.m1.1.1\">𝐷</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.1.1.1.1.m1.1c\">D</annotation></semantics></math>\n</td>\n<td id=\"S1.T1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Batch Size <math id=\"S1.T1.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"b\" display=\"inline\"><semantics id=\"S1.T1.2.2.2.2.m1.1a\"><mi id=\"S1.T1.2.2.2.2.m1.1.1\" xref=\"S1.T1.2.2.2.2.m1.1.1.cmml\">b</mi><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.2.2.2.2.m1.1b\"><ci id=\"S1.T1.2.2.2.2.m1.1.1.cmml\" xref=\"S1.T1.2.2.2.2.m1.1.1\">𝑏</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.2.2.2.2.m1.1c\">b</annotation></semantics></math>\n</td>\n<td id=\"S1.T1.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"6\">Local Epochs <math id=\"S1.T1.3.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"e\" display=\"inline\"><semantics id=\"S1.T1.3.3.3.3.m1.1a\"><mi id=\"S1.T1.3.3.3.3.m1.1.1\" xref=\"S1.T1.3.3.3.3.m1.1.1.cmml\">e</mi><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.3.3.3.3.m1.1b\"><ci id=\"S1.T1.3.3.3.3.m1.1.1.cmml\" xref=\"S1.T1.3.3.3.3.m1.1.1\">𝑒</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.3.3.3.3.m1.1c\">e</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S1.T1.3.3.4.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.3.4.1.1\" class=\"ltx_td ltx_border_l ltx_border_r ltx_border_t\"></td>\n<td id=\"S1.T1.3.3.4.1.2\" class=\"ltx_td ltx_border_r ltx_border_t\"></td>\n<td id=\"S1.T1.3.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S1.T1.3.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S1.T1.3.3.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4</td>\n<td id=\"S1.T1.3.3.4.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8</td>\n<td id=\"S1.T1.3.3.4.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16</td>\n<td id=\"S1.T1.3.3.4.1.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32</td>\n</tr>\n<tr id=\"S1.T1.3.3.5.2\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.3.5.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S1.T1.3.3.5.2.1.1\" class=\"ltx_text\">64</span></td>\n<td id=\"S1.T1.3.3.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8</td>\n<td id=\"S1.T1.3.3.5.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S1.T1.3.3.5.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S1.T1.3.3.5.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.30</td>\n<td id=\"S1.T1.3.3.5.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.0</td>\n<td id=\"S1.T1.3.3.5.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.0</td>\n<td id=\"S1.T1.3.3.5.2.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.0</td>\n</tr>\n<tr id=\"S1.T1.3.3.6.3\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.3.6.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16</td>\n<td id=\"S1.T1.3.3.6.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S1.T1.3.3.6.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S1.T1.3.3.6.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S1.T1.3.3.6.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.35</td>\n<td id=\"S1.T1.3.3.6.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.0</td>\n<td id=\"S1.T1.3.3.6.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.0</td>\n</tr>\n<tr id=\"S1.T1.3.3.7.4\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.3.7.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32</td>\n<td id=\"S1.T1.3.3.7.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S1.T1.3.3.7.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S1.T1.3.3.7.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S1.T1.3.3.7.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S1.T1.3.3.7.4.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.35</td>\n<td id=\"S1.T1.3.3.7.4.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.0</td>\n</tr>\n<tr id=\"S1.T1.3.3.8.5\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.3.8.5.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S1.T1.3.3.8.5.1.1\" class=\"ltx_text\">128</span></td>\n<td id=\"S1.T1.3.3.8.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8</td>\n<td id=\"S1.T1.3.3.8.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S1.T1.3.3.8.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.52</td>\n<td id=\"S1.T1.3.3.8.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.0</td>\n<td id=\"S1.T1.3.3.8.5.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.0</td>\n<td id=\"S1.T1.3.3.8.5.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.0</td>\n<td id=\"S1.T1.3.3.8.5.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.0</td>\n</tr>\n<tr id=\"S1.T1.3.3.9.6\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.3.9.6.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16</td>\n<td id=\"S1.T1.3.3.9.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S1.T1.3.3.9.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S1.T1.3.3.9.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.46</td>\n<td id=\"S1.T1.3.3.9.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.0</td>\n<td id=\"S1.T1.3.3.9.6.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.0</td>\n<td id=\"S1.T1.3.3.9.6.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.0</td>\n</tr>\n<tr id=\"S1.T1.3.3.10.7\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.3.10.7.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">32</td>\n<td id=\"S1.T1.3.3.10.7.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S1.T1.3.3.10.7.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S1.T1.3.3.10.7.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.0</td>\n<td id=\"S1.T1.3.3.10.7.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">.49</td>\n<td id=\"S1.T1.3.3.10.7.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.0</td>\n<td id=\"S1.T1.3.3.10.7.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Our work on gradient disaggregation and the work in (Fleder & Shah, 2020) solve the same core problem: uncovering individual values given observations of their sums. While their work recovers prices of items, our work analogically reconstructs participants’ model gradients. However, a key distinction in (Fleder & Shah, 2020) is the assumption that each item must be purchased individually at least once. This makes their approach unsuitable for disaggregating aggregated model updates as, under the secure aggregation protocol, each aggregated update is composed of more than one participant’s model updates.",
            "We note that it is possible that devices timestamp the exact moment they perform a round of training; in this case, P𝑃P may be revealed directly through the specificity of the constraints (making the disaggregation problem solvable through a simple linear regression). However, even if devices log only the total number of times they performed training (with no timestamped data) and send these analytics back to the server once every few rounds of participation, the central server may piece together these constraints and incorporate them into the formulation above. In other words, just knowing the number of times particular users performed training and collecting this information periodically (both of which are reasonable based on (Bonawitz et al., 2019)), the central server may obtain enough information to carry out the gradient disaggregation attack. Incorporating summary analytics into the gradient disaggregation attack is significant as it greatly reduces the problem space, allowing a solution to a previously intractable problem.",
            "Additionally, we perform experiments on gradient disaggregation using model updates generated by the FedAvg algorithm (McMahan et al., 2017), on Cifar10 (Krizhevsky, 2009) with a LeNet neural network (SGD lr=.01). FedAvg performs multiple epochs of training over the participant’s dataset before sending the final model difference back to the central server. We evaluate gradient disaggregation on updates generated by FedAvg over various parameter settings: local batchsize b𝑏b, epochs e𝑒e, user dataset size D𝐷D (see (McMahan et al., 2017) for more details on these parameters); additionally, we simulate a shift in data distribution by randomly sampling a fraction f𝑓f of participants’ total data set during computation of model updates; finally we test disaggregation on updates generated with and without SGD momentum m𝑚m. Figure 6(b) shows that relative variance of model updates (D=128𝐷128D=128) increases with epochs of training, with momentum and with a shifting data distribution. However, as Table 1 shows we can reconstruct P𝑃P exactly in nearly all cases. The failure cases happen at lower (≤\\leq 1) or higher epochs (≥\\geq 64) of training. At lower epochs, we believe parameters of the update are smaller and less distinguished from each other, making reconstruction more difficult; at higher epochs, reconstruction is more difficult as updates are more noisy. With 2−322322-32 epochs, we are generally able to exactly recover P𝑃P across the settings.",
            "We perform the attack in (Zhu et al., 2019) on an MLP network on Cifar100 and show the effect of inversion with and without gradient disaggregation across multiple users with each user having 1 image in their dataset (submitting full gradients of that image). Figure 7 shows the closest reconstructed image to a user’s data example and Table 2 shows the corresponding PSNR achieved. With gradient disaggregation, we recover the target user’s exact gradient and hence the reconstructed image is high quality. Without disaggregation, reconstruction quality degrades significantly.",
            "We furthermore perform the attack in (Geiping et al., 2020) to invert noisy FedAvg updates. Figure 8 and Table 3 show the results of inverting fedavg updates with local epochs = 4, batch size = 16, user data set size = 64, with and without gradient disaggregation (100 users, 2 layer MLP). With gradient disaggregation we achieve similar quality as inverting a single model update, whereas inverting an update aggregated over multiple users (users=10) significantly degrades reconstruction quality.",
            "We run our experiments using FedAvg model updates, on Cifar10, with 100 users, 200 rounds, participation rate of .1, constraint granularity of 10, SGD lr of 1e-3, on a LeNet CNN model. Table S1 shows the fraction of P𝑃P reconstructed across various FedAvg settings. Results show that with lower lr (1e-3), gradient disaggregation can exactly reconstruct P𝑃P when FedAvg updates are small (1-4 epochs). With more epochs of FedAvg (and larger dataset size or smaller batch size), increased noise prevents reconstruction of P𝑃P. We additionally tried the experiment with higher lr (1e-2), and disaggregation typically failed due to high noise, even with lower epochs of FedAvg. Our results indicate that under the right set of circumstances, the gradient disaggregation attack can be used in an honest but curious scenario; however, more robust results are achieved if the attacker can fix the neural network model.",
            "Table S2 shows the fraction of P𝑃P reconstructed across various proportions of dropped constraints. Results indicate that even when significant proportions of constraints are dropped, P𝑃P may be exactly recovered with more rounds of collected aggregated updates. Note that, when rounds << users reconstruction fails due to the rank being less than the number of users.",
            "Table S3 shows the results of gradient disaggregation when constraint noise 𝒩​(0,μ)𝒩0𝜇\\mathcal{N}(0,\\mu) is added to each constraint. As indicated, even in the presence of noise, gradient disaggregation may exactly recover P𝑃P with enough rounds.",
            "We provide extended data on gradient disaggregation against various parameter values of participation rate. Participation rate is the proportion of users that participate in sending model updates per round and impacts how many updates are summed to yield the aggregated update that is observed by the central server. Unless stated, we enforced a time limit of 10 minutes for solving each column; we use a constraint granularity of 10. We show our extended results in Table S4.",
            "We provide extended data on gradient disaggregation against various parameter values of constraint granularity. Constraint granularity is how precise summary statistics capture user partipation frequency (see main paper for details). Unless stated, we enforced a time limit of 10 minutes for solving each column; we use a constraint granularity of 10. We show our extended results in Table S5.",
            "We provide extended experiments on gradient disaggregation on FedAvg. We show our results in Table S6 and S7."
        ]
    },
    "S2.T2": {
        "caption": "Table S2: Fraction of P𝑃P recovered with gradient disaggregation on partial constraint information. With more rounds, we can exactly recover P𝑃P even when a significant fraction of constraints are missing.",
        "table": "<table id=\"S2.T2.5.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T2.5.1.1.1\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Users</td>\n<td id=\"S2.T2.5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Rounds</td>\n<td id=\"S2.T2.5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"10\">Constraint Fraction</td>\n</tr>\n<tr id=\"S2.T2.5.1.2.2\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.2.2.1\" class=\"ltx_td ltx_border_l ltx_border_r ltx_border_t\"></td>\n<td id=\"S2.T2.5.1.2.2.2\" class=\"ltx_td ltx_border_r ltx_border_t\"></td>\n<td id=\"S2.T2.5.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.9</td>\n<td id=\"S2.T2.5.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.8</td>\n<td id=\"S2.T2.5.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.7</td>\n<td id=\"S2.T2.5.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.6</td>\n<td id=\"S2.T2.5.1.2.2.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.5</td>\n<td id=\"S2.T2.5.1.2.2.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.4</td>\n<td id=\"S2.T2.5.1.2.2.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.3</td>\n<td id=\"S2.T2.5.1.2.2.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.2</td>\n<td id=\"S2.T2.5.1.2.2.12\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.1</td>\n</tr>\n<tr id=\"S2.T2.5.1.3.3\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S2.T2.5.1.3.3.1.1\" class=\"ltx_text\">128</span></td>\n<td id=\"S2.T2.5.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">256</td>\n<td id=\"S2.T2.5.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.3.3.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.97</td>\n<td id=\"S2.T2.5.1.3.3.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.71</td>\n<td id=\"S2.T2.5.1.3.3.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.26</td>\n<td id=\"S2.T2.5.1.3.3.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.08</td>\n<td id=\"S2.T2.5.1.3.3.12\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.03</td>\n</tr>\n<tr id=\"S2.T2.5.1.4.4\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">512</td>\n<td id=\"S2.T2.5.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.4.4.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.4.4.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.4.4.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n<td id=\"S2.T2.5.1.4.4.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.38</td>\n</tr>\n<tr id=\"S2.T2.5.1.5.5\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S2.T2.5.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.5.5.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.5.5.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.5.5.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.5.5.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.5.5.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n</tr>\n<tr id=\"S2.T2.5.1.6.6\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2048</td>\n<td id=\"S2.T2.5.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.6.6.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.6.6.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.6.6.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.6.6.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.6.6.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.6.6.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S2.T2.5.1.7.7\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4096</td>\n<td id=\"S2.T2.5.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.7.7.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.7.7.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.7.7.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.7.7.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.7.7.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.7.7.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S2.T2.5.1.8.8\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S2.T2.5.1.8.8.1.1\" class=\"ltx_text\">256</span></td>\n<td id=\"S2.T2.5.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">256</td>\n<td id=\"S2.T2.5.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.8.8.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.8.8.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.8.8.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.8.8.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.8.8.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.8.8.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.8.8.12\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S2.T2.5.1.9.9\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">512</td>\n<td id=\"S2.T2.5.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.98</td>\n<td id=\"S2.T2.5.1.9.9.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.94</td>\n<td id=\"S2.T2.5.1.9.9.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.63</td>\n<td id=\"S2.T2.5.1.9.9.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.09</td>\n<td id=\"S2.T2.5.1.9.9.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.02</td>\n<td id=\"S2.T2.5.1.9.9.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.9.9.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S2.T2.5.1.10.10\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S2.T2.5.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.10.10.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.10.10.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.10.10.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.10.10.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n<td id=\"S2.T2.5.1.10.10.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.70</td>\n<td id=\"S2.T2.5.1.10.10.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.10</td>\n</tr>\n<tr id=\"S2.T2.5.1.11.11\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2048</td>\n<td id=\"S2.T2.5.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.11.11.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.11.11.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.11.11.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.11.11.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.11.11.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.11.11.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.96</td>\n</tr>\n<tr id=\"S2.T2.5.1.12.12\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.12.12.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4096</td>\n<td id=\"S2.T2.5.1.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.12.12.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.12.12.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.12.12.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.12.12.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.12.12.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.12.12.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.12.12.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.12.12.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S2.T2.5.1.13.13\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.13.13.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S2.T2.5.1.13.13.1.1\" class=\"ltx_text\">512</span></td>\n<td id=\"S2.T2.5.1.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">256</td>\n<td id=\"S2.T2.5.1.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.13.13.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.13.13.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.13.13.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.13.13.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.13.13.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.13.13.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.13.13.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.13.13.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.13.13.12\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S2.T2.5.1.14.14\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.14.14.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">512</td>\n<td id=\"S2.T2.5.1.14.14.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.14.14.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.14.14.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.14.14.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.14.14.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.14.14.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.14.14.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.14.14.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.14.14.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.14.14.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S2.T2.5.1.15.15\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.15.15.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S2.T2.5.1.15.15.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.15.15.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n<td id=\"S2.T2.5.1.15.15.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n<td id=\"S2.T2.5.1.15.15.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.93</td>\n<td id=\"S2.T2.5.1.15.15.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.53</td>\n<td id=\"S2.T2.5.1.15.15.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.09</td>\n<td id=\"S2.T2.5.1.15.15.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.15.15.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.15.15.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.15.15.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S2.T2.5.1.16.16\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.16.16.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2048</td>\n<td id=\"S2.T2.5.1.16.16.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.16.16.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.16.16.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.16.16.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.16.16.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.16.16.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.16.16.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.16.16.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n<td id=\"S2.T2.5.1.16.16.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.55</td>\n<td id=\"S2.T2.5.1.16.16.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.01</td>\n</tr>\n<tr id=\"S2.T2.5.1.17.17\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.17.17.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4096</td>\n<td id=\"S2.T2.5.1.17.17.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.17.17.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.17.17.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.17.17.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.17.17.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.17.17.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.17.17.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.17.17.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.17.17.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.17.17.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.85</td>\n</tr>\n<tr id=\"S2.T2.5.1.18.18\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.18.18.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S2.T2.5.1.18.18.1.1\" class=\"ltx_text\">1024</span></td>\n<td id=\"S2.T2.5.1.18.18.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">256</td>\n<td id=\"S2.T2.5.1.18.18.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.18.18.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.18.18.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.18.18.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.18.18.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.18.18.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.18.18.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.18.18.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.18.18.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.18.18.12\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S2.T2.5.1.19.19\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.19.19.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">512</td>\n<td id=\"S2.T2.5.1.19.19.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.19.19.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.19.19.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.19.19.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.19.19.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.19.19.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.19.19.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.19.19.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.19.19.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.19.19.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S2.T2.5.1.20.20\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.20.20.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S2.T2.5.1.20.20.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.20.20.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.20.20.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.20.20.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.20.20.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.20.20.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.20.20.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.20.20.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.20.20.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.20.20.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S2.T2.5.1.21.21\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.21.21.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2048</td>\n<td id=\"S2.T2.5.1.21.21.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.21.21.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.21.21.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n<td id=\"S2.T2.5.1.21.21.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.95</td>\n<td id=\"S2.T2.5.1.21.21.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.41</td>\n<td id=\"S2.T2.5.1.21.21.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.01</td>\n<td id=\"S2.T2.5.1.21.21.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.21.21.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.21.21.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S2.T2.5.1.21.21.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S2.T2.5.1.22.22\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.1.22.22.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">4096</td>\n<td id=\"S2.T2.5.1.22.22.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.22.22.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.22.22.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.22.22.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.22.22.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.22.22.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S2.T2.5.1.22.22.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">.99</td>\n<td id=\"S2.T2.5.1.22.22.9\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">.81</td>\n<td id=\"S2.T2.5.1.22.22.10\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">.1</td>\n<td id=\"S2.T2.5.1.22.22.11\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Our work on gradient disaggregation and the work in (Fleder & Shah, 2020) solve the same core problem: uncovering individual values given observations of their sums. While their work recovers prices of items, our work analogically reconstructs participants’ model gradients. However, a key distinction in (Fleder & Shah, 2020) is the assumption that each item must be purchased individually at least once. This makes their approach unsuitable for disaggregating aggregated model updates as, under the secure aggregation protocol, each aggregated update is composed of more than one participant’s model updates.",
            "We note that it is possible that devices timestamp the exact moment they perform a round of training; in this case, P𝑃P may be revealed directly through the specificity of the constraints (making the disaggregation problem solvable through a simple linear regression). However, even if devices log only the total number of times they performed training (with no timestamped data) and send these analytics back to the server once every few rounds of participation, the central server may piece together these constraints and incorporate them into the formulation above. In other words, just knowing the number of times particular users performed training and collecting this information periodically (both of which are reasonable based on (Bonawitz et al., 2019)), the central server may obtain enough information to carry out the gradient disaggregation attack. Incorporating summary analytics into the gradient disaggregation attack is significant as it greatly reduces the problem space, allowing a solution to a previously intractable problem.",
            "Additionally, we perform experiments on gradient disaggregation using model updates generated by the FedAvg algorithm (McMahan et al., 2017), on Cifar10 (Krizhevsky, 2009) with a LeNet neural network (SGD lr=.01). FedAvg performs multiple epochs of training over the participant’s dataset before sending the final model difference back to the central server. We evaluate gradient disaggregation on updates generated by FedAvg over various parameter settings: local batchsize b𝑏b, epochs e𝑒e, user dataset size D𝐷D (see (McMahan et al., 2017) for more details on these parameters); additionally, we simulate a shift in data distribution by randomly sampling a fraction f𝑓f of participants’ total data set during computation of model updates; finally we test disaggregation on updates generated with and without SGD momentum m𝑚m. Figure 6(b) shows that relative variance of model updates (D=128𝐷128D=128) increases with epochs of training, with momentum and with a shifting data distribution. However, as Table 1 shows we can reconstruct P𝑃P exactly in nearly all cases. The failure cases happen at lower (≤\\leq 1) or higher epochs (≥\\geq 64) of training. At lower epochs, we believe parameters of the update are smaller and less distinguished from each other, making reconstruction more difficult; at higher epochs, reconstruction is more difficult as updates are more noisy. With 2−322322-32 epochs, we are generally able to exactly recover P𝑃P across the settings.",
            "We perform the attack in (Zhu et al., 2019) on an MLP network on Cifar100 and show the effect of inversion with and without gradient disaggregation across multiple users with each user having 1 image in their dataset (submitting full gradients of that image). Figure 7 shows the closest reconstructed image to a user’s data example and Table 2 shows the corresponding PSNR achieved. With gradient disaggregation, we recover the target user’s exact gradient and hence the reconstructed image is high quality. Without disaggregation, reconstruction quality degrades significantly.",
            "We furthermore perform the attack in (Geiping et al., 2020) to invert noisy FedAvg updates. Figure 8 and Table 3 show the results of inverting fedavg updates with local epochs = 4, batch size = 16, user data set size = 64, with and without gradient disaggregation (100 users, 2 layer MLP). With gradient disaggregation we achieve similar quality as inverting a single model update, whereas inverting an update aggregated over multiple users (users=10) significantly degrades reconstruction quality.",
            "We run our experiments using FedAvg model updates, on Cifar10, with 100 users, 200 rounds, participation rate of .1, constraint granularity of 10, SGD lr of 1e-3, on a LeNet CNN model. Table S1 shows the fraction of P𝑃P reconstructed across various FedAvg settings. Results show that with lower lr (1e-3), gradient disaggregation can exactly reconstruct P𝑃P when FedAvg updates are small (1-4 epochs). With more epochs of FedAvg (and larger dataset size or smaller batch size), increased noise prevents reconstruction of P𝑃P. We additionally tried the experiment with higher lr (1e-2), and disaggregation typically failed due to high noise, even with lower epochs of FedAvg. Our results indicate that under the right set of circumstances, the gradient disaggregation attack can be used in an honest but curious scenario; however, more robust results are achieved if the attacker can fix the neural network model.",
            "Table S2 shows the fraction of P𝑃P reconstructed across various proportions of dropped constraints. Results indicate that even when significant proportions of constraints are dropped, P𝑃P may be exactly recovered with more rounds of collected aggregated updates. Note that, when rounds << users reconstruction fails due to the rank being less than the number of users.",
            "Table S3 shows the results of gradient disaggregation when constraint noise 𝒩​(0,μ)𝒩0𝜇\\mathcal{N}(0,\\mu) is added to each constraint. As indicated, even in the presence of noise, gradient disaggregation may exactly recover P𝑃P with enough rounds.",
            "We provide extended data on gradient disaggregation against various parameter values of participation rate. Participation rate is the proportion of users that participate in sending model updates per round and impacts how many updates are summed to yield the aggregated update that is observed by the central server. Unless stated, we enforced a time limit of 10 minutes for solving each column; we use a constraint granularity of 10. We show our extended results in Table S4.",
            "We provide extended data on gradient disaggregation against various parameter values of constraint granularity. Constraint granularity is how precise summary statistics capture user partipation frequency (see main paper for details). Unless stated, we enforced a time limit of 10 minutes for solving each column; we use a constraint granularity of 10. We show our extended results in Table S5.",
            "We provide extended experiments on gradient disaggregation on FedAvg. We show our results in Table S6 and S7."
        ]
    },
    "S3.T3": {
        "caption": "Table S3: Fraction of P𝑃P recovered with gradient disaggregation when constraints are inexact/noisy. Participation rate=.1, λ𝜆\\lambda=.1, constraint granularity = 10. Reconstruction is more successful with more rounds.",
        "table": "<table id=\"S3.T3.5.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T3.5.1.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Number of Users</td>\n<td id=\"S3.T3.5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Rounds</td>\n<td id=\"S3.T3.5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">Constraint Noise</td>\n</tr>\n<tr id=\"S3.T3.5.1.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"6\"><span id=\"S3.T3.5.1.2.2.1.1\" class=\"ltx_text\">32</span></td>\n<td id=\"S3.T3.5.1.2.2.2\" class=\"ltx_td ltx_border_r ltx_border_t\"></td>\n<td id=\"S3.T3.5.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.3</td>\n<td id=\"S3.T3.5.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.5</td>\n<td id=\"S3.T3.5.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S3.T3.5.1.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">128</td>\n<td id=\"S3.T3.5.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.63</td>\n</tr>\n<tr id=\"S3.T3.5.1.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">256</td>\n<td id=\"S3.T3.5.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.97</td>\n</tr>\n<tr id=\"S3.T3.5.1.5.5\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">512</td>\n<td id=\"S3.T3.5.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S3.T3.5.1.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S3.T3.5.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S3.T3.5.1.7.7\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2048</td>\n<td id=\"S3.T3.5.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S3.T3.5.1.8.8\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S3.T3.5.1.8.8.1.1\" class=\"ltx_text\">64</span></td>\n<td id=\"S3.T3.5.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">128</td>\n<td id=\"S3.T3.5.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.34</td>\n</tr>\n<tr id=\"S3.T3.5.1.9.9\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">256</td>\n<td id=\"S3.T3.5.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.97</td>\n</tr>\n<tr id=\"S3.T3.5.1.10.10\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">512</td>\n<td id=\"S3.T3.5.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S3.T3.5.1.11.11\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S3.T3.5.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S3.T3.5.1.12.12\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.12.12.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2048</td>\n<td id=\"S3.T3.5.1.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.12.12.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S3.T3.5.1.13.13\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.13.13.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S3.T3.5.1.13.13.1.1\" class=\"ltx_text\">128</span></td>\n<td id=\"S3.T3.5.1.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">128</td>\n<td id=\"S3.T3.5.1.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S3.T3.5.1.13.13.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S3.T3.5.1.13.13.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S3.T3.5.1.14.14\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.14.14.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">256</td>\n<td id=\"S3.T3.5.1.14.14.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.14.14.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.14.14.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.7</td>\n</tr>\n<tr id=\"S3.T3.5.1.15.15\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.15.15.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">512</td>\n<td id=\"S3.T3.5.1.15.15.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.15.15.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.15.15.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n</tr>\n<tr id=\"S3.T3.5.1.16.16\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.16.16.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S3.T3.5.1.16.16.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.16.16.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.16.16.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S3.T3.5.1.17.17\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.17.17.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2048</td>\n<td id=\"S3.T3.5.1.17.17.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.17.17.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.17.17.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S3.T3.5.1.18.18\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.18.18.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S3.T3.5.1.18.18.1.1\" class=\"ltx_text\">256</span></td>\n<td id=\"S3.T3.5.1.18.18.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">128</td>\n<td id=\"S3.T3.5.1.18.18.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S3.T3.5.1.18.18.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S3.T3.5.1.18.18.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S3.T3.5.1.19.19\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.19.19.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">256</td>\n<td id=\"S3.T3.5.1.19.19.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S3.T3.5.1.19.19.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S3.T3.5.1.19.19.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S3.T3.5.1.20.20\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.20.20.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">512</td>\n<td id=\"S3.T3.5.1.20.20.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.20.20.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.20.20.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.48</td>\n</tr>\n<tr id=\"S3.T3.5.1.21.21\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.21.21.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S3.T3.5.1.21.21.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.21.21.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.21.21.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S3.T3.5.1.22.22\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.1.22.22.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">2048</td>\n<td id=\"S3.T3.5.1.22.22.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.22.22.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S3.T3.5.1.22.22.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Our work on gradient disaggregation and the work in (Fleder & Shah, 2020) solve the same core problem: uncovering individual values given observations of their sums. While their work recovers prices of items, our work analogically reconstructs participants’ model gradients. However, a key distinction in (Fleder & Shah, 2020) is the assumption that each item must be purchased individually at least once. This makes their approach unsuitable for disaggregating aggregated model updates as, under the secure aggregation protocol, each aggregated update is composed of more than one participant’s model updates.",
            "We note that it is possible that devices timestamp the exact moment they perform a round of training; in this case, P𝑃P may be revealed directly through the specificity of the constraints (making the disaggregation problem solvable through a simple linear regression). However, even if devices log only the total number of times they performed training (with no timestamped data) and send these analytics back to the server once every few rounds of participation, the central server may piece together these constraints and incorporate them into the formulation above. In other words, just knowing the number of times particular users performed training and collecting this information periodically (both of which are reasonable based on (Bonawitz et al., 2019)), the central server may obtain enough information to carry out the gradient disaggregation attack. Incorporating summary analytics into the gradient disaggregation attack is significant as it greatly reduces the problem space, allowing a solution to a previously intractable problem.",
            "Additionally, we perform experiments on gradient disaggregation using model updates generated by the FedAvg algorithm (McMahan et al., 2017), on Cifar10 (Krizhevsky, 2009) with a LeNet neural network (SGD lr=.01). FedAvg performs multiple epochs of training over the participant’s dataset before sending the final model difference back to the central server. We evaluate gradient disaggregation on updates generated by FedAvg over various parameter settings: local batchsize b𝑏b, epochs e𝑒e, user dataset size D𝐷D (see (McMahan et al., 2017) for more details on these parameters); additionally, we simulate a shift in data distribution by randomly sampling a fraction f𝑓f of participants’ total data set during computation of model updates; finally we test disaggregation on updates generated with and without SGD momentum m𝑚m. Figure 6(b) shows that relative variance of model updates (D=128𝐷128D=128) increases with epochs of training, with momentum and with a shifting data distribution. However, as Table 1 shows we can reconstruct P𝑃P exactly in nearly all cases. The failure cases happen at lower (≤\\leq 1) or higher epochs (≥\\geq 64) of training. At lower epochs, we believe parameters of the update are smaller and less distinguished from each other, making reconstruction more difficult; at higher epochs, reconstruction is more difficult as updates are more noisy. With 2−322322-32 epochs, we are generally able to exactly recover P𝑃P across the settings.",
            "We perform the attack in (Zhu et al., 2019) on an MLP network on Cifar100 and show the effect of inversion with and without gradient disaggregation across multiple users with each user having 1 image in their dataset (submitting full gradients of that image). Figure 7 shows the closest reconstructed image to a user’s data example and Table 2 shows the corresponding PSNR achieved. With gradient disaggregation, we recover the target user’s exact gradient and hence the reconstructed image is high quality. Without disaggregation, reconstruction quality degrades significantly.",
            "We furthermore perform the attack in (Geiping et al., 2020) to invert noisy FedAvg updates. Figure 8 and Table 3 show the results of inverting fedavg updates with local epochs = 4, batch size = 16, user data set size = 64, with and without gradient disaggregation (100 users, 2 layer MLP). With gradient disaggregation we achieve similar quality as inverting a single model update, whereas inverting an update aggregated over multiple users (users=10) significantly degrades reconstruction quality.",
            "We run our experiments using FedAvg model updates, on Cifar10, with 100 users, 200 rounds, participation rate of .1, constraint granularity of 10, SGD lr of 1e-3, on a LeNet CNN model. Table S1 shows the fraction of P𝑃P reconstructed across various FedAvg settings. Results show that with lower lr (1e-3), gradient disaggregation can exactly reconstruct P𝑃P when FedAvg updates are small (1-4 epochs). With more epochs of FedAvg (and larger dataset size or smaller batch size), increased noise prevents reconstruction of P𝑃P. We additionally tried the experiment with higher lr (1e-2), and disaggregation typically failed due to high noise, even with lower epochs of FedAvg. Our results indicate that under the right set of circumstances, the gradient disaggregation attack can be used in an honest but curious scenario; however, more robust results are achieved if the attacker can fix the neural network model.",
            "Table S2 shows the fraction of P𝑃P reconstructed across various proportions of dropped constraints. Results indicate that even when significant proportions of constraints are dropped, P𝑃P may be exactly recovered with more rounds of collected aggregated updates. Note that, when rounds << users reconstruction fails due to the rank being less than the number of users.",
            "Table S3 shows the results of gradient disaggregation when constraint noise 𝒩​(0,μ)𝒩0𝜇\\mathcal{N}(0,\\mu) is added to each constraint. As indicated, even in the presence of noise, gradient disaggregation may exactly recover P𝑃P with enough rounds.",
            "We provide extended data on gradient disaggregation against various parameter values of participation rate. Participation rate is the proportion of users that participate in sending model updates per round and impacts how many updates are summed to yield the aggregated update that is observed by the central server. Unless stated, we enforced a time limit of 10 minutes for solving each column; we use a constraint granularity of 10. We show our extended results in Table S4.",
            "We provide extended data on gradient disaggregation against various parameter values of constraint granularity. Constraint granularity is how precise summary statistics capture user partipation frequency (see main paper for details). Unless stated, we enforced a time limit of 10 minutes for solving each column; we use a constraint granularity of 10. We show our extended results in Table S5.",
            "We provide extended experiments on gradient disaggregation on FedAvg. We show our results in Table S6 and S7."
        ]
    },
    "S4.T4": {
        "caption": "Table S4: Fraction of P𝑃P recovered via gradient disaggregation for various participation rates. * indicates settings where the time limit for solving each column was increased to 60 minutes (vs 10 minutes). Generally, using more rounds facilitates more successful reconstruction; note that with larger number of users and rounds, success rate decreased due to exceeding the 10 minute time limit.",
        "table": "<table id=\"S4.T4.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.3.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Users</td>\n<td id=\"S4.T4.3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Rounds</td>\n<td id=\"S4.T4.3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">Participation Rate</td>\n</tr>\n<tr id=\"S4.T4.3.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.2.2.1\" class=\"ltx_td ltx_border_l ltx_border_r ltx_border_t\"></td>\n<td id=\"S4.T4.3.1.2.2.2\" class=\"ltx_td ltx_border_r ltx_border_t\"></td>\n<td id=\"S4.T4.3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.1</td>\n<td id=\"S4.T4.3.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.2</td>\n<td id=\"S4.T4.3.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.4</td>\n</tr>\n<tr id=\"S4.T4.3.1.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S4.T4.3.1.3.3.1.1\" class=\"ltx_text\">128</span></td>\n<td id=\"S4.T4.3.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">256</td>\n<td id=\"S4.T4.3.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.05</td>\n</tr>\n<tr id=\"S4.T4.3.1.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">512</td>\n<td id=\"S4.T4.3.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S4.T4.3.1.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S4.T4.3.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S4.T4.3.1.6.6\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2048</td>\n<td id=\"S4.T4.3.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S4.T4.3.1.7.7\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4096</td>\n<td id=\"S4.T4.3.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S4.T4.3.1.8.8\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S4.T4.3.1.8.8.1.1\" class=\"ltx_text\">256</span></td>\n<td id=\"S4.T4.3.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">256</td>\n<td id=\"S4.T4.3.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S4.T4.3.1.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S4.T4.3.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S4.T4.3.1.9.9\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">512</td>\n<td id=\"S4.T4.3.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.73</td>\n<td id=\"S4.T4.3.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S4.T4.3.1.10.10\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S4.T4.3.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S4.T4.3.1.11.11\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2048</td>\n<td id=\"S4.T4.3.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S4.T4.3.1.12.12\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.12.12.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4096</td>\n<td id=\"S4.T4.3.1.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.12.12.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.31</td>\n</tr>\n<tr id=\"S4.T4.3.1.13.13\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.13.13.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S4.T4.3.1.13.13.1.1\" class=\"ltx_text\">512</span></td>\n<td id=\"S4.T4.3.1.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">256</td>\n<td id=\"S4.T4.3.1.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S4.T4.3.1.13.13.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S4.T4.3.1.13.13.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S4.T4.3.1.14.14\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.14.14.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">512</td>\n<td id=\"S4.T4.3.1.14.14.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S4.T4.3.1.14.14.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S4.T4.3.1.14.14.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S4.T4.3.1.15.15\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.15.15.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S4.T4.3.1.15.15.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.15.15.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.34</td>\n<td id=\"S4.T4.3.1.15.15.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S4.T4.3.1.16.16\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.16.16.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2048</td>\n<td id=\"S4.T4.3.1.16.16.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.16.16.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.16.16.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S4.T4.3.1.17.17\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.17.17.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4096</td>\n<td id=\"S4.T4.3.1.17.17.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.17.17.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.17.17.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.03</td>\n</tr>\n<tr id=\"S4.T4.3.1.18.18\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.18.18.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S4.T4.3.1.18.18.1.1\" class=\"ltx_text\">1024</span></td>\n<td id=\"S4.T4.3.1.18.18.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">256</td>\n<td id=\"S4.T4.3.1.18.18.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S4.T4.3.1.18.18.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S4.T4.3.1.18.18.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S4.T4.3.1.19.19\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.19.19.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">512</td>\n<td id=\"S4.T4.3.1.19.19.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S4.T4.3.1.19.19.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S4.T4.3.1.19.19.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S4.T4.3.1.20.20\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.20.20.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S4.T4.3.1.20.20.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S4.T4.3.1.20.20.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S4.T4.3.1.20.20.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S4.T4.3.1.21.21\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.21.21.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2048</td>\n<td id=\"S4.T4.3.1.21.21.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.21.21.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.02</td>\n<td id=\"S4.T4.3.1.21.21.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S4.T4.3.1.22.22\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.22.22.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4096</td>\n<td id=\"S4.T4.3.1.22.22.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.22.22.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.48</td>\n<td id=\"S4.T4.3.1.22.22.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S4.T4.3.1.23.23\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.1.23.23.1\" class=\"ltx_td ltx_border_b ltx_border_l ltx_border_r\"></td>\n<td id=\"S4.T4.3.1.23.23.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">5120*</td>\n<td id=\"S4.T4.3.1.23.23.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.23.23.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T4.3.1.23.23.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Our work on gradient disaggregation and the work in (Fleder & Shah, 2020) solve the same core problem: uncovering individual values given observations of their sums. While their work recovers prices of items, our work analogically reconstructs participants’ model gradients. However, a key distinction in (Fleder & Shah, 2020) is the assumption that each item must be purchased individually at least once. This makes their approach unsuitable for disaggregating aggregated model updates as, under the secure aggregation protocol, each aggregated update is composed of more than one participant’s model updates.",
            "We note that it is possible that devices timestamp the exact moment they perform a round of training; in this case, P𝑃P may be revealed directly through the specificity of the constraints (making the disaggregation problem solvable through a simple linear regression). However, even if devices log only the total number of times they performed training (with no timestamped data) and send these analytics back to the server once every few rounds of participation, the central server may piece together these constraints and incorporate them into the formulation above. In other words, just knowing the number of times particular users performed training and collecting this information periodically (both of which are reasonable based on (Bonawitz et al., 2019)), the central server may obtain enough information to carry out the gradient disaggregation attack. Incorporating summary analytics into the gradient disaggregation attack is significant as it greatly reduces the problem space, allowing a solution to a previously intractable problem.",
            "Additionally, we perform experiments on gradient disaggregation using model updates generated by the FedAvg algorithm (McMahan et al., 2017), on Cifar10 (Krizhevsky, 2009) with a LeNet neural network (SGD lr=.01). FedAvg performs multiple epochs of training over the participant’s dataset before sending the final model difference back to the central server. We evaluate gradient disaggregation on updates generated by FedAvg over various parameter settings: local batchsize b𝑏b, epochs e𝑒e, user dataset size D𝐷D (see (McMahan et al., 2017) for more details on these parameters); additionally, we simulate a shift in data distribution by randomly sampling a fraction f𝑓f of participants’ total data set during computation of model updates; finally we test disaggregation on updates generated with and without SGD momentum m𝑚m. Figure 6(b) shows that relative variance of model updates (D=128𝐷128D=128) increases with epochs of training, with momentum and with a shifting data distribution. However, as Table 1 shows we can reconstruct P𝑃P exactly in nearly all cases. The failure cases happen at lower (≤\\leq 1) or higher epochs (≥\\geq 64) of training. At lower epochs, we believe parameters of the update are smaller and less distinguished from each other, making reconstruction more difficult; at higher epochs, reconstruction is more difficult as updates are more noisy. With 2−322322-32 epochs, we are generally able to exactly recover P𝑃P across the settings.",
            "We perform the attack in (Zhu et al., 2019) on an MLP network on Cifar100 and show the effect of inversion with and without gradient disaggregation across multiple users with each user having 1 image in their dataset (submitting full gradients of that image). Figure 7 shows the closest reconstructed image to a user’s data example and Table 2 shows the corresponding PSNR achieved. With gradient disaggregation, we recover the target user’s exact gradient and hence the reconstructed image is high quality. Without disaggregation, reconstruction quality degrades significantly.",
            "We furthermore perform the attack in (Geiping et al., 2020) to invert noisy FedAvg updates. Figure 8 and Table 3 show the results of inverting fedavg updates with local epochs = 4, batch size = 16, user data set size = 64, with and without gradient disaggregation (100 users, 2 layer MLP). With gradient disaggregation we achieve similar quality as inverting a single model update, whereas inverting an update aggregated over multiple users (users=10) significantly degrades reconstruction quality.",
            "We run our experiments using FedAvg model updates, on Cifar10, with 100 users, 200 rounds, participation rate of .1, constraint granularity of 10, SGD lr of 1e-3, on a LeNet CNN model. Table S1 shows the fraction of P𝑃P reconstructed across various FedAvg settings. Results show that with lower lr (1e-3), gradient disaggregation can exactly reconstruct P𝑃P when FedAvg updates are small (1-4 epochs). With more epochs of FedAvg (and larger dataset size or smaller batch size), increased noise prevents reconstruction of P𝑃P. We additionally tried the experiment with higher lr (1e-2), and disaggregation typically failed due to high noise, even with lower epochs of FedAvg. Our results indicate that under the right set of circumstances, the gradient disaggregation attack can be used in an honest but curious scenario; however, more robust results are achieved if the attacker can fix the neural network model.",
            "Table S2 shows the fraction of P𝑃P reconstructed across various proportions of dropped constraints. Results indicate that even when significant proportions of constraints are dropped, P𝑃P may be exactly recovered with more rounds of collected aggregated updates. Note that, when rounds << users reconstruction fails due to the rank being less than the number of users.",
            "Table S3 shows the results of gradient disaggregation when constraint noise 𝒩​(0,μ)𝒩0𝜇\\mathcal{N}(0,\\mu) is added to each constraint. As indicated, even in the presence of noise, gradient disaggregation may exactly recover P𝑃P with enough rounds.",
            "We provide extended data on gradient disaggregation against various parameter values of participation rate. Participation rate is the proportion of users that participate in sending model updates per round and impacts how many updates are summed to yield the aggregated update that is observed by the central server. Unless stated, we enforced a time limit of 10 minutes for solving each column; we use a constraint granularity of 10. We show our extended results in Table S4.",
            "We provide extended data on gradient disaggregation against various parameter values of constraint granularity. Constraint granularity is how precise summary statistics capture user partipation frequency (see main paper for details). Unless stated, we enforced a time limit of 10 minutes for solving each column; we use a constraint granularity of 10. We show our extended results in Table S5.",
            "We provide extended experiments on gradient disaggregation on FedAvg. We show our results in Table S6 and S7."
        ]
    },
    "S5.T5": {
        "caption": "Table S5: Fraction of P𝑃P recovered via gradient disaggregation for various constraint granularities. ",
        "table": "<table id=\"S5.T5.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T5.3.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Users</td>\n<td id=\"S5.T5.3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Rounds</td>\n<td id=\"S5.T5.3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"4\">Constraint Granularity</td>\n</tr>\n<tr id=\"S5.T5.3.1.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.2.2.1\" class=\"ltx_td ltx_border_l ltx_border_r ltx_border_t\"></td>\n<td id=\"S5.T5.3.1.2.2.2\" class=\"ltx_td ltx_border_r ltx_border_t\"></td>\n<td id=\"S5.T5.3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10</td>\n<td id=\"S5.T5.3.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">20</td>\n<td id=\"S5.T5.3.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">40</td>\n<td id=\"S5.T5.3.1.2.2.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">80</td>\n</tr>\n<tr id=\"S5.T5.3.1.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S5.T5.3.1.3.3.1.1\" class=\"ltx_text\">128</span></td>\n<td id=\"S5.T5.3.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">256</td>\n<td id=\"S5.T5.3.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n<td id=\"S5.T5.3.1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.95</td>\n</tr>\n<tr id=\"S5.T5.3.1.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">512</td>\n<td id=\"S5.T5.3.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S5.T5.3.1.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S5.T5.3.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S5.T5.3.1.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2048</td>\n<td id=\"S5.T5.3.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S5.T5.3.1.7.7\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4096</td>\n<td id=\"S5.T5.3.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S5.T5.3.1.8.8\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S5.T5.3.1.8.8.1.1\" class=\"ltx_text\">256</span></td>\n<td id=\"S5.T5.3.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">256</td>\n<td id=\"S5.T5.3.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.8.8.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S5.T5.3.1.9.9\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">512</td>\n<td id=\"S5.T5.3.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.94</td>\n<td id=\"S5.T5.3.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.27</td>\n<td id=\"S5.T5.3.1.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.02</td>\n</tr>\n<tr id=\"S5.T5.3.1.10.10\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S5.T5.3.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.97</td>\n<td id=\"S5.T5.3.1.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.44</td>\n</tr>\n<tr id=\"S5.T5.3.1.11.11\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2048</td>\n<td id=\"S5.T5.3.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S5.T5.3.1.12.12\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.12.12.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4096</td>\n<td id=\"S5.T5.3.1.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.12.12.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.12.12.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.86</td>\n</tr>\n<tr id=\"S5.T5.3.1.13.13\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.13.13.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S5.T5.3.1.13.13.1.1\" class=\"ltx_text\">512</span></td>\n<td id=\"S5.T5.3.1.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">256</td>\n<td id=\"S5.T5.3.1.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.13.13.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.13.13.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.13.13.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S5.T5.3.1.14.14\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.14.14.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">512</td>\n<td id=\"S5.T5.3.1.14.14.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.14.14.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.14.14.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.14.14.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S5.T5.3.1.15.15\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.15.15.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S5.T5.3.1.15.15.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.15.15.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.5</td>\n<td id=\"S5.T5.3.1.15.15.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.15.15.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S5.T5.3.1.16.16\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.16.16.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2048</td>\n<td id=\"S5.T5.3.1.16.16.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.16.16.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.16.16.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.98</td>\n<td id=\"S5.T5.3.1.16.16.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.24</td>\n</tr>\n<tr id=\"S5.T5.3.1.17.17\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.17.17.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4096</td>\n<td id=\"S5.T5.3.1.17.17.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.17.17.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.17.17.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.38</td>\n<td id=\"S5.T5.3.1.17.17.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.035</td>\n</tr>\n<tr id=\"S5.T5.3.1.18.18\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.18.18.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S5.T5.3.1.18.18.1.1\" class=\"ltx_text\">1024</span></td>\n<td id=\"S5.T5.3.1.18.18.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">256</td>\n<td id=\"S5.T5.3.1.18.18.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.18.18.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.18.18.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.18.18.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S5.T5.3.1.19.19\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.19.19.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">512</td>\n<td id=\"S5.T5.3.1.19.19.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.19.19.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.19.19.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.19.19.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S5.T5.3.1.20.20\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.20.20.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S5.T5.3.1.20.20.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.20.20.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.20.20.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.20.20.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S5.T5.3.1.21.21\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.21.21.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2048</td>\n<td id=\"S5.T5.3.1.21.21.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.21.21.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.23</td>\n<td id=\"S5.T5.3.1.21.21.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.21.21.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n</tr>\n<tr id=\"S5.T5.3.1.22.22\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.22.22.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">4096</td>\n<td id=\"S5.T5.3.1.22.22.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T5.3.1.22.22.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">.91</td>\n<td id=\"S5.T5.3.1.22.22.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0</td>\n<td id=\"S5.T5.3.1.22.22.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Our work on gradient disaggregation and the work in (Fleder & Shah, 2020) solve the same core problem: uncovering individual values given observations of their sums. While their work recovers prices of items, our work analogically reconstructs participants’ model gradients. However, a key distinction in (Fleder & Shah, 2020) is the assumption that each item must be purchased individually at least once. This makes their approach unsuitable for disaggregating aggregated model updates as, under the secure aggregation protocol, each aggregated update is composed of more than one participant’s model updates.",
            "We note that it is possible that devices timestamp the exact moment they perform a round of training; in this case, P𝑃P may be revealed directly through the specificity of the constraints (making the disaggregation problem solvable through a simple linear regression). However, even if devices log only the total number of times they performed training (with no timestamped data) and send these analytics back to the server once every few rounds of participation, the central server may piece together these constraints and incorporate them into the formulation above. In other words, just knowing the number of times particular users performed training and collecting this information periodically (both of which are reasonable based on (Bonawitz et al., 2019)), the central server may obtain enough information to carry out the gradient disaggregation attack. Incorporating summary analytics into the gradient disaggregation attack is significant as it greatly reduces the problem space, allowing a solution to a previously intractable problem.",
            "Additionally, we perform experiments on gradient disaggregation using model updates generated by the FedAvg algorithm (McMahan et al., 2017), on Cifar10 (Krizhevsky, 2009) with a LeNet neural network (SGD lr=.01). FedAvg performs multiple epochs of training over the participant’s dataset before sending the final model difference back to the central server. We evaluate gradient disaggregation on updates generated by FedAvg over various parameter settings: local batchsize b𝑏b, epochs e𝑒e, user dataset size D𝐷D (see (McMahan et al., 2017) for more details on these parameters); additionally, we simulate a shift in data distribution by randomly sampling a fraction f𝑓f of participants’ total data set during computation of model updates; finally we test disaggregation on updates generated with and without SGD momentum m𝑚m. Figure 6(b) shows that relative variance of model updates (D=128𝐷128D=128) increases with epochs of training, with momentum and with a shifting data distribution. However, as Table 1 shows we can reconstruct P𝑃P exactly in nearly all cases. The failure cases happen at lower (≤\\leq 1) or higher epochs (≥\\geq 64) of training. At lower epochs, we believe parameters of the update are smaller and less distinguished from each other, making reconstruction more difficult; at higher epochs, reconstruction is more difficult as updates are more noisy. With 2−322322-32 epochs, we are generally able to exactly recover P𝑃P across the settings.",
            "We perform the attack in (Zhu et al., 2019) on an MLP network on Cifar100 and show the effect of inversion with and without gradient disaggregation across multiple users with each user having 1 image in their dataset (submitting full gradients of that image). Figure 7 shows the closest reconstructed image to a user’s data example and Table 2 shows the corresponding PSNR achieved. With gradient disaggregation, we recover the target user’s exact gradient and hence the reconstructed image is high quality. Without disaggregation, reconstruction quality degrades significantly.",
            "We furthermore perform the attack in (Geiping et al., 2020) to invert noisy FedAvg updates. Figure 8 and Table 3 show the results of inverting fedavg updates with local epochs = 4, batch size = 16, user data set size = 64, with and without gradient disaggregation (100 users, 2 layer MLP). With gradient disaggregation we achieve similar quality as inverting a single model update, whereas inverting an update aggregated over multiple users (users=10) significantly degrades reconstruction quality.",
            "We run our experiments using FedAvg model updates, on Cifar10, with 100 users, 200 rounds, participation rate of .1, constraint granularity of 10, SGD lr of 1e-3, on a LeNet CNN model. Table S1 shows the fraction of P𝑃P reconstructed across various FedAvg settings. Results show that with lower lr (1e-3), gradient disaggregation can exactly reconstruct P𝑃P when FedAvg updates are small (1-4 epochs). With more epochs of FedAvg (and larger dataset size or smaller batch size), increased noise prevents reconstruction of P𝑃P. We additionally tried the experiment with higher lr (1e-2), and disaggregation typically failed due to high noise, even with lower epochs of FedAvg. Our results indicate that under the right set of circumstances, the gradient disaggregation attack can be used in an honest but curious scenario; however, more robust results are achieved if the attacker can fix the neural network model.",
            "Table S2 shows the fraction of P𝑃P reconstructed across various proportions of dropped constraints. Results indicate that even when significant proportions of constraints are dropped, P𝑃P may be exactly recovered with more rounds of collected aggregated updates. Note that, when rounds << users reconstruction fails due to the rank being less than the number of users.",
            "Table S3 shows the results of gradient disaggregation when constraint noise 𝒩​(0,μ)𝒩0𝜇\\mathcal{N}(0,\\mu) is added to each constraint. As indicated, even in the presence of noise, gradient disaggregation may exactly recover P𝑃P with enough rounds.",
            "We provide extended data on gradient disaggregation against various parameter values of participation rate. Participation rate is the proportion of users that participate in sending model updates per round and impacts how many updates are summed to yield the aggregated update that is observed by the central server. Unless stated, we enforced a time limit of 10 minutes for solving each column; we use a constraint granularity of 10. We show our extended results in Table S4.",
            "We provide extended data on gradient disaggregation against various parameter values of constraint granularity. Constraint granularity is how precise summary statistics capture user partipation frequency (see main paper for details). Unless stated, we enforced a time limit of 10 minutes for solving each column; we use a constraint granularity of 10. We show our extended results in Table S5.",
            "We provide extended experiments on gradient disaggregation on FedAvg. We show our results in Table S6 and S7."
        ]
    },
    "S6.T6": {
        "caption": "Table S6: Fraction of P𝑃P recovered on FedAvg updates using larger LeNet model (last hidden layer size=512), across various settings (mom.= SGD momentum, fraction=fraction of data sampled from the 384 examples to perform FedAvg over). Users=100, rounds=200, participation rate=.1, constraint granularity=10.",
        "table": "<table id=\"S6.T6.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T6.3.1.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">User Dataset Size</td>\n<td id=\"S6.T6.3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Batch Size</td>\n<td id=\"S6.T6.3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"7\">Local Epochs</td>\n</tr>\n<tr id=\"S6.T6.3.1.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S6.T6.3.1.2.2.1.1\" class=\"ltx_text\">384</span></td>\n<td id=\"S6.T6.3.1.2.2.2\" class=\"ltx_td ltx_border_r ltx_border_t\"></td>\n<td id=\"S6.T6.3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S6.T6.3.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4</td>\n<td id=\"S6.T6.3.1.2.2.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">8</td>\n<td id=\"S6.T6.3.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16</td>\n<td id=\"S6.T6.3.1.2.2.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32</td>\n<td id=\"S6.T6.3.1.2.2.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">64</td>\n</tr>\n<tr id=\"S6.T6.3.1.3.3\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8</td>\n<td id=\"S6.T6.3.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.3.3.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S6.T6.3.1.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16</td>\n<td id=\"S6.T6.3.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.4.4.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S6.T6.3.1.5.5\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32</td>\n<td id=\"S6.T6.3.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.5.5.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.5.5.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S6.T6.3.1.6.6\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">64</td>\n<td id=\"S6.T6.3.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.6.6.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.6.6.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.6.6.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S6.T6.3.1.7.7\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S6.T6.3.1.7.7.1.1\" class=\"ltx_text\">384 (mom.=.9)</span></td>\n<td id=\"S6.T6.3.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8</td>\n<td id=\"S6.T6.3.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.96</td>\n<td id=\"S6.T6.3.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.88</td>\n<td id=\"S6.T6.3.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.78</td>\n<td id=\"S6.T6.3.1.7.7.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.97</td>\n<td id=\"S6.T6.3.1.7.7.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.83</td>\n<td id=\"S6.T6.3.1.7.7.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.19</td>\n<td id=\"S6.T6.3.1.7.7.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.01</td>\n</tr>\n<tr id=\"S6.T6.3.1.8.8\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16</td>\n<td id=\"S6.T6.3.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.8.8.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.8.8.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.8.8.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S6.T6.3.1.9.9\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32</td>\n<td id=\"S6.T6.3.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.9.9.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.9.9.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.9.9.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S6.T6.3.1.10.10\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">64</td>\n<td id=\"S6.T6.3.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.10.10.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.10.10.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.10.10.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S6.T6.3.1.11.11\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S6.T6.3.1.11.11.1.1\" class=\"ltx_text\">384 (fraction=.9)</span></td>\n<td id=\"S6.T6.3.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8</td>\n<td id=\"S6.T6.3.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.98</td>\n<td id=\"S6.T6.3.1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.11.11.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.11.11.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n<td id=\"S6.T6.3.1.11.11.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.11.11.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S6.T6.3.1.12.12\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.12.12.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16</td>\n<td id=\"S6.T6.3.1.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.12.12.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.12.12.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.12.12.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.12.12.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.12.12.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S6.T6.3.1.13.13\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.13.13.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32</td>\n<td id=\"S6.T6.3.1.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.13.13.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.13.13.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.13.13.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.13.13.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.13.13.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n</tr>\n<tr id=\"S6.T6.3.1.14.14\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.14.14.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">64</td>\n<td id=\"S6.T6.3.1.14.14.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.14.14.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.14.14.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.14.14.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.14.14.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.14.14.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.14.14.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S6.T6.3.1.15.15\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.15.15.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S6.T6.3.1.15.15.1.1\" class=\"ltx_text\">384 (fraction=.8)</span></td>\n<td id=\"S6.T6.3.1.15.15.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8</td>\n<td id=\"S6.T6.3.1.15.15.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.15.15.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.15.15.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.15.15.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n<td id=\"S6.T6.3.1.15.15.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.83</td>\n<td id=\"S6.T6.3.1.15.15.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.92</td>\n<td id=\"S6.T6.3.1.15.15.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S6.T6.3.1.16.16\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.16.16.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16</td>\n<td id=\"S6.T6.3.1.16.16.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.91</td>\n<td id=\"S6.T6.3.1.16.16.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.95</td>\n<td id=\"S6.T6.3.1.16.16.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n<td id=\"S6.T6.3.1.16.16.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.16.16.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.97</td>\n<td id=\"S6.T6.3.1.16.16.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.84</td>\n<td id=\"S6.T6.3.1.16.16.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.95</td>\n</tr>\n<tr id=\"S6.T6.3.1.17.17\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.17.17.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32</td>\n<td id=\"S6.T6.3.1.17.17.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n<td id=\"S6.T6.3.1.17.17.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.99</td>\n<td id=\"S6.T6.3.1.17.17.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.17.17.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.17.17.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.17.17.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.17.17.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S6.T6.3.1.18.18\" class=\"ltx_tr\">\n<td id=\"S6.T6.3.1.18.18.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">64</td>\n<td id=\"S6.T6.3.1.18.18.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.18.18.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.18.18.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.18.18.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.18.18.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.18.18.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T6.3.1.18.18.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Our work on gradient disaggregation and the work in (Fleder & Shah, 2020) solve the same core problem: uncovering individual values given observations of their sums. While their work recovers prices of items, our work analogically reconstructs participants’ model gradients. However, a key distinction in (Fleder & Shah, 2020) is the assumption that each item must be purchased individually at least once. This makes their approach unsuitable for disaggregating aggregated model updates as, under the secure aggregation protocol, each aggregated update is composed of more than one participant’s model updates.",
            "We note that it is possible that devices timestamp the exact moment they perform a round of training; in this case, P𝑃P may be revealed directly through the specificity of the constraints (making the disaggregation problem solvable through a simple linear regression). However, even if devices log only the total number of times they performed training (with no timestamped data) and send these analytics back to the server once every few rounds of participation, the central server may piece together these constraints and incorporate them into the formulation above. In other words, just knowing the number of times particular users performed training and collecting this information periodically (both of which are reasonable based on (Bonawitz et al., 2019)), the central server may obtain enough information to carry out the gradient disaggregation attack. Incorporating summary analytics into the gradient disaggregation attack is significant as it greatly reduces the problem space, allowing a solution to a previously intractable problem.",
            "Additionally, we perform experiments on gradient disaggregation using model updates generated by the FedAvg algorithm (McMahan et al., 2017), on Cifar10 (Krizhevsky, 2009) with a LeNet neural network (SGD lr=.01). FedAvg performs multiple epochs of training over the participant’s dataset before sending the final model difference back to the central server. We evaluate gradient disaggregation on updates generated by FedAvg over various parameter settings: local batchsize b𝑏b, epochs e𝑒e, user dataset size D𝐷D (see (McMahan et al., 2017) for more details on these parameters); additionally, we simulate a shift in data distribution by randomly sampling a fraction f𝑓f of participants’ total data set during computation of model updates; finally we test disaggregation on updates generated with and without SGD momentum m𝑚m. Figure 6(b) shows that relative variance of model updates (D=128𝐷128D=128) increases with epochs of training, with momentum and with a shifting data distribution. However, as Table 1 shows we can reconstruct P𝑃P exactly in nearly all cases. The failure cases happen at lower (≤\\leq 1) or higher epochs (≥\\geq 64) of training. At lower epochs, we believe parameters of the update are smaller and less distinguished from each other, making reconstruction more difficult; at higher epochs, reconstruction is more difficult as updates are more noisy. With 2−322322-32 epochs, we are generally able to exactly recover P𝑃P across the settings.",
            "We perform the attack in (Zhu et al., 2019) on an MLP network on Cifar100 and show the effect of inversion with and without gradient disaggregation across multiple users with each user having 1 image in their dataset (submitting full gradients of that image). Figure 7 shows the closest reconstructed image to a user’s data example and Table 2 shows the corresponding PSNR achieved. With gradient disaggregation, we recover the target user’s exact gradient and hence the reconstructed image is high quality. Without disaggregation, reconstruction quality degrades significantly.",
            "We furthermore perform the attack in (Geiping et al., 2020) to invert noisy FedAvg updates. Figure 8 and Table 3 show the results of inverting fedavg updates with local epochs = 4, batch size = 16, user data set size = 64, with and without gradient disaggregation (100 users, 2 layer MLP). With gradient disaggregation we achieve similar quality as inverting a single model update, whereas inverting an update aggregated over multiple users (users=10) significantly degrades reconstruction quality.",
            "We run our experiments using FedAvg model updates, on Cifar10, with 100 users, 200 rounds, participation rate of .1, constraint granularity of 10, SGD lr of 1e-3, on a LeNet CNN model. Table S1 shows the fraction of P𝑃P reconstructed across various FedAvg settings. Results show that with lower lr (1e-3), gradient disaggregation can exactly reconstruct P𝑃P when FedAvg updates are small (1-4 epochs). With more epochs of FedAvg (and larger dataset size or smaller batch size), increased noise prevents reconstruction of P𝑃P. We additionally tried the experiment with higher lr (1e-2), and disaggregation typically failed due to high noise, even with lower epochs of FedAvg. Our results indicate that under the right set of circumstances, the gradient disaggregation attack can be used in an honest but curious scenario; however, more robust results are achieved if the attacker can fix the neural network model.",
            "Table S2 shows the fraction of P𝑃P reconstructed across various proportions of dropped constraints. Results indicate that even when significant proportions of constraints are dropped, P𝑃P may be exactly recovered with more rounds of collected aggregated updates. Note that, when rounds << users reconstruction fails due to the rank being less than the number of users.",
            "Table S3 shows the results of gradient disaggregation when constraint noise 𝒩​(0,μ)𝒩0𝜇\\mathcal{N}(0,\\mu) is added to each constraint. As indicated, even in the presence of noise, gradient disaggregation may exactly recover P𝑃P with enough rounds.",
            "We provide extended data on gradient disaggregation against various parameter values of participation rate. Participation rate is the proportion of users that participate in sending model updates per round and impacts how many updates are summed to yield the aggregated update that is observed by the central server. Unless stated, we enforced a time limit of 10 minutes for solving each column; we use a constraint granularity of 10. We show our extended results in Table S4.",
            "We provide extended data on gradient disaggregation against various parameter values of constraint granularity. Constraint granularity is how precise summary statistics capture user partipation frequency (see main paper for details). Unless stated, we enforced a time limit of 10 minutes for solving each column; we use a constraint granularity of 10. We show our extended results in Table S5.",
            "We provide extended experiments on gradient disaggregation on FedAvg. We show our results in Table S6 and S7."
        ]
    },
    "S6.T7": {
        "caption": "Table S7: Fraction of P𝑃P recovered on FedAvg updates using larger LeNet model (last hidden layer size=512), With more users. (rounds=200, participation rate=.1, constraint granularity=10). With many users, P𝑃P is still reconstructable.",
        "table": "<table id=\"S6.T7.5.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T7.5.1.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T7.5.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Number of Users</td>\n<td id=\"S6.T7.5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Batch Size</td>\n<td id=\"S6.T7.5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"6\">Local Epochs</td>\n</tr>\n<tr id=\"S6.T7.5.1.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T7.5.1.2.2.1\" class=\"ltx_td ltx_border_l ltx_border_r ltx_border_t\"></td>\n<td id=\"S6.T7.5.1.2.2.2\" class=\"ltx_td ltx_border_r ltx_border_t\"></td>\n<td id=\"S6.T7.5.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T7.5.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S6.T7.5.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4</td>\n<td id=\"S6.T7.5.1.2.2.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">8</td>\n<td id=\"S6.T7.5.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16</td>\n<td id=\"S6.T7.5.1.2.2.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32</td>\n</tr>\n<tr id=\"S6.T7.5.1.3.3\" class=\"ltx_tr\">\n<td id=\"S6.T7.5.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">512</td>\n<td id=\"S6.T7.5.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16</td>\n<td id=\"S6.T7.5.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T7.5.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T7.5.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T7.5.1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">.996</td>\n<td id=\"S6.T7.5.1.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T7.5.1.3.3.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S6.T7.5.1.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T7.5.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">1024</td>\n<td id=\"S6.T7.5.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">16</td>\n<td id=\"S6.T7.5.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">.998</td>\n<td id=\"S6.T7.5.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T7.5.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">.999</td>\n<td id=\"S6.T7.5.1.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T7.5.1.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n<td id=\"S6.T7.5.1.4.4.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Our work on gradient disaggregation and the work in (Fleder & Shah, 2020) solve the same core problem: uncovering individual values given observations of their sums. While their work recovers prices of items, our work analogically reconstructs participants’ model gradients. However, a key distinction in (Fleder & Shah, 2020) is the assumption that each item must be purchased individually at least once. This makes their approach unsuitable for disaggregating aggregated model updates as, under the secure aggregation protocol, each aggregated update is composed of more than one participant’s model updates.",
            "We note that it is possible that devices timestamp the exact moment they perform a round of training; in this case, P𝑃P may be revealed directly through the specificity of the constraints (making the disaggregation problem solvable through a simple linear regression). However, even if devices log only the total number of times they performed training (with no timestamped data) and send these analytics back to the server once every few rounds of participation, the central server may piece together these constraints and incorporate them into the formulation above. In other words, just knowing the number of times particular users performed training and collecting this information periodically (both of which are reasonable based on (Bonawitz et al., 2019)), the central server may obtain enough information to carry out the gradient disaggregation attack. Incorporating summary analytics into the gradient disaggregation attack is significant as it greatly reduces the problem space, allowing a solution to a previously intractable problem.",
            "Additionally, we perform experiments on gradient disaggregation using model updates generated by the FedAvg algorithm (McMahan et al., 2017), on Cifar10 (Krizhevsky, 2009) with a LeNet neural network (SGD lr=.01). FedAvg performs multiple epochs of training over the participant’s dataset before sending the final model difference back to the central server. We evaluate gradient disaggregation on updates generated by FedAvg over various parameter settings: local batchsize b𝑏b, epochs e𝑒e, user dataset size D𝐷D (see (McMahan et al., 2017) for more details on these parameters); additionally, we simulate a shift in data distribution by randomly sampling a fraction f𝑓f of participants’ total data set during computation of model updates; finally we test disaggregation on updates generated with and without SGD momentum m𝑚m. Figure 6(b) shows that relative variance of model updates (D=128𝐷128D=128) increases with epochs of training, with momentum and with a shifting data distribution. However, as Table 1 shows we can reconstruct P𝑃P exactly in nearly all cases. The failure cases happen at lower (≤\\leq 1) or higher epochs (≥\\geq 64) of training. At lower epochs, we believe parameters of the update are smaller and less distinguished from each other, making reconstruction more difficult; at higher epochs, reconstruction is more difficult as updates are more noisy. With 2−322322-32 epochs, we are generally able to exactly recover P𝑃P across the settings.",
            "We perform the attack in (Zhu et al., 2019) on an MLP network on Cifar100 and show the effect of inversion with and without gradient disaggregation across multiple users with each user having 1 image in their dataset (submitting full gradients of that image). Figure 7 shows the closest reconstructed image to a user’s data example and Table 2 shows the corresponding PSNR achieved. With gradient disaggregation, we recover the target user’s exact gradient and hence the reconstructed image is high quality. Without disaggregation, reconstruction quality degrades significantly.",
            "We furthermore perform the attack in (Geiping et al., 2020) to invert noisy FedAvg updates. Figure 8 and Table 3 show the results of inverting fedavg updates with local epochs = 4, batch size = 16, user data set size = 64, with and without gradient disaggregation (100 users, 2 layer MLP). With gradient disaggregation we achieve similar quality as inverting a single model update, whereas inverting an update aggregated over multiple users (users=10) significantly degrades reconstruction quality.",
            "We run our experiments using FedAvg model updates, on Cifar10, with 100 users, 200 rounds, participation rate of .1, constraint granularity of 10, SGD lr of 1e-3, on a LeNet CNN model. Table S1 shows the fraction of P𝑃P reconstructed across various FedAvg settings. Results show that with lower lr (1e-3), gradient disaggregation can exactly reconstruct P𝑃P when FedAvg updates are small (1-4 epochs). With more epochs of FedAvg (and larger dataset size or smaller batch size), increased noise prevents reconstruction of P𝑃P. We additionally tried the experiment with higher lr (1e-2), and disaggregation typically failed due to high noise, even with lower epochs of FedAvg. Our results indicate that under the right set of circumstances, the gradient disaggregation attack can be used in an honest but curious scenario; however, more robust results are achieved if the attacker can fix the neural network model.",
            "Table S2 shows the fraction of P𝑃P reconstructed across various proportions of dropped constraints. Results indicate that even when significant proportions of constraints are dropped, P𝑃P may be exactly recovered with more rounds of collected aggregated updates. Note that, when rounds << users reconstruction fails due to the rank being less than the number of users.",
            "Table S3 shows the results of gradient disaggregation when constraint noise 𝒩​(0,μ)𝒩0𝜇\\mathcal{N}(0,\\mu) is added to each constraint. As indicated, even in the presence of noise, gradient disaggregation may exactly recover P𝑃P with enough rounds.",
            "We provide extended data on gradient disaggregation against various parameter values of participation rate. Participation rate is the proportion of users that participate in sending model updates per round and impacts how many updates are summed to yield the aggregated update that is observed by the central server. Unless stated, we enforced a time limit of 10 minutes for solving each column; we use a constraint granularity of 10. We show our extended results in Table S4.",
            "We provide extended data on gradient disaggregation against various parameter values of constraint granularity. Constraint granularity is how precise summary statistics capture user partipation frequency (see main paper for details). Unless stated, we enforced a time limit of 10 minutes for solving each column; we use a constraint granularity of 10. We show our extended results in Table S5.",
            "We provide extended experiments on gradient disaggregation on FedAvg. We show our results in Table S6 and S7."
        ]
    }
}