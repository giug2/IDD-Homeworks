{
    "id_table_1": {
        "caption": "Table 1:  The table compares the framework performance for task planning across various language models, using evaluation metrics ranging from 0 to 1. The Pass Rate (PR) measures the success of task execution. Tool Usage Awareness (TUA) reflects the recognition of the need for tools, though this alone does not guarantee success. Accuracy (Acc) ensures the correct decomposition and sequencing of sub-tasks. Dependency Graph Consistency (DGC) verifies that sub-tasks are executed in the proper order.",
        "table": "S3.T1.1.1",
        "footnotes": [],
        "references": [
            "An invention is something that was impossible up to then; thats why governments grant patents.  (Robert A. Heinlein). A patent is a legal document that grants the inventor exclusive rights to make, use, and sell their invention for a specified period in exchange for public disclosure of the inventions details. Patent documents are essential for securing intellectual property rights and serve as a public record of technological innovation. While these documents may vary slightly by jurisdiction (e.g., USPTO, JPO, EPO), they generally consist of several key sections that ensure both legal protection and clear communication of the invention. A patent typically includes a title, abstract, background, summary, detailed description, claims, and illustrations. The title highlights the inventions main innovation, while the abstract provides a brief overview of the invention, focusing on its purpose, key features, and potential applications. The background section outlines existing solutions and the technical challenges the invention addresses. The detailed description provides in-depth technical specifications, including various embodiments (specific versions or implementations of the invention), processes (methods or procedures involved in making or using the invention), and use cases (practical applications or scenarios where the invention could be applied), explaining how the invention addresses the identified challenges. The claims accurately determine the scope of legal protection, outlining the legal boundaries of coverage. These claims can be independent, broadly covering the core features, or dependent, refining the scope by specifying particular embodiments. Other important sections include patent classification codes, citations to prior art, and illustrations accompanied by detailed descriptions that visually depict key aspects of the invention, aiding in the understanding of complex technical details. Collectively, these main sections ensure that the patent document meets legal requirements, communicates the invention effectively, and provides enforceable rights to the inventor. This emphasizes the necessity for precision and clarity in patent drafting to ensure it withstands legal scrutiny and future challenges. In recent times, Large Language Models (LLMs), such as OpenAI GPT-4 [ 1 ]  and Google Gemini [ 11 ] , have excelled in natural language processing tasks due to their proficiency in pattern recognition, contextual understanding, and generating coherent language based on learned data distributions. However, their application to patent-related tasks remains underexplored. Patents, as complex legal documents that protect intellectual property, combine technical information, requiring a deep understanding of the subject matter and precise language to describe complex, domain-specific concepts. Additionally, the rise in patent applications and the increasing difficulty of manual processing have created a growing need for LLMs to automate patent-related workflows. LLMs can enhance patent analysis, knowledge extraction, and document generation, transforming both the analysis and generation of patent content. In patent analysis, LLMs excel at automating the categorization of patents into relevant classes and subclasses based on their subject matter and technological field. For quality assessment, LLMs can evaluate patent novelty by comparing new patents against prior art, estimate the likelihood of approval based on historical data, and predict potential litigation by analyzing claim strength and market relevance. In multilingual translation, they maintain precise technical meanings across languages. In open-domain question answering (ODQA) tasks, LLMs using the Retrieval-Augmented Generation (RAG,  [ 6 ] ) approach, efficiently retrieve and synthesize technical details, claims, and legal information from patents, producing more accurate and grounded responses. Their advanced contextual understanding, combined with few-shot learning, enables them to adapt to new patents and extract precise information for ODQA tasks. Additionally, fine-tuning on domain-specific patents enhances their ability to recognize patterns unique to these tasks, resulting in the generation of detailed, context-relevant answers. This capability extends to more specialized tasks, such as extracting scientific hypotheses from patents, where LLMs support the identification of key scientific concepts, implicit assumptions, and underlying principles of the patented technology. This helps form testable hypotheses that may guide future research and innovation. For new patents, LLMs support the drafting process by generating sections such as descriptions and claims, ensuring the protection of a unique invention through adherence to strict legal requirements. These applications enhance efficiency, democratize access to patent information, and accelerate innovation. However, patent texts present unique challenges for general-purpose language models, such as specialized terminology, long contexts, and the need to generate precise and accurate text, setting them apart from conventional texts. Integrating LLMs into the patent process requires careful consideration of legal, ethical, and quality assurance aspects to ensure the integrity and fairness of the patent workflow. In this work, we present an autonomous multi-agent conversational framework ( PatExpert ) for patent analysis, orchestrated by a meta-agent (top-level agent) that coordinates multiple expert agents (sub-agents), offering a transformative solution for managing patent-related tasks with precision and efficiency. The meta-agent interprets user input, decomposes the complex workflow of patent processing into specialized sub-tasks, and delegates them to task-specific expert agents. Each expert agent is fine-tuned to handle specific patent-related tasks, enhancing both accuracy and relevance. Once an expert agent completes its assigned task, it routes the response back to the meta-agent, which synthesizes the information and provides the final output. By orchestrating collaboration among expert agents, the meta-agent integrates technical expertise with legal considerations, optimizing a wide range of tasks, including patent acceptance prediction, classification, abstractive summarization, claim generation, multi-patent analysis for ODQA tasks, and generating scientific hypotheses from patents. The collaborative problem-solving between the meta-agent and expert agents enhances the management of patent-related tasks, ensuring accuracy, efficiency, and compliance in handling complex challenges. Scientific hypothesis generation from patents, supported by subject-action-object (SAO) analysis, identifies implicit assumptions and core principles, enabling researchers to extract testable ideas and enhance the analysis of patent novelty and technical capabilities. Multi-patent analysis compares claims, technical details, and prior art across multiple patents to reveal novel aspects and streamline patent processing for efficient comparison and question answering. For question answering in multi-patent analysis, the expert agent uses Graph Retrieval-Augmented Generation (GRAG) to improve information extraction by combining semantic similarity with knowledge graphs, which helps in generating more accurate answers. Error handling is an integral component of the framework, managed by a critique agent (Gold-LLM-as-a-Judge and Reward-LLM-as-a-Judge). The critique agent evaluates outputs from the meta-agent using predefined metrics, assessing accuracy and providing feedback for iterative refinement. This feedback loop ensures that outputs are accurate, reducing the risk of inaccuracies. The multi-agent framework, equipped with error-handling mechanisms, offers a comprehensive solution for automating and optimizing complex patent analysis. The framework prioritizes explainability and transparency, with each expert agent providing clear explanations and justifications for its predictions to ensure that the rationale behind every decision is accessible and comprehensible. This approach enhances user trust and facilitates the interpretation of complex patent-related task outputs, such as patent acceptance and claim generation. For multi-patent analysis tasks, the framework is equipped with robust fact-checking and source citation capabilities, ensuring that responses are accurate and supported by reliable evidence. Its ability to effectively retrieve and organize information from structured graph databases allows it to synthesize clear, well-substantiated answers. This makes the framework an invaluable tool for patent analysis and decision-making, especially when handling queries that demand detailed, referenced information. Figure  1  illustrates the proposed framework.",
            "Table  1  presents the comparative performance of various language models and the  PatExpert  framework in task planning, evaluated using four key metrics: Tool Usage Awareness (TUA), Pass Rate (PR), Accuracy (Acc), and Dependency Graph Consistency (DGC). Scores range from 0 to 1, with higher values indicating better performance. Table  2  compares the models performance in tool selection based on the Recall@K, NDCG@K, and COMP@K metrics. Similarly, Table  3  outlines tool calling performance using Parameter Consistency (PC), Error Rate (ER), and Execution Accuracy (EA). Scores range from 0 to 1, with higher values indicating better performance, except for Error Rate, where lower values are preferable. Table  4  compares models using Exact Match (EM) and F1 scores for classification and acceptance prediction tasks. Table  5  shows performance on abstractive summarization using BLEU and ROUGE-L metrics. Table  6  presents patent claim generation results, while Table  7  displays results for multi-patent analysis, both evaluated using BLEU and ROUGE-L metrics. Table  8  presents a user-centric evaluation of various language models and the  PatExpert  framework, assessed using metrics such as Likert-Scale Satisfaction (LSS), Task Completion (TC), Context Awareness (CA), Adaptability (AD), Error Handling (EH), and Qualitative Feedback (QF). Table  9  compares the quality of knowledge graph construction between the  PatExpert  framework utilizing GPT-4o and various language models, using metrics such as Triple Accuracy (TA), Modularity (Mod), Conductance (Cond), and Graph Completeness (GC). In our experiments, baseline results for closed-source models like OpenAI GPT-4, Gemini 1.5 Pro, and Claude 3 were obtained without fine-tuning, given the impracticality of such an approach due to their large size and resource demands on consumer hardware. These proprietary models, accessed via their APIs, were evaluated directly on patent-related tasks without additional tuning. In contrast, our proposed framework  PatExpert  employs a fine-tuning strategy for the computational engines utilized by expert sub-agents, such as GPT-4o mini, customizing them for specific patent-related tasks. This fine-tuning enables the framework to achieve higher task-specific accuracy and relevance, especially in complex scenarios requiring patent expertise. While the frozen baseline models generally perform well due to their large-scale pre-training,  PatExpert s fine-tuned agents demonstrate competitive results, especially in specialized patent workflows. This highlights the benefit of task-specific fine-tuning in improving expert model performance for domain-specific applications. Across all tables,  PatExpert  consistently outperforms other models, achieving the highest scores in task planning, tool selection, and tool calling metrics. It leads with a TUA of 0.94, Acc of 0.91, and DGC of 0.95, reflecting its superior ability to recognize tools and maintain task consistency. In tool selection,  PatExpert  also excels with a Recall@K of 0.95 and an NDCG@K of 0.93, demonstrating high accuracy in selecting and ranking relevant tools. Finally, in tool calling, it shows exceptional performance with a PC of 0.95 and an EA of 0.96, along with the lowest ER of 0.04, indicating minimal errors and high execution accuracy. OpenAI GPT-4 and Claude 3 Opus follow closely in all metrics, while models like Gemini 1.5 Pro and Gemini 1.5 Flash perform moderately. OpenAI GPT-4 Turbo and Claude 3 Haiku score the lowest.  Tables  4 ,  5 ,  6 , and  7  demonstrate that the  PatExpert  framework consistently surpasses other models with a significant margin in tasks such as patent classification, acceptance, abstractive summarization, claim generation, and multi-patent analysis.",
            "where   direct-sum \\bigoplus   represents applying an aggregate-and-synthesize prompt to integrate multiple language model outputs into a coherent, high-quality response. In the final layer, a single, more capable Gold-LLM aggregates and synthesizes the outputs from the previous layers to generate a coherent, high-quality response. In the MoA technique, Gold-LLMs function either as proposers, generating diverse, context-rich responses, or as aggregators, synthesizing these into comprehensive outputs. Our implementation uses the LLaMA-3.1-405B-instruct and NemoTron-4-340B-instruct models as proposers, with GPT-4 Turbo serving as the aggregator. The architecture consists of two layers, each with two proposers and one aggregator working together to produce a coherent response. We utilize the Reward-LLM-as-a-Judge, such as the multidimensional Nvidia NemoTron-4-340B-Reward model, to evaluate the quality of the question-answer pairs, focusing on attributes such as helpfulness, correctness, coherence, complexity, and verbosity. Additionally, we utilize LLM-as-a-Judge, with GPT-4o acting as an expert judge to evaluate answers by rating them on relevance, accuracy, faithfulness, and coherence using a Likert scale from 1 to 5. At each layer, structured feedback from both the Reward-LLM-as-a-Judge and LLM-as-a-Judge models drives iterative refinement without fine-tuning, relying solely on prompting and generation to enhance response quality. In summary, the synthetic data generation pipeline operates by ingesting documents, generating diverse questions covering various scenarios and contexts, and using the MoA architecture to produce detailed, contextually relevant answers to fine-tune the expert model for multi-patent analysis. Our method then employs knowledge distillation to transfer knowledge from teacher models (LLaMA-3.1-405B-instruct, NemoTron-4-340B-instruct, GPT-4 Turbo) to a student model (GPT-4o mini), enabling the student model to replicate the larger models outputs and behaviors through instruction-tuning on synthetic RAFT datasets. We fine-tune the expert model (i.e., GPT-4o mini) on the multi-patent analysis task using synthetic RAFT datasets generated from the MoA framework. During inference, the relevant context related to the end-user question is retrieved from the knowledge graph database (discussed in Subsection  A.1.2 ), enhancing the expert model for multi-patent analysis response generation and resulting in coherent, knowledge-grounded, and accurate answers. In Subsection  A.1.2 , we discuss the construction of knowledge graphs from patents, followed by retrieval of relevant knowledge from the knowledge graph database, providing external context to the expert model for answering questions.",
            "In this work, we employed teacher-student transfer learning via knowledge distillation using Gold-LLMs, such as GPT-4o, to extract or generate scientific hypotheses from granted patents. This process created a synthetic dataset to fine-tune expert models, such as GPT-4o-mini, for the task of patent hypothesis generation. It is important to note that hypothesis generation provides a broader, conceptual explanation of the problem the invention solves and how it works, rather than focusing on legal protection. Patent claims, by contrast, define the specific legal boundaries and technical implementations that the patent protects, emphasizing the structural or procedural aspects of the invention in a precise and formal manner. We utilized Gold-LLMs, such as GPT-4o, to analyze a set of granted patents, extracting Subject-Action-Object (SAO) triplets to uncover the key hypotheses and innovations within the patent documents. The approach began by parsing the granted patents, focusing on key sections such as claims and detailed descriptions, which outline the scope of the patents innovation. The Gold-LLM identified and extracted SAO triplets, which are essential for identifying the central technical contributions of the patents. In this context, the subject represents the core invention or technology, the action describes the method or process being applied, and the object defines the outcome or product of the action. The Gold-LLM then synthesized the SAO triplets for each patent, generating a comprehensive hypothesis that represents the innovations and technical contributions of the patents. This hypothesis provides a high-level overview of the key technologies and innovations within each patent, allowing for the identification of unique contributions, and potential advancements. By extracting a hypothesis for each patent, it becomes possible to cross-reference these insights with prior art and technical literature, ensuring that the innovations are novel and consistent with the granted claims. This process enables scalable, automated analysis of patents, streamlining the detection of valuable technological breakthroughs and trends in patent data. We used standard natural language processing (NLP) metrics such as BLEU, ROUGE, and METEOR to compare the generated hypotheses from expert models against reference summaries from Gold LLMs. Table  10  illustrates the hypothesis generation for the Universal Transformers  [ 2 ]  patent using a Gold LLM such as GPT-4o."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  The table illustrates framework performance on tool selection, evaluated using metrics (Recall@K, NDCG@K, COMP@K: all ranging from 0 to 1) against various proprietary language models. Recall@K measures how many relevant tools were retrieved within the top-K. NDCG@K evaluates how well the relevant tools are ranked in the top-K. COMP@K checks whether all the relevant tools were included in the top-K, focusing on completeness.",
        "table": "S3.T2.1.1",
        "footnotes": [],
        "references": [
            "Table  1  presents the comparative performance of various language models and the  PatExpert  framework in task planning, evaluated using four key metrics: Tool Usage Awareness (TUA), Pass Rate (PR), Accuracy (Acc), and Dependency Graph Consistency (DGC). Scores range from 0 to 1, with higher values indicating better performance. Table  2  compares the models performance in tool selection based on the Recall@K, NDCG@K, and COMP@K metrics. Similarly, Table  3  outlines tool calling performance using Parameter Consistency (PC), Error Rate (ER), and Execution Accuracy (EA). Scores range from 0 to 1, with higher values indicating better performance, except for Error Rate, where lower values are preferable. Table  4  compares models using Exact Match (EM) and F1 scores for classification and acceptance prediction tasks. Table  5  shows performance on abstractive summarization using BLEU and ROUGE-L metrics. Table  6  presents patent claim generation results, while Table  7  displays results for multi-patent analysis, both evaluated using BLEU and ROUGE-L metrics. Table  8  presents a user-centric evaluation of various language models and the  PatExpert  framework, assessed using metrics such as Likert-Scale Satisfaction (LSS), Task Completion (TC), Context Awareness (CA), Adaptability (AD), Error Handling (EH), and Qualitative Feedback (QF). Table  9  compares the quality of knowledge graph construction between the  PatExpert  framework utilizing GPT-4o and various language models, using metrics such as Triple Accuracy (TA), Modularity (Mod), Conductance (Cond), and Graph Completeness (GC). In our experiments, baseline results for closed-source models like OpenAI GPT-4, Gemini 1.5 Pro, and Claude 3 were obtained without fine-tuning, given the impracticality of such an approach due to their large size and resource demands on consumer hardware. These proprietary models, accessed via their APIs, were evaluated directly on patent-related tasks without additional tuning. In contrast, our proposed framework  PatExpert  employs a fine-tuning strategy for the computational engines utilized by expert sub-agents, such as GPT-4o mini, customizing them for specific patent-related tasks. This fine-tuning enables the framework to achieve higher task-specific accuracy and relevance, especially in complex scenarios requiring patent expertise. While the frozen baseline models generally perform well due to their large-scale pre-training,  PatExpert s fine-tuned agents demonstrate competitive results, especially in specialized patent workflows. This highlights the benefit of task-specific fine-tuning in improving expert model performance for domain-specific applications. Across all tables,  PatExpert  consistently outperforms other models, achieving the highest scores in task planning, tool selection, and tool calling metrics. It leads with a TUA of 0.94, Acc of 0.91, and DGC of 0.95, reflecting its superior ability to recognize tools and maintain task consistency. In tool selection,  PatExpert  also excels with a Recall@K of 0.95 and an NDCG@K of 0.93, demonstrating high accuracy in selecting and ranking relevant tools. Finally, in tool calling, it shows exceptional performance with a PC of 0.95 and an EA of 0.96, along with the lowest ER of 0.04, indicating minimal errors and high execution accuracy. OpenAI GPT-4 and Claude 3 Opus follow closely in all metrics, while models like Gemini 1.5 Pro and Gemini 1.5 Flash perform moderately. OpenAI GPT-4 Turbo and Claude 3 Haiku score the lowest.  Tables  4 ,  5 ,  6 , and  7  demonstrate that the  PatExpert  framework consistently surpasses other models with a significant margin in tasks such as patent classification, acceptance, abstractive summarization, claim generation, and multi-patent analysis.",
            "where   direct-sum \\bigoplus   represents applying an aggregate-and-synthesize prompt to integrate multiple language model outputs into a coherent, high-quality response. In the final layer, a single, more capable Gold-LLM aggregates and synthesizes the outputs from the previous layers to generate a coherent, high-quality response. In the MoA technique, Gold-LLMs function either as proposers, generating diverse, context-rich responses, or as aggregators, synthesizing these into comprehensive outputs. Our implementation uses the LLaMA-3.1-405B-instruct and NemoTron-4-340B-instruct models as proposers, with GPT-4 Turbo serving as the aggregator. The architecture consists of two layers, each with two proposers and one aggregator working together to produce a coherent response. We utilize the Reward-LLM-as-a-Judge, such as the multidimensional Nvidia NemoTron-4-340B-Reward model, to evaluate the quality of the question-answer pairs, focusing on attributes such as helpfulness, correctness, coherence, complexity, and verbosity. Additionally, we utilize LLM-as-a-Judge, with GPT-4o acting as an expert judge to evaluate answers by rating them on relevance, accuracy, faithfulness, and coherence using a Likert scale from 1 to 5. At each layer, structured feedback from both the Reward-LLM-as-a-Judge and LLM-as-a-Judge models drives iterative refinement without fine-tuning, relying solely on prompting and generation to enhance response quality. In summary, the synthetic data generation pipeline operates by ingesting documents, generating diverse questions covering various scenarios and contexts, and using the MoA architecture to produce detailed, contextually relevant answers to fine-tune the expert model for multi-patent analysis. Our method then employs knowledge distillation to transfer knowledge from teacher models (LLaMA-3.1-405B-instruct, NemoTron-4-340B-instruct, GPT-4 Turbo) to a student model (GPT-4o mini), enabling the student model to replicate the larger models outputs and behaviors through instruction-tuning on synthetic RAFT datasets. We fine-tune the expert model (i.e., GPT-4o mini) on the multi-patent analysis task using synthetic RAFT datasets generated from the MoA framework. During inference, the relevant context related to the end-user question is retrieved from the knowledge graph database (discussed in Subsection  A.1.2 ), enhancing the expert model for multi-patent analysis response generation and resulting in coherent, knowledge-grounded, and accurate answers. In Subsection  A.1.2 , we discuss the construction of knowledge graphs from patents, followed by retrieval of relevant knowledge from the knowledge graph database, providing external context to the expert model for answering questions."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  The table demonstrates the frameworks performance on tool calling using metrics (PC, ER, EA: 0 to 1) compared to various closed-source language models. PC (Parameter Consistency) measures how consistently correct parameters are identified for tool invocation. ER (Error Rate) measures the proportion of incorrect or missing parameters (lower is better). EA (Execution Accuracy) measures the success rate of tool invocation based on the provided parameters.",
        "table": "S3.T3.1.1",
        "footnotes": [],
        "references": [
            "Table  1  presents the comparative performance of various language models and the  PatExpert  framework in task planning, evaluated using four key metrics: Tool Usage Awareness (TUA), Pass Rate (PR), Accuracy (Acc), and Dependency Graph Consistency (DGC). Scores range from 0 to 1, with higher values indicating better performance. Table  2  compares the models performance in tool selection based on the Recall@K, NDCG@K, and COMP@K metrics. Similarly, Table  3  outlines tool calling performance using Parameter Consistency (PC), Error Rate (ER), and Execution Accuracy (EA). Scores range from 0 to 1, with higher values indicating better performance, except for Error Rate, where lower values are preferable. Table  4  compares models using Exact Match (EM) and F1 scores for classification and acceptance prediction tasks. Table  5  shows performance on abstractive summarization using BLEU and ROUGE-L metrics. Table  6  presents patent claim generation results, while Table  7  displays results for multi-patent analysis, both evaluated using BLEU and ROUGE-L metrics. Table  8  presents a user-centric evaluation of various language models and the  PatExpert  framework, assessed using metrics such as Likert-Scale Satisfaction (LSS), Task Completion (TC), Context Awareness (CA), Adaptability (AD), Error Handling (EH), and Qualitative Feedback (QF). Table  9  compares the quality of knowledge graph construction between the  PatExpert  framework utilizing GPT-4o and various language models, using metrics such as Triple Accuracy (TA), Modularity (Mod), Conductance (Cond), and Graph Completeness (GC). In our experiments, baseline results for closed-source models like OpenAI GPT-4, Gemini 1.5 Pro, and Claude 3 were obtained without fine-tuning, given the impracticality of such an approach due to their large size and resource demands on consumer hardware. These proprietary models, accessed via their APIs, were evaluated directly on patent-related tasks without additional tuning. In contrast, our proposed framework  PatExpert  employs a fine-tuning strategy for the computational engines utilized by expert sub-agents, such as GPT-4o mini, customizing them for specific patent-related tasks. This fine-tuning enables the framework to achieve higher task-specific accuracy and relevance, especially in complex scenarios requiring patent expertise. While the frozen baseline models generally perform well due to their large-scale pre-training,  PatExpert s fine-tuned agents demonstrate competitive results, especially in specialized patent workflows. This highlights the benefit of task-specific fine-tuning in improving expert model performance for domain-specific applications. Across all tables,  PatExpert  consistently outperforms other models, achieving the highest scores in task planning, tool selection, and tool calling metrics. It leads with a TUA of 0.94, Acc of 0.91, and DGC of 0.95, reflecting its superior ability to recognize tools and maintain task consistency. In tool selection,  PatExpert  also excels with a Recall@K of 0.95 and an NDCG@K of 0.93, demonstrating high accuracy in selecting and ranking relevant tools. Finally, in tool calling, it shows exceptional performance with a PC of 0.95 and an EA of 0.96, along with the lowest ER of 0.04, indicating minimal errors and high execution accuracy. OpenAI GPT-4 and Claude 3 Opus follow closely in all metrics, while models like Gemini 1.5 Pro and Gemini 1.5 Flash perform moderately. OpenAI GPT-4 Turbo and Claude 3 Haiku score the lowest.  Tables  4 ,  5 ,  6 , and  7  demonstrate that the  PatExpert  framework consistently surpasses other models with a significant margin in tasks such as patent classification, acceptance, abstractive summarization, claim generation, and multi-patent analysis."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  The table presents the performance of various language models and the  PatExpert  framework. Results are shown for both classification and acceptance tasks, with Exact Match (EM) and F1 scores (range: 0 to 1) for each task. In patent classification and acceptance tasks, Exact Match (EM) measures the percentage of exact prediction matches, while F1 Score balances precision and recall, evaluating how well the model predicts correct classes or decisions.",
        "table": "A1.T4.1.1",
        "footnotes": [],
        "references": [
            "Table  1  presents the comparative performance of various language models and the  PatExpert  framework in task planning, evaluated using four key metrics: Tool Usage Awareness (TUA), Pass Rate (PR), Accuracy (Acc), and Dependency Graph Consistency (DGC). Scores range from 0 to 1, with higher values indicating better performance. Table  2  compares the models performance in tool selection based on the Recall@K, NDCG@K, and COMP@K metrics. Similarly, Table  3  outlines tool calling performance using Parameter Consistency (PC), Error Rate (ER), and Execution Accuracy (EA). Scores range from 0 to 1, with higher values indicating better performance, except for Error Rate, where lower values are preferable. Table  4  compares models using Exact Match (EM) and F1 scores for classification and acceptance prediction tasks. Table  5  shows performance on abstractive summarization using BLEU and ROUGE-L metrics. Table  6  presents patent claim generation results, while Table  7  displays results for multi-patent analysis, both evaluated using BLEU and ROUGE-L metrics. Table  8  presents a user-centric evaluation of various language models and the  PatExpert  framework, assessed using metrics such as Likert-Scale Satisfaction (LSS), Task Completion (TC), Context Awareness (CA), Adaptability (AD), Error Handling (EH), and Qualitative Feedback (QF). Table  9  compares the quality of knowledge graph construction between the  PatExpert  framework utilizing GPT-4o and various language models, using metrics such as Triple Accuracy (TA), Modularity (Mod), Conductance (Cond), and Graph Completeness (GC). In our experiments, baseline results for closed-source models like OpenAI GPT-4, Gemini 1.5 Pro, and Claude 3 were obtained without fine-tuning, given the impracticality of such an approach due to their large size and resource demands on consumer hardware. These proprietary models, accessed via their APIs, were evaluated directly on patent-related tasks without additional tuning. In contrast, our proposed framework  PatExpert  employs a fine-tuning strategy for the computational engines utilized by expert sub-agents, such as GPT-4o mini, customizing them for specific patent-related tasks. This fine-tuning enables the framework to achieve higher task-specific accuracy and relevance, especially in complex scenarios requiring patent expertise. While the frozen baseline models generally perform well due to their large-scale pre-training,  PatExpert s fine-tuned agents demonstrate competitive results, especially in specialized patent workflows. This highlights the benefit of task-specific fine-tuning in improving expert model performance for domain-specific applications. Across all tables,  PatExpert  consistently outperforms other models, achieving the highest scores in task planning, tool selection, and tool calling metrics. It leads with a TUA of 0.94, Acc of 0.91, and DGC of 0.95, reflecting its superior ability to recognize tools and maintain task consistency. In tool selection,  PatExpert  also excels with a Recall@K of 0.95 and an NDCG@K of 0.93, demonstrating high accuracy in selecting and ranking relevant tools. Finally, in tool calling, it shows exceptional performance with a PC of 0.95 and an EA of 0.96, along with the lowest ER of 0.04, indicating minimal errors and high execution accuracy. OpenAI GPT-4 and Claude 3 Opus follow closely in all metrics, while models like Gemini 1.5 Pro and Gemini 1.5 Flash perform moderately. OpenAI GPT-4 Turbo and Claude 3 Haiku score the lowest.  Tables  4 ,  5 ,  6 , and  7  demonstrate that the  PatExpert  framework consistently surpasses other models with a significant margin in tasks such as patent classification, acceptance, abstractive summarization, claim generation, and multi-patent analysis.",
            "As shown in all tables (Table  4 , Table  5 , Table  6 , Table  7 ),  PatExpert  framework consistently outperforms other models in patent classification, acceptance, abstractive summarization, patent claim generation, and multi-patent analysis tasks."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  The table presents the performance of various language models and the  PatExpert  framework for abstractive summarization tasks. The models are evaluated using BLEU and ROUGE-L metrics (range: 0 to 1), where higher values indicate better performance in generating coherent and concise summaries.",
        "table": "A1.T5.1.1",
        "footnotes": [],
        "references": [
            "Table  1  presents the comparative performance of various language models and the  PatExpert  framework in task planning, evaluated using four key metrics: Tool Usage Awareness (TUA), Pass Rate (PR), Accuracy (Acc), and Dependency Graph Consistency (DGC). Scores range from 0 to 1, with higher values indicating better performance. Table  2  compares the models performance in tool selection based on the Recall@K, NDCG@K, and COMP@K metrics. Similarly, Table  3  outlines tool calling performance using Parameter Consistency (PC), Error Rate (ER), and Execution Accuracy (EA). Scores range from 0 to 1, with higher values indicating better performance, except for Error Rate, where lower values are preferable. Table  4  compares models using Exact Match (EM) and F1 scores for classification and acceptance prediction tasks. Table  5  shows performance on abstractive summarization using BLEU and ROUGE-L metrics. Table  6  presents patent claim generation results, while Table  7  displays results for multi-patent analysis, both evaluated using BLEU and ROUGE-L metrics. Table  8  presents a user-centric evaluation of various language models and the  PatExpert  framework, assessed using metrics such as Likert-Scale Satisfaction (LSS), Task Completion (TC), Context Awareness (CA), Adaptability (AD), Error Handling (EH), and Qualitative Feedback (QF). Table  9  compares the quality of knowledge graph construction between the  PatExpert  framework utilizing GPT-4o and various language models, using metrics such as Triple Accuracy (TA), Modularity (Mod), Conductance (Cond), and Graph Completeness (GC). In our experiments, baseline results for closed-source models like OpenAI GPT-4, Gemini 1.5 Pro, and Claude 3 were obtained without fine-tuning, given the impracticality of such an approach due to their large size and resource demands on consumer hardware. These proprietary models, accessed via their APIs, were evaluated directly on patent-related tasks without additional tuning. In contrast, our proposed framework  PatExpert  employs a fine-tuning strategy for the computational engines utilized by expert sub-agents, such as GPT-4o mini, customizing them for specific patent-related tasks. This fine-tuning enables the framework to achieve higher task-specific accuracy and relevance, especially in complex scenarios requiring patent expertise. While the frozen baseline models generally perform well due to their large-scale pre-training,  PatExpert s fine-tuned agents demonstrate competitive results, especially in specialized patent workflows. This highlights the benefit of task-specific fine-tuning in improving expert model performance for domain-specific applications. Across all tables,  PatExpert  consistently outperforms other models, achieving the highest scores in task planning, tool selection, and tool calling metrics. It leads with a TUA of 0.94, Acc of 0.91, and DGC of 0.95, reflecting its superior ability to recognize tools and maintain task consistency. In tool selection,  PatExpert  also excels with a Recall@K of 0.95 and an NDCG@K of 0.93, demonstrating high accuracy in selecting and ranking relevant tools. Finally, in tool calling, it shows exceptional performance with a PC of 0.95 and an EA of 0.96, along with the lowest ER of 0.04, indicating minimal errors and high execution accuracy. OpenAI GPT-4 and Claude 3 Opus follow closely in all metrics, while models like Gemini 1.5 Pro and Gemini 1.5 Flash perform moderately. OpenAI GPT-4 Turbo and Claude 3 Haiku score the lowest.  Tables  4 ,  5 ,  6 , and  7  demonstrate that the  PatExpert  framework consistently surpasses other models with a significant margin in tasks such as patent classification, acceptance, abstractive summarization, claim generation, and multi-patent analysis.",
            "As shown in all tables (Table  4 , Table  5 , Table  6 , Table  7 ),  PatExpert  framework consistently outperforms other models in patent classification, acceptance, abstractive summarization, patent claim generation, and multi-patent analysis tasks."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  The table summarizes the performance of several language models, including the  PatExpert  framework, on patent claim generation tasks. Evaluation is based on BLEU and ROUGE-L metrics (scored from 0 to 1), with higher values reflecting improved accuracy and conciseness in the generated patent claims.",
        "table": "A1.T6.1.1",
        "footnotes": [],
        "references": [
            "Table  1  presents the comparative performance of various language models and the  PatExpert  framework in task planning, evaluated using four key metrics: Tool Usage Awareness (TUA), Pass Rate (PR), Accuracy (Acc), and Dependency Graph Consistency (DGC). Scores range from 0 to 1, with higher values indicating better performance. Table  2  compares the models performance in tool selection based on the Recall@K, NDCG@K, and COMP@K metrics. Similarly, Table  3  outlines tool calling performance using Parameter Consistency (PC), Error Rate (ER), and Execution Accuracy (EA). Scores range from 0 to 1, with higher values indicating better performance, except for Error Rate, where lower values are preferable. Table  4  compares models using Exact Match (EM) and F1 scores for classification and acceptance prediction tasks. Table  5  shows performance on abstractive summarization using BLEU and ROUGE-L metrics. Table  6  presents patent claim generation results, while Table  7  displays results for multi-patent analysis, both evaluated using BLEU and ROUGE-L metrics. Table  8  presents a user-centric evaluation of various language models and the  PatExpert  framework, assessed using metrics such as Likert-Scale Satisfaction (LSS), Task Completion (TC), Context Awareness (CA), Adaptability (AD), Error Handling (EH), and Qualitative Feedback (QF). Table  9  compares the quality of knowledge graph construction between the  PatExpert  framework utilizing GPT-4o and various language models, using metrics such as Triple Accuracy (TA), Modularity (Mod), Conductance (Cond), and Graph Completeness (GC). In our experiments, baseline results for closed-source models like OpenAI GPT-4, Gemini 1.5 Pro, and Claude 3 were obtained without fine-tuning, given the impracticality of such an approach due to their large size and resource demands on consumer hardware. These proprietary models, accessed via their APIs, were evaluated directly on patent-related tasks without additional tuning. In contrast, our proposed framework  PatExpert  employs a fine-tuning strategy for the computational engines utilized by expert sub-agents, such as GPT-4o mini, customizing them for specific patent-related tasks. This fine-tuning enables the framework to achieve higher task-specific accuracy and relevance, especially in complex scenarios requiring patent expertise. While the frozen baseline models generally perform well due to their large-scale pre-training,  PatExpert s fine-tuned agents demonstrate competitive results, especially in specialized patent workflows. This highlights the benefit of task-specific fine-tuning in improving expert model performance for domain-specific applications. Across all tables,  PatExpert  consistently outperforms other models, achieving the highest scores in task planning, tool selection, and tool calling metrics. It leads with a TUA of 0.94, Acc of 0.91, and DGC of 0.95, reflecting its superior ability to recognize tools and maintain task consistency. In tool selection,  PatExpert  also excels with a Recall@K of 0.95 and an NDCG@K of 0.93, demonstrating high accuracy in selecting and ranking relevant tools. Finally, in tool calling, it shows exceptional performance with a PC of 0.95 and an EA of 0.96, along with the lowest ER of 0.04, indicating minimal errors and high execution accuracy. OpenAI GPT-4 and Claude 3 Opus follow closely in all metrics, while models like Gemini 1.5 Pro and Gemini 1.5 Flash perform moderately. OpenAI GPT-4 Turbo and Claude 3 Haiku score the lowest.  Tables  4 ,  5 ,  6 , and  7  demonstrate that the  PatExpert  framework consistently surpasses other models with a significant margin in tasks such as patent classification, acceptance, abstractive summarization, claim generation, and multi-patent analysis.",
            "As shown in all tables (Table  4 , Table  5 , Table  6 , Table  7 ),  PatExpert  framework consistently outperforms other models in patent classification, acceptance, abstractive summarization, patent claim generation, and multi-patent analysis tasks."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  The table summarizes the performance of various language models and the  PatExpert  framework in multi-patent analysis tasks. The models are assessed using BLEU and ROUGE-L metrics (scored from 0 to 1), where higher scores indicate better ability to accurately synthesize information across multiple patents.",
        "table": "A1.T7.1.1",
        "footnotes": [],
        "references": [
            "Table  1  presents the comparative performance of various language models and the  PatExpert  framework in task planning, evaluated using four key metrics: Tool Usage Awareness (TUA), Pass Rate (PR), Accuracy (Acc), and Dependency Graph Consistency (DGC). Scores range from 0 to 1, with higher values indicating better performance. Table  2  compares the models performance in tool selection based on the Recall@K, NDCG@K, and COMP@K metrics. Similarly, Table  3  outlines tool calling performance using Parameter Consistency (PC), Error Rate (ER), and Execution Accuracy (EA). Scores range from 0 to 1, with higher values indicating better performance, except for Error Rate, where lower values are preferable. Table  4  compares models using Exact Match (EM) and F1 scores for classification and acceptance prediction tasks. Table  5  shows performance on abstractive summarization using BLEU and ROUGE-L metrics. Table  6  presents patent claim generation results, while Table  7  displays results for multi-patent analysis, both evaluated using BLEU and ROUGE-L metrics. Table  8  presents a user-centric evaluation of various language models and the  PatExpert  framework, assessed using metrics such as Likert-Scale Satisfaction (LSS), Task Completion (TC), Context Awareness (CA), Adaptability (AD), Error Handling (EH), and Qualitative Feedback (QF). Table  9  compares the quality of knowledge graph construction between the  PatExpert  framework utilizing GPT-4o and various language models, using metrics such as Triple Accuracy (TA), Modularity (Mod), Conductance (Cond), and Graph Completeness (GC). In our experiments, baseline results for closed-source models like OpenAI GPT-4, Gemini 1.5 Pro, and Claude 3 were obtained without fine-tuning, given the impracticality of such an approach due to their large size and resource demands on consumer hardware. These proprietary models, accessed via their APIs, were evaluated directly on patent-related tasks without additional tuning. In contrast, our proposed framework  PatExpert  employs a fine-tuning strategy for the computational engines utilized by expert sub-agents, such as GPT-4o mini, customizing them for specific patent-related tasks. This fine-tuning enables the framework to achieve higher task-specific accuracy and relevance, especially in complex scenarios requiring patent expertise. While the frozen baseline models generally perform well due to their large-scale pre-training,  PatExpert s fine-tuned agents demonstrate competitive results, especially in specialized patent workflows. This highlights the benefit of task-specific fine-tuning in improving expert model performance for domain-specific applications. Across all tables,  PatExpert  consistently outperforms other models, achieving the highest scores in task planning, tool selection, and tool calling metrics. It leads with a TUA of 0.94, Acc of 0.91, and DGC of 0.95, reflecting its superior ability to recognize tools and maintain task consistency. In tool selection,  PatExpert  also excels with a Recall@K of 0.95 and an NDCG@K of 0.93, demonstrating high accuracy in selecting and ranking relevant tools. Finally, in tool calling, it shows exceptional performance with a PC of 0.95 and an EA of 0.96, along with the lowest ER of 0.04, indicating minimal errors and high execution accuracy. OpenAI GPT-4 and Claude 3 Opus follow closely in all metrics, while models like Gemini 1.5 Pro and Gemini 1.5 Flash perform moderately. OpenAI GPT-4 Turbo and Claude 3 Haiku score the lowest.  Tables  4 ,  5 ,  6 , and  7  demonstrate that the  PatExpert  framework consistently surpasses other models with a significant margin in tasks such as patent classification, acceptance, abstractive summarization, claim generation, and multi-patent analysis.",
            "As shown in all tables (Table  4 , Table  5 , Table  6 , Table  7 ),  PatExpert  framework consistently outperforms other models in patent classification, acceptance, abstractive summarization, patent claim generation, and multi-patent analysis tasks."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  The table showcases user-centric evaluation metrics comparing the framework performance to various language models (LSS, CA, AD, EH: 1 to 5; TC: Yes/No; QF: High, Medium-High, Medium).",
        "table": "A1.T8.1.1",
        "footnotes": [],
        "references": [
            "Table  1  presents the comparative performance of various language models and the  PatExpert  framework in task planning, evaluated using four key metrics: Tool Usage Awareness (TUA), Pass Rate (PR), Accuracy (Acc), and Dependency Graph Consistency (DGC). Scores range from 0 to 1, with higher values indicating better performance. Table  2  compares the models performance in tool selection based on the Recall@K, NDCG@K, and COMP@K metrics. Similarly, Table  3  outlines tool calling performance using Parameter Consistency (PC), Error Rate (ER), and Execution Accuracy (EA). Scores range from 0 to 1, with higher values indicating better performance, except for Error Rate, where lower values are preferable. Table  4  compares models using Exact Match (EM) and F1 scores for classification and acceptance prediction tasks. Table  5  shows performance on abstractive summarization using BLEU and ROUGE-L metrics. Table  6  presents patent claim generation results, while Table  7  displays results for multi-patent analysis, both evaluated using BLEU and ROUGE-L metrics. Table  8  presents a user-centric evaluation of various language models and the  PatExpert  framework, assessed using metrics such as Likert-Scale Satisfaction (LSS), Task Completion (TC), Context Awareness (CA), Adaptability (AD), Error Handling (EH), and Qualitative Feedback (QF). Table  9  compares the quality of knowledge graph construction between the  PatExpert  framework utilizing GPT-4o and various language models, using metrics such as Triple Accuracy (TA), Modularity (Mod), Conductance (Cond), and Graph Completeness (GC). In our experiments, baseline results for closed-source models like OpenAI GPT-4, Gemini 1.5 Pro, and Claude 3 were obtained without fine-tuning, given the impracticality of such an approach due to their large size and resource demands on consumer hardware. These proprietary models, accessed via their APIs, were evaluated directly on patent-related tasks without additional tuning. In contrast, our proposed framework  PatExpert  employs a fine-tuning strategy for the computational engines utilized by expert sub-agents, such as GPT-4o mini, customizing them for specific patent-related tasks. This fine-tuning enables the framework to achieve higher task-specific accuracy and relevance, especially in complex scenarios requiring patent expertise. While the frozen baseline models generally perform well due to their large-scale pre-training,  PatExpert s fine-tuned agents demonstrate competitive results, especially in specialized patent workflows. This highlights the benefit of task-specific fine-tuning in improving expert model performance for domain-specific applications. Across all tables,  PatExpert  consistently outperforms other models, achieving the highest scores in task planning, tool selection, and tool calling metrics. It leads with a TUA of 0.94, Acc of 0.91, and DGC of 0.95, reflecting its superior ability to recognize tools and maintain task consistency. In tool selection,  PatExpert  also excels with a Recall@K of 0.95 and an NDCG@K of 0.93, demonstrating high accuracy in selecting and ranking relevant tools. Finally, in tool calling, it shows exceptional performance with a PC of 0.95 and an EA of 0.96, along with the lowest ER of 0.04, indicating minimal errors and high execution accuracy. OpenAI GPT-4 and Claude 3 Opus follow closely in all metrics, while models like Gemini 1.5 Pro and Gemini 1.5 Flash perform moderately. OpenAI GPT-4 Turbo and Claude 3 Haiku score the lowest.  Tables  4 ,  5 ,  6 , and  7  demonstrate that the  PatExpert  framework consistently surpasses other models with a significant margin in tasks such as patent classification, acceptance, abstractive summarization, claim generation, and multi-patent analysis."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  The table presents a comparison of structured knowledge graph quality metrics, which compares the quality of structured knowledge graphs generated by a framework using GPT-4o and various language models. The comparison includes the following metrics: TA (0%-100%), Mod (-1 to 1), Cond (0 to 1), and GC (0 to 1).",
        "table": "A1.T9.1.1",
        "footnotes": [],
        "references": [
            "Table  1  presents the comparative performance of various language models and the  PatExpert  framework in task planning, evaluated using four key metrics: Tool Usage Awareness (TUA), Pass Rate (PR), Accuracy (Acc), and Dependency Graph Consistency (DGC). Scores range from 0 to 1, with higher values indicating better performance. Table  2  compares the models performance in tool selection based on the Recall@K, NDCG@K, and COMP@K metrics. Similarly, Table  3  outlines tool calling performance using Parameter Consistency (PC), Error Rate (ER), and Execution Accuracy (EA). Scores range from 0 to 1, with higher values indicating better performance, except for Error Rate, where lower values are preferable. Table  4  compares models using Exact Match (EM) and F1 scores for classification and acceptance prediction tasks. Table  5  shows performance on abstractive summarization using BLEU and ROUGE-L metrics. Table  6  presents patent claim generation results, while Table  7  displays results for multi-patent analysis, both evaluated using BLEU and ROUGE-L metrics. Table  8  presents a user-centric evaluation of various language models and the  PatExpert  framework, assessed using metrics such as Likert-Scale Satisfaction (LSS), Task Completion (TC), Context Awareness (CA), Adaptability (AD), Error Handling (EH), and Qualitative Feedback (QF). Table  9  compares the quality of knowledge graph construction between the  PatExpert  framework utilizing GPT-4o and various language models, using metrics such as Triple Accuracy (TA), Modularity (Mod), Conductance (Cond), and Graph Completeness (GC). In our experiments, baseline results for closed-source models like OpenAI GPT-4, Gemini 1.5 Pro, and Claude 3 were obtained without fine-tuning, given the impracticality of such an approach due to their large size and resource demands on consumer hardware. These proprietary models, accessed via their APIs, were evaluated directly on patent-related tasks without additional tuning. In contrast, our proposed framework  PatExpert  employs a fine-tuning strategy for the computational engines utilized by expert sub-agents, such as GPT-4o mini, customizing them for specific patent-related tasks. This fine-tuning enables the framework to achieve higher task-specific accuracy and relevance, especially in complex scenarios requiring patent expertise. While the frozen baseline models generally perform well due to their large-scale pre-training,  PatExpert s fine-tuned agents demonstrate competitive results, especially in specialized patent workflows. This highlights the benefit of task-specific fine-tuning in improving expert model performance for domain-specific applications. Across all tables,  PatExpert  consistently outperforms other models, achieving the highest scores in task planning, tool selection, and tool calling metrics. It leads with a TUA of 0.94, Acc of 0.91, and DGC of 0.95, reflecting its superior ability to recognize tools and maintain task consistency. In tool selection,  PatExpert  also excels with a Recall@K of 0.95 and an NDCG@K of 0.93, demonstrating high accuracy in selecting and ranking relevant tools. Finally, in tool calling, it shows exceptional performance with a PC of 0.95 and an EA of 0.96, along with the lowest ER of 0.04, indicating minimal errors and high execution accuracy. OpenAI GPT-4 and Claude 3 Opus follow closely in all metrics, while models like Gemini 1.5 Pro and Gemini 1.5 Flash perform moderately. OpenAI GPT-4 Turbo and Claude 3 Haiku score the lowest.  Tables  4 ,  5 ,  6 , and  7  demonstrate that the  PatExpert  framework consistently surpasses other models with a significant margin in tasks such as patent classification, acceptance, abstractive summarization, claim generation, and multi-patent analysis."
        ]
    },
    "global_footnotes": []
}