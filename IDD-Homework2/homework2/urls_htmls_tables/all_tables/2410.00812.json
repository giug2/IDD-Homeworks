{
    "id_table_1": {
        "caption": "Table 1:  Suffixes added to the prompt for generating the story paragraph for each ROI.",
        "table": "A1.T1.3",
        "footnotes": [],
        "references": [
            "We designed GEM-V to interpret  language encoding models , which are data-driven, LLM-based models of cortical language selectivity. We thus began by fitting encoding models ( Fig.   1 a) for each voxel in each of 3 human subjects using 20 hours of passive language listening fMRI data per subject collected in a prior study  [ 11 ] . The 20 hours of speech stimuli were transcribed and then passed through multiple open-source large language models  [ 12 ,  13 ]  to obtain activation vectors for each word. Multiple models were built using different LLMs to ensure that predictions are stable  [ 14 ,  15 ] , i.e. not idiosyncratic to a single model. Regularized linear regression was then used to predict the response timecourse of each voxel as a weighted combination of LLM activations. Encoding models were tested by predicting responses on held out fMRI data and then computing the correlation between predicted and actual responses. Encoding models using advanced LLMs achieve very high prediction performance  [ 8 ] ; here, only models with sufficient prediction performance ( r > 0.15 r 0.15 r>0.15 italic_r > 0.15 ) were used in further analyses, with many voxels predicted at  r > 0.7 r 0.7 r>0.7 italic_r > 0.7 . Full preprocessing, modeling, and data collection details are provided in the  Methods .",
            "Each encoding model represents the language selectivity of one part of the brain, but does so as a linear combination of LLM activations that are not human-interpretable. The first step in GEM-V is to convert these models into concise natural language explanations: a word or short phrase summarizing the properties of language that drive responses most strongly. This was accomplished by using an instruction-finetuned LLM  [ 16 ]  to generate candidate explanations by summarizing the  n -grams that yield the largest predictions from the model ( Fig.   1 b)  [ 17 ] . The resulting explanation is given in easily understandable natural language. For example, we found that some voxels appeared to be selective for language describing  Food preparation  ( Fig.   1 b). To ensure that the generated explanation correctly captures the function computed by the encoding model, we used the same LLM to generate synthetic text from the explanation and checked that this text could successfully drive the voxel encoding model. However, this does not guarantee that this function is correctly aligned to the activity of the brain in the real world.",
            "To fully close the loop and confirm that a generated explanation accurately reflects language selectivity in the brain, the second step of GEM-V is to automatically design a new neuroimaging experiment to test the generated explanation  in vivo . This was done by prompting an instruction-finetuned LLM  [ 16 ]  to generate narratives that should selectively drive cortical activation based on that explanation ( Fig.   1 c). If the explanation generated for a voxel is accurate, then text generated according to that explanation should elicit large responses in that voxel; this would demonstrate that the explanation has causal influence over that voxels activity. Given a selected set of voxels and their explanations, we constructed narrative stories by iteratively prompting an LLM  [ 16 ]  to prioritize a different voxels explanation as the main focus of each paragraph. This allowed us to test whether voxels selectively respond to paragraphs that match their explanation. The LLM ensures these stories remain coherent and engaging, helping to keep subjects attentive during the fMRI experiment  [ 18 ,  7 ] .",
            "We used GEM-V to generate and test explanations for 17 voxels with strong encoding model predictive performance in each of 3 subjects. The number of voxels was selected so that resulting narratives would be similar in length to the stimuli used to initially fit the encoding models  [ 11 ] . We then measured the average response of each target voxel during the paragraph that was designed to drive it and compared this to the average response to other paragraphs ( Fig.   1 d). Across voxels, responses to driving paragraphs were significantly greater than baseline responses for each subject ( p = 0.020 p 0.020 p=0.020 italic_p = 0.020  (S01),  p < 10  5 p superscript 10 5 p<10^{-5} italic_p < 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT  (S02),  p = 0.009 p 0.009 p=0.009 italic_p = 0.009  (S03); permutation test with Benjamini-Hochberg false discovery rate correction  [ 19 ] ). Of the 51 tested voxels across the 3 subjects, 41 show increased response, with an average increase of 0.198 standard deviations over baseline. Differences in effect size are related to head motion of subjects during test time and variation in sample size ( Table   2 ). This demonstrates that the generated explanations are mostly effective drivers of brain activity for their chosen voxels. Further, because this experiment was not based directly on the LLM encoding model  [ 9 ]  but was designed using the generated explanation, this suggests that we have successfully found explanations that are causally linked to brain activity.",
            "The results in  Fig.   1 d compare responses during the driving paragraphs generated for each voxel to average responses over all other paragraphs. But if two voxels have very similar explanations and both are correct, then we would expect each to also be driven by the others generated stimuli. To explore this question we disaggregated the results by showing the response of each voxel to every driving paragraph in one subject ( Fig.   1 e). Most voxels are driven by their own explanations, as can be seen by the positive values on the main diagonal. However, many voxels are also strongly driven by related explanations (e.g.  directions ,  measurements , and  locations ;  communication  and  emotional expression ). This demonstrates that, in most cases, semantically related explanations will drive similar sets of voxels. We find that the same setting succeeds in driving pairs of voxels rather than individual voxels ( Section   A.5 ).",
            "We next tested where GEM-V could be used to describe differences between regions that appear to have similar selectivity  [ 20 ] . Examining three ROIs known to be selective for places or locations (RSC, PPA, OPA)  [ 25 ] , we found that the driving paragraphs designed independently for each ROI also drove responses in the others ( Fig.   2 c left). To differentiate these ROIs, we used GEM-V to generate new explanations that should selectively drive each one of these regions while suppressing the other two  [ 26 ] . These selective explanations deviated somewhat from the original ROI explanations, e.g. for RSC the explanation changed from  Travel and location names  to just  Location names , suggesting that the  travel  part of the explanation is common across ROIs while  location names  are more unique to RSC. New stories were generated to include only instances of the corresponding explanation, for instance, when driving only RSC, location names were included, while directions were specifically excluded (see prompts in  Section   A.1 ). Since these two categories commonly cooccur in natural language, these synthetic stimuli enabled us to examine differences that might not be present in typical naturalistic studies. Testing these stories in another fMRI experiment showed that GEM-V was able to find explanations that can selectively drive two of the ROIs Location names  for RSC and  Unappetizing foods  for PPAwhile the explanation of  Spatial positioning & directions  for OPA still drove responses in all three ROIs ( Fig.   2 c right;  Fig.   2 d). These results demonstrate that GEM-V can be used to build more nuanced theories of cortical semantic selectivity beyond general trends that are well-understood.",
            "To further understand how driving paragraphs succeed, we measured whether the inclusion of key driving  n -grams extracted from the encoding model drove responses in a temporally specific fashion ( Fig.   1 d). During the fMRI experiment, the presentation of these  n -grams evoked a significant increase in responses, peaking 6 seconds after presentation ( Fig.   4 d;  p = 0.009 p 0.009 p=0.009 italic_p = 0.009 ; one-sided t-test). This strongly aligns with hemodynamic response curves which tend to peak about six seconds after stimulus presentation  [ 31 ] , and are further validation that the generated stimuli are the cause of the driving effects we observe.",
            "This work is related to works that have generated stimuli to drive neurons in the visual cortex  [ 38 ,  15 ,  39 ] , as well as more recent work that has used decoding to provide insight into visual cortex selectivity  [ 40 ] . In language fMRI, the closest work is  Tuckute et al.   2023 , which demonstrated the ability to drive the language network as a whole (rather than for individual voxels or ROIs) using non-generative sampling techniques as demonstrated in  13 . In comparison to these prior works, ours is the first to demonstrate driving at an explanatory level, not only identifying stimuli that can drive a particular voxel or ROI, but also hypothesizing the shared semantic features that cause the underlying activity. The methodology here runs parallel to methods for causal interventions in mechanistic interpretability  [ 41 ,  42 ,  43 ] , which have been applied to understanding artificial neural networks.",
            "Framework for generating explanations  GEM-V yields a short, natural-language explanation describing what elicits the strongest response from each of the 1,500 selected voxel encoding models. To obtain this explanation, we follow the summarize and score (SASC) framework introduced in previous work  [ 17 ]  (see  Fig.   1 b). SASC first generates candidate explanations (using GPT-4  [ 16 ] ) based on the  n -grams ( n = 1 , 2 , 3 n 1 2 3 n=1,2,3 italic_n = 1 , 2 , 3 ) that elicit the most positive response from the LLaMA encoding model. Each candidate explanation is then evaluated by generating synthetic data based on the explanation and testing the response of the encoding model to the data.",
            "Story generation (single-voxel setting)  For follow-up experiments, we prompted a large LLM (GPT-4  [ 16 ] ) to generate stories based on the explanations for our selected voxels. Stories are generated by repeatedly prompting the LLM in a chat to continue the story, one paragraph at a time. For each paragraph, the LLM is asked to focus on one explanation and to include related key  n -grams ( Fig.   4 a, see full prompts in  Section   A.1 ).",
            "For driving ROIs ( Section   4 ), many explanations are related to locations, and so we append a suffix to the prompt for reach paragraph (see  Table   1 ) to help make paragraphs more distinct.",
            "We additionally run experiments seeking to drive multiple descriptions corresponding to the same voxel. These experiments generate and test GEM-V stories in the same manner as testing individual explanations ( Fig.   1 c) but differ in the way they source voxel descriptions. In our first investigation (with subject S02), we select two explanations that both come from SASC, in the case that its summaries of the top ngrams yield two distinct explanations. In our second investigation (now with subject S03), we instead select two explanations that each come from a different encoding model. In both cases, each story tests 8 voxels with two explanations each. We average the results over two stories per subject.",
            "Fig.   12 a shows the the multi-voxel driving pipeline. Pairs of voxels with semantically independent generated explanations are selected and stimuli are generated so that the stimuli have the following pattern: (i) the first voxel is driven without respect to the second voxel by using the explanation for that voxel, (ii) then both are driven simultaneously with a generated stimulus that contains both generated explanations, and then (iii) only the second voxel is driven without respect to the first voxel.  Fig.   12 b demonstrates that we are able to effectively combine the voxel-level explanations to drive separate voxels at the same time, or independently drive one without driving the other. This shows that the generated explanations are resilient to unrelated semantic interventions, demonstrating that they do not work merely because they tend to semantically co-occur with stimuli that actually drives those voxels.",
            "To test the limits of our voxel-driving paradigm, we further examine whether they can be used directly to produce arbitrary activation patterns if explanatory considerations were dropped. The experimental pipeline for this test is shown in  Fig.   13 a. We select a location on the cortical surface to drive by searching our training dataset for areas that contain at least one response from our training dataset with a high cosine similarity to a target checkerboard pattern. We choose a checkerboard pattern for this experiment to demonstrate that driving can be achieved even for relatively complex targets, so long as the target is biologically possible.",
            "We demonstrate that we can partially manifest a checkerboard pattern in prefrontal cortex. As the checkerboard pattern is fairly unnatural, we aim to reconstruct the checkerboard from the response pattern rather than directly drive it ( Fig.   13 b). We compute the sum of the responses for the voxels in the checkerboard location across a single high-variance story, weighted by the cosine similarity of the responses to the target pattern. This results in a pattern that bears some resemblance to the desired checkerboard (pearson correlation coefficient   = 0.67  0.67 \\rho=0.67 italic_ = 0.67 )."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Head motion for different subjects during GEM-V stories. S01 shows substantially larger motion than the other two subjects.",
        "table": "A1.T2.1",
        "footnotes": [],
        "references": [
            "We used GEM-V to generate and test explanations for 17 voxels with strong encoding model predictive performance in each of 3 subjects. The number of voxels was selected so that resulting narratives would be similar in length to the stimuli used to initially fit the encoding models  [ 11 ] . We then measured the average response of each target voxel during the paragraph that was designed to drive it and compared this to the average response to other paragraphs ( Fig.   1 d). Across voxels, responses to driving paragraphs were significantly greater than baseline responses for each subject ( p = 0.020 p 0.020 p=0.020 italic_p = 0.020  (S01),  p < 10  5 p superscript 10 5 p<10^{-5} italic_p < 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT  (S02),  p = 0.009 p 0.009 p=0.009 italic_p = 0.009  (S03); permutation test with Benjamini-Hochberg false discovery rate correction  [ 19 ] ). Of the 51 tested voxels across the 3 subjects, 41 show increased response, with an average increase of 0.198 standard deviations over baseline. Differences in effect size are related to head motion of subjects during test time and variation in sample size ( Table   2 ). This demonstrates that the generated explanations are mostly effective drivers of brain activity for their chosen voxels. Further, because this experiment was not based directly on the LLM encoding model  [ 9 ]  but was designed using the generated explanation, this suggests that we have successfully found explanations that are causally linked to brain activity.",
            "Theories of language selectivity often revolve around regions of interest (ROIs) that are believed to be selective for specific semantic categories  [ 20 ] , such as locations (OPA)  [ 21 ,  22 ]  or body parts (EBA)  [ 23 ,  24 ] . We explored whether GEM-V could be used to independently uncover the semantic selectivity of regions of interest, both for generating new hypotheses as well as re-establishing known functional selectivity. We used GEM-V to find explanations and design synthetic driving stimuli for a diverse selection of individual ROIs that were localized using separate stimuli: the extrastriate body area (EBA), intraparietal sulcus (IPS), occipital face area (OFA), posterior superior temporal sulcus (pSTS), superior premotor ventral (sPMv), retrosplenial cortex (RSC), the parahippocampal place area (PPA), and the occipital place area (OPA). These ROIs were identified in each subject using separate localizer scans.  Fig.   2 a shows the generated explanations and average response above baseline for each ROI during each driving experiment. The explanations found by GEM-V and validated in the follow-up experiment are well-matched to known selectivity, e.g.  Body parts  in EBA, and  Scenes and settings  in PPA. Driving succeeded for every ROI (all  p < 0.05 p 0.05 p<0.05 italic_p < 0.05 ; permutation test with FDR correction), although for some ROIs, e.g. RSC, the effect is stronger.  Fig.   2 b visualizes driving responses for each ROI on a composite flatmap, where each voxel is colored on the basis of how much it was driven by its ROIs explanation. This map suggests that the variability in driving between ROIs reflects functional heterogeneity within each region.",
            "We next tested where GEM-V could be used to describe differences between regions that appear to have similar selectivity  [ 20 ] . Examining three ROIs known to be selective for places or locations (RSC, PPA, OPA)  [ 25 ] , we found that the driving paragraphs designed independently for each ROI also drove responses in the others ( Fig.   2 c left). To differentiate these ROIs, we used GEM-V to generate new explanations that should selectively drive each one of these regions while suppressing the other two  [ 26 ] . These selective explanations deviated somewhat from the original ROI explanations, e.g. for RSC the explanation changed from  Travel and location names  to just  Location names , suggesting that the  travel  part of the explanation is common across ROIs while  location names  are more unique to RSC. New stories were generated to include only instances of the corresponding explanation, for instance, when driving only RSC, location names were included, while directions were specifically excluded (see prompts in  Section   A.1 ). Since these two categories commonly cooccur in natural language, these synthetic stimuli enabled us to examine differences that might not be present in typical naturalistic studies. Testing these stories in another fMRI experiment showed that GEM-V was able to find explanations that can selectively drive two of the ROIs Location names  for RSC and  Unappetizing foods  for PPAwhile the explanation of  Spatial positioning & directions  for OPA still drove responses in all three ROIs ( Fig.   2 c right;  Fig.   2 d). These results demonstrate that GEM-V can be used to build more nuanced theories of cortical semantic selectivity beyond general trends that are well-understood.",
            "While effective, GEM-V has several limitations. Most notably, it focuses on a single explanation, missing voxel activity that is polysemantic, i.e. that is not driven well by any single explanation. See  Section   A.2  for a more detailed discussion of this limitation and how future work might overcome it. Second, the effectiveness of GEM-V depends on staying within the manifold of stimuli trained on during encoding model training. This limitation is explored further in  A.6  where we show that poorly generated, off-manifold stimuli are more difficult to predict and yield inferior driving outcomes. GEM-V is also sensitive to the properties of the underlying encoding modelsusing different encoding models could yield different explanations which might be equally valid. Thus we must be cautious not to interpret the explanations given by GEM-V as uniquely capturing the functional properties of a voxel or ROI. However, this issue will become less severe over time as better and more diverse encoding models become available.",
            "Fig.   12 a shows the the multi-voxel driving pipeline. Pairs of voxels with semantically independent generated explanations are selected and stimuli are generated so that the stimuli have the following pattern: (i) the first voxel is driven without respect to the second voxel by using the explanation for that voxel, (ii) then both are driven simultaneously with a generated stimulus that contains both generated explanations, and then (iii) only the second voxel is driven without respect to the first voxel.  Fig.   12 b demonstrates that we are able to effectively combine the voxel-level explanations to drive separate voxels at the same time, or independently drive one without driving the other. This shows that the generated explanations are resilient to unrelated semantic interventions, demonstrating that they do not work merely because they tend to semantically co-occur with stimuli that actually drives those voxels."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "A1.F9.2",
        "footnotes": [],
        "references": [
            "Although the stories generated by GEM-V are designed to activate single voxels or ROIs, we can measure how much each explanation activates every part of cortex. These experiments can thus be seen as a hybrid between classical block-designs that are easy to analyze and natural stimulus experiments that elicit stronger and more widespread activation in cortex  [ 18 ,  7 ] . To demonstrate this, we computed the average activation map for each explanation by averaging responses over timepoints within each paragraph. The results for six selected explanations in one subject are shown in  Fig.   3 .",
            "Some explanations were similar to previously well-established contrasts ( Fig.   3 a) and gave the expected results. For example, the explanation  Locations  gave strong responses in the place areas RSC, OPA, and PPA  [ 25 ] . Others confirmed newer hypotheses. For example, the  Food Preparation  explanation ( Fig.   3 b  Top ) activates a region in ventral occipital cortex near the fusiform face area (FFA). Selectivity for this area to food was found recently in responses to images  [ 27 ,  28 ] , but these results demonstrate for the first time that this selectivity also extends to linguistic descriptions of food. Finally, some explanations did not clearly map to any known hypotheses ( Fig.   3 c), but still demonstrate suggestive patterns of activation that could be explored in future experiments. Maps for other explanations are provided in Appendix  A.7 .",
            "This work is related to works that have generated stimuli to drive neurons in the visual cortex  [ 38 ,  15 ,  39 ] , as well as more recent work that has used decoding to provide insight into visual cortex selectivity  [ 40 ] . In language fMRI, the closest work is  Tuckute et al.   2023 , which demonstrated the ability to drive the language network as a whole (rather than for individual voxels or ROIs) using non-generative sampling techniques as demonstrated in  13 . In comparison to these prior works, ours is the first to demonstrate driving at an explanatory level, not only identifying stimuli that can drive a particular voxel or ROI, but also hypothesizing the shared semantic features that cause the underlying activity. The methodology here runs parallel to methods for causal interventions in mechanistic interpretability  [ 41 ,  42 ,  43 ] , which have been applied to understanding artificial neural networks.",
            "To test the limits of our voxel-driving paradigm, we further examine whether they can be used directly to produce arbitrary activation patterns if explanatory considerations were dropped. The experimental pipeline for this test is shown in  Fig.   13 a. We select a location on the cortical surface to drive by searching our training dataset for areas that contain at least one response from our training dataset with a high cosine similarity to a target checkerboard pattern. We choose a checkerboard pattern for this experiment to demonstrate that driving can be achieved even for relatively complex targets, so long as the target is biologically possible.",
            "We demonstrate that we can partially manifest a checkerboard pattern in prefrontal cortex. As the checkerboard pattern is fairly unnatural, we aim to reconstruct the checkerboard from the response pattern rather than directly drive it ( Fig.   13 b). We compute the sum of the responses for the voxels in the checkerboard location across a single high-variance story, weighted by the cosine similarity of the responses to the target pattern. This results in a pattern that bears some resemblance to the desired checkerboard (pearson correlation coefficient   = 0.67  0.67 \\rho=0.67 italic_ = 0.67 )."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "A1.F10.2",
        "footnotes": [],
        "references": [
            "First, we considered whether driving failures may arise from a semantic mismatch between the proposed explanation and the generated story stimulus. We measured the match between each driving paragraph and its target explanation for subject S02 by using an LLM to evaluate similarity between the explanation and trigrams in the driving paragraph (see  Methods ). There is a strong correspondence between each voxel explanation and its driving paragraph ( Fig.   4 a, orange diagonal), suggesting that LLM stimulus generation does not account for the failures. Moreover, there is no clear correlation between these scores and driving success (average correlation of -0.05; see  Fig.   9 a). Second, we tested whether the failed cases result from a misalignment between the original encoding model for each voxel and the generated story stimulus. Again, each voxels encoding model showed an increased response for its driving paragraph ( Fig.   4 b, orange diagonal), suggesting the driving paragraphs are successfully aligned with the encoding model. In this case, higher driving scores for the encoding model did yield higher driving scores in the follow-up experiment for 2 of the 3 subjects (average correlation of 0.19; see  Fig.   9 b).",
            "To identify which explanations are stable, we defined a stability score that measures agreement between the predictions of two encoding models built from different LLMs (see  Methods ) for the same voxel using the same fMRI data. We found a strong positive correlation between the stability score of a voxel and its mean driving response across all three subjects ( Fig.   4 c), suggesting that the stability score reliably informs the causal reliability of an explanation. This result underscores the idea that models must be predictive and stable in order to effectively generate hypotheses for follow-up experiments  [ 15 ,  30 ] .",
            "To further understand how driving paragraphs succeed, we measured whether the inclusion of key driving  n -grams extracted from the encoding model drove responses in a temporally specific fashion ( Fig.   1 d). During the fMRI experiment, the presentation of these  n -grams evoked a significant increase in responses, peaking 6 seconds after presentation ( Fig.   4 d;  p = 0.009 p 0.009 p=0.009 italic_p = 0.009 ; one-sided t-test). This strongly aligns with hemodynamic response curves which tend to peak about six seconds after stimulus presentation  [ 31 ] , and are further validation that the generated stimuli are the cause of the driving effects we observe.",
            "Finally, we tested whether the GEM-V framework is sensitive to the particular voxels we selected for followup experiments. To evaluate this, we evaluated whether the driving paragraphs we generated also drove alternative voxels that were assigned the same explanation by GEM-V.  Fig.   4 e shows the mean driving voxel responses for targeted voxels versus alternative voxels and finds that both are driven significantly ( p < 0.05 p 0.05 p<0.05 italic_p < 0.05 ; permutation test with FDR correction). The alternative voxels are driven slightly less reliably (mean 0.14   \\sigma italic_  vs 0.19   \\sigma italic_ ), likely because the prompts for generating the stories contain  n -grams that specify the explanation in slightly more detail than the explanation alone.",
            "Story generation (single-voxel setting)  For follow-up experiments, we prompted a large LLM (GPT-4  [ 16 ] ) to generate stories based on the explanations for our selected voxels. Stories are generated by repeatedly prompting the LLM in a chat to continue the story, one paragraph at a time. For each paragraph, the LLM is asked to focus on one explanation and to include related key  n -grams ( Fig.   4 a, see full prompts in  Section   A.1 ).",
            "Before running follow-up experiments, we checked that each paragraphs text matched its generated explanation ( Fig.   4 a). This match was measured by prompting an LLM  [ 47 ]  to evaluate the fraction of trigrams in the paragraph that are relevant to the paragraphs generating explanation. We also validated that each story paragraph drives the encoding model for its corresponding voxel ( Fig.   4 b). We generated 8 different stories by changing the random seed for each subject and kept the best 2 for S01, the best 6 for S02 (the pilot subject), and the best 2 for S03.",
            "For driving ROIs ( Section   4 ), many explanations are related to locations, and so we append a suffix to the prompt for reach paragraph (see  Table   1 ) to help make paragraphs more distinct.",
            "In this section we provide additional analyses of what voxel-level factors inform whether driving performance will succeed. The strongest correlate is the stability score, given in the main text ( Fig.   4 c)."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A1.F11.2",
        "footnotes": [],
        "references": [
            "The results in  Fig.   1 d compare responses during the driving paragraphs generated for each voxel to average responses over all other paragraphs. But if two voxels have very similar explanations and both are correct, then we would expect each to also be driven by the others generated stimuli. To explore this question we disaggregated the results by showing the response of each voxel to every driving paragraph in one subject ( Fig.   1 e). Most voxels are driven by their own explanations, as can be seen by the positive values on the main diagonal. However, many voxels are also strongly driven by related explanations (e.g.  directions ,  measurements , and  locations ;  communication  and  emotional expression ). This demonstrates that, in most cases, semantically related explanations will drive similar sets of voxels. We find that the same setting succeeds in driving pairs of voxels rather than individual voxels ( Section   A.5 ).",
            "We generally find that we are able to only successfully drive one of the two explanations we seek to drive for an individual voxel ( Fig.   5 ). We believe this is due to SASCs overreliance on summarizing top-driving ngrams, and could potentially be mitigated by summarizing a broader range of encoding model inputs."
        ]
    }
}