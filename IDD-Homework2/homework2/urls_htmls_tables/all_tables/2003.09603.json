{
    "PAPER'S NUMBER OF TABLES": 1,
    "Sx4.T1": {
        "caption": "Table 1: A summary of datasets",
        "table": "<table id=\"Sx4.T1.6\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"Sx4.T1.6.1.1\" class=\"ltx_tr\">\n<th id=\"Sx4.T1.6.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt\"><span id=\"Sx4.T1.6.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Dataset</span></th>\n<th id=\"Sx4.T1.6.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"Sx4.T1.6.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Type</span></th>\n<th id=\"Sx4.T1.6.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"Sx4.T1.6.1.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\"># train</span></th>\n<th id=\"Sx4.T1.6.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"Sx4.T1.6.1.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\"># test</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"Sx4.T1.6.2.1\" class=\"ltx_tr\">\n<td id=\"Sx4.T1.6.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Sx4.T1.6.2.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">MNIST</span></td>\n<td id=\"Sx4.T1.6.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"Sx4.T1.6.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">image</span></td>\n<td id=\"Sx4.T1.6.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"Sx4.T1.6.2.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">60,000</span></td>\n<td id=\"Sx4.T1.6.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"Sx4.T1.6.2.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">10,000</span></td>\n</tr>\n<tr id=\"Sx4.T1.6.3.2\" class=\"ltx_tr\">\n<td id=\"Sx4.T1.6.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span id=\"Sx4.T1.6.3.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">CIFAR-10</span></td>\n<td id=\"Sx4.T1.6.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"Sx4.T1.6.3.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">image</span></td>\n<td id=\"Sx4.T1.6.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"Sx4.T1.6.3.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">50,000</span></td>\n<td id=\"Sx4.T1.6.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"Sx4.T1.6.3.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">10,000</span></td>\n</tr>\n<tr id=\"Sx4.T1.6.4.3\" class=\"ltx_tr\">\n<td id=\"Sx4.T1.6.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_l ltx_border_r\"><span id=\"Sx4.T1.6.4.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Wikitext-2</span></td>\n<td id=\"Sx4.T1.6.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"Sx4.T1.6.4.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">token</span></td>\n<td id=\"Sx4.T1.6.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"Sx4.T1.6.4.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">2,088,628</span></td>\n<td id=\"Sx4.T1.6.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"Sx4.T1.6.4.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">245,569</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We perform several neural architectures on two typical tasks - image classification and language modeling - on two image datasets of MNIST and CIFAR-10 and a natural language dataset of WikiText-2. They are typical public datasets for training machine learning algorithms.\nMNIST111Available at http://yann.lecun.com/exdb/mnist/ and CIFAR-10222https://www.cs.toronto.edu/~kriz/cifar.html are two widely used datasets with hand-written digits and objects respectively.\nWikiText-2333https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip is a dataset for word-level language modeling, which contains more than 2 million tokens extracted from Wikipedia. Our experiments are performed on these three widely-used datasets for validation. Statistics of these three datasets are summarized as in Table 1."
        ]
    }
}