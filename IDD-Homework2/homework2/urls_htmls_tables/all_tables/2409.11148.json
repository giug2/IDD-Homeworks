{
    "id_table_1": {
        "caption": "Table 1:  Blind-VaLM  matches  VaLM  on VLU, NLU and LM tasks when trained on the same setup, while being significantly more efficient to train.",
        "table": "S3.T1.3",
        "footnotes": [],
        "references": [
            "The  VaLM  architecture is composed of three main modules (Figure  1  left): 1) a backbone autoregressive LM (GPT2  (Radford et al.,  2019 ) ), 2) a text-to-image retrieval module based on CLIP  (Radford et al.,  2021 ) , and 3) the Visual Knowledge Fusion Layer (Fusion Layer for short), to fuse the contextual text representations of the LM with the image representations retrieved for the input text. The intuition is that the retrieved visual representations should help to better predict the next token. For further details on the  VaLM  architecture and Fusion Layer, see  Wang et al. ( 2022 ) .",
            "To show that image retrieval and representation are not necessary to augment the backbone LM with visual knowledge, we make one modification to the  VaLM  architecture: Instead of using the CLIP image encoder representations of the retrieved images,  Blind-VaLM  directly uses CLIP text encoder representations of the text itself (see Figure  1  right).",
            "Blind-VaLM  matches  VaLM  on VLU, NLU and LM tasks . Table  1  shows results for  Blind-VaLM  and  VaLM  trained on the same setup, as described in  4 . We observe that our approach matches the original  VaLM  on VLU tasks, as well as NLU and LM tasks: it achieves an average score  1.18 1.18 1.18 1.18  points higher in VLU tasks, and outperforms  VaLM  on 6/7 NLU & LM tasks. This supports our hypothesis that  actually retrieving and encoding images is not required for visual augmentation , since simply utilizing textual representations from an already visually grounded text encoder works equally well. Additionally, as described in section  4   Blind-VaLM  is 2.2x faster  to train, since it skips the time consuming vector retrieval step. Speedups are even more significant at inference time, since generation is not as compute-bound and retrieval latency plays a bigger role."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  When trained within the same compute budget by increasing either model size ( Blind-VaLM-Medium , referred to as  Blind-VaLM-M  in the table) or pre-training compute ( Blind-VaLM+ ), our approach outperforms  VaLM  on both VLU and NLU tasks.",
        "table": "S3.T1.10",
        "footnotes": [],
        "references": [
            "Blind-VaLM  outperforms  VaLM , when trained within the same compute budget . Table  2  shows results for two scaled-up  Blind-VaLM  variants, obtained through scaling either pre-training compute or model size, as described in  4 . We observe that both variants outperform  VaLM , while being trained within the same compute budget as it. For example, in the case of  Blind-VaLM-Medium , we outperform  VaLM  by  2.2 2.2 2.2 2.2  points on average for VLU tasks, and outperform  VaLM  on 6/7 NLU & LM tasks."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:   The prompts and prediction labels used in object color reasoning.",
        "table": "S3.T2.3",
        "footnotes": [],
        "references": [
            "To evaluate VLU and NLU capabilities we follow the prompting approach designed by  VaLM   (Wang et al.,  2022 ) , with some slight modifications. Tables  3 ,  4  and  5  show the prompts we use for color, shape and size evaluation. Similarly, Table  6  defines the prompts for all the NLU tasks we consider. The reported results in the paper are always the average of the results obtained for all the prompts for a given task, with the objective of avoiding any bias towards different prompting choices."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:   The prompts and prediction labels used in object shape reasoning.",
        "table": "S3.T2.10",
        "footnotes": [],
        "references": [
            "Blind-VaLM  matches  VaLM  on VLU, NLU and LM tasks . Table  1  shows results for  Blind-VaLM  and  VaLM  trained on the same setup, as described in  4 . We observe that our approach matches the original  VaLM  on VLU tasks, as well as NLU and LM tasks: it achieves an average score  1.18 1.18 1.18 1.18  points higher in VLU tasks, and outperforms  VaLM  on 6/7 NLU & LM tasks. This supports our hypothesis that  actually retrieving and encoding images is not required for visual augmentation , since simply utilizing textual representations from an already visually grounded text encoder works equally well. Additionally, as described in section  4   Blind-VaLM  is 2.2x faster  to train, since it skips the time consuming vector retrieval step. Speedups are even more significant at inference time, since generation is not as compute-bound and retrieval latency plays a bigger role.",
            "Blind-VaLM  outperforms  VaLM , when trained within the same compute budget . Table  2  shows results for two scaled-up  Blind-VaLM  variants, obtained through scaling either pre-training compute or model size, as described in  4 . We observe that both variants outperform  VaLM , while being trained within the same compute budget as it. For example, in the case of  Blind-VaLM-Medium , we outperform  VaLM  by  2.2 2.2 2.2 2.2  points on average for VLU tasks, and outperform  VaLM  on 6/7 NLU & LM tasks.",
            "To evaluate VLU and NLU capabilities we follow the prompting approach designed by  VaLM   (Wang et al.,  2022 ) , with some slight modifications. Tables  3 ,  4  and  5  show the prompts we use for color, shape and size evaluation. Similarly, Table  6  defines the prompts for all the NLU tasks we consider. The reported results in the paper are always the average of the results obtained for all the prompts for a given task, with the objective of avoiding any bias towards different prompting choices."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:   The prompts and prediction labels used in object size reasoning.",
        "table": "A2.T3.1.1",
        "footnotes": [],
        "references": [
            "To evaluate VLU and NLU capabilities we follow the prompting approach designed by  VaLM   (Wang et al.,  2022 ) , with some slight modifications. Tables  3 ,  4  and  5  show the prompts we use for color, shape and size evaluation. Similarly, Table  6  defines the prompts for all the NLU tasks we consider. The reported results in the paper are always the average of the results obtained for all the prompts for a given task, with the objective of avoiding any bias towards different prompting choices."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:   The prompts and prediction labels used in 4 natural language understanding datasets.  The labels for AGNews are {world, sports, business, technology} and the labels for DBPedia are {company, school, artist, athlete, politics, transportation, building, nature, village, animal, plant, album, film, book}.",
        "table": "A2.T4.1.1",
        "footnotes": [],
        "references": [
            "To evaluate VLU and NLU capabilities we follow the prompting approach designed by  VaLM   (Wang et al.,  2022 ) , with some slight modifications. Tables  3 ,  4  and  5  show the prompts we use for color, shape and size evaluation. Similarly, Table  6  defines the prompts for all the NLU tasks we consider. The reported results in the paper are always the average of the results obtained for all the prompts for a given task, with the objective of avoiding any bias towards different prompting choices."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A2.T5.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_8": {
        "caption": "",
        "table": "A2.T6.1.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "The efficiency gains are two-fold: 1) since we only use a single CLIP text representation in the Fusion Layer, this layer requires less floating-point operations (FLOPs), and 2) since we do away with the dense vector database retrieval, training and inference latency is significantly reduced, leading to improved wall-clock efficiency.",
        "Note that we use",
        "in the wall-clock time sense here, not the FLOPs sense, since the time consuming dense vector retrieval step we remove is not reflected by FLOPs."
    ]
}