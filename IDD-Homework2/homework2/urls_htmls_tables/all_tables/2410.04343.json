{
    "id_table_1": {
        "caption": "Table 1 :  Optimal performance of different methods with varying maximum effective context lengths  L max subscript L max L_{\\text{max}} italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT  (i.e., the total number of input tokens  across all iterations ). ZS QA and MS QA refers to zero-shot QA and many-shot QA respectively. Partial results are omitted for methods that do not further scale with increasing  L max subscript L max L_{\\text{max}} italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT . For clarity, we mark the best results for each  L max subscript L max L_{\\text{max}} italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT  in bold.",
        "table": "S4.T1.7.1",
        "footnotes": [],
        "references": [
            "Building on these strategies, we investigate multiple ways to scale up inference computation. Here, we measure computation by considering the total number of input tokens across all iterations, referred to as the  effective context length . In DRAG, scaling the effective context length can be done by increasing two inference parameters: the number of retrieved documents and in-context examples. In IterDRAG, test-time compute can be further extended by introducing additional generation steps. Since different combinations of inference parameters result in varied allocations of computational resources, our goal is to establish the relationship between RAG  performance , different  scales  and  allocations  of inference computation. Through extensive experiments on benchmark QA datasets, we demonstrate an almost linear relationship between RAG performance and the scale of effective context length by combining both RAG strategies, as shown in  Figure   1  (right). Moreover, our RAG strategies exhibit improved performance than merely scaling the number of documents, achieving state-of-the-art performance with the compact Gemini 1.5 Flash (See evaluation in  Figure   2 ).",
            "We report the optimal performance  P   ( L max ) superscript P subscript L max P^{*}(L_{\\text{max}}) italic_P start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT )  for different inference strategies in  Table   1 , where we identify the optimal inference parameters for each computation budget  L max subscript L max L_{\\text{max}} italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT . Some variants are omitted for certain  L max subscript L max L_{\\text{max}} italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT  because they do not scale to the corresponding context length. For example, the prompt for zero-shot QA cannot be increased, while the number of in-context examples for many-shot QA is capped at  2 8 superscript 2 8 2^{8} 2 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT , so neither scales to  L max = subscript L max absent L_{\\text{max}}= italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT =  32k. Similarly, RAG does not scale to  L max subscript L max L_{\\text{max}} italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT  larger than 128k, and DRAG is limited by the LLMs context window limit of 1M.",
            "To analyze the performance changes with different effective context lengths, we plot the performance of all configurations across datasets in  Figure   4 . Similar to  Figure   1 , we visualize DRAG and IterDRAG and highlight the optimal performance  P   ( L max ) superscript P subscript L max P^{*}(L_{\\text{max}}) italic_P start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT )  for different selections of  L max subscript L max L_{\\text{max}} italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT . The fitting results are shown as grey dashed lines. We provide additional dataset-specific results in  Appendix   D .",
            "The optimal performance exhibits consistent gains as the effective context length expands, demonstrating a strong linear correlation, which we term the  inference scaling laws for RAG . Combined with dataset-specific results, our key observations are: (1)  The optimal performance scales nearly linearly with the order of magnitude of the inference compute.  Such linear relationship suggests that RAG performance can be improved by increasing computation, allowing for more accurate predictions of performance given available compute resources. (2)  For  L max subscript L max L_{\\text{max}} italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT  above  10 5 superscript 10 5 10^{5} 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT , IterDRAG continues to scale effectively with interleaving retrieval and iterative generation.  This aligns with our results in  Table   1 , where IterDRAG better utilizes computation budgets for effective context lengths exceeding 128k. (3)  Gains on optimal performance gradually diminish beyond an effective context length of 1M.  Despite dataset variations, the performance follows similar trends up to 1M tokens. Beyond that, improvements from 1M to 5M are less substantial or plateau, potentially due to limitations in long-context modeling. In summary, while gains are smaller beyond 1M tokens, optimal RAG performance scales almost linearly with increasing inference compute through DRAG and IterDRAG.",
            "We also discuss the impact of long-context modeling w.r.t. RAG performance. In summary, we find that retrieving more documents is generally beneficial for RAG performance, as demonstrated in  Section   4 . Nevertheless, naively extending the context length in each generation step does not always lead to better results. Specifically, DRAG performance peaks at around  10 5 superscript 10 5 10^{5} 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT  tokens, while IterDRAG achieves optimal performance at around  10 6 superscript 10 6 10^{6} 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT  tokens by leveraging multiple rounds of generation. For instance, as seen in the performance plateau in  Figure   1  and  Figure   10 , LLMs struggle to effectively utilize very long contexts (  10 5 absent superscript 10 5 \\geq 10^{5}  10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT  tokens) in each iteration, potentially due to inherent limitations of long-context modeling. Our observations suggest that: (1) the models ability to identify relevant information from extensive context remains to be improved, especially when presented with large quantity of similar documents; (2) the long-context modeling should be further refined to enhance in-context learning capabilities, where multiple lengthy demonstrations are provided.",
            "We present data-specific results on the relationship between the performance and the effective context length.  Figure   10  presents the results on the other three datasets other than MuSiQue (See  Figure   1  for visualized results on MuSiQue). We observe different behavior depending on the datasets. For instance, the gains are more linear and consistent on Bamboogle and MuSiQue, and almost linear on 2WikiMultiHopQA until 1M tokens. However, HotpotQA and 2WikiMultiHopQA with effective context length longer than 100k tokens exhibit more sigmoidal patterns, likely due to the difficulty of the datasets and the quality of the retrieved documents.",
            "We also visualize the predictions on for IterDRAG across different datasets in  Figure   11 , where each subplot represents a dataset and each line corresponds to a document setting ( k k k italic_k ). The inference compute is scaled by increasing the number of in-context examples ( m m m italic_m ) and generation iterations ( n n n italic_n ). Here, we find similar trends to those in  Figure   6 , although IterDRAG shows larger variations compared to DRAG. HotpotQA and 2WikiMultiHopQA show more consistent trends with the predictions, likely due to the predominance of multi-hop queries. In summary, our findings are consistent for both DRAG and IterDRAG, demonstrating that RAG performance can be accurately modeled by our computation allocation model for RAG. For Bamboogle, HotpotQA and 2WikiMultiHopQA, we provide the normalized performance with increasing effective context lengths in  Figure   10 , in which we observe similar trends to the results on MuSiQue (See  Figure   1 ). We also illustrate the prediction surface for both DRAG and IterDRAG in  Figure   12 .",
            "Despite the performance gains from scaling effective context length, RAG performance on challenging datasets like MuSiQue remain moderate, even for IterDRAG. To address this, we analyze the mistakes in both DRAG and IterDRAG to examine the limitations and errors inherent in these approaches. In the following, we explore common failure cases (See  Figure   13 ) to understand where each method falls short and how they could be further improved.",
            "We provide selected example mistakes from  Figure   13(a)  to  Figure   13(d) , with retrieved documents omitted for brevity. The reasons for common errors can be grouped into four categories: (1) inaccurate or outdated retrieval; (2) incorrect or lack of reasoning; (3) hallucination or unfaithful reasoning; and (4) evaluation issues or refusal to answer. We elaborate on these categories below:",
            "Inaccurate or outdated retrieval : A major source of RAG errors stems from the retrieval process, where relevant knowledge is not correctly retrieved. For example, in the first question of  Figure   13(c) , the top-50 retrieved documents do not contain the correct answer. A similar issue occurs in the second QA pair, where outdated retrieval results fail to provide useful information. In the third case, although both battles are retrieved, the initial documents overly focus on the Battle of Manila, leading to an incorrect response.",
            "Incorrect or lack of reasoning : Beyond retrieval issues, incorrect reasoning chains are another common source of errors. For example, in the first case in  Figure   13(b) , although the correct documents are retrieved, the reasoning process is incomplete (i.e., no explicit comparison of the mountain heights), leading to an incorrect answer in DRAG. Similarly, in the second and third cases, the reasoning is either absent (as in DRAG) or flawed. As a result, reasoning-related errors tend to occur more frequently in difficult questions and in the one-step DRAG approach.",
            "In our experiments, we utilize the Gecko-1B (en) embedding model to index both the documents and input queries  (Lee et al.,  2024b ) , using Wikipedia passages from the KILT benchmark as the document source  (Petroni et al.,  2020 ) . In test-time, the input query is compared against all embeddings in the corpus, and the top- k k k italic_k  neighbors are selected for inference. Each document is then truncated on the right side to a maximum of 1024 tokens using whitespace tokenization. For each example, we arrange the elements in the following order: documents, query, and label, with the retrieved documents listed in reverse order, placing the higher-ranked documents closer to the query  (Liu et al.,  2024b ) . Consequently, the prompt comprises of multiple in-context examples, followed by the test documents and test query, as illustrated in  Figure   14 .",
            "For generation, we utilize Gemini 1.5 Flash for more efficient experiments.  In DRAG, inference scaling is achieved by increasing the context length through the combination of documents ( k k k italic_k ) and in-context examples ( m m m italic_m ). Then, the prompt (See  Figure   15 ) is provided to the model for a one-time generation using the default generation parameters. For IterDRAG, the input prompt is constructed in a similar fashion, with the example answers consisting of assembled sub-queries, intermediate answers, and the final answer (See  Figure   16 ). Here, we scale test-time compute by incorporating iterative retrieval and generation, along with the increase of documents and demonstrations. In each iteration, we restrict the generation to adhere to the Self-Ask format, in which the response should start with Follow up: , Intermediate answer:  or So the final answer is:   (Koo et al.,  2024 ) . Each iteration begins with the generation of a sub-query and concludes with the production of an intermediate answer. If a sub-query is generated, additional documents are retrieved and appended to the initial set (i.e., Test Documents in  Figure   14 ), after which the model generates an intermediate answer. We allow up to five iterations, after which the model is forced to produce the final answer."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Ablation study results of the computation allocation model for RAG.",
        "table": "S5.T2.8",
        "footnotes": [],
        "references": [
            "Building on these strategies, we investigate multiple ways to scale up inference computation. Here, we measure computation by considering the total number of input tokens across all iterations, referred to as the  effective context length . In DRAG, scaling the effective context length can be done by increasing two inference parameters: the number of retrieved documents and in-context examples. In IterDRAG, test-time compute can be further extended by introducing additional generation steps. Since different combinations of inference parameters result in varied allocations of computational resources, our goal is to establish the relationship between RAG  performance , different  scales  and  allocations  of inference computation. Through extensive experiments on benchmark QA datasets, we demonstrate an almost linear relationship between RAG performance and the scale of effective context length by combining both RAG strategies, as shown in  Figure   1  (right). Moreover, our RAG strategies exhibit improved performance than merely scaling the number of documents, achieving state-of-the-art performance with the compact Gemini 1.5 Flash (See evaluation in  Figure   2 ).",
            "In  Equation   2 , estimations on  a a a italic_a ,  b b b italic_b  and  c c c italic_c  are specific to a certain model, reflecting how LLMs improve with varying number of documents and shots (i.e., in-context learning / zero-shot capabilities). In contrast,  i i i italic_i  models the performance variations within the selected task (i.e., how external knowledge / demonstrations help responding to the query). Therefore, the computation allocation model can be estimated once and applied to various downstream tasks without requiring additional calibration. To estimate the parameters, varying combinations of    \\theta italic_  are evaluated to perform ordinary least squares on  a a a italic_a ,  b b b italic_b  and  c c c italic_c . We report the parameters for Gemini 1.5 Flash in  Appendix   E .",
            "To verify the effectiveness of the computation allocation model, we perform ablation studies and evaluate the fitting performance of different variants. In particular, we assess: (1) estimation without  b b b italic_b  and  i i i italic_i  (i.e., Exclude  b b b italic_b ); (2) a quadratic form of input  log  (  )  \\log(\\theta) roman_log ( italic_ )  (Quadratic    \\theta italic_ ); (3) linear scaling of  P P P italic_P  (Linear    \\sigma italic_ ); and (4) sigmoid scaling of  P P P italic_P  (Sigmoidal    \\sigma italic_ ). The  R 2 superscript R 2 R^{2} italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  and MSE values for these variants are reported in  Table   2 , in which (4) represents the complete design of our computation allocation model. The results indicate that incorporating the additional  b b b italic_b  with  i i i italic_i  enhances the relevance and reduces error across all tasks. Moreover, applying inverse sigmoid to  P P P italic_P  significantly improves the estimation in comparison to quadratic    \\theta italic_  or linear scaling.",
            "We also examine the generalization of the computation allocation model for RAG for unseen domains. In other words, the parameters of  Equation   2  are tested on the target domain but learnt from the remaining domains. For inference, only  i i i italic_i  is derived from the target domain. We report the results for 1M effective context length in  Table   3 , where we compare to a 8-shot baseline configuration (scaled by increasing retrieved documents) and the optimum results (Oracle). In summary, the results show that computation allocation model significantly outperforms baseline and closely aligns with the oracle results (96.6% of the optimal performance). Notably, Bamboogle and HotpotQA exhibit highly similar target results, with the performance metrics varying by less than 2.5% from the oracle. These results suggest the potential of applying the computation allocation model for RAG to a wider range of knowledge-intensive tasks.",
            "In addition to predictability on unseen domains, we explore the extrapolation of context length based on the computation allocation model. Here, we estimate the parameters of  Equation   2  using experiments with shorter context lengths and assess their predictive accuracy on longer ones. We assess different extrapolation settings and present the predicted metric values in  Table   4 . Our observations are: (1) The predictions are accurate and consistently outperform the 8-shot baseline. For instance, the average difference between the predicted and oracle results from 128k to 1M tokens is just 2.8%. (2) Extrapolating from 32k to 128k is challenging. This is because DRAG performs best around 32k, while IterDRAG typically excels at a long context of 128k, as evidenced in  Figure   4 . Consequently, it creates a discrepancy between training and predicting performance distribution. (3) 5M context length is less predictable, with the average performance difference between predicted and oracle metrics observed at a substantial 5.6%. Overall, length extrapolation with computation allocation model is accurate and more effective for target lengths below 1M.",
            "We also visualize the predictions on for IterDRAG across different datasets in  Figure   11 , where each subplot represents a dataset and each line corresponds to a document setting ( k k k italic_k ). The inference compute is scaled by increasing the number of in-context examples ( m m m italic_m ) and generation iterations ( n n n italic_n ). Here, we find similar trends to those in  Figure   6 , although IterDRAG shows larger variations compared to DRAG. HotpotQA and 2WikiMultiHopQA show more consistent trends with the predictions, likely due to the predominance of multi-hop queries. In summary, our findings are consistent for both DRAG and IterDRAG, demonstrating that RAG performance can be accurately modeled by our computation allocation model for RAG. For Bamboogle, HotpotQA and 2WikiMultiHopQA, we provide the normalized performance with increasing effective context lengths in  Figure   10 , in which we observe similar trends to the results on MuSiQue (See  Figure   1 ). We also illustrate the prediction surface for both DRAG and IterDRAG in  Figure   12 .",
            "To evaluate the estimated parameters within computation allocation model for RAG, we normalized the performance metrics by subtracting the mean and dividing by the standard deviation for each dataset and metric. For DRAG, the effective context length is calculated by counting the tokens in the prompt, while for IterDRAG, it is determined by summing the context tokens across all inference requests. We constrain the last parameter in  b b b italic_b  and perform ordinary least squares to estimate rest six parameters in  Equation   2 . To prevent numerical instability, we shift the values in    \\theta italic_  by a small constant   italic- \\epsilon italic_  of 0.01. When computing  R 2 superscript R 2 R^{2} italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  and MSE, we manage noisy data by excluding peak and valley outliers in our experiments. However, for domain generalization and length extrapolation, all data points are included in the evaluation. To predict downstream task performance,  i i i italic_i  should be computed for each task. Specifically, in each strategy and task:  i doc = P  ( k = 1 , m = 0 , n = 1 )  P  ( k = 0 , m = 0 , n = 1 ) subscript i doc P formulae-sequence k 1 formulae-sequence m 0 n 1 P formulae-sequence k 0 formulae-sequence m 0 n 1 i_{\\text{doc}}=P(k=1,m=0,n=1)-P(k=0,m=0,n=1) italic_i start_POSTSUBSCRIPT doc end_POSTSUBSCRIPT = italic_P ( italic_k = 1 , italic_m = 0 , italic_n = 1 ) - italic_P ( italic_k = 0 , italic_m = 0 , italic_n = 1 ) ,  i shot = P  ( k = 0 , m = 1 , n = 1 )  P  ( k = 0 , m = 0 , n = 1 ) subscript i shot P formulae-sequence k 0 formulae-sequence m 1 n 1 P formulae-sequence k 0 formulae-sequence m 0 n 1 i_{\\text{shot}}=P(k=0,m=1,n=1)-P(k=0,m=0,n=1) italic_i start_POSTSUBSCRIPT shot end_POSTSUBSCRIPT = italic_P ( italic_k = 0 , italic_m = 1 , italic_n = 1 ) - italic_P ( italic_k = 0 , italic_m = 0 , italic_n = 1 ) . For the predicted optimal hyperparameters, we present the actual metric values to validate the efficacy of computation allocation model for RAG."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Domain generalization results of the computation allocation model for RAG.",
        "table": "S5.T3.4",
        "footnotes": [],
        "references": [
            "Demonstration-based RAG (DRAG) leverages in-context learning to exploit the capabilities of long-context LLMs by directly generating answers from an extended input context. DRAG builds upon naive RAG and integrates both documents and in-context examples into the input prompt. This expanded context allows the model to generate answers to the input query within a single inference request (See  Figure   3  left). For both in-context examples and the test-time query, we employ a retrieval model to select the top- k k k italic_k  retrieved documents from a large corpus (e.g., Wikipedia). We reverse the order of the retrieved documents, placing higher-ranked documents closer to the query  (Liu et al.,  2024b ) . As we use instruction-tuned LLMs, we design a similar prompt template following  Agarwal et al.   and align the formatting with prefixes for retrieved documents, input and output (See  Appendix   G ). Unlike previous works  (Press et al.,  2023 ; Trivedi et al.,  2023 ) , DRAG incorporates extensive retrieved documents within the demonstrations, enabling long-context LLMs to learn to extract relevant information and answer questions using a rich input context.",
            "Despite access to external knowledge, complex multi-hop queries remain challenging due to the compositionality gap. To tackle this issue, we introduce iterative demonstration-based RAG (IterDRAG), which handles complex queries by decomposing the query into simpler sub-queries. For each sub-query, retrieval is performed to gather additional contextual information, which is then used to generate intermediate answers. After all sub-queries are resolved, the retrieved context, sub-queries, and their answers are combined to synthesize the final answer (See  Figure   3  right).",
            "We also examine the generalization of the computation allocation model for RAG for unseen domains. In other words, the parameters of  Equation   2  are tested on the target domain but learnt from the remaining domains. For inference, only  i i i italic_i  is derived from the target domain. We report the results for 1M effective context length in  Table   3 , where we compare to a 8-shot baseline configuration (scaled by increasing retrieved documents) and the optimum results (Oracle). In summary, the results show that computation allocation model significantly outperforms baseline and closely aligns with the oracle results (96.6% of the optimal performance). Notably, Bamboogle and HotpotQA exhibit highly similar target results, with the performance metrics varying by less than 2.5% from the oracle. These results suggest the potential of applying the computation allocation model for RAG to a wider range of knowledge-intensive tasks.",
            "Despite the performance gains from scaling effective context length, RAG performance on challenging datasets like MuSiQue remain moderate, even for IterDRAG. To address this, we analyze the mistakes in both DRAG and IterDRAG to examine the limitations and errors inherent in these approaches. In the following, we explore common failure cases (See  Figure   13 ) to understand where each method falls short and how they could be further improved.",
            "We provide selected example mistakes from  Figure   13(a)  to  Figure   13(d) , with retrieved documents omitted for brevity. The reasons for common errors can be grouped into four categories: (1) inaccurate or outdated retrieval; (2) incorrect or lack of reasoning; (3) hallucination or unfaithful reasoning; and (4) evaluation issues or refusal to answer. We elaborate on these categories below:",
            "Inaccurate or outdated retrieval : A major source of RAG errors stems from the retrieval process, where relevant knowledge is not correctly retrieved. For example, in the first question of  Figure   13(c) , the top-50 retrieved documents do not contain the correct answer. A similar issue occurs in the second QA pair, where outdated retrieval results fail to provide useful information. In the third case, although both battles are retrieved, the initial documents overly focus on the Battle of Manila, leading to an incorrect response.",
            "Incorrect or lack of reasoning : Beyond retrieval issues, incorrect reasoning chains are another common source of errors. For example, in the first case in  Figure   13(b) , although the correct documents are retrieved, the reasoning process is incomplete (i.e., no explicit comparison of the mountain heights), leading to an incorrect answer in DRAG. Similarly, in the second and third cases, the reasoning is either absent (as in DRAG) or flawed. As a result, reasoning-related errors tend to occur more frequently in difficult questions and in the one-step DRAG approach."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Length extrapolation results of the computation allocation model for RAG.",
        "table": "S5.T4.4",
        "footnotes": [],
        "references": [
            "To analyze the performance changes with different effective context lengths, we plot the performance of all configurations across datasets in  Figure   4 . Similar to  Figure   1 , we visualize DRAG and IterDRAG and highlight the optimal performance  P   ( L max ) superscript P subscript L max P^{*}(L_{\\text{max}}) italic_P start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT )  for different selections of  L max subscript L max L_{\\text{max}} italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT . The fitting results are shown as grey dashed lines. We provide additional dataset-specific results in  Appendix   D .",
            "In addition to predictability on unseen domains, we explore the extrapolation of context length based on the computation allocation model. Here, we estimate the parameters of  Equation   2  using experiments with shorter context lengths and assess their predictive accuracy on longer ones. We assess different extrapolation settings and present the predicted metric values in  Table   4 . Our observations are: (1) The predictions are accurate and consistently outperform the 8-shot baseline. For instance, the average difference between the predicted and oracle results from 128k to 1M tokens is just 2.8%. (2) Extrapolating from 32k to 128k is challenging. This is because DRAG performs best around 32k, while IterDRAG typically excels at a long context of 128k, as evidenced in  Figure   4 . Consequently, it creates a discrepancy between training and predicting performance distribution. (3) 5M context length is less predictable, with the average performance difference between predicted and oracle metrics observed at a substantial 5.6%. Overall, length extrapolation with computation allocation model is accurate and more effective for target lengths below 1M.",
            "We also discuss the impact of long-context modeling w.r.t. RAG performance. In summary, we find that retrieving more documents is generally beneficial for RAG performance, as demonstrated in  Section   4 . Nevertheless, naively extending the context length in each generation step does not always lead to better results. Specifically, DRAG performance peaks at around  10 5 superscript 10 5 10^{5} 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT  tokens, while IterDRAG achieves optimal performance at around  10 6 superscript 10 6 10^{6} 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT  tokens by leveraging multiple rounds of generation. For instance, as seen in the performance plateau in  Figure   1  and  Figure   10 , LLMs struggle to effectively utilize very long contexts (  10 5 absent superscript 10 5 \\geq 10^{5}  10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT  tokens) in each iteration, potentially due to inherent limitations of long-context modeling. Our observations suggest that: (1) the models ability to identify relevant information from extensive context remains to be improved, especially when presented with large quantity of similar documents; (2) the long-context modeling should be further refined to enhance in-context learning capabilities, where multiple lengthy demonstrations are provided.",
            "In our experiments, we utilize the Gecko-1B (en) embedding model to index both the documents and input queries  (Lee et al.,  2024b ) , using Wikipedia passages from the KILT benchmark as the document source  (Petroni et al.,  2020 ) . In test-time, the input query is compared against all embeddings in the corpus, and the top- k k k italic_k  neighbors are selected for inference. Each document is then truncated on the right side to a maximum of 1024 tokens using whitespace tokenization. For each example, we arrange the elements in the following order: documents, query, and label, with the retrieved documents listed in reverse order, placing the higher-ranked documents closer to the query  (Liu et al.,  2024b ) . Consequently, the prompt comprises of multiple in-context examples, followed by the test documents and test query, as illustrated in  Figure   14 .",
            "For generation, we utilize Gemini 1.5 Flash for more efficient experiments.  In DRAG, inference scaling is achieved by increasing the context length through the combination of documents ( k k k italic_k ) and in-context examples ( m m m italic_m ). Then, the prompt (See  Figure   15 ) is provided to the model for a one-time generation using the default generation parameters. For IterDRAG, the input prompt is constructed in a similar fashion, with the example answers consisting of assembled sub-queries, intermediate answers, and the final answer (See  Figure   16 ). Here, we scale test-time compute by incorporating iterative retrieval and generation, along with the increase of documents and demonstrations. In each iteration, we restrict the generation to adhere to the Self-Ask format, in which the response should start with Follow up: , Intermediate answer:  or So the final answer is:   (Koo et al.,  2024 ) . Each iteration begins with the generation of a sub-query and concludes with the production of an intermediate answer. If a sub-query is generated, additional documents are retrieved and appended to the initial set (i.e., Test Documents in  Figure   14 ), after which the model generates an intermediate answer. We allow up to five iterations, after which the model is forced to produce the final answer."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Retrieval performance of DRAG and IterDRAG ( k = 50 k 50 k=50 italic_k = 50  documents,  m = 2 m 2 m=2 italic_m = 2  shots).",
        "table": "A1.T5.7.1",
        "footnotes": [],
        "references": [
            "To gain further insights into the dynamics of DRAG and IterDRAG, we grid search over different combinations of    \\theta italic_  and evaluate the performance. The results are presented in  Figure   5 , where we visualize DRAG performance using heatmaps (See IterDRAG heatmap in  Appendix   C ). Additionally, we provide further results with varying numbers of documents ( k k k italic_k ) and shots ( m m m italic_m ). In summary, scaling retrieval, demonstrations and more generation steps leads to performance gains in most cases, yet such gains vary by effective context length and method. In particular, we note: (1)  Documents and in-context examples are not equally helpful.  For a fixed configuration, increasing the number of retrieved documents  k k k italic_k  usually leads to more substantial performance gains, as evidenced by the differing slopes in  Figure   5 . (2)  Increasing shots  m m m italic_m  is more helpful for IterDRAG.  For example, increase  m m m italic_m  from 0 to 1 (rather than  k k k italic_k ) is more helpful for IterDRAG, possibly due to demonstrations that leads to improved in-context query decomposition and knowledge extraction. (3)  Scaling saturates differently for DRAG and IterDRAG.  An example can be found in the increase of  m m m italic_m  from 0 to 1, which results in significant improvements for IterDRAG but shows little impact on DRAG. Beyond the soft thresholds, further increases in  k k k italic_k  or  m m m italic_m  yield marginal gains or even results in performance declines. (4)  For a given  L max subscript L max L_{\\text{max}} italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT , the optimal    \\theta italic_  depends on the method, metric and dataset.  As illustrated in  Figure   5(a)  and  Figure   8 , the optimal combinations are sensitive to the metrics and located differently, posing challenges for performance modeling w.r.t.    \\theta italic_ . In conclusion, increasing documents, demonstrations and iterations can enhance RAG performance, but each contributes differently to the overall results. As such, identifying the optimal combination of hyperparameters remains challenging.",
            "One critical factor in improving performance of RAG lies in the quality of the retrieved documents. To study how retrieval impacts final accuracy, we analyze retrieval performance and report the results across different document sizes in  Appendix   A . In all datasets, recall scores demonstrate improvements as the number of documents increases, approaching near-perfect scores with large document sets (e.g.,   similar-to \\sim  1k). Despite consistent gains in recall, the results show diminishing returns on discounted ranking metrics like NDCG, indicating increasing distraction within the context. This trend is also evident in in  Figure   5(b) , where RAG performance peaks between 100 and 500 documents. Our observations suggest the necessity of refining retrieval (e.g., through re-ranking) to further optimize the document relevance, particularly in cases of complex, multi-hop queries. However, how the inference scaling behavior discovered in this paper would change in the presence of such a refining component remains unknown. Alternatively, iterative retrieval, as seen in IterDRAG, improves recall performance by using simpler, straightforward sub-queries to collect additional context for each intermediate answer. In summary, retrieving more documents improves recall but does not necessarily lead to better generation quality if the documents are not effectively ranked or filtered. This highlights the need for retrieval methods that dynamically adjust to minimize irrelevant content.",
            "We assess the retrieval quality of DRAG and IterDRAG using the Gecko-1B model  (Lee et al.,  2024b )  and evaluate their impact on final RAG performance. Specifically, we retrieve varying numbers of documents per input query and measure the retrieval quality using three metrics: Recall, NDCG, and MRR, with document counts ranging from 1 to 2k. The retrieval results of DRAG are shown in  Figure   7 . In addition, we evaluate the quality of iterative retrieval, where a maximum of five interleaving retrieval steps are performed. Here, we retrieve 50 documents at each step and use a 2-shot setting, with the results in comparison to DRAG in  Table   5 .",
            "Unlike the one-step retrieval in DRAG, iterative retrieval based on query decomposition often yields simpler sub-queries, facilitating more effective retrieval. In addition, merging the retrieved documents from different steps typically results in higher overall retrieval performance, as evidenced in  Table   5 . With IterDRAG, the performance gains are consistent and reach the average of 30.5%. Specifically, we observe higher gains for complex multi-hop queries (e.g., 2WikiMultiHopQA), where metric improvements can be as high as 57.1%. Moreover, the gains on ranking-discounted metrics (30.7% in NDCG and 39.9% MRR) show greater improvements compared to recall (21.7%). In summary, these findings highlight the superiority of iterative retrieval with query decomposition over one-step methods, which effectively contribute to the overall performance of IterDRAG.",
            "We report the IterDRAG results averaged across datasets in  Figure   8 , shown as heatmaps where the x-axis represents the number of documents and the y-axis represents the number of shots. Performance is color-coded, with blue indicating lower values and red indicating higher values. The best-performing combinations are located toward the bottom right of each heatmap, which corresponds to longer context lengths. In comparison to DRAG, as reported in  Figure   5(a) , the optimal number of in-context examples is higher at 32, which highlights the importance of in-context demonstrations in enabling better query decomposition and interleaved retrieval. Combined with multiple generation steps, IterDRAG further improves RAG performance over DRAG.",
            "For generation, we utilize Gemini 1.5 Flash for more efficient experiments.  In DRAG, inference scaling is achieved by increasing the context length through the combination of documents ( k k k italic_k ) and in-context examples ( m m m italic_m ). Then, the prompt (See  Figure   15 ) is provided to the model for a one-time generation using the default generation parameters. For IterDRAG, the input prompt is constructed in a similar fashion, with the example answers consisting of assembled sub-queries, intermediate answers, and the final answer (See  Figure   16 ). Here, we scale test-time compute by incorporating iterative retrieval and generation, along with the increase of documents and demonstrations. In each iteration, we restrict the generation to adhere to the Self-Ask format, in which the response should start with Follow up: , Intermediate answer:  or So the final answer is:   (Koo et al.,  2024 ) . Each iteration begins with the generation of a sub-query and concludes with the production of an intermediate answer. If a sub-query is generated, additional documents are retrieved and appended to the initial set (i.e., Test Documents in  Figure   14 ), after which the model generates an intermediate answer. We allow up to five iterations, after which the model is forced to produce the final answer."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Chain-of-thought (CoT) vs. IterDRAG results ( k = 5 k 5 k=5 italic_k = 5  documents,  m = 4 m 4 m=4 italic_m = 4  shots).",
        "table": "A2.T6.10",
        "footnotes": [],
        "references": [
            "We evaluate the computation allocation model for RAG by comparing the predicted metrics to the actual values, with normalized results for DRAG visualized in  Figure   6 . Here, each subplot represents a different dataset, and each line corresponds to a document setting ( k k k italic_k ), we scale the context length by adjusting in-context examples ( m m m italic_m ). As illustrated, the performance improves with the increase of  k k k italic_k  and  m m m italic_m  across datasets, displaying highly consistent trends between the predicted and actual metric values, despite some variations. Notably, each dataset exhibits different levels of consistency: Bamboogle exhibits the highest consistency, while HotpotQA generates more variable results. Our findings demonstrate how external knowledge and in-context learning can effectively enhance RAG performance with long-context capabilities, suggesting the effectiveness of the computation allocation model for RAG and how they may be used to predict benchmark results.",
            "To evaluate different iterative strategies, we compare the commonly used chain-of-thought (CoT) with IterDRAG  (Wei et al.,  2022 ) . In particular, we generate the CoT examples following  Trivedi et al. ( 2023 )  and adopt the 4-shot setting with 5 documents. The results on three larger datasets (HotpotQA, MuSiQue and 2WikiMultiHopQA), as reported in  Table   6 , highlight the performance differences between these strategies, in which IterDRAG consistently outperforms CoT with significant improvements. Such difference can be traced back to three key factors: (1) the retrieval quality of CoT is limited without interleaving retrieval as in IterDRAG; (2) Gemini 1.5 Flash is relatively small and may not perform well in free-form reasoning in comparison to larger LLMs; and (3) the generated CoT examples are less informative than handcrafted ones and underperform compared to constrained decoding with Self-Ask  (Press et al.,  2023 ; Koo et al.,  2024 ) . Consequently, IterDRAG demonstrates its effectiveness as a scalable method for knowledge-intensive tasks.",
            "We also visualize the predictions on for IterDRAG across different datasets in  Figure   11 , where each subplot represents a dataset and each line corresponds to a document setting ( k k k italic_k ). The inference compute is scaled by increasing the number of in-context examples ( m m m italic_m ) and generation iterations ( n n n italic_n ). Here, we find similar trends to those in  Figure   6 , although IterDRAG shows larger variations compared to DRAG. HotpotQA and 2WikiMultiHopQA show more consistent trends with the predictions, likely due to the predominance of multi-hop queries. In summary, our findings are consistent for both DRAG and IterDRAG, demonstrating that RAG performance can be accurately modeled by our computation allocation model for RAG. For Bamboogle, HotpotQA and 2WikiMultiHopQA, we provide the normalized performance with increasing effective context lengths in  Figure   10 , in which we observe similar trends to the results on MuSiQue (See  Figure   1 ). We also illustrate the prediction surface for both DRAG and IterDRAG in  Figure   12 .",
            "For generation, we utilize Gemini 1.5 Flash for more efficient experiments.  In DRAG, inference scaling is achieved by increasing the context length through the combination of documents ( k k k italic_k ) and in-context examples ( m m m italic_m ). Then, the prompt (See  Figure   15 ) is provided to the model for a one-time generation using the default generation parameters. For IterDRAG, the input prompt is constructed in a similar fashion, with the example answers consisting of assembled sub-queries, intermediate answers, and the final answer (See  Figure   16 ). Here, we scale test-time compute by incorporating iterative retrieval and generation, along with the increase of documents and demonstrations. In each iteration, we restrict the generation to adhere to the Self-Ask format, in which the response should start with Follow up: , Intermediate answer:  or So the final answer is:   (Koo et al.,  2024 ) . Each iteration begins with the generation of a sub-query and concludes with the production of an intermediate answer. If a sub-query is generated, additional documents are retrieved and appended to the initial set (i.e., Test Documents in  Figure   14 ), after which the model generates an intermediate answer. We allow up to five iterations, after which the model is forced to produce the final answer."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  StrategyQA accuracy results.",
        "table": "A3.T7.4",
        "footnotes": [],
        "references": [
            "We assess the retrieval quality of DRAG and IterDRAG using the Gecko-1B model  (Lee et al.,  2024b )  and evaluate their impact on final RAG performance. Specifically, we retrieve varying numbers of documents per input query and measure the retrieval quality using three metrics: Recall, NDCG, and MRR, with document counts ranging from 1 to 2k. The retrieval results of DRAG are shown in  Figure   7 . In addition, we evaluate the quality of iterative retrieval, where a maximum of five interleaving retrieval steps are performed. Here, we retrieve 50 documents at each step and use a 2-shot setting, with the results in comparison to DRAG in  Table   5 .",
            "In  Figure   7 , recall demonstrates consistent improvements as the number of documents increases, approaching near-perfect scores when large document sets (e.g., 1k) are retrieved. However, both NDCG and MRR metrics plateau early at around 100 documents, with diminishing gains as the document count further rises. This divergence suggests that while more documents lead to better recall, the relevance and ranking quality (captured by NDCG and MRR) do not improve proportionally, and even introduce extensive noise. Therefore, higher recall doesnt necessarily translate into better final answer quality when the retrieved documents arent effectively ranked or filtered.",
            "We also include the multi-hop and binary StrategyQA dataset in our experiments, see  Table   7   (Geva et al.,  2021 ) . Despite being binary questions, we observe similar trends to our main experiments. For example, DRAG consistently outperforms the baseline QA and RAG methods, with 29.3% accuracy improvement to for the baseline QA model. Furthermore, the performance is boosted with 83.4 accuracy using the iterative IterDRAG. These results demonstrate that even for binary, multi-hop tasks, iterative approaches provide substantial gains, confirming the effectiveness of both long-context and iterative strategies for inference scaling in RAG."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Computation allocation mode of Gemini 1.5 Flash with  p p p italic_p -value,  R 2 superscript R 2 R^{2} italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  and MSE statistics.",
        "table": "A5.T8.9",
        "footnotes": [],
        "references": [
            "To gain further insights into the dynamics of DRAG and IterDRAG, we grid search over different combinations of    \\theta italic_  and evaluate the performance. The results are presented in  Figure   5 , where we visualize DRAG performance using heatmaps (See IterDRAG heatmap in  Appendix   C ). Additionally, we provide further results with varying numbers of documents ( k k k italic_k ) and shots ( m m m italic_m ). In summary, scaling retrieval, demonstrations and more generation steps leads to performance gains in most cases, yet such gains vary by effective context length and method. In particular, we note: (1)  Documents and in-context examples are not equally helpful.  For a fixed configuration, increasing the number of retrieved documents  k k k italic_k  usually leads to more substantial performance gains, as evidenced by the differing slopes in  Figure   5 . (2)  Increasing shots  m m m italic_m  is more helpful for IterDRAG.  For example, increase  m m m italic_m  from 0 to 1 (rather than  k k k italic_k ) is more helpful for IterDRAG, possibly due to demonstrations that leads to improved in-context query decomposition and knowledge extraction. (3)  Scaling saturates differently for DRAG and IterDRAG.  An example can be found in the increase of  m m m italic_m  from 0 to 1, which results in significant improvements for IterDRAG but shows little impact on DRAG. Beyond the soft thresholds, further increases in  k k k italic_k  or  m m m italic_m  yield marginal gains or even results in performance declines. (4)  For a given  L max subscript L max L_{\\text{max}} italic_L start_POSTSUBSCRIPT max end_POSTSUBSCRIPT , the optimal    \\theta italic_  depends on the method, metric and dataset.  As illustrated in  Figure   5(a)  and  Figure   8 , the optimal combinations are sensitive to the metrics and located differently, posing challenges for performance modeling w.r.t.    \\theta italic_ . In conclusion, increasing documents, demonstrations and iterations can enhance RAG performance, but each contributes differently to the overall results. As such, identifying the optimal combination of hyperparameters remains challenging.",
            "We report the IterDRAG results averaged across datasets in  Figure   8 , shown as heatmaps where the x-axis represents the number of documents and the y-axis represents the number of shots. Performance is color-coded, with blue indicating lower values and red indicating higher values. The best-performing combinations are located toward the bottom right of each heatmap, which corresponds to longer context lengths. In comparison to DRAG, as reported in  Figure   5(a) , the optimal number of in-context examples is higher at 32, which highlights the importance of in-context demonstrations in enabling better query decomposition and interleaved retrieval. Combined with multiple generation steps, IterDRAG further improves RAG performance over DRAG.",
            "In addition to multi-hop question answering datasets, we also report results on one-hop datasets, specifically TriviaQA and Natural Questions  (Joshi et al.,  2017 ; Kwiatkowski et al.,  2019 ) . The evaluations for one-hop datasets are performed with DRAG and presented in  Figure   9 , similar to  Figure   8 . For TriviaQA, increasing the number of documents generally leads to improved accuracy, where the highest accuracy of 69.0% is achieved with 50 documents. In Natural Questions, performance increases with the number of documents up to about 10 or 20 documents, but further increases in the document count lead to diminishing returns or even slight declines in accuracy. The highest accuracy of 54.6% is achieved with 20 documents in 1-shot, and performance drops slightly when more documents are included. In summary, the optimal number of shots falls between 1 and 4. While increasing the number of shots and documents leads to initial performance gains, these improvements plateau beyond certain thresholds. This trend, in contrast to multi-hop datasets, may be partially attributed to the nature of the one-hop questions and retrieval relevance.",
            "We further explore the findings on the computation allocation model. In particular, we report the estimated parameters along with  p p p italic_p -values,  R 2 superscript R 2 R^{2} italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , and MSE statistics in  Table   8 . In our implementation, we constrain the last element of  b b b italic_b , leaving six learnable parameters in total. Our analysis shows that all parameters are statistically significant, except for  b 1 subscript b 1 b_{1} italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , which has a  p p p italic_p -value slightly above 0.05. Nonetheless, our experiments suggest that retaining  b 1 subscript b 1 b_{1} italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  improves generalization in many cases, such as IterDRAG on multi-hop datasets. For sigmoid scaling, we fit a custom function between the predicted  P ^ ^ P \\hat{P} over^ start_ARG italic_P end_ARG  and ground truth  P P P italic_P  values, defined as    ( x ) = 3.30 1 + e  1.81  ( x + 0.46 )  2.18  x 3.30 1 superscript e 1.81 x 0.46 2.18 \\sigma(x)=\\frac{3.30}{1+e^{-1.81(x+0.46)}}-2.18 italic_ ( italic_x ) = divide start_ARG 3.30 end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT - 1.81 ( italic_x + 0.46 ) end_POSTSUPERSCRIPT end_ARG - 2.18 ."
        ]
    },
    "global_footnotes": [
        "Different from in-context RAG that prepends documents / QA examples",
        ", we leverage multiple examples comprising of documents, questions and answers to demonstrate the task.",
        "In our implementation, we shift the values within",
        "by a small",
        "to prevent numerical issues with",
        "."
    ]
}