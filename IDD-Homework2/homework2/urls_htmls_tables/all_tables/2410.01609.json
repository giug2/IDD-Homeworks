{
    "id_table_1": {
        "caption": "Table 2:  Selective breakdown results of performance across representative categories.",
        "table": "A4.T8.1.1",
        "footnotes": [],
        "references": [
            "Overall Trend  Table  1  presents the performance of various model configurations, demonstrating the effectiveness of the proposed domain adaptation methods in capturing domain knowledge. Due to their strong baseline performance, LayoutLMv3 and LXMERT were selected as token and entity encoders to construct the joint-grained Domain Knowledge Infusers  A D subscript A D \\mathcal{A}_{D} caligraphic_A start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT  within the framework  F F \\mathcal{F} caligraphic_F . The results show that integrating fine and coarse-grained information within  F F \\mathcal{F} caligraphic_F  outperforms mono-grained baselines, boosting downstream task performance. We note that incorporating fine-grained features significantly enhanced entity representation in FormNLU, with a performance gain of approximately 8% for the printed and 21% for the handwritten sets. All domain adaptation methods, including the novel L2V positional features, improved performance. Detailed analyzes are in subsequent sections.",
            "Few-shot Testing  We evaluated the robustness of our methods with varying amounts of annotated data from  D g subscript D g \\mathbb{D}_{g} blackboard_D start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , using training sizes from 10% to 100% of  D t subscript D t \\mathcal{D}_{t} caligraphic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . As shown in Table  1 , domain adaptation consistently outperformed non-adapted baselines by leveraging domain-specific information from the synthetic dataset  D n subscript D n \\mathbb{D}_{n} blackboard_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , although performance sensitivity varied across different tasks and training sizes. For the entity-level FormNLU, both printed (P) and handwritten (H) test sets improved as training sizes increased. Without domain adaptation, performance was poor in few-shot scenarios. With just 10% of  D g subscript D g \\mathbb{D}_{g} blackboard_D start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT , SDS achieved over 80% accuracy on both P and H sets, demonstrating its ability to capture domain-specific structural information and enhance semantic understanding. For token-level results in CORD, incorporating coarse-grained information improved performance across training sizes. SDS consistently outperformed other configurations, effectively utilizing synthetic structural information from  D n subscript D n \\mathbb{D}_{n} blackboard_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT . However, SIT and SST underperformed in few-shot settings, likely due to reliance on synthetic LLM-generated samples that need more data to bridge distribution gaps.",
            "We evaluated the state-of-the-art LLMs and MLLMs to address VRDU tasks using various mono- and multi-modal prompts across different model checkpoints based on various training approaches, comparing their performance and efficiency with the DAViD framework in Table  5 . For close-source GPT-4o, two prompts were used: the text-only prompt  P t : { K , C } : subscript P t K C P_{t}:\\{K,C\\} italic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT : { italic_K , italic_C } , where  K K K italic_K  is the key text content and  C C C italic_C  is the provided text content, and the text-vision prompt  P t  v : { K , C , I } : subscript P t v K C I P_{tv}:\\{K,C,I\\} italic_P start_POSTSUBSCRIPT italic_t italic_v end_POSTSUBSCRIPT : { italic_K , italic_C , italic_I } , where  I I I italic_I  is the target form image. GPT-3.5 uses  P t subscript P t P_{t} italic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  only and other open source MLLMs are used  P t  v subscript P t v P_{tv} italic_P start_POSTSUBSCRIPT italic_t italic_v end_POSTSUBSCRIPT  to leverage text and vision information. GPT-4o with prompt  P t subscript P t P_{t} italic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  outperforms GPT-3.5 using the same prompt, while with the multimodal prompt  P t  v subscript P t v P_{tv} italic_P start_POSTSUBSCRIPT italic_t italic_v end_POSTSUBSCRIPT , GPT-4o achieves around a 13% increase in F1 score. Other open-source MLLMs show an apparent gap between close GPT-series  4 4 4  Appendix  D.4.1  provides prompt details. Detailed LLM-based analysis are in Appendix  D.4.2 and  D.5 .",
            "To qualitatively demonstrate the effectiveness of the proposed framework, a real-world example from the CORD is presented in Figure  15 . Compared to baseline models, the joint-grained framework produces fewer incorrect predictions, likely due to the integration of coarse-grained information. In this case, while SDS alone does not improve results, the SST approach shows noticeable enhancements. Furthermore, combining both domain adaptation methods results in entirely accurate predictions. This highlights the effectiveness of proposed domain adaptation techniques in leveraging domain knowledge from noisily annotated data to improve downstream task performance  5 5 5 More visualized quantitative examples with analysis could be found in Appendix  E.2 .",
            "In Section  6.1  of the main paper, we analyze the performance under different configurations of selective categories. This section presents detailed experimental results for each sub-category, providing insights into the effects of the proposed methods and modules on specific categories.",
            "The overall and breakdown results of CORD datasets are also represented in Table  10  and  11 . Compared with integrating fine-grained level information to coarse-grained, there is limited improvement on integrating coarse-grained information to fine-grained baselines.",
            "In the FormNLU dataset, both the printed set (P) and handwritten set (H) exhibit similar patterns as represented by Table  12  and Table  13 . While incorporating fine-grained information can enhance performance and robustness, especially when using smaller guidance sets, the overall performance still falls short compared to mono-grained baselines. However, the proposed domain adaptation approaches significantly improve robustness when the guidance set size,  D n subscript D n \\mathbb{D}_{n} blackboard_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , is reduced. In particular, Structural Domain Shifting (SDS) demonstrates a strong ability to capture domain-specific information across all guidance set ratios. Moreover, combining Synthetic Sequence Tagging (SST) with SDS results in even better performance when a larger, well-annotated guidance set is available.",
            "Robustness Analysis - Incorrect Labels  Incorrect label assignments cause models to learn inaccurate information during training, which could be used to assess their robustness in handling noisy or misleading data during training. As shown in Table  15 , the joint-grained framework, warmed on SDS with NST, exhibits superior robustness compared to all other configurations, significantly outperforming the baseline. This highlights the effectiveness of the proposed frameworks and domain adaptation strategies in mitigating the negative impact of incorrect labels and enhancing model robustness in real-world applications.",
            "Robustness Analysis - Incomplete Labels  The absence or unavailability of labels prevents models from learning effectively from samples with missing labels, which is used as another criterion to assess the models robustness in dealing with incomplete datasets. As shown in Table  15 , joint-grained frameworks demonstrate consistent robustness compared to the mono-grained baseline model, highlighting that fusing coarse-grained information leads to a more robust fine-grained document representation. Additionally, after tuning the joint-grained framework on various domain adaptation tasks, the performance is further improved, illustrating that the proposed domain adaptation approach enhances robustness in scenarios where labels are absent.",
            "The prompt details for each employed LLM/MLLM within the FormNLU dataset are provided in Table  16 . The generated outputs are subsequently post-processed to compute the Jaccard distance between target entities, thereby ensuring accurate identification of the entity most closely matching the ground truth. For the CORD dataset, we adopt the LayoutLLM  (Luo et al.,  2024 )  configurations, utilizing ANLS as the evaluation metric.",
            "We show the breakdown performance of different LLMs/MLLMs predictions under zero-shot scenarios of printed set in Table  17  and handwritten set in Table  18 , respectively. The results indicate that closed-source models exhibit relatively lower performance compared to other models. Consistent with the overall performance trends, closed-source models, even when utilizing non-multimodal output forms, tend to underperform against open-source MLLMs across most categories. Notably, the digit-based entities, e.g. ppn, pvp, located within the table remain challenging using text inputs alone, suggesting that incorporating visual information could enhance performance.",
            "As discussed in Section  D.3 , synthetic noise is introduced into the guidance set  D g subscript D g \\mathbb{D}_{g} blackboard_D start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT  of the CORD dataset. This noisy dataset is then used to fine-tune the model, which is subsequently tested on a well-annotated test set  D t subscript D t \\mathbb{D}_{t} blackboard_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Compared to the FormNLU dataset, the CORD dataset shows limited performance improvement. We applied random noise following a normal distribution to demonstrate the robustness of the proposed DAViD framework, rather than focusing solely on performance. This noise is introduced by replacing the original labels with incorrect labels (Figure  11 ) or marking them as unknown (Figure  12 ). Figures  11  and  12  illustrate the distribution of original and noisy labels across varying levels of noise rates."
        ]
    },
    "id_table_2": {
        "caption": "Table 7:  Original and synthetic annotated datasets of adopted datasets.",
        "table": "A4.T9.1.1",
        "footnotes": [],
        "references": [
            "To prepare the benchmark datasets, as shown by Figure  2 , we apply  a) Document Collection Re-allocation  and  b) Synthetic Layout Annotation  for structural adaptation across all datasets. For task-specific knowledge enhancers, additional procedures like  c) Synthetic Sequence Tagging  and  d) Synthetic Inquiry Generation  simulate practical scenarios, enabling DAViD to capture domain-specific variations and semantic relationships.  a) Document Collection Re-allocation  replicates real-world scenarios by dividing the original dataset into three subsets: synthetic annotated, manually annotated, and test sets. The original training set is used as the synthetic annotated set, the validation set as the fully annotated set, and the test set for evaluation. Synthetic annotations are generated using off-the-shelf tools to help the model learn and differentiate between layout and semantic information at various granularities.  b) Synthetic Layout Annotation  extracts grouped textual tokens, textlines, or document semantic entities to capture layout structures. Tools like PDFMiner, OCR tools  1 1 1 For example, PaddleOCR:  https://github.com/PaddlePaddle/PaddleOCR  is widely used. , and layout analysis models generate synthetic layout annotations, capturing bounding box coordinates and textual content.  c) Synthetic Sequence Tagging  creates synthetic annotations for token sequences to support fine-grained sequence tagging. Large language models (LLMs) generate labels for each document, which may differ from manually annotated labels. Fine-tuning these synthetic annotations enhances the models contextual understanding.  d) Synthetic Inquiry Generation  uses question-answer pairs generated by LLMs to leverage general textual knowledge. Prompts are designed to extract QA pairs, then matched with entities from layout analyzers. The highest-matched entity serves as the retrieval target for instructed tuning  2 2 2 More detailed dataset statistics and synthetic data analysis please refer to Appendix  C .",
            "Breakdown Analysis  Table  2  compares performance across various information categories, highlighting the benefits of the joint-grained framework in generating comprehensive representations. This framework enriches entity semantics and token structures, leading to notable improvementssuch as a 58% increase in  cid  in FormNLU-H and an 18% increase in  SubC  in CORD. While L2V enhances feature representation overall, it may introduce inconsistencies in flexible layout categories, like handwritten  cid  in FormNLU. The proposed methods, especially SDS, consistently show robust improvements across most categories, demonstrating their effectiveness in capturing domain-aware knowledge. Although leveraging LLM-generated tags (SST) or QA pairs (SIT) boosts performance, it may lead to occasional instability. For example, combining SDS with SST or SIT improve specific categories but may yield lower results in otherssuch as a 20% decrease in CORDs  SubC  when using SDS+SST compared to SST.",
            "We evaluated the state-of-the-art LLMs and MLLMs to address VRDU tasks using various mono- and multi-modal prompts across different model checkpoints based on various training approaches, comparing their performance and efficiency with the DAViD framework in Table  5 . For close-source GPT-4o, two prompts were used: the text-only prompt  P t : { K , C } : subscript P t K C P_{t}:\\{K,C\\} italic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT : { italic_K , italic_C } , where  K K K italic_K  is the key text content and  C C C italic_C  is the provided text content, and the text-vision prompt  P t  v : { K , C , I } : subscript P t v K C I P_{tv}:\\{K,C,I\\} italic_P start_POSTSUBSCRIPT italic_t italic_v end_POSTSUBSCRIPT : { italic_K , italic_C , italic_I } , where  I I I italic_I  is the target form image. GPT-3.5 uses  P t subscript P t P_{t} italic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  only and other open source MLLMs are used  P t  v subscript P t v P_{tv} italic_P start_POSTSUBSCRIPT italic_t italic_v end_POSTSUBSCRIPT  to leverage text and vision information. GPT-4o with prompt  P t subscript P t P_{t} italic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  outperforms GPT-3.5 using the same prompt, while with the multimodal prompt  P t  v subscript P t v P_{tv} italic_P start_POSTSUBSCRIPT italic_t italic_v end_POSTSUBSCRIPT , GPT-4o achieves around a 13% increase in F1 score. Other open-source MLLMs show an apparent gap between close GPT-series  4 4 4  Appendix  D.4.1  provides prompt details. Detailed LLM-based analysis are in Appendix  D.4.2 and  D.5 .",
            "To qualitatively demonstrate the effectiveness of the proposed framework, a real-world example from the CORD is presented in Figure  15 . Compared to baseline models, the joint-grained framework produces fewer incorrect predictions, likely due to the integration of coarse-grained information. In this case, while SDS alone does not improve results, the SST approach shows noticeable enhancements. Furthermore, combining both domain adaptation methods results in entirely accurate predictions. This highlights the effectiveness of proposed domain adaptation techniques in leveraging domain knowledge from noisily annotated data to improve downstream task performance  5 5 5 More visualized quantitative examples with analysis could be found in Appendix  E.2 .",
            "In the FormNLU dataset, both the printed set (P) and handwritten set (H) exhibit similar patterns as represented by Table  12  and Table  13 . While incorporating fine-grained information can enhance performance and robustness, especially when using smaller guidance sets, the overall performance still falls short compared to mono-grained baselines. However, the proposed domain adaptation approaches significantly improve robustness when the guidance set size,  D n subscript D n \\mathbb{D}_{n} blackboard_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , is reduced. In particular, Structural Domain Shifting (SDS) demonstrates a strong ability to capture domain-specific information across all guidance set ratios. Moreover, combining Synthetic Sequence Tagging (SST) with SDS results in even better performance when a larger, well-annotated guidance set is available.",
            "As discussed in Section  D.3 , synthetic noise is introduced into the guidance set  D g subscript D g \\mathbb{D}_{g} blackboard_D start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT  of the CORD dataset. This noisy dataset is then used to fine-tune the model, which is subsequently tested on a well-annotated test set  D t subscript D t \\mathbb{D}_{t} blackboard_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Compared to the FormNLU dataset, the CORD dataset shows limited performance improvement. We applied random noise following a normal distribution to demonstrate the robustness of the proposed DAViD framework, rather than focusing solely on performance. This noise is introduced by replacing the original labels with incorrect labels (Figure  11 ) or marking them as unknown (Figure  12 ). Figures  11  and  12  illustrate the distribution of original and noisy labels across varying levels of noise rates."
        ]
    },
    "id_table_3": {
        "caption": "Table 8:  Model breakdown performance on FormNLU printed set. Explanation of abbreviations: cnm (Company Name/Scheme), cid (Company ID), hnm (Holder Name), hid (Holder ID), cdt (Change Date), pdt (Previous Notice Date), gdt (Given Date), cls (Class of Securities), ppn (Previous Persons Votes), pvp (Previous Voting Power), cpn (Current Persons Votes), cvp (Current Voting Power).",
        "table": "A4.T10.1.1",
        "footnotes": [],
        "references": [
            "Zero-shot Testing  We evaluated zero-shot performance (Table  3 ) to assess domain knowledge capture. SDS effectively distilled structural knowledge from  D n subscript D n \\mathbb{D}_{n} blackboard_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , achieving 87.42% on FormNLU (printed) and 81.74% (handwritten). In contrast, SIT showed minor improvements on the printed set but decreased on the handwritten set, due to the distribution gap between digital-born QA pairs from  D g subscript D g \\mathbb{D}_{g} blackboard_D start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT  and handwritten tests. For CORD, domain adaptation shows less impact than entity-level tasks, as the joint-grained framework benefits entity representations more than fine-grained token representations. Entities can contextually learn from tokens, improving semantic understanding and attention alignment during domain adaptation and fine-tuning. Tokens gain less from coarse-grained embeddings, highlighting the need for joint-grained frameworks as a future research direction.",
            "To explore the effects of the size of the guidance set on test set performance, we reported and analyzed the performance in Figure  3 . The exact performance of each guidance set ratio is listed in an additional analysis.",
            "In the FormNLU dataset, both the printed set (P) and handwritten set (H) exhibit similar patterns as represented by Table  12  and Table  13 . While incorporating fine-grained information can enhance performance and robustness, especially when using smaller guidance sets, the overall performance still falls short compared to mono-grained baselines. However, the proposed domain adaptation approaches significantly improve robustness when the guidance set size,  D n subscript D n \\mathbb{D}_{n} blackboard_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , is reduced. In particular, Structural Domain Shifting (SDS) demonstrates a strong ability to capture domain-specific information across all guidance set ratios. Moreover, combining Synthetic Sequence Tagging (SST) with SDS results in even better performance when a larger, well-annotated guidance set is available.",
            "As discussed in Section  D.3 , synthetic noise is introduced into the guidance set  D g subscript D g \\mathbb{D}_{g} blackboard_D start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT  of the CORD dataset. This noisy dataset is then used to fine-tune the model, which is subsequently tested on a well-annotated test set  D t subscript D t \\mathbb{D}_{t} blackboard_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Compared to the FormNLU dataset, the CORD dataset shows limited performance improvement. We applied random noise following a normal distribution to demonstrate the robustness of the proposed DAViD framework, rather than focusing solely on performance. This noise is introduced by replacing the original labels with incorrect labels (Figure  11 ) or marking them as unknown (Figure  12 ). Figures  11  and  12  illustrate the distribution of original and noisy labels across varying levels of noise rates."
        ]
    },
    "id_table_4": {
        "caption": "Table 9:  Model breakdown performance on FormNLU handwritten set. Explanation of abbreviations: cnm (Company Name/Scheme), cid (Company ID), hnm (Holder Name), hid (Holder ID), cdt (Change Date), pdt (Previous Notice Date), gdt (Given Date), cls (Class of Securities), ppn (Previous Persons Votes), pvp (Previous Voting Power), cpn (Current Persons Votes), cvp (Current Voting Power).",
        "table": "A4.T11.1.1",
        "footnotes": [],
        "references": [
            "Effects of Training Epochs  We observed that varying the number of training epochs (with ep. 1 representing one epoch in Table  4 ) for different domain adaptation methods impacts fine-tuning results. Insufficient training can result in limited domain-specific information capture. For instance, training the SDS+SST method for just one epoch on the CORD dataset yields about 2.5% lower performance than two epochs. Conversely, increasing training epochs can cause the model distribution to shift closer to  D n subscript D n \\mathbb{D}_{n} blackboard_D start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , but further away from  D g subscript D g \\mathbb{D}_{g} blackboard_D start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT . For example, training SDS+SIT for three epochs on the FUNSD dataset resulted in a performance drop of approximately 2.5% and 5% on sets P and R, respectively. Finding the optimal number of epochs for each domain adaptation strategy requires careful adjustment based on the specific dataset and task.",
            "Effects of Freezing  To retain domain knowledge infused from  D  n D n \\mathbb{D}n blackboard_D italic_n  by the joint-grained encoder  E j  g subscript E j g \\mathcal{E}_{jg} caligraphic_E start_POSTSUBSCRIPT italic_j italic_g end_POSTSUBSCRIPT , freezing its parameters after applying SDS proved beneficial. It preserved the learned structure and semantic insights, leading to better performance during fine-tuning. As shown in Table  4 , unfreezing the models resulted in lower performance. For example, SDS+SIT on FormNLU-P dropped from 92.62% to 88.58% when the parameters were not frozen.",
            "Effects of L2V  We evaluated the impact of the L2V positional feature on domain adaptation methods. As shown in Table  4 , removing L2V led to an approximate 2% performance drop. This suggests that L2V enhances positional-awareness in token and entity representations, contributing to better document understanding.",
            "We evaluated the state-of-the-art LLMs and MLLMs to address VRDU tasks using various mono- and multi-modal prompts across different model checkpoints based on various training approaches, comparing their performance and efficiency with the DAViD framework in Table  5 . For close-source GPT-4o, two prompts were used: the text-only prompt  P t : { K , C } : subscript P t K C P_{t}:\\{K,C\\} italic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT : { italic_K , italic_C } , where  K K K italic_K  is the key text content and  C C C italic_C  is the provided text content, and the text-vision prompt  P t  v : { K , C , I } : subscript P t v K C I P_{tv}:\\{K,C,I\\} italic_P start_POSTSUBSCRIPT italic_t italic_v end_POSTSUBSCRIPT : { italic_K , italic_C , italic_I } , where  I I I italic_I  is the target form image. GPT-3.5 uses  P t subscript P t P_{t} italic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  only and other open source MLLMs are used  P t  v subscript P t v P_{tv} italic_P start_POSTSUBSCRIPT italic_t italic_v end_POSTSUBSCRIPT  to leverage text and vision information. GPT-4o with prompt  P t subscript P t P_{t} italic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  outperforms GPT-3.5 using the same prompt, while with the multimodal prompt  P t  v subscript P t v P_{tv} italic_P start_POSTSUBSCRIPT italic_t italic_v end_POSTSUBSCRIPT , GPT-4o achieves around a 13% increase in F1 score. Other open-source MLLMs show an apparent gap between close GPT-series  4 4 4  Appendix  D.4.1  provides prompt details. Detailed LLM-based analysis are in Appendix  D.4.2 and  D.5 ."
        ]
    }
}