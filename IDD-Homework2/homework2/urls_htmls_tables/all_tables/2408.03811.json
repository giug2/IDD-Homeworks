{
    "id_table_1": {
        "caption": "Table 1:  Distribution of labels in BEETLE and SciEntsBank datasets.",
        "table": "S2.T1.1",
        "footnotes": [],
        "references": [
            "Table  1  contains a summary of the 5-way distributions from which one may derive the 3-way and 2-way distributions. What differentiates this task from typical ASAS is that the 5-way and 3-way classification schemes are not ordinal, which is naturally the consequence of a holistic rubric. This distinguishes the task as a classification task from the Kaggle ASAS dataset  [ 23 ]  and the Powergrading dataset  [ 2 ] . Traditional metrics for evaluating the performance of automated scoring systems  [ 20 ] , like Cohens quadratic weighted kappa, do not apply. This also means traditional regression-based approaches that deliver impressive performance for holistic rubrics, such as the Kaggle dataset (e.g.,  [ 24 ,  25 ] ), should not be applied.",
            "Consider a set of documents,  D D D italic_D , consisting of elements  d 1 , ... , d n subscript d 1 ... subscript d n d_{1},\\ldots,d_{n} italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT . We can represent their normalized embeddings as rows in a matrix  A A A italic_A . If we can convert a query into a vector  x x x italic_x , then finding the  k k k italic_k  most relevant documents to that query in  D D D italic_D  becomes equivalent to identifying the  k k k italic_k  largest values in the vector resulting from the matrix multiplication  A  x A x Ax italic_A italic_x . This process is illustrated in Figure  1 .",
            "Cosine Similarity Loss:  l  o  s  s = ( c  o  s  ( e  m  b  e  d  ( a  n  s  w  e  r 1 ) , e  m  b  e  d  ( a  n  s  w  e  r 2 ) )  l  a  b  e  l ) 2 l o s s superscript c o s e m b e d a n s w e subscript r 1 e m b e d a n s w e subscript r 2 l a b e l 2 loss=(cos(embed(answer_{1}),embed(answer_{2}))-label)^{2} italic_l italic_o italic_s italic_s = ( italic_c italic_o italic_s ( italic_e italic_m italic_b italic_e italic_d ( italic_a italic_n italic_s italic_w italic_e italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , italic_e italic_m italic_b italic_e italic_d ( italic_a italic_n italic_s italic_w italic_e italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ) - italic_l italic_a italic_b italic_e italic_l ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , where  a  n  s  w  e  r 1 a n s w e subscript r 1 answer_{1} italic_a italic_n italic_s italic_w italic_e italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  a  n  s  w  e  r 2 a n s w e subscript r 2 answer_{2} italic_a italic_n italic_s italic_w italic_e italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  are the two answers in a pair,  l  a  b  e  l l a b e l label italic_l italic_a italic_b italic_e italic_l  is the label of the pair,  e  m  b  e  d  (  ) e m b e d embed(*) italic_e italic_m italic_b italic_e italic_d (  )  is the embedding operation using the backbone model, and  c  o  s c o s cos italic_c italic_o italic_s  is presented in Equation  1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Main Results SCIENTSBANK 3-way.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "This paper is organized as follows: We introduce background about datasets, information retrieval, generative language models, retrieval augmented generation, and prompt optimization in Section  2 . We describe the particular way in which we fine-tune our IR pipeline, our implementation of RAG, and the manner in which we optimize prompts in Section  3 . We present our experiments, which includes metrics used, and the relevant results in Section  4 . Lastly, we conclude with a discussion of future research directions in Section  5 .",
            "During the online running phase, our system processes each test response through a sequence of three steps: 1) retrieve  K K K italic_K  similar responses, 2) compose GLM prompt with retrievals, 3) call task GLM and parse the result. The workflow is shown in  2 .",
            "This experiment presents the results of the SCIENTSBANK 3-way classification task across three scenarios. The results are shown in  2 . The Proposed Approach outperforms all other models across all scenarios and metrics. Performance generally decreases from unseen answer to unseen question and unseen domain. This limitation arises because the unseen question and unseen domain test sets contain questions that are not present in the training sets. As a result, the proposed approach cannot provide relevant examples for the RAG GLM. Specifically, by using Claude 3.5 Sonnet, our approach increases  9.04 % percent 9.04 9.04\\% 9.04 % ,  29.44 % percent 29.44 29.44\\% 29.44 % , and  22.08 % percent 22.08 22.08\\% 22.08 %  on macro F1 score to the best previous results on unseen answer, unseen question, and unseen domain, respectively."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Main Results SCIENTSBANK 2-way.",
        "table": "S4.T3.1",
        "footnotes": [],
        "references": [
            "This paper is organized as follows: We introduce background about datasets, information retrieval, generative language models, retrieval augmented generation, and prompt optimization in Section  2 . We describe the particular way in which we fine-tune our IR pipeline, our implementation of RAG, and the manner in which we optimize prompts in Section  3 . We present our experiments, which includes metrics used, and the relevant results in Section  4 . Lastly, we conclude with a discussion of future research directions in Section  5 .",
            "As shown in  3 , the proposed approach also outperforms other results for the SCIENTSBANK 2-way classification task. As the 2-way task is easier than the 3-way task, our approachs performance increase is less. Specifically, by using Claude 3.5 Sonnet, our approach increases  3.69 % percent 3.69 3.69\\% 3.69 % ,  7.09 % percent 7.09 7.09\\% 7.09 % , and  6.09 % percent 6.09 6.09\\% 6.09 %  on macro F1 score to the best previous results on unseen answer, unseen question, and unseen domain, respectively."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Main Results BEETLE 5-way",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": [
            "This paper is organized as follows: We introduce background about datasets, information retrieval, generative language models, retrieval augmented generation, and prompt optimization in Section  2 . We describe the particular way in which we fine-tune our IR pipeline, our implementation of RAG, and the manner in which we optimize prompts in Section  3 . We present our experiments, which includes metrics used, and the relevant results in Section  4 . Lastly, we conclude with a discussion of future research directions in Section  5 .",
            "We were unable to find any reliable benchmark results for the Beetle 5-way task, so we present only our own findings in Table  4 . Its important to note that this is a 5-way classification task, where random guessing would yield only  20 % percent 20 20\\% 20 %  accuracy. In light of this, our proposed approach demonstrates a significant improvement, increasing the accuracy in unseen answer scenario by  267.9 % percent 267.9 267.9\\% 267.9 %  to reach  73.58 % percent 73.58 73.58\\% 73.58 %  when using Claude 3.5 Sonnet. It is interesting to find that Claude 3.5 Sonnet has a worse performance than Claude 3 Haiku in unseen question scenario."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Effectiveness of Input Fields. All results are from Claude3 Haiku.",
        "table": "S4.T5.1",
        "footnotes": [],
        "references": [
            "This paper is organized as follows: We introduce background about datasets, information retrieval, generative language models, retrieval augmented generation, and prompt optimization in Section  2 . We describe the particular way in which we fine-tune our IR pipeline, our implementation of RAG, and the manner in which we optimize prompts in Section  3 . We present our experiments, which includes metrics used, and the relevant results in Section  4 . Lastly, we conclude with a discussion of future research directions in Section  5 .",
            "Table  5  shows the results of an ablation study examining the effectiveness of different input field combinations for the SCIENTSBANK 3-way task across all three scenarios. The fields Answer, Reference, and Question refer to the students answer, reference answer, and question, respectively. The proposed approach uses all three input fields and performs best across all scenarios and metrics. In addition, the improvement is most pronounced in the unseen question and unseen domain scenarios. In detail, our approach increases  7.73 % percent 7.73 7.73\\% 7.73 % ,  173.39 % percent 173.39 173.39\\% 173.39 % , and  107.24 % percent 107.24 107.24\\% 107.24 %  on macro F1 score on unseen answer, unseen question, and unseen domain, respectively."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Effectiveness of Fine-tuned IR. All results are from Claude3 Haiku.",
        "table": "S4.T6.1",
        "footnotes": [],
        "references": [
            "The second ablation study investigates the efficacy of various IR pipelines. The results are presented in Table  6 . \"Pretrained IR\" refers to the use of a pre-trained Sentence Transformer model, specifically all-MiniLM-L6-v2, without any fine-tuning for the IR pipeline. \"Global IR\" denotes the IR pipeline trained using a global training strategy, while \"Question-specific IR\" indicates an IR pipeline trained using a question-specific approach. The proposed approach used the question-specific training strategy with general labeling."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Effectiveness of IR Loss Function. All results are from Claude3 Haiku.",
        "table": "S4.T7.1",
        "footnotes": [],
        "references": [
            "Table  7  presents the results of an ablation study comparing the performance of different loss functions when fine-tuning the IR pipeline. The findings indicate that the Cosine Sentence Loss demonstrated superior performance compared to the other two loss functions. Specifically, it improved the macro-F1 score by  5.92 % percent 5.92 5.92\\% 5.92 %  and  2.74 % percent 2.74 2.74\\% 2.74 %  relative to the triplet loss and cosine similarity, respectively."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Effectiveness of Prompts. All results are from Claude3 Haiku.",
        "table": "S4.T8.1",
        "footnotes": [],
        "references": [
            "Table  8  presents the results of an ablation study comparing the performance of different prompts. In particular, we compare DSPy-style prompt templates with Claude Prompt Generator-generated prompt templates. Using the Claude Prompt Generator-generated prompts, the macro-F1 score is increased by  13.14 % percent 13.14 13.14\\% 13.14 % ,  3.15 % percent 3.15 3.15\\% 3.15 % , and  10.87 % percent 10.87 10.87\\% 10.87 %  on unseen answer, unseen question, and unseen domain compared to DSPy-style prompts respectively."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Effectiveness of RAG. All results are from Claude3 Haiku.",
        "table": "S4.T9.1",
        "footnotes": [],
        "references": [
            "In previous experiments, all models exhibited lower F1 scores on the unseen question and unseen domain scenarios due to domain shift. To address this, we designed an experiment to explore how RAG improves autoscoring performance. The results are presented in Table  9 ."
        ]
    },
    "global_footnotes": [
        "The model checkpoint is hosted at",
        ". The model size is 80 MB."
    ]
}