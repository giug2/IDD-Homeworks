{
    "id_table_1": {
        "caption": "Table 1:  Characterizing prior and proposed works using GFI Framework.",
        "table": "A7.EGx1",
        "footnotes": [],
        "references": [
            "where  x x x italic_x ,  z z z italic_z , and  t t t italic_t  represent the horizontal direction, depth, and time, respectively, and   2 superscript  2 \\nabla^{2}  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  is the Laplacian operator. Using Equation  1 , the  forward problem  in subsurface imaging is to compute seismic waveforms  p p p italic_p  given velocity maps  v v v italic_v  by learning the forward mapping  f v  p : V  P : subscript f  v p  V P f_{v\\rightarrow p}:\\mathcal{V}\\rightarrow\\mathcal{P} italic_f start_POSTSUBSCRIPT italic_v  italic_p end_POSTSUBSCRIPT : caligraphic_V  caligraphic_P . The  inverse problem , also referred to as Full Waveform Inversion (FWI), is then to learn the reverse mapping  f p  v : P  V : subscript f  p v  P V f_{p\\rightarrow v}:\\mathcal{P}\\rightarrow\\mathcal{V} italic_f start_POSTSUBSCRIPT italic_p  italic_v end_POSTSUBSCRIPT : caligraphic_P  caligraphic_V  for inferring velocity maps  v v v italic_v  given observations of  p p p italic_p .",
            "While solving the forward problem requires expensive numerical solutions  (Tago et al.,  2012 )  of Equation  1 , learning the inverse is even more computationally demanding as it involves iterating over velocity maps  v v v italic_v  to minimize the difference between simulated waveforms from the forward model and ground-truth observations of  p p p italic_p   (Tarantola,  1988 ) . As a result, traditional techniques for solving forward and inverse problems in subsurface imaging are difficult to scale in operational settings at the desired resolutions. In response, there is a growing interest in the geophysics community to leverage deep learning for subsurface imaging (DL4SI), in particular, using image-to-image-translation methods to map seismic waveforms to velocity maps and back directly from data.",
            "To answer these questions, we propose a unified framework to systematically characterize prior research in DL4SI termed the  Generalized Forward-Inverse  (GFI) framework (see Figure  1 ). The GFI framework builds on two key philosophies that are at the core of prior research in DL4SI. First, we assume that both velocity  v v v italic_v  and seismic waveforms  p p p italic_p  can be projected to latent space manifolds  v ~ ~ v \\tilde{v} over~ start_ARG italic_v end_ARG  and  p ~ ~ p \\tilde{p} over~ start_ARG italic_p end_ARG  using encoder-decoder pairs that allow reconstruction of the original domains, referred to as the  manifold assumption . Second, we assume that it is possible to learn bidirectional translations between the two manifolds  v ~ ~ v \\tilde{v} over~ start_ARG italic_v end_ARG  and  p ~ ~ p \\tilde{p} over~ start_ARG italic_p end_ARG , referred to as the  latent space translation assumption . We show that GFI encompasses previous works in DL4SI including InversionNet, VelocityGAN, and Auto-Linear, which can be viewed as specific instantiations of GFI. This unifying perspective helps us analyze the field of DL4SI going beyond specific architecture choices and also discover novel formulations in DL4SI such as those explored in this paper.",
            "As shown schematically in Figure  1 , our proposed framework of GFI unifies the learning of the forward problem  f v  p : V  P : subscript f  v p  V P f_{v\\rightarrow p}:\\mathcal{V}\\rightarrow\\mathcal{P} italic_f start_POSTSUBSCRIPT italic_v  italic_p end_POSTSUBSCRIPT : caligraphic_V  caligraphic_P  and the inverse problem  f p  v : P  V : subscript f  p v  P V f_{p\\rightarrow v}:\\mathcal{P}\\rightarrow\\mathcal{V} italic_f start_POSTSUBSCRIPT italic_p  italic_v end_POSTSUBSCRIPT : caligraphic_P  caligraphic_V  using a common architecture involving latent space translations.",
            "Table  1  summarizes how previous works in DL4SI can be viewed as specific realizations of the unifying GFI Framework, as described in the following. Schematic illustrations of all prior works in the framework of GFI are provided in the Appendix Figure  9 .",
            "To investigate the effect of using frozen latent spaces on translation performance, we consider a two-stage Reconstruct then Translate variant of Latent U-Net where encoder-decoder pairs are first trained for reconstruction followed by translation in Figure  6  for varying OpenFWI families as we reduce the training fraction from 100% to 5%. We can see that for complex families such as CVB and CFB, the MAE of Reconstruct then Translate variant is higher for lower training fractions, suggesting that it is useful to influence the training of the latent spaces based on the translation objectives. We see a similar trend for Latent U-Net (Small) shown in Appendix Figure  21 .",
            "As discussed in the quantitative comparisons of Latent U-Net (Large) and Invertible X-Net on the forward problem in Figure  3(b) , we see a surprising trend that Invertible X-Net generally outperforms Latent U-Net (Large) despite having smaller number of parameters. To understand this trend, we analyzed the training behavior of Invertible X-Net to learn that X-Net first learns to solve the inverse problem in the initial epochs of training, followed by a gradual shift in learning the solution to the forward problem in later epochs (see Appendix Figures  11  and  12  for visualizations). Since the model optimizes the combined loss on both velocity and waveforms together, as soon as the model is able to achieve a good solution for velocity, the gradients from the combined loss start helping the model to achieve better forward solutions. This also suggests that Invertible X-Net is seemingly learning the connection between forward and inverse problems for effective bidirectional training, and therefore is able to outperform Latent U-Net (Large) despite lower number of model parameters. To verify this, we compared the performance of Latent U-Net (Large), Invertible X-Net and Invertible X-Net trained only in the forward direction (Appendix Figure  10 ). We observed that indeed Invertible X-Net with forward-only mode shows worst performance followed by Latent U-net (Large), while Invertible X-Net (joint training) shows best performance.",
            "Extrapolating across OpenFWI Datasets:  To assess the out-of-distribution generalizability of best-performing models on unseen datasets, in Figure  7 , we evaluate zero-shot performance of models trained across all datasets (shown as rows) when tested across all datasets (shown as columns), for both forward and inverse problems. Here, the color intensity indicates difference in SSIM performance of two competing methods. We can see that Invertible X-Net outperforms other baselines for both forward and inverse problems across majority of train-test cases. The only exception is when the models are trained on FVB, a relatively simpler dataset than other data families such as CurveVel and FlatVel. From a geophysics perspective, we expect to have better generalizability for models trained on complex geological settings and tested on relatively simpler ones, as complex geological structures can be thought of as juxtapositions of simpler geological structures. Additionally, we compare the zero-shot generalization for all baselines and Latent U-Net models in detail in the Appendix (Figures  13  -  16 ).",
            "Additionally, we also show zero shot generalization of our models on the Marmousi and Overthrust dataset in Tables  8  and  10 . Our model Invertible X-Net shows generalizability in SSIM indicating that overall prediction has better geological understanding than other baseline models.",
            "In this section, we focus on the training of the Invertible X-Net model using a combined loss function (incorporating both the forward and inverse problems), as opposed to training it solely with a forward loss function. Figure  10  shows that when model is trained using only forward loss, then its performance falls short compared to the Latent U-Net (Large) model. This discrepancy can be attributed to the fact that the Latent U-Net has a higher model complexity than Invertible X-Net, despite their similar sizes. Nonetheless, when the Invertible X-Net model is trained with combined loss (forward and inverse), the model is able to outperform Latent U-Net model with a good margin. This highlights the value of joint training, demonstrating that simultaneously learning both the forward and inverse problems can lead to better results than learning the two models separately.",
            "In Figures  11  and  12 , we illustrate asymmetry in learning the translation for the forward and inverse problems using CVB and CFA datasets. From the figures, we observe that the model learns the inverse mapping in the initial epochs and gradually starts to learn the solution to the forward problem in later epochs. Since the network optimizes the combined loss function on both velocity and waveform together, the gradients from the combined loss help the model to achieve better forward solution. This corroborates with our hypothesis that the model trained on combined loss is able to learn the connection between forward and inverse problem.",
            "Figure  13  and  14  show generalization performance of Invertible X-Net and Latent U-Net (Large) models respectively on the inverse problem using MAE, MSE, and SSIM metrics. As described in the main paper, we show models trained across all dataset as rows and evaluated across all test datasets as columns. For comparison, we evaluate metrics such as MAE, MSE, and SSIM and calculate its difference between our models and other baseline models. For MAE and MSE, when the color intensity is blue, our model show better generalizability and vice-versa whereas, for SSIM, when the color intensity is red indicates better generalization of our model and vice-versa.",
            "From Figure  13 , we observe that the Invertible X-Net shows superior generalization over the baseline models (AutoLinear, InversionNet, and VelocityGAN) across all metrics except on the FVB dataset. Figure  13  (d) compare Invertible X-Net with Latent U-Net (Large) model where we see that Invertible X-Net shows better generalization on complex datasets such as CFB, STA, and STB datasets, while Latent U-Net is better on relatively simpler datasets such as CVA, CFA, and more. Similarly, Figure  14  shows the comparison of Latent U-Net (Large) model with other baseline models. As expected, we observe that our model is able to generalize much better than all the baseline models.",
            "Figures  15  and  16  compares the performance of our models Invertible X-Net and Latent U-Net (Large) against baselines. In Figure  15 , we observe strong generalization of Invertible X-Net over all baselines - AutoLinear, FNO, and WaveformNet. Figure  15  (d) shows the comparative performance of Invertible X-Net against Latent U-Net (Large) where we see that Invertible X-Net dominates overall across all metrics. In Figure  16 , we compare the Latent U-Net (Large) model against same baselines as above. The figure indicates Latent U-Net (Large) model has much stronger generalizability than baseline models consistently.",
            "Figure  17(a)  (a) shows how the performance of Latent U-Net model is affected with change in the latent space size. As the latent space size is reduced, the MAE and MSE metrics for mapping seismic waveform to velocity is increasing, while the SSIM is decreasing. The impact is more pronounced on complex datasets such as B group of datasets. These datasets represent geologically complex datasets and therefore may require larger latent space to encode geological heterogeneity. In Figure  17(b) , we compare the large Latent U-Net with the small Latent U-Net model, while latent space size is also reduced. We observe that the performance gap between the two models reduces as the latent space size is also reduced. This underscore the importance of latent space size for encoding the geological features for an effective translation.",
            "Further, we provide visualizations of the first two primary PCA and t-SNE components of the velocity latent space in Figure  18  and  19  respectively. We take the encoder trained on a dataset and get the latent space encoding on several datasets. This visualization shows the in-distribution and out-of-distribution generality of our encoders and highlights the importance of the manifold for latent space translation. Overall, we observe that the larger latent space  70  70 70 70 70\\times 70 70  70  have better structure than  8  8 8 8 8\\times 8 8  8 , indicating that the ideal size of latent space should be decided based on the complexity of dataset and problem being solved.",
            "Figure  21  for Latent U-Net Small further elaborates on our finding that direct translation learning consistently outperforms the two-stage approach, where translation follows reconstruction."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Statistics on the number of samples, the size of the velocity and waveforms for each dataset in OpenFWI  Deng et al. ( 2022 ) .",
        "table": "A7.EGx2",
        "footnotes": [
            ""
        ],
        "references": [
            "In particular, we propose two new architectures within the framework of GFI:  Latent U-Net  (see Figure  2(a) ) and  Invertible X-Net  (see Figure  2(b) ). Latent U-Net uses two separate U-Nets  (Ronneberger et al.,  2015 )  to model latent space translations in both directions. This takes inspiration from the success of U-Nets in mainstream computer vision applications to explore their benefits in DL4SI. Invertible-XNet uses Invertible U-Net (IU-Net)  (Etmann et al.,  2020 )  to simultaneously learn both forward and inverse translations in the latent space using a single architecture. Invertible U-Nets builds on the idea of Invertible Neural Networks (INNs)  (Ardizzone et al.,  2018 )  to learn bijective mappings between input and output domains in DL4SI. We show that both Latent U-Net and Invertible X-Net achieve state-of-the-art (SOTA) performance for forward and inverse problems on a wide range of benchmark datasets commonly used in DL4SI. We show that jointly solving forward and inverse problems using Invertible X-Net especially helps in improving the accuracy on the forward problem. We also investigate the zero-shot effectiveness of our proposed approaches on synthetic and real-world-like datasets, contributing to novel insights in the field of DL4SI.",
            "We propose a novel architecture for solving forward and inverse problems using two latent space translation models implemented using U-Nets, termed  Latent U-Net . As shown in Figure  2(a) , Latent U-Net uses ConvNet backbones for both encoder-decoder pairs: ( E v subscript E v \\mathtt{E}_{v} typewriter_E start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ,  D v subscript D v \\mathtt{D}_{v} typewriter_D start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) and ( E p subscript E p \\mathtt{E}_{p} typewriter_E start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ,  D p subscript D p \\mathtt{D}_{p} typewriter_D start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ), to project  v v v italic_v  and  p p p italic_p  to lower-dimensional representations. We also constrain the sizes of the latent spaces of  v ~ ~ v \\tilde{v} over~ start_ARG italic_v end_ARG  and  p ~ ~ p \\tilde{p} over~ start_ARG italic_p end_ARG  to be identical, i.e.,  dim  ( v ~ ) = dim  ( p ~ ) dim ~ v dim ~ p \\text{dim}(\\tilde{v})=\\text{dim}(\\tilde{p}) dim ( over~ start_ARG italic_v end_ARG ) = dim ( over~ start_ARG italic_p end_ARG ) , so that we can train two separate U-Net models to implement the latent space mappings  L v ~  p ~ subscript L  ~ v ~ p L_{\\tilde{v}\\rightarrow\\tilde{p}} italic_L start_POSTSUBSCRIPT over~ start_ARG italic_v end_ARG  over~ start_ARG italic_p end_ARG end_POSTSUBSCRIPT  and  L p ~  v ~ subscript L  ~ p ~ v L_{\\tilde{p}\\rightarrow\\tilde{v}} italic_L start_POSTSUBSCRIPT over~ start_ARG italic_p end_ARG  over~ start_ARG italic_v end_ARG end_POSTSUBSCRIPT . Note that while U-Nets cannot be applied directly in the original spaces of velocity and waveforms due to a mismatch in dimensionalities, Latent U-Net allows us to bring to bear the power of U-Net in the domain of subsurface imaging, which has been very successful in several applications of computer vision.",
            "To investigate the effect of using frozen latent spaces on translation performance, we consider a two-stage Reconstruct then Translate variant of Latent U-Net where encoder-decoder pairs are first trained for reconstruction followed by translation in Figure  6  for varying OpenFWI families as we reduce the training fraction from 100% to 5%. We can see that for complex families such as CVB and CFB, the MAE of Reconstruct then Translate variant is higher for lower training fractions, suggesting that it is useful to influence the training of the latent spaces based on the translation objectives. We see a similar trend for Latent U-Net (Small) shown in Appendix Figure  21 .",
            "As discussed in the quantitative comparisons of Latent U-Net (Large) and Invertible X-Net on the forward problem in Figure  3(b) , we see a surprising trend that Invertible X-Net generally outperforms Latent U-Net (Large) despite having smaller number of parameters. To understand this trend, we analyzed the training behavior of Invertible X-Net to learn that X-Net first learns to solve the inverse problem in the initial epochs of training, followed by a gradual shift in learning the solution to the forward problem in later epochs (see Appendix Figures  11  and  12  for visualizations). Since the model optimizes the combined loss on both velocity and waveforms together, as soon as the model is able to achieve a good solution for velocity, the gradients from the combined loss start helping the model to achieve better forward solutions. This also suggests that Invertible X-Net is seemingly learning the connection between forward and inverse problems for effective bidirectional training, and therefore is able to outperform Latent U-Net (Large) despite lower number of model parameters. To verify this, we compared the performance of Latent U-Net (Large), Invertible X-Net and Invertible X-Net trained only in the forward direction (Appendix Figure  10 ). We observed that indeed Invertible X-Net with forward-only mode shows worst performance followed by Latent U-net (Large), while Invertible X-Net (joint training) shows best performance.",
            "The OpenFWI comprises multi-structural benchmark datasets of significant size that can be used for solving full waveform inversion (FWI) using machine learning techniques  (Deng et al.,  2022 ) . In particular, the repository contains 3 major groups of data: (1) Vel Family, (2) Fault Family, and (3) Style Family. These groups represent simple to complex sub-surface geological settings with seismic velocity and waveforms information. The Vel family is the simplest geological patterns including four datasets - (1) FlatVel-A (FVA), (2) FlatVel-B (FVB), (3) CurveVel-A (CVA), and (4) CurveVel-B (CVB). The difference between FlatVel and CurveVel is that the former represents low-energy geological environments where the rock layers are deposited horizontally and the latter consists of curved layers which are formed due to structural deformation of flat layers. The Fault family also has four datasets - (1) FlatFault-A (FFA), (2) FlatFault-B (FFB), (3) CurveFault-A (CFA), and (4) CurveFault-B (CFB). Unlike Vel datasets, the Fault family contains fault-like deformations, which is fracturing of rocks under certain pressure conditions. Due to the presence of faults, the Fault family becomes more complicated and challenging to model. The Style family has two datasets - (1) Style-A (STA), and (2) Style-B (STB). This dataset is generated using the style transfer method where the COCO dataset  (Lin et al.,  2014 )  is set as the content images and the Marmousi dataset is set as the style image. This is the most complex OpenFWI dataset as it represents highly complex geological settings where the velocity is changing rapidly and abruptly. In summary, the details about the Vel and Fault datasets are described in table  2 .",
            "For training Invertible X-Net (Cycle) model, we use loss function shown in Equation  23 .",
            "In Figures  11  and  12 , we illustrate asymmetry in learning the translation for the forward and inverse problems using CVB and CFA datasets. From the figures, we observe that the model learns the inverse mapping in the initial epochs and gradually starts to learn the solution to the forward problem in later epochs. Since the network optimizes the combined loss function on both velocity and waveform together, the gradients from the combined loss help the model to achieve better forward solution. This corroborates with our hypothesis that the model trained on combined loss is able to learn the connection between forward and inverse problem.",
            "In this section, we study the impact of skip connections of Latent U-Net model for the inverse problem. Figure  20  shows how the MAE and MSE are increasing and SSIM is decreasing when the latent space size is decreasing from  70  70 70 70 70\\times 70 70  70  to  8  8 8 8 8\\times 8 8  8 . We observe that the impact of skip connections is more pronounced at smaller latent space size as opposed to larger space.",
            "Figure  21  for Latent U-Net Small further elaborates on our finding that direct translation learning consistently outperforms the two-stage approach, where translation follows reconstruction."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Architecture Details of Seismic Waveform and Velocity Encoder-Decoder models.",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "Quantitative Comparisons:  Figure  3(a)  compares MAE on velocity predictions while solving the inverse problem across different OpenFWI datasets. We can see that our proposed models, which capture complex non-linear relationships in the latent space using U-Net and IU-Net architectures, consistently outperform baseline methods InversionNet, VelocityGAN, & Auto-Linear across majority of datasets. Overall, the large Latent U-Net model shows the best performance in inverse modeling followed by the small Latent U-Net model. On complex datasets such as Style family and CFB, Invertible X-Net with and without cycle loss are also comparable to the small Latent U-Net model. In most cases, the Invertible X-Net model trained with the cycle loss performs at par with the Invertible X-Net trained with supervised loss except in a few cases like CFA and FFA. In addition to MAE, we also evaluate baselines using other metrics reported in Table  7  in the Appendix.",
            "Figure  3(b)  compares the performance of baseline methods on the forward problem (see Appendix Table  9  for other evaluation metrics). We can see that both Latent U-Net (Large) and Invertible X-Net show better performance than most baselines across all datasets. However, in contrast to the trends observed in the inverse problem, we observe that Invertible X-Net with and without cycle loss shows the best performance compared to Latent U-Net models. We further notice that for simple datasets such as FVA and FFA, FNO has significantly lower MAE compared to other models. We later qualitatively explain this pattern by showing that FNO tends to capture dominant modes of the waveform distribution while neglecting subtler reflection patterns.",
            "As discussed in the quantitative comparisons of Latent U-Net (Large) and Invertible X-Net on the forward problem in Figure  3(b) , we see a surprising trend that Invertible X-Net generally outperforms Latent U-Net (Large) despite having smaller number of parameters. To understand this trend, we analyzed the training behavior of Invertible X-Net to learn that X-Net first learns to solve the inverse problem in the initial epochs of training, followed by a gradual shift in learning the solution to the forward problem in later epochs (see Appendix Figures  11  and  12  for visualizations). Since the model optimizes the combined loss on both velocity and waveforms together, as soon as the model is able to achieve a good solution for velocity, the gradients from the combined loss start helping the model to achieve better forward solutions. This also suggests that Invertible X-Net is seemingly learning the connection between forward and inverse problems for effective bidirectional training, and therefore is able to outperform Latent U-Net (Large) despite lower number of model parameters. To verify this, we compared the performance of Latent U-Net (Large), Invertible X-Net and Invertible X-Net trained only in the forward direction (Appendix Figure  10 ). We observed that indeed Invertible X-Net with forward-only mode shows worst performance followed by Latent U-net (Large), while Invertible X-Net (joint training) shows best performance.",
            "Extrapolating across OpenFWI Datasets:  To assess the out-of-distribution generalizability of best-performing models on unseen datasets, in Figure  7 , we evaluate zero-shot performance of models trained across all datasets (shown as rows) when tested across all datasets (shown as columns), for both forward and inverse problems. Here, the color intensity indicates difference in SSIM performance of two competing methods. We can see that Invertible X-Net outperforms other baselines for both forward and inverse problems across majority of train-test cases. The only exception is when the models are trained on FVB, a relatively simpler dataset than other data families such as CurveVel and FlatVel. From a geophysics perspective, we expect to have better generalizability for models trained on complex geological settings and tested on relatively simpler ones, as complex geological structures can be thought of as juxtapositions of simpler geological structures. Additionally, we compare the zero-shot generalization for all baselines and Latent U-Net models in detail in the Appendix (Figures  13  -  16 ).",
            "For training Invertible X-Net (Cycle) model, we use loss function shown in Equation  23 .",
            "We provide more details related to model architecture, layers, and number of parameters related to seismic and velocity encoder-decoder architecture in Table  3 , and regarding the latent models used for translation in Table  4 . Further, we compare our model parameters with baseline models in Table  6 .",
            "Figure  13  and  14  show generalization performance of Invertible X-Net and Latent U-Net (Large) models respectively on the inverse problem using MAE, MSE, and SSIM metrics. As described in the main paper, we show models trained across all dataset as rows and evaluated across all test datasets as columns. For comparison, we evaluate metrics such as MAE, MSE, and SSIM and calculate its difference between our models and other baseline models. For MAE and MSE, when the color intensity is blue, our model show better generalizability and vice-versa whereas, for SSIM, when the color intensity is red indicates better generalization of our model and vice-versa.",
            "From Figure  13 , we observe that the Invertible X-Net shows superior generalization over the baseline models (AutoLinear, InversionNet, and VelocityGAN) across all metrics except on the FVB dataset. Figure  13  (d) compare Invertible X-Net with Latent U-Net (Large) model where we see that Invertible X-Net shows better generalization on complex datasets such as CFB, STA, and STB datasets, while Latent U-Net is better on relatively simpler datasets such as CVA, CFA, and more. Similarly, Figure  14  shows the comparison of Latent U-Net (Large) model with other baseline models. As expected, we observe that our model is able to generalize much better than all the baseline models."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Architecture details of Latent U-Net and IU-Net latent space translation models",
        "table": "S3.T1.1.1.1.3.1",
        "footnotes": [],
        "references": [
            "We train Latent U-Net in an end-to-end manner to directly learn latent spaces useful for translation. This is in contrast to the two-stage training process employed in Auto-Linear where latent spaces are first pre-trained for reconstruction, followed by translation. Specifically, for the forward problem, the components  D p subscript D p \\mathtt{D}_{p} typewriter_D start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ,  L v ~  p ~ subscript L  ~ v ~ p L_{\\tilde{v}\\rightarrow\\tilde{p}} italic_L start_POSTSUBSCRIPT over~ start_ARG italic_v end_ARG  over~ start_ARG italic_p end_ARG end_POSTSUBSCRIPT , and  E v subscript E v \\mathtt{E}_{v} typewriter_E start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT  are trained together by optimizing Equation  4 , while for the inverse problem, the components  D v subscript D v \\mathtt{D}_{v} typewriter_D start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ,  L p ~  v ~ subscript L  ~ p ~ v L_{\\tilde{p}\\rightarrow\\tilde{v}} italic_L start_POSTSUBSCRIPT over~ start_ARG italic_p end_ARG  over~ start_ARG italic_v end_ARG end_POSTSUBSCRIPT , and  E p subscript E p \\mathtt{E}_{p} typewriter_E start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  are trained together by optimizing Equation  5 . Since the training of forward and inverse modeling components of Latent U-Net are disjoint from each other, Latent U-Net does not explicitly learns manifolds useful for reconstruction. However, it is possible to modify the training objective of Latent U-Net by adding reconstruction losses in both forward and inverse problems to perform manifold learning.",
            "Qualitative Comparisons:  In Figure  4(a) , we compare inverse model predictions on three datasets, CVB, CFB, and STA, choosing one from each family of OpenFWI. We observe that our proposed methods (Latent U-Net and Invertible X-Net) produce considerably better velocity maps than the two SOTA baselines, InversionNet and Auto-Linear. We chose these 3 datasets as they are complex and have high heterogeneity as the velocity rapidly changes in all directions. For heterogeneous velocity maps, the predictions of baseline methods are smooth and they fail to delineate sharp changes in velocities. On the other hand, Latent U-Net and Invertible X-Net are able to predict better velocity fields capturing rapid variations in both shallow and deeper parts (highlighted using black-boxes in the figure). Between Latent U-Net (Large) and Invertible X-Net, we can see that the performance of Latent U-Net is slightly better especially on CFB and STA datasets. Note that Invertible X-Net uses a smaller number of parameters (24M) compared to Latent U-Net (Large) (33M). We provide additional visualizations of more baseline methods and across other datasets in the Appendix.",
            "Figure  4(b)  shows visualizations of model predictions for the forward problem on three datasets. Since these datasets are heterogeneous, the recorded seismic waveforms exhibit convoluted interference patterns. In particular, we can see that in all ground truth visualizations, there is a direct arrival in the shallower layers represented as a slanted line signature. While capturing the direct arrival is relatively simple, the primary challenge in solving forward problems is to capture the subtler reflections in the deeper layers, where even small magnitudes of errors can result in large differences in inferred velocity maps. We can see that while all baseline methods are able to predict shallow waveforms easily (like direct arrivals), methods such as FNO and Auto-Linear are struggling to predict the seismic waveforms, especially in the deeper regions (highlighted using black-boxes in the Figure). On the other hand, our proposed models consistently improve results over baselines across all datasets. Additional visualizations for the forward problem are provided in the Appendix.",
            "We provide more details related to model architecture, layers, and number of parameters related to seismic and velocity encoder-decoder architecture in Table  3 , and regarding the latent models used for translation in Table  4 . Further, we compare our model parameters with baseline models in Table  6 .",
            "Figure  13  and  14  show generalization performance of Invertible X-Net and Latent U-Net (Large) models respectively on the inverse problem using MAE, MSE, and SSIM metrics. As described in the main paper, we show models trained across all dataset as rows and evaluated across all test datasets as columns. For comparison, we evaluate metrics such as MAE, MSE, and SSIM and calculate its difference between our models and other baseline models. For MAE and MSE, when the color intensity is blue, our model show better generalizability and vice-versa whereas, for SSIM, when the color intensity is red indicates better generalization of our model and vice-versa.",
            "From Figure  13 , we observe that the Invertible X-Net shows superior generalization over the baseline models (AutoLinear, InversionNet, and VelocityGAN) across all metrics except on the FVB dataset. Figure  13  (d) compare Invertible X-Net with Latent U-Net (Large) model where we see that Invertible X-Net shows better generalization on complex datasets such as CFB, STA, and STB datasets, while Latent U-Net is better on relatively simpler datasets such as CVA, CFA, and more. Similarly, Figure  14  shows the comparison of Latent U-Net (Large) model with other baseline models. As expected, we observe that our model is able to generalize much better than all the baseline models."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Hyperparameter details for training Latent U-Net and Invertible X-Net models.",
        "table": "S3.T1.1.1.1.4.1",
        "footnotes": [],
        "references": [
            "We train Latent U-Net in an end-to-end manner to directly learn latent spaces useful for translation. This is in contrast to the two-stage training process employed in Auto-Linear where latent spaces are first pre-trained for reconstruction, followed by translation. Specifically, for the forward problem, the components  D p subscript D p \\mathtt{D}_{p} typewriter_D start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ,  L v ~  p ~ subscript L  ~ v ~ p L_{\\tilde{v}\\rightarrow\\tilde{p}} italic_L start_POSTSUBSCRIPT over~ start_ARG italic_v end_ARG  over~ start_ARG italic_p end_ARG end_POSTSUBSCRIPT , and  E v subscript E v \\mathtt{E}_{v} typewriter_E start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT  are trained together by optimizing Equation  4 , while for the inverse problem, the components  D v subscript D v \\mathtt{D}_{v} typewriter_D start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ,  L p ~  v ~ subscript L  ~ p ~ v L_{\\tilde{p}\\rightarrow\\tilde{v}} italic_L start_POSTSUBSCRIPT over~ start_ARG italic_p end_ARG  over~ start_ARG italic_v end_ARG end_POSTSUBSCRIPT , and  E p subscript E p \\mathtt{E}_{p} typewriter_E start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  are trained together by optimizing Equation  5 . Since the training of forward and inverse modeling components of Latent U-Net are disjoint from each other, Latent U-Net does not explicitly learns manifolds useful for reconstruction. However, it is possible to modify the training objective of Latent U-Net by adding reconstruction losses in both forward and inverse problems to perform manifold learning.",
            "In Figure  5  (a), we investigate the impact of varying latent space sizes on the quality of domain translations using the Latent U-Net (Large) model on the inverse problem. We compare MAE by reducing latent space dimensions from  70  70 70 70 70\\times 70 70  70  to  8  8 8 8 8\\times 8 8  8 . We can see that the quality of translations is not affected much for A-family of OpenFWI datasets, while the B-family shows monotonically increasing trend in MAE as we reduce latent dimension. Since the A-family was intentionally created to represent simpler velocity distributions, even the smallest latent space ( 8  8 8 8 8\\times 8 8  8 ) is sufficient to estimate velocity maps. However, the B-family of datasets represent more complex velocity distributions, requiring a sufficiently larger latent space for the translation. This indicates that the ideal size of the latent space should be decided based on the complexity of problem being solved.",
            "We further study the role of model size on tranlslation performance by comparing Latent U-Net (Small) and Latent U-Net (Large) with varying latent dimensions in Figure  5  (b). We find that for a given latent dimension, Latent U-Net (Large) outperforms Latent U-Net (Small), motivating architectures with sufficient complexity for domain translations. We further study the impact of using skip connections in the latent space translation models on varying latent space dimensions in Figure  5  (c).. We can see that adding skip connections shows marginal improvements in performance for larger latent dimensions. However, when the number of dimensions is low (less than 20), we can see a sharp degradation in performance by removing skip-connections. This indicates that smaller latent representations are weaker and thus insufficient for domain translation in U-Net frameworks without being augmented with skip connections.",
            "For training, we normalize the velocity using min-max normalization and seismic waveform using standard normalization to rescale the data to mean 0 and standard deviation as 1. Table  5  shows other hyperparameter details for training Latent U-Net and Invertible X-Net models on OpenFWI datasets.",
            "Figures  15  and  16  compares the performance of our models Invertible X-Net and Latent U-Net (Large) against baselines. In Figure  15 , we observe strong generalization of Invertible X-Net over all baselines - AutoLinear, FNO, and WaveformNet. Figure  15  (d) shows the comparative performance of Invertible X-Net against Latent U-Net (Large) where we see that Invertible X-Net dominates overall across all metrics. In Figure  16 , we compare the Latent U-Net (Large) model against same baselines as above. The figure indicates Latent U-Net (Large) model has much stronger generalizability than baseline models consistently."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Comparison of encoder, decoder, and latent model parameters for our model (Latent U-Net and Invertible X-Net) with other baseline models.",
        "table": "S3.T1.1.1.1.5.1",
        "footnotes": [],
        "references": [
            "To investigate the effect of using frozen latent spaces on translation performance, we consider a two-stage Reconstruct then Translate variant of Latent U-Net where encoder-decoder pairs are first trained for reconstruction followed by translation in Figure  6  for varying OpenFWI families as we reduce the training fraction from 100% to 5%. We can see that for complex families such as CVB and CFB, the MAE of Reconstruct then Translate variant is higher for lower training fractions, suggesting that it is useful to influence the training of the latent spaces based on the translation objectives. We see a similar trend for Latent U-Net (Small) shown in Appendix Figure  21 .",
            "Extrapolating across OpenFWI Datasets:  To assess the out-of-distribution generalizability of best-performing models on unseen datasets, in Figure  7 , we evaluate zero-shot performance of models trained across all datasets (shown as rows) when tested across all datasets (shown as columns), for both forward and inverse problems. Here, the color intensity indicates difference in SSIM performance of two competing methods. We can see that Invertible X-Net outperforms other baselines for both forward and inverse problems across majority of train-test cases. The only exception is when the models are trained on FVB, a relatively simpler dataset than other data families such as CurveVel and FlatVel. From a geophysics perspective, we expect to have better generalizability for models trained on complex geological settings and tested on relatively simpler ones, as complex geological structures can be thought of as juxtapositions of simpler geological structures. Additionally, we compare the zero-shot generalization for all baselines and Latent U-Net models in detail in the Appendix (Figures  13  -  16 ).",
            "We provide more details related to model architecture, layers, and number of parameters related to seismic and velocity encoder-decoder architecture in Table  3 , and regarding the latent models used for translation in Table  4 . Further, we compare our model parameters with baseline models in Table  6 .",
            "Figures  15  and  16  compares the performance of our models Invertible X-Net and Latent U-Net (Large) against baselines. In Figure  15 , we observe strong generalization of Invertible X-Net over all baselines - AutoLinear, FNO, and WaveformNet. Figure  15  (d) shows the comparative performance of Invertible X-Net against Latent U-Net (Large) where we see that Invertible X-Net dominates overall across all metrics. In Figure  16 , we compare the Latent U-Net (Large) model against same baselines as above. The figure indicates Latent U-Net (Large) model has much stronger generalizability than baseline models consistently."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Comparison of our models (Latent U-Net (Large) and Invertible X-Net) with other baseline models for the inverse problem across 10 OpenFWI datasets. The bold highlights the best performing model on that dataset.",
        "table": "S3.T1.1.5.4.2.1",
        "footnotes": [],
        "references": [
            "Quantitative Comparisons:  Figure  3(a)  compares MAE on velocity predictions while solving the inverse problem across different OpenFWI datasets. We can see that our proposed models, which capture complex non-linear relationships in the latent space using U-Net and IU-Net architectures, consistently outperform baseline methods InversionNet, VelocityGAN, & Auto-Linear across majority of datasets. Overall, the large Latent U-Net model shows the best performance in inverse modeling followed by the small Latent U-Net model. On complex datasets such as Style family and CFB, Invertible X-Net with and without cycle loss are also comparable to the small Latent U-Net model. In most cases, the Invertible X-Net model trained with the cycle loss performs at par with the Invertible X-Net trained with supervised loss except in a few cases like CFA and FFA. In addition to MAE, we also evaluate baselines using other metrics reported in Table  7  in the Appendix.",
            "Extrapolating across OpenFWI Datasets:  To assess the out-of-distribution generalizability of best-performing models on unseen datasets, in Figure  7 , we evaluate zero-shot performance of models trained across all datasets (shown as rows) when tested across all datasets (shown as columns), for both forward and inverse problems. Here, the color intensity indicates difference in SSIM performance of two competing methods. We can see that Invertible X-Net outperforms other baselines for both forward and inverse problems across majority of train-test cases. The only exception is when the models are trained on FVB, a relatively simpler dataset than other data families such as CurveVel and FlatVel. From a geophysics perspective, we expect to have better generalizability for models trained on complex geological settings and tested on relatively simpler ones, as complex geological structures can be thought of as juxtapositions of simpler geological structures. Additionally, we compare the zero-shot generalization for all baselines and Latent U-Net models in detail in the Appendix (Figures  13  -  16 ).",
            "We provide more detailed comparison of our models with other baseline models in Table  7 . Our proposed models consistently outperform baseline models on multiple datasets indicating superior generalizability on in-distributions examples.",
            "Figure  17(a)  (a) shows how the performance of Latent U-Net model is affected with change in the latent space size. As the latent space size is reduced, the MAE and MSE metrics for mapping seismic waveform to velocity is increasing, while the SSIM is decreasing. The impact is more pronounced on complex datasets such as B group of datasets. These datasets represent geologically complex datasets and therefore may require larger latent space to encode geological heterogeneity. In Figure  17(b) , we compare the large Latent U-Net with the small Latent U-Net model, while latent space size is also reduced. We observe that the performance gap between the two models reduces as the latent space size is also reduced. This underscore the importance of latent space size for encoding the geological features for an effective translation."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Comparison of our models (Latent U-Net (Large) and Invertible X-Net) on real world like datasets - Marmousi, Overthrust, Marmousi Smooth, Overthrust Smooth for the Inverse Problem. The bold highlights the best performing model on that dataset.",
        "table": "S3.T1.1.5.4.5.1",
        "footnotes": [],
        "references": [
            "Extrapolating on Marmousi and Overthrust Datasets:  We show the zero-shot generalizability of our models on the more complex and real-world-like geological settings of Marmousi and Overthrust in Figure  8 . Here, we chose models trained on the most complex OpenFWI datasets, i.e., STA (for Marmousi) and STB (for overthrust) datasets, and compared their zero-shot performance in predicting velocity maps (inverse problem). We observe that none of the baselines are able to invert velocity maps at high resolution. However, the predictions of Latent U-Net and Invertible X-Net are still able to delineate the sharp changes in velocity maps especially in the shallow part of the data. On the forward problem, our models are able to accurately predict seismic waveforms for both Marmousi and Overthrust data relative to baselines.",
            "Additionally, we also show zero shot generalization of our models on the Marmousi and Overthrust dataset in Tables  8  and  10 . Our model Invertible X-Net shows generalizability in SSIM indicating that overall prediction has better geological understanding than other baseline models.",
            "Further, we provide visualizations of the first two primary PCA and t-SNE components of the velocity latent space in Figure  18  and  19  respectively. We take the encoder trained on a dataset and get the latent space encoding on several datasets. This visualization shows the in-distribution and out-of-distribution generality of our encoders and highlights the importance of the manifold for latent space translation. Overall, we observe that the larger latent space  70  70 70 70 70\\times 70 70  70  have better structure than  8  8 8 8 8\\times 8 8  8 , indicating that the ideal size of latent space should be decided based on the complexity of dataset and problem being solved."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Comparison of our models (Latent U-Net (Large) and Invertible X-Net) with other baseline models for the forward problem across 10 OpenFWI datasets. The bold highlights the best performing model on that dataset.",
        "table": "S3.T1.1.6.5.1.1",
        "footnotes": [],
        "references": [
            "Table  1  summarizes how previous works in DL4SI can be viewed as specific realizations of the unifying GFI Framework, as described in the following. Schematic illustrations of all prior works in the framework of GFI are provided in the Appendix Figure  9 .",
            "Figure  3(b)  compares the performance of baseline methods on the forward problem (see Appendix Table  9  for other evaluation metrics). We can see that both Latent U-Net (Large) and Invertible X-Net show better performance than most baselines across all datasets. However, in contrast to the trends observed in the inverse problem, we observe that Invertible X-Net with and without cycle loss shows the best performance compared to Latent U-Net models. We further notice that for simple datasets such as FVA and FFA, FNO has significantly lower MAE compared to other models. We later qualitatively explain this pattern by showing that FNO tends to capture dominant modes of the waveform distribution while neglecting subtler reflection patterns.",
            "Figure  9  provides additional schematic illustrations of prior works in DL4SI such as InversionNet, WaveformNet, and AutoLinear as special cases of our proposed GFI framework.",
            "Similar to the inverse problem, we provide detailed comparison of our models with other baseline models in Table  9 . Our proposed models consistently outperform baseline models on multiple datasets indicating superior in-distributions generalizability.",
            "Further, we provide visualizations of the first two primary PCA and t-SNE components of the velocity latent space in Figure  18  and  19  respectively. We take the encoder trained on a dataset and get the latent space encoding on several datasets. This visualization shows the in-distribution and out-of-distribution generality of our encoders and highlights the importance of the manifold for latent space translation. Overall, we observe that the larger latent space  70  70 70 70 70\\times 70 70  70  have better structure than  8  8 8 8 8\\times 8 8  8 , indicating that the ideal size of latent space should be decided based on the complexity of dataset and problem being solved."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Comparison of our models (Latent U-Net (Large) and Invertible X-Net) on real world like datasets - Marmousi, Overthrust, Marmousi Smooth, Overthrust Smooth for the Forward Problem. The bold highlights the best performing model on that dataset.",
        "table": "S3.T1.1.6.5.2.1",
        "footnotes": [],
        "references": [
            "As discussed in the quantitative comparisons of Latent U-Net (Large) and Invertible X-Net on the forward problem in Figure  3(b) , we see a surprising trend that Invertible X-Net generally outperforms Latent U-Net (Large) despite having smaller number of parameters. To understand this trend, we analyzed the training behavior of Invertible X-Net to learn that X-Net first learns to solve the inverse problem in the initial epochs of training, followed by a gradual shift in learning the solution to the forward problem in later epochs (see Appendix Figures  11  and  12  for visualizations). Since the model optimizes the combined loss on both velocity and waveforms together, as soon as the model is able to achieve a good solution for velocity, the gradients from the combined loss start helping the model to achieve better forward solutions. This also suggests that Invertible X-Net is seemingly learning the connection between forward and inverse problems for effective bidirectional training, and therefore is able to outperform Latent U-Net (Large) despite lower number of model parameters. To verify this, we compared the performance of Latent U-Net (Large), Invertible X-Net and Invertible X-Net trained only in the forward direction (Appendix Figure  10 ). We observed that indeed Invertible X-Net with forward-only mode shows worst performance followed by Latent U-net (Large), while Invertible X-Net (joint training) shows best performance.",
            "Additionally, we also show zero shot generalization of our models on the Marmousi and Overthrust dataset in Tables  8  and  10 . Our model Invertible X-Net shows generalizability in SSIM indicating that overall prediction has better geological understanding than other baseline models.",
            "In this section, we focus on the training of the Invertible X-Net model using a combined loss function (incorporating both the forward and inverse problems), as opposed to training it solely with a forward loss function. Figure  10  shows that when model is trained using only forward loss, then its performance falls short compared to the Latent U-Net (Large) model. This discrepancy can be attributed to the fact that the Latent U-Net has a higher model complexity than Invertible X-Net, despite their similar sizes. Nonetheless, when the Invertible X-Net model is trained with combined loss (forward and inverse), the model is able to outperform Latent U-Net model with a good margin. This highlights the value of joint training, demonstrating that simultaneously learning both the forward and inverse problems can lead to better results than learning the two models separately."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "S3.T1.1.6.5.5.1",
        "footnotes": [],
        "references": [
            "As discussed in the quantitative comparisons of Latent U-Net (Large) and Invertible X-Net on the forward problem in Figure  3(b) , we see a surprising trend that Invertible X-Net generally outperforms Latent U-Net (Large) despite having smaller number of parameters. To understand this trend, we analyzed the training behavior of Invertible X-Net to learn that X-Net first learns to solve the inverse problem in the initial epochs of training, followed by a gradual shift in learning the solution to the forward problem in later epochs (see Appendix Figures  11  and  12  for visualizations). Since the model optimizes the combined loss on both velocity and waveforms together, as soon as the model is able to achieve a good solution for velocity, the gradients from the combined loss start helping the model to achieve better forward solutions. This also suggests that Invertible X-Net is seemingly learning the connection between forward and inverse problems for effective bidirectional training, and therefore is able to outperform Latent U-Net (Large) despite lower number of model parameters. To verify this, we compared the performance of Latent U-Net (Large), Invertible X-Net and Invertible X-Net trained only in the forward direction (Appendix Figure  10 ). We observed that indeed Invertible X-Net with forward-only mode shows worst performance followed by Latent U-net (Large), while Invertible X-Net (joint training) shows best performance.",
            "In Figures  11  and  12 , we illustrate asymmetry in learning the translation for the forward and inverse problems using CVB and CFA datasets. From the figures, we observe that the model learns the inverse mapping in the initial epochs and gradually starts to learn the solution to the forward problem in later epochs. Since the network optimizes the combined loss function on both velocity and waveform together, the gradients from the combined loss help the model to achieve better forward solution. This corroborates with our hypothesis that the model trained on combined loss is able to learn the connection between forward and inverse problem."
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "S3.T1.1.7.6.1.1",
        "footnotes": [],
        "references": [
            "As discussed in the quantitative comparisons of Latent U-Net (Large) and Invertible X-Net on the forward problem in Figure  3(b) , we see a surprising trend that Invertible X-Net generally outperforms Latent U-Net (Large) despite having smaller number of parameters. To understand this trend, we analyzed the training behavior of Invertible X-Net to learn that X-Net first learns to solve the inverse problem in the initial epochs of training, followed by a gradual shift in learning the solution to the forward problem in later epochs (see Appendix Figures  11  and  12  for visualizations). Since the model optimizes the combined loss on both velocity and waveforms together, as soon as the model is able to achieve a good solution for velocity, the gradients from the combined loss start helping the model to achieve better forward solutions. This also suggests that Invertible X-Net is seemingly learning the connection between forward and inverse problems for effective bidirectional training, and therefore is able to outperform Latent U-Net (Large) despite lower number of model parameters. To verify this, we compared the performance of Latent U-Net (Large), Invertible X-Net and Invertible X-Net trained only in the forward direction (Appendix Figure  10 ). We observed that indeed Invertible X-Net with forward-only mode shows worst performance followed by Latent U-net (Large), while Invertible X-Net (joint training) shows best performance.",
            "In Figures  11  and  12 , we illustrate asymmetry in learning the translation for the forward and inverse problems using CVB and CFA datasets. From the figures, we observe that the model learns the inverse mapping in the initial epochs and gradually starts to learn the solution to the forward problem in later epochs. Since the network optimizes the combined loss function on both velocity and waveform together, the gradients from the combined loss help the model to achieve better forward solution. This corroborates with our hypothesis that the model trained on combined loss is able to learn the connection between forward and inverse problem."
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "S3.T1.1.7.6.2.1",
        "footnotes": [],
        "references": [
            "Extrapolating across OpenFWI Datasets:  To assess the out-of-distribution generalizability of best-performing models on unseen datasets, in Figure  7 , we evaluate zero-shot performance of models trained across all datasets (shown as rows) when tested across all datasets (shown as columns), for both forward and inverse problems. Here, the color intensity indicates difference in SSIM performance of two competing methods. We can see that Invertible X-Net outperforms other baselines for both forward and inverse problems across majority of train-test cases. The only exception is when the models are trained on FVB, a relatively simpler dataset than other data families such as CurveVel and FlatVel. From a geophysics perspective, we expect to have better generalizability for models trained on complex geological settings and tested on relatively simpler ones, as complex geological structures can be thought of as juxtapositions of simpler geological structures. Additionally, we compare the zero-shot generalization for all baselines and Latent U-Net models in detail in the Appendix (Figures  13  -  16 ).",
            "Figure  13  and  14  show generalization performance of Invertible X-Net and Latent U-Net (Large) models respectively on the inverse problem using MAE, MSE, and SSIM metrics. As described in the main paper, we show models trained across all dataset as rows and evaluated across all test datasets as columns. For comparison, we evaluate metrics such as MAE, MSE, and SSIM and calculate its difference between our models and other baseline models. For MAE and MSE, when the color intensity is blue, our model show better generalizability and vice-versa whereas, for SSIM, when the color intensity is red indicates better generalization of our model and vice-versa.",
            "From Figure  13 , we observe that the Invertible X-Net shows superior generalization over the baseline models (AutoLinear, InversionNet, and VelocityGAN) across all metrics except on the FVB dataset. Figure  13  (d) compare Invertible X-Net with Latent U-Net (Large) model where we see that Invertible X-Net shows better generalization on complex datasets such as CFB, STA, and STB datasets, while Latent U-Net is better on relatively simpler datasets such as CVA, CFA, and more. Similarly, Figure  14  shows the comparison of Latent U-Net (Large) model with other baseline models. As expected, we observe that our model is able to generalize much better than all the baseline models."
        ]
    },
    "id_table_14": {
        "caption": "",
        "table": "S3.T1.1.7.6.5.1",
        "footnotes": [],
        "references": [
            "Figure  13  and  14  show generalization performance of Invertible X-Net and Latent U-Net (Large) models respectively on the inverse problem using MAE, MSE, and SSIM metrics. As described in the main paper, we show models trained across all dataset as rows and evaluated across all test datasets as columns. For comparison, we evaluate metrics such as MAE, MSE, and SSIM and calculate its difference between our models and other baseline models. For MAE and MSE, when the color intensity is blue, our model show better generalizability and vice-versa whereas, for SSIM, when the color intensity is red indicates better generalization of our model and vice-versa.",
            "From Figure  13 , we observe that the Invertible X-Net shows superior generalization over the baseline models (AutoLinear, InversionNet, and VelocityGAN) across all metrics except on the FVB dataset. Figure  13  (d) compare Invertible X-Net with Latent U-Net (Large) model where we see that Invertible X-Net shows better generalization on complex datasets such as CFB, STA, and STB datasets, while Latent U-Net is better on relatively simpler datasets such as CVA, CFA, and more. Similarly, Figure  14  shows the comparison of Latent U-Net (Large) model with other baseline models. As expected, we observe that our model is able to generalize much better than all the baseline models."
        ]
    },
    "id_table_15": {
        "caption": "",
        "table": "A7.EGx3",
        "footnotes": [],
        "references": [
            "Figures  15  and  16  compares the performance of our models Invertible X-Net and Latent U-Net (Large) against baselines. In Figure  15 , we observe strong generalization of Invertible X-Net over all baselines - AutoLinear, FNO, and WaveformNet. Figure  15  (d) shows the comparative performance of Invertible X-Net against Latent U-Net (Large) where we see that Invertible X-Net dominates overall across all metrics. In Figure  16 , we compare the Latent U-Net (Large) model against same baselines as above. The figure indicates Latent U-Net (Large) model has much stronger generalizability than baseline models consistently."
        ]
    },
    "id_table_16": {
        "caption": "",
        "table": "A7.EGx4",
        "footnotes": [],
        "references": [
            "Extrapolating across OpenFWI Datasets:  To assess the out-of-distribution generalizability of best-performing models on unseen datasets, in Figure  7 , we evaluate zero-shot performance of models trained across all datasets (shown as rows) when tested across all datasets (shown as columns), for both forward and inverse problems. Here, the color intensity indicates difference in SSIM performance of two competing methods. We can see that Invertible X-Net outperforms other baselines for both forward and inverse problems across majority of train-test cases. The only exception is when the models are trained on FVB, a relatively simpler dataset than other data families such as CurveVel and FlatVel. From a geophysics perspective, we expect to have better generalizability for models trained on complex geological settings and tested on relatively simpler ones, as complex geological structures can be thought of as juxtapositions of simpler geological structures. Additionally, we compare the zero-shot generalization for all baselines and Latent U-Net models in detail in the Appendix (Figures  13  -  16 ).",
            "Figures  15  and  16  compares the performance of our models Invertible X-Net and Latent U-Net (Large) against baselines. In Figure  15 , we observe strong generalization of Invertible X-Net over all baselines - AutoLinear, FNO, and WaveformNet. Figure  15  (d) shows the comparative performance of Invertible X-Net against Latent U-Net (Large) where we see that Invertible X-Net dominates overall across all metrics. In Figure  16 , we compare the Latent U-Net (Large) model against same baselines as above. The figure indicates Latent U-Net (Large) model has much stronger generalizability than baseline models consistently."
        ]
    },
    "id_table_17": {
        "caption": "",
        "table": "A7.EGx5",
        "footnotes": [],
        "references": [
            "Figure  17(a)  (a) shows how the performance of Latent U-Net model is affected with change in the latent space size. As the latent space size is reduced, the MAE and MSE metrics for mapping seismic waveform to velocity is increasing, while the SSIM is decreasing. The impact is more pronounced on complex datasets such as B group of datasets. These datasets represent geologically complex datasets and therefore may require larger latent space to encode geological heterogeneity. In Figure  17(b) , we compare the large Latent U-Net with the small Latent U-Net model, while latent space size is also reduced. We observe that the performance gap between the two models reduces as the latent space size is also reduced. This underscore the importance of latent space size for encoding the geological features for an effective translation."
        ]
    },
    "id_table_18": {
        "caption": "",
        "table": "A7.EGx6",
        "footnotes": [],
        "references": [
            "Further, we provide visualizations of the first two primary PCA and t-SNE components of the velocity latent space in Figure  18  and  19  respectively. We take the encoder trained on a dataset and get the latent space encoding on several datasets. This visualization shows the in-distribution and out-of-distribution generality of our encoders and highlights the importance of the manifold for latent space translation. Overall, we observe that the larger latent space  70  70 70 70 70\\times 70 70  70  have better structure than  8  8 8 8 8\\times 8 8  8 , indicating that the ideal size of latent space should be decided based on the complexity of dataset and problem being solved."
        ]
    },
    "id_table_19": {
        "caption": "",
        "table": "A7.EGx7",
        "footnotes": [],
        "references": [
            "Further, we provide visualizations of the first two primary PCA and t-SNE components of the velocity latent space in Figure  18  and  19  respectively. We take the encoder trained on a dataset and get the latent space encoding on several datasets. This visualization shows the in-distribution and out-of-distribution generality of our encoders and highlights the importance of the manifold for latent space translation. Overall, we observe that the larger latent space  70  70 70 70 70\\times 70 70  70  have better structure than  8  8 8 8 8\\times 8 8  8 , indicating that the ideal size of latent space should be decided based on the complexity of dataset and problem being solved."
        ]
    },
    "id_table_20": {
        "caption": "",
        "table": "A7.EGx8",
        "footnotes": [],
        "references": [
            "In this section, we study the impact of skip connections of Latent U-Net model for the inverse problem. Figure  20  shows how the MAE and MSE are increasing and SSIM is decreasing when the latent space size is decreasing from  70  70 70 70 70\\times 70 70  70  to  8  8 8 8 8\\times 8 8  8 . We observe that the impact of skip connections is more pronounced at smaller latent space size as opposed to larger space."
        ]
    },
    "id_table_21": {
        "caption": "",
        "table": "A7.EGx9",
        "footnotes": [],
        "references": [
            "To investigate the effect of using frozen latent spaces on translation performance, we consider a two-stage Reconstruct then Translate variant of Latent U-Net where encoder-decoder pairs are first trained for reconstruction followed by translation in Figure  6  for varying OpenFWI families as we reduce the training fraction from 100% to 5%. We can see that for complex families such as CVB and CFB, the MAE of Reconstruct then Translate variant is higher for lower training fractions, suggesting that it is useful to influence the training of the latent spaces based on the translation objectives. We see a similar trend for Latent U-Net (Small) shown in Appendix Figure  21 .",
            "Figure  21  for Latent U-Net Small further elaborates on our finding that direct translation learning consistently outperforms the two-stage approach, where translation follows reconstruction."
        ]
    },
    "id_table_22": {
        "caption": "",
        "table": "A7.EGx10",
        "footnotes": [],
        "references": []
    },
    "id_table_23": {
        "caption": "",
        "table": "A1.T2.1",
        "footnotes": [],
        "references": [
            "For training Invertible X-Net (Cycle) model, we use loss function shown in Equation  23 ."
        ]
    },
    "id_table_24": {
        "caption": "",
        "table": "A7.EGx11",
        "footnotes": [],
        "references": []
    },
    "id_table_25": {
        "caption": "",
        "table": "A7.EGx12",
        "footnotes": [],
        "references": []
    },
    "id_table_26": {
        "caption": "",
        "table": "A7.EGx13",
        "footnotes": [],
        "references": []
    },
    "id_table_27": {
        "caption": "",
        "table": "A7.EGx14",
        "footnotes": [],
        "references": []
    },
    "id_table_28": {
        "caption": "",
        "table": "A4.T3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_29": {
        "caption": "",
        "table": "A4.T3.1.1.1.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_30": {
        "caption": "",
        "table": "A4.T3.1.4.3.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_31": {
        "caption": "",
        "table": "A4.T3.1.5.4.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_32": {
        "caption": "",
        "table": "A4.T4.1",
        "footnotes": [],
        "references": []
    },
    "id_table_33": {
        "caption": "",
        "table": "A4.T4.1.1.1.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_34": {
        "caption": "",
        "table": "A4.T5.1",
        "footnotes": [],
        "references": []
    },
    "id_table_35": {
        "caption": "",
        "table": "A4.T6.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_36": {
        "caption": "",
        "table": "A4.T6.1.1.1.1.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_37": {
        "caption": "",
        "table": "A4.T6.1.1.1.1.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_38": {
        "caption": "",
        "table": "A4.T6.1.1.1.1.4.1",
        "footnotes": [],
        "references": []
    },
    "id_table_39": {
        "caption": "",
        "table": "A4.T6.1.1.1.1.5.1",
        "footnotes": [],
        "references": []
    },
    "id_table_40": {
        "caption": "",
        "table": "A4.T6.1.1.1.1.6.1",
        "footnotes": [],
        "references": []
    },
    "id_table_41": {
        "caption": "",
        "table": "A4.T6.1.1.1.1.7.1",
        "footnotes": [],
        "references": []
    },
    "id_table_42": {
        "caption": "",
        "table": "A5.T7.3.3",
        "footnotes": [],
        "references": []
    },
    "id_table_43": {
        "caption": "",
        "table": "A5.T8.3.3",
        "footnotes": [],
        "references": []
    },
    "id_table_44": {
        "caption": "",
        "table": "A5.T9.3.3",
        "footnotes": [],
        "references": []
    },
    "id_table_45": {
        "caption": "",
        "table": "A5.T10.3.3",
        "footnotes": [],
        "references": []
    }
}