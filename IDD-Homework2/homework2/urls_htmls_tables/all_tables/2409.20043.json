{
    "id_table_1": {
        "caption": "Table 1 :  Quantitative comparison with baseline methods on the proposed Cooking-Perturbation benchmark. The grayed rows indicate the quantitatively and visually defective or even failed results on our benchmark.",
        "table": "S3.E5",
        "footnotes": [],
        "references": [
            "Despite rendering photorealistic results, existing NeRFs are designed based on a key assumption that the rendering target stays perfectly still throughout the training and testing phases, hence overfitting to a single static scene  (Trevithick and Yang,  2021 ) . However, in real-life 3D scenes, the assumption does not hold because perturbations such as foreground movements, illumination variations and data contaminations widely exist. A straightforward solution to these test-time scene changes is to apply generalizable methods  (Yu et al,  2021 ; Wang et al,  2021 ; Trevithick and Yang,  2021 ; Chen et al,  2021 ; Liu et al,  2022 ; Johari et al,  2022 ; Xu et al,  2022 )  which combine the NeRF-like model with scene geometry. In this way, they condition the neural renderer on scene priors such as convolutional image features  (Trevithick and Yang,  2021 ; Yu et al,  2021 ; Wang et al,  2021 ; Johari et al,  2022 ) , cost volumes  (Chen et al,  2021 ; Liu et al,  2022 )  and attentional activation maps  (Wang et al,  2023b ) . While it only costs thousands of fine-tuning iterations for generalizable NeRFs to quickly generalize to unseen novel scenes  (Wang et al,  2021 ; Liu et al,  2022 ) , we surprisingly observe that they poorly tackle such small but unpredictable test-time perturbations on the original rendering target. As shown in Fig.  1 , advanced generalizable NeRFs  (Chen et al,  2021 ; Liu et al,  2022 ; Johari et al,  2022 )  produce significantly defective rendering results or even fail when addressing unexpected test-time scene changes. Moreover, although scene variations can be modeled temporally via spatio-temporal reconstruction  (Pumarola et al,  2021 ; Gao et al,  2021 ) , it is infeasible to learn the motion patterns given only a single timeframe at the training phase.",
            "In this section, we firstly review the preliminary on the neural representation in NeRFs which our method is based on (Sec.  3.1 ). Then we elaborate on the framework of OPONeRF which consists of the overall representation (Sec.  3.2.1 ), the geometry encoder (Sec.  3.2.2 ) and the OPONeRF decoder (Sec.  3.2.3 ) personalized for discriminative and probabilistic point representation (Sec.  3.2.4 ). Finally we present the rendering and optimizing target (Sec.  3.2.5 ), implementation details (Sec.  3.3 ) and provide a more in-depth comparison with other related methods (Sec.  3.4 ).",
            "where  f x subscript f x f_{\\mathbf{x}} italic_f start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  represents the point-wise feature at  x x \\mathbf{x} bold_x  aggregated from the support views. Similar to the mapping mechanism of vanilla NeRFs in Eqn. ( 1 ), the NeRF parameters    \\Theta roman_  are still shared by all sampled points and fixed at test time. We perform the physical and differentiable volume rendering  (Kajiya and Von Herzen,  1984 )  as post-processing to obtain the pixel properties on the querying views.",
            "As shown in Fig.  2 , the final probabilistic point representation  F x subscript F x \\mathcal{F}_{\\mathbf{x}} caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  is consumed by the OPONeRF renderer, which consists of both targeted layers personalized for  x x \\mathbf{x} bold_x  and the other layers shared by all  x x \\mathbf{x} bold_x s. The renderer architecture is the Ray Transformer  (Wang et al,  2021 )  where a sampled point attends to other points along the ray. The target layers are the query (Q), key (K), value (V) projection MLPs and feed-forward layer of Ray Transformer. We conduct the volume rendering in Sec.  3.1  to post-process the output of renderer to obtain the pixel values on the querying view. The volume rendering process is supervised by the photometric loss formulated by:",
            "The quantitative results on Cooking-Perturbation and MeetRoom-Perturbation are shown in Table  1  and Table  2  respectively. The observation is 2-fold. Firstly, while GeoNeRF and GNT learn stronger scene priors via advanced architectures, they demonstrate much inferior rendering performance when tackling test-time foreground movements as shown qualitative in Fig.  7 . Also, as shown in Fig.  1 , GeoNeRF produces extremely blurred results on Cooking-Perturbation and even renders all-black images on MeetRoom-Perturbation. This indicates that the stronger generalization ability cannot guarantee the robustness to small but unpredictable test-time scene changes. It also demonstrates that our benchmark, widely existing in daily life, is much more challenging than the conventional generalization-based settings. In contrast, methods with fewer scene priors and less sophisticated architectures output visually better results on Cooking-Perturbation, such as MVSNeRF and NeuRay. Note that MVSNeRF still struggles on MeetRoom-Perturbation. Secondly, quantitatve results show that our OPONeRF consistently achieves state-of-the-art results on all 3 metrics, which validates the effectiveness of the proposed method. With sufficient training samples for the mapping from point to parameters of neural renderer, OPONeRF is able to adapt to unseen changed foreground and preserve the background details with only a single training timeframe. Notably, on Cook Spinache, Flame Steak and VR Headset tasks, OPONeRF outperforms the second-best NeuRay by a sizable margin ( > > > 1.5 PSNR).",
            "To simulate more sophisticated foreground movements in the same environment, we also conducted  cross-task  evaluation by training on the Cook Spinach task and evaluating on the other 3 tasks in the Cooking-Perturbation benchmark. Results in Table  3  and Fig.  1  show that OPONeRF demonstrates substantially superior robustness to more complex test-time motions. It is worth noting that while intuitively cross-task rendering is more challenging than within-task, both NeuRay and OPONeRF achieve better results on the cross-task inference compared with Table  1 . Given the fact that both training and testing timeframes share the same background and that the test-time motions are unpredictable, we postulate that little difference lies between the settings of cross-task and within-task inference.",
            "Settings:   Similar to Sec.  4.1.1 , we used the initial scene ( t = 0 t 0 t=0 italic_t = 0 ) and its changed versions ( t > 0 t 0 t>0 italic_t > 0 ) as the training&validation scene and test scenes respectively. We conducted benchmark experiments on IBRNet  (Wang et al,  2021 ) , MVSNeRF  (Chen et al,  2021 ) , NeuRay  (Liu et al,  2022 )  and our OPONeRF.",
            "The quantitative results are shown in Table  6 . Both image-level and feature-level cause sizable performance degradation to MVSNeRF. NeuRay achieves favorable robustness against feature-level noises while OPONeRF even further eases the negative effects of test-time noises. On both metrics of PSNR and SSIM, OPONeRF achieves the best results among all the compared trials, which indicate that the flexibility brought by the One-Point-One-NeRF mechanism does not sacrifice the robustness to outlier noises. Notably, under test-time feature-level noises, OPONeRF impressively drops by less than  0.4 % percent 0.4 0.4\\% 0.4 %  on both PSNR and SSIM, where the noises can hardly be noticed as shown in Fig.  10 (b).",
            "Fig.  11  shows the qualitative results on the generalization benchmarks for spatial and spatio-temporal reconstruction. OPONeRF improves the detail preserving of unseen scenes or motions via the comparison with other baseline methods. Quantitatively, the improvement of OPONeRF is much more obvious in spatio-temporal reconstruction. We infer that our OPONeRF helps to significantly fertile the training samples from an ill-posed monocular video. In contrast, traditional generalization-based benchmarks for spatial reconstruction are provided with sufficient training views, which benefits the network design with stronger scene priors.",
            "Way of Obtaining Adaptiveness Factor:   By default we firstly interpolate using spatial query then apply regression layer to obtain the adaptiveness factor  a x subscript a x \\mathbf{a}_{x} bold_a start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT  (ID 3 in Table  9 ). An alternative is to keep the fan-out dimension unchanged and directly regress  F x subscript F x \\mathcal{F}_{\\mathbf{x}} caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  into  A x subscript A x \\mathcal{A}_{\\mathbf{x}} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  and discard the fusion in Eqn. ( 15 ) (ID 4 in Table  9 ). This implementation shows inferior performance which implies the importance of learning a geometric-aware adaptiveness factor. Also, the default implementation models  f x subscript f x f_{\\mathbf{x}} italic_f start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  and  a x subscript a x a_{\\mathbf{x}} italic_a start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  separately which tackles the uncertainty in scene representation and the corresponding responses to scene variations respectively. Such a design choice outperforms the naive mapping from point to NeRF as indicated by the quantitative results.",
            "Values of Weight Hyper-parameters:   The hyper-parameters    \\alpha italic_  in  F x subscript F x \\mathcal{F}_{\\mathbf{x}} caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  and    \\gamma italic_  in  F o  p  o subscript F o p o \\mathcal{F}_{opo} caligraphic_F start_POSTSUBSCRIPT italic_o italic_p italic_o end_POSTSUBSCRIPT  control the gradients of the final point representation and the balance between  L a  p  p  r subscript L a p p r \\mathcal{L}_{appr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_p italic_p italic_r end_POSTSUBSCRIPT  and  L r  e  c subscript L r e c \\mathcal{L}_{rec} caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_c end_POSTSUBSCRIPT . We tested different values and have summarized the results on Beef scene in Fig.  12 . We can see that the rendering with test-time changes is more sensitive when tuning the ingredients of  F x subscript F x \\mathcal{F}_{\\mathbf{x}} caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT . As for the balance between  L a  p  p  r subscript L a p p r \\mathcal{L}_{appr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_p italic_p italic_r end_POSTSUBSCRIPT  and  L r  e  c subscript L r e c \\mathcal{L}_{rec} caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_c end_POSTSUBSCRIPT , we observe minor fluctuations when ablating    \\gamma italic_ . By default we set   = 0.3  0.3 \\alpha=0.3 italic_ = 0.3  and   = 1.0  1.0 \\gamma=1.0 italic_ = 1.0 .",
            "Infeasibility of Spatio-temporal Construction in Our Benchmark:   To demonstrate the difficulties in spatio-temporal reconstruction in our settings where only a single timeframe is provided, we conducted experiments on our Cooking-Perturbation benchmark using D-NeRF and compared with OPONeRF. We experimented on the Cut Roasted Beef, Flame Steak and Sear Steak tasks. Quantitative results in Table.  10  validate that D-NeRF performs poorly in the absence of temporal variations, where the rendering quality of OPONeRF is improved significantly measured by LPIPS.",
            "Limitations of OPONeRF:   Given one training timestamp and not specifically designed for scenes with variable daylight  (Song and Funkhouser,  2019 ) , OPONeRF struggles at handling complex illuminations. As indicated by Fig.  13 , OPONeRF also produces blurred results when faced with significant scene changes, e.g., trained on Cook Spinache and evaluated on VR Headset. Moreover, compared with the recent Gaussian Splatting method  (Wu et al,  2024 ) , OPONeRF shares with other NeRF-based methods the slower training speed."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Quantitative comparison with baseline methods on the proposed MeetRoom-Perturbation benchmark. The grayed rows indicate that GeoNeRF  (Johari et al,  2022 )  and GNT  (Wang et al,  2023b )  visually  fail  on our benchmark.",
        "table": "S3.E8",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "In this section, we firstly review the preliminary on the neural representation in NeRFs which our method is based on (Sec.  3.1 ). Then we elaborate on the framework of OPONeRF which consists of the overall representation (Sec.  3.2.1 ), the geometry encoder (Sec.  3.2.2 ) and the OPONeRF decoder (Sec.  3.2.3 ) personalized for discriminative and probabilistic point representation (Sec.  3.2.4 ). Finally we present the rendering and optimizing target (Sec.  3.2.5 ), implementation details (Sec.  3.3 ) and provide a more in-depth comparison with other related methods (Sec.  3.4 ).",
            "Our proposed OPONeRF aims to tackle the robust rendering under the scenario of test-time unseen local representations or scene perturbations. The overall framework is presented in Fig.  2 . Note that the framework in Fig.  2  is shared by all experiments on the robustness benchmark.",
            "The overall network architecture in Fig.  2  contains a geometry encoder which extracts rich geometric features, a series of parallel parameter candidate decoders (PCD) which provide a geometric-aware and layer-variant parameter pool, and the final personalized neural renderer discriminative to each sampled point with probabilistic modeling.",
            "Step 2. Discriminative Parameter Personalization:   For each sampled  x x \\mathbf{x} bold_x , OPONeRF discriminatively personalizes its point-wise parameters of the target layers in the neural renderer. A naive attempt is to learn a totally unique renderer responsible for each 3D location. Suppose for an image with  h h h italic_h  the height,  w w w italic_w  the width and  N N N italic_N  the number of sampled points on a ray, it requires  h  w  N h w N h\\times w\\times N italic_h  italic_w  italic_N  set of learnable matrices for a single target layer which brings unacceptable computational and optimization overhead. Given this, we regard  W a l subscript superscript W l a \\mathbf{W}^{l}_{a} bold_W start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  as a parameter pool and select from it the proper ones via introducing an adaptiveness factor  A x  R L subscript A x superscript R L \\mathcal{A}_{\\mathbf{x}}\\in\\mathbb{R}^{L} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT  discriminative to each  x x \\mathbf{x} bold_x .  L L L italic_L  is the number of target layers in the OPONeRF renderer. We detail the calculation of  A x subscript A x \\mathbf{A}_{x} bold_A start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT  in Sec.  3.2.4 . Considering the large quantities of sampled points,  A x subscript A x \\mathcal{A}_{\\mathbf{x}} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  learns how to personalize the neural renderer in a layer-wise manner rather than element-wise masks  M l superscript M l M^{l} italic_M start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT .  A x subscript A x \\mathcal{A}_{\\mathbf{x}} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  adaptively employs the parameter candidates in Eqn. ( 8 ):",
            "As shown in Fig.  2 , the final probabilistic point representation  F x subscript F x \\mathcal{F}_{\\mathbf{x}} caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  is consumed by the OPONeRF renderer, which consists of both targeted layers personalized for  x x \\mathbf{x} bold_x  and the other layers shared by all  x x \\mathbf{x} bold_x s. The renderer architecture is the Ray Transformer  (Wang et al,  2021 )  where a sampled point attends to other points along the ray. The target layers are the query (Q), key (K), value (V) projection MLPs and feed-forward layer of Ray Transformer. We conduct the volume rendering in Sec.  3.1  to post-process the output of renderer to obtain the pixel values on the querying view. The volume rendering process is supervised by the photometric loss formulated by:",
            "Incorporating OPONeRF into Other Methods:   The idea of OPONeRF can be flexibly incorporated into other popular methods. We set the target layers as all linear layers but the last regression heads. For example, to implement One-Point-One MVSNeRF (OPO-MVSNeRF), the target layers are all layers except for the radiance and density MLPs in its NeRF renderer. We follow Sec.  3.2.3  to correspondingly learn a PCD module that generates candidate parameters for the target layers, then regress per-point adaptiveness factors to personalize neural renderers for each sampled point. We validated the effectiveness of the spirit of OPONeRF via implementing such baseline variants and conducting benchmark experiments. Please refer to Sec.  4.6  for details.",
            "The quantitative results on Cooking-Perturbation and MeetRoom-Perturbation are shown in Table  1  and Table  2  respectively. The observation is 2-fold. Firstly, while GeoNeRF and GNT learn stronger scene priors via advanced architectures, they demonstrate much inferior rendering performance when tackling test-time foreground movements as shown qualitative in Fig.  7 . Also, as shown in Fig.  1 , GeoNeRF produces extremely blurred results on Cooking-Perturbation and even renders all-black images on MeetRoom-Perturbation. This indicates that the stronger generalization ability cannot guarantee the robustness to small but unpredictable test-time scene changes. It also demonstrates that our benchmark, widely existing in daily life, is much more challenging than the conventional generalization-based settings. In contrast, methods with fewer scene priors and less sophisticated architectures output visually better results on Cooking-Perturbation, such as MVSNeRF and NeuRay. Note that MVSNeRF still struggles on MeetRoom-Perturbation. Secondly, quantitatve results show that our OPONeRF consistently achieves state-of-the-art results on all 3 metrics, which validates the effectiveness of the proposed method. With sufficient training samples for the mapping from point to parameters of neural renderer, OPONeRF is able to adapt to unseen changed foreground and preserve the background details with only a single training timeframe. Notably, on Cook Spinache, Flame Steak and VR Headset tasks, OPONeRF outperforms the second-best NeuRay by a sizable margin ( > > > 1.5 PSNR).",
            "We used the Kubric engine  (Greff et al,  2022 )  and followed the settings in Sec.  4.2 , except that we only modified the object existence(s). We constructed three synthetic scenes with 5,6 and 7 objects respectively. 21 viewpoints are captured in each scene, some of which are shown in Figure  9 .",
            "Values of Weight Hyper-parameters:   The hyper-parameters    \\alpha italic_  in  F x subscript F x \\mathcal{F}_{\\mathbf{x}} caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  and    \\gamma italic_  in  F o  p  o subscript F o p o \\mathcal{F}_{opo} caligraphic_F start_POSTSUBSCRIPT italic_o italic_p italic_o end_POSTSUBSCRIPT  control the gradients of the final point representation and the balance between  L a  p  p  r subscript L a p p r \\mathcal{L}_{appr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_p italic_p italic_r end_POSTSUBSCRIPT  and  L r  e  c subscript L r e c \\mathcal{L}_{rec} caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_c end_POSTSUBSCRIPT . We tested different values and have summarized the results on Beef scene in Fig.  12 . We can see that the rendering with test-time changes is more sensitive when tuning the ingredients of  F x subscript F x \\mathcal{F}_{\\mathbf{x}} caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT . As for the balance between  L a  p  p  r subscript L a p p r \\mathcal{L}_{appr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_p italic_p italic_r end_POSTSUBSCRIPT  and  L r  e  c subscript L r e c \\mathcal{L}_{rec} caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_c end_POSTSUBSCRIPT , we observe minor fluctuations when ablating    \\gamma italic_ . By default we set   = 0.3  0.3 \\alpha=0.3 italic_ = 0.3  and   = 1.0  1.0 \\gamma=1.0 italic_ = 1.0 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Quantitative comparison with baseline methods by pretraining on the task of Cook Spinach and evaluating on the other tasks.",
        "table": "S3.E13",
        "footnotes": [],
        "references": [
            "In this section, we firstly review the preliminary on the neural representation in NeRFs which our method is based on (Sec.  3.1 ). Then we elaborate on the framework of OPONeRF which consists of the overall representation (Sec.  3.2.1 ), the geometry encoder (Sec.  3.2.2 ) and the OPONeRF decoder (Sec.  3.2.3 ) personalized for discriminative and probabilistic point representation (Sec.  3.2.4 ). Finally we present the rendering and optimizing target (Sec.  3.2.5 ), implementation details (Sec.  3.3 ) and provide a more in-depth comparison with other related methods (Sec.  3.4 ).",
            "Step 1. Parameter Candidates Generation:   Suppose the  l l l italic_l -th target layer in the OPONeRF renderer is parameterized by  W ^ l  R C i l  C o l superscript ^ W l superscript R superscript subscript C i l superscript subscript C o l \\hat{\\mathbf{W}}^{l}\\in\\mathbb{R}^{C_{i}^{l}\\times C_{o}^{l}} over^ start_ARG bold_W end_ARG start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT  italic_C start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT . We design a series of parallel parameter candidate decoders (PCD, shown in Fig.  3 ) to firstly decode  F F \\mathbf{F} bold_F  into weight candidates for the target layers:",
            "Step 2. Discriminative Parameter Personalization:   For each sampled  x x \\mathbf{x} bold_x , OPONeRF discriminatively personalizes its point-wise parameters of the target layers in the neural renderer. A naive attempt is to learn a totally unique renderer responsible for each 3D location. Suppose for an image with  h h h italic_h  the height,  w w w italic_w  the width and  N N N italic_N  the number of sampled points on a ray, it requires  h  w  N h w N h\\times w\\times N italic_h  italic_w  italic_N  set of learnable matrices for a single target layer which brings unacceptable computational and optimization overhead. Given this, we regard  W a l subscript superscript W l a \\mathbf{W}^{l}_{a} bold_W start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  as a parameter pool and select from it the proper ones via introducing an adaptiveness factor  A x  R L subscript A x superscript R L \\mathcal{A}_{\\mathbf{x}}\\in\\mathbb{R}^{L} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT  discriminative to each  x x \\mathbf{x} bold_x .  L L L italic_L  is the number of target layers in the OPONeRF renderer. We detail the calculation of  A x subscript A x \\mathbf{A}_{x} bold_A start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT  in Sec.  3.2.4 . Considering the large quantities of sampled points,  A x subscript A x \\mathcal{A}_{\\mathbf{x}} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  learns how to personalize the neural renderer in a layer-wise manner rather than element-wise masks  M l superscript M l M^{l} italic_M start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT .  A x subscript A x \\mathcal{A}_{\\mathbf{x}} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  adaptively employs the parameter candidates in Eqn. ( 8 ):",
            "As shown in Fig.  2 , the final probabilistic point representation  F x subscript F x \\mathcal{F}_{\\mathbf{x}} caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  is consumed by the OPONeRF renderer, which consists of both targeted layers personalized for  x x \\mathbf{x} bold_x  and the other layers shared by all  x x \\mathbf{x} bold_x s. The renderer architecture is the Ray Transformer  (Wang et al,  2021 )  where a sampled point attends to other points along the ray. The target layers are the query (Q), key (K), value (V) projection MLPs and feed-forward layer of Ray Transformer. We conduct the volume rendering in Sec.  3.1  to post-process the output of renderer to obtain the pixel values on the querying view. The volume rendering process is supervised by the photometric loss formulated by:",
            "Incorporating OPONeRF into Other Methods:   The idea of OPONeRF can be flexibly incorporated into other popular methods. We set the target layers as all linear layers but the last regression heads. For example, to implement One-Point-One MVSNeRF (OPO-MVSNeRF), the target layers are all layers except for the radiance and density MLPs in its NeRF renderer. We follow Sec.  3.2.3  to correspondingly learn a PCD module that generates candidate parameters for the target layers, then regress per-point adaptiveness factors to personalize neural renderers for each sampled point. We validated the effectiveness of the spirit of OPONeRF via implementing such baseline variants and conducting benchmark experiments. Please refer to Sec.  4.6  for details.",
            "To simulate more sophisticated foreground movements in the same environment, we also conducted  cross-task  evaluation by training on the Cook Spinach task and evaluating on the other 3 tasks in the Cooking-Perturbation benchmark. Results in Table  3  and Fig.  1  show that OPONeRF demonstrates substantially superior robustness to more complex test-time motions. It is worth noting that while intuitively cross-task rendering is more challenging than within-task, both NeuRay and OPONeRF achieve better results on the cross-task inference compared with Table  1 . Given the fact that both training and testing timeframes share the same background and that the test-time motions are unpredictable, we postulate that little difference lies between the settings of cross-task and within-task inference.",
            "Limitations of OPONeRF:   Given one training timestamp and not specifically designed for scenes with variable daylight  (Song and Funkhouser,  2019 ) , OPONeRF struggles at handling complex illuminations. As indicated by Fig.  13 , OPONeRF also produces blurred results when faced with significant scene changes, e.g., trained on Cook Spinache and evaluated on VR Headset. Moreover, compared with the recent Gaussian Splatting method  (Wu et al,  2024 ) , OPONeRF shares with other NeRF-based methods the slower training speed."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Quantitative comparison with baseline methods on the robustness to synthetic test-time illumination changes.",
        "table": "S3.E18",
        "footnotes": [],
        "references": [
            "In this section, we firstly review the preliminary on the neural representation in NeRFs which our method is based on (Sec.  3.1 ). Then we elaborate on the framework of OPONeRF which consists of the overall representation (Sec.  3.2.1 ), the geometry encoder (Sec.  3.2.2 ) and the OPONeRF decoder (Sec.  3.2.3 ) personalized for discriminative and probabilistic point representation (Sec.  3.2.4 ). Finally we present the rendering and optimizing target (Sec.  3.2.5 ), implementation details (Sec.  3.3 ) and provide a more in-depth comparison with other related methods (Sec.  3.4 ).",
            "Step 2. Discriminative Parameter Personalization:   For each sampled  x x \\mathbf{x} bold_x , OPONeRF discriminatively personalizes its point-wise parameters of the target layers in the neural renderer. A naive attempt is to learn a totally unique renderer responsible for each 3D location. Suppose for an image with  h h h italic_h  the height,  w w w italic_w  the width and  N N N italic_N  the number of sampled points on a ray, it requires  h  w  N h w N h\\times w\\times N italic_h  italic_w  italic_N  set of learnable matrices for a single target layer which brings unacceptable computational and optimization overhead. Given this, we regard  W a l subscript superscript W l a \\mathbf{W}^{l}_{a} bold_W start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  as a parameter pool and select from it the proper ones via introducing an adaptiveness factor  A x  R L subscript A x superscript R L \\mathcal{A}_{\\mathbf{x}}\\in\\mathbb{R}^{L} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT  discriminative to each  x x \\mathbf{x} bold_x .  L L L italic_L  is the number of target layers in the OPONeRF renderer. We detail the calculation of  A x subscript A x \\mathbf{A}_{x} bold_A start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT  in Sec.  3.2.4 . Considering the large quantities of sampled points,  A x subscript A x \\mathcal{A}_{\\mathbf{x}} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  learns how to personalize the neural renderer in a layer-wise manner rather than element-wise masks  M l superscript M l M^{l} italic_M start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT .  A x subscript A x \\mathcal{A}_{\\mathbf{x}} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  adaptively employs the parameter candidates in Eqn. ( 8 ):",
            "As shown in Fig.  4 , we implement the final probabilistic point feature  F x subscript F x \\mathcal{F}_{\\mathbf{x}} caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  with a residual connection for steady training, i.e.,  F x := f x +   ( f x I + f x V ) assign subscript F x subscript f x  subscript superscript f I x subscript superscript f V x \\mathcal{F}_{\\mathbf{x}}:=f_{\\mathbf{x}}+\\alpha(f^{I}_{\\mathbf{x}}+f^{V}_{% \\mathbf{x}}) caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT := italic_f start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT + italic_ ( italic_f start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT + italic_f start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT ) .    \\alpha italic_  is a weight hyper-parameter. Then we formulate the adaptiveness factor  A x subscript A x \\mathcal{A}_{\\mathbf{x}} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  as weighted fusion:",
            "a x subscript a x \\mathbf{a}_{\\mathbf{x}} bold_a start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  is computed by spatial interpolation similar to  f x subscript f x f_{\\mathbf{x}} italic_f start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT . Specifically, we modify the geometric encoder in Eqn. ( 4 ) into:",
            "Incorporating OPONeRF into Other Methods:   The idea of OPONeRF can be flexibly incorporated into other popular methods. We set the target layers as all linear layers but the last regression heads. For example, to implement One-Point-One MVSNeRF (OPO-MVSNeRF), the target layers are all layers except for the radiance and density MLPs in its NeRF renderer. We follow Sec.  3.2.3  to correspondingly learn a PCD module that generates candidate parameters for the target layers, then regress per-point adaptiveness factors to personalize neural renderers for each sampled point. We validated the effectiveness of the spirit of OPONeRF via implementing such baseline variants and conducting benchmark experiments. Please refer to Sec.  4.6  for details.",
            "As indicated by D-NeRF setting  (Pumarola et al,  2021 )  in Fig.  6 , scene variations can also be modeled by the methods designed for the spatio-temporal reconstruction benchmarks  (Pumarola et al,  2021 ; Tian et al,  2023 ; Gao et al,  2021 ) . However, they are built on the premise of multiple timeframes at the training phase. In our settings, the deformation mapping  ( x , d , t )  ( c ,  )  x d t c  (\\mathbf{x},\\mathbf{d},t)\\rightarrow(\\mathbf{c},\\sigma) ( bold_x , bold_d , italic_t )  ( bold_c , italic_ )  in D-NeRF downgrades to  ( x , d )  ( c ,  )  x d c  (\\mathbf{x},\\mathbf{d})\\rightarrow(\\mathbf{c},\\sigma) ( bold_x , bold_d )  ( bold_c , italic_ )  in the absence of temporal variation. Therefore, the temporal modeling in D-NeRF is infeasible in our constructed benchmark, which we quantitatively show in Sec.  4.6 .",
            "Settings:   Similar to Sec.  4.1.1 , we used the initial scene ( t = 0 t 0 t=0 italic_t = 0 ) and its changed versions ( t > 0 t 0 t>0 italic_t > 0 ) as the training&validation scene and test scenes respectively. We conducted benchmark experiments on IBRNet  (Wang et al,  2021 ) , MVSNeRF  (Chen et al,  2021 ) , NeuRay  (Liu et al,  2022 )  and our OPONeRF.",
            "The quantitative and qualitative results are summarized in Table  4  and Fig.  8  respectively. It can be seen that the overall performance on the synthetic dataset is much better than real-world benchmarks numerically. We infer it is due to the relatively low geometric complexity, so that the baselines with generalization power can well handle the test-time object movements. However, we observe abnormal shadows around both the static and moving objects in the rendered results of NeuRay, which indicates the confusion of illumination conditions at test-time while OPONeRF more accurately preserves the lighting conditions through illustrative comparisons. Quantitatively, in both backgrounds, OPONeRF shows substantially better numerical results in 3 evaluation metrics, regardless of the background color.",
            "We used the Kubric engine  (Greff et al,  2022 )  and followed the settings in Sec.  4.2 , except that we only modified the object existence(s). We constructed three synthetic scenes with 5,6 and 7 objects respectively. 21 viewpoints are captured in each scene, some of which are shown in Figure  9 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Quantitative results of the setting of modifying the object existence(s). The results are evaluated by PSNR   \\uparrow  .",
        "table": "S4.T1.12.12",
        "footnotes": [
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "In this section, we firstly review the preliminary on the neural representation in NeRFs which our method is based on (Sec.  3.1 ). Then we elaborate on the framework of OPONeRF which consists of the overall representation (Sec.  3.2.1 ), the geometry encoder (Sec.  3.2.2 ) and the OPONeRF decoder (Sec.  3.2.3 ) personalized for discriminative and probabilistic point representation (Sec.  3.2.4 ). Finally we present the rendering and optimizing target (Sec.  3.2.5 ), implementation details (Sec.  3.3 ) and provide a more in-depth comparison with other related methods (Sec.  3.4 ).",
            "then the adaptiveness of neural renderer to the test-time scene variations is limited by the one-shot learning framework. This is because we simply condition the parameters of neural renderer ( M  subscript M  \\mathcal{M}_{\\Theta} caligraphic_M start_POSTSUBSCRIPT roman_ end_POSTSUBSCRIPT ) on the  global  representation of the given scene, as shown in the upper part in Fig.  5 . Therefore, changes can hardly be captured by training with a single and frozen timeframe. However, the incorporation of  local  adaptiveness factors converts the non-trivial one-shot learning into a more generalizable framework. As shown in the lower part of Fig.  5 , OPONeRF learns to personalize a unique renderer for every sampled  x x x italic_x . By covering all the densely sampled 3D positions, OPONeRF hence learns the respond to the abundant local variations even from a single frozen timeframe, broadening the concept of seen variations from scene-level adaptation to point-level generalization. Since our experimental setting focuses on local scene changes, we assume that the awareness of rich training-time local variation is beneficial to the adaptiveness and robustness to those test-time changes.",
            "The quantitative results of cross-scene evaluations on the robustness to test-time object existence changes are summarized in Table  5 . We can see that compared with NeuRay  (Liu et al,  2022 ) , our OPONeRF demonstrates much more robustness, regardless of object removal or insertion. Moreover, as suggested by the diagonal cells in Table  5 , OPONeRF precedes the compared method when the training and testing scenes are the same, which demonstrates the effectiveness of OPONeRF on the setting of per-scene optimization.",
            "Way of Obtaining Adaptiveness Factor:   By default we firstly interpolate using spatial query then apply regression layer to obtain the adaptiveness factor  a x subscript a x \\mathbf{a}_{x} bold_a start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT  (ID 3 in Table  9 ). An alternative is to keep the fan-out dimension unchanged and directly regress  F x subscript F x \\mathcal{F}_{\\mathbf{x}} caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  into  A x subscript A x \\mathcal{A}_{\\mathbf{x}} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  and discard the fusion in Eqn. ( 15 ) (ID 4 in Table  9 ). This implementation shows inferior performance which implies the importance of learning a geometric-aware adaptiveness factor. Also, the default implementation models  f x subscript f x f_{\\mathbf{x}} italic_f start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  and  a x subscript a x a_{\\mathbf{x}} italic_a start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  separately which tackles the uncertainty in scene representation and the corresponding responses to scene variations respectively. Such a design choice outperforms the naive mapping from point to NeRF as indicated by the quantitative results."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  Quantitative comparison with baseline methods on the Cut Roasted Beef scene with test-time noises. The experiments were on the  validation  set, i.e. the per-scene optimization setting. The percentage of performance degradation is also shown.",
        "table": "S4.T2.9.9",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "NeRF Benchmarks:   NeRF benchmarks have recently been constructed with various application scenarios. For example, they evaluate NeRFs from the aspects of rendering quality  (Mildenhall et al,  2020 ; Barron et al,  2021 ) , training and testing efficiency  (Hu et al,  2022 ; Reiser et al,  2021 ) , editting controllability  (Yuan et al,  2022 ) , scability  (Tancik et al,  2022 ; Turki et al,  2022 )  or applicability to downstream tasks  (Zhi et al,  2021 ; Jeong et al,  2022 ; Liu et al,  2023 ; Xu et al,  2023 ) . More relevant to our benchmark are the generalization and spatio-temporal reconstruction benchmarks, as shown in Fig.  6 . In the generalization setting  (Trevithick and Yang,  2021 ; Chen et al,  2021 ; Liu et al,  2022 ; Johari et al,  2022 ; Wang et al,  2023b ) , the training phase has already seen massive scene variations, thereby benefitting the fast generalization to test-time unseen scenes within a few fine-tuning iterations. In spatio-temporal reconstructions, multiple timeframes are provided to model the additional temporal dimension via NeRF+t  (Gao et al,  2021 )  or neural ODE solver  (Tian et al,  2023 ) . In contrast, only a single timeframe is provided during training phase in our proposed benchmark, which is much more challenging for generalizable methods  (Chen et al,  2021 ; Liu et al,  2022 ; Wang et al,  2021 ; Johari et al,  2022 ; Wang et al,  2023b )  and even infeasible for temporal modeling  (Gao et al,  2021 ; Tian et al,  2023 ) . In  (Wang et al,  2023a ) , the authors also present a benchmark with image blurring, noises, fogging and pixelate to measure the robustness of NeRFs. Such data corruptions are exerted at both training and testing stages in  (Wang et al,  2023a )  while our proposed benchmark includes a different scenario, i.e., test-time data contaminations.",
            "Incorporating OPONeRF into Other Methods:   The idea of OPONeRF can be flexibly incorporated into other popular methods. We set the target layers as all linear layers but the last regression heads. For example, to implement One-Point-One MVSNeRF (OPO-MVSNeRF), the target layers are all layers except for the radiance and density MLPs in its NeRF renderer. We follow Sec.  3.2.3  to correspondingly learn a PCD module that generates candidate parameters for the target layers, then regress per-point adaptiveness factors to personalize neural renderers for each sampled point. We validated the effectiveness of the spirit of OPONeRF via implementing such baseline variants and conducting benchmark experiments. Please refer to Sec.  4.6  for details.",
            "Settings:   For all tasks in two benchmarks, we evenly sampled 21 frames along the timeline for each camera and used the first timeframe for training and validation. All views in the rest 20 timeframes are adopted as test set. Note that the evaluation on the validation set equals the classic  per-scene optimization  benchmark as in  (Mildenhall et al,  2020 ) . We briefly illustrate the experimental settings in Fig.  6 , where only one viewpoint is shown for simplicity.",
            "As indicated by D-NeRF setting  (Pumarola et al,  2021 )  in Fig.  6 , scene variations can also be modeled by the methods designed for the spatio-temporal reconstruction benchmarks  (Pumarola et al,  2021 ; Tian et al,  2023 ; Gao et al,  2021 ) . However, they are built on the premise of multiple timeframes at the training phase. In our settings, the deformation mapping  ( x , d , t )  ( c ,  )  x d t c  (\\mathbf{x},\\mathbf{d},t)\\rightarrow(\\mathbf{c},\\sigma) ( bold_x , bold_d , italic_t )  ( bold_c , italic_ )  in D-NeRF downgrades to  ( x , d )  ( c ,  )  x d c  (\\mathbf{x},\\mathbf{d})\\rightarrow(\\mathbf{c},\\sigma) ( bold_x , bold_d )  ( bold_c , italic_ )  in the absence of temporal variation. Therefore, the temporal modeling in D-NeRF is infeasible in our constructed benchmark, which we quantitatively show in Sec.  4.6 .",
            "The quantitative results are shown in Table  6 . Both image-level and feature-level cause sizable performance degradation to MVSNeRF. NeuRay achieves favorable robustness against feature-level noises while OPONeRF even further eases the negative effects of test-time noises. On both metrics of PSNR and SSIM, OPONeRF achieves the best results among all the compared trials, which indicate that the flexibility brought by the One-Point-One-NeRF mechanism does not sacrifice the robustness to outlier noises. Notably, under test-time feature-level noises, OPONeRF impressively drops by less than  0.4 % percent 0.4 0.4\\% 0.4 %  on both PSNR and SSIM, where the noises can hardly be noticed as shown in Fig.  10 (b).",
            "As indicated by Fig.  6 , our proposed experimental setting can be viewed as an extreme case of the conventional generalization-based setting. To validate the effectiveness of OPONeRF on existing popular benchmarks, we further conducted experiments on generalization-based benchmarks."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :  Quantitative comparison with baseline methods on the generalization benchmarks for spatial reconstruction. Highlights are the  best  and  runner-up .",
        "table": "S4.T3.9.9",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Full-sized Output of PCD:   As the width of a target layer can be very large ( C i l , C o l > 1000 subscript superscript C l i subscript superscript C l o 1000 C^{l}_{i},C^{l}_{o}>1000 italic_C start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_C start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT > 1000 ), the computational cost can be expensive for the parameter candidate decoders if we obtain the full-sized output in Eqn. ( 7 ) at once. Therefore, we firstly generate a down-sized output of Eqn. ( 7 ) then apply linear layers to upscale the fan-in and fan-out dimensions to their raw resolution. Inspired by  (Chen and Wang,  2022 ) , we element-wisely multiply the upscaled output with another set of full-sized matrices that are randomly initialized and iteratively optimized. The similar technique is adopted in 3D object detection  (Zheng et al,  2022 ) , where the additional full-sized matrices can be regarded as the scene-agnostic representation.",
            "The quantitative results on Cooking-Perturbation and MeetRoom-Perturbation are shown in Table  1  and Table  2  respectively. The observation is 2-fold. Firstly, while GeoNeRF and GNT learn stronger scene priors via advanced architectures, they demonstrate much inferior rendering performance when tackling test-time foreground movements as shown qualitative in Fig.  7 . Also, as shown in Fig.  1 , GeoNeRF produces extremely blurred results on Cooking-Perturbation and even renders all-black images on MeetRoom-Perturbation. This indicates that the stronger generalization ability cannot guarantee the robustness to small but unpredictable test-time scene changes. It also demonstrates that our benchmark, widely existing in daily life, is much more challenging than the conventional generalization-based settings. In contrast, methods with fewer scene priors and less sophisticated architectures output visually better results on Cooking-Perturbation, such as MVSNeRF and NeuRay. Note that MVSNeRF still struggles on MeetRoom-Perturbation. Secondly, quantitatve results show that our OPONeRF consistently achieves state-of-the-art results on all 3 metrics, which validates the effectiveness of the proposed method. With sufficient training samples for the mapping from point to parameters of neural renderer, OPONeRF is able to adapt to unseen changed foreground and preserve the background details with only a single training timeframe. Notably, on Cook Spinache, Flame Steak and VR Headset tasks, OPONeRF outperforms the second-best NeuRay by a sizable margin ( > > > 1.5 PSNR).",
            "Experimental Results and Analysis:   Quantitative results on the generalization benchmarks for spatial reconstruction are summarized in Table  7 . For fair comparison, the compared numbers are drawn from  (Liu et al,  2022 ; Wang et al,  2023b )  only while other baselines such as GeoNeRF  (Johari et al,  2022 )  and LIRF  (Huang et al,  2023 )  are trained on different databases. It can be seen that OPONeRF still achieve competitive results compared with advanced generalizable NeRFs such as GNT  (Wang et al,  2023b )  and GPNR  (Suhail et al,  2022 ) . Especially, it shows the best rendering results when measured by PSNR on all 3 test sets."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 :  Quantitative comparison with baseline methods on the generalization benchmarks for spatio-temporal reconstruction.",
        "table": "S4.T4.8.8",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "Step 2. Discriminative Parameter Personalization:   For each sampled  x x \\mathbf{x} bold_x , OPONeRF discriminatively personalizes its point-wise parameters of the target layers in the neural renderer. A naive attempt is to learn a totally unique renderer responsible for each 3D location. Suppose for an image with  h h h italic_h  the height,  w w w italic_w  the width and  N N N italic_N  the number of sampled points on a ray, it requires  h  w  N h w N h\\times w\\times N italic_h  italic_w  italic_N  set of learnable matrices for a single target layer which brings unacceptable computational and optimization overhead. Given this, we regard  W a l subscript superscript W l a \\mathbf{W}^{l}_{a} bold_W start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  as a parameter pool and select from it the proper ones via introducing an adaptiveness factor  A x  R L subscript A x superscript R L \\mathcal{A}_{\\mathbf{x}}\\in\\mathbb{R}^{L} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT  discriminative to each  x x \\mathbf{x} bold_x .  L L L italic_L  is the number of target layers in the OPONeRF renderer. We detail the calculation of  A x subscript A x \\mathbf{A}_{x} bold_A start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT  in Sec.  3.2.4 . Considering the large quantities of sampled points,  A x subscript A x \\mathcal{A}_{\\mathbf{x}} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  learns how to personalize the neural renderer in a layer-wise manner rather than element-wise masks  M l superscript M l M^{l} italic_M start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT .  A x subscript A x \\mathcal{A}_{\\mathbf{x}} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  adaptively employs the parameter candidates in Eqn. ( 8 ):",
            "The quantitative and qualitative results are summarized in Table  4  and Fig.  8  respectively. It can be seen that the overall performance on the synthetic dataset is much better than real-world benchmarks numerically. We infer it is due to the relatively low geometric complexity, so that the baselines with generalization power can well handle the test-time object movements. However, we observe abnormal shadows around both the static and moving objects in the rendered results of NeuRay, which indicates the confusion of illumination conditions at test-time while OPONeRF more accurately preserves the lighting conditions through illustrative comparisons. Quantitatively, in both backgrounds, OPONeRF shows substantially better numerical results in 3 evaluation metrics, regardless of the background color.",
            "Experimental Results and Analysis:   Quantitative results on the generalization benchmarks for spatio-temporal reconstruction are summarized in Table  8 . Our MonoNeRF implementation consistently improves the generalization to unseen motions upon the MonoNeRF baseline. Given better expressiveness and stronger flexibility, it can better model the sophisticated physical motions which widely exist in real world."
        ]
    },
    "id_table_9": {
        "caption": "Table 9 :   Experimental results of ablation studies on OPONeRF. Beef and White indicates the Cut Roasted Beef task of realistic benchmark and scene set with white background of synthetic benchmark. The metric is PSNR (   \\uparrow  ).",
        "table": "S4.T5.5",
        "footnotes": [],
        "references": [
            "where  W x l subscript superscript W l x \\mathbf{W}^{l}_{x} bold_W start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ,  A x  [ l ] subscript A x delimited-[] l \\mathcal{A}_{\\mathbf{x}}[l] caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT [ italic_l ] ,  Z l superscript Z l \\mathcal{Z}^{l} caligraphic_Z start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT  are the final parameters of the  l l l italic_l -th target layer personalized for  x x \\mathbf{x} bold_x ,  l l l italic_l -th element of  A x subscript A x \\mathcal{A}_{\\mathbf{x}} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  and another layer-variant code of the  l l l italic_l -th target layer respectively.  Z l superscript Z l \\mathcal{Z}^{l} caligraphic_Z start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT  is randomly initialized and iteratively updated similar to  Z l superscript Z l Z^{l} italic_Z start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT . We do not apply threshold-based quantization to binarize  A x subscript A x \\mathcal{A}_{\\mathbf{x}} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  but rather adopt soft weighted aggregation of  W a l subscript superscript W l a \\mathbf{W}^{l}_{a} bold_W start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  and  Z l superscript Z l \\mathcal{Z}^{l} caligraphic_Z start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT  in Eqn. ( 9 ). Although we aim to personalize unique renderer parameters for each  x x \\mathbf{x} bold_x , Eqn. ( 9 ) can be implemented with matrix multiplication (e.g.,  torch.bmm ) for a batched points to maintain the computational efficiency, which we show in the experiment section.",
            "We used the Kubric engine  (Greff et al,  2022 )  and followed the settings in Sec.  4.2 , except that we only modified the object existence(s). We constructed three synthetic scenes with 5,6 and 7 objects respectively. 21 viewpoints are captured in each scene, some of which are shown in Figure  9 .",
            "Efficacy of Designed Components:   The trial of removing adaptiveness factor  a x subscript a x \\mathbf{a}_{x} bold_a start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT  is indicated by ID 1 in Table  9 , where the responses to the test-time scene changes are solely guided by the shared geometric representation and probabilistic point representation. Compared with default implementation of OPONeRF, the trial achieves lower PSNR but still higher than NeuRay. This indicates that the adaptiveness to the given scene is truly related to spatial locality and requires discriminative learning w.r.t. sampled coordinates, which is effectively captured by our proposed adaptiveness factor. ID 2 represents using vanilla  f x subscript f x f_{\\mathbf{x}} italic_f start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  and  a x subscript a x a_{\\mathbf{x}} italic_a start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  and removing the usage of probabilistic inference, i.e.,  L r  e  c subscript L r e c \\mathcal{L}_{rec} caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_c end_POSTSUBSCRIPT  and  L a  p  p  r subscript L a p p r \\mathcal{L}_{appr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_p italic_p italic_r end_POSTSUBSCRIPT . The rendering quality drop by 1.50 PSNR and 1.10 PSNR on 2 scenes respectively, which indicates that OPONeRF can effectively respond to test-time scene changes via the unsupervised modeling of unexpected local variance.",
            "Way of Obtaining Adaptiveness Factor:   By default we firstly interpolate using spatial query then apply regression layer to obtain the adaptiveness factor  a x subscript a x \\mathbf{a}_{x} bold_a start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT  (ID 3 in Table  9 ). An alternative is to keep the fan-out dimension unchanged and directly regress  F x subscript F x \\mathcal{F}_{\\mathbf{x}} caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  into  A x subscript A x \\mathcal{A}_{\\mathbf{x}} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  and discard the fusion in Eqn. ( 15 ) (ID 4 in Table  9 ). This implementation shows inferior performance which implies the importance of learning a geometric-aware adaptiveness factor. Also, the default implementation models  f x subscript f x f_{\\mathbf{x}} italic_f start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  and  a x subscript a x a_{\\mathbf{x}} italic_a start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  separately which tackles the uncertainty in scene representation and the corresponding responses to scene variations respectively. Such a design choice outperforms the naive mapping from point to NeRF as indicated by the quantitative results.",
            "Flexibility of Learned Adaptiveness:   The flexibility of learned adaptiveness is largely controlled by the binary weight mask  M l superscript M l M^{l} italic_M start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT  and soft layer mask  A x subscript A x \\mathcal{A}_{\\mathbf{x}} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT . Here we ablated such flexibility by setting them as all-ones and all-zeros matrices respectively (ID 5 and 6 in Table  9 ). Note that ID 5 equals discarding all divide-and-conquer designs, which suffers from sensitivity to scene changes. The poorer results of trial ID 6 indicates the strategy of learning a set of layer-variant embeddings for candidate weights can effectively prevent performance degradation. After removing the quantization in the PCD module and use soft  M l superscript M l M^{l} italic_M start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT  (ID 7 in Table  9 ), significant performance drop is observed. This implies the rich geometric features condensed in parameter candidates can be better preserved via directly retrieving their individual elements. Then we tested the removal of  L d  i  v subscript L d i v \\mathcal{L}_{div} caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_v end_POSTSUBSCRIPT  (ID 8). The usage of  L d  i  v subscript L d i v \\mathcal{L}_{div} caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_v end_POSTSUBSCRIPT  strengthens the diverse response of OPONeRF to local variations, especially in synthetic database which shows larger performance decline after removing  L d  i  v subscript L d i v \\mathcal{L}_{div} caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_v end_POSTSUBSCRIPT  (   \\downarrow  0.80 v.s.    \\downarrow  0.31).",
            "Decomposition of Point Representation:   By default we represent the final  F x subscript F x \\mathcal{F}_{\\mathbf{x}} caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  as the combination of deterministic invariance ( f x I superscript subscript f x I f_{\\mathbf{x}}^{I} italic_f start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT ) and unexpected variance ( f x V superscript subscript f x V f_{\\mathbf{x}}^{V} italic_f start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT ), along with a residual connection ( f x subscript f x f_{\\mathbf{x}} italic_f start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT ). Here we ablated the component of  F x subscript F x \\mathcal{F}_{\\mathbf{x}} caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  via removing  f x I superscript subscript f x I f_{\\mathbf{x}}^{I} italic_f start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT  or  f x subscript f x f_{\\mathbf{x}} italic_f start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT , indicated by ID 9 and 10 respectively in Table  9 . We observe that the removal of residual connection or deterministic invariance causes enormous rendering degradation, of which the trial 9 presents a severer PSNR decline. We infer that while  f x I superscript subscript f x I f_{\\mathbf{x}}^{I} italic_f start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT  and  f x subscript f x f_{\\mathbf{x}} italic_f start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  are both supposed to model the deterministic mapping from the queried point to the renderer parameters,  f x subscript f x f_{\\mathbf{x}} italic_f start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  further helps with the steady gradients as the residual path  (He et al,  2016 ) ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10 :   Quantitative comparison between D-NeRF and OPONeRF on our Cooking-Perturbation benchmark, evaluated by mean LPIPS   \\downarrow  . The assumptions in D-NeRF do not hold in our experimental setting.",
        "table": "S4.T6.1.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "The quantitative results are shown in Table  6 . Both image-level and feature-level cause sizable performance degradation to MVSNeRF. NeuRay achieves favorable robustness against feature-level noises while OPONeRF even further eases the negative effects of test-time noises. On both metrics of PSNR and SSIM, OPONeRF achieves the best results among all the compared trials, which indicate that the flexibility brought by the One-Point-One-NeRF mechanism does not sacrifice the robustness to outlier noises. Notably, under test-time feature-level noises, OPONeRF impressively drops by less than  0.4 % percent 0.4 0.4\\% 0.4 %  on both PSNR and SSIM, where the noises can hardly be noticed as shown in Fig.  10 (b).",
            "Infeasibility of Spatio-temporal Construction in Our Benchmark:   To demonstrate the difficulties in spatio-temporal reconstruction in our settings where only a single timeframe is provided, we conducted experiments on our Cooking-Perturbation benchmark using D-NeRF and compared with OPONeRF. We experimented on the Cut Roasted Beef, Flame Steak and Sear Steak tasks. Quantitative results in Table.  10  validate that D-NeRF performs poorly in the absence of temporal variations, where the rendering quality of OPONeRF is improved significantly measured by LPIPS."
        ]
    },
    "id_table_11": {
        "caption": "Table 11 :  Quantitative comparison with baseline methods with and without incorporating OPONeRF into their renderer architectures. We did not test GeoNeRF or OPO-GeoNeRF on the VR Headset task because they both produce all-black results which are totally indistinguishable.",
        "table": "S4.T6.2.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Fig.  11  shows the qualitative results on the generalization benchmarks for spatial and spatio-temporal reconstruction. OPONeRF improves the detail preserving of unseen scenes or motions via the comparison with other baseline methods. Quantitatively, the improvement of OPONeRF is much more obvious in spatio-temporal reconstruction. We infer that our OPONeRF helps to significantly fertile the training samples from an ill-posed monocular video. In contrast, traditional generalization-based benchmarks for spatial reconstruction are provided with sufficient training views, which benefits the network design with stronger scene priors."
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "S4.T7.6.6",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Values of Weight Hyper-parameters:   The hyper-parameters    \\alpha italic_  in  F x subscript F x \\mathcal{F}_{\\mathbf{x}} caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  and    \\gamma italic_  in  F o  p  o subscript F o p o \\mathcal{F}_{opo} caligraphic_F start_POSTSUBSCRIPT italic_o italic_p italic_o end_POSTSUBSCRIPT  control the gradients of the final point representation and the balance between  L a  p  p  r subscript L a p p r \\mathcal{L}_{appr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_p italic_p italic_r end_POSTSUBSCRIPT  and  L r  e  c subscript L r e c \\mathcal{L}_{rec} caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_c end_POSTSUBSCRIPT . We tested different values and have summarized the results on Beef scene in Fig.  12 . We can see that the rendering with test-time changes is more sensitive when tuning the ingredients of  F x subscript F x \\mathcal{F}_{\\mathbf{x}} caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT . As for the balance between  L a  p  p  r subscript L a p p r \\mathcal{L}_{appr} caligraphic_L start_POSTSUBSCRIPT italic_a italic_p italic_p italic_r end_POSTSUBSCRIPT  and  L r  e  c subscript L r e c \\mathcal{L}_{rec} caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_c end_POSTSUBSCRIPT , we observe minor fluctuations when ablating    \\gamma italic_ . By default we set   = 0.3  0.3 \\alpha=0.3 italic_ = 0.3  and   = 1.0  1.0 \\gamma=1.0 italic_ = 1.0 ."
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "S4.T8.6.6",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Limitations of OPONeRF:   Given one training timestamp and not specifically designed for scenes with variable daylight  (Song and Funkhouser,  2019 ) , OPONeRF struggles at handling complex illuminations. As indicated by Fig.  13 , OPONeRF also produces blurred results when faced with significant scene changes, e.g., trained on Cook Spinache and evaluated on VR Headset. Moreover, compared with the recent Gaussian Splatting method  (Wu et al,  2024 ) , OPONeRF shares with other NeRF-based methods the slower training speed."
        ]
    },
    "id_table_14": {
        "caption": "",
        "table": "S4.T9.8.6",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "S4.T11.3.3",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "Way of Obtaining Adaptiveness Factor:   By default we firstly interpolate using spatial query then apply regression layer to obtain the adaptiveness factor  a x subscript a x \\mathbf{a}_{x} bold_a start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT  (ID 3 in Table  9 ). An alternative is to keep the fan-out dimension unchanged and directly regress  F x subscript F x \\mathcal{F}_{\\mathbf{x}} caligraphic_F start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  into  A x subscript A x \\mathcal{A}_{\\mathbf{x}} caligraphic_A start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  and discard the fusion in Eqn. ( 15 ) (ID 4 in Table  9 ). This implementation shows inferior performance which implies the importance of learning a geometric-aware adaptiveness factor. Also, the default implementation models  f x subscript f x f_{\\mathbf{x}} italic_f start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  and  a x subscript a x a_{\\mathbf{x}} italic_a start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT  separately which tackles the uncertainty in scene representation and the corresponding responses to scene variations respectively. Such a design choice outperforms the naive mapping from point to NeRF as indicated by the quantitative results."
        ]
    },
    "id_table_16": {
        "caption": "",
        "table": "S4.T11.6.3",
        "footnotes": [
            "",
            ""
        ],
        "references": []
    }
}