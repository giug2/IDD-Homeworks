{
    "PAPER'S NUMBER OF TABLES": 3,
    "S3.T1": {
        "caption": "Table 1: Evaluation on CIFAR-100 dataset.\n",
        "table": "",
        "footnotes": "\n\n\n\n\nAverage Accuracy\nAverage forgetting\nTraining time (s)\nTraining time (s)\nServer Runtime (s)\n\n\n𝒜~~𝒜\\tilde{\\mathcal{A}} (%)\nf~~𝑓\\tilde{f}(%)\n(T=0𝑇0T=0)\n(T≥1𝑇1T\\geq 1)\n\n\n\n\nFedAvg\n22.27±0.22plus-or-minus22.270.2222.27\\pm 0.22\n78.77±0.83plus-or-minus78.770.8378.77\\pm 0.83\n≈1.2absent1.2\\approx 1.2\n≈1.2absent1.2\\approx 1.2\n≈1.8absent1.8\\approx 1.8\n\nFedProx\n22.00±0.31plus-or-minus22.000.3122.00\\pm 0.31\n78.17±0.33plus-or-minus78.170.3378.17\\pm 0.33\n≈1.98absent1.98\\approx 1.98\n≈1.98absent1.98\\approx 1.98\n≈1.8absent1.8\\approx 1.8\n\nFedCIL\n26.8±0.44plus-or-minus26.80.4426.8\\pm 0.44\n38.19±0.31plus-or-minus38.190.3138.19\\pm 0.31\n≈17.8absent17.8\\approx 17.8\n≈24.5absent24.5\\approx 24.5\n≈2.5absent2.5\\approx 2.5 for T=1𝑇1T=1, ≈4.55absent4.55\\approx 4.55 for T>1𝑇1T>1\n\nFedLwF-2T\n22.17±0.13plus-or-minus22.170.1322.17\\pm 0.13\n75.08±0.72plus-or-minus75.080.7275.08\\pm 0.72\n≈1.2absent1.2\\approx 1.2\n≈3.4absent3.4\\approx 3.4\n≈1.8absent1.8\\approx 1.8\n\nMFCL (Ours)\n43.87±0.12plus-or-minus43.870.12\\mathbf{43.87\\pm 0.12}\n28.3±0.78plus-or-minus28.30.78\\mathbf{28.3\\pm 0.78}\n≈1.2absent1.2\\approx 1.2\n≈3.7absent3.7\\approx 3.7\n≈330absent330\\approx 330 (once per task), ≈1.8absent1.8\\approx 1.8 O.W.\n\nOracle\n67.12±0.4plus-or-minus67.120.467.12\\pm 0.4\n−⁣−--\n≈1.2absent1.2\\approx 1.2\n≈1.2×Tabsent1.2𝑇\\approx 1.2\\times\\ T\n≈1.8absent1.8\\approx 1.8\n\n",
        "references": [
            "Table 1 shows each method’s average forgetting and accuracies. FedAvg and FedProx have the highest forgetting as they are not designed for FCL. Also, high forgetting for FedLwF-2T indicates that extra teachers cannot be effective in the absence of old data. FedCIL and MFCL have lower forgetting and better accuracy. MFCL outperforms FedCIL because the generative models in FedCIL need to train for a long time to generate effective synthetic data."
        ]
    },
    "A1.T2": {
        "caption": "Table 2: Generative model Architecture",
        "table": "",
        "footnotes": "",
        "references": [
            "In Table 2, we show the generative model architectures used for CIFAR-100. The global model has ResNet18 architecture, we change the first CONV layer kernel size to 3×3333\\times 3 from 7×7777\\times 7.\nIn this table, CONV layers are reported as CONV​K×K​(Ci​n,Co​u​t)CONV𝐾𝐾subscript𝐶𝑖𝑛subscript𝐶𝑜𝑢𝑡\\texttt{CONV}K\\times K(C_{in},C_{out}), where K𝐾K, Ci​nsubscript𝐶𝑖𝑛C_{in} and Co​u​tsubscript𝐶𝑜𝑢𝑡C_{out} are the size of the kernel, input channel and output channel of the layer, respectively."
        ]
    },
    "A1.T3": {
        "caption": "Table 3: Parameter Settings in different datasets",
        "table": "",
        "footnotes": "",
        "references": [
            "Table 3 presents some of the more important parameters."
        ]
    }
}