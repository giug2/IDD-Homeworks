{
    "PAPER'S NUMBER OF TABLES": 3,
    "S3.T1": {
        "caption": "TABLE I: Class Counts for training and testing data",
        "table": "<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S3.T1.1.1.1.1.1\" class=\"ltx_text\">Class</span></th>\n<th id=\"S3.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S3.T1.1.1.1.2.1\" class=\"ltx_text\">Object</span></th>\n<th id=\"S3.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\">Instances</th>\n</tr>\n<tr id=\"S3.T1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Training</th>\n<th id=\"S3.T1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Testing</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">0</th>\n<th id=\"S3.T1.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Caries</th>\n<td id=\"S3.T1.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2051</td>\n<td id=\"S3.T1.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">35</td>\n</tr>\n<tr id=\"S3.T1.1.4.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">1</th>\n<th id=\"S3.T1.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Ulcer</th>\n<td id=\"S3.T1.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">364</td>\n<td id=\"S3.T1.1.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5</td>\n</tr>\n<tr id=\"S3.T1.1.5.3\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.5.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">2</th>\n<th id=\"S3.T1.1.5.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Tooth Discoloration</th>\n<td id=\"S3.T1.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">315</td>\n<td id=\"S3.T1.1.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">18</td>\n</tr>\n<tr id=\"S3.T1.1.6.4\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.6.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">3</th>\n<th id=\"S3.T1.1.6.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">Gingivitis</th>\n<td id=\"S3.T1.1.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">653</td>\n<td id=\"S3.T1.1.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">106</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "The dental condition dataset is a collection of images curated specifically for dental research and analysis. The dataset comprises a total of 1058 images and four classes encompassing a wide range of dental conditions including Caries, Gingivitis, tooth discoloration, and ulcers. The images are sourced from multiple hospitals and dental websites. This is made to ensure the authenticity and diversity of the dental conditions.\nThe dataset is curated using MakeSense AI, which provides a user-friendly interface for annotating and augmenting images. The labeling of data is done in YOLO labeling format with object class references mentioned in Table I.\nFurthermore, the dataset is divided into 1465 training images and 43 testing images as displayed in Table I."
        ]
    },
    "S4.T2": {
        "caption": "TABLE II: Performance Metrics of Federated Framework \nNumber of Communication rounds required to achieve the target mAP \nTarget mAP = 80%",
        "table": "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">E</th>\n<th id=\"S4.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Rounds</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Federated - YOLOv5</td>\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S4.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">168</td>\n</tr>\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Federated - YOLOv5</td>\n<td id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">5</td>\n<td id=\"S4.T2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">59</td>\n</tr>\n<tr id=\"S4.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Federated - YOLOv5</td>\n<td id=\"S4.T2.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">10</td>\n<td id=\"S4.T2.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">122</td>\n</tr>\n<tr id=\"S4.T2.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Federated - YOLOv8</td>\n<td id=\"S4.T2.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">1</td>\n<td id=\"S4.T2.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">64</td>\n</tr>\n<tr id=\"S4.T2.1.6.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Federated - YOLOv8</td>\n<td id=\"S4.T2.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">5</td>\n<td id=\"S4.T2.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">51</td>\n</tr>\n<tr id=\"S4.T2.1.7.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\">Federated - YOLOv8</td>\n<td id=\"S4.T2.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">10</td>\n<td id=\"S4.T2.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">97</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "This section discusses the performance of the object detection models YOLOv5 and YOLOv8 when trained using local data alone and when federated learning is used. The performance metrics used in evaluating the two frameworks are:",
                "Mean Average Precision",
                "F1-Score",
                "Given below are the mathematical representations of the evaluation metrics:",
                "As evident from Tables ",
                "II",
                " and ",
                "III",
                ", and discussed in ",
                "[",
                "9",
                "]",
                ", initially a larger E value gave faster rates of achieving the target mAP. But as the dataset is more non-IID, different values of E produce different rates of convergence.",
                "The YOLOv8 model contains significant improvements over its preceding YOLOv5 model. The YOLOv8 model utilizes an anchor-free model to predict the center of an object without using an offset from the anchor box. Additionally, the YOLOv8 model shows notable enhancement in accuracy when tested on the COCO dataset ",
                "[",
                "23",
                "]",
                ". On comparing the two YOLO models on locally trained data, it is evident that the YOLOv8 model performs better than the YOLOv5 model as tabulated in Table ",
                "III",
                ".",
                "Mean Average Precision(mAP) is an evaluation metric often used to evaluate the performance of an object detection model. It utilizes the concept of Intersection over Union(IoU) to measure the overlap of the predicted object boundary against the ground truth boundary. A threshold of 0.5 was used for IoU during the experiments. The precision obtained from this is calculated for all objects and grouped and averaged on the basis of the class to which they belong.",
                "Calculation of the mean of the average precision values provides the final mean average precision metric. Eq. ",
                "2",
                " provides the mathematical description of the metric where AP refers to the average precision of each class and n refers to the total number of classes.\nThe locally trained framework produced an mAP value of 72.6 after the completion of 100 epochs on the YOLOv5 model and a value of 78.8 after the completion of 100 epochs on the YOLOv8 model while the federated learning framework achieved a target mAP of 80 after a certain number of communication rounds with the server depending on the value of E and the model in use as evident from Tables ",
                "II",
                " and ",
                "III",
                " as well as Figures ",
                "3",
                " and ",
                "4",
                ". This indicates the overall better performance of the federated framework.",
                "The F1-score is a machine learning performance metric that utilizes the precision and recall metrics. Popularly used for object detection in imbalanced datasets, it provides an integration of the precision and recall metrics and is hence used as an ideal measure of performance for object detection purposes. Eq. ",
                "3",
                " provides the mathematical description of F1-Score.",
                "As evident from Table ",
                "III",
                " the YOLOv8 model provides a better F1-score in comparison to the YOLOv5 model and is hence considered an overall better model for implementation of object detection for this use-case."
            ]
        ]
    },
    "S4.T3": {
        "caption": "TABLE III: Performance Metrics of Local Training",
        "table": "<table id=\"S4.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S4.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">F1-Score</th>\n<th id=\"S4.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">mAP</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">YOLOv5</td>\n<td id=\"S4.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">75.7%</td>\n<td id=\"S4.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">72.6%</td>\n</tr>\n<tr id=\"S4.T3.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\">YOLOv8</td>\n<td id=\"S4.T3.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T3.1.3.2.2.1\" class=\"ltx_text ltx_font_bold\">82.3%</span></td>\n<td id=\"S4.T3.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T3.1.3.2.3.1\" class=\"ltx_text ltx_font_bold\">78.8%</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "The YOLOv8 model contains significant improvements over its preceding YOLOv5 model. The YOLOv8 model utilizes an anchor-free model to predict the center of an object without using an offset from the anchor box. Additionally, the YOLOv8 model shows notable enhancement in accuracy when tested on the COCO dataset [23]. On comparing the two YOLO models on locally trained data, it is evident that the YOLOv8 model performs better than the YOLOv5 model as tabulated in Table III.",
            "As evident from Table III the YOLOv8 model provides a better F1-score in comparison to the YOLOv5 model and is hence considered an overall better model for implementation of object detection for this use-case."
        ]
    }
}