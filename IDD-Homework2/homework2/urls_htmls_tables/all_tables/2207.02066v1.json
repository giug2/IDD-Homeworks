{
    "S4.T1": {
        "caption": "Table 1: The results of pre-training using different multi-task network architecture (top row) with reconstruction loss as the auxiliary loss LA​u​x=LR​e​csubscript𝐿𝐴𝑢𝑥subscript𝐿𝑅𝑒𝑐L_{Aux}=L_{Rec}. We also investigate the results of using different auxiliary losses with MAXL (bottom row) using our multi-task network architecture. Aux stands for auxiliary. All of the experiments are trained with the primary loss LP​r​isubscript𝐿𝑃𝑟𝑖L_{Pri} and the specified auxiliary loss LA​u​xsubscript𝐿𝐴𝑢𝑥L_{Aux}.",
        "table": null,
        "footnotes": [],
        "references": [
            "In this experiment, we conduct two different experiments to demonstrate our contribution.\nWe train the network using DIV2K dataset with synthetic degradation.\nFor the evaluation, we conduct the validation using validation images from DIV2K synthetic degradation, while testing using images from the Nam dataset with real noise.\nQuantitative results in terms of PSNR and SSIM metrics can be seen in Table 1.",
            "In the first experiment, we compare two different architectures of the multi-task network fθ1subscript𝑓subscript𝜃1f_{\\theta_{1}} to evaluate which network will get more benefits when trained using auxiliary loss.\nThe first architecture is our proposed architecture which uses a sequential design where the auxiliary head is placed after the primary head.\nMeanwhile, the second architecture is the architecture from (Chi et al., 2021) which is a single feature extractor with two parallel heads (primary & auxiliary).\nThe details of this architecture can be seen in Figure 4 in the supplementary material.\nBoth networks have a similar number of parameters.\nTo train the network, we use the primary loss and change the auxiliary loss to reconstruction loss LR​e​csubscript𝐿𝑅𝑒𝑐L_{Rec}.\nWe change the loss to measure the capability of the multi-task network in an isolated manner without any effect from the mask generation network gθ2subscript𝑔subscript𝜃2g_{\\theta_{2}}.\nThe results can be seen in the top row of Table 1 where our proposed architecture outperforms the other architectures by 0.3 dB on validation and 2.3 dB on the testing PSNR.\nBased on the SSIM metric, our architecture also achieves better validation SSIM but lower testing SSIM.\nThis shows that our architecture achieves better generalization when trained with auxiliary loss compared to the baseline especially in the case of unseen real noise.\nThe worse results of the network architecture from (Chi et al., 2021) may be due to the placement of the auxiliary head that can hurt the performance when placed after the feature extractor i.e. negative transfer issue problem.",
            "After validating that our proposed architecture is a better option when trained with an auxiliary loss LA​u​xsubscript𝐿𝐴𝑢𝑥L_{Aux}, we compare different auxiliary loss functions trained with MAXL.\nWhen trained with MAXL (bottom row of Table 1), the validation and testing performance of using masked reconstruction loss (LM​a​s​k​R​e​csubscript𝐿𝑀𝑎𝑠𝑘𝑅𝑒𝑐L_{MaskRec}) are consistently better compared to using reconstruction loss (LR​e​csubscript𝐿𝑅𝑒𝑐L_{Rec}).\nSince using MAXL also cannot prevent the collapse situation, we use two-way image gradient loss (LGsubscript𝐿𝐺L_{G}) to regularize the mask thus preventing the collapse situation.\nThis loss further improves the generalization to unseen noise by improving both PSNR and SSIM scores but decreases the validation performance.\nSince our goal is to improve the generalization of multi-task networks on unseen real noise, we use the network that achieves the best testing performance to be fine-tuned using the meta-transfer learning algorithm."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: The results of meta-transfer learning using the proposed algorithm compared with other SOTA methods.",
        "table": null,
        "footnotes": [],
        "references": [
            "Quantitative comparison can be seen in Table 2.\nThe results show that using meta-testing to adapt the network for each image in the evaluation dataset can consistently improve the validation performance and testing performance.\nIn addition, we can see that our method achieve the best result compared to SOTA methods.\nCompared to the MIRNet result, our method with meta-testing improves the validation performance by 0.46 dB and 0.0027 in terms of PSNR and SSIM respectively.\nThe testing performance in terms of PSNR also improves by 0.28 dB.\nInterestingly, even though the number of parameters of our network is very small compared to other SOTAs (∼similar-to\\sim2x and 48x compared to RIDNet and MIRNet respectively), we can still outperform the SOTA method even though the SSIM score is slightly lower compared to MIRNet.\nThis shows a promising research direction where we can pursue an improvement in denoising tasks by modifying the training algorithm where the current trend of the SOTA methods is dominated by the improvement in the design of network architecture.\nThe reason for slight performance gain after adaptation (0.03 dB) is likely due to the small learning capacity of the network where the number of parameters of the adapted head is only 0.12 M which can be considered small.\nWe conjecture that better improvement can be observed if we increase the number of parameters by making the larger network.\nHowever, due to the limitation of our machine, we cannot experiment using a larger network because of the multi-gradient computation.\nWe conduct an additional feature visualization study (Section B.3 in the supplementary material) which shows a high difference in feature maps after adaptation."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: The results of the ablation study. Top row: the results of fine-tuning with meta-transfer learning (MTL) using different losses on our MAXL pre-trained network. Middle row: the results of fine-tuning using MTL on a Randomly Initialized (RI) network. Bottom row: the results of updating all of the network parameters instead of primary and auxiliary heads only. All of the results are the result after running meta-testing.",
        "table": null,
        "footnotes": [],
        "references": [
            "In this study, we investigate the impact of our proposed method such as the impact of MAXL pre-training, masked reconstruction loss, and updating only the head of the multi-task network.\nThe top row and middle row of Table 3 shows that training with masked reconstruction loss objective consistently outperforms reconstruction loss, especially on the testing performance.\nThe proposed masked reconstruction loss can improve the testing performance around 0.1 dB which shows that the network can achieve better generalization when trained using this objective, especially when coupled with the meta-transfer learning.\nDifferent from the results of training using MAXL, we observe no collapse situation throughout fine-tuning both on MAXL pre-trained network and randomly initialized network.\nFigure 5 and Figure 6 in the supplementary material show the evolution of the mask through training where the mask constantly evolves in early training then becomes constant when the multi-task network starts to converge (i.e. after epoch 100).\nThis shows the benefit of masked reconstruction loss that can provide the benefit of choosing the certain region that needs to be reconstructed so it can improve the performance of the primary task in different stages of learning.",
            "To study the impact of pre-training, the result in the top row and middle row of Table 3 can be compared.\nThe results show that pre-training is indeed required to achieve better performance both on validation and testing.\nNote that the performance gap is not caused by the convergence issue since both settings (fine-tune on the pre-trained network and randomly initialized network) are already converged.\nThe bottom row of Table 3 again consolidates this fact where the gap between the result of fine-tuning on the pre-trained network compared to the randomly initialized network is around 1.5 dB both on validation and testing performance.",
            "The last study is to investigate the impact of updating only the multi-task network’s head in the inner loop of the fine-tuning stage.\nThe result can be seen in the top row and bottom row of Table 3.\nValidation performance of updating only head compared to updating all of the network parameters is similar.\nHowever, the testing performance in terms of PSNR can be improved by 0.07 even though the SSIM scores slightly drop.\nThis denotes that both updating only head and all of the network parameters in the inner loop of fine-tuning achieve similar results.\nSimilar observation also can be seen in (Raghu et al., 2020)."
        ]
    }
}