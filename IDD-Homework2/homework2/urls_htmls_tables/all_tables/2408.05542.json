{
    "id_table_1": {
        "caption": "TABLE I :  Structure of query augmentation prompt.",
        "table": "S3.T1.4",
        "footnotes": [],
        "references": [
            "The overall framework of  ChatDANCE  is presented in Figure  1 . Our approach consists of three stages: (a) data augmentation via ChatGPT, (b) data filtering, and (c) model training. In part (a) of Figure  1 , we demonstrate how we achieve data augmentation using ChatGPT. Given a query-code pair, we first construct query and code augmentation prompts using different prompt templates for the query and code, respectively. The prompt template is crucial for effectively interacting with ChatGPT. It includes the definition of the data rewriting task, important additional information such as the number of augmented data, and prior knowledge for completing the task. We then request ChatGPT to rewrite the original data based on the prompt information, generating augmented data. Finally, we extract the augmented data from ChatGPTs response using regular expressions.",
            "This section introduces our straightforward and effective data augmentation framework via ChatGPT for augmenting training data on code search. The framework consists of three subsequent stages,  the data augmentation stage ,  the data filtering stage , and  the model training stage . An overview of the framework, when applied to augment query-code pairs in the training dataset, is shown in Figure  1 .",
            "Following the prompt schema in Section  III-A 1 , We design different prompts based on the characteristics of query and code data, respectively. Next, we separately elaborate on the content of the query prompt and code prompt.",
            "Query Augmentation Prompt:  The details of the query augmentation prompt is shown in Table  I . (1)  Instruction : We aim to generate augmented queries through data reformulation to increase the diversity of queries. Therefore, we request ChatGPT reformulate the query without changing its original semantics. (2)  Emphasis : We firstly specify to ChatGPT the desired quantity of generated queries. And then, we illustrate to ChatGPT using the CoSQA dataset example that queries are brief, ensuring that it generates concise queries. (3)  Caution : To further ensure the brevity of queries, we limit the length of the augmented query by Equation  1 . In the experiment, we set the    \\alpha italic_  to 1.6.",
            "Figure  2  shows the five augmented code examples generated by ChatGPT under the guidance of the five proposed code rewriting techniques. In Figure  1(b) , it is shown that ChatGPT can understand the meaning of the code and generate augmented data by rewriting the function name from  get_tri_area  to  calculate_triangle_area . Figure  1(c)  shows that ChatGPT can understand variable abbreviations and convert them into semantically precise variable names, such as converting  pts  to  points . In Figure  1(d) , ChatGPT implements the function using a different library function,  math.sqrt .  In Figure  1(e) , ChatGPT generates enhanced code by using a different library function,  cdst , and a different mathematical formula while ensuring that the semantics remain unchanged. Finally, in Figure  1(f) , we see that ChatGPT simplifies the code snippet by removing statement 2 from the original code.",
            "After obtaining the augmented data, we filter the data and generate the augmented training dataset using the algorithm shown in Algorithm  1 .  We first train a neural model to measure the semantic relevance between a query and a code snippet, and then filter out code and query pairs with low semantic relevance scores. Specifically,  in line 2, we first train a filtering model  M M M italic_M  based on cross-encoder architecture on training dataset  D D D italic_D . In line 3-21, we generate an augmented sample for each query-code pair  < q , c > <q,c> < italic_q , italic_c >  in training dataset  D D D italic_D . In line 4, we initialize a list to collect the filtered code. In line 5-12, we iterate through the augmented codes and score the original query  q q q italic_q  and augmented codes using the filtering model. Then we filter out the augmented code  c a  u  g subscript c a u g c_{aug} italic_c start_POSTSUBSCRIPT italic_a italic_u italic_g end_POSTSUBSCRIPT  with a score below the threshold, and we synthesize augmented samples by combining  q q q italic_q  and  q a  u  g subscript q a u g q_{aug} italic_q start_POSTSUBSCRIPT italic_a italic_u italic_g end_POSTSUBSCRIPT . Meanwhile, we add the filtered codes to  c  o  d  e  _  l  i  s  t c o d e _ l i s t code\\_list italic_c italic_o italic_d italic_e _ italic_l italic_i italic_s italic_t . Similarly, we filter the queries using the same method in line 13-20. But note that in line 17-18, we generate an augmented sample by combining augmented query  q a  u  g subscript q a u g q_{aug} italic_q start_POSTSUBSCRIPT italic_a italic_u italic_g end_POSTSUBSCRIPT  and  c s  a  m  p  l  e subscript c s a m p l e c_{sample} italic_c start_POSTSUBSCRIPT italic_s italic_a italic_m italic_p italic_l italic_e end_POSTSUBSCRIPT  sampled from  c  o  d  e  _  l  i  s  t c o d e _ l i s t code\\_list italic_c italic_o italic_d italic_e _ italic_l italic_i italic_s italic_t  randomly to increase the diversity of augmented samples. In our experiments, we have manually checked the quality of the augmented data, and the results show that the augmented code can correctly answer the query. Details can be found in Appendix of replication package [ 42 ] ."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II :  Structure of code augmentation prompt.",
        "table": "S3.T1.4.2.2.1",
        "footnotes": [],
        "references": [
            "We use the ChatGPT API (gpt-3.5-turbo-0301) with default parameter settings to perform data augmentation. For query augmentation, we request ChatGPT to generate 15 augmented queries for each query based on the prompt template shown in Table  I . For code augmentation, we utilize five different rewriting techniques mentioned in Section  III-A 2  to create five prompts based on the templates shown in Table  II . We generated 15 augmented codes per original code, using each of the five prompts to generate three new codes. After receiving a response from ChatGPT, we use regular expressions to extract the generated data.",
            "Figure  2  shows the five augmented code examples generated by ChatGPT under the guidance of the five proposed code rewriting techniques. In Figure  1(b) , it is shown that ChatGPT can understand the meaning of the code and generate augmented data by rewriting the function name from  get_tri_area  to  calculate_triangle_area . Figure  1(c)  shows that ChatGPT can understand variable abbreviations and convert them into semantically precise variable names, such as converting  pts  to  points . In Figure  1(d) , ChatGPT implements the function using a different library function,  math.sqrt .  In Figure  1(e) , ChatGPT generates enhanced code by using a different library function,  cdst , and a different mathematical formula while ensuring that the semantics remain unchanged. Finally, in Figure  1(f) , we see that ChatGPT simplifies the code snippet by removing statement 2 from the original code."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III :  Augmented query samples generated by ChatGPT query augmentation.",
        "table": "S3.T1.4.3.2.1",
        "footnotes": [],
        "references": [
            "As an example, Table  III-A 3  shows the five augmented queries generated by ChatGPT through the rewriting of the original query. We can see that ChatGPT has a good understanding of the semantics of the query and can produce high-quality and diverse queries that preserve the original semantics by replacing synonyms, changing syntax structures, and using other techniques. Overall, the generated queries are of high quality and exhibit good diversity.",
            "In code search, we use deep learning models to score query-code pairs as the basis for their matching. There are two common architectures for code search models: bi-encoder  [ 9 ,  10 ,  11 ,  15 ,  16 ,  26 ,  27 ,  28 ,  18 ,  19 ,  20 ,  12 ,  22 ]  and cross-encoder  [ 17 ,  25 ,  34 ,  13 ,  21 ,  23 ,  14 ] . We denote bi-encoder model as  f b  i subscript f b i f_{bi} italic_f start_POSTSUBSCRIPT italic_b italic_i end_POSTSUBSCRIPT  and cross-encoder model as  f c  r  o  s  s subscript f c r o s s f_{cross} italic_f start_POSTSUBSCRIPT italic_c italic_r italic_o italic_s italic_s end_POSTSUBSCRIPT . As shown in Figure  3 (a), given a query-code pair  < Q , C > <Q,C> < italic_Q , italic_C > , the bi-encoder model encodes the query sequence and code sequence into query vector  q   q \\vec{q} over start_ARG italic_q end_ARG  and code vector  c   c \\vec{c} over start_ARG italic_c end_ARG , respectively:",
            "For the cross-encoder model shown in Figure  3 (b), we first concatenate the query sequence  Q Q Q italic_Q  and code sequence  C C C italic_C  into a single sequence and then input it into the cross-encoder model to generate the matching score end-to-end:",
            "NatGen  (De-Naturalizing Source Code)  [ 39 ]  performs code augmentation by rewriting code based on six semantic-preserving transformations:  (1) Loop Transformation, (2) DeadCode Injection, (3) Operand Swap, (4) Block Swap, (5) Variable Renaming, (6) Confusing Code Insertion. We use the first five transformations as we find the last transformation is ineffective for the codes in the training set. Similar to code augmentation mentioned in Section  III-A 3 , we use each transformation rule three times and generate 15 augmented codes in total."
        ]
    },
    "id_table_4": {
        "caption": "(a)   Original code",
        "table": "S3.T1.4.4.2.1",
        "footnotes": [],
        "references": [
            "Figure  4  presents the top-1 code snippets returned by QRA, NatGen, and  ChatDANCE  for the query how to remove blank lines from a text file in python. We can see that  ChatDANCE  returns the correct code snippet, which reads the file content and removes blank lines. In contrast, the code snippet returned by QRA and NatGen is incorrect, which can remove blank lines but fails to operate on a file.",
            "We investigate the impact of hyper-parameters including query filtering threshold   q subscript  q \\theta_{q} italic_ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , code filtering threshold   c subscript  c \\theta_{c} italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , the average number of augmentations per sample  N a  u  g subscript N a u g N_{aug} italic_N start_POSTSUBSCRIPT italic_a italic_u italic_g end_POSTSUBSCRIPT , and learning rate  l  r l r lr italic_l italic_r . We conduct the experiments within ranges surrounding the default values, and the results are shown in Figure  5 .The results show that the performance preserves stable as   q subscript  q \\theta_{q} italic_ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  varies from 0.7 to 0.95. Similarly,  the results show that the performance remains stable when   c subscript  c \\theta_{c} italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  varies from 0.7 to 0.9. However, we observe a sharp decline in performance when   c subscript  c \\theta_{c} italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  changes from 0.9 to 0.95. This is because when   c subscript  c \\theta_{c} italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  is set to 0.95, a large number of code augmentation samples are filtered out, leading to a significant decrease in the total number of samples and thus a decline in performance. In Figure  4(c) , we find that the performance improves significantly as the average number of augmented samples per original sample increases from 0 to 5. Beyond this point, the performance improvement gradually stabilizes. Finally, from Figure  4(d) , we observe that  ChatDANCE  is stable when learning rate varies from 1e-5 to 5e-5."
        ]
    },
    "id_table_5": {
        "caption": "(b)   Rewrite method name",
        "table": "S3.T2.4",
        "footnotes": [],
        "references": [
            "We investigate the impact of hyper-parameters including query filtering threshold   q subscript  q \\theta_{q} italic_ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , code filtering threshold   c subscript  c \\theta_{c} italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , the average number of augmentations per sample  N a  u  g subscript N a u g N_{aug} italic_N start_POSTSUBSCRIPT italic_a italic_u italic_g end_POSTSUBSCRIPT , and learning rate  l  r l r lr italic_l italic_r . We conduct the experiments within ranges surrounding the default values, and the results are shown in Figure  5 .The results show that the performance preserves stable as   q subscript  q \\theta_{q} italic_ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  varies from 0.7 to 0.95. Similarly,  the results show that the performance remains stable when   c subscript  c \\theta_{c} italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  varies from 0.7 to 0.9. However, we observe a sharp decline in performance when   c subscript  c \\theta_{c} italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  changes from 0.9 to 0.95. This is because when   c subscript  c \\theta_{c} italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  is set to 0.95, a large number of code augmentation samples are filtered out, leading to a significant decrease in the total number of samples and thus a decline in performance. In Figure  4(c) , we find that the performance improves significantly as the average number of augmented samples per original sample increases from 0 to 5. Beyond this point, the performance improvement gradually stabilizes. Finally, from Figure  4(d) , we observe that  ChatDANCE  is stable when learning rate varies from 1e-5 to 5e-5."
        ]
    },
    "id_table_6": {
        "caption": "(c)   Rewrite variables",
        "table": "S3.T2.4.2.2.1",
        "footnotes": [],
        "references": [
            "The  l a  l  i  g  n subscript l a l i g n \\ell_{align} roman_l start_POSTSUBSCRIPT italic_a italic_l italic_i italic_g italic_n end_POSTSUBSCRIPT - l u  n  i  f  o  r  m  i  t  y subscript l u n i f o r m i t y \\ell_{uniformity} roman_l start_POSTSUBSCRIPT italic_u italic_n italic_i italic_f italic_o italic_r italic_m italic_i italic_t italic_y end_POSTSUBSCRIPT  plot shown in Figure  6  reveals several observations. Firstly, our approach reduces both  l a  l  i  g  n  m  e  n  t subscript l a l i g n m e n t \\ell_{alignment} roman_l start_POSTSUBSCRIPT italic_a italic_l italic_i italic_g italic_n italic_m italic_e italic_n italic_t end_POSTSUBSCRIPT  and  l u  n  i  f  o  r  m  i  t  y subscript l u n i f o r m i t y \\ell_{uniformity} roman_l start_POSTSUBSCRIPT italic_u italic_n italic_i italic_f italic_o italic_r italic_m italic_i italic_t italic_y end_POSTSUBSCRIPT  compared to the two baselines. This implies that our approach can enhance both the alignment and uniformity of the learned representations, resulting in superior performance and generalization. In contrast, the baselines only reduce the uniformity loss but increase the alignment loss, indicating a deterioration in the alignment of the learned representations. Therefore, we believe that better alignment and uniformity are crucial factors in achieving superior performance and generalization in our approach. Secondly, our approach can simultaneously improve the alignment and uniformity of the learned representations, even when data augmentation is applied only to either code or query. In contrast, the baselines fail to achieve this. We attribute this success to our approachs ability to generate a large amount of high-quality and diverse data, thereby improving uniformity while ensuring the quality and diversity of the generated data. This allows the model to better comprehend the semantic meaning of both query and code, ultimately improving the alignment of the learned representations.",
            "We visualize the distribution of representations learned by four models shown in Figure  7  to intuitively explore why our approach works. First, we sample 300 query-code pairs from the test set of CosQA and obtain their representations in the high-dimensional vector space by feeding them into the model. Then, we use t-SNE  [ 49 ]  to perform dimensionality reduction on the representations and visualize their distribution. The experimental results are shown in Figure  7 . We visualized the distribution of representations learned by four models: UniXcoder, QRA, NatGen, and  ChatDANCE , which are shown in Figure  6(a) ,  6(b) ,  6(c) , and  6(d) , respectively. The red dots represent the code, and the blue dots represent the query. The lines between the red and blue dots indicate the distance between the code and the query.",
            "From the experimental results, we can observe that: (1) In Figure  6(a) ,  6(b) ,  6(c) , and  6(d) , most of the green lines are very short, indicating that for most query-code pairs, the model is able to align their representations in the high-dimensional vector space to close locations. (2) In Figure  7 , compared to the other three figures, Figure  6(d)  has the fewest long green lines. This suggests that our approach can enable the model to have better alignment compared to the other methods, as there is the fewest number of representations with long distances. Overall, the visualization results intuitively demonstrate that our approach can enable the model to learn better representations compared to the baselines by improving the alignment and uniformity of the learned representations."
        ]
    },
    "id_table_7": {
        "caption": "(d)   Use different library functions",
        "table": "S3.T2.4.3.2.1",
        "footnotes": [],
        "references": [
            "We visualize the distribution of representations learned by four models shown in Figure  7  to intuitively explore why our approach works. First, we sample 300 query-code pairs from the test set of CosQA and obtain their representations in the high-dimensional vector space by feeding them into the model. Then, we use t-SNE  [ 49 ]  to perform dimensionality reduction on the representations and visualize their distribution. The experimental results are shown in Figure  7 . We visualized the distribution of representations learned by four models: UniXcoder, QRA, NatGen, and  ChatDANCE , which are shown in Figure  6(a) ,  6(b) ,  6(c) , and  6(d) , respectively. The red dots represent the code, and the blue dots represent the query. The lines between the red and blue dots indicate the distance between the code and the query.",
            "From the experimental results, we can observe that: (1) In Figure  6(a) ,  6(b) ,  6(c) , and  6(d) , most of the green lines are very short, indicating that for most query-code pairs, the model is able to align their representations in the high-dimensional vector space to close locations. (2) In Figure  7 , compared to the other three figures, Figure  6(d)  has the fewest long green lines. This suggests that our approach can enable the model to have better alignment compared to the other methods, as there is the fewest number of representations with long distances. Overall, the visualization results intuitively demonstrate that our approach can enable the model to learn better representations compared to the baselines by improving the alignment and uniformity of the learned representations."
        ]
    },
    "id_table_8": {
        "caption": "(e)   Rewrite the code with the same semantics",
        "table": "S4.T4.4",
        "footnotes": [],
        "references": [
            "We investigate why  ChatDANCE  works by studying the distribution of data representations learned by models. We use  l a  l  i  g  n subscript l a l i g n \\ell_{align} roman_l start_POSTSUBSCRIPT italic_a italic_l italic_i italic_g italic_n end_POSTSUBSCRIPT  and  l u  n  i  f  o  r  m  i  t  y subscript l u n i f o r m i t y \\ell_{uniformity} roman_l start_POSTSUBSCRIPT italic_u italic_n italic_i italic_f italic_o italic_r italic_m italic_i italic_t italic_y end_POSTSUBSCRIPT   [ 45 ]  metrics to evaluate the quality of the representations learned by models, which are widely used in contrastive learning  [ 45 ,  46 ,  47 ,  48 ] . The  l a  l  i  g  n subscript l a l i g n \\ell_{align} roman_l start_POSTSUBSCRIPT italic_a italic_l italic_i italic_g italic_n end_POSTSUBSCRIPT  and  l u  n  i  f  o  r  m  i  t  y subscript l u n i f o r m i t y \\ell_{uniformity} roman_l start_POSTSUBSCRIPT italic_u italic_n italic_i italic_f italic_o italic_r italic_m italic_i italic_t italic_y end_POSTSUBSCRIPT  metrics are defined as follows in Equation  8 :",
            "where  ( x , y )  D p  a  i  r similar-to x y subscript D p a i r (x,y)\\sim D_{pair} ( italic_x , italic_y )  italic_D start_POSTSUBSCRIPT italic_p italic_a italic_i italic_r end_POSTSUBSCRIPT  means that  x x x italic_x  and  y y y italic_y  (such as query and code) are paired, and  ( x , y )  D similar-to x y D (x,y)\\sim D ( italic_x , italic_y )  italic_D  means that  x x x italic_x  and  y y y italic_y  are independently and identically distributed.  f  ( x ) f x f(x) italic_f ( italic_x )  and  f  ( y ) f y f(y) italic_f ( italic_y )  represent the representations learned by the model, and   f  ( x )  f  ( y )  2 subscript delimited- f x f y 2 \\lVert f(x)-f(y)\\rVert_{2}  italic_f ( italic_x ) - italic_f ( italic_y )  start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  represents the 2-norm distance between the representations. The hyperparameters    \\alpha italic_  and  t t t italic_t  are set to 2 in our experiments. The  l a  l  i  g  n  m  e  n  t subscript l a l i g n m e n t \\ell_{alignment} roman_l start_POSTSUBSCRIPT italic_a italic_l italic_i italic_g italic_n italic_m italic_e italic_n italic_t end_POSTSUBSCRIPT  is defined as the expected distance between paired representations, which measures the degree of matching between paired representations. From Equation  8 , we know that the smaller the distance between paired representations, the closer the alignment loss is to 0. In the extreme case, if the distance between all paired representations is 0, the alignment loss is 0. The  l u  n  i  f  o  r  m  i  t  y subscript l u n i f o r m i t y \\ell_{uniformity} roman_l start_POSTSUBSCRIPT italic_u italic_n italic_i italic_f italic_o italic_r italic_m italic_i italic_t italic_y end_POSTSUBSCRIPT  measures the uniformity of the distribution of representations. From Equation  8 , we know that the closer the distribution of representations is to uniformity, the closer the value of uniformity loss is to negative infinity. According to  [ 45 ,  46 ] , better alignment and uniformity can enable the model to have better performance and generalization. In our experiments, we use  l a  l  i  g  n  m  e  n  t subscript l a l i g n m e n t \\ell_{alignment} roman_l start_POSTSUBSCRIPT italic_a italic_l italic_i italic_g italic_n italic_m italic_e italic_n italic_t end_POSTSUBSCRIPT  and  l u  n  i  f  o  r  m  i  t  y subscript l u n i f o r m i t y \\ell_{uniformity} roman_l start_POSTSUBSCRIPT italic_u italic_n italic_i italic_f italic_o italic_r italic_m italic_i italic_t italic_y end_POSTSUBSCRIPT  to measure these two properties. The lower the values of two losses, the better the alignment and uniformity of the learned representations."
        ]
    },
    "id_table_9": {
        "caption": "(f)   Simplify the code",
        "table": "S5.T6.4",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "Figure 2 :  Augmented code samples generated by ChatGPT.",
        "table": "S5.T6.4.5.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "Figure 3 :  The architectures of bi-encoder and cross-encoder.",
        "table": "S5.T6.4.5.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "Algorithm 1    Filtering Algorithm",
        "table": "S5.T6.4.5.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "TABLE IV :  Details of CoSQA dataset.",
        "table": "S5.T7.5",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "Ensheng Shi is the corresponding authors."
    ]
}