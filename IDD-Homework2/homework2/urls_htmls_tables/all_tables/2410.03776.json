{
    "id_table_1": {
        "caption": "Table 1 :  MSE losses of different fBm Hurst-estimators by sequence length. To enable direct comparisons with other solutions in the literature, we also included the performance of  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  where only shift invariance is ensured by turning off the standardizing layer ( M LSTM  superscript subscript M LSTM M_{\\text{LSTM}}^{*} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ), here the training and evaluation was performed on    FBM ( H , n , 0 , 1 )  FBM H n 0 1 \\Delta\\operatorname*{FBM}(H,n,0,1) roman_ roman_FBM ( italic_H , italic_n , 0 , 1 ) .",
        "table": "A6.EGx1",
        "footnotes": [],
        "references": [
            "where   j , j  Z subscript  j j Z \\zeta_{j},\\ j\\in\\mathbb{Z} italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_j  blackboard_Z  is a white-noise sequence. It is known that when   j , j  Z subscript  j j Z \\zeta_{j},\\ j\\in\\mathbb{Z} italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_j  blackboard_Z  is ergodic (e.g. in the sense of Definition 2.8.4 in  [ 13 ] ), and  d = 0 d 0 d\\neq 0 italic_d = 0 , there exists a unique stationary solution to Equation ( 1 )  see Theorem 7.2.2 in  [ 14 ] .",
            "In the first experiment we fine-tuned the neural models  M conv subscript M conv M_{\\text{conv}} italic_M start_POSTSUBSCRIPT conv end_POSTSUBSCRIPT  and  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  up to  n = 12800 n 12800 n=12800 italic_n = 12800 . An initial training phase on trajectories of length  n = 100 n 100 n=100 italic_n = 100  was performed on  200 200 200 200  and  100 100 100 100  virtual epochs each containing  100 000 100000 100\\,000 100 000  sequences for  M conv subscript M conv M_{\\text{conv}} italic_M start_POSTSUBSCRIPT conv end_POSTSUBSCRIPT  and  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  respectively. The models were fine-tuned on  n = n absent n= italic_n =   200 200 200 200 ,  400 400 400 400 ,  800 800 800 800 ,  1600 1600 1600 1600 ,  3200 3200 3200 3200 ,  6400 6400 6400 6400  and  12800 12800 12800 12800  for an additional  20 20 20 20  and  10 10 10 10  virtual epochs for  M conv subscript M conv M_{\\text{conv}} italic_M start_POSTSUBSCRIPT conv end_POSTSUBSCRIPT  and  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  respectively. At the end we got a model which was trained on all of the sequence lengths in the list, albeit not all at the same time. The results can be found in Table  1  and Figure  1(a) , where the loss for sequence length of  n n n italic_n  was measured after fine tuning on trajectories of length  n n n italic_n . The neural models outperform all baseline estimators especially as the sequence length increases. It is worth noting that while  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  achieved slightly smaller loss than  M conv subscript M conv M_{\\text{conv}} italic_M start_POSTSUBSCRIPT conv end_POSTSUBSCRIPT , it requires somewhat more computational resources. Figure  2  compares the empirical bias and standard deviation of the final fine-tuned neural models and the baseline estimators as a function on the Hurst parameter.",
            "We evaluated the empirical consistency of  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  trained exclusively on certain length sequences. We can see the results in Table  2 , and Figure  1(b) . Trained on shorter sequences the performance of  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  improved when tested on longer sequences, but not as fast as  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  variants trained on longer sequences.  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  variants trained on longer sequences still performed well on shorter sequences, but not as well as dedicated variants.",
            "Scale-invariance is a central feature of our approach. To underscore its significance in developing reliable neural Hurst-estimators, we designed an experiment to assess various estimators across fBm realizations sampled from a range of time-horizons. Considering that adjusting the sample time-horizon is equivalent to scaling by a factor of    \\lambda italic_ , we calculated the MSE losses of the estimators for fBm realizations scaled by various    \\lambda italic_  values (Figure  7 ). Our  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  model incorporates a standardizing layer and, as anticipated, demonstrates scale-independent performance. Additionally, none of the baseline statistical estimators indicate any scale-dependency. In Section  5.2  of the main article we include the performance of  M LSTM  superscript subscript M LSTM M_{\\text{LSTM}}^{*} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in Table  1 .  M LSTM  superscript subscript M LSTM M_{\\text{LSTM}}^{*} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is trained and evaluated on    FBM ( H , 1600 , 0 , 1 )  FBM H 1600 0 1 \\Delta\\operatorname*{FBM}(H,1600,0,1) roman_ roman_FBM ( italic_H , 1600 , 0 , 1 )  realizations, and outperforms  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  locally by not including the standardizing layer. Figure  7  indicates the limited range of applicability of  M LSTM  superscript subscript M LSTM M_{\\text{LSTM}}^{*} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  if the time horizon is also an unknown parameter of the process, which is typically the case in practice. In order to further highlight the importance of the standardizing layer, we trained a model  M LSTM   superscript subscript M LSTM absent M_{\\text{LSTM}}^{**} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT   end_POSTSUPERSCRIPT  on      FBM ( H , 1600 , 0 , 1 )    FBM H 1600 0 1 \\lambda\\cdot\\Delta\\operatorname*{FBM}(H,1600,0,1) italic_  roman_ roman_FBM ( italic_H , 1600 , 0 , 1 )  realizations where we sampled    \\lambda italic_  from  Exp ( 1 ) Exp 1 \\operatorname*{Exp}(1) roman_Exp ( 1 ) . While the performance of  M LSTM   superscript subscript M LSTM absent M_{\\text{LSTM}}^{**} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT   end_POSTSUPERSCRIPT  is less extremely localized around    1  1 \\lambda\\approx 1 italic_  1 , as opposed to  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT , it does not outperform the baseline estimators at any scale.",
            "Figure  8  illustrates the effects of adding Gaussian white noise to fBm realizations before inference. Note that this scenario can be considered a special case of the stress-test in Subsection  C.1 , given that as  H  0  H 0 H\\to 0 italic_H  0 , fBm increments resemble white noise. Figure  8(a)  shows how the error of various estimators changes with increasing noise strength (   \\sigma italic_ ).  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  reacts to increasing noise levels quite rapidly, closely following Whittles method. It is noteworthy that the  R / S R S R/S italic_R / italic_S  statistic is mostly agnostic to the amount of the added white noise.",
            "The overlapping of smoothing windows contributes significantly to the increase in the estimated Hurst parameter. As shown in Figure  10 , when we use non-overlapping windows, the models still overestimate  H H H italic_H , but to a much smaller degree. This can likely be explained by considering that overlapping windows increase autocorrelation, which directly affects the Hurst estimation. A similar effect can be observed when we apply our Hurst-estimators to market volatility data. For example, the Chicago Board Options Exchange Volatility Index (VIX), also known as the fear index, is calculated from the S&P 500 and measures the markets expectation of volatility over the next 30 days. Figure  11  shows what happens, when we apply the Hurst-estimators directly to this data. The Hurst estimations are much larger than those in Figure  5  of the main article, which also contains Hurst estimations of the volatility calculated from the S&P 500, but within non-overlapping time windows.",
            "where    \\alpha italic_  is the speed of mean reversion and  w t  N  ( 0 , 1 ) similar-to subscript w t N 0 1 w_{t}\\sim N(0,1) italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_N ( 0 , 1 ) . The estimators were trained on fBm, fOU and ARFIMA processes (in the case of ARFIMA, originally trained for estimating parameter  d d d italic_d  instead of  H H H italic_H ). Figure  12  presents the results of the test.",
            "Driven by these motivations, after training  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  for the parameter estimation of the fBm, we tested it on the above fBm sums. We considered cases where  H 1 subscript H 1 H_{1} italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  was fixed and  H 2  U  ( 0 , 1 ) similar-to subscript H 2 U 0 1 H_{2}\\sim U(0,1) italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  italic_U ( 0 , 1 )  was random. The resulting scatter plot is shown in Figure  13 . Apparently,  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  tends to infer values  H ^  ( H 1 , H 2 ) ^ H subscript H 1 subscript H 2 \\hat{H}\\in(H_{1},H_{2}) over^ start_ARG italic_H end_ARG  ( italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) . Consequently, the model appears not to learn either the box dimension or the memory explicitly but rather an intermediate quantity. Interestingly, when  H 2 subscript H 2 H_{2} italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  is near 1, the estimates provided by  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  are smaller than the average of  H 1 subscript H 1 H_{1} italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  H 2 subscript H 2 H_{2} italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . Conversely, when  H 2 > 0.5 subscript H 2 0.5 H_{2}>0.5 italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT > 0.5  is not close to 1 and  H 1 < 0.5 subscript H 1 0.5 H_{1}<0.5 italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < 0.5 , the estimates are larger than the average, suggesting that the memory aspect associated with  H 2 subscript H 2 H_{2} italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  has a greater influence. This behavior suggests that the estimator captures different characteristics of the process, and that in different  H 2 subscript H 2 H_{2} italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  ranges, different characteristics dominate the sum.",
            "Several  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  Hurst-estimators, fine-tuned on fBm sequences of different lengths, were evaluated on    \\alpha italic_ -stable Levy processes, with outcomes illustrated in Figure  14 . Notably, for   = 2  2 \\alpha=2 italic_ = 2 , the characteristics of an    \\alpha italic_ -stable Levy process overlap with those of standard Brownian motion, which corresponds to a fBm with a Hurst parameter of  H = 0.5 H 0.5 H=0.5 italic_H = 0.5 . Thus, the inferred value is around  0.5 0.5 0.5 0.5  at   = 2  2 \\alpha=2 italic_ = 2 , as anticipated.",
            "Table  10  presents a comparison of execution times for generating fBm sequences using the Python library  fbm   [ 6 ]  and our implementations. Among the evaluated approaches, our implementation of Kroeses method achieved the shortest runtime for generating a single sequence, making it particularly suitable for training our Hurst-estimators, where a large number of realizations with varying Hurst parameters is necessary to build robust models.",
            "To ensure that our results were not biased by dependencies within our project, we validated our models on process realizations generated by the R package  YUIMA   [ 5 ] . The outcomes of the evaluation using neural fBm and fOU Hurst-estimators are presented in Tables  11  and  12 , respectively. In both instances, the models were trained on process realizations that were generated by our implementation, with the training procedure involving fine-tuning on progressively longer sequences up to the maximum sequence lengths specified in the corresponding tables ( 12 800 12800 12\\,800 12 800  and  6400 6400 6400 6400 ). Each metric was measured using these final fine-tuned models, which may result in suboptimal performance for shorter sequences when compared to the results presented in the main article. The values reported in the tables were computed based on  100 000 100000 100\\,000 100 000  independent process realizations generated by the  YUIMA  package. For the fOU process, the prior distributions of the parameters were defined according to Section  5.4 . Overall, the models performed similarly to what was observed with our generators, confirming that the models perform as expected even on process realizations generated independently of our implementation.",
            "To provide a clearer understanding of the training process and its objectives, in this section we present supplementary visualizations. Figure  15  illustrates the typical progression of model loss over the course of a training session, while Figures  16 ,  17 , and  18  show different realizations of fOU, fBm and ARFIMA processes by the key target parameters."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  MSE losses of LSTM-based fBm Hurst-estimators trained on different sequence lengths.",
        "table": "A6.EGx2",
        "footnotes": [],
        "references": [
            "The success of the utilized networks (Section  4.3 ) largely stems from a large volume of high-quality training data, manifested with the software that was built around the framework of the so-called isonormal processes (see  [ 35 ]  for the mathematical background, and Section  4.2  on the implementation). The underlying path-generating methodology includes the circulant embedding of covariance matrices and the utilization of fast Fourier transform (FFT).",
            "where  X k =  i = 1 k Z i subscript X k superscript subscript i 1 k subscript Z i X_{k}=\\sum_{i=1}^{k}Z_{i} italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT =  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . The analysis is done via assuming an asymptotics for the statistics in Equation ( 2 ), namely we postulate that on the long run, it holds that  R / S  ( n )  c  n h R S n c superscript n h R/S(n)\\approx cn^{h} italic_R / italic_S ( italic_n )  italic_c italic_n start_POSTSUPERSCRIPT italic_h end_POSTSUPERSCRIPT , where  c c c italic_c  is an unknown constant and  h h h italic_h  is the parameter we are looking for. Utilizing a one parameter log-log regression on the above formula, that is, using the relation  log  ( R / S  ( n ) ) = log  ( c ) + h  log  ( n ) R S n c h n \\log(R/S(n))=\\log(c)+h\\log(n) roman_log ( italic_R / italic_S ( italic_n ) ) = roman_log ( italic_c ) + italic_h roman_log ( italic_n ) , one can estimate  h h h italic_h .",
            "A known limitation of this methodology, when the underlying process is a fractional Brownian motion, is that using the statistics in ( 2 ) produces inferred values that are lower when the true value of  H H H italic_H  is in the long memory range, and substantially higher values when the time series shows heavy anti-persistence. A possible mitigation of this is to introduce a correction that calibrates the method to fit fractional Brownian motion data and use the corrected estimator as a baseline.",
            "In the first experiment we fine-tuned the neural models  M conv subscript M conv M_{\\text{conv}} italic_M start_POSTSUBSCRIPT conv end_POSTSUBSCRIPT  and  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  up to  n = 12800 n 12800 n=12800 italic_n = 12800 . An initial training phase on trajectories of length  n = 100 n 100 n=100 italic_n = 100  was performed on  200 200 200 200  and  100 100 100 100  virtual epochs each containing  100 000 100000 100\\,000 100 000  sequences for  M conv subscript M conv M_{\\text{conv}} italic_M start_POSTSUBSCRIPT conv end_POSTSUBSCRIPT  and  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  respectively. The models were fine-tuned on  n = n absent n= italic_n =   200 200 200 200 ,  400 400 400 400 ,  800 800 800 800 ,  1600 1600 1600 1600 ,  3200 3200 3200 3200 ,  6400 6400 6400 6400  and  12800 12800 12800 12800  for an additional  20 20 20 20  and  10 10 10 10  virtual epochs for  M conv subscript M conv M_{\\text{conv}} italic_M start_POSTSUBSCRIPT conv end_POSTSUBSCRIPT  and  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  respectively. At the end we got a model which was trained on all of the sequence lengths in the list, albeit not all at the same time. The results can be found in Table  1  and Figure  1(a) , where the loss for sequence length of  n n n italic_n  was measured after fine tuning on trajectories of length  n n n italic_n . The neural models outperform all baseline estimators especially as the sequence length increases. It is worth noting that while  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  achieved slightly smaller loss than  M conv subscript M conv M_{\\text{conv}} italic_M start_POSTSUBSCRIPT conv end_POSTSUBSCRIPT , it requires somewhat more computational resources. Figure  2  compares the empirical bias and standard deviation of the final fine-tuned neural models and the baseline estimators as a function on the Hurst parameter.",
            "We evaluated the empirical consistency of  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  trained exclusively on certain length sequences. We can see the results in Table  2 , and Figure  1(b) . Trained on shorter sequences the performance of  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  improved when tested on longer sequences, but not as fast as  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  variants trained on longer sequences.  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  variants trained on longer sequences still performed well on shorter sequences, but not as well as dedicated variants.",
            "Estimating the parameters of the fractional Ornstein-Uhlenbeck process is significantly more difficult. We evaluated  M LSTM subscript M LSTM {\\mathcal{M}}_{\\text{LSTM}} caligraphic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  on the estimation of the Hurst parameter of fOU. Here  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  does not work with the increments, but includes standardization to ensure scale and shift invariance. As we stated in Section  2.3  these invariances enable training on  FOU (  , H ,  , 0 , 1 ) FOU  H  0 1 \\operatorname*{FOU}(\\eta,H,\\alpha,0,1) roman_FOU ( italic_ , italic_H , italic_ , 0 , 1 )  without the loss of generality. Additionally we evaluated the quadratic generalized variation (QGV) estimator for the Hurst parameter, as described in  [ 4 ] . We trained  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  on sequences of length  200 200 200 200 ,  400 400 400 400 ,  800 800 800 800 ,  1600 1600 1600 1600 ,  3200 3200 3200 3200 ,  6400 6400 6400 6400  and  12800 12800 12800 12800  with the fine-tuning technique similar to the one in Section  5.2 . We generated the sequences for training and evaluation of the Hurst estimators from  FOU (  , H ,  , 0 , 1 ) FOU  H  0 1 \\operatorname*{FOU}(\\eta,H,\\alpha,0,1) roman_FOU ( italic_ , italic_H , italic_ , 0 , 1 ) , where  H  U  ( 0 , 1 ) ,   Exp ( 100 ) formulae-sequence similar-to H U 0 1 similar-to  Exp 100 H\\sim U(0,1),\\alpha\\sim\\operatorname*{Exp}(100) italic_H  italic_U ( 0 , 1 ) , italic_  roman_Exp ( 100 )  and    N  ( 0 , 1 ) similar-to  N 0 1 \\eta\\sim N(0,1) italic_  italic_N ( 0 , 1 )  are random.",
            "Scale-invariance is a central feature of our approach. To underscore its significance in developing reliable neural Hurst-estimators, we designed an experiment to assess various estimators across fBm realizations sampled from a range of time-horizons. Considering that adjusting the sample time-horizon is equivalent to scaling by a factor of    \\lambda italic_ , we calculated the MSE losses of the estimators for fBm realizations scaled by various    \\lambda italic_  values (Figure  7 ). Our  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  model incorporates a standardizing layer and, as anticipated, demonstrates scale-independent performance. Additionally, none of the baseline statistical estimators indicate any scale-dependency. In Section  5.2  of the main article we include the performance of  M LSTM  superscript subscript M LSTM M_{\\text{LSTM}}^{*} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in Table  1 .  M LSTM  superscript subscript M LSTM M_{\\text{LSTM}}^{*} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is trained and evaluated on    FBM ( H , 1600 , 0 , 1 )  FBM H 1600 0 1 \\Delta\\operatorname*{FBM}(H,1600,0,1) roman_ roman_FBM ( italic_H , 1600 , 0 , 1 )  realizations, and outperforms  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  locally by not including the standardizing layer. Figure  7  indicates the limited range of applicability of  M LSTM  superscript subscript M LSTM M_{\\text{LSTM}}^{*} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  if the time horizon is also an unknown parameter of the process, which is typically the case in practice. In order to further highlight the importance of the standardizing layer, we trained a model  M LSTM   superscript subscript M LSTM absent M_{\\text{LSTM}}^{**} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT   end_POSTSUPERSCRIPT  on      FBM ( H , 1600 , 0 , 1 )    FBM H 1600 0 1 \\lambda\\cdot\\Delta\\operatorname*{FBM}(H,1600,0,1) italic_  roman_ roman_FBM ( italic_H , 1600 , 0 , 1 )  realizations where we sampled    \\lambda italic_  from  Exp ( 1 ) Exp 1 \\operatorname*{Exp}(1) roman_Exp ( 1 ) . While the performance of  M LSTM   superscript subscript M LSTM absent M_{\\text{LSTM}}^{**} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT   end_POSTSUPERSCRIPT  is less extremely localized around    1  1 \\lambda\\approx 1 italic_  1 , as opposed to  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT , it does not outperform the baseline estimators at any scale.",
            "where    \\alpha italic_  is the speed of mean reversion and  w t  N  ( 0 , 1 ) similar-to subscript w t N 0 1 w_{t}\\sim N(0,1) italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_N ( 0 , 1 ) . The estimators were trained on fBm, fOU and ARFIMA processes (in the case of ARFIMA, originally trained for estimating parameter  d d d italic_d  instead of  H H H italic_H ). Figure  12  presents the results of the test.",
            "Tables  6  and  7  provide additional information about the fBm Hurst-estimators which appear in Table  2  and Figure  2  in Section  5.2 .",
            "To ensure that our results were not biased by dependencies within our project, we validated our models on process realizations generated by the R package  YUIMA   [ 5 ] . The outcomes of the evaluation using neural fBm and fOU Hurst-estimators are presented in Tables  11  and  12 , respectively. In both instances, the models were trained on process realizations that were generated by our implementation, with the training procedure involving fine-tuning on progressively longer sequences up to the maximum sequence lengths specified in the corresponding tables ( 12 800 12800 12\\,800 12 800  and  6400 6400 6400 6400 ). Each metric was measured using these final fine-tuned models, which may result in suboptimal performance for shorter sequences when compared to the results presented in the main article. The values reported in the tables were computed based on  100 000 100000 100\\,000 100 000  independent process realizations generated by the  YUIMA  package. For the fOU process, the prior distributions of the parameters were defined according to Section  5.4 . Overall, the models performed similarly to what was observed with our generators, confirming that the models perform as expected even on process realizations generated independently of our implementation."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Overall running times for fBm Hurst-inference measured on  10000 10000 10000 10000  sequences of length  3200 3200 3200 3200 .",
        "table": "S5.T1.76.70",
        "footnotes": [],
        "references": [
            "The success of the utilized networks (Section  4.3 ) largely stems from a large volume of high-quality training data, manifested with the software that was built around the framework of the so-called isonormal processes (see  [ 35 ]  for the mathematical background, and Section  4.2  on the implementation). The underlying path-generating methodology includes the circulant embedding of covariance matrices and the utilization of fast Fourier transform (FFT).",
            "In our experimental setup, the neural network estimators demonstrated a favorable balance between accuracy and speed at inference. The advantage was only partly due to GPU usage, as suggested by the overall inference times shown in Table  3 . The indicated inference times depend on various factors, including the implementation and available hardware resources; they reflect the specific setup used at the time of writing.",
            "We trained  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  models for estimating the parameter  d d d italic_d  of the  ARFIMA  ( 0 , d , 0 ) ARFIMA 0 d 0 \\text{ARFIMA}(0,d,0) ARFIMA ( 0 , italic_d , 0 )  process. These models contain no standardization, and work with the input sequence, not the increments. The models were trained on sequences of length  200 200 200 200 ,  400 400 400 400 ,  800 800 800 800 ,  1600 1600 1600 1600 ,  3200 3200 3200 3200 ,  6400 6400 6400 6400  and  12800 12800 12800 12800 . Classical Hurst estimation techniques were evaluated for the inference of  d d d italic_d  as described in Section  3.5 . By adjusting the spectrum, Whittles method was calibrated specifically for the ARFIMA  d d d italic_d -estimation. The results are presented in Table  4  and Figure  3 .",
            "Estimating the parameters of the fractional Ornstein-Uhlenbeck process is significantly more difficult. We evaluated  M LSTM subscript M LSTM {\\mathcal{M}}_{\\text{LSTM}} caligraphic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  on the estimation of the Hurst parameter of fOU. Here  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  does not work with the increments, but includes standardization to ensure scale and shift invariance. As we stated in Section  2.3  these invariances enable training on  FOU (  , H ,  , 0 , 1 ) FOU  H  0 1 \\operatorname*{FOU}(\\eta,H,\\alpha,0,1) roman_FOU ( italic_ , italic_H , italic_ , 0 , 1 )  without the loss of generality. Additionally we evaluated the quadratic generalized variation (QGV) estimator for the Hurst parameter, as described in  [ 4 ] . We trained  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  on sequences of length  200 200 200 200 ,  400 400 400 400 ,  800 800 800 800 ,  1600 1600 1600 1600 ,  3200 3200 3200 3200 ,  6400 6400 6400 6400  and  12800 12800 12800 12800  with the fine-tuning technique similar to the one in Section  5.2 . We generated the sequences for training and evaluation of the Hurst estimators from  FOU (  , H ,  , 0 , 1 ) FOU  H  0 1 \\operatorname*{FOU}(\\eta,H,\\alpha,0,1) roman_FOU ( italic_ , italic_H , italic_ , 0 , 1 ) , where  H  U  ( 0 , 1 ) ,   Exp ( 100 ) formulae-sequence similar-to H U 0 1 similar-to  Exp 100 H\\sim U(0,1),\\alpha\\sim\\operatorname*{Exp}(100) italic_H  italic_U ( 0 , 1 ) , italic_  roman_Exp ( 100 )  and    N  ( 0 , 1 ) similar-to  N 0 1 \\eta\\sim N(0,1) italic_  italic_N ( 0 , 1 )  are random.",
            "Driven by these motivations, after training  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  for the parameter estimation of the fBm, we tested it on the above fBm sums. We considered cases where  H 1 subscript H 1 H_{1} italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  was fixed and  H 2  U  ( 0 , 1 ) similar-to subscript H 2 U 0 1 H_{2}\\sim U(0,1) italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  italic_U ( 0 , 1 )  was random. The resulting scatter plot is shown in Figure  13 . Apparently,  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  tends to infer values  H ^  ( H 1 , H 2 ) ^ H subscript H 1 subscript H 2 \\hat{H}\\in(H_{1},H_{2}) over^ start_ARG italic_H end_ARG  ( italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) . Consequently, the model appears not to learn either the box dimension or the memory explicitly but rather an intermediate quantity. Interestingly, when  H 2 subscript H 2 H_{2} italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  is near 1, the estimates provided by  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  are smaller than the average of  H 1 subscript H 1 H_{1} italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  H 2 subscript H 2 H_{2} italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . Conversely, when  H 2 > 0.5 subscript H 2 0.5 H_{2}>0.5 italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT > 0.5  is not close to 1 and  H 1 < 0.5 subscript H 1 0.5 H_{1}<0.5 italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < 0.5 , the estimates are larger than the average, suggesting that the memory aspect associated with  H 2 subscript H 2 H_{2} italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  has a greater influence. This behavior suggests that the estimator captures different characteristics of the process, and that in different  H 2 subscript H 2 H_{2} italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  ranges, different characteristics dominate the sum.",
            "Similarly, Tables  8  and  9  provide additional information about the ARFIMA  d d d italic_d -estimators which appear in Table  4  and Figure  3  in Section  5.3 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  MSE losses of different  ARFIMA  ( 0 , d , 0 ) ARFIMA 0 d 0 \\text{ARFIMA}(0,d,0) ARFIMA ( 0 , italic_d , 0 )   d d d italic_d -estimators by sequence length.",
        "table": "S5.T2.66",
        "footnotes": [],
        "references": [
            "The success of the utilized networks (Section  4.3 ) largely stems from a large volume of high-quality training data, manifested with the software that was built around the framework of the so-called isonormal processes (see  [ 35 ]  for the mathematical background, and Section  4.2  on the implementation). The underlying path-generating methodology includes the circulant embedding of covariance matrices and the utilization of fast Fourier transform (FFT).",
            "where  I  (  ) I  I(\\lambda) italic_I ( italic_ )  is the periodogram, an unbiased estimator of the spectral density  f H subscript f H f_{H} italic_f start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT , defined as  I  (  ) =  j =  ( n  1 ) n  1  ^  ( j )  e i  j   I  superscript subscript j n 1 n 1 ^  j superscript e i j  I(\\lambda)=\\sum_{j=-(n-1)}^{n-1}\\hat{\\gamma}(j)e^{\\mathrm{i}j\\lambda} italic_I ( italic_ ) =  start_POSTSUBSCRIPT italic_j = - ( italic_n - 1 ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT over^ start_ARG italic_ end_ARG ( italic_j ) italic_e start_POSTSUPERSCRIPT roman_i italic_j italic_ end_POSTSUPERSCRIPT , with the complex imaginary unit  i i \\mathrm{i} roman_i , and where the sample autocovariance   ^  ( j ) ^  j \\hat{\\gamma}(j) over^ start_ARG italic_ end_ARG ( italic_j ) , using the sample average  X   = 1 n   k = 1 n X k   X 1 n superscript subscript k 1 n subscript X k \\bar{X}=\\frac{1}{n}\\sum_{k=1}^{n}X_{k} over  start_ARG italic_X end_ARG = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG  start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , is   ^  ( j ) =  k = 0 n  | j |  1 ( X k  X   )  ( X k + | j |  X   ) ^  j superscript subscript k 0 n j 1 subscript X k   X subscript X k j   X \\hat{\\gamma}(j)=\\sum_{k=0}^{n-|j|-1}\\left(X_{k}-\\bar{X}\\right)\\left(X_{k+|j|}-% \\bar{X}\\right) over^ start_ARG italic_ end_ARG ( italic_j ) =  start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n - | italic_j | - 1 end_POSTSUPERSCRIPT ( italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - over  start_ARG italic_X end_ARG ) ( italic_X start_POSTSUBSCRIPT italic_k + | italic_j | end_POSTSUBSCRIPT - over  start_ARG italic_X end_ARG ) . The quantity in Equation ( 4 ) is usually approximated with the sum  Q ~  ( H ) =  k = 1  n / 2  I  (  k ) f H  (  k ) ~ Q H superscript subscript k 1 n 2 I subscript  k subscript f H subscript  k \\tilde{Q}(H)=\\sum_{k=1}^{\\lfloor n/2\\rfloor}\\frac{I(\\lambda_{k})}{f_{H}(% \\lambda_{k})} over~ start_ARG italic_Q end_ARG ( italic_H ) =  start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  italic_n / 2  end_POSTSUPERSCRIPT divide start_ARG italic_I ( italic_ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_ARG start_ARG italic_f start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_ARG , with   k = 2    k n subscript  k 2  k n \\lambda_{k}=\\frac{2\\pi k}{n} italic_ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = divide start_ARG 2 italic_ italic_k end_ARG start_ARG italic_n end_ARG , to obtain an asymptotically correct estimate  H ^ ^ H \\hat{H} over^ start_ARG italic_H end_ARG  of the Hurst parameter  H H H italic_H .",
            "We trained  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  models for estimating the parameter  d d d italic_d  of the  ARFIMA  ( 0 , d , 0 ) ARFIMA 0 d 0 \\text{ARFIMA}(0,d,0) ARFIMA ( 0 , italic_d , 0 )  process. These models contain no standardization, and work with the input sequence, not the increments. The models were trained on sequences of length  200 200 200 200 ,  400 400 400 400 ,  800 800 800 800 ,  1600 1600 1600 1600 ,  3200 3200 3200 3200 ,  6400 6400 6400 6400  and  12800 12800 12800 12800 . Classical Hurst estimation techniques were evaluated for the inference of  d d d italic_d  as described in Section  3.5 . By adjusting the spectrum, Whittles method was calibrated specifically for the ARFIMA  d d d italic_d -estimation. The results are presented in Table  4  and Figure  3 .",
            "We conducted cross-tests on  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  models, by training them on ARFIMA and fBm trajectories, and then evaluating them against the alternate process. The ARFIMA  d d d italic_d -estimator was trained on  n = 12800 n 12800 n=12800 italic_n = 12800 , and the fBm  H H H italic_H -estimator was fine-tuned up to  n = 12800 n 12800 n=12800 italic_n = 12800 . In both cases the models were tested on sequences of length  12800 12800 12800 12800 . As Figure  4  shows, the models perform remarkably, with minor asymmetric bias with respect to the parameter range. This suggests that the models either capture the decay rate of autocovariance of fractional noise or some fractal property of sample paths.",
            "Several  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  Hurst-estimators, fine-tuned on fBm sequences of different lengths, were evaluated on    \\alpha italic_ -stable Levy processes, with outcomes illustrated in Figure  14 . Notably, for   = 2  2 \\alpha=2 italic_ = 2 , the characteristics of an    \\alpha italic_ -stable Levy process overlap with those of standard Brownian motion, which corresponds to a fBm with a Hurst parameter of  H = 0.5 H 0.5 H=0.5 italic_H = 0.5 . Thus, the inferred value is around  0.5 0.5 0.5 0.5  at   = 2  2 \\alpha=2 italic_ = 2 , as anticipated.",
            "Similarly, Tables  8  and  9  provide additional information about the ARFIMA  d d d italic_d -estimators which appear in Table  4  and Figure  3  in Section  5.3 .",
            "To ensure that our results were not biased by dependencies within our project, we validated our models on process realizations generated by the R package  YUIMA   [ 5 ] . The outcomes of the evaluation using neural fBm and fOU Hurst-estimators are presented in Tables  11  and  12 , respectively. In both instances, the models were trained on process realizations that were generated by our implementation, with the training procedure involving fine-tuning on progressively longer sequences up to the maximum sequence lengths specified in the corresponding tables ( 12 800 12800 12\\,800 12 800  and  6400 6400 6400 6400 ). Each metric was measured using these final fine-tuned models, which may result in suboptimal performance for shorter sequences when compared to the results presented in the main article. The values reported in the tables were computed based on  100 000 100000 100\\,000 100 000  independent process realizations generated by the  YUIMA  package. For the fOU process, the prior distributions of the parameters were defined according to Section  5.4 . Overall, the models performed similarly to what was observed with our generators, confirming that the models perform as expected even on process realizations generated independently of our implementation."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Performance metrics of different fOU Hurst-estimators by sequence length.",
        "table": "S5.T3.16.12",
        "footnotes": [],
        "references": [
            "To evaluate the real-world performance of our neural fBm Hurst-estimators, we analyzed historical data from the S&P 500 stock market index. Volatility in financial markets can be modeled using fBm with Hurst exponent  H  0.1 H 0.1 H\\approx 0.1 italic_H  0.1   [ 12 ] . Volatility is often calculated with overlapping sliding windows, but this increases the correlation between the neighboring values and skews the Hurst-estimates to be around  0.5 0.5 0.5 0.5 . To avoid this, we calculated the daily volatility from 15 minute log-returns, which enabled us to estimate the Hurst parameter in yearly windows, without overlaps in the volatility calculation. Figure  5  shows the results, indicating that  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  and  M conv subscript M conv M_{\\text{conv}} italic_M start_POSTSUBSCRIPT conv end_POSTSUBSCRIPT  effectively capture the temporal dynamics of market volatility, correlating well with traditional methods, most closely with Whittles method, which was the best-performing baseline statistical estimator in the synthetic measurements. The neural models yield Hurst-estimations in the  ( 0 ,  0.3 ) 0 0.3 (0,\\,0.3) ( 0 , 0.3 )  range, in the anti-persistent Hurst regime.",
            "We trained  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  models for estimating the parameter  d d d italic_d  of the  ARFIMA  ( 0 , d , 0 ) ARFIMA 0 d 0 \\text{ARFIMA}(0,d,0) ARFIMA ( 0 , italic_d , 0 )  process. These models contain no standardization, and work with the input sequence, not the increments. The models were trained on sequences of length  200 200 200 200 ,  400 400 400 400 ,  800 800 800 800 ,  1600 1600 1600 1600 ,  3200 3200 3200 3200 ,  6400 6400 6400 6400  and  12800 12800 12800 12800 . Classical Hurst estimation techniques were evaluated for the inference of  d d d italic_d  as described in Section  3.5 . By adjusting the spectrum, Whittles method was calibrated specifically for the ARFIMA  d d d italic_d -estimation. The results are presented in Table  4  and Figure  3 .",
            "Estimating the parameters of the fractional Ornstein-Uhlenbeck process is significantly more difficult. We evaluated  M LSTM subscript M LSTM {\\mathcal{M}}_{\\text{LSTM}} caligraphic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  on the estimation of the Hurst parameter of fOU. Here  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  does not work with the increments, but includes standardization to ensure scale and shift invariance. As we stated in Section  2.3  these invariances enable training on  FOU (  , H ,  , 0 , 1 ) FOU  H  0 1 \\operatorname*{FOU}(\\eta,H,\\alpha,0,1) roman_FOU ( italic_ , italic_H , italic_ , 0 , 1 )  without the loss of generality. Additionally we evaluated the quadratic generalized variation (QGV) estimator for the Hurst parameter, as described in  [ 4 ] . We trained  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  on sequences of length  200 200 200 200 ,  400 400 400 400 ,  800 800 800 800 ,  1600 1600 1600 1600 ,  3200 3200 3200 3200 ,  6400 6400 6400 6400  and  12800 12800 12800 12800  with the fine-tuning technique similar to the one in Section  5.2 . We generated the sequences for training and evaluation of the Hurst estimators from  FOU (  , H ,  , 0 , 1 ) FOU  H  0 1 \\operatorname*{FOU}(\\eta,H,\\alpha,0,1) roman_FOU ( italic_ , italic_H , italic_ , 0 , 1 ) , where  H  U  ( 0 , 1 ) ,   Exp ( 100 ) formulae-sequence similar-to H U 0 1 similar-to  Exp 100 H\\sim U(0,1),\\alpha\\sim\\operatorname*{Exp}(100) italic_H  italic_U ( 0 , 1 ) , italic_  roman_Exp ( 100 )  and    N  ( 0 , 1 ) similar-to  N 0 1 \\eta\\sim N(0,1) italic_  italic_N ( 0 , 1 )  are random.",
            "Scale-invariance is a central feature of our approach. To underscore its significance in developing reliable neural Hurst-estimators, we designed an experiment to assess various estimators across fBm realizations sampled from a range of time-horizons. Considering that adjusting the sample time-horizon is equivalent to scaling by a factor of    \\lambda italic_ , we calculated the MSE losses of the estimators for fBm realizations scaled by various    \\lambda italic_  values (Figure  7 ). Our  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  model incorporates a standardizing layer and, as anticipated, demonstrates scale-independent performance. Additionally, none of the baseline statistical estimators indicate any scale-dependency. In Section  5.2  of the main article we include the performance of  M LSTM  superscript subscript M LSTM M_{\\text{LSTM}}^{*} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in Table  1 .  M LSTM  superscript subscript M LSTM M_{\\text{LSTM}}^{*} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is trained and evaluated on    FBM ( H , 1600 , 0 , 1 )  FBM H 1600 0 1 \\Delta\\operatorname*{FBM}(H,1600,0,1) roman_ roman_FBM ( italic_H , 1600 , 0 , 1 )  realizations, and outperforms  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  locally by not including the standardizing layer. Figure  7  indicates the limited range of applicability of  M LSTM  superscript subscript M LSTM M_{\\text{LSTM}}^{*} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  if the time horizon is also an unknown parameter of the process, which is typically the case in practice. In order to further highlight the importance of the standardizing layer, we trained a model  M LSTM   superscript subscript M LSTM absent M_{\\text{LSTM}}^{**} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT   end_POSTSUPERSCRIPT  on      FBM ( H , 1600 , 0 , 1 )    FBM H 1600 0 1 \\lambda\\cdot\\Delta\\operatorname*{FBM}(H,1600,0,1) italic_  roman_ roman_FBM ( italic_H , 1600 , 0 , 1 )  realizations where we sampled    \\lambda italic_  from  Exp ( 1 ) Exp 1 \\operatorname*{Exp}(1) roman_Exp ( 1 ) . While the performance of  M LSTM   superscript subscript M LSTM absent M_{\\text{LSTM}}^{**} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT   end_POSTSUPERSCRIPT  is less extremely localized around    1  1 \\lambda\\approx 1 italic_  1 , as opposed to  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT , it does not outperform the baseline estimators at any scale.",
            "The overlapping of smoothing windows contributes significantly to the increase in the estimated Hurst parameter. As shown in Figure  10 , when we use non-overlapping windows, the models still overestimate  H H H italic_H , but to a much smaller degree. This can likely be explained by considering that overlapping windows increase autocorrelation, which directly affects the Hurst estimation. A similar effect can be observed when we apply our Hurst-estimators to market volatility data. For example, the Chicago Board Options Exchange Volatility Index (VIX), also known as the fear index, is calculated from the S&P 500 and measures the markets expectation of volatility over the next 30 days. Figure  11  shows what happens, when we apply the Hurst-estimators directly to this data. The Hurst estimations are much larger than those in Figure  5  of the main article, which also contains Hurst estimations of the volatility calculated from the S&P 500, but within non-overlapping time windows.",
            "Tables  6  and  7  provide additional information about the fBm Hurst-estimators which appear in Table  2  and Figure  2  in Section  5.2 .",
            "Similarly, Tables  8  and  9  provide additional information about the ARFIMA  d d d italic_d -estimators which appear in Table  4  and Figure  3  in Section  5.3 .",
            "To ensure that our results were not biased by dependencies within our project, we validated our models on process realizations generated by the R package  YUIMA   [ 5 ] . The outcomes of the evaluation using neural fBm and fOU Hurst-estimators are presented in Tables  11  and  12 , respectively. In both instances, the models were trained on process realizations that were generated by our implementation, with the training procedure involving fine-tuning on progressively longer sequences up to the maximum sequence lengths specified in the corresponding tables ( 12 800 12800 12\\,800 12 800  and  6400 6400 6400 6400 ). Each metric was measured using these final fine-tuned models, which may result in suboptimal performance for shorter sequences when compared to the results presented in the main article. The values reported in the tables were computed based on  100 000 100000 100\\,000 100 000  independent process realizations generated by the  YUIMA  package. For the fOU process, the prior distributions of the parameters were defined according to Section  5.4 . Overall, the models performed similarly to what was observed with our generators, confirming that the models perform as expected even on process realizations generated independently of our implementation.",
            "To provide a clearer understanding of the training process and its objectives, in this section we present supplementary visualizations. Figure  15  illustrates the typical progression of model loss over the course of a training session, while Figures  16 ,  17 , and  18  show different realizations of fOU, fBm and ARFIMA processes by the key target parameters."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  Absolute area under Hurstbias curve of different fBm Hurst-estimators by sequence length.",
        "table": "S5.T4.55.51",
        "footnotes": [],
        "references": [
            "We stress-tested the  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  Hurst-estimators trained on fBm, fOU and ARFIMA processes (in the case of ARFIMA, originally trained for estimating  d d d italic_d ) on the standard Ornstein-Uhlenbeck process  FOU ( 0 , 0.5 ,  , 0 , 1 ) FOU 0 0.5  0 1 \\operatorname*{FOU}(0,0.5,\\alpha,0,1) roman_FOU ( 0 , 0.5 , italic_ , 0 , 1 ) ; results are shown in Figure  6 . The inferred value at   = 0  0 \\alpha=0 italic_ = 0  is  0.5 0.5 0.5 0.5  as expected since the model receives an input that it already encountered in the learning phase. As the parameter    \\alpha italic_  increases, all models return values tending towards zero. This could suggest that larger    \\alpha italic_  values have a similar effect to smaller Hurst values, and sequences generated with a larger speed of mean reversion resemble trajectories that have greater anti-persistence. Moreover when   > 0  0 \\alpha>0 italic_ > 0 , the Ornstein-Uhlenbeck autocovariance shows exponential decay, contrary to the power decay associated with the data that the models were calibrated on. However, the fOU Hurst-estimator does stay around  0.5 0.5 0.5 0.5  longer, corresponding to the    \\alpha italic_  range it encountered during training.",
            "Tables  6  and  7  provide additional information about the fBm Hurst-estimators which appear in Table  2  and Figure  2  in Section  5.2 .",
            "To provide a clearer understanding of the training process and its objectives, in this section we present supplementary visualizations. Figure  15  illustrates the typical progression of model loss over the course of a training session, while Figures  16 ,  17 , and  18  show different realizations of fOU, fBm and ARFIMA processes by the key target parameters."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :  Absolute area under Hurstdeviation curve of different fBm Hurst-estimators by sequence length.",
        "table": "S5.T5.65",
        "footnotes": [],
        "references": [
            "Scale-invariance is a central feature of our approach. To underscore its significance in developing reliable neural Hurst-estimators, we designed an experiment to assess various estimators across fBm realizations sampled from a range of time-horizons. Considering that adjusting the sample time-horizon is equivalent to scaling by a factor of    \\lambda italic_ , we calculated the MSE losses of the estimators for fBm realizations scaled by various    \\lambda italic_  values (Figure  7 ). Our  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  model incorporates a standardizing layer and, as anticipated, demonstrates scale-independent performance. Additionally, none of the baseline statistical estimators indicate any scale-dependency. In Section  5.2  of the main article we include the performance of  M LSTM  superscript subscript M LSTM M_{\\text{LSTM}}^{*} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in Table  1 .  M LSTM  superscript subscript M LSTM M_{\\text{LSTM}}^{*} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is trained and evaluated on    FBM ( H , 1600 , 0 , 1 )  FBM H 1600 0 1 \\Delta\\operatorname*{FBM}(H,1600,0,1) roman_ roman_FBM ( italic_H , 1600 , 0 , 1 )  realizations, and outperforms  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  locally by not including the standardizing layer. Figure  7  indicates the limited range of applicability of  M LSTM  superscript subscript M LSTM M_{\\text{LSTM}}^{*} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  if the time horizon is also an unknown parameter of the process, which is typically the case in practice. In order to further highlight the importance of the standardizing layer, we trained a model  M LSTM   superscript subscript M LSTM absent M_{\\text{LSTM}}^{**} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT   end_POSTSUPERSCRIPT  on      FBM ( H , 1600 , 0 , 1 )    FBM H 1600 0 1 \\lambda\\cdot\\Delta\\operatorname*{FBM}(H,1600,0,1) italic_  roman_ roman_FBM ( italic_H , 1600 , 0 , 1 )  realizations where we sampled    \\lambda italic_  from  Exp ( 1 ) Exp 1 \\operatorname*{Exp}(1) roman_Exp ( 1 ) . While the performance of  M LSTM   superscript subscript M LSTM absent M_{\\text{LSTM}}^{**} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT   end_POSTSUPERSCRIPT  is less extremely localized around    1  1 \\lambda\\approx 1 italic_  1 , as opposed to  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT , it does not outperform the baseline estimators at any scale.",
            "Tables  6  and  7  provide additional information about the fBm Hurst-estimators which appear in Table  2  and Figure  2  in Section  5.2 .",
            "To provide a clearer understanding of the training process and its objectives, in this section we present supplementary visualizations. Figure  15  illustrates the typical progression of model loss over the course of a training session, while Figures  16 ,  17 , and  18  show different realizations of fOU, fBm and ARFIMA processes by the key target parameters."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 :  Absolute area under  d d d italic_d bias curve of different  ARFIMA ( 0 , d , 0 ) ARFIMA 0 d 0 \\operatorname*{ARFIMA}(0,d,0) roman_ARFIMA ( 0 , italic_d , 0 )   d d d italic_d -estimators by sequence length.",
        "table": "A4.T6.70.70",
        "footnotes": [],
        "references": [
            "Figure  8  illustrates the effects of adding Gaussian white noise to fBm realizations before inference. Note that this scenario can be considered a special case of the stress-test in Subsection  C.1 , given that as  H  0  H 0 H\\to 0 italic_H  0 , fBm increments resemble white noise. Figure  8(a)  shows how the error of various estimators changes with increasing noise strength (   \\sigma italic_ ).  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  reacts to increasing noise levels quite rapidly, closely following Whittles method. It is noteworthy that the  R / S R S R/S italic_R / italic_S  statistic is mostly agnostic to the amount of the added white noise.",
            "Similarly, Tables  8  and  9  provide additional information about the ARFIMA  d d d italic_d -estimators which appear in Table  4  and Figure  3  in Section  5.3 .",
            "To provide a clearer understanding of the training process and its objectives, in this section we present supplementary visualizations. Figure  15  illustrates the typical progression of model loss over the course of a training session, while Figures  16 ,  17 , and  18  show different realizations of fOU, fBm and ARFIMA processes by the key target parameters."
        ]
    },
    "id_table_9": {
        "caption": "Table 9 :  Absolute area under  d d d italic_d deviation curve of different  ARFIMA ( 0 , d , 0 ) ARFIMA 0 d 0 \\operatorname*{ARFIMA}(0,d,0) roman_ARFIMA ( 0 , italic_d , 0 )   d d d italic_d -estimators by sequence length.",
        "table": "A4.T7.70.70",
        "footnotes": [],
        "references": [
            "Figure  9  shows how estimation errors change with increasing moving average window sizes, indicating that when the smoothing windows overlap, there is a notable increase in the estimated Hurst parameter. This suggests that the estimators perceive smoothed data as exhibiting stronger long-range dependence. Among the estimators tested,  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  responds most rapidly. In contrast, the  R / S R S R/S italic_R / italic_S  statistic reacts more slowly but is not entirely insensitive to the size of the smoothing window.",
            "Similarly, Tables  8  and  9  provide additional information about the ARFIMA  d d d italic_d -estimators which appear in Table  4  and Figure  3  in Section  5.3 ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10 :  Running times of generating fBm sequences of length  100 000 100000 100\\,000 100 000 .",
        "table": "A4.T8.57.51",
        "footnotes": [],
        "references": [
            "The overlapping of smoothing windows contributes significantly to the increase in the estimated Hurst parameter. As shown in Figure  10 , when we use non-overlapping windows, the models still overestimate  H H H italic_H , but to a much smaller degree. This can likely be explained by considering that overlapping windows increase autocorrelation, which directly affects the Hurst estimation. A similar effect can be observed when we apply our Hurst-estimators to market volatility data. For example, the Chicago Board Options Exchange Volatility Index (VIX), also known as the fear index, is calculated from the S&P 500 and measures the markets expectation of volatility over the next 30 days. Figure  11  shows what happens, when we apply the Hurst-estimators directly to this data. The Hurst estimations are much larger than those in Figure  5  of the main article, which also contains Hurst estimations of the volatility calculated from the S&P 500, but within non-overlapping time windows.",
            "Table  10  presents a comparison of execution times for generating fBm sequences using the Python library  fbm   [ 6 ]  and our implementations. Among the evaluated approaches, our implementation of Kroeses method achieved the shortest runtime for generating a single sequence, making it particularly suitable for training our Hurst-estimators, where a large number of realizations with varying Hurst parameters is necessary to build robust models."
        ]
    },
    "id_table_11": {
        "caption": "Table 11 :  Performance metrics of fBm Hurst-estimators by sequence length on  100 000 100000 100\\,000 100 000  sequences generated by the  YUIMA  package.",
        "table": "A4.T9.57.51",
        "footnotes": [],
        "references": [
            "The overlapping of smoothing windows contributes significantly to the increase in the estimated Hurst parameter. As shown in Figure  10 , when we use non-overlapping windows, the models still overestimate  H H H italic_H , but to a much smaller degree. This can likely be explained by considering that overlapping windows increase autocorrelation, which directly affects the Hurst estimation. A similar effect can be observed when we apply our Hurst-estimators to market volatility data. For example, the Chicago Board Options Exchange Volatility Index (VIX), also known as the fear index, is calculated from the S&P 500 and measures the markets expectation of volatility over the next 30 days. Figure  11  shows what happens, when we apply the Hurst-estimators directly to this data. The Hurst estimations are much larger than those in Figure  5  of the main article, which also contains Hurst estimations of the volatility calculated from the S&P 500, but within non-overlapping time windows.",
            "To ensure that our results were not biased by dependencies within our project, we validated our models on process realizations generated by the R package  YUIMA   [ 5 ] . The outcomes of the evaluation using neural fBm and fOU Hurst-estimators are presented in Tables  11  and  12 , respectively. In both instances, the models were trained on process realizations that were generated by our implementation, with the training procedure involving fine-tuning on progressively longer sequences up to the maximum sequence lengths specified in the corresponding tables ( 12 800 12800 12\\,800 12 800  and  6400 6400 6400 6400 ). Each metric was measured using these final fine-tuned models, which may result in suboptimal performance for shorter sequences when compared to the results presented in the main article. The values reported in the tables were computed based on  100 000 100000 100\\,000 100 000  independent process realizations generated by the  YUIMA  package. For the fOU process, the prior distributions of the parameters were defined according to Section  5.4 . Overall, the models performed similarly to what was observed with our generators, confirming that the models perform as expected even on process realizations generated independently of our implementation."
        ]
    },
    "id_table_12": {
        "caption": "Table 12 :  Performance metrics of  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  the fOU Hurst-estimator by sequence length, measured on  100 000 100000 100\\,000 100 000  sequences generated by the  YUIMA  package.",
        "table": "A5.T10.12",
        "footnotes": [],
        "references": [
            "where    \\alpha italic_  is the speed of mean reversion and  w t  N  ( 0 , 1 ) similar-to subscript w t N 0 1 w_{t}\\sim N(0,1) italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_N ( 0 , 1 ) . The estimators were trained on fBm, fOU and ARFIMA processes (in the case of ARFIMA, originally trained for estimating parameter  d d d italic_d  instead of  H H H italic_H ). Figure  12  presents the results of the test.",
            "To ensure that our results were not biased by dependencies within our project, we validated our models on process realizations generated by the R package  YUIMA   [ 5 ] . The outcomes of the evaluation using neural fBm and fOU Hurst-estimators are presented in Tables  11  and  12 , respectively. In both instances, the models were trained on process realizations that were generated by our implementation, with the training procedure involving fine-tuning on progressively longer sequences up to the maximum sequence lengths specified in the corresponding tables ( 12 800 12800 12\\,800 12 800  and  6400 6400 6400 6400 ). Each metric was measured using these final fine-tuned models, which may result in suboptimal performance for shorter sequences when compared to the results presented in the main article. The values reported in the tables were computed based on  100 000 100000 100\\,000 100 000  independent process realizations generated by the  YUIMA  package. For the fOU process, the prior distributions of the parameters were defined according to Section  5.4 . Overall, the models performed similarly to what was observed with our generators, confirming that the models perform as expected even on process realizations generated independently of our implementation."
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A6.T11.71.69",
        "footnotes": [],
        "references": [
            "Driven by these motivations, after training  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  for the parameter estimation of the fBm, we tested it on the above fBm sums. We considered cases where  H 1 subscript H 1 H_{1} italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  was fixed and  H 2  U  ( 0 , 1 ) similar-to subscript H 2 U 0 1 H_{2}\\sim U(0,1) italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  italic_U ( 0 , 1 )  was random. The resulting scatter plot is shown in Figure  13 . Apparently,  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  tends to infer values  H ^  ( H 1 , H 2 ) ^ H subscript H 1 subscript H 2 \\hat{H}\\in(H_{1},H_{2}) over^ start_ARG italic_H end_ARG  ( italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) . Consequently, the model appears not to learn either the box dimension or the memory explicitly but rather an intermediate quantity. Interestingly, when  H 2 subscript H 2 H_{2} italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  is near 1, the estimates provided by  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  are smaller than the average of  H 1 subscript H 1 H_{1} italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  H 2 subscript H 2 H_{2} italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . Conversely, when  H 2 > 0.5 subscript H 2 0.5 H_{2}>0.5 italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT > 0.5  is not close to 1 and  H 1 < 0.5 subscript H 1 0.5 H_{1}<0.5 italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < 0.5 , the estimates are larger than the average, suggesting that the memory aspect associated with  H 2 subscript H 2 H_{2} italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  has a greater influence. This behavior suggests that the estimator captures different characteristics of the process, and that in different  H 2 subscript H 2 H_{2} italic_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  ranges, different characteristics dominate the sum."
        ]
    },
    "id_table_14": {
        "caption": "",
        "table": "A6.T12.37.33",
        "footnotes": [],
        "references": [
            "Several  M LSTM subscript M LSTM M_{\\text{LSTM}} italic_M start_POSTSUBSCRIPT LSTM end_POSTSUBSCRIPT  Hurst-estimators, fine-tuned on fBm sequences of different lengths, were evaluated on    \\alpha italic_ -stable Levy processes, with outcomes illustrated in Figure  14 . Notably, for   = 2  2 \\alpha=2 italic_ = 2 , the characteristics of an    \\alpha italic_ -stable Levy process overlap with those of standard Brownian motion, which corresponds to a fBm with a Hurst parameter of  H = 0.5 H 0.5 H=0.5 italic_H = 0.5 . Thus, the inferred value is around  0.5 0.5 0.5 0.5  at   = 2  2 \\alpha=2 italic_ = 2 , as anticipated."
        ]
    }
}