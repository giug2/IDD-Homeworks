{
    "id_table_1": {
        "caption": "Table 1:  Scene description results.",
        "table": "S5.T1.1",
        "footnotes": [],
        "references": [
            "In this work, we present a synthetic data generation pipeline designed to enhance the performance of assistive robotic systems for BLV individuals as shown in Fig.  1 . Our generated synthetic datasets are tailored for key tasks, including tactile paving detection and scene description, which can be critical for safe street crossing. We evaluate the effectiveness of state-of-the-art object detection and vision-language models trained on our synthetic data, demonstrating that synthetic data can improve model performance across these tasks. To facilitate further research and development in assistive technologies, we have made our synthetic dataset publicly available.",
            "As shown in Table  1 , incorporating additional real-world and synthetic data led to comparable performance improvements over the baseline, except for precision. Although both types of data augmentation yielded similar benefits, real-world data demonstrated slightly higher performance in terms of precision and recall. The higher BLEU scores highlight the potential of synthetic data in enhancing vision-language models to generate the precise wording preferred in scene descriptions."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Qualitative results for the scene description task.",
        "table": "S5.T2.2",
        "footnotes": [],
        "references": [
            "In this work, we propose a synthetic dataset generation pipeline utilizing Unreal Engine 4 (UE4) with the NVIDIA Deep Learning Dataset Synthesizer (NDDS) plugin  [ 24 ]  for photorealistic rendering and automated data annotation. This approach enables the generation of ground-truth labels and synthetic data customized for specific robotic mobility aids, accounting for variations in viewpoints and lighting conditions. The synthetic data generation environment of the pipeline is illustrated in Fig.  2 .",
            "We collected data from two Environment packages available from the UE4 Marketplace, designed to reflect real-world scenarios. The City Park environment covers land with varying terrains, objects, and road paths, closely resembling a typical park setting. This pre-made environment includes approximately 800 objects commonly found in parks, such as benches, water fountains, and varying vegetation. The Suburban environment includes urban roads and sidewalks populated with a variety of objects such as buildings, traffic lights, and curbs (see Fig.  2  (a)). Although this environment is smaller in scale, it is densely packed with around 2,000 objects, providing a detailed urban setting compared to the city park. To enhance the realism of the synthetic data, we generated datasets under varying lighting and weather conditions, reflecting the complexities of real-world environments.",
            "Our pipeline leverages UE4s flexibility to simulate various viewpoints corresponding to different robotic mobility aids. This allows for efficient collection of synthetic data from multiple perspectives, accommodating the unique camera placements of these aids (e.g., front, side, and bottom-facing cameras), which often present significant differences in visual appearance. In both environments, we established a range of camera trajectories to capture diverse viewpoints, ensuring a comprehensive representation of the perspectives likely encountered by robotic systems in real-world applications as shown in Fig.  2  (b).",
            "Qualitative results of the fine-tuned Florence-2 model are presented in Table  2 , where the model exhibited strong performance in generating accurate descriptions across varied conditions, though some inaccuracies and missing information were found. For example, the model did not provide information on the pedestrian light in the first example in Table  2 ."
        ]
    }
}