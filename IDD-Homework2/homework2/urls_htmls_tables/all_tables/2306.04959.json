{
    "PAPER'S NUMBER OF TABLES": 3,
    "S1.T1": {
        "caption": "Table 1. Types of Defenses and Their Implementations Against Specific Attacks",
        "table": "<table id=\"S1.T1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S1.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S1.T1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Type of Defenses</span></td>\n<td id=\"S1.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S1.T1.1.2.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Implementations</span></td>\n<td id=\"S1.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S1.T1.1.2.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Type of Attacks Against</span></td>\n</tr>\n<tr id=\"S1.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"7\"><span id=\"S1.T1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Before-aggregation defenses</span></td>\n<td id=\"S1.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.3.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">SLSGDÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.3.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Xie etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.3.2.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib89\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T1.1.3.2.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S1.T1.1.3.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Data poisoning attacks, e.g., label flipping backdoor attackÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.3.2.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tolpegin etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.3.2.3.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib81\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T1.1.3.2.3.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T1.1.3.2.3.5\" class=\"ltx_text\" style=\"font-size:80%;\">,</span>\n</td>\n</tr>\n<tr id=\"S1.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.4.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Residual Reweighting DefenseÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.4.3.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Fu etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.4.3.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S1.T1.1.4.3.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S1.T1.1.4.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Backdoor attacks</span></td>\n</tr>\n<tr id=\"S1.T1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.5.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FoolsgoldÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.5.4.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Fung etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.5.4.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T1.1.5.4.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S1.T1.1.5.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Backdoor attacks and Byzantine attacks</span></td>\n</tr>\n<tr id=\"S1.T1.1.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">KrumÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Blanchard etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S1.T1.1.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T1.1.1.1.5\" class=\"ltx_text\" style=\"font-size:80%;\"> â€ƒ</span><math id=\"S1.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics id=\"S1.T1.1.1.1.m1.1a\"><mi mathsize=\"80%\" id=\"S1.T1.1.1.1.m1.1.1\" xref=\"S1.T1.1.1.1.m1.1.1.cmml\">m</mi><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.1.1.1.m1.1b\"><ci id=\"S1.T1.1.1.1.m1.1.1.cmml\" xref=\"S1.T1.1.1.1.m1.1.1\">ğ‘š</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.1.1.1.m1.1c\">m</annotation></semantics></math><span id=\"S1.T1.1.1.1.6\" class=\"ltx_text\" style=\"font-size:80%;\">-KrumÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.1.1.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Blanchard etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.1.1.8.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S1.T1.1.1.1.9.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T1.1.1.1.10\" class=\"ltx_text\" style=\"font-size:80%;\">â€ƒCClipÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.1.1.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Karimireddy etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.1.1.12.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T1.1.1.1.13.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T1.1.1.1.14\" class=\"ltx_text\" style=\"font-size:80%;\">â€ƒweak DPÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.1.1.15.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Sun etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.1.1.16.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib79\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S1.T1.1.1.1.17.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Model poisoning attacks, <span id=\"S1.T1.1.1.2.1.1\" class=\"ltx_text ltx_font_italic\">e</span>.<span id=\"S1.T1.1.1.2.1.2\" class=\"ltx_text ltx_font_italic\">g</span>., Byzantine attacksÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen etÂ al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2017</a>; Fang etÂ al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2020</a>; Lin etÂ al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">2019</a>)</cite> or Backdoor attacks that attack by poisoning model updates Â <cite class=\"ltx_cite ltx_citemacro_citep\">(Bagdasaryan etÂ al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">2020</a>)</cite></span></td>\n</tr>\n<tr id=\"S1.T1.1.6.5\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S1.T1.1.6.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Norm ClippingÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.6.5.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Sun etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.6.5.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib79\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S1.T1.1.6.5.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T1.1.6.5.1.5\" class=\"ltx_text\" style=\"font-size:80%;\">â€ƒFl-wbcÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.6.5.1.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Sun etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.6.5.1.7.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib78\" title=\"\" class=\"ltx_ref\">2021</a><span id=\"S1.T1.1.6.5.1.8.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T1.1.6.5.1.9\" class=\"ltx_text\" style=\"font-size:80%;\"> â€ƒBulyan DefenseÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.6.5.1.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Guerraoui etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.6.5.1.11.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S1.T1.1.6.5.1.12.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n</tr>\n<tr id=\"S1.T1.1.7.6\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S1.T1.1.7.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">coordinate-wise medianÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.7.6.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Yin etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.7.6.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib93\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S1.T1.1.7.6.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n</tr>\n<tr id=\"S1.T1.1.8.7\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S1.T1.1.8.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">coordinate-wise trimmed meanÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.8.7.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Yin etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.8.7.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib93\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S1.T1.1.8.7.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n</tr>\n<tr id=\"S1.T1.1.9.8\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.1.9.8.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">On-aggregation defenses</span></td>\n<td id=\"S1.T1.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.9.8.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Robust Learning RateÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.9.8.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Ozdayi etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.9.8.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib65\" title=\"\" class=\"ltx_ref\">2021</a><span id=\"S1.T1.1.9.8.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S1.T1.1.9.8.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Backdoor attacks</span></td>\n</tr>\n<tr id=\"S1.T1.1.10.9\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.10.9.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">SLSGDÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.10.9.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Xie etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.10.9.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib89\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T1.1.10.9.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S1.T1.1.10.9.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Data poisoning attacks, e.g., label flipping backdoor attackÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.10.9.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tolpegin etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.10.9.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib81\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T1.1.10.9.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T1.1.10.9.2.5\" class=\"ltx_text\" style=\"font-size:80%;\">,</span>\n</td>\n</tr>\n<tr id=\"S1.T1.1.11.10\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.11.10.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.11.10.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">geometric medianÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.11.10.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Chen etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.11.10.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S1.T1.1.11.10.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S1.T1.1.11.10.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Byzantine attacks</span></td>\n</tr>\n<tr id=\"S1.T1.1.12.11\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.12.11.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.12.11.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">RFAÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.12.11.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Pillutla etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.12.11.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib66\" title=\"\" class=\"ltx_ref\">2022</a><span id=\"S1.T1.1.12.11.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.12.11.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S1.T1.1.12.11.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Byzantine attacks</span></td>\n</tr>\n<tr id=\"S1.T1.1.13.12\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.13.12.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S1.T1.1.13.12.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">After-aggregation defenses</span></td>\n<td id=\"S1.T1.1.13.12.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.13.12.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">CClipÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.13.12.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Karimireddy etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.13.12.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T1.1.13.12.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.13.12.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S1.T1.1.13.12.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Byzantine attacks or backdoor attacks</span></td>\n</tr>\n<tr id=\"S1.T1.1.14.13\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.14.13.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.14.13.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">CRFLÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.14.13.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Xie etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.14.13.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib88\" title=\"\" class=\"ltx_ref\">2021</a><span id=\"S1.T1.1.14.13.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.14.13.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S1.T1.1.14.13.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Backdoor attacks</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Federated Learning (FL)Â ",
                "(McMahan etÂ al",
                ".",
                ", ",
                "2017a",
                ")",
                " facilitates training across distributed data and empowers individual clients to utilize their local data to collaboratively train machine learning models. Instead of collecting data to a centralized server, FL clients train models on their local data and share the local models with the FL server, where the local models are aggregated into a global model.",
                "FL has attracted considerable attention across various domains and has been utilized in numerous areas such as next-word predictionÂ ",
                "(Hard etÂ al",
                ".",
                ", ",
                "2018",
                "; Chen etÂ al",
                ".",
                ", ",
                "2019",
                "; Ramaswamy etÂ al",
                ".",
                ", ",
                "2019",
                ")",
                ", hot-word detectionÂ ",
                "(Leroy etÂ al",
                ".",
                ", ",
                "2019",
                ")",
                ", financial risk assessmentÂ ",
                "(Byrd and Polychroniadou, ",
                "2020",
                ")",
                ", and cancer risk predictionÂ ",
                "(Chowdhury etÂ al",
                ".",
                ", ",
                "2022",
                ")",
                ", demonstrating its wide-ranging versatility.\nRecently, FL has found applications in large language models (LLMs) that expand its use cases. Referred to as ",
                "federated LLMs",
                ", these models utilize FL during pre-training and finetuning as well as for prompt engineering ",
                "(Chen etÂ al",
                ".",
                ", ",
                "2023",
                ")",
                ". Currently, there are industry products that utilize FL (or distributed training) to train LLMs, including Deepspeed ZeROÂ ",
                "(Rajbhandari etÂ al",
                ".",
                ", ",
                "2020",
                "; Wang etÂ al",
                ".",
                ", ",
                "2023",
                ")",
                ", HuggingFace AccelerateÂ ",
                "(Gugger, ",
                "2021",
                ")",
                ", Pytorch Lightning FabricÂ ",
                "(Antiga, ",
                "2023",
                ")",
                ". FL can facilitate LLM training due to the following reasons: ",
                "i",
                ") ",
                "Distributed nature of LLM training data:",
                " LLMs are pre-trained using large amounts of data, which often reside in different locations. Collecting such data to a central server is expensive and may also leak sensitive user information, while a viable way is to train LLMs in a federated manner.\n",
                "ii",
                ") ",
                "Scalability and efficiency:",
                " LLMs, such as GPT-3Â ",
                "(Brown etÂ al",
                ".",
                ", ",
                "2020",
                ")",
                ", have an extremely large number of parameters. Training LLMs on a single machine is infeasible and inflexible, while FL can be a good choice.\n",
                "iii",
                ") ",
                "Continuous improvement with user data:",
                " LLMs can be deployed in a federated manner and local instances of the models can be further finetuned based on the local data,\nenabling the global model to improve over time based on usersâ€™ data without ever having direct access to that data. This is particularly relevant for privacy-sensitive fields such as healthcare or personal communications.",
                "FL, as well as federated LLMs, ",
                "aims to",
                " maintain privacy and security of client data by allowing clients to train locally without spreading their data to other parties.\nHowever, its decentralized and collaborative nature might inadvertently introduce privacy and security vulnerabilities.\n",
                "Recent works have spotlighted specific attack mechanisms in FL. Adversarial clients compromise the integrity of global model by submitting spurious models to prevent the global model from convergingÂ ",
                "(Chen etÂ al",
                ".",
                ", ",
                "2017",
                "; Fang etÂ al",
                ".",
                ", ",
                "2020",
                "; Lin etÂ al",
                ".",
                ", ",
                "2019",
                "; Baruch etÂ al",
                ".",
                ", ",
                "2019",
                "; Bagdasaryan etÂ al",
                ".",
                ", ",
                "2020",
                "; Wang, ",
                "2022",
                "; Fraboni etÂ al",
                ".",
                ", ",
                "2021",
                ")",
                "), manipulating data samples to induce the global model to mis-classify specific samplesÂ ",
                "(Tolpegin etÂ al",
                ".",
                ", ",
                "2020",
                "; Wang etÂ al",
                ".",
                ", ",
                "2020b",
                ")",
                ", and / or planting backdoorsÂ ",
                "(Baruch etÂ al",
                ".",
                ", ",
                "2019",
                "; Bagdasaryan etÂ al",
                ".",
                ", ",
                "2020",
                "; Tolpegin etÂ al",
                ".",
                ", ",
                "2020",
                "; Wang etÂ al",
                ".",
                ", ",
                "2020b",
                ")",
                ". Adversaries can also reconstruct private data from shared model updatesÂ ",
                "(Zhu etÂ al",
                ".",
                ", ",
                "2019",
                "; Geiping etÂ al",
                ".",
                ", ",
                "2020",
                "; Dang etÂ al",
                ".",
                ", ",
                "2021",
                ")",
                ". ",
                "\nMeanwhile, a wide range of defense mechanisms has emerged to mitigate the impact of these attacksÂ ",
                "(Li etÂ al",
                ".",
                ", ",
                "2022",
                "; Kumari etÂ al",
                ".",
                ", ",
                "2023",
                "; Sun etÂ al",
                ".",
                ", ",
                "2019",
                "; Ozdayi etÂ al",
                ".",
                ", ",
                "2021",
                "; Blanchard etÂ al",
                ".",
                ", ",
                "2017",
                "; Xie etÂ al",
                ".",
                ", ",
                "2020",
                "; Chen etÂ al",
                ".",
                ", ",
                "2017",
                "; Sun etÂ al",
                ".",
                ", ",
                "2019",
                "; Karimireddy etÂ al",
                ".",
                ", ",
                "2020",
                "; Yin etÂ al",
                ".",
                ", ",
                "2018",
                "; Pillutla etÂ al",
                ".",
                ", ",
                "2022",
                "; Fung etÂ al",
                ".",
                ", ",
                "2020",
                "; Xie etÂ al",
                ".",
                ", ",
                "2021",
                "; Yin etÂ al",
                ".",
                ", ",
                "2018",
                "; Ma etÂ al",
                ".",
                ", ",
                "2022",
                "; Kumar etÂ al",
                ".",
                ", ",
                "2022",
                "; Chen etÂ al",
                ".",
                ", ",
                "2022",
                ")",
                ".\nDespite the efforts for addressing the vulnerability of FL systems, there still lacks a comprehensive benchmark for comparing approaches under unified sittings.\nMoreover, while existing works have explored effectiveness of attacks and defenses on small-scale models, there remains a significant gap in understanding how these mechanisms perform against large-scale models, such as LLMs. Given that LLMs possess a large number of parameters and are trained on complex datasets obtained from unregulated sources, the effectiveness of attacks and defenses may be diminished when applied to them.\nThese motivate an urgent need for a standardized and comprehensive benchmark to evaluate baseline attack and defense mechanisms in the context of FL and federated LLMs.",
                "This paper introduces FedMLSecurity",
                "1",
                "1",
                "1",
                "Code: https://github.com/FedML-AI/FedML/tree/master/python/fedml/core/security",
                ", a benchmark that simulates attacks and defenses in FedMLÂ ",
                "(He etÂ al",
                ".",
                ", ",
                "2020b",
                ")",
                ".\nFedMLSecurity comprises two primary components: FedMLAttacker and FedMLDefender.\nFedMLAttacker simulates attacks in FL to help understand and prepare for potential security risks, while\nFedMLDefender is equipped with ",
                "state-of-the-arts",
                " defense mechanisms to counteract the attacks injected by FedMLAttacker. We summarize our contributions as follows:\n",
                "i",
                ") Enabling benchmarking of several different attacks and defenses in FL",
                ". FedMLSecurity implements attacks and defenses that are widely considered in the literature. ",
                "We summarize the defenses and the attacks in TableÂ ",
                "1",
                " and TableÂ ",
                "2",
                ", respectively.",
                "ii",
                ") Supporting flexible configuration and customization. ",
                "\nFedMLSecurity supports configurations using a .yaml file. Sample configurations for attacks and defenses are shown in FiguresÂ ",
                "1(a)",
                "\nand FiguresÂ ",
                "1(b)",
                ", respectively. FedMLSecurity also provides APIs to enable customizing attacks and defenses.",
                "iii",
                ") Supporting various models and FL optimizers.",
                "\nFedMLSecurity can be utilized with a wide range of models, including Logistic Regression, LeNetÂ ",
                "(LeCun etÂ al",
                ".",
                ", ",
                "1998",
                ")",
                ", ResNetÂ ",
                "(He etÂ al",
                ".",
                ", ",
                "2015",
                ")",
                ", CNNÂ ",
                "(LeCun etÂ al",
                ".",
                ", ",
                "1989",
                ")",
                ", RNNÂ ",
                "(Rumelhart etÂ al",
                ".",
                ", ",
                "1986",
                ")",
                ", GANÂ ",
                "(Goodfellow etÂ al",
                ".",
                ", ",
                "2014",
                ")",
                ", and so on. FedMLSecurity is compatible with various FL optimizers, such as FedAVGÂ ",
                "(McMahan etÂ al",
                ".",
                ", ",
                "2016",
                ")",
                ", FedSGDÂ ",
                "(Shokri and Shmatikov, ",
                "2015",
                ")",
                ", FedOPTÂ ",
                "(Reddi etÂ al",
                ".",
                ", ",
                "2021",
                ")",
                ", FedPROXÂ ",
                "(Li etÂ al",
                ".",
                ", ",
                "2020",
                ")",
                ", FedGKTÂ ",
                "(He etÂ al",
                ".",
                ", ",
                "2020a",
                ")",
                ", FedGANÂ ",
                "(Rasouli etÂ al",
                ".",
                ", ",
                "2020",
                ")",
                ", FedNASÂ ",
                "(He etÂ al",
                ".",
                ", ",
                "2021",
                ")",
                ", FedNOVAÂ ",
                "(Wang etÂ al",
                ".",
                ", ",
                "2020a",
                ")",
                ", etc.",
                "iv",
                ") Extensions to federated LLMs and real-world applications.",
                "\nFedMLSecurity can simulate attacks and defenses during training of federated LLMs. It can also be integrated with real-world FL applications; see ",
                "Exp 7",
                ", where we utilize edge devices from Theta NetworkÂ ",
                "(Theta Network., ",
                "2023",
                ")",
                " instead of simulations.",
                "Key takeaways",
                ": ",
                "i",
                ")\nWhile defense mechanisms can help mitigate attacks, it might also bring a potential loss of accuracy to the aggregation results. Therefore, when integrating defenses into FL applications, itâ€™s crucial to weigh the benefits against potential drawbacks.\n",
                "ii",
                ") Nearly all existing defense mechanisms are impractical in real-world FL applications, as they compromise accuracy even if no attack happened. A defense that is practical for real-world systems is in need, where the defense should satisfy: 1) it must detect if attacks have happened, and only activates defensive mechanisms when attacks are detected; and 2) it must identify malicious clients accurately without harming benign local models. "
            ]
        ]
    },
    "S1.T2": {
        "caption": "Table 2. Attacks Implemented in FedMLSecurity",
        "table": "<table id=\"S1.T2.6\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S1.T2.6.1.1\" class=\"ltx_tr\">\n<th id=\"S1.T2.6.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">\n<span id=\"S1.T2.6.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.1.1.1.1.1\" class=\"ltx_p\" style=\"width:74.0pt;\"><span id=\"S1.T2.6.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Type of Attacks</span></span>\n</span>\n</th>\n<th id=\"S1.T2.6.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S1.T2.6.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.1.1.2.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Implementations</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S1.T2.6.2.1\" class=\"ltx_tr\">\n<th id=\"S1.T2.6.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"4\">\n<span id=\"S1.T2.6.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.2.1.1.1.1\" class=\"ltx_p\" style=\"width:74.0pt;\"><span id=\"S1.T2.6.2.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Model poisoning attacks</span></span>\n</span>\n</th>\n<td id=\"S1.T2.6.2.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S1.T2.6.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.2.1.2.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.2.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Byzantine attackÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.2.1.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Chen etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.2.1.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2017</a>; Fang etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.2.1.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2020</a>; Lin etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.2.1.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S1.T2.6.2.1.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T2.6.2.1.2.1.1.5\" class=\"ltx_text\" style=\"font-size:80%;\">: (1) zero mode (2) random mode (3) flipping mode</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T2.6.3.2\" class=\"ltx_tr\">\n<td id=\"S1.T2.6.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S1.T2.6.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.3.2.1.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.3.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Minimizing Distance Backdoor AttackÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.3.2.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Baruch etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.3.2.1.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S1.T2.6.3.2.1.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T2.6.4.3\" class=\"ltx_tr\">\n<td id=\"S1.T2.6.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S1.T2.6.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.4.3.1.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.4.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Model Replacement Backdoor AttackÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.4.3.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Bagdasaryan etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.4.3.1.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T2.6.4.3.1.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T2.6.5.4\" class=\"ltx_tr\">\n<td id=\"S1.T2.6.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S1.T2.6.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.5.4.1.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.5.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Lazy Worker (or Free Rider) AttackÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.5.4.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Wang<span id=\"S1.T2.6.5.4.1.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib85\" title=\"\" class=\"ltx_ref\">2022</a>; Fraboni etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.5.4.1.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">2021</a><span id=\"S1.T2.6.5.4.1.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T2.6.6.5\" class=\"ltx_tr\">\n<th id=\"S1.T2.6.6.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"2\">\n<span id=\"S1.T2.6.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.6.5.1.1.1\" class=\"ltx_p\" style=\"width:74.0pt;\"><span id=\"S1.T2.6.6.5.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Data poisoning attacks</span></span>\n</span>\n</th>\n<td id=\"S1.T2.6.6.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S1.T2.6.6.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.6.5.2.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.6.5.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Label Flipping Backdoor attackÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.6.5.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tolpegin etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.6.5.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib81\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T2.6.6.5.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T2.6.7.6\" class=\"ltx_tr\">\n<td id=\"S1.T2.6.7.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S1.T2.6.7.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.7.6.1.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.7.6.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Edge Case Backdoor AttackÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.7.6.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Wang etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.7.6.1.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib84\" title=\"\" class=\"ltx_ref\">2020b</a><span id=\"S1.T2.6.7.6.1.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T2.6.8.7\" class=\"ltx_tr\">\n<th id=\"S1.T2.6.8.7.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"3\">\n<span id=\"S1.T2.6.8.7.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.8.7.1.1.1\" class=\"ltx_p\" style=\"width:74.0pt;\"><span id=\"S1.T2.6.8.7.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Data reconstruction attacks</span></span>\n</span>\n</th>\n<td id=\"S1.T2.6.8.7.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S1.T2.6.8.7.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.8.7.2.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.8.7.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Deep Leakage AttackÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.8.7.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Zhu etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.8.7.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib96\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S1.T2.6.8.7.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T2.6.9.8\" class=\"ltx_tr\">\n<td id=\"S1.T2.6.9.8.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S1.T2.6.9.8.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.9.8.1.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.9.8.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Inverting Gradient AttackÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.9.8.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Geiping etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.9.8.1.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T2.6.9.8.1.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T2.6.10.9\" class=\"ltx_tr\">\n<td id=\"S1.T2.6.10.9.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span id=\"S1.T2.6.10.9.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.10.9.1.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.10.9.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Revealing Labels AttackÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.10.9.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Dang etÂ al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.10.9.1.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">2021</a><span id=\"S1.T2.6.10.9.1.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Federated Learning (FL)Â ",
                "(McMahan etÂ al",
                ".",
                ", ",
                "2017a",
                ")",
                " facilitates training across distributed data and empowers individual clients to utilize their local data to collaboratively train machine learning models. Instead of collecting data to a centralized server, FL clients train models on their local data and share the local models with the FL server, where the local models are aggregated into a global model.",
                "FL has attracted considerable attention across various domains and has been utilized in numerous areas such as next-word predictionÂ ",
                "(Hard etÂ al",
                ".",
                ", ",
                "2018",
                "; Chen etÂ al",
                ".",
                ", ",
                "2019",
                "; Ramaswamy etÂ al",
                ".",
                ", ",
                "2019",
                ")",
                ", hot-word detectionÂ ",
                "(Leroy etÂ al",
                ".",
                ", ",
                "2019",
                ")",
                ", financial risk assessmentÂ ",
                "(Byrd and Polychroniadou, ",
                "2020",
                ")",
                ", and cancer risk predictionÂ ",
                "(Chowdhury etÂ al",
                ".",
                ", ",
                "2022",
                ")",
                ", demonstrating its wide-ranging versatility.\nRecently, FL has found applications in large language models (LLMs) that expand its use cases. Referred to as ",
                "federated LLMs",
                ", these models utilize FL during pre-training and finetuning as well as for prompt engineering ",
                "(Chen etÂ al",
                ".",
                ", ",
                "2023",
                ")",
                ". Currently, there are industry products that utilize FL (or distributed training) to train LLMs, including Deepspeed ZeROÂ ",
                "(Rajbhandari etÂ al",
                ".",
                ", ",
                "2020",
                "; Wang etÂ al",
                ".",
                ", ",
                "2023",
                ")",
                ", HuggingFace AccelerateÂ ",
                "(Gugger, ",
                "2021",
                ")",
                ", Pytorch Lightning FabricÂ ",
                "(Antiga, ",
                "2023",
                ")",
                ". FL can facilitate LLM training due to the following reasons: ",
                "i",
                ") ",
                "Distributed nature of LLM training data:",
                " LLMs are pre-trained using large amounts of data, which often reside in different locations. Collecting such data to a central server is expensive and may also leak sensitive user information, while a viable way is to train LLMs in a federated manner.\n",
                "ii",
                ") ",
                "Scalability and efficiency:",
                " LLMs, such as GPT-3Â ",
                "(Brown etÂ al",
                ".",
                ", ",
                "2020",
                ")",
                ", have an extremely large number of parameters. Training LLMs on a single machine is infeasible and inflexible, while FL can be a good choice.\n",
                "iii",
                ") ",
                "Continuous improvement with user data:",
                " LLMs can be deployed in a federated manner and local instances of the models can be further finetuned based on the local data,\nenabling the global model to improve over time based on usersâ€™ data without ever having direct access to that data. This is particularly relevant for privacy-sensitive fields such as healthcare or personal communications.",
                "FL, as well as federated LLMs, ",
                "aims to",
                " maintain privacy and security of client data by allowing clients to train locally without spreading their data to other parties.\nHowever, its decentralized and collaborative nature might inadvertently introduce privacy and security vulnerabilities.\n",
                "Recent works have spotlighted specific attack mechanisms in FL. Adversarial clients compromise the integrity of global model by submitting spurious models to prevent the global model from convergingÂ ",
                "(Chen etÂ al",
                ".",
                ", ",
                "2017",
                "; Fang etÂ al",
                ".",
                ", ",
                "2020",
                "; Lin etÂ al",
                ".",
                ", ",
                "2019",
                "; Baruch etÂ al",
                ".",
                ", ",
                "2019",
                "; Bagdasaryan etÂ al",
                ".",
                ", ",
                "2020",
                "; Wang, ",
                "2022",
                "; Fraboni etÂ al",
                ".",
                ", ",
                "2021",
                ")",
                "), manipulating data samples to induce the global model to mis-classify specific samplesÂ ",
                "(Tolpegin etÂ al",
                ".",
                ", ",
                "2020",
                "; Wang etÂ al",
                ".",
                ", ",
                "2020b",
                ")",
                ", and / or planting backdoorsÂ ",
                "(Baruch etÂ al",
                ".",
                ", ",
                "2019",
                "; Bagdasaryan etÂ al",
                ".",
                ", ",
                "2020",
                "; Tolpegin etÂ al",
                ".",
                ", ",
                "2020",
                "; Wang etÂ al",
                ".",
                ", ",
                "2020b",
                ")",
                ". Adversaries can also reconstruct private data from shared model updatesÂ ",
                "(Zhu etÂ al",
                ".",
                ", ",
                "2019",
                "; Geiping etÂ al",
                ".",
                ", ",
                "2020",
                "; Dang etÂ al",
                ".",
                ", ",
                "2021",
                ")",
                ". ",
                "\nMeanwhile, a wide range of defense mechanisms has emerged to mitigate the impact of these attacksÂ ",
                "(Li etÂ al",
                ".",
                ", ",
                "2022",
                "; Kumari etÂ al",
                ".",
                ", ",
                "2023",
                "; Sun etÂ al",
                ".",
                ", ",
                "2019",
                "; Ozdayi etÂ al",
                ".",
                ", ",
                "2021",
                "; Blanchard etÂ al",
                ".",
                ", ",
                "2017",
                "; Xie etÂ al",
                ".",
                ", ",
                "2020",
                "; Chen etÂ al",
                ".",
                ", ",
                "2017",
                "; Sun etÂ al",
                ".",
                ", ",
                "2019",
                "; Karimireddy etÂ al",
                ".",
                ", ",
                "2020",
                "; Yin etÂ al",
                ".",
                ", ",
                "2018",
                "; Pillutla etÂ al",
                ".",
                ", ",
                "2022",
                "; Fung etÂ al",
                ".",
                ", ",
                "2020",
                "; Xie etÂ al",
                ".",
                ", ",
                "2021",
                "; Yin etÂ al",
                ".",
                ", ",
                "2018",
                "; Ma etÂ al",
                ".",
                ", ",
                "2022",
                "; Kumar etÂ al",
                ".",
                ", ",
                "2022",
                "; Chen etÂ al",
                ".",
                ", ",
                "2022",
                ")",
                ".\nDespite the efforts for addressing the vulnerability of FL systems, there still lacks a comprehensive benchmark for comparing approaches under unified sittings.\nMoreover, while existing works have explored effectiveness of attacks and defenses on small-scale models, there remains a significant gap in understanding how these mechanisms perform against large-scale models, such as LLMs. Given that LLMs possess a large number of parameters and are trained on complex datasets obtained from unregulated sources, the effectiveness of attacks and defenses may be diminished when applied to them.\nThese motivate an urgent need for a standardized and comprehensive benchmark to evaluate baseline attack and defense mechanisms in the context of FL and federated LLMs.",
                "This paper introduces FedMLSecurity",
                "1",
                "1",
                "1",
                "Code: https://github.com/FedML-AI/FedML/tree/master/python/fedml/core/security",
                ", a benchmark that simulates attacks and defenses in FedMLÂ ",
                "(He etÂ al",
                ".",
                ", ",
                "2020b",
                ")",
                ".\nFedMLSecurity comprises two primary components: FedMLAttacker and FedMLDefender.\nFedMLAttacker simulates attacks in FL to help understand and prepare for potential security risks, while\nFedMLDefender is equipped with ",
                "state-of-the-arts",
                " defense mechanisms to counteract the attacks injected by FedMLAttacker. We summarize our contributions as follows:\n",
                "i",
                ") Enabling benchmarking of several different attacks and defenses in FL",
                ". FedMLSecurity implements attacks and defenses that are widely considered in the literature. ",
                "We summarize the defenses and the attacks in TableÂ ",
                "1",
                " and TableÂ ",
                "2",
                ", respectively.",
                "ii",
                ") Supporting flexible configuration and customization. ",
                "\nFedMLSecurity supports configurations using a .yaml file. Sample configurations for attacks and defenses are shown in FiguresÂ ",
                "1(a)",
                "\nand FiguresÂ ",
                "1(b)",
                ", respectively. FedMLSecurity also provides APIs to enable customizing attacks and defenses.",
                "iii",
                ") Supporting various models and FL optimizers.",
                "\nFedMLSecurity can be utilized with a wide range of models, including Logistic Regression, LeNetÂ ",
                "(LeCun etÂ al",
                ".",
                ", ",
                "1998",
                ")",
                ", ResNetÂ ",
                "(He etÂ al",
                ".",
                ", ",
                "2015",
                ")",
                ", CNNÂ ",
                "(LeCun etÂ al",
                ".",
                ", ",
                "1989",
                ")",
                ", RNNÂ ",
                "(Rumelhart etÂ al",
                ".",
                ", ",
                "1986",
                ")",
                ", GANÂ ",
                "(Goodfellow etÂ al",
                ".",
                ", ",
                "2014",
                ")",
                ", and so on. FedMLSecurity is compatible with various FL optimizers, such as FedAVGÂ ",
                "(McMahan etÂ al",
                ".",
                ", ",
                "2016",
                ")",
                ", FedSGDÂ ",
                "(Shokri and Shmatikov, ",
                "2015",
                ")",
                ", FedOPTÂ ",
                "(Reddi etÂ al",
                ".",
                ", ",
                "2021",
                ")",
                ", FedPROXÂ ",
                "(Li etÂ al",
                ".",
                ", ",
                "2020",
                ")",
                ", FedGKTÂ ",
                "(He etÂ al",
                ".",
                ", ",
                "2020a",
                ")",
                ", FedGANÂ ",
                "(Rasouli etÂ al",
                ".",
                ", ",
                "2020",
                ")",
                ", FedNASÂ ",
                "(He etÂ al",
                ".",
                ", ",
                "2021",
                ")",
                ", FedNOVAÂ ",
                "(Wang etÂ al",
                ".",
                ", ",
                "2020a",
                ")",
                ", etc.",
                "iv",
                ") Extensions to federated LLMs and real-world applications.",
                "\nFedMLSecurity can simulate attacks and defenses during training of federated LLMs. It can also be integrated with real-world FL applications; see ",
                "Exp 7",
                ", where we utilize edge devices from Theta NetworkÂ ",
                "(Theta Network., ",
                "2023",
                ")",
                " instead of simulations.",
                "Key takeaways",
                ": ",
                "i",
                ")\nWhile defense mechanisms can help mitigate attacks, it might also bring a potential loss of accuracy to the aggregation results. Therefore, when integrating defenses into FL applications, itâ€™s crucial to weigh the benefits against potential drawbacks.\n",
                "ii",
                ") Nearly all existing defense mechanisms are impractical in real-world FL applications, as they compromise accuracy even if no attack happened. A defense that is practical for real-world systems is in need, where the defense should satisfy: 1) it must detect if attacks have happened, and only activates defensive mechanisms when attacks are detected; and 2) it must identify malicious clients accurately without harming benign local models. "
            ]
        ]
    },
    "S4.T3": {
        "caption": "Table 3. Models and datasets for evaluations.",
        "table": "<table id=\"S4.T3.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.4.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S4.T3.4.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<td id=\"S4.T3.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.4.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">ResNet20Â </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.1.1.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>He etÂ al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.1.1.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S4.T3.4.1.1.2.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.4.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">ResNet56Â </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.1.1.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>He etÂ al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.1.1.3.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S4.T3.4.1.1.3.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.4.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">CNNÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.1.1.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>McMahan etÂ al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.1.1.4.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib60\" title=\"\" class=\"ltx_ref\">2017a</a><span id=\"S4.T3.4.1.1.4.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.4.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">RNN (bi-LSTM)Â </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.1.1.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>McMahan etÂ al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.1.1.5.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib60\" title=\"\" class=\"ltx_ref\">2017a</a><span id=\"S4.T3.4.1.1.5.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.4.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">BERTÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.1.1.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Devlin etÂ al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.1.1.6.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S4.T3.4.1.1.6.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.4.1.1.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">Pythia-1BÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.1.1.7.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Biderman etÂ al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.1.1.7.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2023</a><span id=\"S4.T3.4.1.1.7.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n</tr>\n<tr id=\"S4.T3.4.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.2.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset</span></th>\n<td id=\"S4.T3.4.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<span id=\"S4.T3.4.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">CIFAR10Â </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Krizhevsky etÂ al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.2.2.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2009</a><span id=\"S4.T3.4.2.2.2.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<span id=\"S4.T3.4.2.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">CIFAR100Â </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.2.2.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Krizhevsky etÂ al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.2.2.3.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2009</a><span id=\"S4.T3.4.2.2.3.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<span id=\"S4.T3.4.2.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">FEMNISTÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.2.2.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Caldas etÂ al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.2.2.4.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S4.T3.4.2.2.4.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<span id=\"S4.T3.4.2.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">ShakespeareÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.2.2.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>McMahan etÂ al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.2.2.5.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib61\" title=\"\" class=\"ltx_ref\">2017b</a><span id=\"S4.T3.4.2.2.5.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<span id=\"S4.T3.4.2.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">20NewsÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.2.2.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Lang<span id=\"S4.T3.4.2.2.6.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib48\" title=\"\" class=\"ltx_ref\">1995</a><span id=\"S4.T3.4.2.2.6.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<span id=\"S4.T3.4.2.2.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">PubMedQAÂ </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.2.2.7.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Luo etÂ al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.2.2.7.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib57\" title=\"\" class=\"ltx_ref\">2022</a><span id=\"S4.T3.4.2.2.7.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Experimental setting. \nA summary of datasets and models for evaluations can be found in TableÂ 3. We utilize FedAVG in our experiments. By default, we employ ResNet20 and the non-i.i.d.Â CIFAR10 dataset (partition parameter Î±=0.5ğ›¼0.5\\alpha=0.5), as the non-i.i.d.Â setting closely captures real-world scenarios. We further extend our evaluations to i.i.d.Â cases and various other models and datasets.\nFor evaluations on LLMs, we utilize FedLLMÂ (FedML Inc., 2023) that trains LLMs in a federated manner. We employ the Pythia-1B modelÂ (Biderman etÂ al., 2023) and PubMedQAÂ (Jin etÂ al., 2019), a non-i.i.d. biomedical research dataset that contains 212,269 questions for question answering. We utilize the â€œartificialâ€ subset for training and the â€œlabelledâ€ subset for testing.\nEvaluations are conducted on a server with 8 NVIDIA A100-SXM4-80GB GPUs."
        ]
    }
}