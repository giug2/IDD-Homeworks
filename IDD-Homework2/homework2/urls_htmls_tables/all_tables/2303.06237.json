{
    "PAPER'S NUMBER OF TABLES": 5,
    "S4.T1": {
        "caption": "Table 1: Training hyper-parameters for SA and IC models",
        "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">Optimizer</th>\n<th id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S4.T1.1.1.1.3.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T1.1.1.1.3.1.1\" class=\"ltx_p\">Weight</span>\n<span id=\"S4.T1.1.1.1.3.1.2\" class=\"ltx_p\">initializer</span>\n</span>\n</th>\n<th id=\"S4.T1.1.1.1.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S4.T1.1.1.1.4.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T1.1.1.1.4.1.1\" class=\"ltx_p\">Client</span>\n<span id=\"S4.T1.1.1.1.4.1.2\" class=\"ltx_p\">LR</span>\n</span>\n</th>\n<th id=\"S4.T1.1.1.1.5\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S4.T1.1.1.1.5.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T1.1.1.1.5.1.1\" class=\"ltx_p\">Aggregation</span>\n<span id=\"S4.T1.1.1.1.5.1.2\" class=\"ltx_p\">ratio</span>\n</span>\n</th>\n<th id=\"S4.T1.1.1.1.6\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S4.T1.1.1.1.6.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T1.1.1.1.6.1.1\" class=\"ltx_p\">Batch</span>\n<span id=\"S4.T1.1.1.1.6.1.2\" class=\"ltx_p\">size</span>\n</span>\n</th>\n<th id=\"S4.T1.1.1.1.7\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">Epoch</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">SA</td>\n<td id=\"S4.T1.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Adam</td>\n<td id=\"S4.T1.1.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">he_uniform</td>\n<td id=\"S4.T1.1.2.1.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.01</td>\n<td id=\"S4.T1.1.2.1.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">1.5</td>\n<td id=\"S4.T1.1.2.1.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">64</td>\n<td id=\"S4.T1.1.2.1.7\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">5</td>\n</tr>\n<tr id=\"S4.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">IC</td>\n<td id=\"S4.T1.1.3.2.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">Adam</td>\n<td id=\"S4.T1.1.3.2.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">he_uniform</td>\n<td id=\"S4.T1.1.3.2.4\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">0.01</td>\n<td id=\"S4.T1.1.3.2.5\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">1.5</td>\n<td id=\"S4.T1.1.3.2.6\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">64</td>\n<td id=\"S4.T1.1.3.2.7\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">5</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We implement CS with Flower¬†(Beutel et¬†al. 2020) and Tensorflow. The experiments are conducted on a Ubuntu Linux cluster (Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz with 512GB memory, 4 NVIDIA P100-SXM2 GPUs with 64GB total memory). We tested CS with different hyper-parameters, and only present the convergence progress with the hyper-parameters that led to the best results.\nTable¬†1 shows the training hyper-parameters for the two models.\nWe set the aggregation ratio (Œ∑‚Ä≤superscriptùúÇ‚Ä≤\\eta^{\\prime} in equation¬†8) to 1.5 to avoid clients‚Äô training outcomes being pruned away if they are too small.\nWe set the server model sparsity to 50%, unless otherwise specified."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Client sparsity vs. server sparsity for SA",
        "table": "<table id=\"S4.T2.2.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.2.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.2.2.3.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S4.T2.2.2.3.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T2.2.2.3.1.2.1\" class=\"ltx_text\">\n<span id=\"S4.T2.2.2.3.1.2.1.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T2.2.2.3.1.2.1.1.1\" class=\"ltx_p\">Model</span>\n<span id=\"S4.T2.2.2.3.1.2.1.1.2\" class=\"ltx_p\">layer</span>\n</span></span></th>\n<th id=\"S4.T2.2.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"4\">Server model sparsity</th>\n</tr>\n<tr id=\"S4.T2.2.2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.4.2.1\" class=\"ltx_td ltx_border_l ltx_border_r\"></td>\n<td id=\"S4.T2.2.2.4.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.5</td>\n<td id=\"S4.T2.2.2.4.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.6</td>\n<td id=\"S4.T2.2.2.4.2.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.7</td>\n<td id=\"S4.T2.2.2.4.2.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.8</td>\n</tr>\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T2.1.1.1.2.1\" class=\"ltx_text\">\n<span id=\"S4.T2.1.1.1.2.1.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T2.1.1.1.2.1.1.1\" class=\"ltx_p\">Client</span>\n<span id=\"S4.T2.1.1.1.2.1.1.2\" class=\"ltx_p\">model</span>\n<span id=\"S4.T2.1.1.1.2.1.1.3\" class=\"ltx_p\">sparsity</span>\n</span></span></td>\n<td id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Dense (768<math id=\"S4.T2.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T2.1.1.1.1.m1.1a\"><mo id=\"S4.T2.1.1.1.1.m1.1.1\" xref=\"S4.T2.1.1.1.1.m1.1.1.cmml\">√ó</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.1.1.1.1.m1.1b\"><times id=\"S4.T2.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.1.1.1.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.1.1.1.1.m1.1c\">\\times</annotation></semantics></math>32)</td>\n<td id=\"S4.T2.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.933</td>\n<td id=\"S4.T2.1.1.1.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.885</td>\n<td id=\"S4.T2.1.1.1.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.841</td>\n<td id=\"S4.T2.1.1.1.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.812</td>\n</tr>\n<tr id=\"S4.T2.2.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Output (32<math id=\"S4.T2.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T2.2.2.2.1.m1.1a\"><mo id=\"S4.T2.2.2.2.1.m1.1.1\" xref=\"S4.T2.2.2.2.1.m1.1.1.cmml\">√ó</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.2.2.2.1.m1.1b\"><times id=\"S4.T2.2.2.2.1.m1.1.1.cmml\" xref=\"S4.T2.2.2.2.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.2.2.2.1.m1.1c\">\\times</annotation></semantics></math>3 )</td>\n<td id=\"S4.T2.2.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.887</td>\n<td id=\"S4.T2.2.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.851</td>\n<td id=\"S4.T2.2.2.2.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.833</td>\n<td id=\"S4.T2.2.2.2.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.810</td>\n</tr>\n<tr id=\"S4.T2.2.2.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T2.2.2.5.3.1.1\" class=\"ltx_text ltx_font_bold\">Full model</span></td>\n<td id=\"S4.T2.2.2.5.3.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T2.2.2.5.3.2.1\" class=\"ltx_text ltx_font_bold\">0.932</span></td>\n<td id=\"S4.T2.2.2.5.3.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T2.2.2.5.3.3.1\" class=\"ltx_text ltx_font_bold\">0.884</span></td>\n<td id=\"S4.T2.2.2.5.3.4\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T2.2.2.5.3.4.1\" class=\"ltx_text ltx_font_bold\">0.841</span></td>\n<td id=\"S4.T2.2.2.5.3.5\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T2.2.2.5.3.5.1\" class=\"ltx_text ltx_font_bold\">0.812</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Client model sparsity. Sparsity is the percentage of zero weights in the model. A model with high sparsity can save both computation and communication cost in FL. In CS, the client model applies the inverted pruning mask, but in practice the client model sparsity is much higher than the complementary percentage of the server model sparsity. This is because when a client trains the global sparse model, only a portion of the zero weights in the global sparse model gets updated.\nTables¬†2 and¬†3 show the client model sparsity of SA and IC averaged over the number of rounds until they converge, while varying the server model sparsity. Let us note that we do not include the mask in the communication overhead, due to its small size.\nThe server model sparsity indicates the communication cost saving from the server to the clients, while the client model sparsity represents the saving from the clients to the server. In general, the client model becomes sparser when the server model is denser. The results also show that the layers with more parameters benefit more from CS, as they are sparser than the small layers. The results demonstrate a substantial reduction in the communication overhead. For example, in Table¬†2, when the reduction in the communication from the server to the clients is 80% (i.e., server model sparsity), for SA, the reduction in the communication from the clients to server is 81.2%.\nWe observe similar results for IC (Table¬†3)."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Client sparsity vs. server sparsity for IC",
        "table": "<table id=\"S4.T3.5.5\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.5.5.6.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.5.5.6.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S4.T3.5.5.6.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T3.5.5.6.1.2.1\" class=\"ltx_text\">\n<span id=\"S4.T3.5.5.6.1.2.1.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T3.5.5.6.1.2.1.1.1\" class=\"ltx_p\">Model</span>\n<span id=\"S4.T3.5.5.6.1.2.1.1.2\" class=\"ltx_p\">layer</span>\n</span></span></th>\n<th id=\"S4.T3.5.5.6.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"4\">Server model sparsity</th>\n</tr>\n<tr id=\"S4.T3.5.5.7.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.5.5.7.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<td id=\"S4.T3.5.5.7.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.5</td>\n<td id=\"S4.T3.5.5.7.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.6</td>\n<td id=\"S4.T3.5.5.7.2.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.7</td>\n<td id=\"S4.T3.5.5.7.2.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.8</td>\n</tr>\n<tr id=\"S4.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"6\"><span id=\"S4.T3.1.1.1.2.1\" class=\"ltx_text\">\n<span id=\"S4.T3.1.1.1.2.1.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T3.1.1.1.2.1.1.1\" class=\"ltx_p\">Client</span>\n<span id=\"S4.T3.1.1.1.2.1.1.2\" class=\"ltx_p\">model</span>\n<span id=\"S4.T3.1.1.1.2.1.1.3\" class=\"ltx_p\">sparsity</span>\n</span></span></th>\n<th id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.1.1.1.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T3.1.1.1.1.1.2\" class=\"ltx_p\">Conv2D</span>\n<span id=\"S4.T3.1.1.1.1.1.1\" class=\"ltx_p\">(<math id=\"S4.T3.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"3\\times 3\\times 32\" display=\"inline\"><semantics id=\"S4.T3.1.1.1.1.1.1.m1.1a\"><mrow id=\"S4.T3.1.1.1.1.1.1.m1.1.1\" xref=\"S4.T3.1.1.1.1.1.1.m1.1.1.cmml\"><mn id=\"S4.T3.1.1.1.1.1.1.m1.1.1.2\" xref=\"S4.T3.1.1.1.1.1.1.m1.1.1.2.cmml\">3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S4.T3.1.1.1.1.1.1.m1.1.1.1\" xref=\"S4.T3.1.1.1.1.1.1.m1.1.1.1.cmml\">√ó</mo><mn id=\"S4.T3.1.1.1.1.1.1.m1.1.1.3\" xref=\"S4.T3.1.1.1.1.1.1.m1.1.1.3.cmml\">3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S4.T3.1.1.1.1.1.1.m1.1.1.1a\" xref=\"S4.T3.1.1.1.1.1.1.m1.1.1.1.cmml\">√ó</mo><mn id=\"S4.T3.1.1.1.1.1.1.m1.1.1.4\" xref=\"S4.T3.1.1.1.1.1.1.m1.1.1.4.cmml\">32</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.1.1.1.1.1.1.m1.1b\"><apply id=\"S4.T3.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.1.1.1.1.1.1.m1.1.1\"><times id=\"S4.T3.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T3.1.1.1.1.1.1.m1.1.1.1\"></times><cn type=\"integer\" id=\"S4.T3.1.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T3.1.1.1.1.1.1.m1.1.1.2\">3</cn><cn type=\"integer\" id=\"S4.T3.1.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T3.1.1.1.1.1.1.m1.1.1.3\">3</cn><cn type=\"integer\" id=\"S4.T3.1.1.1.1.1.1.m1.1.1.4.cmml\" xref=\"S4.T3.1.1.1.1.1.1.m1.1.1.4\">32</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.1.1.1.1.1.1.m1.1c\">3\\times 3\\times 32</annotation></semantics></math>)</span>\n</span>\n</th>\n<td id=\"S4.T3.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.569</td>\n<td id=\"S4.T3.1.1.1.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.528</td>\n<td id=\"S4.T3.1.1.1.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.587</td>\n<td id=\"S4.T3.1.1.1.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.788</td>\n</tr>\n<tr id=\"S4.T3.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.2.2.2.1.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T3.2.2.2.1.1.2\" class=\"ltx_p\">Conv2D</span>\n<span id=\"S4.T3.2.2.2.1.1.1\" class=\"ltx_p\">(<math id=\"S4.T3.2.2.2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"32\\times 3\\times 3\\times 64\" display=\"inline\"><semantics id=\"S4.T3.2.2.2.1.1.1.m1.1a\"><mrow id=\"S4.T3.2.2.2.1.1.1.m1.1.1\" xref=\"S4.T3.2.2.2.1.1.1.m1.1.1.cmml\"><mn id=\"S4.T3.2.2.2.1.1.1.m1.1.1.2\" xref=\"S4.T3.2.2.2.1.1.1.m1.1.1.2.cmml\">32</mn><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S4.T3.2.2.2.1.1.1.m1.1.1.1\" xref=\"S4.T3.2.2.2.1.1.1.m1.1.1.1.cmml\">√ó</mo><mn id=\"S4.T3.2.2.2.1.1.1.m1.1.1.3\" xref=\"S4.T3.2.2.2.1.1.1.m1.1.1.3.cmml\">3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S4.T3.2.2.2.1.1.1.m1.1.1.1a\" xref=\"S4.T3.2.2.2.1.1.1.m1.1.1.1.cmml\">√ó</mo><mn id=\"S4.T3.2.2.2.1.1.1.m1.1.1.4\" xref=\"S4.T3.2.2.2.1.1.1.m1.1.1.4.cmml\">3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S4.T3.2.2.2.1.1.1.m1.1.1.1b\" xref=\"S4.T3.2.2.2.1.1.1.m1.1.1.1.cmml\">√ó</mo><mn id=\"S4.T3.2.2.2.1.1.1.m1.1.1.5\" xref=\"S4.T3.2.2.2.1.1.1.m1.1.1.5.cmml\">64</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.2.2.2.1.1.1.m1.1b\"><apply id=\"S4.T3.2.2.2.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.2.2.2.1.1.1.m1.1.1\"><times id=\"S4.T3.2.2.2.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T3.2.2.2.1.1.1.m1.1.1.1\"></times><cn type=\"integer\" id=\"S4.T3.2.2.2.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T3.2.2.2.1.1.1.m1.1.1.2\">32</cn><cn type=\"integer\" id=\"S4.T3.2.2.2.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T3.2.2.2.1.1.1.m1.1.1.3\">3</cn><cn type=\"integer\" id=\"S4.T3.2.2.2.1.1.1.m1.1.1.4.cmml\" xref=\"S4.T3.2.2.2.1.1.1.m1.1.1.4\">3</cn><cn type=\"integer\" id=\"S4.T3.2.2.2.1.1.1.m1.1.1.5.cmml\" xref=\"S4.T3.2.2.2.1.1.1.m1.1.1.5\">64</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.2.2.2.1.1.1.m1.1c\">32\\times 3\\times 3\\times 64</annotation></semantics></math>)</span>\n</span>\n</th>\n<td id=\"S4.T3.2.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.842</td>\n<td id=\"S4.T3.2.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.788</td>\n<td id=\"S4.T3.2.2.2.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.800</td>\n<td id=\"S4.T3.2.2.2.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.900</td>\n</tr>\n<tr id=\"S4.T3.3.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.3.3.3.1.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T3.3.3.3.1.1.2\" class=\"ltx_p\">Conv2D</span>\n<span id=\"S4.T3.3.3.3.1.1.1\" class=\"ltx_p\">(<math id=\"S4.T3.3.3.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"64\\times 3\\times 3\\times 64\" display=\"inline\"><semantics id=\"S4.T3.3.3.3.1.1.1.m1.1a\"><mrow id=\"S4.T3.3.3.3.1.1.1.m1.1.1\" xref=\"S4.T3.3.3.3.1.1.1.m1.1.1.cmml\"><mn id=\"S4.T3.3.3.3.1.1.1.m1.1.1.2\" xref=\"S4.T3.3.3.3.1.1.1.m1.1.1.2.cmml\">64</mn><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S4.T3.3.3.3.1.1.1.m1.1.1.1\" xref=\"S4.T3.3.3.3.1.1.1.m1.1.1.1.cmml\">√ó</mo><mn id=\"S4.T3.3.3.3.1.1.1.m1.1.1.3\" xref=\"S4.T3.3.3.3.1.1.1.m1.1.1.3.cmml\">3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S4.T3.3.3.3.1.1.1.m1.1.1.1a\" xref=\"S4.T3.3.3.3.1.1.1.m1.1.1.1.cmml\">√ó</mo><mn id=\"S4.T3.3.3.3.1.1.1.m1.1.1.4\" xref=\"S4.T3.3.3.3.1.1.1.m1.1.1.4.cmml\">3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S4.T3.3.3.3.1.1.1.m1.1.1.1b\" xref=\"S4.T3.3.3.3.1.1.1.m1.1.1.1.cmml\">√ó</mo><mn id=\"S4.T3.3.3.3.1.1.1.m1.1.1.5\" xref=\"S4.T3.3.3.3.1.1.1.m1.1.1.5.cmml\">64</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.3.3.3.1.1.1.m1.1b\"><apply id=\"S4.T3.3.3.3.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.3.3.3.1.1.1.m1.1.1\"><times id=\"S4.T3.3.3.3.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T3.3.3.3.1.1.1.m1.1.1.1\"></times><cn type=\"integer\" id=\"S4.T3.3.3.3.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T3.3.3.3.1.1.1.m1.1.1.2\">64</cn><cn type=\"integer\" id=\"S4.T3.3.3.3.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T3.3.3.3.1.1.1.m1.1.1.3\">3</cn><cn type=\"integer\" id=\"S4.T3.3.3.3.1.1.1.m1.1.1.4.cmml\" xref=\"S4.T3.3.3.3.1.1.1.m1.1.1.4\">3</cn><cn type=\"integer\" id=\"S4.T3.3.3.3.1.1.1.m1.1.1.5.cmml\" xref=\"S4.T3.3.3.3.1.1.1.m1.1.1.5\">64</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.3.3.3.1.1.1.m1.1c\">64\\times 3\\times 3\\times 64</annotation></semantics></math>)</span>\n</span>\n</th>\n<td id=\"S4.T3.3.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.917</td>\n<td id=\"S4.T3.3.3.3.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.837</td>\n<td id=\"S4.T3.3.3.3.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.791</td>\n<td id=\"S4.T3.3.3.3.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.868</td>\n</tr>\n<tr id=\"S4.T3.4.4.4\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.4.4.4.1.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T3.4.4.4.1.1.2\" class=\"ltx_p\">Dense</span>\n<span id=\"S4.T3.4.4.4.1.1.1\" class=\"ltx_p\">(<math id=\"S4.T3.4.4.4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"64\\times 16\\times 100\" display=\"inline\"><semantics id=\"S4.T3.4.4.4.1.1.1.m1.1a\"><mrow id=\"S4.T3.4.4.4.1.1.1.m1.1.1\" xref=\"S4.T3.4.4.4.1.1.1.m1.1.1.cmml\"><mn id=\"S4.T3.4.4.4.1.1.1.m1.1.1.2\" xref=\"S4.T3.4.4.4.1.1.1.m1.1.1.2.cmml\">64</mn><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S4.T3.4.4.4.1.1.1.m1.1.1.1\" xref=\"S4.T3.4.4.4.1.1.1.m1.1.1.1.cmml\">√ó</mo><mn id=\"S4.T3.4.4.4.1.1.1.m1.1.1.3\" xref=\"S4.T3.4.4.4.1.1.1.m1.1.1.3.cmml\">16</mn><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S4.T3.4.4.4.1.1.1.m1.1.1.1a\" xref=\"S4.T3.4.4.4.1.1.1.m1.1.1.1.cmml\">√ó</mo><mn id=\"S4.T3.4.4.4.1.1.1.m1.1.1.4\" xref=\"S4.T3.4.4.4.1.1.1.m1.1.1.4.cmml\">100</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.4.4.4.1.1.1.m1.1b\"><apply id=\"S4.T3.4.4.4.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.4.4.4.1.1.1.m1.1.1\"><times id=\"S4.T3.4.4.4.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T3.4.4.4.1.1.1.m1.1.1.1\"></times><cn type=\"integer\" id=\"S4.T3.4.4.4.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T3.4.4.4.1.1.1.m1.1.1.2\">64</cn><cn type=\"integer\" id=\"S4.T3.4.4.4.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T3.4.4.4.1.1.1.m1.1.1.3\">16</cn><cn type=\"integer\" id=\"S4.T3.4.4.4.1.1.1.m1.1.1.4.cmml\" xref=\"S4.T3.4.4.4.1.1.1.m1.1.1.4\">100</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.4.4.4.1.1.1.m1.1c\">64\\times 16\\times 100</annotation></semantics></math>)</span>\n</span>\n</th>\n<td id=\"S4.T3.4.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.920</td>\n<td id=\"S4.T3.4.4.4.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.863</td>\n<td id=\"S4.T3.4.4.4.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.853</td>\n<td id=\"S4.T3.4.4.4.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.909</td>\n</tr>\n<tr id=\"S4.T3.5.5.5\" class=\"ltx_tr\">\n<th id=\"S4.T3.5.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Output (<math id=\"S4.T3.5.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"100\\times 62\" display=\"inline\"><semantics id=\"S4.T3.5.5.5.1.m1.1a\"><mrow id=\"S4.T3.5.5.5.1.m1.1.1\" xref=\"S4.T3.5.5.5.1.m1.1.1.cmml\"><mn id=\"S4.T3.5.5.5.1.m1.1.1.2\" xref=\"S4.T3.5.5.5.1.m1.1.1.2.cmml\">100</mn><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S4.T3.5.5.5.1.m1.1.1.1\" xref=\"S4.T3.5.5.5.1.m1.1.1.1.cmml\">√ó</mo><mn id=\"S4.T3.5.5.5.1.m1.1.1.3\" xref=\"S4.T3.5.5.5.1.m1.1.1.3.cmml\">62</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.5.5.5.1.m1.1b\"><apply id=\"S4.T3.5.5.5.1.m1.1.1.cmml\" xref=\"S4.T3.5.5.5.1.m1.1.1\"><times id=\"S4.T3.5.5.5.1.m1.1.1.1.cmml\" xref=\"S4.T3.5.5.5.1.m1.1.1.1\"></times><cn type=\"integer\" id=\"S4.T3.5.5.5.1.m1.1.1.2.cmml\" xref=\"S4.T3.5.5.5.1.m1.1.1.2\">100</cn><cn type=\"integer\" id=\"S4.T3.5.5.5.1.m1.1.1.3.cmml\" xref=\"S4.T3.5.5.5.1.m1.1.1.3\">62</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.5.5.5.1.m1.1c\">100\\times 62</annotation></semantics></math>)</th>\n<td id=\"S4.T3.5.5.5.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.756</td>\n<td id=\"S4.T3.5.5.5.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.722</td>\n<td id=\"S4.T3.5.5.5.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.721</td>\n<td id=\"S4.T3.5.5.5.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.698</td>\n</tr>\n<tr id=\"S4.T3.5.5.8.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.5.5.8.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T3.5.5.8.3.1.1\" class=\"ltx_text ltx_font_bold\">Full model</span></th>\n<td id=\"S4.T3.5.5.8.3.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T3.5.5.8.3.2.1\" class=\"ltx_text ltx_font_bold\">0.904</span></td>\n<td id=\"S4.T3.5.5.8.3.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T3.5.5.8.3.3.1\" class=\"ltx_text ltx_font_bold\">0.843</span></td>\n<td id=\"S4.T3.5.5.8.3.4\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T3.5.5.8.3.4.1\" class=\"ltx_text ltx_font_bold\">0.828</span></td>\n<td id=\"S4.T3.5.5.8.3.5\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T3.5.5.8.3.5.1\" class=\"ltx_text ltx_font_bold\">0.891</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Client model sparsity. Sparsity is the percentage of zero weights in the model. A model with high sparsity can save both computation and communication cost in FL. In CS, the client model applies the inverted pruning mask, but in practice the client model sparsity is much higher than the complementary percentage of the server model sparsity. This is because when a client trains the global sparse model, only a portion of the zero weights in the global sparse model gets updated.\nTables¬†2 and¬†3 show the client model sparsity of SA and IC averaged over the number of rounds until they converge, while varying the server model sparsity. Let us note that we do not include the mask in the communication overhead, due to its small size.\nThe server model sparsity indicates the communication cost saving from the server to the clients, while the client model sparsity represents the saving from the clients to the server. In general, the client model becomes sparser when the server model is denser. The results also show that the layers with more parameters benefit more from CS, as they are sparser than the small layers. The results demonstrate a substantial reduction in the communication overhead. For example, in Table¬†2, when the reduction in the communication from the server to the clients is 80% (i.e., server model sparsity), for SA, the reduction in the communication from the clients to server is 81.2%.\nWe observe similar results for IC (Table¬†3)."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: CS training FLOPs saving vs. server sparsity for SA",
        "table": "<table id=\"S4.T4.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S4.T4.1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T4.1.1.1.1.2.1\" class=\"ltx_text\">\n<span id=\"S4.T4.1.1.1.1.2.1.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T4.1.1.1.1.2.1.1.1\" class=\"ltx_p\">Model layer/</span>\n<span id=\"S4.T4.1.1.1.1.2.1.1.2\" class=\"ltx_p\">FLOPs</span>\n</span></span></th>\n<th id=\"S4.T4.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"4\">Server model sparsity</th>\n</tr>\n<tr id=\"S4.T4.1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.1.2.2.1\" class=\"ltx_td ltx_border_l ltx_border_r\"></td>\n<td id=\"S4.T4.1.1.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.5</td>\n<td id=\"S4.T4.1.1.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.6</td>\n<td id=\"S4.T4.1.1.2.2.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.7</td>\n<td id=\"S4.T4.1.1.2.2.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.8</td>\n</tr>\n<tr id=\"S4.T4.1.1.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T4.1.1.3.3.1.1\" class=\"ltx_text\">\n<span id=\"S4.T4.1.1.3.3.1.1.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T4.1.1.3.3.1.1.1.1\" class=\"ltx_p\">FLOPs</span>\n<span id=\"S4.T4.1.1.3.3.1.1.1.2\" class=\"ltx_p\">saved</span>\n<span id=\"S4.T4.1.1.3.3.1.1.1.3\" class=\"ltx_p\">(%)</span>\n</span></span></td>\n<td id=\"S4.T4.1.1.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Dense/147744</td>\n<td id=\"S4.T4.1.1.3.3.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">31.1</td>\n<td id=\"S4.T4.1.1.3.3.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">36.1</td>\n<td id=\"S4.T4.1.1.3.3.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">41.3</td>\n<td id=\"S4.T4.1.1.3.3.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">47.0</td>\n</tr>\n<tr id=\"S4.T4.1.1.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Output/585</td>\n<td id=\"S4.T4.1.1.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">29.1</td>\n<td id=\"S4.T4.1.1.4.4.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">34.5</td>\n<td id=\"S4.T4.1.1.4.4.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">40.5</td>\n<td id=\"S4.T4.1.1.4.4.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">46.3</td>\n</tr>\n<tr id=\"S4.T4.1.1.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T4.1.1.5.5.1.1\" class=\"ltx_text ltx_font_bold\">Full model/148329</span></td>\n<td id=\"S4.T4.1.1.5.5.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T4.1.1.5.5.2.1\" class=\"ltx_text ltx_font_bold\">31.1</span></td>\n<td id=\"S4.T4.1.1.5.5.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T4.1.1.5.5.3.1\" class=\"ltx_text ltx_font_bold\">36.1</span></td>\n<td id=\"S4.T4.1.1.5.5.4\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T4.1.1.5.5.4.1\" class=\"ltx_text ltx_font_bold\">41.3</span></td>\n<td id=\"S4.T4.1.1.5.5.5\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T4.1.1.5.5.5.1\" class=\"ltx_text ltx_font_bold\">47.0</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Comparison with vanilla FL.",
                " Figure¬†",
                "3",
                " shows the SA accuracy over training rounds when all users participate in every training round. In terms of best performance, the accuracy of CS is comparable with vanilla FL (73.3% vs. 76.1%). The less than 3% difference is the cost of the significant overhead reduction in CS, which will be shown later in this section. In the initial rounds, there is a gap in accuracy due to pruning and the fact that clients did not have yet time to recover the performance loss through local training. However, as FL proceeds, CS allows clients to implicitly fine-tune the pruned model. The accuracy gap between CS and vanilla FL gradually decreases, until overfitting occurs for CS. Nevertheless, the system can use the best model for inference.",
                "In FL on mobile and IoT devices, however, it is more realistic that only a small portion of users participate in each training round due to resource constraints on the devices. Figure¬†",
                "3",
                " shows the SA accuracy over communication rounds when 10 randomly selected users participate in each training round. In terms of best accuracy, CS (74.3%) competes with vanilla FL (76.9%). An advantage of CS is that its learning curve fluctuates significantly less than vanilla FL. This is because the effect of non-IID data is alleviated by runing in CS, while it is fully observed in vanilla FL.\nThis phenomenon is further confirmed by the IC model.",
                "Figure¬†",
                "5",
                " shows the IC model accuracy over communication rounds when 10 randomly selected users participate in each round of training, which is a more realistic case than all users participating in every round. Overall, the learning curves between CS and vanilla FL are close, with best accuracy of 72.5% and 76.3%, respectively. FEMNIST is a much larger dataset with more users than Twitter, and IC is a more complex model, with more possible output classes than SA. Therefore, it takes IC more rounds (up to 500) to converge. Let us note that the overfitting in Figure¬†",
                "3",
                " does not appear in Figure¬†",
                "5",
                ". This is because a larger model is less likely to be overfitted by smaller amounts of information (partial participation of clients every round). Figure¬†",
                "5",
                " shows a zoomed in portion of the graph in Figure¬†",
                "5",
                ". The results demonstrate that vanilla FL fluctuates more abruptly than CS during the training. This is an important advantage for CS in practice. In real-world FL over mobile or IoT devices, the data gradually accumulate as FL proceeds, but the system can not wait hundreds of rounds for a final best model or does not have a fully representative test dataset to select the best model for users. In CS, it is safe to distribute the latest model to users, while the latest model for vanilla FL may suffer from inferior accuracy.",
                "Comparison with baselines.",
                " Figures¬†",
                "3",
                " and¬†",
                "5",
                " also show the model accuracy comparison between CS and the baselines. For SA, PruneFL and PQSU reach best model accuracy of 71.5% and 73.4%, which are 2.8% and 0.9% lower than CS, respectively. On the larger FEMNIST dataset, the results of IC show a clearer advantage for CS. For IC, PruneFL and PQSU reach best model accuracy of 55.5% and 57.9%, which are 17% and 15% lower than CS, respectively.",
                "The original PruneFL paper¬†",
                "(Xu et¬†al. ",
                "2021",
                ")",
                " show comparable performance with vanilla FL on the FEMNIST dataset. However, the experiments used only the data of 193 users (out of 3597), the 193 users were further mixed and treated as 10 ‚Äúsuper-users‚Äù, and all these users participated in every round of training. We believe our experiments represent a more realistic scenario because we use all users and do not mix multiple users into a ‚Äúsuper-user‚Äù. For PQSU, the over-optimization on clients overfits the global model quickly. Thus, PQSU cannot benefit from additional data and training rounds. To conclude, the baselines suffer from poor performance in realistic conditions for large datasets.",
                "Next, we present a qualitative discussion to explain that the baselines have higher overhead. For communication overhead reduction, PruneFL uses an adaptive process in which the model not only shrinks, but also grows to reach the final targeted model sparsity. During the communications rounds with a grown model, PruneFL has higher communication overhead than CS. PQSU, on the other hand, can only save communication overhead in one direction, when clients transfer the sparse model to server. When PQUS transfers the model from the server to the clients, the communication overhead is higher than CS. Therefore, for the same targeted model sparsity, both PruneFL and PQSU have higher communication overhead than CS. For computation overhead, PruneFL imposes additional computation overhead including importance measure, importance aggregation, and model reconfiguration, while PQSU requires clients to further fine-tune their sparse models after the training. Thus, they also suffer from higher computation overhead than CS, because CS only needs to remove weights from the dense model.",
                "Global sparse model vs. aggregated dense model.",
                " Figures¬†",
                "7",
                " and¬†",
                "7",
                " show the comparison between the global sparse model and the aggregated dense model (i.e., the model before sparsification in each round) in CS for SA and IC models. Overall, the global sparse model exhibits a smooth learning curve and outperforms the aggregated model. This demonstrates the effectiveness of the low-overhead model pruning in CS, which reduces communication overhead and maintains good model performance by removing weights in low magnitude. In CS, the aggregated model not only captures the global distribution, but also gets polluted by the noisy distribution shift induced from the clients data. In each round, simply removing the weights with low magnitudes from the newly aggregated model can effectively eliminate the noisy distribution shift, and the global sparse model can steadily learn the global data distribution.",
                "Client model sparsity.",
                " Sparsity is the percentage of zero weights in the model. A model with high sparsity can save both computation and communication cost in FL. In CS, the client model applies the inverted pruning mask, but in practice the client model sparsity is much higher than the complementary percentage of the server model sparsity. This is because when a client trains the global sparse model, only a portion of the zero weights in the global sparse model gets updated.\nTables¬†",
                "2",
                " and¬†",
                "3",
                " show the client model sparsity of SA and IC averaged over the number of rounds until they converge, while varying the server model sparsity. Let us note that we do not include the mask in the communication overhead, due to its small size.\nThe server model sparsity indicates the communication cost saving from the server to the clients, while the client model sparsity represents the saving from the clients to the server. In general, the client model becomes sparser when the server model is denser. The results also show that the layers with more parameters benefit more from CS, as they are sparser than the small layers. The results demonstrate a substantial reduction in the communication overhead. For example, in Table¬†",
                "2",
                ", when the reduction in the communication from the server to the clients is 80% (i.e., server model sparsity), for SA, the reduction in the communication from the clients to server is 81.2%.\nWe observe similar results for IC (Table¬†",
                "3",
                ").",
                "Training FLOPs savings.",
                " To evaluate the reduction in the computation overhead at the clients, we compute the training FLOPs savings based on the server and client model sparsity. We consider the number of multiply-accumulate (MAC) operations performed by each layer for both the forward and the backward pass during the training. In the forward pass, the clients perform FLOPs on the non-zero weights received from the server. In the backward pass, the MAC operations are counted for both the hidden state and the derivative. The hidden state MAC operations are fully counted as FLOPs.\nFor the derivative, only the MAC operations on weights with non-zero values are counted as FLOPs. Here, the non-zero weights include both the non-zero set of weights received from the server and the zero weights that are updated to non-zero by the client. Let us note that the inverted pruning mask is applied after clients training, and therefore it does not help with FLOPs savings.",
                "Tables¬†",
                "4",
                " and¬†",
                "5",
                " show the CS training FLOPs savings for both SA and IC, as a percentage of the FLOPs needed by vanilla FL.\nThe values displayed in the second column of the tables are the training FLOPs of a single sample in vanilla FL. We compare them with CS training FLOPs under different server model sparsity.\nWe observe that CS can save up to 49.3% training FLOPs, and the savings increase as the server model sparsity becomes higher. Similar with the communication savings, the layers with more parameters save a higher percentage of FLOPs.",
                "Server model sparsity vs. model accuracy.",
                " Figures¬†",
                "9",
                " and¬†",
                "9",
                " show how the model accuracy varies with the server model sparsity for SA and IC. Since the server model sparsity is a parameter that can be set to different values for different models, it allows the system operators to achieve the desired trade-off between the model accuracy and the reduction in communication/computation overhead. In general, the model performs better when the server model sparsity is low. The results show that for SA, even a sparsity of 90% can lead to good performance (an accuracy deterioration of merely 2% compared to 50% sparsity). However, for IC, the sparsity should be kept to at most 70% to achieve acceptable performance."
            ]
        ]
    },
    "S4.T5": {
        "caption": "Table 5: CS training FLOPs saving vs. server sparsity for IC",
        "table": "<table id=\"S4.T5.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T5.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.1.1.1.1\" class=\"ltx_td ltx_border_l ltx_border_r ltx_border_t\"></td>\n<td id=\"S4.T5.1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T5.1.1.1.1.2.1\" class=\"ltx_text\">\n<span id=\"S4.T5.1.1.1.1.2.1.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T5.1.1.1.1.2.1.1.1\" class=\"ltx_p\">Model layer/</span>\n<span id=\"S4.T5.1.1.1.1.2.1.1.2\" class=\"ltx_p\">FLOPs</span>\n</span></span></td>\n<td id=\"S4.T5.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"4\">Server model sparsity</td>\n</tr>\n<tr id=\"S4.T5.1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.1.2.2.1\" class=\"ltx_td ltx_border_l ltx_border_r\"></td>\n<td id=\"S4.T5.1.1.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.5</td>\n<td id=\"S4.T5.1.1.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.6</td>\n<td id=\"S4.T5.1.1.2.2.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.7</td>\n<td id=\"S4.T5.1.1.2.2.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">0.8</td>\n</tr>\n<tr id=\"S4.T5.1.1.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"6\"><span id=\"S4.T5.1.1.3.3.1.1\" class=\"ltx_text\">\n<span id=\"S4.T5.1.1.3.3.1.1.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T5.1.1.3.3.1.1.1.1\" class=\"ltx_p\">FLOPs</span>\n<span id=\"S4.T5.1.1.3.3.1.1.1.2\" class=\"ltx_p\">saved</span>\n<span id=\"S4.T5.1.1.3.3.1.1.1.3\" class=\"ltx_p\">(%)</span>\n</span></span></td>\n<td id=\"S4.T5.1.1.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Conv2D/1168224</td>\n<td id=\"S4.T5.1.1.3.3.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">19.0</td>\n<td id=\"S4.T5.1.1.3.3.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">24.3</td>\n<td id=\"S4.T5.1.1.3.3.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">32.9</td>\n<td id=\"S4.T5.1.1.3.3.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">46.3</td>\n</tr>\n<tr id=\"S4.T5.1.1.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Conv2D/13381824</td>\n<td id=\"S4.T5.1.1.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">28.1</td>\n<td id=\"S4.T5.1.1.4.4.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">32.9</td>\n<td id=\"S4.T5.1.1.4.4.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">40.0</td>\n<td id=\"S4.T5.1.1.4.4.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">50.0</td>\n</tr>\n<tr id=\"S4.T5.1.1.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Conv2D/17916096</td>\n<td id=\"S4.T5.1.1.5.5.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">30.6</td>\n<td id=\"S4.T5.1.1.5.5.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">34.6</td>\n<td id=\"S4.T5.1.1.5.5.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">39.7</td>\n<td id=\"S4.T5.1.1.5.5.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">48.9</td>\n</tr>\n<tr id=\"S4.T5.1.1.6.6\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Dense/614700</td>\n<td id=\"S4.T5.1.1.6.6.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">30.7</td>\n<td id=\"S4.T5.1.1.6.6.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">35.4</td>\n<td id=\"S4.T5.1.1.6.6.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">41.7</td>\n<td id=\"S4.T5.1.1.6.6.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">50.3</td>\n</tr>\n<tr id=\"S4.T5.1.1.7.7\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Output/37386</td>\n<td id=\"S4.T5.1.1.7.7.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">25.1</td>\n<td id=\"S4.T5.1.1.7.7.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">30.6</td>\n<td id=\"S4.T5.1.1.7.7.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">37.2</td>\n<td id=\"S4.T5.1.1.7.7.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">43.1</td>\n</tr>\n<tr id=\"S4.T5.1.1.8.8\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.1.8.8.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.1.1.8.8.1.1\" class=\"ltx_inline-block\">\n<span id=\"S4.T5.1.1.8.8.1.1.1\" class=\"ltx_p\"><span id=\"S4.T5.1.1.8.8.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Full model/33118230</span></span>\n</span>\n</td>\n<td id=\"S4.T5.1.1.8.8.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T5.1.1.8.8.2.1\" class=\"ltx_text ltx_font_bold\">29.1</span></td>\n<td id=\"S4.T5.1.1.8.8.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T5.1.1.8.8.3.1\" class=\"ltx_text ltx_font_bold\">33.6</span></td>\n<td id=\"S4.T5.1.1.8.8.4\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T5.1.1.8.8.4.1\" class=\"ltx_text ltx_font_bold\">39.6</span></td>\n<td id=\"S4.T5.1.1.8.8.5\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T5.1.1.8.8.5.1\" class=\"ltx_text ltx_font_bold\">49.3</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Comparison with vanilla FL.",
                " Figure¬†",
                "3",
                " shows the SA accuracy over training rounds when all users participate in every training round. In terms of best performance, the accuracy of CS is comparable with vanilla FL (73.3% vs. 76.1%). The less than 3% difference is the cost of the significant overhead reduction in CS, which will be shown later in this section. In the initial rounds, there is a gap in accuracy due to pruning and the fact that clients did not have yet time to recover the performance loss through local training. However, as FL proceeds, CS allows clients to implicitly fine-tune the pruned model. The accuracy gap between CS and vanilla FL gradually decreases, until overfitting occurs for CS. Nevertheless, the system can use the best model for inference.",
                "In FL on mobile and IoT devices, however, it is more realistic that only a small portion of users participate in each training round due to resource constraints on the devices. Figure¬†",
                "3",
                " shows the SA accuracy over communication rounds when 10 randomly selected users participate in each training round. In terms of best accuracy, CS (74.3%) competes with vanilla FL (76.9%). An advantage of CS is that its learning curve fluctuates significantly less than vanilla FL. This is because the effect of non-IID data is alleviated by runing in CS, while it is fully observed in vanilla FL.\nThis phenomenon is further confirmed by the IC model.",
                "Figure¬†",
                "5",
                " shows the IC model accuracy over communication rounds when 10 randomly selected users participate in each round of training, which is a more realistic case than all users participating in every round. Overall, the learning curves between CS and vanilla FL are close, with best accuracy of 72.5% and 76.3%, respectively. FEMNIST is a much larger dataset with more users than Twitter, and IC is a more complex model, with more possible output classes than SA. Therefore, it takes IC more rounds (up to 500) to converge. Let us note that the overfitting in Figure¬†",
                "3",
                " does not appear in Figure¬†",
                "5",
                ". This is because a larger model is less likely to be overfitted by smaller amounts of information (partial participation of clients every round). Figure¬†",
                "5",
                " shows a zoomed in portion of the graph in Figure¬†",
                "5",
                ". The results demonstrate that vanilla FL fluctuates more abruptly than CS during the training. This is an important advantage for CS in practice. In real-world FL over mobile or IoT devices, the data gradually accumulate as FL proceeds, but the system can not wait hundreds of rounds for a final best model or does not have a fully representative test dataset to select the best model for users. In CS, it is safe to distribute the latest model to users, while the latest model for vanilla FL may suffer from inferior accuracy.",
                "Comparison with baselines.",
                " Figures¬†",
                "3",
                " and¬†",
                "5",
                " also show the model accuracy comparison between CS and the baselines. For SA, PruneFL and PQSU reach best model accuracy of 71.5% and 73.4%, which are 2.8% and 0.9% lower than CS, respectively. On the larger FEMNIST dataset, the results of IC show a clearer advantage for CS. For IC, PruneFL and PQSU reach best model accuracy of 55.5% and 57.9%, which are 17% and 15% lower than CS, respectively.",
                "The original PruneFL paper¬†",
                "(Xu et¬†al. ",
                "2021",
                ")",
                " show comparable performance with vanilla FL on the FEMNIST dataset. However, the experiments used only the data of 193 users (out of 3597), the 193 users were further mixed and treated as 10 ‚Äúsuper-users‚Äù, and all these users participated in every round of training. We believe our experiments represent a more realistic scenario because we use all users and do not mix multiple users into a ‚Äúsuper-user‚Äù. For PQSU, the over-optimization on clients overfits the global model quickly. Thus, PQSU cannot benefit from additional data and training rounds. To conclude, the baselines suffer from poor performance in realistic conditions for large datasets.",
                "Next, we present a qualitative discussion to explain that the baselines have higher overhead. For communication overhead reduction, PruneFL uses an adaptive process in which the model not only shrinks, but also grows to reach the final targeted model sparsity. During the communications rounds with a grown model, PruneFL has higher communication overhead than CS. PQSU, on the other hand, can only save communication overhead in one direction, when clients transfer the sparse model to server. When PQUS transfers the model from the server to the clients, the communication overhead is higher than CS. Therefore, for the same targeted model sparsity, both PruneFL and PQSU have higher communication overhead than CS. For computation overhead, PruneFL imposes additional computation overhead including importance measure, importance aggregation, and model reconfiguration, while PQSU requires clients to further fine-tune their sparse models after the training. Thus, they also suffer from higher computation overhead than CS, because CS only needs to remove weights from the dense model.",
                "Global sparse model vs. aggregated dense model.",
                " Figures¬†",
                "7",
                " and¬†",
                "7",
                " show the comparison between the global sparse model and the aggregated dense model (i.e., the model before sparsification in each round) in CS for SA and IC models. Overall, the global sparse model exhibits a smooth learning curve and outperforms the aggregated model. This demonstrates the effectiveness of the low-overhead model pruning in CS, which reduces communication overhead and maintains good model performance by removing weights in low magnitude. In CS, the aggregated model not only captures the global distribution, but also gets polluted by the noisy distribution shift induced from the clients data. In each round, simply removing the weights with low magnitudes from the newly aggregated model can effectively eliminate the noisy distribution shift, and the global sparse model can steadily learn the global data distribution.",
                "Client model sparsity.",
                " Sparsity is the percentage of zero weights in the model. A model with high sparsity can save both computation and communication cost in FL. In CS, the client model applies the inverted pruning mask, but in practice the client model sparsity is much higher than the complementary percentage of the server model sparsity. This is because when a client trains the global sparse model, only a portion of the zero weights in the global sparse model gets updated.\nTables¬†",
                "2",
                " and¬†",
                "3",
                " show the client model sparsity of SA and IC averaged over the number of rounds until they converge, while varying the server model sparsity. Let us note that we do not include the mask in the communication overhead, due to its small size.\nThe server model sparsity indicates the communication cost saving from the server to the clients, while the client model sparsity represents the saving from the clients to the server. In general, the client model becomes sparser when the server model is denser. The results also show that the layers with more parameters benefit more from CS, as they are sparser than the small layers. The results demonstrate a substantial reduction in the communication overhead. For example, in Table¬†",
                "2",
                ", when the reduction in the communication from the server to the clients is 80% (i.e., server model sparsity), for SA, the reduction in the communication from the clients to server is 81.2%.\nWe observe similar results for IC (Table¬†",
                "3",
                ").",
                "Training FLOPs savings.",
                " To evaluate the reduction in the computation overhead at the clients, we compute the training FLOPs savings based on the server and client model sparsity. We consider the number of multiply-accumulate (MAC) operations performed by each layer for both the forward and the backward pass during the training. In the forward pass, the clients perform FLOPs on the non-zero weights received from the server. In the backward pass, the MAC operations are counted for both the hidden state and the derivative. The hidden state MAC operations are fully counted as FLOPs.\nFor the derivative, only the MAC operations on weights with non-zero values are counted as FLOPs. Here, the non-zero weights include both the non-zero set of weights received from the server and the zero weights that are updated to non-zero by the client. Let us note that the inverted pruning mask is applied after clients training, and therefore it does not help with FLOPs savings.",
                "Tables¬†",
                "4",
                " and¬†",
                "5",
                " show the CS training FLOPs savings for both SA and IC, as a percentage of the FLOPs needed by vanilla FL.\nThe values displayed in the second column of the tables are the training FLOPs of a single sample in vanilla FL. We compare them with CS training FLOPs under different server model sparsity.\nWe observe that CS can save up to 49.3% training FLOPs, and the savings increase as the server model sparsity becomes higher. Similar with the communication savings, the layers with more parameters save a higher percentage of FLOPs.",
                "Server model sparsity vs. model accuracy.",
                " Figures¬†",
                "9",
                " and¬†",
                "9",
                " show how the model accuracy varies with the server model sparsity for SA and IC. Since the server model sparsity is a parameter that can be set to different values for different models, it allows the system operators to achieve the desired trade-off between the model accuracy and the reduction in communication/computation overhead. In general, the model performs better when the server model sparsity is low. The results show that for SA, even a sparsity of 90% can lead to good performance (an accuracy deterioration of merely 2% compared to 50% sparsity). However, for IC, the sparsity should be kept to at most 70% to achieve acceptable performance."
            ]
        ]
    }
}