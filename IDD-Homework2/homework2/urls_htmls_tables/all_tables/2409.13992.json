{
    "id_table_1": {
        "caption": "Table 1:  Dataset statistics and experimental settings for NQ, TQA, HotpotQA, FEVER, and FM2 tasks.",
        "table": "S3.T1.1.1",
        "footnotes": [],
        "references": [
            "where  r r r italic_r  represents the relevance scores between the query and each context, and  K weighted subscript K weighted K_{\\text{weighted}} italic_K start_POSTSUBSCRIPT weighted end_POSTSUBSCRIPT  is a  conflict-adjusted similarity matrix  that balances textual similarity and conflict penalties. The symmetrized conflict matrix  C C C italic_C  is computed as described in Section  2.1 , and the weighted similarity matrix is initialized as:",
            "We evaluate SMART on three diverse knowledge-intensive tasks using few-shot in-context learning  Radford et al. ( 2019 ); Brown et al. ( 2020 ); Ram et al. ( 2023 ) . Following prior work  Trivedi et al. ( 2022 ) , we limit the experiments to the first 500 samples from each dataset to manage computational costs. Since test sets are unavailable for datasets from the KILT benchmark  Petroni et al. ( 2020 )  (e.g., HotpotQA, FEVER), we report results on the development sets. The tasks include open-domain question answering, multi-hop reasoning, and fact verification, evaluated on established datasets such as NaturalQuestions (NQ), TriviaQA (TQA), HotpotQA, FEVER, and FOOL ME TWICE (FM2).  An overview of the datasets, evaluation metrics, and experimental settings is provided in Table  1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Optimal hyperparameters (   \\beta italic_  and    \\gamma italic_ ) used for each dataset.",
        "table": "S3.T2.2.2",
        "footnotes": [],
        "references": [
            "where  r r r italic_r  represents the relevance scores between the query and each context, and  K weighted subscript K weighted K_{\\text{weighted}} italic_K start_POSTSUBSCRIPT weighted end_POSTSUBSCRIPT  is a  conflict-adjusted similarity matrix  that balances textual similarity and conflict penalties. The symmetrized conflict matrix  C C C italic_C  is computed as described in Section  2.1 , and the weighted similarity matrix is initialized as:",
            "We evaluate the Llama3-8b-instruct model using the December 2018 Wikipedia dump as the retrieval corpus in a 5-shot in-context learning setup. Initially, we use Contriever  Izacard et al. ( 2021 )  to retrieve the top 50 documents per query. We then apply BGE for pre-ranking, selecting the top 30 sentences with the highest BGE query-context scores. Finally, the SMART method and other context selection baselines are used to select the final set of sentences from this pre-ranked pool for input. Table  2  shows the optimal    \\beta italic_  and    \\gamma italic_  values for each dataset used in our experiments. Additional details on decoding strategies, prompts, few-shot examples, context segmentation, and embedding models are provided in Appendix  B .",
            "\\beta italic_  controls the balance between relevance and diversity in context selection. As shown in Figure  2 , the best results are typically obtained with    \\beta italic_  values between 0.7 and 0.8, depending on the dataset. For example, in FM2,   = 0.8  0.8 \\beta=0.8 italic_ = 0.8  strikes an ideal balance, avoiding redundancy while maintaining relevance. In NQ,   = 0.7  0.7 \\beta=0.7 italic_ = 0.7  provides sufficient diversity to cover multiple facets of the query. Setting    \\beta italic_  too high leads to diminished performance due to redundant context selection.",
            "The number of selected sentences,  top k subscript top k \\text{top}_{k} top start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , is fixed at 5. We perform a grid search over a subset of the development set to determine the optimal hyperparameters, with the results detailed in Table  2 . The beta parameter    \\beta italic_ , which controls the trade-off between relevance and diversity, is adjusted across values  { 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , 1.0 } 0.5 0.6 0.7 0.8 0.9 1.0 \\{0.5,0.6,0.7,0.8,0.9,1.0\\} { 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , 1.0 } . For   = 1.0  1.0 \\beta=1.0 italic_ = 1.0 , the model relies almost entirely on relevance, and we provide the BGE-ranking-only results for comparison. The gamma parameter    \\gamma italic_ , which influences the handling of conflict resolution, is tuned within the range  { 0.1 , 0.2 , ... , 0.9 } 0.1 0.2 ... 0.9 \\{0.1,0.2,\\dots,0.9\\} { 0.1 , 0.2 , ... , 0.9 } ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Performance comparison of various context selection methods across datasets, using the primary evaluation metric mentioned in Table  1 .",
        "table": "S3.T3.1.1",
        "footnotes": [],
        "references": [
            "Table  3  presents the performance of SMART alongside baseline methods across multiple datasets, using Exact Match (EM) for NQ, TQA, FEVER, FM2, and F1 for HotpotQA."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Ablation study comparing the performance of SMART with different settings for relevance and conflict modeling across datasets.",
        "table": "S4.T4.1.1",
        "footnotes": [],
        "references": [
            "The results from the ablation experiments (Table  4 ) clearly highlight the crucial role of both relevance and conflict modeling in SMARTs performance. Removing conflict resolution leads to a significant decline in factual consistency, especially in datasets like  FM2  and  FEVER , where maintaining coherence and accuracy is essential. The most pronounced performance drops are observed when both relevance and conflict modeling are excluded, underscoring the importance of integrating these components for high-quality, diverse, and coherent context selection. These findings reaffirm that managing relevance and conflict is essential for achieving optimal performance in fact-based QA tasks."
        ]
    },
    "global_footnotes": []
}