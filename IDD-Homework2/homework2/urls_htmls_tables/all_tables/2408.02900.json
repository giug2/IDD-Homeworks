{
    "id_table_1": {
        "caption": "Table 2:  Comparison of alignment scores between our generated fine-grained captionsand human annotations.",
        "table": "S3.SS3.SSS0.Px4.1.fig1.1",
        "footnotes": [],
        "references": [
            "We also demonstrate the multigranular textual descriptions in our dataset with those in other common forms. As illustrated in   Figure   1 , our textual description is multigranular with more attributes than radiology report of chest x-rays dataset MIMIC-CXR  [ 21 ] , visual QA dataset SLAKE [ 22 ]  and radiology objects caption dataset ROCO [ 18 ] .",
            "Given a medical image, we aim to generate corresponding multigranular visual and texual annotations by leveraging MLLMs.  Specifically, as shown in  Figure   2 , our pipeline can be decomposed into two stages -  Data Processing  and  Generation of Multigranular Text Description .  In the  Data Processing  stage ( Section   3.2.1 ), we address the lack of domain-specific knowledge in general-purpose MLLMs by leveraging expert grounding models and retrieval-augmented generation (RAG). This stage includes three key steps:  1)  Metadata Integration  to produce coarse captions encapsulating fundamental image information such as modality and disease types;  2)  ROI Locating  to identify regions of abnormalities;  and 3)  Medical Knowledge Retrieval  to extract relevant fine-grained medical details.  Based on the processed data, we then prompt MLLMs to generate multigranular text descriptions, resulting in the creation of fine-grained captions, as detailed in  Section   3.2.2 .",
            "We provide both quantitative analysis and qualitative examples to show the richness of our generated multigranular compare to other medical dataset. Qualitative examples are shown in  Figure   1 , our textual description is multigranular with more attributes than radiology report of chest x-rays dataset MIMIC-CXR  [ 21 ] , visual QA dataset SLAKE [ 22 ]  and radiology objects caption dataset ROCO [ 18 ] . To demonstrate the multi-granularity of our data, we compared the average word count of text descriptions in our dataset, MedTrinity-25M, with those in other medical datasets, as illustrated in  Figure   10 . The word count in our dataset is significantly higher, indicating greater richness.",
            "We conducted an alignment study on SLAKE  [ 22 ]  and MIMIC-CXR  [ 21 ] , randomly selecting 50 samples to compare with fine-grained captions for evaluating alignment scores against human annotations. As shown in  Table   2(b) , the alignment scores were 8.2 and 8.9 for SLAKE and MIMIC-CXR, respectively. The criteria of modality, structure detection, and ROI analysis nearly achieved perfect scores, demonstrating the validity and accuracy of the generated data compared to human annotations. An example of perfect alignment score results evaluated by GPT-4V is shown in  Figure   11 . In these examples, GPT-4V fully aligned with human annotations across all five criteria, resulting in perfect alignment scores.",
            "The prompt used to query GPT-4V for evaluating the alignment score is shown in  Figure   14  of supplementary.",
            "To justify the selection of our specialized medical model, LLaVA-Med++, over GPT-4V for generating textual descriptions, we conducted a quantitative comparison of the outputs generated by both models. We assessed the level of detail by comparing the average word count of text descriptions generated for the same sample. As shown in  Figure   12 , LLaVA-Med++, after task-specific fine-tuning, outperformed GPT-4V by 3.6% in word count, indicating that the descriptions generated by LLaVA-Med++ are more detailed.  Based on these findings, we selected LLaVA-Med++  to generate fine-grained captions  for our entire MedTrinity-25M.",
            "As detailed in Section 3.1 of the main paper, the regions of interest (ROIs) identified using expert grounding models predominantly contain pathological findings such as lesions, inflammation, neoplasms, infections, or other potential abnormalities. In the few instances where no abnormalities are present, the ROIs typically highlight the primary object or organ in the image. Examples of ROIs without abnormalities are shown in Figure  13(b) .",
            "The prompt used to query GPT-4V for evaluating the alignment score is shown in  Figure   14 ."
        ]
    },
    "id_table_2": {
        "caption": "(a)  Alignment Scores on SLAKE",
        "table": "S3.T2.1.1",
        "footnotes": [],
        "references": [
            "Our dataset comprises triplets of  { image , ROI , description } image ROI description \\{\\texttt{image},\\texttt{ROI},\\texttt{description}\\} { image , ROI , description } . Each  ROI  is associated with an abnormality and is represented by a bounding box or a segmentation mask, specifying the relevant region within the  image . For each image, we provide a multigranular textual  description , which includes the disease/lesion type, modality, region-specific description, and inter-regional relationships as illustrated in  Figure   2 .",
            "Given a medical image, we aim to generate corresponding multigranular visual and texual annotations by leveraging MLLMs.  Specifically, as shown in  Figure   2 , our pipeline can be decomposed into two stages -  Data Processing  and  Generation of Multigranular Text Description .  In the  Data Processing  stage ( Section   3.2.1 ), we address the lack of domain-specific knowledge in general-purpose MLLMs by leveraging expert grounding models and retrieval-augmented generation (RAG). This stage includes three key steps:  1)  Metadata Integration  to produce coarse captions encapsulating fundamental image information such as modality and disease types;  2)  ROI Locating  to identify regions of abnormalities;  and 3)  Medical Knowledge Retrieval  to extract relevant fine-grained medical details.  Based on the processed data, we then prompt MLLMs to generate multigranular text descriptions, resulting in the creation of fine-grained captions, as detailed in  Section   3.2.2 .",
            "We conducted an alignment study on SLAKE  [ 22 ]  and MIMIC-CXR  [ 21 ] , randomly selecting 50 samples to compare with fine-grained captions for evaluating alignment scores against human annotations. As shown in  Table   2(b) , the alignment scores were 8.2 and 8.9 for SLAKE and MIMIC-CXR, respectively. The criteria of modality, structure detection, and ROI analysis nearly achieved perfect scores, demonstrating the validity and accuracy of the generated data compared to human annotations. An example of perfect alignment score results evaluated by GPT-4V is shown in  Figure   11 . In these examples, GPT-4V fully aligned with human annotations across all five criteria, resulting in perfect alignment scores.",
            "To justify the selection of our specialized medical model, LLaVA-Med++, over GPT-4V for generating textual descriptions, we conducted a quantitative comparison of the outputs generated by both models. We assessed the level of detail by comparing the average word count of text descriptions generated for the same sample. As shown in  Figure   12 , LLaVA-Med++, after task-specific fine-tuning, outperformed GPT-4V by 3.6% in word count, indicating that the descriptions generated by LLaVA-Med++ are more detailed.  Based on these findings, we selected LLaVA-Med++  to generate fine-grained captions  for our entire MedTrinity-25M."
        ]
    },
    "id_table_3": {
        "caption": "(b)  Alignment Scores on MIMIC-CXR",
        "table": "S3.T2.st1.1",
        "footnotes": [],
        "references": [
            "Given a medical image, we aim to generate corresponding multigranular visual and texual annotations by leveraging MLLMs.  Specifically, as shown in  Figure   2 , our pipeline can be decomposed into two stages -  Data Processing  and  Generation of Multigranular Text Description .  In the  Data Processing  stage ( Section   3.2.1 ), we address the lack of domain-specific knowledge in general-purpose MLLMs by leveraging expert grounding models and retrieval-augmented generation (RAG). This stage includes three key steps:  1)  Metadata Integration  to produce coarse captions encapsulating fundamental image information such as modality and disease types;  2)  ROI Locating  to identify regions of abnormalities;  and 3)  Medical Knowledge Retrieval  to extract relevant fine-grained medical details.  Based on the processed data, we then prompt MLLMs to generate multigranular text descriptions, resulting in the creation of fine-grained captions, as detailed in  Section   3.2.2 .",
            "We aim to generate coarse captions that provide fundamental information for a given image, including modality, organ labels, disease types, and optionally, camera views and equipment information.  Instead of extracting features directly from the images, we generate these captions by integrating dataset metadata. We first extract metadata from the datasets and then apply a fixed rule to integrate this information into coarse captions.  For example, for an image from the QaTa-COV19 dataset 1 1 1 https://www.kaggle.com/aysendegerli/qatacov19-dataset. , we derive metadata from the datasets accompanying paper or documentation, indicating that it consists of COVID-19 chest X-ray images.  Next, we construct coarse captions like A chest X-ray image with COVID-19 in the lungs highlighting the modality, organ types, and disease labels.  If the image contains additional textual information like radiological findings, this is also integrated to enhance the richness of the caption.  The effectiveness of adding coarse captions when generating fine-grained captions is illustrated in  Figure   3 .  In contrast to the scenario without a coarse caption where MLLMs fails to recognize the disease, providing MLLMs with a coarse caption that includes the disease type COVID-19 enables it to identify and categorize the disease, thereby laying the foundation for further analysis.",
            "We initially pretrained LLaVA-Med++  using the methodology of LLaVA-Med  [ 9 ]  as our baseline. Subsequently, for each VQA dataset evaluation, we further pretrained our model on the corresponding MedTrinity-25M  subset to achieve multigranular alignment. The model was then fine-tuned on VQA datasets for three epochs, with performance results presented in  Table   3 . A comparative experiment was conducted without pretraining on MedTrinity-25M , maintaining all other settings. Results clearly demonstrate that LLaVA-Med++ achieves state-of-the-art performance in two of the three VQA benchmarks and ranks third in the remaining one.  Pretraining on MedTrinity-25M exhibits performance improvements of approximately 10.75% on VQA-RAD, 6.1% on SLAKE, and 13.25% on PathVQA compared to the model trained without pretraining on it. This enhancement underscores the efficacy of pretraining on MedTrinity-25M for downstream multimodal medical tasks, particularly in visual question answering.",
            "As detailed in Section 3.1 of the main paper, the regions of interest (ROIs) identified using expert grounding models predominantly contain pathological findings such as lesions, inflammation, neoplasms, infections, or other potential abnormalities. In the few instances where no abnormalities are present, the ROIs typically highlight the primary object or organ in the image. Examples of ROIs without abnormalities are shown in Figure  13(b) ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Data sources for MedTrinity-25M from various medical image datasets, detailing their modalities, biological structures, quantities, and annotations.",
        "table": "S3.T2.st2.1",
        "footnotes": [],
        "references": [
            "Without ROIs, the original description is limited to a brief global analysis of the image. However, with ROIs, MLLMs  can perform a more detailed local analysis of the ROIs and assess the impact of lesion ROIs on the surrounding normal regions, as demonstrated in  Figure   4 .",
            "The prompt used to query GPT-4V for evaluating the alignment score is shown in  Figure   14  of supplementary.",
            "The prompt used to query GPT-4V for evaluating the alignment score is shown in  Figure   14 .",
            "Has the dataset been used already?   Yes. Multigranular annotations enable a wide range of tasks like Medical Visual Question Answering, which we discuss in  Section   4 .",
            "Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet? If not, what are the limitations?   Yes. The preprocessing and collection procedures align with the motivation of creating a comprehensive, large-scale multimodal dataset to support the development of advanced medical AI models. The datasets multigranular annotations enable a wide range of tasks like Medical Visual Question Answering, which we discuss in  Section   4 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  List of expert models used to generate ROIs for different datasets.",
        "table": "S4.T3.1.1",
        "footnotes": [],
        "references": [
            "As detailed in Section 3.2.1 of the main paper, for datasets lacking localization information such as segmentation masks and bounding boxes, we employ various pretrained expert models to identify the ROIs. The specific expert models used for each dataset are listed in  Table   5 ."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A1.T4.1",
        "footnotes": [],
        "references": [
            "We employ various strategies to locate Regions of Interest (ROIs) in images. For datasets that already include localization annotations, such as segmentation masks or bounding boxes, we derive the ROIs from these existing annotations. Specifically, bounding boxes are directly used as the ROIs, while segmentation masks are converted to ROIs by creating the smallest bounding box that covers the mask. When such localization annotations are not available, we apply different pretrained expert models listed in the Appendix to generate ROIs. For text-prompt driven grounding model [ 29 ] , we use disease and organ information in coarse captions as text prompts to guide the model in segmenting specific parts.  Examples of generated ROIs from various modalities with different models are demonstrated in  Figure   6 . It is important to note that for modalities such as X-ray and MRI scans viewed from the z-axis, our ROI localization employs a coordinate system relative to the human body, resulting in a left-right reversal in the image representation.",
            "To ensure that the MLLMs are guided by relevant medical information not inherently present in their training data, we incorporate the processed data (coarse captions, ROIs, and retrieved medical knowledge) into the prompts.  Specifically, for global information, coarse captions are directly integrated into the prompt. For local information, ROIs on images are converted into textual descriptions based on their coordinates and area ratio within the images. Examples of these textual descriptions are shown in  Figure   6 , using terms such as left-center and area ratio: 1.2%."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A1.T4.1.1.1.3.1",
        "footnotes": [],
        "references": [
            "For a given image, we retrieve relevant medical knowledge by using its coarse caption, which is generated through metadata integration. Specifically, we encode the coarse captions, including disease and organ classifications, into vectors using the Med-CPT text encoder. We then perform a vector similarity search in the medical vector database, retrieving the top eight medical knowledge snippets that semantically match the query. These snippets provide the external medical knowledge paired with the image.  A qualitative example demonstrating the effectiveness of incorporating external medical knowledge is shown in  Figure   7 . With access to COVID-19-related medical knowledge, MLLMs can standardize medical terminology and refine diagnoses based on the disease progressions outlined in medical literature."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A1.T4.1.1.1.6.1",
        "footnotes": [],
        "references": [
            "After fine-tuning, we then use this specialized model to generate the multigranular text descriptions on our entire dataset, resulting in 25 million image-ROI-description triplets. The fine-tuning process leverages the advanced language organization capabilities of GPT-4V, providing an effective template for fine-grained captions, which our model uses to learn the formatting of fine-grained captions. As a result, our model generates more detailed descriptions compared to GPT-4V, as illustrated in  Figure   8 . We also show a detailed quantitative comparison in  appendix   B  in the supplementary material."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A1.T4.1.8.7.3.1",
        "footnotes": [],
        "references": [
            "Our dataset encompasses a wide range of 10 imaging modalties, with more than 65 diseases across various anatomical structures in human. The distribution of Anatomical and biological structures in MedTrinity-25M is shown in  Figure   9(b) . Meanwhile, the number of samples in the dataset for each modality are shown in  Figure   9(a) , spanning from common ones with over 1 million samples each (CT, MRI, X-ray) to rare modalities(ultrasound, dermoscopy), demonstrating a much more balanced distribution compared to other large-scale dataset like SA-Med2D-20M [ 38 ] , which only contain thousands of ultrasound and dermoscopy samples.",
            "Figure   9(c)  shows the amount of our dataset, which is significantly larger than previous datasets. To the best of our knowledge, this is the largest open-source, multi-modal multigranular medical dataset to date.",
            "The datasets involved in constructing MedTrinity-25M primarily focus on disease diagnosis and medical discovery.  In MedTrinity-25M, diseases are given in the free-form text. The same disease may be referred to using different terms, allowing for elaborate identification and analysis.  Figure   9(d)  illustrates the frequently used words related to diseases in our dataset."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A1.T4.1.10.9.3.1",
        "footnotes": [],
        "references": [
            "We provide both quantitative analysis and qualitative examples to show the richness of our generated multigranular compare to other medical dataset. Qualitative examples are shown in  Figure   1 , our textual description is multigranular with more attributes than radiology report of chest x-rays dataset MIMIC-CXR  [ 21 ] , visual QA dataset SLAKE [ 22 ]  and radiology objects caption dataset ROCO [ 18 ] . To demonstrate the multi-granularity of our data, we compared the average word count of text descriptions in our dataset, MedTrinity-25M, with those in other medical datasets, as illustrated in  Figure   10 . The word count in our dataset is significantly higher, indicating greater richness."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "A1.T4.1.11.10.3.1",
        "footnotes": [],
        "references": [
            "We conducted an alignment study on SLAKE  [ 22 ]  and MIMIC-CXR  [ 21 ] , randomly selecting 50 samples to compare with fine-grained captions for evaluating alignment scores against human annotations. As shown in  Table   2(b) , the alignment scores were 8.2 and 8.9 for SLAKE and MIMIC-CXR, respectively. The criteria of modality, structure detection, and ROI analysis nearly achieved perfect scores, demonstrating the validity and accuracy of the generated data compared to human annotations. An example of perfect alignment score results evaluated by GPT-4V is shown in  Figure   11 . In these examples, GPT-4V fully aligned with human annotations across all five criteria, resulting in perfect alignment scores."
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "A1.T4.1.12.11.3.1",
        "footnotes": [],
        "references": [
            "To justify the selection of our specialized medical model, LLaVA-Med++, over GPT-4V for generating textual descriptions, we conducted a quantitative comparison of the outputs generated by both models. We assessed the level of detail by comparing the average word count of text descriptions generated for the same sample. As shown in  Figure   12 , LLaVA-Med++, after task-specific fine-tuning, outperformed GPT-4V by 3.6% in word count, indicating that the descriptions generated by LLaVA-Med++ are more detailed.  Based on these findings, we selected LLaVA-Med++  to generate fine-grained captions  for our entire MedTrinity-25M."
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A1.T4.1.14.13.3.1",
        "footnotes": [],
        "references": [
            "As detailed in Section 3.1 of the main paper, the regions of interest (ROIs) identified using expert grounding models predominantly contain pathological findings such as lesions, inflammation, neoplasms, infections, or other potential abnormalities. In the few instances where no abnormalities are present, the ROIs typically highlight the primary object or organ in the image. Examples of ROIs without abnormalities are shown in Figure  13(b) ."
        ]
    },
    "id_table_14": {
        "caption": "",
        "table": "A1.T4.1.15.14.2.1",
        "footnotes": [],
        "references": [
            "The prompt used to query GPT-4V for evaluating the alignment score is shown in  Figure   14  of supplementary.",
            "The prompt used to query GPT-4V for evaluating the alignment score is shown in  Figure   14 ."
        ]
    },
    "id_table_15": {
        "caption": "",
        "table": "A1.T4.1.15.14.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A1.T4.1.23.22.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "A1.T4.1.24.23.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "A1.T4.1.24.23.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "A1.T4.1.25.24.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "A1.T4.1.25.24.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "",
        "table": "A1.T4.1.26.25.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "",
        "table": "A1.T4.1.27.26.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_23": {
        "caption": "",
        "table": "A1.T4.1.28.27.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_24": {
        "caption": "",
        "table": "A1.T4.1.28.27.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_25": {
        "caption": "",
        "table": "A1.T4.1.29.28.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_26": {
        "caption": "",
        "table": "A1.T4.1.29.28.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_27": {
        "caption": "",
        "table": "A1.T4.1.31.30.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_28": {
        "caption": "",
        "table": "A1.T4.1.32.31.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_29": {
        "caption": "",
        "table": "A1.T4.1.33.32.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_30": {
        "caption": "",
        "table": "A1.T4.1.33.32.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_31": {
        "caption": "",
        "table": "A4.T5.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}