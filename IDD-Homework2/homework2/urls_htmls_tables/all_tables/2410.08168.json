{
    "id_table_1": {
        "caption": "Table 1 :  Quantitative evaluation. All metrics are computed on the whole image. Different sections indicate methods, from top to bottom: lighting estimation, image-based compositing and ours. OR and IV refer to OpenRooms  [ 44 ]  and InteriorVerse  [ 91 ] .",
        "table": "id11.3",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "To achieve this, our key idea is to train  ZeroComp  on a simpler proxy task: given a decomposition of an image into its intrinsic componentsdepth, normals, albedo, and partial input shadinggenerate a fully relit image. This network is trained using synthetic datasets like OpenRooms  [ 44 ]  or InteriorVerse  [ 91 ] . At inference, new 3D objects are inserted into the depth, normals, and albedo layers as naive composites. The trained model then generates a fully-shaded version of the object, faithful to the scene lighting while retaining object identity. In short,  ZeroComp  acts as a neural renderer, specifically trained to generate illumination effects such as shading and cast shadows ( Fig.   1 ).",
            "The ControlNet framework does not guarantee a perfect reconstruction of the background image. As demonstrated in previous studies, small details are susceptible to loss  [ 93 ] . To mitigate this, we take inspiration from the differential compositing framework of Debevec  [ 18 ]  and generate a shadow opacity ratio from two predictions of  ZeroComp . Denoting  f   ( i ) subscript f  i f_{\\theta}(\\mathbf{i}) italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_i )  as a full inference pass of the model on input intrinsic maps  i = { i d , i n , i a , i s } i subscript i d subscript i n subscript i a subscript i s \\mathbf{i}=\\{i_{d},i_{n},i_{a},i_{s}\\} bold_i = { italic_i start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT }  (containing depth, normals, albedo and shading resp., c.f.  Sec.   3.1 ), we compute the shadow opacity ratio:",
            "Traditional lighting-based compositing methods set the benchmark by estimating scene lighting for realistic 3D object insertion. These methods use a full 3D object, a delicately curated model for shadow casting, a physically-based rendering engine, and a suitable lighting representation (e.g., parametric lights [ 22 ,  21 ,  72 ] , spherical functions  [ 23 ,  84 ,  17 ] , etc.). For optimal results, everything must be perfectly aligned. In contrast,  ZeroComp  only requires placing the object in 2D, generating intrinsics using simple shaders (depth, normals, and albedo), and relies on the network understanding to infer missing information. Despite the task is more challenging,  ZeroComp  achieves competitive results, surpassing many explicit lighting-based techniques  [ 22 ,  70 ,  72 ] , as shown in  Tab.   1 . Qualitative comparisons in  Fig.   4  show that  ZeroComp  realistically shades these objects while maintaining their appearance, acting as a strong contender to traditional approaches.",
            "The quantitative results in  Tab.   1  (middle bracket) show a superior score for  ZeroComp  in all metrics against image-based compositing methods. We identify two main issues with other approaches: they tend to 1) modify the object itself or its pose  [ 12 ,  85 ] ; or 2) generate shadows of limited quality  [ 11 ,  46 ] . In contrast,  ZeroComp  preserves the original appearance and pose of objects and generates complex and realistic shadows even without access to the full 3D model of the virtual object.",
            "As shown in  Tab.   2 , our method trained on OpenRooms achieves a 45% confusion rate, indicating a strong preference for the realism of our composites.  ZeroComp  trained on InteriorVerse (IV) doesnt perform as well, presumably due to weaker shadows. The method with the best quantitative score from  Tab.   1 , Garon19  [ 23 ] , is ranked fourth with 31.5%, corroborating recent findings that image evaluation metrics do not correlate well with human perception  [ 27 ] . We achieve statistically significant better results against all methods, except with EMLight. We therefore conduct a second user study where  N = 19 N 19 N=19 italic_N = 19  participants were shown 100 pairs of images, each pair containing one result from  ZeroComp  and the other from EMLight, in randomized order. Users selected our method 55.4   plus-or-minus \\pm   2.2% of the time, demonstrating preference for  ZeroComp ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Results of our 2AFC user study indicates the perceived realism of the composites, sorted by decreasing confusion (perfect confusion is 50%), and 95% confidence intervals (where * indicate a statistically significant difference with  ZeroComp  OR).",
        "table": "S3.F3.16",
        "footnotes": [],
        "references": [
            "A crucial element of  ZeroComp  involves training it to develop an implicit understanding of the lighting and geometry from the intrinsic maps, enabling it to correct the appearance of the shading where it is missing. During inference, objects can be inserted into scenes using their albedo, normals, and depth maps. For the shading component, regions corresponding to the inserted objects are masked, and our trained  ZeroComp  adjusts this shading to align with the scenes original lighting. An overview of our zero-shot intrinsic compositing approach is illustrated in  Fig.   2 .",
            "Our goal is to place objects into photographs to achieve a seamless blend without prior access to intrinsic scene information. To accomplish this, we utilize available pretrained, off-the-shelf models that infer the intrinsic properties of the background  i bg subscript i bg \\mathbf{i}_{\\mathrm{bg}} bold_i start_POSTSUBSCRIPT roman_bg end_POSTSUBSCRIPT  from the given photograph ( Fig.   2 , top). Specifically, we employ ZoeDepth  [ 4 ]  to extract depth maps from the input images. For normals, we leverage StableNormal  [ 76 ] . Albedo is estimated using Intrinsic Image Diffusion (IID)  [ 37 ] . The shading information is derived by dividing the original image with its albedo.",
            "For the objects we intend to insert ( Fig.   2 , middle), we use the Blender  [ 16 ]  graphics engine to render its intrinsic layers  i obj subscript i obj \\mathbf{i}_{\\mathrm{obj}} bold_i start_POSTSUBSCRIPT roman_obj end_POSTSUBSCRIPT , except shading which is unknown. As with traditional IBL, this allows the user full control over the object pose and location in the target image. Each intrinsic map from the background and object are then composited together through simple compositing",
            "where  i c  { i d , i n , i a , i s } subscript i c subscript i d subscript i n subscript i a subscript i s i_{c}\\in\\{i_{d},i_{n},i_{a},i_{s}\\} italic_i start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  { italic_i start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT }  denotes one of the intrinsic maps and  m m m italic_m  the object mask obtained from the graphics engine, resulting in a set of composite intrinsics ( Fig.   2 , bottom). Since the depth scale of the object and the background may not match, we align the object footprint depth (planar projection on the vertical axis) with the background depth by fitting an affine transform to the footprint and applying it to  i d , obj subscript i d obj i_{d,\\mathrm{obj}} italic_i start_POSTSUBSCRIPT italic_d , roman_obj end_POSTSUBSCRIPT . An object also affects surroundings (e.g., by casting shadows), so we mask any pixel from the shading map if the pixel estimated 3D position is within a distance threshold",
            "Finally, our trained  ZeroComp  is run on the composite intrinsics to obtain the final output ( Fig.   2 , right), where the newly added object appears as a natural part of the original scene. This approach enables the insertion of objects into various scenes with realistic lighting interactions including reshading and casting shadows, achieving zero-shot compositing. For all our experiments, we use seed 469, which was shown in  [ 74 ]  to produce the highest-quality generations in SD among 1000 seeds.",
            "where  i comp subscript i comp \\mathbf{i}_{\\mathrm{comp}} bold_i start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT  and  i bg subscript i bg \\mathbf{i}_{\\mathrm{bg}} bold_i start_POSTSUBSCRIPT roman_bg end_POSTSUBSCRIPT  are the intrinsic maps of the composite and background resp., see  Eq.   2 . Note that  Eq.   4  is computed on grayscale and the result is clamped to  [ 0 , 1 ] 0 1 [0,1] [ 0 , 1 ] . We fix the diffusion with the same seed and use the same shading mask on the background  i s subscript i s i_{s} italic_i start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  to minimize discrepancies between both predictions. To further avoid unnecessary opacity unrelated to the object, we set the opacity to 1 if they are outside of the shading mask  m m m italic_m  computed earlier (c.f.  Sec.   3.2 ). A Gaussian blur with kernel  15  15 15 15 15\\times 15 15  15  and   = 1.5  1.5 \\sigma=1.5 italic_ = 1.5  is applied to  m m m italic_m  to avoid blending artifacts. The final compositing equation is",
            "As shown in  Tab.   2 , our method trained on OpenRooms achieves a 45% confusion rate, indicating a strong preference for the realism of our composites.  ZeroComp  trained on InteriorVerse (IV) doesnt perform as well, presumably due to weaker shadows. The method with the best quantitative score from  Tab.   1 , Garon19  [ 23 ] , is ranked fourth with 31.5%, corroborating recent findings that image evaluation metrics do not correlate well with human perception  [ 27 ] . We achieve statistically significant better results against all methods, except with EMLight. We therefore conduct a second user study where  N = 19 N 19 N=19 italic_N = 19  participants were shown 100 pairs of images, each pair containing one result from  ZeroComp  and the other from EMLight, in randomized order. Users selected our method 55.4   plus-or-minus \\pm   2.2% of the time, demonstrating preference for  ZeroComp .",
            "2D object compositing.   ZeroComp  can also be applied to 2D objects segmented from real images, where a 3D model is not available. Here, we rely on intrinsic estimators ( Sec.   3.2 ) to estimate the object depth and normals. We use the RGB as the albedo to avoid detrimental noise in the image texture while keeping the rest of the pipeline unchanged. For demonstration purposes, the object was segmented and placed in the target image manually.  Fig.   9  shows several such examples, showing our method can be easily extended to the case of 2D object compositing."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Ablation study on the shading radius, different inputs. The baseline and input-ablated models are trained for 220k steps.",
        "table": "S4.F4.35",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "The ControlNet framework does not guarantee a perfect reconstruction of the background image. As demonstrated in previous studies, small details are susceptible to loss  [ 93 ] . To mitigate this, we take inspiration from the differential compositing framework of Debevec  [ 18 ]  and generate a shadow opacity ratio from two predictions of  ZeroComp . Denoting  f   ( i ) subscript f  i f_{\\theta}(\\mathbf{i}) italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_i )  as a full inference pass of the model on input intrinsic maps  i = { i d , i n , i a , i s } i subscript i d subscript i n subscript i a subscript i s \\mathbf{i}=\\{i_{d},i_{n},i_{a},i_{s}\\} bold_i = { italic_i start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT }  (containing depth, normals, albedo and shading resp., c.f.  Sec.   3.1 ), we compute the shadow opacity ratio:",
            "where  i comp subscript i comp \\mathbf{i}_{\\mathrm{comp}} bold_i start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT  and  i bg subscript i bg \\mathbf{i}_{\\mathrm{bg}} bold_i start_POSTSUBSCRIPT roman_bg end_POSTSUBSCRIPT  are the intrinsic maps of the composite and background resp., see  Eq.   2 . Note that  Eq.   4  is computed on grayscale and the result is clamped to  [ 0 , 1 ] 0 1 [0,1] [ 0 , 1 ] . We fix the diffusion with the same seed and use the same shading mask on the background  i s subscript i s i_{s} italic_i start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  to minimize discrepancies between both predictions. To further avoid unnecessary opacity unrelated to the object, we set the opacity to 1 if they are outside of the shading mask  m m m italic_m  computed earlier (c.f.  Sec.   3.2 ). A Gaussian blur with kernel  15  15 15 15 15\\times 15 15  15  and   = 1.5  1.5 \\sigma=1.5 italic_ = 1.5  is applied to  m m m italic_m  to avoid blending artifacts. The final compositing equation is",
            "where  C C C italic_C  is a color balance factor computed as the average color ratio of background  x bg subscript x bg x_{\\mathrm{bg}} italic_x start_POSTSUBSCRIPT roman_bg end_POSTSUBSCRIPT  and the network output  f  ( i comp ) f subscript i comp f(\\mathbf{i}_{\\mathrm{comp}}) italic_f ( bold_i start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT )  to account for global color shifts. In  Fig.   3 , we compare our composition and direct output of the network.",
            "In  Tab.   3 , we ablate shading radius (Radius), various intrinsic maps as input (Input). The quantitative difference due to the shading radius is confirmed visually in  Fig.   5 . However, metrics contradict visual observations when it comes to using different inputs. From  Fig.   6 , not using the depth or normal maps results in a loss of realism.",
            "2D object compositing.   ZeroComp  can also be applied to 2D objects segmented from real images, where a 3D model is not available. Here, we rely on intrinsic estimators ( Sec.   3.2 ) to estimate the object depth and normals. We use the RGB as the albedo to avoid detrimental noise in the image texture while keeping the rest of the pipeline unchanged. For demonstration purposes, the object was segmented and placed in the target image manually.  Fig.   9  shows several such examples, showing our method can be easily extended to the case of 2D object compositing."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "S5.T1.7.7",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "where  i comp subscript i comp \\mathbf{i}_{\\mathrm{comp}} bold_i start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT  and  i bg subscript i bg \\mathbf{i}_{\\mathrm{bg}} bold_i start_POSTSUBSCRIPT roman_bg end_POSTSUBSCRIPT  are the intrinsic maps of the composite and background resp., see  Eq.   2 . Note that  Eq.   4  is computed on grayscale and the result is clamped to  [ 0 , 1 ] 0 1 [0,1] [ 0 , 1 ] . We fix the diffusion with the same seed and use the same shading mask on the background  i s subscript i s i_{s} italic_i start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  to minimize discrepancies between both predictions. To further avoid unnecessary opacity unrelated to the object, we set the opacity to 1 if they are outside of the shading mask  m m m italic_m  computed earlier (c.f.  Sec.   3.2 ). A Gaussian blur with kernel  15  15 15 15 15\\times 15 15  15  and   = 1.5  1.5 \\sigma=1.5 italic_ = 1.5  is applied to  m m m italic_m  to avoid blending artifacts. The final compositing equation is",
            "Finally, we render the object using physically based rendering in Blender  [ 16 ] . To account for spatially-varying indoor lighting, we warp the panorama by converting it to a 3D mesh according to its depth map from  [ 56 ] , and reproject it at the center of the objects bounding box. This approach provides high-quality simulated ground truth, enabling the generation of many more scenes than the 20 available in  [ 23 ] . The last column of  Fig.   4  shows a representative subset of realistic image composites. This dataset will be released publicly upon publication of the paper.",
            "In this section, we conduct a comprehensive evaluation of  ZeroComp s performance as a neural renderer for zero-shot compositing. Leveraging the evaluation dataset introduced in  Sec.   4 , we quantitatively and qualitatively compare against state-of-the-art methods on a range of metrics to offer a multi-faceted assessment of image quality. We use standard metrics to compare composites with the simulated ground truth, including Peak Signal-to-Noise Ratio (PSNR), Root Mean Square Error (RMSE) and its scale-invariant version (si-RMSE), Mean Absolute Difference (MAE), Structural Similarity Index Measure (SSIM)  [ 71 ] , Learned Perceptual Image Patch Similarity (LPIPS)  [ 87 ] , and FLIP  [ 1 ] . While we agree that perceptual metrics are better suited for our task, several researchers  [ 28 ,  35 ,  26 ]  highlight the vulnerability of neural network-based metrics like LPIPS to noise and adversarial attacks. To mitigate the influence of the rendering noise present in the training datasets  [ 44 ,  91 ] , we resize both the test images and references to  256  256 256 256 256\\times 256 256  256  for all methods on LPIPS. Additionally, FLIP addresses this issue by applying a spatial filter removing high frequency details imperceptible to humans. During evaluation, we demonstrate that our approach achieves performance comparable to most lighting estimation methods, all without explicitly modeling lighting conditions. We also contrast our method with diffusion and intrinsic image-based baselines  [ 12 ,  85 ,  11 ,  46 ] , showcasing superiority on most metrics.",
            "Traditional lighting-based compositing methods set the benchmark by estimating scene lighting for realistic 3D object insertion. These methods use a full 3D object, a delicately curated model for shadow casting, a physically-based rendering engine, and a suitable lighting representation (e.g., parametric lights [ 22 ,  21 ,  72 ] , spherical functions  [ 23 ,  84 ,  17 ] , etc.). For optimal results, everything must be perfectly aligned. In contrast,  ZeroComp  only requires placing the object in 2D, generating intrinsics using simple shaders (depth, normals, and albedo), and relies on the network understanding to infer missing information. Despite the task is more challenging,  ZeroComp  achieves competitive results, surpassing many explicit lighting-based techniques  [ 22 ,  70 ,  72 ] , as shown in  Tab.   1 . Qualitative comparisons in  Fig.   4  show that  ZeroComp  realistically shades these objects while maintaining their appearance, acting as a strong contender to traditional approaches."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S5.T2.8",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "where  m y , obj subscript m y obj m_{y,\\mathrm{obj}} italic_m start_POSTSUBSCRIPT italic_y , roman_obj end_POSTSUBSCRIPT  represents the (3D)  y y y italic_y  coordinate of a pixel in the object mask  m m m italic_m  (obtained from the depth map  i d , obj subscript i d obj i_{d,\\mathrm{obj}} italic_i start_POSTSUBSCRIPT italic_d , roman_obj end_POSTSUBSCRIPT ). This threshold is motivated by the fact that the length of a shadow is typically proportional to the object height. In practice, we set the relative shading radius   = 1.0  1.0 \\lambda=1.0 italic_ = 1.0 , and explore different values in  Sec.   5 . Pixels in the shading map directly above the object are never masked, to avoid unnecessary shadows (on the ceiling, for instance).",
            "In  Tab.   3 , we ablate shading radius (Radius), various intrinsic maps as input (Input). The quantitative difference due to the shading radius is confirmed visually in  Fig.   5 . However, metrics contradict visual observations when it comes to using different inputs. From  Fig.   6 , not using the depth or normal maps results in a loss of realism."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "S5.T3.5",
        "footnotes": [],
        "references": [
            "In  Tab.   3 , we ablate shading radius (Radius), various intrinsic maps as input (Input). The quantitative difference due to the shading radius is confirmed visually in  Fig.   5 . However, metrics contradict visual observations when it comes to using different inputs. From  Fig.   6 , not using the depth or normal maps results in a loss of realism."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "S5.F5.9",
        "footnotes": [],
        "references": [
            "Material editing.  By training on additional intrinsic maps such as roughness and metallic available in the InteriorVerse dataset  [ 91 ] ,  ZeroComp  can also adjust the materials of the virtual object. In  Fig.   7 , we modify the roughness (RG) and metallicity (MT) to demonstrate the effectiveness of  ZeroComp  in handling more advanced materials."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "S5.F6.12",
        "footnotes": [],
        "references": [
            "Outdoor images.  Despite being trained only on indoor imagery,  ZeroComp  also generalizes to outdoor scenes and can generate realistic shadows, as demonstrated in  Fig.   8 ."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "S5.F7.6",
        "footnotes": [],
        "references": [
            "2D object compositing.   ZeroComp  can also be applied to 2D objects segmented from real images, where a 3D model is not available. Here, we rely on intrinsic estimators ( Sec.   3.2 ) to estimate the object depth and normals. We use the RGB as the albedo to avoid detrimental noise in the image texture while keeping the rest of the pipeline unchanged. For demonstration purposes, the object was segmented and placed in the target image manually.  Fig.   9  shows several such examples, showing our method can be easily extended to the case of 2D object compositing.",
            "Limitations.   ZeroComp  depends on intrinsic estimators to decompose the background image. While the model shows robustness to these estimators (see supplement), even with synthetic training data, errors in their predictions can affect rendering quality. Nonetheless,  ZeroComp  has demonstrated impressive results when fully leveraging estimated albedo and shading for both the background and inserted objects (see  Fig.   9 ), suggesting that non-synthetic training data could enhance the models robustness."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "S5.F8.3",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "S5.F9.6",
        "footnotes": [],
        "references": []
    }
}