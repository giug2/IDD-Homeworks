{
    "PAPER'S NUMBER OF TABLES": 8,
    "S4.T1": {
        "caption": "Table 1: Statistical information of datasets.",
        "table": "<div id=\"S4.T1.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:147.4pt;height:45.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-41.9pt,13.0pt) scale(0.63784,0.63784) ;\">\n<p id=\"S4.T1.4.1\" class=\"ltx_p\"><span id=\"S4.T1.4.1.1\" class=\"ltx_text\">\n<span id=\"S4.T1.4.1.1.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:231.1pt;height:72pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span id=\"S4.T1.4.1.1.1.1\" class=\"ltx_p\"><span id=\"S4.T1.4.1.1.1.1.1\" class=\"ltx_text\">\n<span id=\"S4.T1.4.1.1.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S4.T1.4.1.1.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S4.T1.4.1.1.1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">Dataset</span>\n<span id=\"S4.T1.4.1.1.1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">Feature Length</span>\n<span id=\"S4.T1.4.1.1.1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">Subject Number</span></span>\n<span id=\"S4.T1.4.1.1.1.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S4.T1.4.1.1.1.1.1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">FEMNIST</span>\n<span id=\"S4.T1.4.1.1.1.1.1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">1*28*28</span>\n<span id=\"S4.T1.4.1.1.1.1.1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">3,580</span></span>\n<span id=\"S4.T1.4.1.1.1.1.1.1.3\" class=\"ltx_tr\">\n<span id=\"S4.T1.4.1.1.1.1.1.1.3.1\" class=\"ltx_td ltx_align_center\">Shakespeare</span>\n<span id=\"S4.T1.4.1.1.1.1.1.1.3.2\" class=\"ltx_td ltx_align_center\">32</span>\n<span id=\"S4.T1.4.1.1.1.1.1.1.3.3\" class=\"ltx_td ltx_align_center\">404</span></span>\n<span id=\"S4.T1.4.1.1.1.1.1.1.4\" class=\"ltx_tr\">\n<span id=\"S4.T1.4.1.1.1.1.1.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">Synthetic</span>\n<span id=\"S4.T1.4.1.1.1.1.1.1.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">60</span>\n<span id=\"S4.T1.4.1.1.1.1.1.1.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">200</span></span>\n</span></span></span>\n</span></span></span></p>\n</span></div>\n\n",
        "footnotes": "\n\n\n\n\nDataset\nFeature Length\nSubject Number\n\nFEMNIST\n1*28*28\n3,580\n\nShakespeare\n32\n404\n\nSynthetic\n60\n200\n\n",
        "references": [
            "Apart from those two already existing datasets, we also construct a Synthetic dataset following the guide of [39]. We model each subject as a random (and valid) mean and covariance matrix in a multivariate Gaussian distribution to obtain well-separated and characterized subjects.  In particular, we enforce a minimum pair-wise (L2 >> 0.35) separation between all subject distributions’ means to avoid overlap. The label of a generated data point is the outcome of an XOR operation between a list of indicator values, each representing whether the specific feature is larger than 0. For the input feature x=[x1,…,xp]𝑥subscript𝑥1…subscript𝑥𝑝x=[x_{1},\\dots,x_{p}], the label y=⊕i=1pII​(xi≥0)𝑦superscriptsubscriptdirect-sum𝑖1𝑝IIsubscript𝑥𝑖0y=\\oplus_{i=1}^{p}\\text{I\\kern-1.19995ptI}(x_{i}\\geq 0), where ⊕direct-sum\\oplus is the XOR operation and II is the identity function.We construct a Synthetic dataset with 200 subjects, each with 400 data points. The length of the feature is 60, and the number of categories is two due to the range of XOR values. We summarize the statistics of each dataset in Table 1."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: The averaged metric values of 50 runs (subjects) under various methods.",
        "table": "<div id=\"S5.T2.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:424.9pt;height:53.2pt;vertical-align:-0.5pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-244.5pt,30.3pt) scale(0.46491,0.46491) ;\">\n<p id=\"S5.T2.4.1\" class=\"ltx_p\"><span id=\"S5.T2.4.1.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:914.0pt;height:114.4pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span id=\"S5.T2.4.1.1.1.1\" class=\"ltx_p\"><span id=\"S5.T2.4.1.1.1.1.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_2\"><span id=\"S5.T2.4.1.1.1.1.1.1.1.1.1\" class=\"ltx_text\">Dataset</span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_4\">Averaged Accuracy</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_4\">Averaged Precision</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_4\">Averaged Recall</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_4\">Averaged F1</span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.4.1.1.1.1.1.1.2.1.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Avg Loss</span></span>\n</span></span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.4.1.1.1.1.1.1.2.2.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.2.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.2.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Min Loss</span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.2.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.2.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Time</span></span>\n</span></span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.4.1.1.1.1.1.1.2.3.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.3.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.3.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.3.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">SLSIA</span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.3.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.3.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(CNN)</span></span>\n</span></span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.4.1.1.1.1.1.1.2.4.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.4.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.4.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.4.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">SLSIA</span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.4.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.4.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(SVM)</span></span>\n</span></span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.4.1.1.1.1.1.1.2.5.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.5.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.5.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.5.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Avg Loss</span></span>\n</span></span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.4.1.1.1.1.1.1.2.6.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.6.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.6.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.6.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Min Loss</span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.6.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.6.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Time</span></span>\n</span></span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.4.1.1.1.1.1.1.2.7.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.7.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.7.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.7.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">SLSIA</span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.7.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.7.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(CNN)</span></span>\n</span></span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.4.1.1.1.1.1.1.2.8.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.8.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.8.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.8.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">SLSIA</span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.8.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.8.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(SVM)</span></span>\n</span></span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.4.1.1.1.1.1.1.2.9.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.9.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.9.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.9.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Avg Loss</span></span>\n</span></span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.10\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.4.1.1.1.1.1.1.2.10.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.10.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.10.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.10.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Min Loss</span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.10.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.10.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Time</span></span>\n</span></span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.11\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.4.1.1.1.1.1.1.2.11.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.11.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.11.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.11.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">SLSIA</span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.11.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.11.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(CNN)</span></span>\n</span></span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.12\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.4.1.1.1.1.1.1.2.12.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.12.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.12.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.12.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">SLSIA</span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.12.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.12.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(SVM)</span></span>\n</span></span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.13\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.4.1.1.1.1.1.1.2.13.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.13.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.13.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.13.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Avg Loss</span></span>\n</span></span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.14\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.4.1.1.1.1.1.1.2.14.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.14.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.14.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.14.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Min Loss</span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.14.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.14.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Time</span></span>\n</span></span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.15\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.4.1.1.1.1.1.1.2.15.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.15.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.15.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.15.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">SLSIA</span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.15.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.15.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(CNN)</span></span>\n</span></span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.16\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.4.1.1.1.1.1.1.2.16.1\" class=\"ltx_text\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.16.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.16.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.16.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">SLSIA</span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.16.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.2.16.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(SVM)</span></span>\n</span></span></span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\">FEMNIST</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.424</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.372</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.504</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.484</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.424</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\">0.372</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\">0.375</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.9\" class=\"ltx_td ltx_align_center ltx_border_t\">0.356</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.10\" class=\"ltx_td ltx_align_center ltx_border_t\">0.424</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.11\" class=\"ltx_td ltx_align_center ltx_border_t\">0.372</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.12\" class=\"ltx_td ltx_align_center ltx_border_t\">0.304</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.13\" class=\"ltx_td ltx_align_center ltx_border_t\">0.344</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.14\" class=\"ltx_td ltx_align_center ltx_border_t\">0.364</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.15\" class=\"ltx_td ltx_align_center ltx_border_t\">0.332</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.16\" class=\"ltx_td ltx_align_center ltx_border_t\">0.048</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.3.17\" class=\"ltx_td ltx_align_center ltx_border_t\">0.071</span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.1\" class=\"ltx_td ltx_align_center\">Shakespeare</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.2\" class=\"ltx_td ltx_align_center\">0.452</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.3\" class=\"ltx_td ltx_align_center\">0.340</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.4\" class=\"ltx_td ltx_align_center\">0.638</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.5\" class=\"ltx_td ltx_align_center\">0.648</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.6\" class=\"ltx_td ltx_align_center\">0.452</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.7\" class=\"ltx_td ltx_align_center\">0.340</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.8\" class=\"ltx_td ltx_align_center\">0.628</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.9\" class=\"ltx_td ltx_align_center\">0.578</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.10\" class=\"ltx_td ltx_align_center\">0.452</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.11\" class=\"ltx_td ltx_align_center\">0.340</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.12\" class=\"ltx_td ltx_align_center\">0.720</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.13\" class=\"ltx_td ltx_align_center\">0.584</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.14\" class=\"ltx_td ltx_align_center\">0.432</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.15\" class=\"ltx_td ltx_align_center\">0.280</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.16\" class=\"ltx_td ltx_align_center\">0.622</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.4.17\" class=\"ltx_td ltx_align_center\">0.483</span></span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5\" class=\"ltx_tr\">\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.1\" class=\"ltx_td ltx_align_center ltx_border_b\">Synthetic</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.2\" class=\"ltx_td ltx_align_center ltx_border_b\">0.480</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.360</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.888</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0.868</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0.480</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.7\" class=\"ltx_td ltx_align_center ltx_border_b\">0.360</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.8\" class=\"ltx_td ltx_align_center ltx_border_b\">0.911</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.9\" class=\"ltx_td ltx_align_center ltx_border_b\">0.937</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.10\" class=\"ltx_td ltx_align_center ltx_border_b\">0.480</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.11\" class=\"ltx_td ltx_align_center ltx_border_b\">0.360</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.12\" class=\"ltx_td ltx_align_center ltx_border_b\">0.876</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.13\" class=\"ltx_td ltx_align_center ltx_border_b\">0.760</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.14\" class=\"ltx_td ltx_align_center ltx_border_b\">0.460</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.15\" class=\"ltx_td ltx_align_center ltx_border_b\">0.360</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.16\" class=\"ltx_td ltx_align_center ltx_border_b\">0.870</span>\n<span id=\"S5.T2.4.1.1.1.1.1.1.5.17\" class=\"ltx_td ltx_align_center ltx_border_b\">0.800</span></span>\n</span></span></span>\n</span></span></span></p>\n</span></div>\n\n",
        "footnotes": "\n\n\n\n\nDataset\nAveraged Accuracy\nAveraged Precision\nAveraged Recall\nAveraged F1\n\n\n\n\nAvg Loss\n\n\n\n\nMin Loss\n\nTime\n\n\n\n\nSLSIA\n\n(CNN)\n\n\n\n\nSLSIA\n\n(SVM)\n\n\n\n\nAvg Loss\n\n\n\n\nMin Loss\n\nTime\n\n\n\n\nSLSIA\n\n(CNN)\n\n\n\n\nSLSIA\n\n(SVM)\n\n\n\n\nAvg Loss\n\n\n\n\nMin Loss\n\nTime\n\n\n\n\nSLSIA\n\n(CNN)\n\n\n\n\nSLSIA\n\n(SVM)\n\n\n\n\nAvg Loss\n\n\n\n\nMin Loss\n\nTime\n\n\n\n\nSLSIA\n\n(CNN)\n\n\n\n\nSLSIA\n\n(SVM)\n\n\nFEMNIST\n0.424\n0.372\n0.504\n0.484\n0.424\n0.372\n0.375\n0.356\n0.424\n0.372\n0.304\n0.344\n0.364\n0.332\n0.048\n0.071\n\nShakespeare\n0.452\n0.340\n0.638\n0.648\n0.452\n0.340\n0.628\n0.578\n0.452\n0.340\n0.720\n0.584\n0.432\n0.280\n0.622\n0.483\n\nSynthetic\n0.480\n0.360\n0.888\n0.868\n0.480\n0.360\n0.911\n0.937\n0.480\n0.360\n0.876\n0.760\n0.460\n0.360\n0.870\n0.800\n\n",
        "references": [
            "Besides the accuracy, we also observe the higher values of the other three metrics in our methods compared with the other two approaches. Table 2 shows the average value of each metric list obtained from 50 runs (subjects) under each method. The average metric values (accuracy, precision, recall, and F1) of both Avg Loss and Min Loss Time are respectively about 0.45 and 0.35, which is close to random guess.\nThe inefficiency of those two methods indicates that the model trained with the data from the target subject will not obtain a lower loss and will not frequently get a minor loss among all local models on evaluation data points from the target subject. There are two reasons behind this phenomenon. First, the evaluation data points from the target subject are not included in the weight update during FL training, which can not guarantee a lower loss and frequent minor loss. The second reason is that the local model trains one round of FL before attacking, which makes the local model learn insufficient information from the target subject’s data (Dstcsuperscriptsubscript𝐷subscript𝑠𝑡𝑐D_{s_{t}}^{c}). It is hard to determine the FL round where the target local clients learn better from the target subject data than random local clients. It is also one of the reasons that led us to perform our SLSIA at the first FL round.",
            "From Figure 2, we observe that the accuracy of the attack varies from close to 0 to 1. To further explore the reason behind accuracy variance, we measure the average input feature distance between the target and other random subjects in the training data of pre-trained and local models. Table 3 exposes the average input feature distance of the target subject with a high accuracy and the target subject with a low accuracy under each dataset. As expected, the table shows that the target subject with high attack accuracy has input features relatively far from other random subjects, unlike the target subject with relatively low accuracy, which has relatively compact input features among target and random subjects.\nThe average input feature distance is the average pair-wise Euclidean distance between two subjects’ flattened data input features. For example, the target subject with an accuracy of 0.8 in FEMNIST has average input feature distances of 5.413, 5.472, 5.383, and 5.340, which are about 8% higher than the target subject with an accuracy of 0.1 with distances of 5.018, 5.065, 4.780, and 4.738. Other datasets have a similar phenomenon with different input feature distance gap levels. We note that Shakespeare’s average input feature distance is enormous as we use the index of words in the dictionary as the input feature rather than the embedding of each word after the embedding layer. It will not change the comparison of high and low-accuracy target subjects, as the embedding for the same word is the same. From the above analysis, we conclude that the target subjects, separate from other random subjects, are more vulnerable to our SLSIA. By comparing the average actual (rather than the dictionary index in Shakespeare) input feature distance between the FEMNIST and Synthetic datasets and the attack performance in Table 2, we can also observe the dataset with a relatively higher average input feature distance will have a relatively higher attack accuracy under our SLSIA. In other words, the dataset with subjects slightly far from each other is more vulnerable. If the subject is relatively far from other subjects, the models could learn separate embeddings for its data points, which makes the binary attack model easier to learn and transfer from the pre-trained models to the local models."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: The averaged input feature distance between the data from the target and random subjects.",
        "table": "<div id=\"S5.T3.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:277.5pt;height:101pt;vertical-align:-0.6pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-92.7pt,33.5pt) scale(0.59953,0.59953) ;\">\n<p id=\"S5.T3.4.1\" class=\"ltx_p\"><span id=\"S5.T3.4.1.1\" class=\"ltx_text\">\n<span id=\"S5.T3.4.1.1.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:462.9pt;height:168.4pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span id=\"S5.T3.4.1.1.1.1\" class=\"ltx_p\"><span id=\"S5.T3.4.1.1.1.1.1\" class=\"ltx_text\">\n<span id=\"S5.T3.4.1.1.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_2\"><span id=\"S5.T3.4.1.1.1.1.1.1.1.1.1\" class=\"ltx_text\">Dataset</span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_2\"><span id=\"S5.T3.4.1.1.1.1.1.1.1.2.1\" class=\"ltx_text\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.1.2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.1.2.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.1.2.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">SLSIA</span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.1.2.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.1.2.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(CNN)</span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.1.2.1.1.3\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.1.2.1.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">accuracy</span></span>\n</span></span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_4\">the source of random subjects</span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.4.1.1.1.1.1.1.2.1.1\" class=\"ltx_text\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">random pre-trained</span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">models</span></span>\n</span></span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.4.1.1.1.1.1.1.2.2.1\" class=\"ltx_text\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.2.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.2.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">target pre-trained</span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.2.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.2.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">models</span></span>\n</span></span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.4.1.1.1.1.1.1.2.3.1\" class=\"ltx_text\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.3.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.3.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.3.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">random local</span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.3.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.3.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">models</span></span>\n</span></span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.4.1.1.1.1.1.1.2.4.1\" class=\"ltx_text\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.4.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.4.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.4.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">target local</span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.4.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.2.4.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">models</span></span>\n</span></span></span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.3\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_2\"><span id=\"S5.T3.4.1.1.1.1.1.1.3.1.1\" class=\"ltx_text\">FEMNIST</span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.8</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">5.413</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">5.472</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">5.383</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">5.340</span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.4\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.4.1\" class=\"ltx_td ltx_align_center\">0.1</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.4.2\" class=\"ltx_td ltx_align_center\">5.018</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.4.3\" class=\"ltx_td ltx_align_center\">5.065</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.4.4\" class=\"ltx_td ltx_align_center\">4.780</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.4.5\" class=\"ltx_td ltx_align_center\">4.738</span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.5\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.5.1\" class=\"ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_2\"><span id=\"S5.T3.4.1.1.1.1.1.1.5.1.1\" class=\"ltx_text\">Shakespeare</span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.9</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">71881.506</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">72077.575</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">71604.278</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\">71579.257</span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.6\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.6.1\" class=\"ltx_td ltx_align_center\">0.4</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.6.2\" class=\"ltx_td ltx_align_center\">69823.702</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.6.3\" class=\"ltx_td ltx_align_center\">70085.036</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.6.4\" class=\"ltx_td ltx_align_center\">69905.794</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.6.5\" class=\"ltx_td ltx_align_center\">69608.564</span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.7\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.7.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t ltx_rowspan ltx_rowspan_2\"><span id=\"S5.T3.4.1.1.1.1.1.1.7.1.1\" class=\"ltx_text\">Synthetic</span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\">1.0</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\">14.642</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\">16.464</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\">15.921</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.7.6\" class=\"ltx_td ltx_align_center ltx_border_t\">18.239</span></span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.8\" class=\"ltx_tr\">\n<span id=\"S5.T3.4.1.1.1.1.1.1.8.1\" class=\"ltx_td ltx_align_center ltx_border_b\">0.6</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.8.2\" class=\"ltx_td ltx_align_center ltx_border_b\">14.044</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.8.3\" class=\"ltx_td ltx_align_center ltx_border_b\">15.689</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.8.4\" class=\"ltx_td ltx_align_center ltx_border_b\">15.298</span>\n<span id=\"S5.T3.4.1.1.1.1.1.1.8.5\" class=\"ltx_td ltx_align_center ltx_border_b\">17.202</span></span>\n</span></span></span>\n</span></span></span></p>\n</span></div>\n\n",
        "footnotes": "\n\n\n\n\nDataset\n\n\n\nSLSIA\n\n(CNN)\n\naccuracy\n\nthe source of random subjects\n\n\n\n\nrandom pre-trained\n\nmodels\n\n\n\n\ntarget pre-trained\n\nmodels\n\n\n\n\nrandom local\n\nmodels\n\n\n\n\ntarget local\n\nmodels\n\n\nFEMNIST\n0.8\n5.413\n5.472\n5.383\n5.340\n\n0.1\n5.018\n5.065\n4.780\n4.738\n\nShakespeare\n0.9\n71881.506\n72077.575\n71604.278\n71579.257\n\n0.4\n69823.702\n70085.036\n69905.794\n69608.564\n\nSynthetic\n1.0\n14.642\n16.464\n15.921\n18.239\n\n0.6\n14.044\n15.689\n15.298\n17.202\n\n",
        "references": [
            "From Figure 2, we observe that the accuracy of the attack varies from close to 0 to 1. To further explore the reason behind accuracy variance, we measure the average input feature distance between the target and other random subjects in the training data of pre-trained and local models. Table 3 exposes the average input feature distance of the target subject with a high accuracy and the target subject with a low accuracy under each dataset. As expected, the table shows that the target subject with high attack accuracy has input features relatively far from other random subjects, unlike the target subject with relatively low accuracy, which has relatively compact input features among target and random subjects.\nThe average input feature distance is the average pair-wise Euclidean distance between two subjects’ flattened data input features. For example, the target subject with an accuracy of 0.8 in FEMNIST has average input feature distances of 5.413, 5.472, 5.383, and 5.340, which are about 8% higher than the target subject with an accuracy of 0.1 with distances of 5.018, 5.065, 4.780, and 4.738. Other datasets have a similar phenomenon with different input feature distance gap levels. We note that Shakespeare’s average input feature distance is enormous as we use the index of words in the dictionary as the input feature rather than the embedding of each word after the embedding layer. It will not change the comparison of high and low-accuracy target subjects, as the embedding for the same word is the same. From the above analysis, we conclude that the target subjects, separate from other random subjects, are more vulnerable to our SLSIA. By comparing the average actual (rather than the dictionary index in Shakespeare) input feature distance between the FEMNIST and Synthetic datasets and the attack performance in Table 2, we can also observe the dataset with a relatively higher average input feature distance will have a relatively higher attack accuracy under our SLSIA. In other words, the dataset with subjects slightly far from each other is more vulnerable. If the subject is relatively far from other subjects, the models could learn separate embeddings for its data points, which makes the binary attack model easier to learn and transfer from the pre-trained models to the local models."
        ]
    },
    "S5.T4": {
        "caption": "Table 4: The averaged accuracy of 10 runs (subjects) with various rates of data from the target subject in the target local client.",
        "table": "<div id=\"S5.T4.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:208.1pt;height:149.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-60.6pt,43.6pt) scale(0.63204,0.63204) ;\">\n<p id=\"S5.T4.4.1\" class=\"ltx_p\"><span id=\"S5.T4.4.1.1\" class=\"ltx_text\">\n<span id=\"S5.T4.4.1.1.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:329.3pt;height:236.7pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span id=\"S5.T4.4.1.1.1.1\" class=\"ltx_p\"><span id=\"S5.T4.4.1.1.1.1.1\" class=\"ltx_text\">\n<span id=\"S5.T4.4.1.1.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_3\"><span id=\"S5.T4.4.1.1.1.1.1.1.1.1.1\" class=\"ltx_text\">Methods</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_9\">the rate of data from target subject in the target local client</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_3\">50%</span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_3\">30%</span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_3\">10%</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.3\" class=\"ltx_tr\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.1.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:46.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:46.4pt;transform:translate(-19.78pt,-19.78pt) rotate(-90deg) ;\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.1.1.1\" class=\"ltx_p\">FEMNIST</span>\n</span></span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.2.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.9pt;height:53.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:53.1pt;transform:translate(-22.13pt,-21.15pt) rotate(-90deg) ;\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.2.1.1\" class=\"ltx_p\">Shakespeare</span>\n</span></span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.3.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.9pt;height:41.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:41.1pt;transform:translate(-16.11pt,-15.14pt) rotate(-90deg) ;\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.3.1.1\" class=\"ltx_p\">Synthetic</span>\n</span></span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.4.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:46.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:46.4pt;transform:translate(-19.78pt,-19.78pt) rotate(-90deg) ;\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.4.1.1\" class=\"ltx_p\">FEMNIST</span>\n</span></span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.5.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.9pt;height:53.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:53.1pt;transform:translate(-22.13pt,-21.15pt) rotate(-90deg) ;\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.5.1.1\" class=\"ltx_p\">Shakespeare</span>\n</span></span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.6.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.9pt;height:41.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:41.1pt;transform:translate(-16.11pt,-15.14pt) rotate(-90deg) ;\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.6.1.1\" class=\"ltx_p\">Synthetic</span>\n</span></span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.7.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:46.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:46.4pt;transform:translate(-19.78pt,-19.78pt) rotate(-90deg) ;\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.7.1.1\" class=\"ltx_p\">FEMNIST</span>\n</span></span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.8.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.9pt;height:53.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:53.1pt;transform:translate(-22.13pt,-21.15pt) rotate(-90deg) ;\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.8.1.1\" class=\"ltx_p\">Shakespeare</span>\n</span></span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.9\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.9.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.9pt;height:41.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:41.1pt;transform:translate(-16.11pt,-15.14pt) rotate(-90deg) ;\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.3.9.1.1\" class=\"ltx_p\">Synthetic</span>\n</span></span></span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.4\" class=\"ltx_tr\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Avg Loss</span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.30</span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.40</span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.54</span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.42</span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.52</span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\">0.56</span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.4.8\" class=\"ltx_td ltx_align_center ltx_border_t\">0.42</span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.4.9\" class=\"ltx_td ltx_align_center ltx_border_t\">0.66</span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.4.10\" class=\"ltx_td ltx_align_center ltx_border_t\">0.63</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.5\" class=\"ltx_tr\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.5.1\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.5.1.1\" class=\"ltx_text\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.5.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.5.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.5.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Min Loss</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.5.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.5.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Time</span></span>\n</span></span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.5.2.1\" class=\"ltx_text\">0.36</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.5.3.1\" class=\"ltx_text\">0.30</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.5.4.1\" class=\"ltx_text\">0.40</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.5.5\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.5.5.1\" class=\"ltx_text\">0.44</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.5.6\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.5.6.1\" class=\"ltx_text\">0.46</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.5.7\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.5.7.1\" class=\"ltx_text\">0.36</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.5.8\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.5.8.1\" class=\"ltx_text\">0.42</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.5.9\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.5.9.1\" class=\"ltx_text\">0.58</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.5.10\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.5.10.1\" class=\"ltx_text\">0.30</span></span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.6\" class=\"ltx_tr\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.6.1\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.6.1.1\" class=\"ltx_text\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.6.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.6.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.6.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">SLSIA</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.6.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.6.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(CNN)</span></span>\n</span></span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.6.2.1\" class=\"ltx_text\">0.67</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.6.3.1\" class=\"ltx_text\">0.68</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.6.4.1\" class=\"ltx_text\">0.91</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.6.5.1\" class=\"ltx_text\">0.62</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.6.6.1\" class=\"ltx_text\">0.61</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.6.7\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.6.7.1\" class=\"ltx_text\">0.88</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.6.8\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.6.8.1\" class=\"ltx_text\">0.48</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.6.9\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.6.9.1\" class=\"ltx_text\">0.54</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.6.10\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.4.1.1.1.1.1.1.6.10.1\" class=\"ltx_text\">0.93</span></span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.7\" class=\"ltx_tr\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.7.1\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T4.4.1.1.1.1.1.1.7.1.1\" class=\"ltx_text\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.7.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.7.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.7.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">SLSIA</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.7.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T4.4.1.1.1.1.1.1.7.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(SVM)</span></span>\n</span></span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.7.2\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T4.4.1.1.1.1.1.1.7.2.1\" class=\"ltx_text\">0.62</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.7.3\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T4.4.1.1.1.1.1.1.7.3.1\" class=\"ltx_text\">0.70</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.7.4\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T4.4.1.1.1.1.1.1.7.4.1\" class=\"ltx_text\">0.88</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.7.5\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T4.4.1.1.1.1.1.1.7.5.1\" class=\"ltx_text\">0.52</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.7.6\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T4.4.1.1.1.1.1.1.7.6.1\" class=\"ltx_text\">0.62</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.7.7\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T4.4.1.1.1.1.1.1.7.7.1\" class=\"ltx_text\">0.87</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.7.8\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T4.4.1.1.1.1.1.1.7.8.1\" class=\"ltx_text\">0.53</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.7.9\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T4.4.1.1.1.1.1.1.7.9.1\" class=\"ltx_text\">0.53</span></span>\n<span id=\"S5.T4.4.1.1.1.1.1.1.7.10\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T4.4.1.1.1.1.1.1.7.10.1\" class=\"ltx_text\">0.97</span></span></span>\n</span></span></span>\n</span></span></span></p>\n</span></div>\n\n",
        "footnotes": "\n\n\n\n\nMethods\nthe rate of data from target subject in the target local client\n\n50%\n30%\n10%\n\n\n\nFEMNIST\n\n\n\nShakespeare\n\n\n\nSynthetic\n\n\n\nFEMNIST\n\n\n\nShakespeare\n\n\n\nSynthetic\n\n\n\nFEMNIST\n\n\n\nShakespeare\n\n\n\nSynthetic\n\n\nAvg Loss\n0.30\n0.40\n0.54\n0.42\n0.52\n0.56\n0.42\n0.66\n0.63\n\n\n\n\nMin Loss\n\nTime\n\n0.36\n0.30\n0.40\n0.44\n0.46\n0.36\n0.42\n0.58\n0.30\n\n\n\n\nSLSIA\n\n(CNN)\n\n0.67\n0.68\n0.91\n0.62\n0.61\n0.88\n0.48\n0.54\n0.93\n\n\n\n\nSLSIA\n\n(SVM)\n\n0.62\n0.70\n0.88\n0.52\n0.62\n0.87\n0.53\n0.53\n0.97\n\n",
        "references": [
            "(1) Changing the percentage of the target subject’s data points in the local client. In the default setting, we assign a dataset to each target local client, with half of the data points from the target subject and the other half from a random subject.\nTherefore, we change this rate from 50% to 30% and 10% by increasing the number of random subjects and data points from random subjects in each local client and keeping the number of data points from the target subject unchanged. Table 4 shows the average accuracy under various rates. We find that Avg Loss with three datasets and Min Loss Time with FEMNIST and Shakespeare will increase their attack performance while decreasing the rate of the data from the target subject. For example, Avg Loss obtains an accuracy of 0.54, 0.56, and 0.63 under the rate of 50%, 30%, and 10% separately with Synthetic. As the rate of data points from the target subject decreases, the total number of data points in the training data of the local client increases with the addition of data points from random subjects. More training data points could improve the generalizability of the previous low-performance local model. We guess that more data leads the local model to learn the target subject better. Hence, the evaluation data points from the target subject (Dstesuperscriptsubscript𝐷subscript𝑠𝑡𝑒D_{s_{t}}^{e}) obtain a lower loss on the local clients trained with the data from the target subject, which makes Avg Loss and Min Loss Time a higher attack accuracy. Our SLSIA drops within FEMNIST and Shakespeare while decreasing the rate. However, our SLSIA is still higher than the two previous methods, apart from the Shakespeare under a rate of 10% (10% lower in average accuracy). For Synthetic, our SLSIA achieves accuracy of 0.93 and 0.97 even under a rate of 10%. It indicates that if the subjects are distinguishable enough, our SLSIA could have an average attack accuracy higher than 0.9 with a low rate of data points from the target subject. We attribute the average accuracy drop of our SLSIS with FEMNIST and Shakespeare to the fact that subjects are not distinguishable enough in those two datasets."
        ]
    },
    "S5.T5": {
        "caption": "Table 5: The attack accuracy of one run (subject) with different layers under each type of model.",
        "table": "<div id=\"S5.T5.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:381.6pt;height:83.2pt;vertical-align:-0.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-83.9pt,18.1pt) scale(0.69464,0.69464) ;\">\n<p id=\"S5.T5.4.1\" class=\"ltx_p\"><span id=\"S5.T5.4.1.1\" class=\"ltx_text\">\n<span id=\"S5.T5.4.1.1.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:549.3pt;height:119.8pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span id=\"S5.T5.4.1.1.1.1\" class=\"ltx_p\"><span id=\"S5.T5.4.1.1.1.1.1\" class=\"ltx_text\">\n<span id=\"S5.T5.4.1.1.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_2\"><span id=\"S5.T5.4.1.1.1.1.1.1.1.1.1\" class=\"ltx_text\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">SLSIA</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.1.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.1.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">methods</span></span>\n</span></span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_6\">CNN (FEMNIST) layer index</span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_3\">LSTM (Shakespeare) layer index</span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_3\">MLP (Synthetic) layer index</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">layer 0</span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">layer 1</span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">layer 2</span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">layer 3</span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">layer 4</span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">layer 5</span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">layer 0</span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\">layer 1</span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.2.9\" class=\"ltx_td ltx_align_center ltx_border_t\">layer 2</span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.2.10\" class=\"ltx_td ltx_align_center ltx_border_t\">layer 0</span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.2.11\" class=\"ltx_td ltx_align_center ltx_border_t\">layer 1</span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.2.12\" class=\"ltx_td ltx_align_center ltx_border_t\">layer 2</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.3\" class=\"ltx_tr\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.4.1.1.1.1.1.1.3.1.1\" class=\"ltx_text\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">SLSIA</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(CNN)</span></span>\n</span></span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.4.1.1.1.1.1.1.3.2.1\" class=\"ltx_text\">0.8</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.4.1.1.1.1.1.1.3.3.1\" class=\"ltx_text\">0.8</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.4.1.1.1.1.1.1.3.4.1\" class=\"ltx_text\">0.6</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.4.1.1.1.1.1.1.3.5.1\" class=\"ltx_text\">0.5</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.4.1.1.1.1.1.1.3.6.1\" class=\"ltx_text\">0.6</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.4.1.1.1.1.1.1.3.7.1\" class=\"ltx_text\">0.5</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.4.1.1.1.1.1.1.3.8.1\" class=\"ltx_text\">0.8</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.4.1.1.1.1.1.1.3.9.1\" class=\"ltx_text\">0.8</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.10\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.4.1.1.1.1.1.1.3.10.1\" class=\"ltx_text\">0.5</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.11\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.4.1.1.1.1.1.1.3.11.1\" class=\"ltx_text\">1.0</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.12\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.4.1.1.1.1.1.1.3.12.1\" class=\"ltx_text\">0.1</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.3.13\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.4.1.1.1.1.1.1.3.13.1\" class=\"ltx_text\">0.2</span></span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.4\" class=\"ltx_tr\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T5.4.1.1.1.1.1.1.4.1.1\" class=\"ltx_text\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">SLSIA</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(SVM)</span></span>\n</span></span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.2\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T5.4.1.1.1.1.1.1.4.2.1\" class=\"ltx_text\">0.5</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.3\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T5.4.1.1.1.1.1.1.4.3.1\" class=\"ltx_text\">0.8</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.4\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T5.4.1.1.1.1.1.1.4.4.1\" class=\"ltx_text\">0.7</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.5\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T5.4.1.1.1.1.1.1.4.5.1\" class=\"ltx_text\">0.8</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.6\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T5.4.1.1.1.1.1.1.4.6.1\" class=\"ltx_text\">0.8</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.7\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T5.4.1.1.1.1.1.1.4.7.1\" class=\"ltx_text\">0.8</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.8\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T5.4.1.1.1.1.1.1.4.8.1\" class=\"ltx_text\">0.8</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.9\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T5.4.1.1.1.1.1.1.4.9.1\" class=\"ltx_text\">0.8</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.10\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T5.4.1.1.1.1.1.1.4.10.1\" class=\"ltx_text\">0.6</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.11\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T5.4.1.1.1.1.1.1.4.11.1\" class=\"ltx_text\">1.0</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.12\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T5.4.1.1.1.1.1.1.4.12.1\" class=\"ltx_text\">0.1</span></span>\n<span id=\"S5.T5.4.1.1.1.1.1.1.4.13\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T5.4.1.1.1.1.1.1.4.13.1\" class=\"ltx_text\">0.1</span></span></span>\n</span></span></span>\n</span></span></span></p>\n</span></div>\n\n",
        "footnotes": "\n\n\n\n\n\n\n\nSLSIA\n\nmethods\n\nCNN (FEMNIST) layer index\nLSTM (Shakespeare) layer index\nMLP (Synthetic) layer index\n\nlayer 0\nlayer 1\nlayer 2\nlayer 3\nlayer 4\nlayer 5\nlayer 0\nlayer 1\nlayer 2\nlayer 0\nlayer 1\nlayer 2\n\n\n\n\nSLSIA\n\n(CNN)\n\n0.8\n0.8\n0.6\n0.5\n0.6\n0.5\n0.8\n0.8\n0.5\n1.0\n0.1\n0.2\n\n\n\n\nSLSIA\n\n(SVM)\n\n0.5\n0.8\n0.7\n0.8\n0.8\n0.8\n0.8\n0.8\n0.6\n1.0\n0.1\n0.1\n\n",
        "references": [
            "(2)  Different layers of the target (pre-trained or local) model as embeddings.\nThe embeddings in input to our attack model can be generated from different layers of target models. In this study, we explore the effectiveness of our attack while generating the embeddings from different layers.\nBy default, we extract the embeddings from layer 1 in CNN (the flattened output of the second convolutional layer), layer 0 in LSTM (the last hidden state of the LSTM layer), and layer 0 in MLP (the output of the first linear layer). From Table 5, we can see how the attack accuracy of three target subjects from three datasets with different layers changes. As we can see, our selected layers have relatively higher performance (0.8, 0.8, and 1.0 with CNN-based SLSIA) than the others. This indicates that the early layers of the model, rather than the last two layers (logits and confidence scores) of each model in the table, will expose more information about the data from the target subject. This corresponds to the finding that the early layer will learn easy but distinguishable features while the higher layer will obtain abstract features [24, 41].The early layer is beneficial to our SLSIA."
        ]
    },
    "S5.T6": {
        "caption": "Table 6: The averaged accuracy of 10 runs (subjects) with various local training epochs.",
        "table": "<div id=\"S5.T6.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:208.1pt;height:149.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-60.6pt,43.6pt) scale(0.63204,0.63204) ;\">\n<p id=\"S5.T6.4.1\" class=\"ltx_p\"><span id=\"S5.T6.4.1.1\" class=\"ltx_text\">\n<span id=\"S5.T6.4.1.1.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:329.3pt;height:236.7pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span id=\"S5.T6.4.1.1.1.1\" class=\"ltx_p\"><span id=\"S5.T6.4.1.1.1.1.1\" class=\"ltx_text\">\n<span id=\"S5.T6.4.1.1.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_3\"><span id=\"S5.T6.4.1.1.1.1.1.1.1.1.1\" class=\"ltx_text\">Methods</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_9\">local training epochs (pretraining epochs)</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_3\">5</span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_3\">10</span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_3\">15</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.3\" class=\"ltx_tr\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.1.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:46.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:46.4pt;transform:translate(-19.78pt,-19.78pt) rotate(-90deg) ;\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.1.1.1\" class=\"ltx_p\">FEMNIST</span>\n</span></span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.2.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.9pt;height:53.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:53.1pt;transform:translate(-22.13pt,-21.15pt) rotate(-90deg) ;\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.2.1.1\" class=\"ltx_p\">Shakespeare</span>\n</span></span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.3.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.9pt;height:41.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:41.1pt;transform:translate(-16.11pt,-15.14pt) rotate(-90deg) ;\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.3.1.1\" class=\"ltx_p\">Synthetic</span>\n</span></span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.4.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:46.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:46.4pt;transform:translate(-19.78pt,-19.78pt) rotate(-90deg) ;\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.4.1.1\" class=\"ltx_p\">FEMNIST</span>\n</span></span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.5.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.9pt;height:53.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:53.1pt;transform:translate(-22.13pt,-21.15pt) rotate(-90deg) ;\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.5.1.1\" class=\"ltx_p\">Shakespeare</span>\n</span></span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.6.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.9pt;height:41.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:41.1pt;transform:translate(-16.11pt,-15.14pt) rotate(-90deg) ;\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.6.1.1\" class=\"ltx_p\">Synthetic</span>\n</span></span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.7.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:46.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:46.4pt;transform:translate(-19.78pt,-19.78pt) rotate(-90deg) ;\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.7.1.1\" class=\"ltx_p\">FEMNIST</span>\n</span></span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.8.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.9pt;height:53.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:53.1pt;transform:translate(-22.13pt,-21.15pt) rotate(-90deg) ;\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.8.1.1\" class=\"ltx_p\">Shakespeare</span>\n</span></span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.9\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.9.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.9pt;height:41.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:41.1pt;transform:translate(-16.11pt,-15.14pt) rotate(-90deg) ;\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.3.9.1.1\" class=\"ltx_p\">Synthetic</span>\n</span></span></span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.4\" class=\"ltx_tr\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Avg Loss</span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.30</span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.40</span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.54</span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.26</span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.54</span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\">0.54</span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.4.8\" class=\"ltx_td ltx_align_center ltx_border_t\">0.40</span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.4.9\" class=\"ltx_td ltx_align_center ltx_border_t\">0.78</span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.4.10\" class=\"ltx_td ltx_align_center ltx_border_t\">0.61</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.5\" class=\"ltx_tr\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.5.1\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.5.1.1\" class=\"ltx_text\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.5.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.5.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.5.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Min Loss</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.5.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.5.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Time</span></span>\n</span></span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.5.2.1\" class=\"ltx_text\">0.36</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.5.3.1\" class=\"ltx_text\">0.30</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.5.4.1\" class=\"ltx_text\">0.40</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.5.5\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.5.5.1\" class=\"ltx_text\">0.36</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.5.6\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.5.6.1\" class=\"ltx_text\">0.42</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.5.7\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.5.7.1\" class=\"ltx_text\">0.30</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.5.8\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.5.8.1\" class=\"ltx_text\">0.50</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.5.9\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.5.9.1\" class=\"ltx_text\">0.42</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.5.10\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.5.10.1\" class=\"ltx_text\">0.24</span></span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.6\" class=\"ltx_tr\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.6.1\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.6.1.1\" class=\"ltx_text\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.6.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.6.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.6.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">SLSIA</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.6.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.6.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(CNN)</span></span>\n</span></span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.6.2.1\" class=\"ltx_text\">0.67</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.6.3.1\" class=\"ltx_text\">0.68</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.6.4.1\" class=\"ltx_text\">0.91</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.6.5.1\" class=\"ltx_text\">0.52</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.6.6.1\" class=\"ltx_text\">0.61</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.6.7\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.6.7.1\" class=\"ltx_text\">0.96</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.6.8\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.6.8.1\" class=\"ltx_text\">0.53</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.6.9\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.6.9.1\" class=\"ltx_text\">0.60</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.6.10\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.4.1.1.1.1.1.1.6.10.1\" class=\"ltx_text\">0.95</span></span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.7\" class=\"ltx_tr\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.7.1\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T6.4.1.1.1.1.1.1.7.1.1\" class=\"ltx_text\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.7.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.7.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.7.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">SLSIA</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.7.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T6.4.1.1.1.1.1.1.7.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(SVM)</span></span>\n</span></span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.7.2\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T6.4.1.1.1.1.1.1.7.2.1\" class=\"ltx_text\">0.62</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.7.3\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T6.4.1.1.1.1.1.1.7.3.1\" class=\"ltx_text\">0.70</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.7.4\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T6.4.1.1.1.1.1.1.7.4.1\" class=\"ltx_text\">0.88</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.7.5\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T6.4.1.1.1.1.1.1.7.5.1\" class=\"ltx_text\">0.57</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.7.6\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T6.4.1.1.1.1.1.1.7.6.1\" class=\"ltx_text\">0.59</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.7.7\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T6.4.1.1.1.1.1.1.7.7.1\" class=\"ltx_text\">0.98</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.7.8\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T6.4.1.1.1.1.1.1.7.8.1\" class=\"ltx_text\">0.52</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.7.9\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T6.4.1.1.1.1.1.1.7.9.1\" class=\"ltx_text\">0.67</span></span>\n<span id=\"S5.T6.4.1.1.1.1.1.1.7.10\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T6.4.1.1.1.1.1.1.7.10.1\" class=\"ltx_text\">0.95</span></span></span>\n</span></span></span>\n</span></span></span></p>\n</span></div>\n\n",
        "footnotes": "\n\n\n\n\nMethods\nlocal training epochs (pretraining epochs)\n\n5\n10\n15\n\n\n\nFEMNIST\n\n\n\nShakespeare\n\n\n\nSynthetic\n\n\n\nFEMNIST\n\n\n\nShakespeare\n\n\n\nSynthetic\n\n\n\nFEMNIST\n\n\n\nShakespeare\n\n\n\nSynthetic\n\n\nAvg Loss\n0.30\n0.40\n0.54\n0.26\n0.54\n0.54\n0.40\n0.78\n0.61\n\n\n\n\nMin Loss\n\nTime\n\n0.36\n0.30\n0.40\n0.36\n0.42\n0.30\n0.50\n0.42\n0.24\n\n\n\n\nSLSIA\n\n(CNN)\n\n0.67\n0.68\n0.91\n0.52\n0.61\n0.96\n0.53\n0.60\n0.95\n\n\n\n\nSLSIA\n\n(SVM)\n\n0.62\n0.70\n0.88\n0.57\n0.59\n0.98\n0.52\n0.67\n0.95\n\n",
        "references": [
            "(3) The local training epochs in pertaining and actual FL. As we mentioned, the low performance of the Avg Loss and Min Loss Time methods might be related to the inefficient learning of the model at the first FL round in Section 5.2. Therefore, we increase the local epoch of the first round to observe the performance alteration in Table 6. We find a similar rule as the exploration of decreasing the rate of data points from the target subject in local clients. Avg Loss and Min Loss Time mainly increase their performance with more local training epochs apart from the Synthetic under Min Loss Time. For example, Avg Loss obtains an accuracy of 0.40, 0.54, and 0.78 under the local training epochs of 5, 10, and 15 separately within Synthetic. It indicates more local training epochs at the first FL round increase the performance of the two previous attacking methods, which proves our guess that better learning of the local models will increase their attack performance. Instead of improving the generalizability of the local model via more data points, increasing the local training epoch is also a way to make the local model learn better about the target subject, which reduces the loss of the data points from the target subject and increases the attack performance of Avg Loss and Min Loss Time as they rely on the low loss of the local client on data points from the target subject. Even though our SLSIA drops with more local training epochs under the FEMNIST and Shakespeare, our SLSIA is still higher than the previous two methods. In Synthetic, our SLSIA obtains an accuracy of over 0.9 under different local training epochs, which also indicates the effectiveness of our SLSIA."
        ]
    },
    "S5.T7": {
        "caption": "Table 7: The average privacy budget, average accuracy change, and average attack performance alteration of 10 runs with subject-level differential privacy.",
        "table": "<div id=\"S5.T7.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:294.9pt;height:68.9pt;vertical-align:-0.6pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-85.9pt,19.9pt) scale(0.63195,0.63195) ;\">\n<p id=\"S5.T7.4.4\" class=\"ltx_p\"><span id=\"S5.T7.4.4.4\" class=\"ltx_text\">\n<span id=\"S5.T7.4.4.4.4\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:466.6pt;height:109pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span id=\"S5.T7.4.4.4.4.4\" class=\"ltx_p\"><span id=\"S5.T7.4.4.4.4.4.4\" class=\"ltx_text\">\n<span id=\"S5.T7.4.4.4.4.4.4.4\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T7.3.3.3.3.3.3.3.3\" class=\"ltx_tr\">\n<span id=\"S5.T7.3.3.3.3.3.3.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_3\"><span id=\"S5.T7.3.3.3.3.3.3.3.3.4.1\" class=\"ltx_text\">Dataset</span></span>\n<span id=\"S5.T7.3.3.3.3.3.3.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_3\">non-private</span>\n<span id=\"S5.T7.3.3.3.3.3.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_4\"><math id=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma=0.5\" display=\"inline\"><semantics id=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1a\"><mrow id=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1.1\" xref=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1.1.2\" xref=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml\">σ</mi><mo id=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1.1.1\" xref=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1.1.3\" xref=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1b\"><apply id=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1.1\"><eq id=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1.1.1\"></eq><ci id=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1.1.2\">𝜎</ci><cn type=\"float\" id=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.1.1.1.1.1.1.1.1.1.m1.1c\">\\sigma=0.5</annotation></semantics></math>, <math id=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"C=1.0\" display=\"inline\"><semantics id=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1a\"><mrow id=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1.1\" xref=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1.1.cmml\"><mi id=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1.1.2\" xref=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1.1.2.cmml\">C</mi><mo id=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1.1.1\" xref=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1.1.1.cmml\">=</mo><mn id=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1.1.3\" xref=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1.1.3.cmml\">1.0</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1b\"><apply id=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1.1.cmml\" xref=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1.1\"><eq id=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1.1.1.cmml\" xref=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1.1.1\"></eq><ci id=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1.1.2.cmml\" xref=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1.1.2\">𝐶</ci><cn type=\"float\" id=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1.1.3.cmml\" xref=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1.1.3\">1.0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.2.2.2.2.2.2.2.2.2.m2.1c\">C=1.0</annotation></semantics></math>, <math id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1\" class=\"ltx_Math\" alttext=\"\\delta=10^{-5}\" display=\"inline\"><semantics id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1a\"><mrow id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.cmml\"><mi id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.2\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.2.cmml\">δ</mi><mo id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.1\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.1.cmml\">=</mo><msup id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.cmml\"><mn id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.2\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.2.cmml\">10</mn><mrow id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.3\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.3.cmml\"><mo id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.3a\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.3.cmml\">−</mo><mn id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.3.2\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.3.2.cmml\">5</mn></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1b\"><apply id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.cmml\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1\"><eq id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.1.cmml\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.1\"></eq><ci id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.2.cmml\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.2\">𝛿</ci><apply id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.cmml\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.1.cmml\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3\">superscript</csymbol><cn type=\"integer\" id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.2.cmml\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.2\">10</cn><apply id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.3.cmml\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.3\"><minus id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.3.1.cmml\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.3\"></minus><cn type=\"integer\" id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.3.2.cmml\" xref=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1.1.3.3.2\">5</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.3.3.3.3.3.3.3.3.3.m3.1c\">\\delta=10^{-5}</annotation></semantics></math></span></span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.4\" class=\"ltx_tr\">\n<span id=\"S5.T7.4.4.4.4.4.4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_2\">Avg task accuracy</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_2\"><span id=\"S5.T7.4.4.4.4.4.4.4.4.3.1\" class=\"ltx_text\">\n<span id=\"S5.T7.4.4.4.4.4.4.4.4.3.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T7.4.4.4.4.4.4.4.4.3.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T7.4.4.4.4.4.4.4.4.3.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Avg SLSIA (CNN)</span></span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.4.3.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T7.4.4.4.4.4.4.4.4.3.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">accuracy</span></span>\n</span></span></span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_2\"><span id=\"S5.T7.4.4.4.4.4.4.4.4.1.1\" class=\"ltx_text\">Avg <math id=\"S5.T7.4.4.4.4.4.4.4.4.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\epsilon\" display=\"inline\"><semantics id=\"S5.T7.4.4.4.4.4.4.4.4.1.1.m1.1a\"><mi id=\"S5.T7.4.4.4.4.4.4.4.4.1.1.m1.1.1\" xref=\"S5.T7.4.4.4.4.4.4.4.4.1.1.m1.1.1.cmml\">ϵ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.4.4.4.4.4.4.4.4.1.1.m1.1b\"><ci id=\"S5.T7.4.4.4.4.4.4.4.4.1.1.m1.1.1.cmml\" xref=\"S5.T7.4.4.4.4.4.4.4.4.1.1.m1.1.1\">italic-ϵ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.4.4.4.4.4.4.4.4.1.1.m1.1c\">\\epsilon</annotation></semantics></math></span></span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_2\">Avg task accuracy</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_2\"><span id=\"S5.T7.4.4.4.4.4.4.4.4.5.1\" class=\"ltx_text\">\n<span id=\"S5.T7.4.4.4.4.4.4.4.4.5.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T7.4.4.4.4.4.4.4.4.5.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T7.4.4.4.4.4.4.4.4.5.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Avg SLSIA (CNN)</span></span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.4.5.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T7.4.4.4.4.4.4.4.4.5.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">accuracy</span></span>\n</span></span></span></span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.5\" class=\"ltx_tr\">\n<span id=\"S5.T7.4.4.4.4.4.4.4.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\">train</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">test</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">train</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">test</span></span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.6\" class=\"ltx_tr\">\n<span id=\"S5.T7.4.4.4.4.4.4.4.6.1\" class=\"ltx_td ltx_align_center ltx_border_t\">FEMNIST</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\">17.6%</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\">8%</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.61</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\">30.6</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.6.6\" class=\"ltx_td ltx_align_center ltx_border_t\">11.6%</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.6.7\" class=\"ltx_td ltx_align_center ltx_border_t\">11.5%</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.6.8\" class=\"ltx_td ltx_align_center ltx_border_t\">0.48</span></span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.7\" class=\"ltx_tr\">\n<span id=\"S5.T7.4.4.4.4.4.4.4.7.1\" class=\"ltx_td ltx_align_center\">Shakespeare</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.7.2\" class=\"ltx_td ltx_align_center\">4.7%</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.7.3\" class=\"ltx_td ltx_align_center\">2.8%</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.7.4\" class=\"ltx_td ltx_align_center\">0.73</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.7.5\" class=\"ltx_td ltx_align_center\">15.0</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.7.6\" class=\"ltx_td ltx_align_center\">0.002%</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.7.7\" class=\"ltx_td ltx_align_center\">0.009%</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.7.8\" class=\"ltx_td ltx_align_center\">0.68</span></span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.8\" class=\"ltx_tr\">\n<span id=\"S5.T7.4.4.4.4.4.4.4.8.1\" class=\"ltx_td ltx_align_center ltx_border_b\">Synthetic</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.8.2\" class=\"ltx_td ltx_align_center ltx_border_b\">74.8%</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.8.3\" class=\"ltx_td ltx_align_center ltx_border_b\">50.0%</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.8.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.89</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.8.5\" class=\"ltx_td ltx_align_center ltx_border_b\">22.0</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.8.6\" class=\"ltx_td ltx_align_center ltx_border_b\">54.9%</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.8.7\" class=\"ltx_td ltx_align_center ltx_border_b\">50.4%</span>\n<span id=\"S5.T7.4.4.4.4.4.4.4.8.8\" class=\"ltx_td ltx_align_center ltx_border_b\">0.53</span></span>\n</span></span></span>\n</span></span></span></p>\n</span></div>\n\n",
        "footnotes": "\n\n\n\n\nDataset\nnon-private\nσ=0.5𝜎0.5\\sigma=0.5, C=1.0𝐶1.0C=1.0, δ=10−5𝛿superscript105\\delta=10^{-5}\n\nAvg task accuracy\n\n\n\nAvg SLSIA (CNN)\n\naccuracy\n\nAvg ϵitalic-ϵ\\epsilon\nAvg task accuracy\n\n\n\nAvg SLSIA (CNN)\n\naccuracy\n\n\ntrain\ntest\ntrain\ntest\n\nFEMNIST\n17.6%\n8%\n0.61\n30.6\n11.6%\n11.5%\n0.48\n\nShakespeare\n4.7%\n2.8%\n0.73\n15.0\n0.002%\n0.009%\n0.68\n\nSynthetic\n74.8%\n50.0%\n0.89\n22.0\n54.9%\n50.4%\n0.53\n\n",
        "references": [
            "Table 7 results from applying subject-level differential privacy, and Table 8 results from applying item-level differential privacy. Subject-level differential privacy closes the training and test accuracy to a random guess. For the Synthetic dataset, the drop of 20% in the training accuracy causes a 36% decrease in the average accuracy of SLSIA (CNN). As expected, the reduction of attack performance is related to a significant accuracy drop in the original task. For the Shakespeare dataset, the attack accuracy is only a 4% drop even though the accuracy of the original task decreases by more than 1000X, indicating the current extent of subject-level differential privacy can not prevent our SLSIA completely. While applying item-level differential privacy with a similar level of privacy budget, the accuracy change of the original task is slight apart from a 20% drop in the training accuracy of the Synthetic dataset, and the attack accuracy of SLSIA (CNN) has no apparent drop with Shakespeare and even slightly increase under FEMNIST. For Synthetic, the attack accuracy has a drop of more than 30% to a random guess. The Synthetic dataset is sensitive to item-level and subject-level differential privacy, while the other two are slightly defended under item-level differential privacy. It indicates that subject-level differential privacy is more potent than item-level differential privacy to defend both SMIA [39] and our SLSIA, while still achieving a higher attack accuracy concerning the compared attacks, without defense, presented in Section 5.2."
        ]
    },
    "S5.T8": {
        "caption": "Table 8: The average privacy budget, average accuracy change, and average attack performance alteration of 10 runs with item-level differential privacy.",
        "table": "<div id=\"S5.T8.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:294.9pt;height:70.3pt;vertical-align:-0.6pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-80.9pt,19.1pt) scale(0.64579,0.64579) ;\">\n<p id=\"S5.T8.4.4\" class=\"ltx_p\"><span id=\"S5.T8.4.4.4\" class=\"ltx_text\">\n<span id=\"S5.T8.4.4.4.4\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:456.6pt;height:109pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span id=\"S5.T8.4.4.4.4.4\" class=\"ltx_p\"><span id=\"S5.T8.4.4.4.4.4.4\" class=\"ltx_text\">\n<span id=\"S5.T8.4.4.4.4.4.4.4\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T8.3.3.3.3.3.3.3.3\" class=\"ltx_tr\">\n<span id=\"S5.T8.3.3.3.3.3.3.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_3\"><span id=\"S5.T8.3.3.3.3.3.3.3.3.4.1\" class=\"ltx_text\">Dataset</span></span>\n<span id=\"S5.T8.3.3.3.3.3.3.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_3\">non-private</span>\n<span id=\"S5.T8.3.3.3.3.3.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_4\"><math id=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma=0.5\" display=\"inline\"><semantics id=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1a\"><mrow id=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1.1\" xref=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1.1.2\" xref=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml\">σ</mi><mo id=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1.1.1\" xref=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1.1.3\" xref=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1b\"><apply id=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1.1\"><eq id=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1.1.1\"></eq><ci id=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1.1.2\">𝜎</ci><cn type=\"float\" id=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.1.1.1.1.1.1.1.1.1.m1.1c\">\\sigma=0.5</annotation></semantics></math>, <math id=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"C=1.0\" display=\"inline\"><semantics id=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1a\"><mrow id=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1.1\" xref=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1.1.cmml\"><mi id=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1.1.2\" xref=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1.1.2.cmml\">C</mi><mo id=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1.1.1\" xref=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1.1.1.cmml\">=</mo><mn id=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1.1.3\" xref=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1.1.3.cmml\">1.0</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1b\"><apply id=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1.1.cmml\" xref=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1.1\"><eq id=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1.1.1.cmml\" xref=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1.1.1\"></eq><ci id=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1.1.2.cmml\" xref=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1.1.2\">𝐶</ci><cn type=\"float\" id=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1.1.3.cmml\" xref=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1.1.3\">1.0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.2.2.2.2.2.2.2.2.2.m2.1c\">C=1.0</annotation></semantics></math>, <math id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1\" class=\"ltx_Math\" alttext=\"\\delta=10^{-5}\" display=\"inline\"><semantics id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1a\"><mrow id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.cmml\"><mi id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.2\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.2.cmml\">δ</mi><mo id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.1\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.1.cmml\">=</mo><msup id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.cmml\"><mn id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.2\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.2.cmml\">10</mn><mrow id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.3\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.3.cmml\"><mo id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.3a\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.3.cmml\">−</mo><mn id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.3.2\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.3.2.cmml\">5</mn></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1b\"><apply id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.cmml\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1\"><eq id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.1.cmml\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.1\"></eq><ci id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.2.cmml\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.2\">𝛿</ci><apply id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.cmml\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.1.cmml\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3\">superscript</csymbol><cn type=\"integer\" id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.2.cmml\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.2\">10</cn><apply id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.3.cmml\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.3\"><minus id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.3.1.cmml\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.3\"></minus><cn type=\"integer\" id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.3.2.cmml\" xref=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1.1.3.3.2\">5</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.3.3.3.3.3.3.3.3.3.m3.1c\">\\delta=10^{-5}</annotation></semantics></math></span></span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.4\" class=\"ltx_tr\">\n<span id=\"S5.T8.4.4.4.4.4.4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_2\">Avg task accuracy</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_2\"><span id=\"S5.T8.4.4.4.4.4.4.4.4.3.1\" class=\"ltx_text\">\n<span id=\"S5.T8.4.4.4.4.4.4.4.4.3.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T8.4.4.4.4.4.4.4.4.3.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T8.4.4.4.4.4.4.4.4.3.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Avg SLSIA (CNN)</span></span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.4.3.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T8.4.4.4.4.4.4.4.4.3.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">accuracy</span></span>\n</span></span></span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_2\"><span id=\"S5.T8.4.4.4.4.4.4.4.4.1.1\" class=\"ltx_text\">Avg <math id=\"S5.T8.4.4.4.4.4.4.4.4.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\epsilon\" display=\"inline\"><semantics id=\"S5.T8.4.4.4.4.4.4.4.4.1.1.m1.1a\"><mi id=\"S5.T8.4.4.4.4.4.4.4.4.1.1.m1.1.1\" xref=\"S5.T8.4.4.4.4.4.4.4.4.1.1.m1.1.1.cmml\">ϵ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.4.4.4.4.4.4.4.4.1.1.m1.1b\"><ci id=\"S5.T8.4.4.4.4.4.4.4.4.1.1.m1.1.1.cmml\" xref=\"S5.T8.4.4.4.4.4.4.4.4.1.1.m1.1.1\">italic-ϵ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.4.4.4.4.4.4.4.4.1.1.m1.1c\">\\epsilon</annotation></semantics></math></span></span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_2\">Avg task accuracy</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_2\"><span id=\"S5.T8.4.4.4.4.4.4.4.4.5.1\" class=\"ltx_text\">\n<span id=\"S5.T8.4.4.4.4.4.4.4.4.5.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T8.4.4.4.4.4.4.4.4.5.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T8.4.4.4.4.4.4.4.4.5.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Avg SLSIA (CNN)</span></span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.4.5.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T8.4.4.4.4.4.4.4.4.5.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">accuracy</span></span>\n</span></span></span></span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.5\" class=\"ltx_tr\">\n<span id=\"S5.T8.4.4.4.4.4.4.4.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\">train</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">test</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">train</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">test</span></span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.6\" class=\"ltx_tr\">\n<span id=\"S5.T8.4.4.4.4.4.4.4.6.1\" class=\"ltx_td ltx_align_center ltx_border_t\">FEMNIST</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\">17.7%</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\">7.1%</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.56</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\">30.7</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.6.6\" class=\"ltx_td ltx_align_center ltx_border_t\">17.0%</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.6.7\" class=\"ltx_td ltx_align_center ltx_border_t\">7.2%</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.6.8\" class=\"ltx_td ltx_align_center ltx_border_t\">0.58</span></span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.7\" class=\"ltx_tr\">\n<span id=\"S5.T8.4.4.4.4.4.4.4.7.1\" class=\"ltx_td ltx_align_center\">Shakespeare</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.7.2\" class=\"ltx_td ltx_align_center\">5.0%</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.7.3\" class=\"ltx_td ltx_align_center\">3.3%</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.7.4\" class=\"ltx_td ltx_align_center\">0.62</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.7.5\" class=\"ltx_td ltx_align_center\">15.4</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.7.6\" class=\"ltx_td ltx_align_center\">2.0%</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.7.7\" class=\"ltx_td ltx_align_center\">1.0%</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.7.8\" class=\"ltx_td ltx_align_center\">0.58</span></span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.8\" class=\"ltx_tr\">\n<span id=\"S5.T8.4.4.4.4.4.4.4.8.1\" class=\"ltx_td ltx_align_center ltx_border_b\">Synthetic</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.8.2\" class=\"ltx_td ltx_align_center ltx_border_b\">76.5%</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.8.3\" class=\"ltx_td ltx_align_center ltx_border_b\">51.4%</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.8.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.88</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.8.5\" class=\"ltx_td ltx_align_center ltx_border_b\">22.0</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.8.6\" class=\"ltx_td ltx_align_center ltx_border_b\">56.5%</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.8.7\" class=\"ltx_td ltx_align_center ltx_border_b\">50.9%</span>\n<span id=\"S5.T8.4.4.4.4.4.4.4.8.8\" class=\"ltx_td ltx_align_center ltx_border_b\">0.54</span></span>\n</span></span></span>\n</span></span></span></p>\n</span></div>\n\n",
        "footnotes": "\n\n\n\n\nDataset\nnon-private\nσ=0.5𝜎0.5\\sigma=0.5, C=1.0𝐶1.0C=1.0, δ=10−5𝛿superscript105\\delta=10^{-5}\n\nAvg task accuracy\n\n\n\nAvg SLSIA (CNN)\n\naccuracy\n\nAvg ϵitalic-ϵ\\epsilon\nAvg task accuracy\n\n\n\nAvg SLSIA (CNN)\n\naccuracy\n\n\ntrain\ntest\ntrain\ntest\n\nFEMNIST\n17.7%\n7.1%\n0.56\n30.7\n17.0%\n7.2%\n0.58\n\nShakespeare\n5.0%\n3.3%\n0.62\n15.4\n2.0%\n1.0%\n0.58\n\nSynthetic\n76.5%\n51.4%\n0.88\n22.0\n56.5%\n50.9%\n0.54\n\n",
        "references": [
            "Table 7 results from applying subject-level differential privacy, and Table 8 results from applying item-level differential privacy. Subject-level differential privacy closes the training and test accuracy to a random guess. For the Synthetic dataset, the drop of 20% in the training accuracy causes a 36% decrease in the average accuracy of SLSIA (CNN). As expected, the reduction of attack performance is related to a significant accuracy drop in the original task. For the Shakespeare dataset, the attack accuracy is only a 4% drop even though the accuracy of the original task decreases by more than 1000X, indicating the current extent of subject-level differential privacy can not prevent our SLSIA completely. While applying item-level differential privacy with a similar level of privacy budget, the accuracy change of the original task is slight apart from a 20% drop in the training accuracy of the Synthetic dataset, and the attack accuracy of SLSIA (CNN) has no apparent drop with Shakespeare and even slightly increase under FEMNIST. For Synthetic, the attack accuracy has a drop of more than 30% to a random guess. The Synthetic dataset is sensitive to item-level and subject-level differential privacy, while the other two are slightly defended under item-level differential privacy. It indicates that subject-level differential privacy is more potent than item-level differential privacy to defend both SMIA [39] and our SLSIA, while still achieving a higher attack accuracy concerning the compared attacks, without defense, presented in Section 5.2."
        ]
    }
}