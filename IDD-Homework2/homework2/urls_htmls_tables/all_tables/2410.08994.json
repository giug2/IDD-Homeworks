{
    "id_table_1": {
        "caption": "Table 1:  Summary of UCI imbalanced datasets : sample sizes, negative:positive ratios, feature numbers.",
        "table": "S7.T1.1.1",
        "footnotes": [],
        "references": [
            "Secondly, we obtain the asymptotic normality of the proposed pseudo MLE (see Theorem  1  and Theorem  2 ). The estimator is unbiased and has generally larger variance than the full-sample estimator (with zero downsampling rate). But for some small values of    \\alpha italic_  the asymptotic variance stays the same as that of the full-sampling estimator, meaning that downsampling with these    \\alpha italic_  results in no efficiency loss at all. We conduct numerical experiments and apply our estimator to logistic regression for different values of   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT . We use the mean squared error metric and compare the pseudo MLE with two commonly used alternative estimators: the inverse-weighting estimator ( 8 ) employed by  [ 50 ] , and the conditional MLE ( 9 ). The findings are that our estimator outperforms both of them when   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  is large, but as   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  decreases, it loses its advantage gradually, which is consistent with our theoretical conclusions.",
            "Lemma  1  in Appendix  B  shows a counterexample to illustrate why this method can lead to biased prediction score for some covariates. Specifically, Lemma  1  constructs a counterexample where   ^ 1 subscript ^  1 \\hat{\\theta}_{1} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  leads to biasedness. And if  h  ( x ) =  ~ 1 T  x h x superscript subscript ~  1 T x h(x)=\\tilde{\\theta}_{1}^{T}x italic_h ( italic_x ) = over~ start_ARG italic_ end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x  maps two distinct  x , x  x superscript x  x,x^{\\prime} italic_x , italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  to the same value while    T  x =   T  x  superscript subscript  T x superscript subscript  T superscript x  \\theta_{*}^{T}x\\neq\\theta_{*}^{T}x^{\\prime} italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x = italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , and the c.d.f.  F  (  ) F  F(\\cdot) italic_F (  )  is strictly increasing, then the true model has different probabilities for  x , x  x superscript x  x,x^{\\prime} italic_x , italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  whereas the model trained on the downsample maps them to the same prediction score, thus both parameter estimator and the induced prediction score model are biased.",
            "Proposition  1  illustrates that the downsample random variables generated from GLM still follow a GLM, where  F  ( z ) = G  ( z )  + ( 1   )  G  ( z ) F z G z  1  G z F(z)=\\frac{G(z)}{\\alpha+(1-\\alpha)G(z)} italic_F ( italic_z ) = divide start_ARG italic_G ( italic_z ) end_ARG start_ARG italic_ + ( 1 - italic_ ) italic_G ( italic_z ) end_ARG , indicating that the true class probabilities are the monotonic transformations of the predicted class probabilities conditional on the downsample. Based upon this, the joint likelihood of the downsample random variables is as follows.",
            "Note that ( 5 ) holds only when   n    subscript  n \\tau_{n}\\rightarrow\\infty italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT   , but the estimator   ^  subscript ^  \\hat{\\theta}_{*} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  may be not consistent when   n <  subscript  n \\tau_{n}<\\infty italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT <  . So in practice the pseudo MLE should perform well for large values of   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , and its benefit could disappear as   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  decays. We verify this claim through numerical experiment on logistic regression models by comparing the performance of   ^  subscript ^  \\hat{\\theta}_{*} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  under different values of   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  in Section  6.1 .",
            "where  G  (  ) G  G(\\cdot) italic_G (  )  is defined as in ( 2 ). By the theory of M-estimators  [ 49 ] , both of the estimators are consistent, i.e.   ^ I   p    superscript ^  I p  subscript  \\hat{\\theta}^{I}\\overset{p}{\\rightarrow}\\theta_{*} over^ start_ARG italic_ end_ARG start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT overitalic_p start_ARG  end_ARG italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  and   ^ C   p    superscript ^  C p  subscript  \\hat{\\theta}^{C}\\overset{p}{\\rightarrow}\\theta_{*} over^ start_ARG italic_ end_ARG start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT overitalic_p start_ARG  end_ARG italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT . We compare the performance of our proposed pseudo MLE   ^  subscript ^  \\hat{\\theta}_{*} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  as ( 6 ) with both   ^ I superscript ^  I \\hat{\\theta}^{I} over^ start_ARG italic_ end_ARG start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT  and   ^ C superscript ^  C \\hat{\\theta}^{C} over^ start_ARG italic_ end_ARG start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT  through numerical experiments on synthetic data (see Section  6.1 ) and empirical data (see Section  7  and Appendix  A ). We find that   ^  subscript ^  \\hat{\\theta}_{*} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  outperforms   ^ I superscript ^  I \\hat{\\theta}^{I} over^ start_ARG italic_ end_ARG start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT  and   ^ C superscript ^  C \\hat{\\theta}^{C} over^ start_ARG italic_ end_ARG start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT  for large values of   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , but it gradually under-performs as we decrease the value   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , which verifies our conjecture as in remark  1 , implying its value under rare-event setup.",
            "Suppose Assumptions  1 ,  2 ,  3  hold, and for some    R d  superscript R d \\Theta\\subset\\mathbb{R}^{d} roman_  blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT  as a neighborhood of    subscript  \\theta_{*} italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT ,  { F  (  n +  1 T  x ) :  1   } conditional-set F subscript  n superscript subscript  1 T x subscript  1  \\{F(\\tau_{n}+\\theta_{1}^{T}x):\\theta_{1}\\in\\Theta\\} { italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) : italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  roman_ }  is differentiable in quadratic mean at    subscript  \\theta_{*} italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT . Assume that  n  ( 1  F  (  n ) )    n 1 F subscript  n n(1-F(\\tau_{n}))\\rightarrow\\infty italic_n ( 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) )   ,  lim n   ( 1   ) 2  ( 1  F  (  n ) )  = c , subscript  n superscript 1  2 1 F subscript  n  c \\lim_{n\\rightarrow\\infty}\\frac{(1-\\alpha)^{2}(1-F(\\tau_{n}))}{\\alpha}=c, roman_lim start_POSTSUBSCRIPT italic_n   end_POSTSUBSCRIPT divide start_ARG ( 1 - italic_ ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ) end_ARG start_ARG italic_ end_ARG = italic_c ,  and",
            "Theorem  1  suggests that if",
            "We demonstrate the necessity of condition ( 10 ) through a numerical illustration.",
            "Imagine that when    \\alpha italic_  is too close to zero, the right hand side of ( 10 ) can blow up so the condition will be violated. We look at a case of logistic regression where   n = 10 subscript  n 10 \\tau_{n}=10 italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = 10 ,    = 0.5 subscript  0.5 \\theta_{*}=0.5 italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT = 0.5 , and  F  (  n +   T  x ) = e  n +   T  x 1 + e  n +   T  x F subscript  n superscript subscript  T x superscript e subscript  n superscript subscript  T x 1 superscript e subscript  n superscript subscript  T x F(\\tau_{n}+\\theta_{*}^{T}x)=\\frac{e^{\\tau_{n}+\\theta_{*}^{T}x}}{1+e^{\\tau_{n}+% \\theta_{*}^{T}x}} italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) = divide start_ARG italic_e start_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x end_POSTSUPERSCRIPT end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x end_POSTSUPERSCRIPT end_ARG  and  X  Unif  [ 0 , 1 ] similar-to X Unif 0 1 X\\sim\\mathrm{Unif}[0,1] italic_X  roman_Unif [ 0 , 1 ] . We generate  10 5 superscript 10 5 10^{5} 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT  random samples and  P  ( Y = 1 )  5.89  10  5 P Y 1 5.89 superscript 10 5 \\mathbb{P}(Y=1)\\approx 5.89\\times 10^{-5} blackboard_P ( italic_Y = 1 )  5.89  10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT . The estimation error is computed as the average value of  |     ^  | subscript  subscript ^  |\\theta_{*}-\\hat{\\theta}_{*}| | italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT - over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT  end_POSTSUBSCRIPT |  from  500 500 500 500  random experiments. The  x x x italic_x -axis corresponds to the    \\alpha italic_  as the downsample rate from negative samples. Figure  1  shows that when    \\alpha italic_  is too close to  0 0 , the estimation error is large. But when    \\alpha italic_  falls in a proper range such that condition ( 10 ) holds, the estimation error for   ^  subscript ^  \\hat{\\theta}_{*} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  stabilizes and is kept as a small value, consistent with the theoretical findings that the mean squared error should be small and  E  [   ^      2 ]  tr  [ V  1 ] / ( n  ( 1  F  (  n ) ) ) E delimited-[] superscript norm subscript ^  subscript  2 tr delimited-[] superscript V 1 n 1 F subscript  n \\mathbb{E}[\\|\\hat{\\theta}_{*}-\\theta_{*}\\|^{2}]\\approx\\mathrm{tr}[\\mathbf{V}^{% -1}]/(n(1-F(\\tau_{n}))) blackboard_E [  over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT  end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]  roman_tr [ bold_V start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ] / ( italic_n ( 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ) ) .",
            "Theorem  1  and Theorem  2  indicate that full-sampling (i.e.   = 1  1 \\alpha=1 italic_ = 1 ) and downsampling with a rate    \\alpha italic_  such that    1 much-less-than  1 \\alpha\\ll 1 italic_  1  and  1  F  (  n )  = o  ( 1 ) 1 F subscript  n  o 1 \\frac{1-F(\\tau_{n})}{\\alpha}=\\mathrm{o}(1) divide start_ARG 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) end_ARG start_ARG italic_ end_ARG = roman_o ( 1 )  lead to the same rate of convergence and asymptotic MSE. At the same time, if we choose    P  ( Y = 1 )  P Y 1 \\alpha\\approx\\mathbb{P}(Y=1) italic_  blackboard_P ( italic_Y = 1 ) , we still have the same convergence rate while the trace norm of the asymptotic covariance is kept as  O  ( 1 ) O 1 \\mathrm{O}(1) roman_O ( 1 ) . Intuitively, downsampling with a much smaller    \\alpha italic_  can help to reduce computational cost significantly while maintaining statistical efficiency under the current rare event setup. This motivates us to formulate the choice of optimal downsampling rate by considering this tradeoff between statistical efficiency and computational cost with a budget constraint.",
            "Recall from Theorem  1  that under regular conditions, when   n    subscript  n \\tau_{n}\\rightarrow\\infty italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT   , we have  n  ( 1  F  (  n ) )  (  ^     )   d  N  ( 0 , V  1 ) . n 1 F subscript  n subscript ^  subscript  d  N 0 superscript V 1 \\sqrt{n(1-F(\\tau_{n}))}(\\hat{\\theta}_{*}-\\theta_{*})\\overset{d}{\\rightarrow}% \\mathcal{N}\\left(\\mathbf{0},\\mathbf{V}^{-1}\\right). square-root start_ARG italic_n ( 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ) end_ARG ( over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT  end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT ) overitalic_d start_ARG  end_ARG caligraphic_N ( bold_0 , bold_V start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) .  So  n  ( 1  F  (  n ) )  tr  [ Cov  (  ^     ) ] tr  ( V  1 )  1 as  n   . formulae-sequence  n 1 F subscript  n tr delimited-[] Cov subscript ^  subscript  tr superscript V 1 1  as n \\frac{n(1-F(\\tau_{n}))\\mathrm{tr}\\left[\\mathrm{Cov}(\\hat{\\theta}_{*}-\\theta_{*% })\\right]}{\\mathrm{tr}\\left(\\mathbf{V}^{-1}\\right)}\\rightarrow 1\\ \\ \\mbox{as}% \\ n\\rightarrow\\infty. divide start_ARG italic_n ( 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ) roman_tr [ roman_Cov ( over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT  end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT ) ] end_ARG start_ARG roman_tr ( bold_V start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) end_ARG  1 as italic_n   .  Under the definition of the efficiency cost ( 12 ), we have  c 0  ( p 1 +   ( 1  p 1 ) )  d 2 tr  ( ( 1  F  (  n ) )  V )    (  ; p 1 )  c 0  ( p 1 +   ( 1  p 1 ) )    d 2 tr  ( ( 1  F  (  n ) )  V ) , subscript c 0 subscript p 1  1 subscript p 1 superscript d 2 tr 1 F subscript  n V   subscript p 1 subscript c 0 subscript p 1  1 subscript p 1  superscript d 2 tr 1 F subscript  n V \\frac{c_{0}(p_{1}+\\alpha(1-p_{1}))d^{2}}{\\mathrm{tr}\\left((1-F(\\tau_{n}))% \\mathbf{V}\\right)}\\leq\\nu(\\alpha;p_{1})\\leq\\frac{c_{0}(p_{1}+\\alpha(1-p_{1}))% \\kappa d^{2}}{\\mathrm{tr}\\left((1-F(\\tau_{n}))\\mathbf{V}\\right)}, divide start_ARG italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_ ( 1 - italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG roman_tr ( ( 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ) bold_V ) end_ARG  italic_ ( italic_ ; italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  divide start_ARG italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_ ( 1 - italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) italic_ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG roman_tr ( ( 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ) bold_V ) end_ARG ,  where    \\kappa italic_  is the condition number of  V V \\mathbf{V} bold_V . A natural objective for efficiency optimization is  min   [ 0 , 1 ]  lim n   p 1 +   ( 1  p 1 ) tr  ( V ) . subscript  0 1 subscript  n subscript p 1  1 subscript p 1 tr V \\min_{\\alpha\\in[0,1]}\\lim_{n\\rightarrow\\infty}\\frac{p_{1}+\\alpha(1-p_{1})}{% \\mathrm{tr}\\left(\\mathbf{V}\\right)}. roman_min start_POSTSUBSCRIPT italic_  [ 0 , 1 ] end_POSTSUBSCRIPT roman_lim start_POSTSUBSCRIPT italic_n   end_POSTSUBSCRIPT divide start_ARG italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_ ( 1 - italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) end_ARG start_ARG roman_tr ( bold_V ) end_ARG .",
            "Suppose Assumptions  1 ,  2 ,  3 , and   n    subscript  n \\tau_{n}\\rightarrow\\infty italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT   ,  n  ( 1  F  (  n ) )    n 1 F subscript  n n(1-F(\\tau_{n}))\\rightarrow\\infty italic_n ( 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) )   , then the optimal choice of downsampling rate is",
            "Equipped with all the findings from the previous sections, we now focus on the logistic regression model as an application. Given a sequence of   n    subscript  n \\tau_{n}\\rightarrow\\infty italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT   , define  F  (  n +  1 T  x ) := e  n +  1 T  x 1 + e  n +  1 T  x assign F subscript  n superscript subscript  1 T x superscript e subscript  n superscript subscript  1 T x 1 superscript e subscript  n superscript subscript  1 T x F(\\tau_{n}+\\theta_{1}^{T}x):=\\frac{e^{\\tau_{n}+\\theta_{1}^{T}x}}{1+e^{\\tau_{n}% +\\theta_{1}^{T}x}} italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) := divide start_ARG italic_e start_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x end_POSTSUPERSCRIPT end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x end_POSTSUPERSCRIPT end_ARG . A direct application of Theorem  1  indicates:",
            "Our estimator is different from  [ 50 ] , where  [ 50 ]  uses an inverse-weighting estimator for the inverse-weighting estimator ( 8 ). We illustrate the differences in the performance of our estimator and that of the estimator considered by  [ 50 ]  further through numerical experiments in Section  6.1 .",
            "Firstly, we compare the mean squared estimation error of our estimator and the inverse-weighting estimator  [ 50 ]  for   n = 6 , 7 , 8 , 9 subscript  n 6 7 8 9 \\tau_{n}=6,7,8,9 italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = 6 , 7 , 8 , 9  for some of the    [ 0.00005 , 0.5 ]  0.00005 0.5 \\alpha\\in[0.00005,0.5] italic_  [ 0.00005 , 0.5 ] . These values   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  correspond to  P  ( Y = 1 ) P Y 1 \\mathbb{P}(Y=1) blackboard_P ( italic_Y = 1 )  approximately equal to  0.002 , 0.0007 , 0.0002 , 0.000097 0.002 0.0007 0.0002 0.000097 0.002,0.0007,0.0002,0.000097 0.002 , 0.0007 , 0.0002 , 0.000097 . The numerical results shown by Figure  6(a)  (in Appendix  A ) are consistent with our findings: when    \\alpha italic_  is small and falls into the proper range satisfying the conditions of Theorem  1 , the mean-squared-error is close to that generated by   = 0.5  0.5 \\alpha=0.5 italic_ = 0.5 .",
            "From Figure  2  we see that for   n = 10 , 9.8 subscript  n 10 9.8 \\tau_{n}=10,9.8 italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = 10 , 9.8  (large), our proposed estimator outperforms both the inverse-weighting estimator and the conditional maximum likelihood estimator. This verifies our statement in Remark  1 . However for   n = 6 , 5 subscript  n 6 5 \\tau_{n}=6,5 italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = 6 , 5  (small), our proposed estimator is worse than the other two. Note that our estimator is not consistent when   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  is small by remark  1 , so its under-performance is not surprising in this scenario.",
            "To verify the performance of the proposed pseudo MLE on real imbalanced data, we compare the performance of our estimator with the inverse-weighting estimator on  UCI imbalanced datasets  in Table  1 .",
            "In order to adapt to our setup where we consider the regime with   n    subscript  n \\tau_{n}\\rightarrow\\infty italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT   , we set   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  such that  1 / ( 1 + e  n ) = p 1 1 1 superscript e subscript  n subscript p 1 1/(1+e^{\\tau_{n}})=p_{1} 1 / ( 1 + italic_e start_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) = italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , where the values of  p 1 subscript p 1 p_{1} italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  are the positive ratio in the imbalanced dataset. Then we use inverse-weighting estimator and our pseudo-MLE estimator to fit    subscript  \\theta_{*} italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  (the coefficient for the features), and we compute the log-losses for both estimators on the testing dataset. We replicate the experiment  500 500 500 500  times, and during each round we randomly split the dataset into  80 % percent 80 80\\% 80 %  for training and  20 % percent 20 20\\% 20 %  for testing. We then plot the average log-losses and the confidence intervals for the log-losses for each downsampling rate    \\alpha italic_  in Figure  7 , where these    \\alpha italic_ s are chosen close to  p 1 subscript p 1 p_{1} italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  (i.e. positive ratio) of each dataset. We refer the readers to Appendix  A.1  for performance with additional moderate and small values of   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT .",
            "Then ( 1 ) leads to a biased estimator for    subscript  \\theta_{*} italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT .",
            "Furthermore, if  { x  X |  ~ 1 T  x = 0 } =  conditional-set x X superscript subscript ~  1 T x 0 \\{x\\in\\mathcal{X}|\\tilde{\\theta}_{1}^{T}x=0\\}\\neq\\emptyset { italic_x  caligraphic_X | over~ start_ARG italic_ end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x = 0 } =   and  { x  X |   T  x = 0 }  { x  X |  ~ 1 T  x = 0 }  {  , X } conditional-set x X superscript subscript  T x 0 conditional-set x X superscript subscript ~  1 T x 0 X \\{x\\in\\mathcal{X}|\\theta_{*}^{T}x=0\\}\\cap\\{x\\in\\mathcal{X}|\\tilde{\\theta}_{1}^% {T}x=0\\}\\notin\\{\\emptyset,\\mathcal{X}\\} { italic_x  caligraphic_X | italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x = 0 }  { italic_x  caligraphic_X | over~ start_ARG italic_ end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x = 0 }  {  , caligraphic_X } , then the prediction score obtained by the procedure described above (i.e. solving ( 1 ) and then applying isotonic regression) is also biased.",
            "The first-order condition for ( 1 ) is",
            "then ( 13 ) defines a probability distribution  P ~ ~ P \\tilde{P} over~ start_ARG italic_P end_ARG  with respect to downsample random variables  ( Y ~ i , X ~ i ) subscript ~ Y i subscript ~ X i (\\tilde{Y}_{i},\\tilde{X}_{i}) ( over~ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over~ start_ARG italic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) .",
            "The downsampled data  { ( X ~ i , Y ~ i ) } i = 1 N superscript subscript subscript ~ X i subscript ~ Y i i 1 N \\{(\\tilde{X}_{i},\\tilde{Y}_{i})\\}_{i=1}^{N} { ( over~ start_ARG italic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over~ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT  are i.i.d. generated with respect to  P ~ ~ P \\tilde{P} over~ start_ARG italic_P end_ARG  defined in ( 13 ) of Lemma  2 .",
            "where (a) uses the fact that  1  ( ( Y i , X i ) = ( y , x ) , Y i = 1 ) 1 formulae-sequence subscript Y i subscript X i y x subscript Y i 1 \\mathbf{1}((Y_{i},X_{i})=(y,x),Y_{i}=1) bold_1 ( ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = ( italic_y , italic_x ) , italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1 )  and  1  ( ( Y i , X i ) = ( y , x ) , Y i = 0 , U   ) 1 formulae-sequence subscript Y i subscript X i y x formulae-sequence subscript Y i 0 U  \\mathbf{1}((Y_{i},X_{i})=(y,x),Y_{i}=0,U\\leq\\alpha) bold_1 ( ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = ( italic_y , italic_x ) , italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0 , italic_U  italic_ )  are disjoint events, and (b) uses the fact that  ( Y ~ j , X ~ j ) = L  ( Y j , X j , U ) subscript ~ Y j subscript ~ X j L subscript Y j subscript X j U (\\tilde{Y}_{j},\\tilde{X}_{j})=\\mathcal{L}(Y_{j},X_{j},U) ( over~ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , over~ start_ARG italic_X end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) = caligraphic_L ( italic_Y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_U )  as some joint law of  ( Y j , X j , U ) subscript Y j subscript X j U (Y_{j},X_{j},U) ( italic_Y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_U ) , which is independent of  ( Y i , X i ) subscript Y i subscript X i (Y_{i},X_{i}) ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) . Moreover, (c) uses the fact that  P  ( ( Y ~ i , X ~ i ) = ( y , x ) ) = P  ( ( Y i , X i ) = ( 1 , x ) ) + P  ( ( Y i , X i ) = ( 0 , x ) , U   ) P subscript ~ Y i subscript ~ X i y x P subscript Y i subscript X i 1 x P formulae-sequence subscript Y i subscript X i 0 x U  \\mathbb{P}\\left(\\left(\\tilde{Y}_{i},\\tilde{X}_{i}\\right)=(y,x)\\right)=\\mathbb{% P}\\left((Y_{i},X_{i})=(1,x)\\right)+\\mathbb{P}\\left((Y_{i},X_{i})=(0,x),U\\leq% \\alpha\\right) blackboard_P ( ( over~ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over~ start_ARG italic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = ( italic_y , italic_x ) ) = blackboard_P ( ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = ( 1 , italic_x ) ) + blackboard_P ( ( italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = ( 0 , italic_x ) , italic_U  italic_ )  according to ( 14 ). Thus  ( Y ~ i , X ~ i ) subscript ~ Y i subscript ~ X i (\\tilde{Y}_{i},\\tilde{X}_{i}) ( over~ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over~ start_ARG italic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  and  ( Y ~ j , X ~ j ) subscript ~ Y j subscript ~ X j (\\tilde{Y}_{j},\\tilde{X}_{j}) ( over~ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , over~ start_ARG italic_X end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )  are independent with respect to  P P \\mathbb{P} blackboard_P .",
            "From ( 14 ) we know that for  y  { 0 , 1 } y 0 1 y\\in\\{0,1\\} italic_y  { 0 , 1 } ,  x  X x X x\\in\\mathcal{X} italic_x  caligraphic_X , and  P P \\mathbb{P} blackboard_P  as the joint law of  ( Y , X ) Y X (Y,X) ( italic_Y , italic_X ) , we have",
            "From ( 14 ) and previous proofs, with  P P \\mathbb{P} blackboard_P  denoting the joint law of  ( Y , X ) Y X (Y,X) ( italic_Y , italic_X ) , we have",
            "then following similar steps as in the proof of Theorem  1 , we use the same definition of  L n  (  1 ) subscript L n subscript  1 L_{n}(\\theta_{1}) italic_L start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  and  a n subscript a n a_{n} italic_a start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , and we can get",
            "Then the rest of the proof follows by similar steps of checking regularity conditions, etc. as in Theorem  1 .",
            "g 1  (  1 T  x ) = lim n   F   (  n +  1 T  x ) 1  F  (  n +  1 T  x ) = lim n   F  (  n +  1 T  x ) = 1 subscript g 1 superscript subscript  1 T x subscript  n superscript F  subscript  n superscript subscript  1 T x 1 F subscript  n superscript subscript  1 T x subscript  n F subscript  n superscript subscript  1 T x 1 g_{1}(\\theta_{1}^{T}x)=\\lim_{n\\rightarrow\\infty}\\frac{F^{\\prime}(\\tau_{n}+% \\theta_{1}^{T}x)}{1-F(\\tau_{n}+\\theta_{1}^{T}x)}=\\lim_{n\\rightarrow\\infty}F(% \\tau_{n}+\\theta_{1}^{T}x)=1 italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) = roman_lim start_POSTSUBSCRIPT italic_n   end_POSTSUBSCRIPT divide start_ARG italic_F start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) end_ARG start_ARG 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) end_ARG = roman_lim start_POSTSUBSCRIPT italic_n   end_POSTSUBSCRIPT italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) = 1 , and also we have  g 2  (  1 T  x ) = lim n   F   (  n +  1 T  x ) 1  F  (  n +  1 T  x ) =  1 subscript g 2 superscript subscript  1 T x subscript  n superscript F  subscript  n superscript subscript  1 T x 1 F subscript  n superscript subscript  1 T x 1 g_{2}(\\theta_{1}^{T}x)=\\lim_{n\\rightarrow\\infty}\\frac{F^{\\prime\\prime}(\\tau_{n% }+\\theta_{1}^{T}x)}{1-F(\\tau_{n}+\\theta_{1}^{T}x)}=-1 italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) = roman_lim start_POSTSUBSCRIPT italic_n   end_POSTSUBSCRIPT divide start_ARG italic_F start_POSTSUPERSCRIPT   end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) end_ARG start_ARG 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) end_ARG = - 1 ,  g 3  (  1 T  x ) = lim n   F ( 3 )  (  n +  1 T  x ) 1  F  (  n +  1 T  x ) = 1 subscript g 3 superscript subscript  1 T x subscript  n superscript F 3 subscript  n superscript subscript  1 T x 1 F subscript  n superscript subscript  1 T x 1 g_{3}(\\theta_{1}^{T}x)=\\lim_{n\\rightarrow\\infty}\\frac{F^{(3)}(\\tau_{n}+\\theta_% {1}^{T}x)}{1-F(\\tau_{n}+\\theta_{1}^{T}x)}=1 italic_g start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) = roman_lim start_POSTSUBSCRIPT italic_n   end_POSTSUBSCRIPT divide start_ARG italic_F start_POSTSUPERSCRIPT ( 3 ) end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) end_ARG start_ARG 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) end_ARG = 1 , so by Theorem  1  we have",
            "g 3  (  1 T  x ) = lim n   F ( 3 )  (  n +  1 T  x ) 1  F  (  n +  1 T  x ) = 0 subscript g 3 superscript subscript  1 T x subscript  n superscript F 3 subscript  n superscript subscript  1 T x 1 F subscript  n superscript subscript  1 T x 0 g_{3}(\\theta_{1}^{T}x)=\\lim_{n\\rightarrow\\infty}\\frac{F^{(3)}(\\tau_{n}+\\theta_% {1}^{T}x)}{1-F(\\tau_{n}+\\theta_{1}^{T}x)}=0 italic_g start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) = roman_lim start_POSTSUBSCRIPT italic_n   end_POSTSUBSCRIPT divide start_ARG italic_F start_POSTSUPERSCRIPT ( 3 ) end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) end_ARG start_ARG 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) end_ARG = 0 . Thus the conditions in Assumption  3  are satisfied. Further note that the conditions of Theorem  1  also hold. Then by Theorem  3 ,"
        ]
    },
    "id_table_2": {
        "caption": "",
        "table": "A5.EGx1",
        "footnotes": [],
        "references": [
            "Secondly, we obtain the asymptotic normality of the proposed pseudo MLE (see Theorem  1  and Theorem  2 ). The estimator is unbiased and has generally larger variance than the full-sample estimator (with zero downsampling rate). But for some small values of    \\alpha italic_  the asymptotic variance stays the same as that of the full-sampling estimator, meaning that downsampling with these    \\alpha italic_  results in no efficiency loss at all. We conduct numerical experiments and apply our estimator to logistic regression for different values of   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT . We use the mean squared error metric and compare the pseudo MLE with two commonly used alternative estimators: the inverse-weighting estimator ( 8 ) employed by  [ 50 ] , and the conditional MLE ( 9 ). The findings are that our estimator outperforms both of them when   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  is large, but as   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  decreases, it loses its advantage gradually, which is consistent with our theoretical conclusions.",
            "Proposition  2  exhibits the empirical maximum likelihood estimator of downsample GLM with respect to the joint distribution of  ( Y ~ , X ~ ) ~ Y ~ X (\\tilde{Y},\\tilde{X}) ( over~ start_ARG italic_Y end_ARG , over~ start_ARG italic_X end_ARG ) . However, we note that ( 4 ) utilizes the marginal density of full-sample  X X X italic_X , which is expensive to estimate for large-scale imbalanced data. To tackle this, we first note that the density of down-sample  X ~ ~ X \\tilde{X} over~ start_ARG italic_X end_ARG  can be written as   ~  ( x ) = [ 1  ( 1   )  F  (  n +   T  x ) ]    ( x ) P  ( Y = 1 ) +   P  ( Y = 0 ) ~  x delimited-[] 1 1  F subscript  n superscript subscript  T x  x P Y 1  P Y 0 \\tilde{\\mu}(x)=\\frac{[1-(1-\\alpha)F(\\tau_{n}+\\theta_{*}^{T}x)]\\mu(x)}{\\mathbb{% P}(Y=1)+\\alpha\\mathbb{P}(Y=0)} over~ start_ARG italic_ end_ARG ( italic_x ) = divide start_ARG [ 1 - ( 1 - italic_ ) italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) ] italic_ ( italic_x ) end_ARG start_ARG blackboard_P ( italic_Y = 1 ) + italic_ blackboard_P ( italic_Y = 0 ) end_ARG  (see Lemma  4  in Appendix  B ), where    (  )   \\mu(\\cdot) italic_ (  )  is the density of full-sample  X X X italic_X . Thus the last term in ( 4 ) can be rewritten as",
            "where  G  (  ) G  G(\\cdot) italic_G (  )  is defined as in ( 2 ). By the theory of M-estimators  [ 49 ] , both of the estimators are consistent, i.e.   ^ I   p    superscript ^  I p  subscript  \\hat{\\theta}^{I}\\overset{p}{\\rightarrow}\\theta_{*} over^ start_ARG italic_ end_ARG start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT overitalic_p start_ARG  end_ARG italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  and   ^ C   p    superscript ^  C p  subscript  \\hat{\\theta}^{C}\\overset{p}{\\rightarrow}\\theta_{*} over^ start_ARG italic_ end_ARG start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT overitalic_p start_ARG  end_ARG italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT . We compare the performance of our proposed pseudo MLE   ^  subscript ^  \\hat{\\theta}_{*} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  as ( 6 ) with both   ^ I superscript ^  I \\hat{\\theta}^{I} over^ start_ARG italic_ end_ARG start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT  and   ^ C superscript ^  C \\hat{\\theta}^{C} over^ start_ARG italic_ end_ARG start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT  through numerical experiments on synthetic data (see Section  6.1 ) and empirical data (see Section  7  and Appendix  A ). We find that   ^  subscript ^  \\hat{\\theta}_{*} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  outperforms   ^ I superscript ^  I \\hat{\\theta}^{I} over^ start_ARG italic_ end_ARG start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT  and   ^ C superscript ^  C \\hat{\\theta}^{C} over^ start_ARG italic_ end_ARG start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT  for large values of   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , but it gradually under-performs as we decrease the value   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , which verifies our conjecture as in remark  1 , implying its value under rare-event setup.",
            "Suppose Assumptions  1 ,  2 ,  3  hold, and for some    R d  superscript R d \\Theta\\subset\\mathbb{R}^{d} roman_  blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT  as a neighborhood of    subscript  \\theta_{*} italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT ,  { F  (  n +  1 T  x ) :  1   } conditional-set F subscript  n superscript subscript  1 T x subscript  1  \\{F(\\tau_{n}+\\theta_{1}^{T}x):\\theta_{1}\\in\\Theta\\} { italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_x ) : italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  roman_ }  is differentiable in quadratic mean at    subscript  \\theta_{*} italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT . Assume that  n  ( 1  F  (  n ) )    n 1 F subscript  n n(1-F(\\tau_{n}))\\rightarrow\\infty italic_n ( 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) )   ,  lim n   ( 1   ) 2  ( 1  F  (  n ) )  = c , subscript  n superscript 1  2 1 F subscript  n  c \\lim_{n\\rightarrow\\infty}\\frac{(1-\\alpha)^{2}(1-F(\\tau_{n}))}{\\alpha}=c, roman_lim start_POSTSUBSCRIPT italic_n   end_POSTSUBSCRIPT divide start_ARG ( 1 - italic_ ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ) end_ARG start_ARG italic_ end_ARG = italic_c ,  and",
            "Theorem  1  and Theorem  2  indicate that full-sampling (i.e.   = 1  1 \\alpha=1 italic_ = 1 ) and downsampling with a rate    \\alpha italic_  such that    1 much-less-than  1 \\alpha\\ll 1 italic_  1  and  1  F  (  n )  = o  ( 1 ) 1 F subscript  n  o 1 \\frac{1-F(\\tau_{n})}{\\alpha}=\\mathrm{o}(1) divide start_ARG 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) end_ARG start_ARG italic_ end_ARG = roman_o ( 1 )  lead to the same rate of convergence and asymptotic MSE. At the same time, if we choose    P  ( Y = 1 )  P Y 1 \\alpha\\approx\\mathbb{P}(Y=1) italic_  blackboard_P ( italic_Y = 1 ) , we still have the same convergence rate while the trace norm of the asymptotic covariance is kept as  O  ( 1 ) O 1 \\mathrm{O}(1) roman_O ( 1 ) . Intuitively, downsampling with a much smaller    \\alpha italic_  can help to reduce computational cost significantly while maintaining statistical efficiency under the current rare event setup. This motivates us to formulate the choice of optimal downsampling rate by considering this tradeoff between statistical efficiency and computational cost with a budget constraint.",
            "Recall from Theorem  1  that under regular conditions, when   n    subscript  n \\tau_{n}\\rightarrow\\infty italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT   , we have  n  ( 1  F  (  n ) )  (  ^     )   d  N  ( 0 , V  1 ) . n 1 F subscript  n subscript ^  subscript  d  N 0 superscript V 1 \\sqrt{n(1-F(\\tau_{n}))}(\\hat{\\theta}_{*}-\\theta_{*})\\overset{d}{\\rightarrow}% \\mathcal{N}\\left(\\mathbf{0},\\mathbf{V}^{-1}\\right). square-root start_ARG italic_n ( 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ) end_ARG ( over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT  end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT ) overitalic_d start_ARG  end_ARG caligraphic_N ( bold_0 , bold_V start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) .  So  n  ( 1  F  (  n ) )  tr  [ Cov  (  ^     ) ] tr  ( V  1 )  1 as  n   . formulae-sequence  n 1 F subscript  n tr delimited-[] Cov subscript ^  subscript  tr superscript V 1 1  as n \\frac{n(1-F(\\tau_{n}))\\mathrm{tr}\\left[\\mathrm{Cov}(\\hat{\\theta}_{*}-\\theta_{*% })\\right]}{\\mathrm{tr}\\left(\\mathbf{V}^{-1}\\right)}\\rightarrow 1\\ \\ \\mbox{as}% \\ n\\rightarrow\\infty. divide start_ARG italic_n ( 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ) roman_tr [ roman_Cov ( over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT  end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT ) ] end_ARG start_ARG roman_tr ( bold_V start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) end_ARG  1 as italic_n   .  Under the definition of the efficiency cost ( 12 ), we have  c 0  ( p 1 +   ( 1  p 1 ) )  d 2 tr  ( ( 1  F  (  n ) )  V )    (  ; p 1 )  c 0  ( p 1 +   ( 1  p 1 ) )    d 2 tr  ( ( 1  F  (  n ) )  V ) , subscript c 0 subscript p 1  1 subscript p 1 superscript d 2 tr 1 F subscript  n V   subscript p 1 subscript c 0 subscript p 1  1 subscript p 1  superscript d 2 tr 1 F subscript  n V \\frac{c_{0}(p_{1}+\\alpha(1-p_{1}))d^{2}}{\\mathrm{tr}\\left((1-F(\\tau_{n}))% \\mathbf{V}\\right)}\\leq\\nu(\\alpha;p_{1})\\leq\\frac{c_{0}(p_{1}+\\alpha(1-p_{1}))% \\kappa d^{2}}{\\mathrm{tr}\\left((1-F(\\tau_{n}))\\mathbf{V}\\right)}, divide start_ARG italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_ ( 1 - italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG roman_tr ( ( 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ) bold_V ) end_ARG  italic_ ( italic_ ; italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  divide start_ARG italic_c start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_ ( 1 - italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) italic_ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG roman_tr ( ( 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ) bold_V ) end_ARG ,  where    \\kappa italic_  is the condition number of  V V \\mathbf{V} bold_V . A natural objective for efficiency optimization is  min   [ 0 , 1 ]  lim n   p 1 +   ( 1  p 1 ) tr  ( V ) . subscript  0 1 subscript  n subscript p 1  1 subscript p 1 tr V \\min_{\\alpha\\in[0,1]}\\lim_{n\\rightarrow\\infty}\\frac{p_{1}+\\alpha(1-p_{1})}{% \\mathrm{tr}\\left(\\mathbf{V}\\right)}. roman_min start_POSTSUBSCRIPT italic_  [ 0 , 1 ] end_POSTSUBSCRIPT roman_lim start_POSTSUBSCRIPT italic_n   end_POSTSUBSCRIPT divide start_ARG italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_ ( 1 - italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) end_ARG start_ARG roman_tr ( bold_V ) end_ARG .",
            "Suppose Assumptions  1 ,  2 ,  3 , and   n    subscript  n \\tau_{n}\\rightarrow\\infty italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT   ,  n  ( 1  F  (  n ) )    n 1 F subscript  n n(1-F(\\tau_{n}))\\rightarrow\\infty italic_n ( 1 - italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) )   , then the optimal choice of downsampling rate is",
            "Henceforth, we have obtained explicit formulations of the optimization problems for selecting the optimal downsample rate. Theorem  3  has provided guidelines for downsampling schemes in practice. The result can easily be extended to generalized scaled result from Theorem  2 .",
            "From Figure  2  we see that for   n = 10 , 9.8 subscript  n 10 9.8 \\tau_{n}=10,9.8 italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = 10 , 9.8  (large), our proposed estimator outperforms both the inverse-weighting estimator and the conditional maximum likelihood estimator. This verifies our statement in Remark  1 . However for   n = 6 , 5 subscript  n 6 5 \\tau_{n}=6,5 italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = 6 , 5  (small), our proposed estimator is worse than the other two. Note that our estimator is not consistent when   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  is small by remark  1 , so its under-performance is not surprising in this scenario.",
            "Lastly, we also apply our method to neural networks on some of those datasets. The simulation details and insights are presented in Appendix  A.2 .",
            "where (1) follows from Proposition  2  and (2) uses Lemma  4 . Thus by Lemma  7 ,   ^ 1   p   ~ 1 subscript ^  1 p  subscript ~  1 \\hat{\\theta}_{1}\\overset{p}{\\rightarrow}\\tilde{\\theta}_{1} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT overitalic_p start_ARG  end_ARG over~ start_ARG italic_ end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  such that  E X  [ [ 1  ( 1   )  F  (  n +   T  X ) ]  F   (  n +  ~ 1 T  X )  X T 1  ( 1   )  F  (  n +  ~ 1 T  X ) ] = 0 subscript E X delimited-[] delimited-[] 1 1  F subscript  n superscript subscript  T X superscript F  subscript  n superscript subscript ~  1 T X superscript X T 1 1  F subscript  n superscript subscript ~  1 T X 0 \\mathbb{E}_{X}\\left[\\frac{[1-(1-\\alpha)F(\\tau_{n}+\\theta_{*}^{T}X)]F^{\\prime}(% \\tau_{n}+\\tilde{\\theta}_{1}^{T}X)X^{T}}{1-(1-\\alpha)F(\\tau_{n}+\\tilde{\\theta}_% {1}^{T}X)}\\right]=\\mathbf{0} blackboard_E start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT [ divide start_ARG [ 1 - ( 1 - italic_ ) italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_X ) ] italic_F start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + over~ start_ARG italic_ end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_X ) italic_X start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG 1 - ( 1 - italic_ ) italic_F ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + over~ start_ARG italic_ end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_X ) end_ARG ] = bold_0 .",
            "where (1) uses Proposition  2  and (2) uses Lemma  4 , and  E X subscript E X \\mathbb{E}_{X} blackboard_E start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT  is the expectation taken with respect to the distribution of  X X X italic_X  from the full data. Thus under the given conditions,  E X  [ F   (  n +   T  X )  X T ] = 0 subscript E X delimited-[] superscript F  subscript  n superscript subscript  T X superscript X T 0 \\mathbb{E}_{X}\\left[F^{\\prime}(\\tau_{n}+\\theta_{*}^{T}X)X^{T}\\right]\\neq% \\mathbf{0} blackboard_E start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT [ italic_F start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_X ) italic_X start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ] = bold_0 , while there exists some   ~ 1 subscript ~  1 \\tilde{\\theta}_{1} over~ start_ARG italic_ end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,",
            "The downsampled data  { ( X ~ i , Y ~ i ) } i = 1 N superscript subscript subscript ~ X i subscript ~ Y i i 1 N \\{(\\tilde{X}_{i},\\tilde{Y}_{i})\\}_{i=1}^{N} { ( over~ start_ARG italic_X end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over~ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT  are i.i.d. generated with respect to  P ~ ~ P \\tilde{P} over~ start_ARG italic_P end_ARG  defined in ( 13 ) of Lemma  2 .",
            "First note that Assumption  2  holds. We use  g 1  (  ) , g 2  (  ) , g 3  (  ) subscript g 1  subscript g 2  subscript g 3  g_{1}(\\cdot),g_{2}(\\cdot),g_{3}(\\cdot) italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT (  ) , italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT (  ) , italic_g start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT (  )  to denote   h ( 1 )  (  ) / h  (  ) ,  h ( 2 )  (  ) / h  (  ) ,  h ( 3 )  (  ) / h  (  ) superscript h 1  h  superscript h 2  h  superscript h 3  h  -h^{(1)}(\\cdot)/h(\\cdot),-h^{(2)}(\\cdot)/h(\\cdot),-h^{(3)}(\\cdot)/h(\\cdot) - italic_h start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT (  ) / italic_h (  ) , - italic_h start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT (  ) / italic_h (  ) , - italic_h start_POSTSUPERSCRIPT ( 3 ) end_POSTSUPERSCRIPT (  ) / italic_h (  ) . By definition, for"
        ]
    }
}