{
    "id_table_1": {
        "caption": "Table 2:  Total number of queries and records in the datasets",
        "table": "S1.2.fig1.1",
        "footnotes": [],
        "references": [
            "We present  \\name , a family of approaches to  n on-parametrically fine-t u ne embe d din g s  e fficiently.   \\name  methods (or  \\name s ) are surprisingly effective, model-agnostic, and incur no additional deployment cost.  \\name s take a novel non-parametric view of embedding fine-tuning: they view the embeddings themselves as parameters of the  k k k italic_k -NN retrieval algorithm and directly modify the embeddings of data records to maximize the accuracy of  k k k italic_k -NN retrieval. Although we show that the underlying optimization problem is NP-Hard in general, and can lead to overfitting if data embeddings are allowed to change arbitrarily,  \\name s efficiently solve constrained variations of this problem formulated to avoid overfitting. As shown in Fig.  1 ,  \\name s change each data embedding to maximize the similarity between the data embedding and the training queries to which the data record is a correct answer, while constraining how and by how much the embedding can change. Intuitively, the constraints allow for enough modifications to the embeddings to improve accuracy on the dataset in hand while avoiding large distortion that would offset the semantics learned during pre-training. Fig.  1  shows an example of the constrained region used, where new embeddings are constrained to be normalized (i.e., to fall on the unit ball), and the magnitude of changes to the embeddings to be bounded.  \\name s solve the constrained optimization problems in closed form, presenting simple and effective update formulae for embedding fine-tuning.",
            "We formalize the notion of non-parametric embedding fine-tuning and show that the underlying unconstrained non-parametric optimization problem is NP-Hard  (Sec.  3.1 ).",
            "Our  \\name  approach views embeddings as parameters of the  k k k italic_k -NN retrieval algorithm, optimizing the embeddings directly to improve retrieval accuracy. In Sec.  3.1 , we formalize the notion of non-parametric embedding fine-tuning by stating two optimization problems, one directly maximizing a retrieval accuracy metric and one maximizing similarity between queries and their ground-truth answers. Maximizing accuracy is the final goal, but similarity is a simpler surrogate to optimize in practice. Nonetheless, we show that the former is NP-hard and the latter is unbounded (i.e., the objective can be improved indefinitely). Moreover, since both optimization problems allow embeddings to arbitrarily change (i.e., are unconstrained), directly solving either problem can lead to overfitting. In Sec.  3.2 , we present  \\name , a family of approaches that solve a combination of constrained variations of the two optimization problems to address generalization and efficiency challenges.",
            "Theorem  1  is proved by reduction from the Maximum Feasible Linear Subsystem problem as studied in   Amaldi & Kann ( 1995 ) , see Appx.  B.1 . Apart from the NP-hardness, MaxA-EFT allows data embeddings to be arbitrarily changed by    {\\bm{\\Delta}} bold_ . This can distort the semantics captured in  D D {\\bm{D}} bold_italic_D  by the pre-trained model, and lead to poor generalization to queries outside of  Q Q {\\bm{Q}} bold_italic_Q .",
            "Because of the potential for overfitting and the computational challenges due to NP-hardness, we do not solve either MaxA-EFT or MaxS-EFT directly. Instead, we introduce  \\name , a family of approaches that solve constrained variations of MaxA-EFT and MaxS-EFT, designed to avoid overfitting, while being efficient. We discuss two main approaches,  \\name -M and  \\name -N, in Secs.  3.2.1  and  3.2.2  and present other practical extensions in Appx.  C .",
            "I i V  (  ) superscript subscript I i V  \\mathcal{I}_{i}^{V}({\\bm{\\Delta}}) caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT ( bold_ )  denotes  I i subscript I i \\mathcal{I}_{i} caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  (see Eq.  1 ) on the validation set  Q V , Y V superscript Q V superscript Y V {\\bm{Q}}^{V},Y^{V} bold_italic_Q start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT , so   i  [ n V ] I i V  (  ) subscript i delimited-[] subscript n V superscript subscript I i V  \\sum_{i\\in[n_{V}]}\\mathcal{I}_{i}^{V}({\\bm{\\Delta}})  start_POSTSUBSCRIPT italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ] end_POSTSUBSCRIPT caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT ( bold_ )  is the validation accuracy after fine-tuning with    {\\bm{\\Delta}} bold_ . This problem is referred to as  Bi-level Maximization with bounded Magnitude, BiMax-M . We denote the optimal solution to BiMax-M by   M superscript  M {\\bm{\\Delta}}^{M} bold_ start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT .",
            "The optimal    superscript  \\gamma^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in Eq.  3  is calculated by solving linear inequalities resulting from the definition of  I i V superscript subscript I i V \\mathcal{I}_{i}^{V} caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT . We provide an overview here and leave the details to Appx.  B.2 . First, note that  MaxS-M  (  ) =   G  G  MaxS-M   G norm G \\texttt{MaxS-M}(\\gamma)=\\gamma\\frac{{\\bm{G}}}{\\|{\\bm{G}}\\|} MaxS-M ( italic_ ) = italic_ divide start_ARG bold_italic_G end_ARG start_ARG  bold_italic_G  end_ARG , found using the KKT points of the optimization problem. Thus, BiMax-M reduces to finding a    \\gamma italic_  that maximizes   i  [ n V ] I i V  (   G  G  ) subscript i delimited-[] subscript n V superscript subscript I i V  G norm G \\sum_{i\\in[n_{V}]}\\mathcal{I}_{i}^{V}(\\gamma\\frac{{\\bm{G}}}{\\|{\\bm{G}}\\|})  start_POSTSUBSCRIPT italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ] end_POSTSUBSCRIPT caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT ( italic_ divide start_ARG bold_italic_G end_ARG start_ARG  bold_italic_G  end_ARG ) .  For each  i  [ n V ] i delimited-[] subscript n V i\\in[n_{V}] italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ] , substituting   =   G  G    G norm G {\\bm{\\Delta}}=\\gamma\\frac{{\\bm{G}}}{\\|{\\bm{G}}\\|} bold_ = italic_ divide start_ARG bold_italic_G end_ARG start_ARG  bold_italic_G  end_ARG  into the definition of  I i V  (  ) superscript subscript I i V  \\mathcal{I}_{i}^{V}({\\bm{\\Delta}}) caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT ( bold_ )  in Eq.  1 , we have  I i V  (   G  G  ) = 1 superscript subscript I i V  G norm G 1 \\mathcal{I}_{i}^{V}(\\gamma\\frac{{\\bm{G}}}{\\|{\\bm{G}}\\|})=1 caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT ( italic_ divide start_ARG bold_italic_G end_ARG start_ARG  bold_italic_G  end_ARG ) = 1  if the following holds, and zero otherwise:",
            "Finding the optimal    superscript  \\gamma^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is similar to  \\name -M, where we first use KKT to solve  MaxS-N  (  ) MaxS-N  \\texttt{MaxS-N}(\\gamma) MaxS-N ( italic_ )  and substitute the resulting    {\\bm{\\Delta}} bold_  into the definition of  I i V subscript superscript I V i \\mathcal{I}^{V}_{i} caligraphic_I start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  in Eq.  1 . The optimal    superscript  \\gamma^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is then found as the value that satisfies the maximum number of the resulting inequalities. The resulting inequalities are, in this case, quadratic due to    ( 4   )  4  \\sqrt{\\gamma(4-\\gamma)} square-root start_ARG italic_ ( 4 - italic_ ) end_ARG  in the solution to  MaxS-N  (  ) MaxS-N  \\texttt{MaxS-N}(\\gamma) MaxS-N ( italic_ ) , but can still be solved in closed-form. Nevertheless, from a practical perspective, solving the quadratic equations is tedious as it requires considering various special cases. We observed that performing a grid search to find    superscript  \\gamma^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  that maximizes validation accuracy finds good enough solutions and is almost as efficient. Thus, in our experiments, we use this practical implementation.",
            "Baselines . We report results using the embeddings without fine-tuning, called No Fine-Tuning, in addition to training Adaptors and fine-tuning the pre-trained model, referred to as PTFT (see Appx.  E.1  for implementation details). Due to computational constraints, we report results for the latter only for the small open-source model BGE-S in our main experiments. For both, we present two versions. By default, we use the Multiple Negative Ranking (MNR) loss  (Henderson et al.,  2017 )  for training, which is the standard contrastive fine-tuning loss when positive query/answer pairs are available, suggested by SentenceTransformers  (SentenceTransformers,  2024a )  and used by LlamaIndex  (LlamaIndex,  2024d )  for fine-tuning (although LlamaIndex only uses a single positive example per query  (LlamaIndex,  2024e ) ). Despite our hyperparameter tuning effort, we observed no accuracy improvements on some datasets through fine-tuning with this loss (see Table  6 ). We then modified the loss, so that only negative samples whose cosine similarity is at least equal to a threshold are included in the loss (see  E.1  for details), which improved accuracy on some datasets but worsened it on other. We use the suffix -L to denote the baselines using this modified loss. We report results using this loss to gain more insight into the baselines behavior. We use cosine similarity as the retrieval distance metric for No Fine-Tuning, Adaptor, and PTFT.",
            "Moreover, Table  5  shows the total fine-tuning time to run BGE-S on our text datasets (i.e., time obtain the associated results in Table  3 ) using an Nvidia A100 GPU as well as using 32 core Intel Broadwell CPUs. The reported time excludes the time to embed the data records (which is the same across all methods).  \\name  variants run in 1-2 minutes with GPU and in up to 11 minutes using CPUs, which is, respectively, more than 3 and 11 times faster than Adaptor and more than 200 times faster than PTFT, which cannot be run on CPUs in a reasonable time-frame. For both Adaptor and PTFT, the reported run times are for optimized implementations that include early stopping and optimizations for efficient calculation of validation accuracy (see Appx.  E.1 ).",
            "Appx.  B.1 - B.3 , respectively, provide the proofs of Theorems.  1 - 3 . The proofs of technical lemmas are presented in Appx.  B.4",
            "-M . Alg.  1  presents  \\name -M, which formalizes the above procedure, and also incorporates the constraint    0  0 \\gamma\\geq 0 italic_  0 . Lines  6 - 15  find the intervals  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,   i  [ n V ] for-all i delimited-[] subscript n V \\forall i\\in[n_{V}]  italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ]  and add it to a list  I I I italic_I  (intersection of half intervals can be found by calculating the maximum of lower bounds and minimum of upper bounds of the intervals, done in lines  9 -  11 ). The algorithm returns new data embeddings by simply performing a single addition.",
            "Time complexity . Alg.  1  can be implemented so that lines  11 - 7  with a single pass over the dataset. Therefore, finding  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  for all queries (lines  6 - 15 ) take  O  ( n V  n  d ) O subscript n V n d O(n_{V}\\times n\\times d) italic_O ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT  italic_n  italic_d ) . Calculating  G G G italic_G  takes time  O  ( n T  d ) O subscript n T d O(n_{T}\\times d) italic_O ( italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  italic_d ) . Finding    superscript  \\gamma^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in line  16  is the basic problem of finding the maximum number of overlapping ranges and can be done in  O  ( n V  log  ( n V ) ) O subscript n V subscript n V O(n_{V}\\log(n_{V})) italic_O ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT roman_log ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ) ) , by first sorting the ranges in  I I I italic_I  based on their lower bound and iteratively traversing the sorted list and keeping track of the number of overlapping ranges. Thus, Alg.  1  can be implemented in  O ( n V ( n  d + log ( n V ) + n T  d ) O(n_{V}(n\\times d+\\log(n_{V})+n_{T}\\times d) italic_O ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ( italic_n  italic_d + roman_log ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ) + italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  italic_d ) .",
            "From Eq.  10 ,",
            "Substituting this into Eq.  13 , and since   > 0  0 \\mu>0 italic_ > 0 , we have",
            "and substituting back into Eq.  14 , we have",
            "where Eq.  15  is obtained by setting the gradient of the Lagrangian to zero, and Eq.  19  and   16  are obtained by substituting",
            "To find the points satisfying all Eq.  15 - 20 , we consider 4 setting depending on whether   = 0  0 \\lambda=0 italic_ = 0  or   = 0  0 \\mu=0 italic_ = 0  or not.",
            "Case 1,   = 0 ,  = 0 formulae-sequence  0  0 \\lambda=0,\\mu=0 italic_ = 0 , italic_ = 0 . Substituting   = 0  0 \\lambda=0 italic_ = 0  and   = 0  0 \\mu=0 italic_ = 0  in Eq.  15 , we have  G i = 0 subscript G i 0 {\\bm{G}}_{i}=\\mathbf{0} bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_0 . Thus, no solution with   = 0  0 \\lambda=0 italic_ = 0  and   = 0  0 \\mu=0 italic_ = 0  exists since  G i = 0 subscript G i 0 {\\bm{G}}_{i}\\neq\\mathbf{0} bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_0 .",
            "Case 2,   = 0 ,  > 0 formulae-sequence  0  0 \\lambda=0,\\mu>0 italic_ = 0 , italic_ > 0 . Having   = 0  0 \\lambda=0 italic_ = 0  and   > 0  0 \\mu>0 italic_ > 0 , we have from Eq.  15",
            "substituting which into Eq.  17  with   > 0  0 \\mu>0 italic_ > 0  w have",
            "and therefore   i =   G i  G i  subscript  i  subscript G i norm subscript G i {\\bm{\\Delta}}_{i}=\\sqrt{\\gamma}\\frac{{\\bm{G}}_{i}}{\\|{\\bm{G}}_{i}\\|} bold_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = square-root start_ARG italic_ end_ARG divide start_ARG bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG  bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  end_ARG  (KKT conditions become similar to the ones in Theorem  2 ). Because   > 0  0 \\mu>0 italic_ > 0 , we must have    i  2 =  superscript norm subscript  i 2  \\|{\\bm{\\Delta}}_{i}\\|^{2}=\\gamma  bold_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_  due to Eq.  17 , so that from Eq.  19  we have",
            "Case 3,   > 0 ,  = 0 formulae-sequence  0  0 \\lambda>0,\\mu=0 italic_ > 0 , italic_ = 0 . Substituting   = 0  0 \\mu=0 italic_ = 0  in Eq.  15  we have",
            "Substituting this in Eq.  16 , we have",
            "Note that to satisfy Eq.  18 , we must have",
            "Case 4,   > 0 ,  > 0 formulae-sequence  0  0 \\lambda>0,\\mu>0 italic_ > 0 , italic_ > 0 . From Eq.  15 , we have",
            "and since    i    = 0 norm subscript  i  0 \\|{\\bm{\\Delta}}_{i}\\|-\\gamma=0  bold_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  - italic_ = 0 , Eq.  16  simplifies to   + 2   i  D i = 0   2 subscript  i subscript D i 0 \\gamma+2{\\bm{\\Delta}}_{i}\\cdot{\\bm{D}}_{i}=0 italic_ + 2 bold_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  bold_italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0 , so that",
            "Substituting this in Eq. 21 , we have",
            "Now, substituting Eq.  23  in Eq.  17 , we get",
            "-IM performs gradient descent on loss in Eq.  31  with learning rate    \\alpha italic_  for  t t t italic_t  iterations (   \\alpha italic_  and  t t t italic_t  determined through hyperparameter tuning to maximize validation accuracy) with normalized gradients:",
            "Problem Formulation . Both MaxS-EFT and MaxA-EFT can be modified to utilize multiple labels. For MaxS-EFT we can change the objective to   i  [ n T ]  j   Y i T Q i T  ( D j  +  j  ) subscript i delimited-[] subscript n T subscript superscript j superscript subscript Y i T  subscript superscript Q T i subscript D superscript j subscript  superscript j \\sum_{i\\in[n_{T}]}\\sum_{j^{*}\\in Y_{i}^{T}}{\\bm{Q}}^{T}_{i}\\cdot({\\bm{D}}_{j^{%  *}}+{\\bm{\\Delta}}_{j^{*}})  start_POSTSUBSCRIPT italic_i  [ italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ] end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_j start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_POSTSUBSCRIPT bold_italic_Q start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  ( bold_italic_D start_POSTSUBSCRIPT italic_j start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT + bold_ start_POSTSUBSCRIPT italic_j start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) , where summation over  Y i T superscript subscript Y i T Y_{i}^{T} italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  can optionally be weighted by relevance scores. For MaxA-EFT, we can adjust the inequalities in the definition of the correct answer to a query (i.e., Eq.  1 ), so that for a query  q q {\\bm{q}} bold_italic_q  with two relevance scores  r 1 subscript r 1 r_{1} italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  r 2 subscript r 2 r_{2} italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ,  r 1 > r 2 subscript r 1 subscript r 2 r_{1}>r_{2} italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , and for  R 1 subscript R 1 R_{1} italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  R 2 subscript R 2 R_{2} italic_R start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  containing document indexes with  r 1 subscript r 1 r_{1} italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  r 2 subscript r 2 r_{2} italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  relevance scores, we say  q q {\\bm{q}} bold_italic_q  is answered correctly when",
            ". To solve BiMax-M and BiMax-N, observe that the above modifications cause marginal changes for  MaxS-M  and  MaxS-N , and only require modifying the definition of  G G {\\bm{G}} bold_italic_G  in the corresponding optimal solutions. However, the solutions to the outer optimization problem in BiMax-M and BiMax-N require further modifications since now a different set of inequalities needs to be solved to find the range of    \\gamma italic_  for which a query is answered correctly. However, each inequality is still of the same form as before (compare Eq.  32  with Eq.  1 ), and thus, the same methodology applies.",
            "Appx.  E.1  discussed details on the implementation of Adaptor and PTFT, including hyper-parameter tuning, loss, and efficiency considerations.",
            "Tables  9 - 12  present the per dataset results for the embedding models GTE-L, TE3-L, CLIP-B and CLIP-L. The tables (in addition to Table  6 ) present the detailed results from which Tables  3 - 5  are generated.",
            "We compare  \\name -N, with  \\name -IN (as described in Sec.  C ) with two other potential variants to understand the impact of normalization.  \\name -M+N is a variant that first performs  \\name -M and then normalizes embeddings post-hoc.  \\name -IM+R is a variation of  \\name -IM with L2 regularization added to the loss to penalize embeddings with large norms. Table  13  shows the results for this experiment, showing that normalizing embeddings after optimization, i.e.,  \\name -M+N performs worse than when normalization is considered as part of optimization, which is the case for both  \\name -IN and  \\name -N.  \\name -IM+R performs worse than all methods, showing an advantage for enforcing a normalization constraint over L2 regularization. Meanwhile,  \\name -IN and  \\name -N perform similarly (see Sec.  C  for a discussion between the two)."
        ]
    },
    "id_table_2": {
        "caption": "Table 3:  Average results across text datasets grouped by the embedding model used",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "We present  \\name -M and  \\name -N, two methods that optimally solve constrained variations of the problem (Sec.  3.2 ).   \\name -M updates data embeddings to maximize the similarity between the embeddings of queries and their ground-truth data records subject to a bound on the magnitude of change to the embeddings.   \\name -N adopts a similar approach, but additionally constrains the embeddings to be normalized.",
            "Our  \\name  approach views embeddings as parameters of the  k k k italic_k -NN retrieval algorithm, optimizing the embeddings directly to improve retrieval accuracy. In Sec.  3.1 , we formalize the notion of non-parametric embedding fine-tuning by stating two optimization problems, one directly maximizing a retrieval accuracy metric and one maximizing similarity between queries and their ground-truth answers. Maximizing accuracy is the final goal, but similarity is a simpler surrogate to optimize in practice. Nonetheless, we show that the former is NP-hard and the latter is unbounded (i.e., the objective can be improved indefinitely). Moreover, since both optimization problems allow embeddings to arbitrarily change (i.e., are unconstrained), directly solving either problem can lead to overfitting. In Sec.  3.2 , we present  \\name , a family of approaches that solve a combination of constrained variations of the two optimization problems to address generalization and efficiency challenges.",
            "referred to as  MaxS-EFT . Here, we change data embeddings to maximize the similarity between queries and ground-truth answers,  a standard optimization objective (e.g.,  Henderson et al. ( 2017 ) ,  SentenceTransformers ( 2024b ) ). However, the non-parametric formulation makes Eq.  2  an unconstrained optimization problem with a linear objective, so the problem is unbounded and has no optimal solution. Moreover, setting   Y i subscript  subscript Y i {\\bm{\\Delta}}_{Y_{i}} bold_ start_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT  so that  Q i   Y i > 0  subscript Q i subscript  subscript Y i 0 {\\bm{Q}}_{i}\\cdot{\\bm{\\Delta}}_{Y_{i}}>0 bold_italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  bold_ start_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT > 0  and increasing the magnitude of   Y i subscript  subscript Y i {\\bm{\\Delta}}_{Y_{i}} bold_ start_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT  arbitrarily improves the objective, yielding trivial solutions with poor generalization to unseen queries.",
            "Because of the potential for overfitting and the computational challenges due to NP-hardness, we do not solve either MaxA-EFT or MaxS-EFT directly. Instead, we introduce  \\name , a family of approaches that solve constrained variations of MaxA-EFT and MaxS-EFT, designed to avoid overfitting, while being efficient. We discuss two main approaches,  \\name -M and  \\name -N, in Secs.  3.2.1  and  3.2.2  and present other practical extensions in Appx.  C .",
            "-M solves MaxS-EFT on the training set, but with the added constraint    i    norm subscript  i  \\|{\\bm{\\Delta}}_{i}\\|\\leq\\gamma  bold_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT   italic_ ,   i  [ n ] for-all i delimited-[] n \\forall i\\in[n]  italic_i  [ italic_n ] , for a scalar    0  0 \\gamma\\geq 0 italic_  0 .    \\gamma italic_  controls how much each embedding can change during fine-tuning.  \\name -M sets    \\gamma italic_  by solving MaxA-EFT on the validation set. Intuitively, this (1) changes data embeddings to maximize the similarity between embeddings and queries on the training set, (2) ensures that the magnitude of the changes to the embeddings is bounded to avoid overfitting, and (3) decides how much the embeddings are allowed to change by maximizing validation accuracy.  \\name -M provides a closed-form solution to do this. We provide an overview of the solution here and leave the details and formal proofs to Appx.  B.2 .",
            "The optimal    superscript  \\gamma^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in Eq.  3  is calculated by solving linear inequalities resulting from the definition of  I i V superscript subscript I i V \\mathcal{I}_{i}^{V} caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT . We provide an overview here and leave the details to Appx.  B.2 . First, note that  MaxS-M  (  ) =   G  G  MaxS-M   G norm G \\texttt{MaxS-M}(\\gamma)=\\gamma\\frac{{\\bm{G}}}{\\|{\\bm{G}}\\|} MaxS-M ( italic_ ) = italic_ divide start_ARG bold_italic_G end_ARG start_ARG  bold_italic_G  end_ARG , found using the KKT points of the optimization problem. Thus, BiMax-M reduces to finding a    \\gamma italic_  that maximizes   i  [ n V ] I i V  (   G  G  ) subscript i delimited-[] subscript n V superscript subscript I i V  G norm G \\sum_{i\\in[n_{V}]}\\mathcal{I}_{i}^{V}(\\gamma\\frac{{\\bm{G}}}{\\|{\\bm{G}}\\|})  start_POSTSUBSCRIPT italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ] end_POSTSUBSCRIPT caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT ( italic_ divide start_ARG bold_italic_G end_ARG start_ARG  bold_italic_G  end_ARG ) .  For each  i  [ n V ] i delimited-[] subscript n V i\\in[n_{V}] italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ] , substituting   =   G  G    G norm G {\\bm{\\Delta}}=\\gamma\\frac{{\\bm{G}}}{\\|{\\bm{G}}\\|} bold_ = italic_ divide start_ARG bold_italic_G end_ARG start_ARG  bold_italic_G  end_ARG  into the definition of  I i V  (  ) superscript subscript I i V  \\mathcal{I}_{i}^{V}({\\bm{\\Delta}}) caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT ( bold_ )  in Eq.  1 , we have  I i V  (   G  G  ) = 1 superscript subscript I i V  G norm G 1 \\mathcal{I}_{i}^{V}(\\gamma\\frac{{\\bm{G}}}{\\|{\\bm{G}}\\|})=1 caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT ( italic_ divide start_ARG bold_italic_G end_ARG start_ARG  bold_italic_G  end_ARG ) = 1  if the following holds, and zero otherwise:",
            "-N additionally constrains the norm of the fine-tuned embedding. This constraint serves as an additional regularization, helping with out-of-distribution generalization (see Sec. 4.2 ).",
            "Datasets . For text retrieval datasets we use 7 standard datasets: SciFacts  (Wadden et al.,  2020 ) , Fever  (Fever,  2024 ) , ArguAna  (Arguana,  2024 )  (we use their BEIR  (Thakur et al.,  2021 )  versions), TriviaQA  Joshi et al. ( 2017 ) , HotpotQA  (Yang et al.,  2018 ) , and Natural Questions (Kwiatkowski et al.,  2019 )  (we use their KILT  (Petroni et al.,  2021 )  versions), and NF-Corpus  (Boteva et al.,  2016 )  (although all datasets have a BEIR version, we use non-BEIR versions whenever that is larger). We use the datasets as is, without any preprocessing step, except for datasets from KILT, where we only use Wikipedia pages that contain an answer to at least one query (i.e., pages where we expect fine-tuning to have an impact). For image retrieval, we use COCO  (Lin et al.,  2014 )  (we use the dataset from 2014) and Flickr  (Young et al.,  2014 )  datasets. We use image captions as queries to retrieve the corresponding image. For all text and image datasets, we use 0.7-0.1-0.2 train, validation and test split, but limit test and validation sizes to at most 10,000 queries if there is more. Statistics about data and query size are presented in Table  2 .",
            "Summary of results . Tables  3 - 5  present our accuracy results averaged across all text or image datasets for different models. Table  6  presents the per dataset results for BGE-S. The per dataset results for other models followed similar trends and are deferred to Appx.  E.2 .",
            "To better understand the results, we remark on the performance on ArguAna and Fever datasets. Fever, where  \\name -M performs better than  \\name -N has a skewed label distribution, where the same paragraph is the correct answer for many queries. This allows for setting the magnitude of the embeddings based on label distribution, assigning embeddings with larger magnitudes to more frequently accessed passages. Such an assignment can improve the accuracy when the label distribution is fixed and skewed, but leads to worse generalization when there is no skew (as the results on other datasets in Table  6  show) or when there is a distribution shift (see Sec.  4.2 ). Finally, in ArguAna, each data record is a factual argument and each query provides an argument and asks for a counterargument to the given argument. In such a setting, to improve accuracy, we expect larger systematic changes to the embedding space to be required to be able to retrieve the semantically opposite (instead of similar). Learning such changes from a small training set is challenging and perhaps a more task-specific methodology is required for this dataset.",
            "Setup . In the discussion below, for simplicity, we assume for any  i  [ n ] i delimited-[] n i\\in[n] italic_i  [ italic_n ]   G i  D i  0  subscript G i subscript D i 0 {\\bm{G}}_{i}\\cdot{\\bm{D}}_{i}\\geq 0 bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  bold_italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  0 . Although the discussion below can be extended to consider  G i  D i < 0  subscript G i subscript D i 0 {\\bm{G}}_{i}\\cdot{\\bm{D}}_{i}<0 bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  bold_italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT < 0 , the results will be more tedious (see Lemma  2 ). Moreover, in practice we expect  G i  D i  0  subscript G i subscript D i 0 {\\bm{G}}_{i}\\cdot{\\bm{D}}_{i}\\geq 0 bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  bold_italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  0  to hold, as otherwise queries and their correct answers will have negative similarity suggesting either a poor training dataset or pre-trained embedding model. We simply set  G i = 0 subscript G i 0 {\\bm{G}}_{i}=\\textbf{0} bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0  whenever  G i  D i < 0  subscript G i subscript D i 0 {\\bm{G}}_{i}\\cdot{\\bm{D}}_{i}<0 bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  bold_italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT < 0 .  Moreover, similar to Appx.  B.2 , we rewrite the  MaxS-N  objective as",
            "First, consider the case simpler setting when   G j  = 0 norm subscript G j 0 \\|{\\bm{G}}_{j}\\|\\neq 0  bold_italic_G start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  = 0  for all  j  [ n ] j delimited-[] n j\\in[n] italic_j  [ italic_n ] . Let     i , j = max  {  i ,  j } subscript    i j subscript  i subscript  j \\bar{\\theta}_{i,j}=\\max\\{\\theta_{i},\\theta_{j}\\} over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = roman_max { italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT }  and     i , j = min  {  i ,  j } subscript    i j subscript  i subscript  j \\underaccent{\\bar}{\\theta}_{i,j}=\\min\\{\\theta_{i},\\theta_{j}\\} under  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = roman_min { italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } . Substituting the values from Lemma  2 , we have Eq.  7  is equivalent to",
            "To find the points satisfying all Eq.  15 - 20 , we consider 4 setting depending on whether   = 0  0 \\lambda=0 italic_ = 0  or   = 0  0 \\mu=0 italic_ = 0  or not.",
            "and therefore   i =   G i  G i  subscript  i  subscript G i norm subscript G i {\\bm{\\Delta}}_{i}=\\sqrt{\\gamma}\\frac{{\\bm{G}}_{i}}{\\|{\\bm{G}}_{i}\\|} bold_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = square-root start_ARG italic_ end_ARG divide start_ARG bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG  bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  end_ARG  (KKT conditions become similar to the ones in Theorem  2 ). Because   > 0  0 \\mu>0 italic_ > 0 , we must have    i  2 =  superscript norm subscript  i 2  \\|{\\bm{\\Delta}}_{i}\\|^{2}=\\gamma  bold_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_  due to Eq.  17 , so that from Eq.  19  we have",
            "Substituting this in Eq. 21 , we have",
            "Now, substituting Eq.  23  in Eq.  17 , we get",
            "Note that we must also have   > 0  0 \\mu>0 italic_ > 0 , so substituting Eq.  24  into Eq.  22  and rearranging we must have",
            "Observe that, for   i  [ 0 ,  2 ] subscript  i 0  2 \\theta_{i}\\in[0,\\frac{\\pi}{2}] italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  [ 0 , divide start_ARG italic_ end_ARG start_ARG 2 end_ARG ] , both  sin   i subscript  i \\sin\\theta_{i} roman_sin italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  cos   i subscript  i \\cos\\theta_{i} roman_cos italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  are non-negative, so that when   < 2  2 \\gamma<2 italic_ < 2  only the positive branch is able to satisfy Eq.  25 . In this case, we have",
            "for   i  [ 0 ,  2 ] subscript  i 0  2 \\theta_{i}\\in[0,\\frac{\\pi}{2}] italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  [ 0 , divide start_ARG italic_ end_ARG start_ARG 2 end_ARG ] , and thus   > 0  0 \\lambda>0 italic_ > 0  is satisfied. Simplifying Eq.  25 , observe that for   i  [ 0 ,  2 ] subscript  i 0  2 \\theta_{i}\\in[0,\\frac{\\pi}{2}] italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  [ 0 , divide start_ARG italic_ end_ARG start_ARG 2 end_ARG ]  and  0 <  < 2 0  2 0<\\gamma<2 0 < italic_ < 2 ,",
            "To summarize, taking the positive branch in Eq.  24 , we showed that",
            "Finally, consider the negative branch in Eq.  24  and   > 2  2 \\gamma>2 italic_ > 2 . To have   > 0  0 \\mu>0 italic_ > 0 , following an argument similar to Eq.  27 , we must have",
            "If there are zero roots,  f  (  ) > 0 f  0 f(\\gamma)>0 italic_f ( italic_ ) > 0  either for all of  ( 0 , 4 ) 0 4 (0,4) ( 0 , 4 )  or none of it, which can be determined by checking the function value at 0 or 4 using Eq.  28  and  29  if  c = 0 c 0 c\\neq 0 italic_c = 0  or  b = 0 b 0 b\\neq 0 italic_b = 0 . If  c = b = 0 c b 0 c=b=0 italic_c = italic_b = 0 , then we can solve  f  (  ) > 0 f  0 f(\\gamma)>0 italic_f ( italic_ ) > 0  based on whether the function has a minimum or maximum using Eq.  30 , or is a constant when  a = 0 a 0 a=0 italic_a = 0 .",
            "If there is one root,   0 subscript  0 \\gamma_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , then  f  (  ) f  f(\\gamma) italic_f ( italic_ )  is either positive after   0 subscript  0 \\gamma_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  or before it. If  c = 0 c 0 c\\neq 0 italic_c = 0 , we can find this by checking  f  ( 0 ) f 0 f(0) italic_f ( 0 )  using Eq.  28 . If  c = 0 c 0 c=0 italic_c = 0 , then  f  (  ) f  f(\\gamma) italic_f ( italic_ )  has two roots, at  0 0  and   0 subscript  0 \\gamma_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , and  f  (  ) > 0 f  0 f(\\gamma)>0 italic_f ( italic_ ) > 0  can be determined based on whether the function has a minimum or maximum using Eq.  30 . Both  c c c italic_c  and  a a a italic_a  cannot be zero, because the function  b   b  b\\gamma italic_b italic_  cannot be zero both at   = 0  0 \\gamma=0 italic_ = 0  and   =  0  subscript  0 \\gamma=\\gamma_{0} italic_ = italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  for   0 = 0 subscript  0 0 \\gamma_{0}\\neq 0 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 0 .",
            "If there are two roots, then  f  (  ) > 0 f  0 f(\\gamma)>0 italic_f ( italic_ ) > 0  either between the two roots, or outside the interval between the two roots, which can be checked based on  f  ( 0 ) f 0 f(0) italic_f ( 0 )  using Eq  28 .  c c c italic_c  cannot be zero because  f  (  ) f  f(\\gamma) italic_f ( italic_ )  cannot be zero at   = 0  0 \\gamma=0 italic_ = 0 ,   =  0  subscript  0 \\gamma=\\gamma_{0} italic_ = italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  and   =  1  subscript  1 \\gamma=\\gamma_{1} italic_ = italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  for 3 distinct values fo  0 ,  0 ,  1 0 subscript  0 subscript  1 0,\\gamma_{0},\\gamma_{1} 0 , italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT .",
            ". To solve BiMax-M and BiMax-N, observe that the above modifications cause marginal changes for  MaxS-M  and  MaxS-N , and only require modifying the definition of  G G {\\bm{G}} bold_italic_G  in the corresponding optimal solutions. However, the solutions to the outer optimization problem in BiMax-M and BiMax-N require further modifications since now a different set of inequalities needs to be solved to find the range of    \\gamma italic_  for which a query is answered correctly. However, each inequality is still of the same form as before (compare Eq.  32  with Eq.  1 ), and thus, the same methodology applies.",
            "Appx.  E.2  contains detailed per dataset results summarized in the papers main body.",
            "Tables  9 - 12  present the per dataset results for the embedding models GTE-L, TE3-L, CLIP-B and CLIP-L. The tables (in addition to Table  6 ) present the detailed results from which Tables  3 - 5  are generated.",
            "To better understand the differences and failure modes of the approaches, Fig.  2  shows the validation and training accuracy for Adaptor, PTFT and  \\name -IM, where T and V respectively signify training and validation sets.",
            "Fig.  2  (a) and (b) show two failure modes for Adaptor, overfitting, and underfitting. Specifically, Fig.  2  (a) shows  \\name  provides much better validation accuracy compared with Adaptors at the same training accuracy, suggesting that Adaptor simply overfits to the training set instead of learning generalizable patterns. Fig.  2  (b) shows the other end of the spectrum, where Adaptor fails to fit the training set at all (while  \\name  both fits the training set and improves validation accuracy). We observed this behavior on large data and query sets. We also observed (but not shown here) that increasing the number of parameters, e.g., by introducing additional layers, did not improve accuracy, suggesting that perhaps using adaptors is a wrong modeling choice. Fig.  2  (c) shows the only dataset where Adaptor performs well, where it both fits the training set and improves validation accuracy."
        ]
    },
    "id_table_3": {
        "caption": "Table 4:  Average results across image datasets grouped by the embedding model used",
        "table": "S4.T3.1",
        "footnotes": [],
        "references": [
            "We formalize the notion of non-parametric embedding fine-tuning and show that the underlying unconstrained non-parametric optimization problem is NP-Hard  (Sec.  3.1 ).",
            "We present  \\name -M and  \\name -N, two methods that optimally solve constrained variations of the problem (Sec.  3.2 ).   \\name -M updates data embeddings to maximize the similarity between the embeddings of queries and their ground-truth data records subject to a bound on the magnitude of change to the embeddings.   \\name -N adopts a similar approach, but additionally constrains the embeddings to be normalized.",
            "Our  \\name  approach views embeddings as parameters of the  k k k italic_k -NN retrieval algorithm, optimizing the embeddings directly to improve retrieval accuracy. In Sec.  3.1 , we formalize the notion of non-parametric embedding fine-tuning by stating two optimization problems, one directly maximizing a retrieval accuracy metric and one maximizing similarity between queries and their ground-truth answers. Maximizing accuracy is the final goal, but similarity is a simpler surrogate to optimize in practice. Nonetheless, we show that the former is NP-hard and the latter is unbounded (i.e., the objective can be improved indefinitely). Moreover, since both optimization problems allow embeddings to arbitrarily change (i.e., are unconstrained), directly solving either problem can lead to overfitting. In Sec.  3.2 , we present  \\name , a family of approaches that solve a combination of constrained variations of the two optimization problems to address generalization and efficiency challenges.",
            "Because of the potential for overfitting and the computational challenges due to NP-hardness, we do not solve either MaxA-EFT or MaxS-EFT directly. Instead, we introduce  \\name , a family of approaches that solve constrained variations of MaxA-EFT and MaxS-EFT, designed to avoid overfitting, while being efficient. We discuss two main approaches,  \\name -M and  \\name -N, in Secs.  3.2.1  and  3.2.2  and present other practical extensions in Appx.  C .",
            "Eq.  3  presents the simple update rule used by  \\name -M to fine-tune data embeddings, using which the fine-tuned embeddings are computed as  D  = D +  M superscript D D superscript  M {\\bm{D}}^{*}={\\bm{D}}+{\\bm{\\Delta}}^{M} bold_italic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = bold_italic_D + bold_ start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT . Observe that  G i subscript G i {\\bm{G}}_{i} bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the sum of query embeddings whose ground-truth answer is  D i subscript D i {\\bm{D}}_{i} bold_italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , so that data embeddings are moved towards the queries for which they are the ground-truth answers. We also note that  O  ( n T  d ) O subscript n T d O(n_{T}d) italic_O ( italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT italic_d )  is the complexity of a single iteration over training data, and  O  ( n V  n  d ) O subscript n V n d O(n_{V}nd) italic_O ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT italic_n italic_d )  is the complexity of calculating validation accuracy once. Thus, ignoring the log term, the above time complexity is equal to a single training and validation iteration for parametric approaches (i.e., Adaptors or PTFT), and has smaller constant factors since it does not perform any model forward passes.",
            "The optimal    superscript  \\gamma^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in Eq.  3  is calculated by solving linear inequalities resulting from the definition of  I i V superscript subscript I i V \\mathcal{I}_{i}^{V} caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT . We provide an overview here and leave the details to Appx.  B.2 . First, note that  MaxS-M  (  ) =   G  G  MaxS-M   G norm G \\texttt{MaxS-M}(\\gamma)=\\gamma\\frac{{\\bm{G}}}{\\|{\\bm{G}}\\|} MaxS-M ( italic_ ) = italic_ divide start_ARG bold_italic_G end_ARG start_ARG  bold_italic_G  end_ARG , found using the KKT points of the optimization problem. Thus, BiMax-M reduces to finding a    \\gamma italic_  that maximizes   i  [ n V ] I i V  (   G  G  ) subscript i delimited-[] subscript n V superscript subscript I i V  G norm G \\sum_{i\\in[n_{V}]}\\mathcal{I}_{i}^{V}(\\gamma\\frac{{\\bm{G}}}{\\|{\\bm{G}}\\|})  start_POSTSUBSCRIPT italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ] end_POSTSUBSCRIPT caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT ( italic_ divide start_ARG bold_italic_G end_ARG start_ARG  bold_italic_G  end_ARG ) .  For each  i  [ n V ] i delimited-[] subscript n V i\\in[n_{V}] italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ] , substituting   =   G  G    G norm G {\\bm{\\Delta}}=\\gamma\\frac{{\\bm{G}}}{\\|{\\bm{G}}\\|} bold_ = italic_ divide start_ARG bold_italic_G end_ARG start_ARG  bold_italic_G  end_ARG  into the definition of  I i V  (  ) superscript subscript I i V  \\mathcal{I}_{i}^{V}({\\bm{\\Delta}}) caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT ( bold_ )  in Eq.  1 , we have  I i V  (   G  G  ) = 1 superscript subscript I i V  G norm G 1 \\mathcal{I}_{i}^{V}(\\gamma\\frac{{\\bm{G}}}{\\|{\\bm{G}}\\|})=1 caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT ( italic_ divide start_ARG bold_italic_G end_ARG start_ARG  bold_italic_G  end_ARG ) = 1  if the following holds, and zero otherwise:",
            "and  G i subscript G i {\\bm{G}}_{i} bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is as defined in Eq.  3 , for an optimally chosen scalar    superscript  \\gamma^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT .",
            "Summary of results . Tables  3 - 5  present our accuracy results averaged across all text or image datasets for different models. Table  6  presents the per dataset results for BGE-S. The per dataset results for other models followed similar trends and are deferred to Appx.  E.2 .",
            "As Tables  3 - 5  show, both  \\name -M and  \\name -N provide significant accuracy gains, providing up to 14.3% NDCG@10 boost over No Fine-Tuning while PTFT and Adaptor only improve NDCG@10 up to 4.0%, when averaged across datasets, for both text and image retrieval. Interestingly, the accuracy gains from  \\name -M and  \\name -N depends on the pre-trained model. GTE-L outperforms TE3-L without fine-tuning, but using  \\name  TE3-L outperforms GTE-L.",
            "Moreover, Table  5  shows the total fine-tuning time to run BGE-S on our text datasets (i.e., time obtain the associated results in Table  3 ) using an Nvidia A100 GPU as well as using 32 core Intel Broadwell CPUs. The reported time excludes the time to embed the data records (which is the same across all methods).  \\name  variants run in 1-2 minutes with GPU and in up to 11 minutes using CPUs, which is, respectively, more than 3 and 11 times faster than Adaptor and more than 200 times faster than PTFT, which cannot be run on CPUs in a reasonable time-frame. For both Adaptor and PTFT, the reported run times are for optimized implementations that include early stopping and optimizations for efficient calculation of validation accuracy (see Appx.  E.1 ).",
            "The results show that parametric approaches are unreliable and fail to provide significant accuracy improvements despite requiring significantly more computational resources. The reported results are after hyperparameter tuning as well as tweaking the loss function (i.e., -L variants). We observed that the latter does help improve (and sometimes worsen) accuracy for parametric approaches on some datasets, with Adaptor-L and PTFT-L providing accuracy improvements on Fever and HotpotQA where Adaptor and PTFT provided none. Meanwhile,  \\name  consistently provides significant accuracy boosts  without any hyperparameter tuning . We provide a detailed discussion of the failure modes of the parametric approaches in Appx.  E.3 .",
            "Appx.  B.1 - B.3 , respectively, provide the proofs of Theorems.  1 - 3 . The proofs of technical lemmas are presented in Appx.  B.4",
            "Now whenever   G j  = 0 norm subscript G j 0 \\|{\\bm{G}}_{j}\\|=0  bold_italic_G start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  = 0  for any  j  [ n ] j delimited-[] n j\\in[n] italic_j  [ italic_n ] , Eq.  7  changes since we need to also consider   Y i V  = 0 superscript subscript  superscript subscript Y i V  0 {\\bm{\\Delta}}_{Y_{i}^{V}}^{\\gamma}=0 bold_ start_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT = 0 ,   j  = 0 superscript subscript  j  0 {\\bm{\\Delta}}_{j}^{\\gamma}=0 bold_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT = 0  or both. Each case leads to a similar set of inequalities to Eq.  8 , which are similarly solved using Lemm  3 . Moreover, we calculate validation accuracy separately for when   = 0  0 \\gamma=0 italic_ = 0 .",
            "-N .  \\name -N follows the above procedure. It first calculates  G G {\\bm{G}} bold_italic_G  and  Z Z {\\bm{Z}} bold_italic_Z , so that for the  i i i italic_i -th query, and  j j j italic_j -th data points,  j = Y i V j superscript subscript Y i V j\\neq Y_{i}^{V} italic_j = italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT  applying Lemma  3  to Eq.  8  gives  I i , j subscript I i j I_{i,j} italic_I start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT . Then, it computes  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  by a single pass over  I i , j subscript I i j I_{i,j} italic_I start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  values, and finally finds a    \\gamma italic_  value that intersects most  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT s by sorting their beginning and end and iteration through the list.",
            "Substituting this into Eq.  13 , and since   > 0  0 \\mu>0 italic_ > 0 , we have",
            "Now, substituting Eq.  23  in Eq.  17 , we get",
            "If there are zero roots,  f  (  ) > 0 f  0 f(\\gamma)>0 italic_f ( italic_ ) > 0  either for all of  ( 0 , 4 ) 0 4 (0,4) ( 0 , 4 )  or none of it, which can be determined by checking the function value at 0 or 4 using Eq.  28  and  29  if  c = 0 c 0 c\\neq 0 italic_c = 0  or  b = 0 b 0 b\\neq 0 italic_b = 0 . If  c = b = 0 c b 0 c=b=0 italic_c = italic_b = 0 , then we can solve  f  (  ) > 0 f  0 f(\\gamma)>0 italic_f ( italic_ ) > 0  based on whether the function has a minimum or maximum using Eq.  30 , or is a constant when  a = 0 a 0 a=0 italic_a = 0 .",
            "If there is one root,   0 subscript  0 \\gamma_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , then  f  (  ) f  f(\\gamma) italic_f ( italic_ )  is either positive after   0 subscript  0 \\gamma_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  or before it. If  c = 0 c 0 c\\neq 0 italic_c = 0 , we can find this by checking  f  ( 0 ) f 0 f(0) italic_f ( 0 )  using Eq.  28 . If  c = 0 c 0 c=0 italic_c = 0 , then  f  (  ) f  f(\\gamma) italic_f ( italic_ )  has two roots, at  0 0  and   0 subscript  0 \\gamma_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , and  f  (  ) > 0 f  0 f(\\gamma)>0 italic_f ( italic_ ) > 0  can be determined based on whether the function has a minimum or maximum using Eq.  30 . Both  c c c italic_c  and  a a a italic_a  cannot be zero, because the function  b   b  b\\gamma italic_b italic_  cannot be zero both at   = 0  0 \\gamma=0 italic_ = 0  and   =  0  subscript  0 \\gamma=\\gamma_{0} italic_ = italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  for   0 = 0 subscript  0 0 \\gamma_{0}\\neq 0 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 0 .",
            "-IM performs gradient descent on loss in Eq.  31  with learning rate    \\alpha italic_  for  t t t italic_t  iterations (   \\alpha italic_  and  t t t italic_t  determined through hyperparameter tuning to maximize validation accuracy) with normalized gradients:",
            ". To solve BiMax-M and BiMax-N, observe that the above modifications cause marginal changes for  MaxS-M  and  MaxS-N , and only require modifying the definition of  G G {\\bm{G}} bold_italic_G  in the corresponding optimal solutions. However, the solutions to the outer optimization problem in BiMax-M and BiMax-N require further modifications since now a different set of inequalities needs to be solved to find the range of    \\gamma italic_  for which a query is answered correctly. However, each inequality is still of the same form as before (compare Eq.  32  with Eq.  1 ), and thus, the same methodology applies.",
            "Appx.  E.3  presents experiments on the training processes of Adaptors and PTFT to understand their failure modes.",
            "Tables  9 - 12  present the per dataset results for the embedding models GTE-L, TE3-L, CLIP-B and CLIP-L. The tables (in addition to Table  6 ) present the detailed results from which Tables  3 - 5  are generated.",
            "We compare  \\name -N, with  \\name -IN (as described in Sec.  C ) with two other potential variants to understand the impact of normalization.  \\name -M+N is a variant that first performs  \\name -M and then normalizes embeddings post-hoc.  \\name -IM+R is a variation of  \\name -IM with L2 regularization added to the loss to penalize embeddings with large norms. Table  13  shows the results for this experiment, showing that normalizing embeddings after optimization, i.e.,  \\name -M+N performs worse than when normalization is considered as part of optimization, which is the case for both  \\name -IN and  \\name -N.  \\name -IM+R performs worse than all methods, showing an advantage for enforcing a normalization constraint over L2 regularization. Meanwhile,  \\name -IN and  \\name -N perform similarly (see Sec.  C  for a discussion between the two)."
        ]
    },
    "id_table_4": {
        "caption": "Table 5:  Run time using BGE-S",
        "table": "S4.T5.fig1.1",
        "footnotes": [],
        "references": [
            "We show  \\name  variants consistently outperform parametric fine-tuning methods with thorough experiments on 5 embedding models and 9 standard retrieval datasets (Sec.  4 ).",
            "Eq.  4  is a set of inequalities in    \\gamma italic_ . Denote the solution to the inequalities by  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , so that    I i  subscript I i \\gamma\\in I_{i} italic_  italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  if and only if  I i V  (   G  G  ) = 1 superscript subscript I i V  G norm G 1 \\mathcal{I}_{i}^{V}(\\gamma\\frac{{\\bm{G}}}{\\|{\\bm{G}}\\|})=1 caligraphic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT ( italic_ divide start_ARG bold_italic_G end_ARG start_ARG  bold_italic_G  end_ARG ) = 1 . Since the inequalities are linear in    \\gamma italic_ ,  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is an interval in  R R \\mathcal{R} caligraphic_R . Consequently,",
            "-N additionally constrains the norm of the fine-tuned embedding. This constraint serves as an additional regularization, helping with out-of-distribution generalization (see Sec. 4.2 ).",
            "To better understand the results, we remark on the performance on ArguAna and Fever datasets. Fever, where  \\name -M performs better than  \\name -N has a skewed label distribution, where the same paragraph is the correct answer for many queries. This allows for setting the magnitude of the embeddings based on label distribution, assigning embeddings with larger magnitudes to more frequently accessed passages. Such an assignment can improve the accuracy when the label distribution is fixed and skewed, but leads to worse generalization when there is no skew (as the results on other datasets in Table  6  show) or when there is a distribution shift (see Sec.  4.2 ). Finally, in ArguAna, each data record is a factual argument and each query provides an argument and asks for a counterargument to the given argument. In such a setting, to improve accuracy, we expect larger systematic changes to the embedding space to be required to be able to retrieve the semantically opposite (instead of similar). Learning such changes from a small training set is challenging and perhaps a more task-specific methodology is required for this dataset.",
            "Average results across text datasets and with BGE-S model are shown in Table  4 . As the table shows,  \\name -N performs the best on the out-of-distribution test set, even outperforming No Fine-Tuning, while providing a significant accuracy boost on the in-distribution samples. Although  \\name -M performs well on in-distribution samples, its performance deteriorates on out-of-distribution queries. The main difference between  \\name -M and  \\name -N is that  \\name -Ms embeddings are not normalized. Thus, when retrieving top- k k k italic_k  results using inner product as similarity metric, a fine-tuned embedding with large magnitude can adversely impact the query answers for out-of-distribution queries. However, by keeping embeddings normalized,  \\name -N ensures fine-tuned embeddings do not change the answer to queries that are far from fine-tuned data records, thus avoiding performance degradation on out-of-distribution queries. Finally, Adaptor provides little gain on in-distribution queries, while worsening accuracy on out-of-distribution samples.",
            "Appx.  B.1 - B.3 , respectively, provide the proofs of Theorems.  1 - 3 . The proofs of technical lemmas are presented in Appx.  B.4",
            "and substituting back into Eq.  14 , we have",
            "Note that we must also have   > 0  0 \\mu>0 italic_ > 0 , so substituting Eq.  24  into Eq.  22  and rearranging we must have",
            "To summarize, taking the positive branch in Eq.  24 , we showed that",
            "Finally, consider the negative branch in Eq.  24  and   > 2  2 \\gamma>2 italic_ > 2 . To have   > 0  0 \\mu>0 italic_ > 0 , following an argument similar to Eq.  27 , we must have",
            "Appx.  E.4  provides an ablation study of various normalization methods in  \\name ."
        ]
    },
    "id_table_5": {
        "caption": "Table 6:  NDCG@10 results for using BGE-S on text datasets",
        "table": "S4.T5.fig2.1",
        "footnotes": [],
        "references": [
            "and the solution to Eq.  5  is a    \\gamma italic_  that intersects the most number of intervals across all  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,  i  [ n V ] i delimited-[] subscript n V i\\in[n_{V}] italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ] , which can be found by a single iteration over the intervals after sorting their start and end points.",
            "Summary of results . Tables  3 - 5  present our accuracy results averaged across all text or image datasets for different models. Table  6  presents the per dataset results for BGE-S. The per dataset results for other models followed similar trends and are deferred to Appx.  E.2 .",
            "As Tables  3 - 5  show, both  \\name -M and  \\name -N provide significant accuracy gains, providing up to 14.3% NDCG@10 boost over No Fine-Tuning while PTFT and Adaptor only improve NDCG@10 up to 4.0%, when averaged across datasets, for both text and image retrieval. Interestingly, the accuracy gains from  \\name -M and  \\name -N depends on the pre-trained model. GTE-L outperforms TE3-L without fine-tuning, but using  \\name  TE3-L outperforms GTE-L.",
            "Moreover, Table  5  shows the total fine-tuning time to run BGE-S on our text datasets (i.e., time obtain the associated results in Table  3 ) using an Nvidia A100 GPU as well as using 32 core Intel Broadwell CPUs. The reported time excludes the time to embed the data records (which is the same across all methods).  \\name  variants run in 1-2 minutes with GPU and in up to 11 minutes using CPUs, which is, respectively, more than 3 and 11 times faster than Adaptor and more than 200 times faster than PTFT, which cannot be run on CPUs in a reasonable time-frame. For both Adaptor and PTFT, the reported run times are for optimized implementations that include early stopping and optimizations for efficient calculation of validation accuracy (see Appx.  E.1 ).",
            "-M . Alg.  1  presents  \\name -M, which formalizes the above procedure, and also incorporates the constraint    0  0 \\gamma\\geq 0 italic_  0 . Lines  6 - 15  find the intervals  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,   i  [ n V ] for-all i delimited-[] subscript n V \\forall i\\in[n_{V}]  italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ]  and add it to a list  I I I italic_I  (intersection of half intervals can be found by calculating the maximum of lower bounds and minimum of upper bounds of the intervals, done in lines  9 -  11 ). The algorithm returns new data embeddings by simply performing a single addition.",
            "Time complexity . Alg.  1  can be implemented so that lines  11 - 7  with a single pass over the dataset. Therefore, finding  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  for all queries (lines  6 - 15 ) take  O  ( n V  n  d ) O subscript n V n d O(n_{V}\\times n\\times d) italic_O ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT  italic_n  italic_d ) . Calculating  G G G italic_G  takes time  O  ( n T  d ) O subscript n T d O(n_{T}\\times d) italic_O ( italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  italic_d ) . Finding    superscript  \\gamma^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in line  16  is the basic problem of finding the maximum number of overlapping ranges and can be done in  O  ( n V  log  ( n V ) ) O subscript n V subscript n V O(n_{V}\\log(n_{V})) italic_O ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT roman_log ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ) ) , by first sorting the ranges in  I I I italic_I  based on their lower bound and iteratively traversing the sorted list and keeping track of the number of overlapping ranges. Thus, Alg.  1  can be implemented in  O ( n V ( n  d + log ( n V ) + n T  d ) O(n_{V}(n\\times d+\\log(n_{V})+n_{T}\\times d) italic_O ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ( italic_n  italic_d + roman_log ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ) + italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  italic_d ) .",
            "where Eq.  15  is obtained by setting the gradient of the Lagrangian to zero, and Eq.  19  and   16  are obtained by substituting",
            "To find the points satisfying all Eq.  15 - 20 , we consider 4 setting depending on whether   = 0  0 \\lambda=0 italic_ = 0  or   = 0  0 \\mu=0 italic_ = 0  or not.",
            "Case 1,   = 0 ,  = 0 formulae-sequence  0  0 \\lambda=0,\\mu=0 italic_ = 0 , italic_ = 0 . Substituting   = 0  0 \\lambda=0 italic_ = 0  and   = 0  0 \\mu=0 italic_ = 0  in Eq.  15 , we have  G i = 0 subscript G i 0 {\\bm{G}}_{i}=\\mathbf{0} bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_0 . Thus, no solution with   = 0  0 \\lambda=0 italic_ = 0  and   = 0  0 \\mu=0 italic_ = 0  exists since  G i = 0 subscript G i 0 {\\bm{G}}_{i}\\neq\\mathbf{0} bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_0 .",
            "Case 2,   = 0 ,  > 0 formulae-sequence  0  0 \\lambda=0,\\mu>0 italic_ = 0 , italic_ > 0 . Having   = 0  0 \\lambda=0 italic_ = 0  and   > 0  0 \\mu>0 italic_ > 0 , we have from Eq.  15",
            "Case 3,   > 0 ,  = 0 formulae-sequence  0  0 \\lambda>0,\\mu=0 italic_ > 0 , italic_ = 0 . Substituting   = 0  0 \\mu=0 italic_ = 0  in Eq.  15  we have",
            "Case 4,   > 0 ,  > 0 formulae-sequence  0  0 \\lambda>0,\\mu>0 italic_ > 0 , italic_ > 0 . From Eq.  15 , we have",
            "Observe that, for   i  [ 0 ,  2 ] subscript  i 0  2 \\theta_{i}\\in[0,\\frac{\\pi}{2}] italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  [ 0 , divide start_ARG italic_ end_ARG start_ARG 2 end_ARG ] , both  sin   i subscript  i \\sin\\theta_{i} roman_sin italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  cos   i subscript  i \\cos\\theta_{i} roman_cos italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  are non-negative, so that when   < 2  2 \\gamma<2 italic_ < 2  only the positive branch is able to satisfy Eq.  25 . In this case, we have",
            "for   i  [ 0 ,  2 ] subscript  i 0  2 \\theta_{i}\\in[0,\\frac{\\pi}{2}] italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  [ 0 , divide start_ARG italic_ end_ARG start_ARG 2 end_ARG ] , and thus   > 0  0 \\lambda>0 italic_ > 0  is satisfied. Simplifying Eq.  25 , observe that for   i  [ 0 ,  2 ] subscript  i 0  2 \\theta_{i}\\in[0,\\frac{\\pi}{2}] italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  [ 0 , divide start_ARG italic_ end_ARG start_ARG 2 end_ARG ]  and  0 <  < 2 0  2 0<\\gamma<2 0 < italic_ < 2 ,",
            "The above lemma implies using gradient descent with suitable    \\alpha italic_  and  t t t italic_t  can also provide accurate solutions, but at the cost of efficiency due to iterative updates and hyperparameter tuning (instead of using the closed-form solutions), and the added challenge of finding suitable    \\alpha italic_  and  t t t italic_t . Appx.  E.5  presents an experimental study of these trade-offs.",
            "Appx.  E.5  provides an experimental comparison between  \\name -M and its corresponding iterative variant  \\name -IM.",
            "Tables  9 - 12  present the per dataset results for the embedding models GTE-L, TE3-L, CLIP-B and CLIP-L. The tables (in addition to Table  6 ) present the detailed results from which Tables  3 - 5  are generated."
        ]
    },
    "id_table_6": {
        "caption": "Table 7:  Distribution shift results using BGE-S, average over text datasets",
        "table": "S4.T6.1",
        "footnotes": [],
        "references": [
            "Eq.  6  is the update rule used by  \\name -N to fine-tune data embeddings, using which the fine-tuned embeddings are obtained as  D  = D +  N superscript D D superscript  N {\\bm{D}}^{*}={\\bm{D}}+{\\bm{\\Delta}}^{N} bold_italic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = bold_italic_D + bold_ start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT . This moves the data embedding on the unit ball (to satisfy   D i +  i  = 1 norm subscript D i subscript  i 1 \\|{\\bm{D}}_{i}+{\\bm{\\Delta}}_{i}\\|=1  bold_italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  = 1 ) between  D i subscript D i {\\bm{D}}_{i} bold_italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  G i  G i  subscript G i norm subscript G i \\frac{{\\bm{G}}_{i}}{\\|{\\bm{G}}_{i}\\|} divide start_ARG bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG  bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  end_ARG , where  G i  G i  subscript G i norm subscript G i \\frac{{\\bm{G}}_{i}}{\\|{\\bm{G}}_{i}\\|} divide start_ARG bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG  bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  end_ARG  is the normalized sum of embeddings of queries whose ground-truth answer is  D i subscript D i {\\bm{D}}_{i} bold_italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .    superscript  \\gamma^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  determines how much to move the embedding, and  Z i subscript Z i {\\bm{Z}}_{i} bold_italic_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  determines the direction.  Z i subscript Z i {\\bm{Z}}_{i} bold_italic_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the normalized projection of  G i subscript G i {\\bm{G}}_{i} bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  onto the tangent plane of the unit ball at  D i subscript D i {\\bm{D}}_{i} bold_italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , so that moving in the direction of  Z i subscript Z i {\\bm{Z}}_{i} bold_italic_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  maximally increases  MaxS-N  objective.",
            "Baselines . We report results using the embeddings without fine-tuning, called No Fine-Tuning, in addition to training Adaptors and fine-tuning the pre-trained model, referred to as PTFT (see Appx.  E.1  for implementation details). Due to computational constraints, we report results for the latter only for the small open-source model BGE-S in our main experiments. For both, we present two versions. By default, we use the Multiple Negative Ranking (MNR) loss  (Henderson et al.,  2017 )  for training, which is the standard contrastive fine-tuning loss when positive query/answer pairs are available, suggested by SentenceTransformers  (SentenceTransformers,  2024a )  and used by LlamaIndex  (LlamaIndex,  2024d )  for fine-tuning (although LlamaIndex only uses a single positive example per query  (LlamaIndex,  2024e ) ). Despite our hyperparameter tuning effort, we observed no accuracy improvements on some datasets through fine-tuning with this loss (see Table  6 ). We then modified the loss, so that only negative samples whose cosine similarity is at least equal to a threshold are included in the loss (see  E.1  for details), which improved accuracy on some datasets but worsened it on other. We use the suffix -L to denote the baselines using this modified loss. We report results using this loss to gain more insight into the baselines behavior. We use cosine similarity as the retrieval distance metric for No Fine-Tuning, Adaptor, and PTFT.",
            "Summary of results . Tables  3 - 5  present our accuracy results averaged across all text or image datasets for different models. Table  6  presents the per dataset results for BGE-S. The per dataset results for other models followed similar trends and are deferred to Appx.  E.2 .",
            "Detailed Results . Table  6  shows the detailed retrieval accuracy on text datasets for BGE-S.  \\name  significantly outperforms parametric methods on almost all datasets.  \\name -N outperforms  \\name -M on most datasets, showing the benefit of constraining embeddings to be normalized.",
            "To better understand the results, we remark on the performance on ArguAna and Fever datasets. Fever, where  \\name -M performs better than  \\name -N has a skewed label distribution, where the same paragraph is the correct answer for many queries. This allows for setting the magnitude of the embeddings based on label distribution, assigning embeddings with larger magnitudes to more frequently accessed passages. Such an assignment can improve the accuracy when the label distribution is fixed and skewed, but leads to worse generalization when there is no skew (as the results on other datasets in Table  6  show) or when there is a distribution shift (see Sec.  4.2 ). Finally, in ArguAna, each data record is a factual argument and each query provides an argument and asks for a counterargument to the given argument. In such a setting, to improve accuracy, we expect larger systematic changes to the embedding space to be required to be able to retrieve the semantically opposite (instead of similar). Learning such changes from a small training set is challenging and perhaps a more task-specific methodology is required for this dataset.",
            "We provide an ablation study to better understand the impact of various constraints in  \\name . In addition to  \\name -M and  \\name -N, we present  \\name -M+N, which performs normalization on the output of  \\name -M. That is, instead of incorporating normalization as a constraint in the optimization problem,  \\name -M+N simply normalizes the embeddings after performing  \\name -M. We also present  \\name -NU, which is a variation of  \\name -N that only includes the normalization constraint, but not the constraint on the magnitude of the change in the embeddings (the solution is equivalent to the first branch of Eq.  6 ).",
            "-M . Alg.  1  presents  \\name -M, which formalizes the above procedure, and also incorporates the constraint    0  0 \\gamma\\geq 0 italic_  0 . Lines  6 - 15  find the intervals  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,   i  [ n V ] for-all i delimited-[] subscript n V \\forall i\\in[n_{V}]  italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ]  and add it to a list  I I I italic_I  (intersection of half intervals can be found by calculating the maximum of lower bounds and minimum of upper bounds of the intervals, done in lines  9 -  11 ). The algorithm returns new data embeddings by simply performing a single addition.",
            "Time complexity . Alg.  1  can be implemented so that lines  11 - 7  with a single pass over the dataset. Therefore, finding  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  for all queries (lines  6 - 15 ) take  O  ( n V  n  d ) O subscript n V n d O(n_{V}\\times n\\times d) italic_O ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT  italic_n  italic_d ) . Calculating  G G G italic_G  takes time  O  ( n T  d ) O subscript n T d O(n_{T}\\times d) italic_O ( italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  italic_d ) . Finding    superscript  \\gamma^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in line  16  is the basic problem of finding the maximum number of overlapping ranges and can be done in  O  ( n V  log  ( n V ) ) O subscript n V subscript n V O(n_{V}\\log(n_{V})) italic_O ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT roman_log ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ) ) , by first sorting the ranges in  I I I italic_I  based on their lower bound and iteratively traversing the sorted list and keeping track of the number of overlapping ranges. Thus, Alg.  1  can be implemented in  O ( n V ( n  d + log ( n V ) + n T  d ) O(n_{V}(n\\times d+\\log(n_{V})+n_{T}\\times d) italic_O ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ( italic_n  italic_d + roman_log ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ) + italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  italic_d ) .",
            "where Eq.  15  is obtained by setting the gradient of the Lagrangian to zero, and Eq.  19  and   16  are obtained by substituting",
            "Substituting this in Eq.  16 , we have",
            "and since    i    = 0 norm subscript  i  0 \\|{\\bm{\\Delta}}_{i}\\|-\\gamma=0  bold_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  - italic_ = 0 , Eq.  16  simplifies to   + 2   i  D i = 0   2 subscript  i subscript D i 0 \\gamma+2{\\bm{\\Delta}}_{i}\\cdot{\\bm{D}}_{i}=0 italic_ + 2 bold_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  bold_italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0 , so that",
            "Tables  9 - 12  present the per dataset results for the embedding models GTE-L, TE3-L, CLIP-B and CLIP-L. The tables (in addition to Table  6 ) present the detailed results from which Tables  3 - 5  are generated.",
            "PTFT, on the other hand, has a much smaller generalization gap compared with Adaptor. However, both validation and training accuracy increase at a much slower pace, and eventually plateau. Especially for NQ, we observe that the model underfits the training set. We hypothesized that one reason could be due to the loss function used, where indeed Table.  6  shows our attempt at modifying the loss function does help improve accuracy on NQ, but not consistently across datasets (and worsens the accuracy on other datasets)."
        ]
    },
    "id_table_7": {
        "caption": "Table 8:  Ablation of  \\name",
        "table": "S4.1.fig1.1",
        "footnotes": [],
        "references": [
            "Time complexity . Alg.  1  can be implemented so that lines  11 - 7  with a single pass over the dataset. Therefore, finding  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  for all queries (lines  6 - 15 ) take  O  ( n V  n  d ) O subscript n V n d O(n_{V}\\times n\\times d) italic_O ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT  italic_n  italic_d ) . Calculating  G G G italic_G  takes time  O  ( n T  d ) O subscript n T d O(n_{T}\\times d) italic_O ( italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  italic_d ) . Finding    superscript  \\gamma^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in line  16  is the basic problem of finding the maximum number of overlapping ranges and can be done in  O  ( n V  log  ( n V ) ) O subscript n V subscript n V O(n_{V}\\log(n_{V})) italic_O ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT roman_log ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ) ) , by first sorting the ranges in  I I I italic_I  based on their lower bound and iteratively traversing the sorted list and keeping track of the number of overlapping ranges. Thus, Alg.  1  can be implemented in  O ( n V ( n  d + log ( n V ) + n T  d ) O(n_{V}(n\\times d+\\log(n_{V})+n_{T}\\times d) italic_O ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ( italic_n  italic_d + roman_log ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ) + italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  italic_d ) .",
            "First, consider the case simpler setting when   G j  = 0 norm subscript G j 0 \\|{\\bm{G}}_{j}\\|\\neq 0  bold_italic_G start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  = 0  for all  j  [ n ] j delimited-[] n j\\in[n] italic_j  [ italic_n ] . Let     i , j = max  {  i ,  j } subscript    i j subscript  i subscript  j \\bar{\\theta}_{i,j}=\\max\\{\\theta_{i},\\theta_{j}\\} over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = roman_max { italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT }  and     i , j = min  {  i ,  j } subscript    i j subscript  i subscript  j \\underaccent{\\bar}{\\theta}_{i,j}=\\min\\{\\theta_{i},\\theta_{j}\\} under  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = roman_min { italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } . Substituting the values from Lemma  2 , we have Eq.  7  is equivalent to",
            "Thus, for the  i i i italic_i -th query and the  j j j italic_j -th data record,  i  [ n V ] i delimited-[] subscript n V i\\in[n_{V}] italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ] ,  j  [ n ]  Y i V j delimited-[] n superscript subscript Y i V j\\in[n]\\setminus Y_{i}^{V} italic_j  [ italic_n ]  italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT , we can use Eq.  9  together with Eq.  8  to obtain a set  I i , j  ( 0 , 4 ) subscript I i j 0 4 I_{i,j}\\subseteq{(0,4)} italic_I start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  ( 0 , 4 )  such that Eq.  7  holds if and only if    I i , j  subscript I i j \\gamma\\in I_{i,j} italic_  italic_I start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT . Moreover,  I i , j subscript I i j I_{i,j} italic_I start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  will consist of at most 3 intervals in   ( 0 , 4 ) absent 0 4 \\subseteq{(0,4)}  ( 0 , 4 ) . Let  I i =  j  [ n ] = Y i V I i , j subscript I i subscript j delimited-[] n superscript subscript Y i V subscript I i j I_{i}=\\cap_{j\\in[n]\\neq Y_{i}^{V}}I_{i,j} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =  start_POSTSUBSCRIPT italic_j  [ italic_n ] = italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_I start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT , and note that the  i i i italic_i -th query will be answered accurately if and only if    I i  subscript I i \\gamma\\in I_{i} italic_  italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . Finally,    = arg  max    i  [ n V ] I  [   I i ] superscript  subscript  subscript i delimited-[] subscript n V I delimited-[]  subscript I i \\gamma^{*}=\\arg\\max_{\\gamma}\\sum_{i\\in[n_{V}]}\\mathds{I}[\\gamma\\in I_{i}] italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = roman_arg roman_max start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ] end_POSTSUBSCRIPT blackboard_I [ italic_  italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ]  is an optimal solution to the MAxSS problem.",
            "Now whenever   G j  = 0 norm subscript G j 0 \\|{\\bm{G}}_{j}\\|=0  bold_italic_G start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  = 0  for any  j  [ n ] j delimited-[] n j\\in[n] italic_j  [ italic_n ] , Eq.  7  changes since we need to also consider   Y i V  = 0 superscript subscript  superscript subscript Y i V  0 {\\bm{\\Delta}}_{Y_{i}^{V}}^{\\gamma}=0 bold_ start_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT = 0 ,   j  = 0 superscript subscript  j  0 {\\bm{\\Delta}}_{j}^{\\gamma}=0 bold_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT = 0  or both. Each case leads to a similar set of inequalities to Eq.  8 , which are similarly solved using Lemm  3 . Moreover, we calculate validation accuracy separately for when   = 0  0 \\gamma=0 italic_ = 0 .",
            "substituting which into Eq.  17  with   > 0  0 \\mu>0 italic_ > 0  w have",
            "and therefore   i =   G i  G i  subscript  i  subscript G i norm subscript G i {\\bm{\\Delta}}_{i}=\\sqrt{\\gamma}\\frac{{\\bm{G}}_{i}}{\\|{\\bm{G}}_{i}\\|} bold_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = square-root start_ARG italic_ end_ARG divide start_ARG bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG  bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  end_ARG  (KKT conditions become similar to the ones in Theorem  2 ). Because   > 0  0 \\mu>0 italic_ > 0 , we must have    i  2 =  superscript norm subscript  i 2  \\|{\\bm{\\Delta}}_{i}\\|^{2}=\\gamma  bold_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_  due to Eq.  17 , so that from Eq.  19  we have",
            "Now, substituting Eq.  23  in Eq.  17 , we get",
            "Finally, consider the negative branch in Eq.  24  and   > 2  2 \\gamma>2 italic_ > 2 . To have   > 0  0 \\mu>0 italic_ > 0 , following an argument similar to Eq.  27 , we must have"
        ]
    },
    "id_table_8": {
        "caption": "Table 9:  NDCG@10 results for GTE-L on text datasets",
        "table": "S4.2.fig2.1",
        "footnotes": [],
        "references": [
            "Table  8  shows the results of comparing the above variants across text datasets using the BGE-S model. The table shows normalizing the embeddings as part of the optimization ( \\name -N) is better than simply normalizing the output after optimization ( \\name -M+N), although both work better than using unnormalized embeddings ( \\name -M). Moreover, allowing the magnitude of change to the embeddings to be unbounded ( \\name -NU) performs worse than all other variants, showing the benefit of constraining how much embeddings can change during fine-tuning. Overall, comparing  \\name -N, No Fine-Tuning, and all other variants, we see that the benefits of  \\name -N come from a combination of all design choices made.",
            "Observe that finding the values of    \\gamma italic_  for which the inequalities in Eq.  8  hold requires solving inequalities of the form   a    ( 4   ) + b   + c > 0 , a  4  b  c 0 a\\sqrt{\\gamma(4-\\gamma)}+b\\gamma+c>0, italic_a square-root start_ARG italic_ ( 4 - italic_ ) end_ARG + italic_b italic_ + italic_c > 0 ,  for some  a , b , c  R a b c R a,b,c\\in\\mathcal{R} italic_a , italic_b , italic_c  caligraphic_R .",
            "Thus, for the  i i i italic_i -th query and the  j j j italic_j -th data record,  i  [ n V ] i delimited-[] subscript n V i\\in[n_{V}] italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ] ,  j  [ n ]  Y i V j delimited-[] n superscript subscript Y i V j\\in[n]\\setminus Y_{i}^{V} italic_j  [ italic_n ]  italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT , we can use Eq.  9  together with Eq.  8  to obtain a set  I i , j  ( 0 , 4 ) subscript I i j 0 4 I_{i,j}\\subseteq{(0,4)} italic_I start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  ( 0 , 4 )  such that Eq.  7  holds if and only if    I i , j  subscript I i j \\gamma\\in I_{i,j} italic_  italic_I start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT . Moreover,  I i , j subscript I i j I_{i,j} italic_I start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  will consist of at most 3 intervals in   ( 0 , 4 ) absent 0 4 \\subseteq{(0,4)}  ( 0 , 4 ) . Let  I i =  j  [ n ] = Y i V I i , j subscript I i subscript j delimited-[] n superscript subscript Y i V subscript I i j I_{i}=\\cap_{j\\in[n]\\neq Y_{i}^{V}}I_{i,j} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =  start_POSTSUBSCRIPT italic_j  [ italic_n ] = italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_I start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT , and note that the  i i i italic_i -th query will be answered accurately if and only if    I i  subscript I i \\gamma\\in I_{i} italic_  italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . Finally,    = arg  max    i  [ n V ] I  [   I i ] superscript  subscript  subscript i delimited-[] subscript n V I delimited-[]  subscript I i \\gamma^{*}=\\arg\\max_{\\gamma}\\sum_{i\\in[n_{V}]}\\mathds{I}[\\gamma\\in I_{i}] italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = roman_arg roman_max start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ] end_POSTSUBSCRIPT blackboard_I [ italic_  italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ]  is an optimal solution to the MAxSS problem.",
            "Now whenever   G j  = 0 norm subscript G j 0 \\|{\\bm{G}}_{j}\\|=0  bold_italic_G start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  = 0  for any  j  [ n ] j delimited-[] n j\\in[n] italic_j  [ italic_n ] , Eq.  7  changes since we need to also consider   Y i V  = 0 superscript subscript  superscript subscript Y i V  0 {\\bm{\\Delta}}_{Y_{i}^{V}}^{\\gamma}=0 bold_ start_POSTSUBSCRIPT italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT = 0 ,   j  = 0 superscript subscript  j  0 {\\bm{\\Delta}}_{j}^{\\gamma}=0 bold_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT = 0  or both. Each case leads to a similar set of inequalities to Eq.  8 , which are similarly solved using Lemm  3 . Moreover, we calculate validation accuracy separately for when   = 0  0 \\gamma=0 italic_ = 0 .",
            "-N .  \\name -N follows the above procedure. It first calculates  G G {\\bm{G}} bold_italic_G  and  Z Z {\\bm{Z}} bold_italic_Z , so that for the  i i i italic_i -th query, and  j j j italic_j -th data points,  j = Y i V j superscript subscript Y i V j\\neq Y_{i}^{V} italic_j = italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT  applying Lemma  3  to Eq.  8  gives  I i , j subscript I i j I_{i,j} italic_I start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT . Then, it computes  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  by a single pass over  I i , j subscript I i j I_{i,j} italic_I start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  values, and finally finds a    \\gamma italic_  value that intersects most  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT s by sorting their beginning and end and iteration through the list.",
            "Note that to satisfy Eq.  18 , we must have",
            "If there are zero roots,  f  (  ) > 0 f  0 f(\\gamma)>0 italic_f ( italic_ ) > 0  either for all of  ( 0 , 4 ) 0 4 (0,4) ( 0 , 4 )  or none of it, which can be determined by checking the function value at 0 or 4 using Eq.  28  and  29  if  c = 0 c 0 c\\neq 0 italic_c = 0  or  b = 0 b 0 b\\neq 0 italic_b = 0 . If  c = b = 0 c b 0 c=b=0 italic_c = italic_b = 0 , then we can solve  f  (  ) > 0 f  0 f(\\gamma)>0 italic_f ( italic_ ) > 0  based on whether the function has a minimum or maximum using Eq.  30 , or is a constant when  a = 0 a 0 a=0 italic_a = 0 .",
            "If there is one root,   0 subscript  0 \\gamma_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , then  f  (  ) f  f(\\gamma) italic_f ( italic_ )  is either positive after   0 subscript  0 \\gamma_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  or before it. If  c = 0 c 0 c\\neq 0 italic_c = 0 , we can find this by checking  f  ( 0 ) f 0 f(0) italic_f ( 0 )  using Eq.  28 . If  c = 0 c 0 c=0 italic_c = 0 , then  f  (  ) f  f(\\gamma) italic_f ( italic_ )  has two roots, at  0 0  and   0 subscript  0 \\gamma_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , and  f  (  ) > 0 f  0 f(\\gamma)>0 italic_f ( italic_ ) > 0  can be determined based on whether the function has a minimum or maximum using Eq.  30 . Both  c c c italic_c  and  a a a italic_a  cannot be zero, because the function  b   b  b\\gamma italic_b italic_  cannot be zero both at   = 0  0 \\gamma=0 italic_ = 0  and   =  0  subscript  0 \\gamma=\\gamma_{0} italic_ = italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  for   0 = 0 subscript  0 0 \\gamma_{0}\\neq 0 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 0 .",
            "If there are two roots, then  f  (  ) > 0 f  0 f(\\gamma)>0 italic_f ( italic_ ) > 0  either between the two roots, or outside the interval between the two roots, which can be checked based on  f  ( 0 ) f 0 f(0) italic_f ( 0 )  using Eq  28 .  c c c italic_c  cannot be zero because  f  (  ) f  f(\\gamma) italic_f ( italic_ )  cannot be zero at   = 0  0 \\gamma=0 italic_ = 0 ,   =  0  subscript  0 \\gamma=\\gamma_{0} italic_ = italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  and   =  1  subscript  1 \\gamma=\\gamma_{1} italic_ = italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  for 3 distinct values fo  0 ,  0 ,  1 0 subscript  0 subscript  1 0,\\gamma_{0},\\gamma_{1} 0 , italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_9": {
        "caption": "Table 10:  NDCG@10 results for TE3-L on text datasets",
        "table": "A4.T9.1",
        "footnotes": [],
        "references": [
            "-M . Alg.  1  presents  \\name -M, which formalizes the above procedure, and also incorporates the constraint    0  0 \\gamma\\geq 0 italic_  0 . Lines  6 - 15  find the intervals  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,   i  [ n V ] for-all i delimited-[] subscript n V \\forall i\\in[n_{V}]  italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ]  and add it to a list  I I I italic_I  (intersection of half intervals can be found by calculating the maximum of lower bounds and minimum of upper bounds of the intervals, done in lines  9 -  11 ). The algorithm returns new data embeddings by simply performing a single addition.",
            "Thus, for the  i i i italic_i -th query and the  j j j italic_j -th data record,  i  [ n V ] i delimited-[] subscript n V i\\in[n_{V}] italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ] ,  j  [ n ]  Y i V j delimited-[] n superscript subscript Y i V j\\in[n]\\setminus Y_{i}^{V} italic_j  [ italic_n ]  italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT , we can use Eq.  9  together with Eq.  8  to obtain a set  I i , j  ( 0 , 4 ) subscript I i j 0 4 I_{i,j}\\subseteq{(0,4)} italic_I start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  ( 0 , 4 )  such that Eq.  7  holds if and only if    I i , j  subscript I i j \\gamma\\in I_{i,j} italic_  italic_I start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT . Moreover,  I i , j subscript I i j I_{i,j} italic_I start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  will consist of at most 3 intervals in   ( 0 , 4 ) absent 0 4 \\subseteq{(0,4)}  ( 0 , 4 ) . Let  I i =  j  [ n ] = Y i V I i , j subscript I i subscript j delimited-[] n superscript subscript Y i V subscript I i j I_{i}=\\cap_{j\\in[n]\\neq Y_{i}^{V}}I_{i,j} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =  start_POSTSUBSCRIPT italic_j  [ italic_n ] = italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_I start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT , and note that the  i i i italic_i -th query will be answered accurately if and only if    I i  subscript I i \\gamma\\in I_{i} italic_  italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . Finally,    = arg  max    i  [ n V ] I  [   I i ] superscript  subscript  subscript i delimited-[] subscript n V I delimited-[]  subscript I i \\gamma^{*}=\\arg\\max_{\\gamma}\\sum_{i\\in[n_{V}]}\\mathds{I}[\\gamma\\in I_{i}] italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = roman_arg roman_max start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ] end_POSTSUBSCRIPT blackboard_I [ italic_  italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ]  is an optimal solution to the MAxSS problem.",
            "where Eq.  15  is obtained by setting the gradient of the Lagrangian to zero, and Eq.  19  and   16  are obtained by substituting",
            "and therefore   i =   G i  G i  subscript  i  subscript G i norm subscript G i {\\bm{\\Delta}}_{i}=\\sqrt{\\gamma}\\frac{{\\bm{G}}_{i}}{\\|{\\bm{G}}_{i}\\|} bold_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = square-root start_ARG italic_ end_ARG divide start_ARG bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG  bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  end_ARG  (KKT conditions become similar to the ones in Theorem  2 ). Because   > 0  0 \\mu>0 italic_ > 0 , we must have    i  2 =  superscript norm subscript  i 2  \\|{\\bm{\\Delta}}_{i}\\|^{2}=\\gamma  bold_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_  due to Eq.  17 , so that from Eq.  19  we have",
            "If there are zero roots,  f  (  ) > 0 f  0 f(\\gamma)>0 italic_f ( italic_ ) > 0  either for all of  ( 0 , 4 ) 0 4 (0,4) ( 0 , 4 )  or none of it, which can be determined by checking the function value at 0 or 4 using Eq.  28  and  29  if  c = 0 c 0 c\\neq 0 italic_c = 0  or  b = 0 b 0 b\\neq 0 italic_b = 0 . If  c = b = 0 c b 0 c=b=0 italic_c = italic_b = 0 , then we can solve  f  (  ) > 0 f  0 f(\\gamma)>0 italic_f ( italic_ ) > 0  based on whether the function has a minimum or maximum using Eq.  30 , or is a constant when  a = 0 a 0 a=0 italic_a = 0 .",
            "Tables  9 - 12  present the per dataset results for the embedding models GTE-L, TE3-L, CLIP-B and CLIP-L. The tables (in addition to Table  6 ) present the detailed results from which Tables  3 - 5  are generated."
        ]
    },
    "id_table_10": {
        "caption": "Table 11:  NDCG@10 results for CLIP-B on image datasets",
        "table": "A4.T10.1",
        "footnotes": [],
        "references": [
            "From Eq.  10 ,"
        ]
    },
    "id_table_11": {
        "caption": "Table 12:  NDCG@10 results for CLIP-L on image datasets",
        "table": "A5.T12.fig1.1",
        "footnotes": [],
        "references": [
            "-M . Alg.  1  presents  \\name -M, which formalizes the above procedure, and also incorporates the constraint    0  0 \\gamma\\geq 0 italic_  0 . Lines  6 - 15  find the intervals  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,   i  [ n V ] for-all i delimited-[] subscript n V \\forall i\\in[n_{V}]  italic_i  [ italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ]  and add it to a list  I I I italic_I  (intersection of half intervals can be found by calculating the maximum of lower bounds and minimum of upper bounds of the intervals, done in lines  9 -  11 ). The algorithm returns new data embeddings by simply performing a single addition.",
            "Time complexity . Alg.  1  can be implemented so that lines  11 - 7  with a single pass over the dataset. Therefore, finding  I i subscript I i I_{i} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  for all queries (lines  6 - 15 ) take  O  ( n V  n  d ) O subscript n V n d O(n_{V}\\times n\\times d) italic_O ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT  italic_n  italic_d ) . Calculating  G G G italic_G  takes time  O  ( n T  d ) O subscript n T d O(n_{T}\\times d) italic_O ( italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  italic_d ) . Finding    superscript  \\gamma^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in line  16  is the basic problem of finding the maximum number of overlapping ranges and can be done in  O  ( n V  log  ( n V ) ) O subscript n V subscript n V O(n_{V}\\log(n_{V})) italic_O ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT roman_log ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ) ) , by first sorting the ranges in  I I I italic_I  based on their lower bound and iteratively traversing the sorted list and keeping track of the number of overlapping ranges. Thus, Alg.  1  can be implemented in  O ( n V ( n  d + log ( n V ) + n T  d ) O(n_{V}(n\\times d+\\log(n_{V})+n_{T}\\times d) italic_O ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ( italic_n  italic_d + roman_log ( italic_n start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ) + italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  italic_d ) ."
        ]
    },
    "id_table_12": {
        "caption": "Table 13:  Normalization Study, avg. BGE-S accuracy",
        "table": "A5.T12.fig2.1",
        "footnotes": [],
        "references": [
            "Tables  9 - 12  present the per dataset results for the embedding models GTE-L, TE3-L, CLIP-B and CLIP-L. The tables (in addition to Table  6 ) present the detailed results from which Tables  3 - 5  are generated."
        ]
    },
    "id_table_13": {
        "caption": "Table 14:  Accuracy/efficiency of iterative and non-iterative  \\name  variants (BGE-S on text datasets)",
        "table": "A5.T13.1",
        "footnotes": [],
        "references": [
            "Substituting this into Eq.  13 , and since   > 0  0 \\mu>0 italic_ > 0 , we have",
            "We compare  \\name -N, with  \\name -IN (as described in Sec.  C ) with two other potential variants to understand the impact of normalization.  \\name -M+N is a variant that first performs  \\name -M and then normalizes embeddings post-hoc.  \\name -IM+R is a variation of  \\name -IM with L2 regularization added to the loss to penalize embeddings with large norms. Table  13  shows the results for this experiment, showing that normalizing embeddings after optimization, i.e.,  \\name -M+N performs worse than when normalization is considered as part of optimization, which is the case for both  \\name -IN and  \\name -N.  \\name -IM+R performs worse than all methods, showing an advantage for enforcing a normalization constraint over L2 regularization. Meanwhile,  \\name -IN and  \\name -N perform similarly (see Sec.  C  for a discussion between the two)."
        ]
    },
    "id_table_14": {
        "caption": "",
        "table": "A5.T14.3",
        "footnotes": [],
        "references": [
            "and substituting back into Eq.  14 , we have"
        ]
    },
    "global_footnotes": [
        "Code available at",
        "E.g., OpenAI currently does not provide an interface for fine-tuning embedding model",
        "."
    ]
}