{
    "id_table_1": {
        "caption": "Table 1 :  Candidate operations used in the second level search",
        "table": "S5.EGx1",
        "footnotes": [],
        "references": [
            "The aim of this work is to develop a highly stable automatic architecture for audio-visual deepfake detection. So we propose Gumbel-Rao Monte carlo based bi-modal neural architecture search (GRMC-BMNAS) which adaptively learns architectures from a pool of operations for audio-visual deepfake detection and trains faster and offers better results on the test set performance. GRMC-BMNAS adopts a two-level search similar to  [ 18 ]  where it learns unimodal features from the backbone network by sampling the search space by varying the temperature parameter and Monte Carlo samples. In the second-level search, we utilize the weighted fusion strategy by varying the temperature and Monte Carlo samples. Increasing Monte Carlo samples expands the search space of primitive operations, allowing for a more accurate selection based on softmax probabilities as shown in Figure  1 . As illustrated in Figure  2 , the average entropy of GRMC-BMNAS consistently outperforms both STGS-BMNAS and the standard Softmax baseline  [ 22 ] . This indicates that GRMC-BMNAS achieves a lower entropy, suggesting a faster convergence during training. Our proposed framework matches the performance of STGS-BMNAS  [ 18 ]  while requiring significantly less training time and computational resources (GPU days). Moreover, our model demonstrates superior generalization on test data. The main contributions of this paper are as follows:"
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Comparison of our proposed GRMC-BMNAS with SOTA approaches tested on our test data",
        "table": "S3.T1.11",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "The aim of this work is to develop a highly stable automatic architecture for audio-visual deepfake detection. So we propose Gumbel-Rao Monte carlo based bi-modal neural architecture search (GRMC-BMNAS) which adaptively learns architectures from a pool of operations for audio-visual deepfake detection and trains faster and offers better results on the test set performance. GRMC-BMNAS adopts a two-level search similar to  [ 18 ]  where it learns unimodal features from the backbone network by sampling the search space by varying the temperature parameter and Monte Carlo samples. In the second-level search, we utilize the weighted fusion strategy by varying the temperature and Monte Carlo samples. Increasing Monte Carlo samples expands the search space of primitive operations, allowing for a more accurate selection based on softmax probabilities as shown in Figure  1 . As illustrated in Figure  2 , the average entropy of GRMC-BMNAS consistently outperforms both STGS-BMNAS and the standard Softmax baseline  [ 22 ] . This indicates that GRMC-BMNAS achieves a lower entropy, suggesting a faster convergence during training. Our proposed framework matches the performance of STGS-BMNAS  [ 18 ]  while requiring significantly less training time and computational resources (GPU days). Moreover, our model demonstrates superior generalization on test data. The main contributions of this paper are as follows:",
            "Table  2  presents a comparison of our proposed GRMC-BMNAS model with SOTA methods using a combined dataset. Our model surpases the recent SOTA models POI-AV  [ 20 ] , Multimodaltrace  [ 19 ] , and ID-Reveal  [ 5 ] , STGS-BMNAS  [ 18 ]  on combined datasets in terms of accuracy (ACC) and area under the curve (AUC) metrics, while using fewer model parameters. Our model exhibits lower variance compared to STGS-BMNAS, supporting Proposition 1 (Figure given in the supplementary material). Additionally, our model achieves lower mean squared error (MSE) for learnable parameters    \\alpha italic_  and    \\gamma italic_ , corroborating Proposition 2 (For figure see supplementary material). Optimal architecture with minimal GPU days is obtained with K=100 and   = 0.1  0.1 \\lambda=0.1 italic_ = 0.1  is shown in Figure  3  . Receiver operating characteristic (ROC) curves for   = 0.1  0.1 \\lambda=0.1 italic_ = 0.1  and K=100, distinguishing between real and fake data, are provided in the supplementary material."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Generalisation of our proposed model to the seen and unseen data",
        "table": "S3.T2.2",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "The inverse cumulative density function (ICDF) is also called quantile function given by equation  3  and equation  3  is used in inverse transform sampling to transform sample from Uniform distribution  U  ( 0 , 1 ) U 0 1 U(0,1) italic_U ( 0 , 1 )  into a Gumbel via a double logarithmic relation.",
            "The Gumbel-max technique is a strategy for drawing samples from a categorical random variable denoted by  I  C  a  t  (  ) similar-to I C a t  I\\sim Cat(\\pi) italic_I  italic_C italic_a italic_t ( italic_ ) . It involves the addition of Gumbel-distributed noise, which is independent and identically distributed, to the log probabilities before normalization. More specifically  I = a  r  g  m  a  x i  D { G ( i ) + l  o  g   i }  C  a  t  (  ) I subscript a r g m a x i D superscript G i l o g subscript  i similar-to C a t  I=\\mathop{argmax}_{i\\in D}\\>\\{G^{(i)}+log\\>\\theta_{i}\\}\\sim Cat(\\pi) italic_I = start_BIGOP italic_a italic_r italic_g italic_m italic_a italic_x end_BIGOP start_POSTSUBSCRIPT italic_i  italic_D end_POSTSUBSCRIPT { italic_G start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT + italic_l italic_o italic_g italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }  italic_C italic_a italic_t ( italic_ ) , where  G ( 1 ) , G ( 2 ) , G ( 3 )  ...  G ( D ) superscript G 1 superscript G 2 superscript G 3 ... superscript G D G^{(1)},G^{(2)},G^{(3)}...G^{(D)} italic_G start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , italic_G start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT , italic_G start_POSTSUPERSCRIPT ( 3 ) end_POSTSUPERSCRIPT ... italic_G start_POSTSUPERSCRIPT ( italic_D ) end_POSTSUPERSCRIPT  are the i.i.d samples drawn from Gumbel distribution  3 .",
            "Table  2  presents a comparison of our proposed GRMC-BMNAS model with SOTA methods using a combined dataset. Our model surpases the recent SOTA models POI-AV  [ 20 ] , Multimodaltrace  [ 19 ] , and ID-Reveal  [ 5 ] , STGS-BMNAS  [ 18 ]  on combined datasets in terms of accuracy (ACC) and area under the curve (AUC) metrics, while using fewer model parameters. Our model exhibits lower variance compared to STGS-BMNAS, supporting Proposition 1 (Figure given in the supplementary material). Additionally, our model achieves lower mean squared error (MSE) for learnable parameters    \\alpha italic_  and    \\gamma italic_ , corroborating Proposition 2 (For figure see supplementary material). Optimal architecture with minimal GPU days is obtained with K=100 and   = 0.1  0.1 \\lambda=0.1 italic_ = 0.1  is shown in Figure  3  . Receiver operating characteristic (ROC) curves for   = 0.1  0.1 \\lambda=0.1 italic_ = 0.1  and K=100, distinguishing between real and fake data, are provided in the supplementary material.",
            "Model performance on unseen data:  Table  3  presents the performance of our model on unseen data. Our model significantly outperforms STGS-BMNAS demonstrating its superior generalization capabilities. This improved performance can be attributed to the models reduced variance and lower mean squared error as established in previous sections."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Evaluation of searched architecture with different temperature values and with varying Monte carlo samples",
        "table": "S3.T3.2",
        "footnotes": [],
        "references": [
            "Instead of producing discrete or hard samples from a categorical distribution that lacks structure, one can create soft samples, which are especially beneficial for estimating gradients. To grasp the relationship between these hard and soft samples, its essential to analyze the hard samples when they are expressed in their one-hot encoded form, that is,  1   { 0 , 1 } N subscript double-struck-1  superscript 0 1 N \\mathbb{1}_{\\omega}\\in\\{0,1\\}^{N} blackboard_1 start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  { 0 , 1 } start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT . Then  z = o  n  e  h  o  t  ( a  r  g  m  a  x i  D { G ( i ) + l  o  g   i } ) z o n e h o t subscript a r g m a x i D superscript G i l o g subscript  i z=onehot(\\mathop{argmax}_{i\\in D}\\>\\{G^{(i)}+log\\>\\theta_{i}\\}) italic_z = italic_o italic_n italic_e italic_h italic_o italic_t ( start_BIGOP italic_a italic_r italic_g italic_m italic_a italic_x end_BIGOP start_POSTSUBSCRIPT italic_i  italic_D end_POSTSUBSCRIPT { italic_G start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT + italic_l italic_o italic_g italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } )  From  [ 9 ,  14 ]  we derive PDF of this distribution and denoted by  G  S  (  ,  ) . G S   GS(\\pi,\\lambda). italic_G italic_S ( italic_ , italic_ ) .  More specifically, the  i t  h superscript i t h i^{th} italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT  index of soft sample  S   { R  0 N : | S  | = 1 } subscript S  conditional-set subscript superscript R N absent 0 subscript S  1 S_{\\lambda}\\>\\in\\{\\mathbb{R}^{N}_{\\geq 0}:|S_{\\lambda}|=1\\} italic_S start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  { roman_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT start_POSTSUBSCRIPT  0 end_POSTSUBSCRIPT : | italic_S start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT | = 1 }  is as defined in the Equation  4 .",
            "Table  4  presents the outcomes of an ablation study examining the influence of temperature and Monte Carlo samples on model performance. Consistent with Proposition 2, increasing the number of Monte Carlo samples generally leads to smaller model sizes and higher AUC values, albeit at the expense of increased computational cost. Based on these findings, an optimal architecture was determined with   = 0.1  0.1 \\lambda=0.1 italic_ = 0.1  and K=100. Respective architectures produced using different parameter settings can be found in the supplementary material."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S3.T4.4",
        "footnotes": [],
        "references": [
            "Continuous one-hot vector relaxation excels in learning representations and sequences. However, for tasks requiring discrete values, such as reinforcement learning actions, compressed data, or architecture search, we discretize the continuous output using argmax. In this family, the forward computation of f is unchanged, but backpropagation is computed through a surrogate. This surrogate is known as straight through gumbel softmax (STGS) defined in equation  5 ."
        ]
    }
}