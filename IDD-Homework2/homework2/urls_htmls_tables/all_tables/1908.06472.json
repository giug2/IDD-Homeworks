{
    "S2.T1": {
        "caption": "Table 1: Related work in generative data for training DL models.",
        "table": "<table id=\"S2.T1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.1.1.1.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S2.T1.1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Year</span></span>\n</span>\n</th>\n<th id=\"S2.T1.1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\"><span id=\"S2.T1.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Purpose</span></span>\n</span>\n</th>\n<th id=\"S2.T1.1.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.1.1.3.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><span id=\"S2.T1.1.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Ref.</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.2.1.1.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\">2007</span>\n</span>\n</td>\n<td id=\"S2.T1.1.2.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.2.1.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">Simulating fluorescence microscope images of cell populations for automated image cytometry</span>\n</span>\n</td>\n<td id=\"S2.T1.1.2.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.2.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.2.1.3.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.3.2.1.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\">2016</span>\n</span>\n</td>\n<td id=\"S2.T1.1.3.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.3.2.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">Enhancing soil images coming from X-ray tomography, generating roots to help the model identify the roots from the soils</span>\n</span>\n</td>\n<td id=\"S2.T1.1.3.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.3.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.3.2.3.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">3</a>]</cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.4.3.1.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\">2016</span>\n</span>\n</td>\n<td id=\"S2.T1.1.4.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.4.3.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">Simulating top-down images of overlapping plants on soil background, to classify 23 different weed species and maize.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.4.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.4.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.4.3.3.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">4</a>]</cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.5.4.1.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\">2016</span>\n</span>\n</td>\n<td id=\"S2.T1.1.5.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.5.4.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">Generating fully labeled, dynamic, and photo-realistic proxy virtual world, with a focus on objects of interest, e.g. cars.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.5.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.5.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.5.4.3.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">5</a>]</cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.6.5\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.6.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.6.5.1.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\">2016</span>\n</span>\n</td>\n<td id=\"S2.T1.1.6.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.6.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.6.5.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">Generating synthetic data for semantic segmentation of outdoor scenes, for recognizing aspects such as roads, buildings, cars, people, lights etc.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.6.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.6.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.6.5.3.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">19</a>]</cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.7.6\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.7.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.7.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.7.6.1.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\">2016</span>\n</span>\n</td>\n<td id=\"S2.T1.1.7.6.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.7.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.7.6.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">Automatically generating realistic synthetic images with pixel-level annotations for semantic segmentation</span>\n</span>\n</td>\n<td id=\"S2.T1.1.7.6.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.7.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.7.6.3.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">20</a>]</cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.8.7\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.8.7.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.8.7.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.8.7.1.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\">2017</span>\n</span>\n</td>\n<td id=\"S2.T1.1.8.7.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.8.7.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.8.7.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">Creating synthetic images to predict number of tomatoes in the images.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.8.7.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.8.7.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.8.7.3.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">18</a>]</cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.9.8\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.9.8.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.9.8.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.9.8.1.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\">2018</span>\n</span>\n</td>\n<td id=\"S2.T1.1.9.8.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.9.8.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.9.8.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">Generating synthetic data to identify melanoma skin cancer.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.9.8.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.9.8.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.9.8.3.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">7</a>]</cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.10.9\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.10.9.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.10.9.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.10.9.1.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\">2018</span>\n</span>\n</td>\n<td id=\"S2.T1.1.10.9.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.10.9.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.10.9.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">Synthetic data for 2D bounding box car detection.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.10.9.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.10.9.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.10.9.3.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">16</a>]</cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.11.10\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.11.10.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.11.10.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.11.10.1.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\">2018</span>\n</span>\n</td>\n<td id=\"S2.T1.1.11.10.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.11.10.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.11.10.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">Generating 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layout, for teaching an agent to navigate in an unseen 3D environment.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.11.10.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.11.10.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.11.10.3.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.12.11\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.12.11.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.12.11.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.12.11.1.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\">2018</span>\n</span>\n</td>\n<td id=\"S2.T1.1.12.11.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.12.11.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.12.11.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">Generating scenes for teaching an artificial agent to execute tasks in a simulated household environment.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.12.11.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.12.11.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.12.11.3.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">17</a>]</cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.13.12\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.13.12.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.13.12.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.13.12.1.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\">2019</span>\n</span>\n</td>\n<td id=\"S2.T1.1.13.12.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.13.12.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.13.12.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">a) Generating data for semantic segmentation of aerial views of roadways. b) Simulating urban scenes for object detection in urban car driving.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.13.12.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.1.13.12.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.13.12.3.1.1\" class=\"ltx_p\" style=\"width:22.8pt;\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Table 1 lists related work in the field of generating training data to train DL models. The year of publication for every paper reveals how modern this technique is. Please note that we avoided adding details about performance metrics and evaluation results for each paper, because each author used different metrics and experimented on different real-world datasets for testing. However, the general conclusion in all papers was that the performance according to the metric(s) used, was better than baseline (i.e. datasets not enhanced with synthetic data) or state-of-art related work.",
            "From Table 1, it is evident that related work has not entered yet the domain of UAV-based imagery analysis. The only exception is Meta-Sim [11], which tries to learn a generative model of synthetic scenes automatically, via probabilistic scene grammars, and then it obtains images and their corresponding ground-truth via a graphics engine. Meta-Sim validates this idea addressing the problem of semantic segmentation of simulated aerial views of simple roadways. Beyond this work, to our knowledge, no other work has focused yet on generative data-based approaches for UAV-based imaging-related applications."
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Number of images used for training and testing of the DL models.",
        "table": "<table id=\"S3.T2.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.1.1.1.1\" class=\"ltx_p\" style=\"width:76.8pt;\"><span id=\"S3.T2.1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Scenario</span></span>\n</span>\n</th>\n<th id=\"S3.T2.1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T2.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Purpose</span></span>\n</span>\n</th>\n<th id=\"S3.T2.1.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.1.3.1.1\" class=\"ltx_p\" style=\"width:202.0pt;\"><span id=\"S3.T2.1.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">No. of images</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.1.1.1.1\" class=\"ltx_p\" style=\"width:76.8pt;\">Fire identification</span>\n</span>\n</td>\n<td id=\"S3.T2.1.2.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.1.2.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Training</span>\n</span>\n</td>\n<td id=\"S3.T2.1.2.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.2.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.1.3.1.1\" class=\"ltx_p\" style=\"width:202.0pt;\">2,000 synthetic images</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.3.2.1.1.1\" class=\"ltx_p\" style=\"width:76.8pt;\">Fire identification</span>\n</span>\n</td>\n<td id=\"S3.T2.1.3.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.3.2.2.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Testing</span>\n</span>\n</td>\n<td id=\"S3.T2.1.3.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.3.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.3.2.3.1.1\" class=\"ltx_p\" style=\"width:202.0pt;\">100 real-world aerial photos (classified as 50 images of forest and 50 images of fire)</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.4.3.1.1.1\" class=\"ltx_p\" style=\"width:76.8pt;\">Counting houses</span>\n</span>\n</td>\n<td id=\"S3.T2.1.4.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.4.3.2.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Training</span>\n</span>\n</td>\n<td id=\"S3.T2.1.4.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.4.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.4.3.3.1.1\" class=\"ltx_p\" style=\"width:202.0pt;\">10,000 synthetic images (labelled with exact number of houses)</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.5.4\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.5.4.1.1.1\" class=\"ltx_p\" style=\"width:76.8pt;\">Counting houses</span>\n</span>\n</td>\n<td id=\"S3.T2.1.5.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.5.4.2.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Testing</span>\n</span>\n</td>\n<td id=\"S3.T2.1.5.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.5.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.5.4.3.1.1\" class=\"ltx_p\" style=\"width:202.0pt;\">60 real-world aerial photos (labelled with exact number of houses)</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Regarding the real-world datasets (used for testing the DL model), for the fire identification case, 100 aerial photos were downloaded from Google Images, 50 of them showing forest areas and another 50 showing a forest fire. For the counting houses case, 20 aerial photos from urban areas of Tanzania have been selected, from the Open AI Tanzania Challenge333Open AI Tanzania Challenge. https://blog.werobotics.org/2018/08/06/welcome-to-the-open-ai-tanzania-challenge/. We cropped these photos in 100x100 pixel images, and counted the number of houses manually at each cropped photo. The result was a dataset of 60 images, each having [0,38]038[0,38] houses from an aerial view. Samples of the real-world datasets for the two scenarios under study are depicted in Figure 2 (bottom). Table 2 describes the number of images used for training and testing of the two scenarios under study."
        ]
    }
}