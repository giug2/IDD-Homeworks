{
    "id_table_1": {
        "caption": "Table 1:  Overview of all real datasets used in our comparative study",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "Bias in the data related to fairness and class imbalance are not mutually exclusive. More often than not, they occur simultaneously  [ 17 ] , leading to extreme population imbalance for individuals from a minority group who are assigned underrepresented class labels. For example, in the popular Adult dataset, females with a high-income class label are the most under-represented subgroup, accounting for only  11 % percent 11 11\\% 11 %  of the total data (see Figure  1 ). These populations can become even smaller under \"intersectional-fairness\" when more than one protected attribute exists  [ 23 ]  or for multi-class classification.",
            "We use four real-world tabular datasets, frequently used in fairness-aware learning  [ 17 ] . These datasets comprise demographic attributes of individuals, aimed at predicting their financial status, such as occupation, income, credit score, etc. In Table  1  we list the basic characteristics of all datasets, namely, the  Adult ,  German credit ,  Dutch census , and  Credit card clients . The protected attribute chosen for all datasets is the binary feature \"sex\" (Male/Female). We observe class imbalance for the  Adult ,  German credit , and  Credit card clients  datasets, as well as, a mixed feature space. The  Dutch census  dataset exhibits a less pronounced class imbalance and includes solely discrete features. Both class and group imbalances for all datasets are visualized in the first column of Fig  1 .",
            "All generative methods covered in our study (described in Section  2.4 ) can be trained on a set of tabular data, and then used to generate an arbitrary number of synthetic samples. Given our assumption of a single binary protected attribute and a binary classification task, this results in  4 4 4 4  homogeneous subgroups for each dataset. Additionally, as defined in Section  3 , a sampling strategy dictates the number of synthetic samples to draw from each subgroup, to create the final augmented training set. In this work, we propose four such sampling strategies aimed at addressing class imbalance, group imbalance, or both. Namely,  class  and  protected , sample data to achieve class, and group balance, respectively. Furthermore,  class & protected , and  class (ratio) , sample synthetic data to achieve both class and group parity. We define each sampling strategy in detail hereafter and provide a visual representation of the final distributions of the augmented data for each sampling strategy on all datasets in Figure  1 .",
            "ROC AUC (class imbalance):  Sampling strategies focusing on  class balancing , such as  class  and  class & protected   improve the ROC AUC score for imbalanced datasets. For the  dutch  dataset, we do not observe any improvement, due to the lack of inherent class imbalance in the data (see Table  1 ). Notably, the best ROC AUC score in most cases is achieved with CART-generated data.",
            "Fairness:  We observe that the Tabfairgan baseline, although specifically optimized for statistical parity (SP), can also lead to improvements in terms of Eq. Odds and Eq. Opp. However, it is evident that using generative methods, especially with class (ratio) sampling, leads to superior fairness metrics while maintaining higher utility (ROC AUC). This can be attributed to the fact that for most datasets (excluding Dutch census), fewer synthetic samples are needed to achieve equal class ratios between different subgroups, i.e. a lower  r a  u  g subscript r a u g r_{aug} italic_r start_POSTSUBSCRIPT italic_a italic_u italic_g end_POSTSUBSCRIPT  (see Fig.  1 ). On the other hand, the class & protected strategy requires the highest number of synthetic samples to maintain class and group balance. This increases the risk of producing out-of-distribution samples, which can degrade performance."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Training and sampling runtime comparison for all generative methods on the Adult dataset.",
        "table": "S5.T2.21",
        "footnotes": [],
        "references": [
            "The rest of this paper is organized as follows. We describe all relevant works related to fairness, class imbalance, and generative methods in Section  2 , we present the problem formulation, dataset details, and evaluation metrics in Section  3 , we define all sampling strategies in Section  4 , and include all experiments and results in Section  5 . We conclude the paper with a discussion and opportunities for future work in Section  6 .",
            "In our study, we focus on  group fairness , which considers parity over different groups of individuals, distinguished by one or more protected attributes, such as sex, race, age, etc. Several metrics have been defined to measure the group fairness of a classification model (see Section  3.2 ). While there are various methods for enhancing group fairness, the main focus is on i) creating methods that specifically optimize for fairness  [ 14 ] , by incorporating constraints to existing models, for example via adding fairness objectives to the loss function  [ 24 ] , and ii) model-agnostic, pre-processing methods  [ 8 ,  16 ,  26 ]  that overcome bias by modifying the training data.",
            "Nonetheless, recently several generative models have been proposed for generating synthetic mixed tabular data, for example, based on neural networks  [ 27 ] , classification trees  [ 22 ] , probabilistic approaches  [ 11 ] , or even large language models  [ 2 ] . In this work, we evaluate such synthetic tabular data generation methods (defined in Section  2.4 ) for class imbalance and fairness, while considering different sampling strategies.",
            "All generative methods covered in our study (described in Section  2.4 ) can be trained on a set of tabular data, and then used to generate an arbitrary number of synthetic samples. Given our assumption of a single binary protected attribute and a binary classification task, this results in  4 4 4 4  homogeneous subgroups for each dataset. Additionally, as defined in Section  3 , a sampling strategy dictates the number of synthetic samples to draw from each subgroup, to create the final augmented training set. In this work, we propose four such sampling strategies aimed at addressing class imbalance, group imbalance, or both. Namely,  class  and  protected , sample data to achieve class, and group balance, respectively. Furthermore,  class & protected , and  class (ratio) , sample synthetic data to achieve both class and group parity. We define each sampling strategy in detail hereafter and provide a visual representation of the final distributions of the augmented data for each sampling strategy on all datasets in Figure  1 .",
            "class & protected:  For the largest group (e.g. Male) we sample instances for the minority class, to match the number of instances in the majority class. For all other groups (e.g. Female) we sample for both the majority class and the minority class, to match the number of instances in the majority class in the largest group. Therefore, we achieve the same number of samples for all  4 4 4 4  subgroups. It is worth noting that the  class & protected  strategy is described and used by the FSMOTE method  [ 4 ]  (refer to Section  2.3 ).",
            "We perform experiments for all four datasets, four sampling methods, and five generative models. To ensure robustness, each experiment on the downstream task is 3-fold cross-validated and repeated two times over different random seeds. We report average results over all repetitions, highlighting the best results in bold, and underlining the second-best. For the accumulated results of Section  5.2 , we further shade with  blue color  the experiments on synthetic data, which exhibit better performance than training on the original real data (first row).",
            "In all previous experiments, we assume a single binary protected attribute and class label, leading to  4 4 4 4  subgroups (see Section  3 ). Nonetheless, in some cases multiple protected attributes can exist, partitioning the data into further groups. To study this effect, we conduct an experiment on the Adult dataset, with race = {White, Other} as the additional protected attribute, splitting the data into  8 8 8 8  subgroups. In Table  5 , we present the results when using  race  and  sex & race  (intersection) as protected attributes. In line with our previous results, we observe that the class (ratio) strategy and CART generative method significantly enhance fairness without compromising utility. The non-parametric nature of CART enables consistent generation even from the most under-represented subgroups under extreme data scarcity. For example, in the Adult dataset, the subgroup (sex, race, class) = (Female, Other, 1) accounts for only  0.5 % percent 0.5 0.5\\% 0.5 %  of the total data, i.e. under  200 200 200 200  instances, as seen in Fig.  2 .",
            "We conclude our experiments by performing a runtime comparison of all generative methods. We report runtime for, i) training (fitting) on the Adult dataset, ii) sampling  10.000 10.000 10.000 10.000  synthetic instances, and iii) training and sampling, since this overall runtime is the most significant metric in our comparison. From Table  2  it becomes evident that the CART method outperforms all others significantly in terms of overall runtime."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Results for Adult and German credit datasets.",
        "table": "S5.T3.10",
        "footnotes": [],
        "references": [
            "The rest of this paper is organized as follows. We describe all relevant works related to fairness, class imbalance, and generative methods in Section  2 , we present the problem formulation, dataset details, and evaluation metrics in Section  3 , we define all sampling strategies in Section  4 , and include all experiments and results in Section  5 . We conclude the paper with a discussion and opportunities for future work in Section  6 .",
            "In our study, we focus on  group fairness , which considers parity over different groups of individuals, distinguished by one or more protected attributes, such as sex, race, age, etc. Several metrics have been defined to measure the group fairness of a classification model (see Section  3.2 ). While there are various methods for enhancing group fairness, the main focus is on i) creating methods that specifically optimize for fairness  [ 14 ] , by incorporating constraints to existing models, for example via adding fairness objectives to the loss function  [ 24 ] , and ii) model-agnostic, pre-processing methods  [ 8 ,  16 ,  26 ]  that overcome bias by modifying the training data.",
            "All generative methods covered in our study (described in Section  2.4 ) can be trained on a set of tabular data, and then used to generate an arbitrary number of synthetic samples. Given our assumption of a single binary protected attribute and a binary classification task, this results in  4 4 4 4  homogeneous subgroups for each dataset. Additionally, as defined in Section  3 , a sampling strategy dictates the number of synthetic samples to draw from each subgroup, to create the final augmented training set. In this work, we propose four such sampling strategies aimed at addressing class imbalance, group imbalance, or both. Namely,  class  and  protected , sample data to achieve class, and group balance, respectively. Furthermore,  class & protected , and  class (ratio) , sample synthetic data to achieve both class and group parity. We define each sampling strategy in detail hereafter and provide a visual representation of the final distributions of the augmented data for each sampling strategy on all datasets in Figure  1 .",
            "class & protected:  For the largest group (e.g. Male) we sample instances for the minority class, to match the number of instances in the majority class. For all other groups (e.g. Female) we sample for both the majority class and the minority class, to match the number of instances in the majority class in the largest group. Therefore, we achieve the same number of samples for all  4 4 4 4  subgroups. It is worth noting that the  class & protected  strategy is described and used by the FSMOTE method  [ 4 ]  (refer to Section  2.3 ).",
            "For each sampling strategy in the figure, we report the  augmentation ratio   r a  u  g subscript r a u g r_{aug} italic_r start_POSTSUBSCRIPT italic_a italic_u italic_g end_POSTSUBSCRIPT , as defined in Section  3 , i.e. the percentage of synthetic samples in the final augmented dataset. To visualize this, the number of synthetic samples in each bar plot is depicted with a darker color than the real data.",
            "The following Table  3  and Table  4  show the results of our comparative study for the  Adult  and  German credit  datasets, and the  Dutch census  and  Credit card clients  datasets, respectively. As previously mentioned, we present average metrics for all sampling strategies and generative methods. The first two rows in each table (for each dataset) correspond to baselines, i.e. training the classifier using the real data, and synthetic data generated with Tabfairgan  [ 21 ] . Subsequent rows display results for augmented training data generated through various combinations of the five generative methods and four sampling strategies. We highlight in blue the experiments with superior performance compared to training on the real dataset. Testing (evaluation) is always performed on, previously-unseen, test data from the real dataset.",
            "In all previous experiments, we assume a single binary protected attribute and class label, leading to  4 4 4 4  subgroups (see Section  3 ). Nonetheless, in some cases multiple protected attributes can exist, partitioning the data into further groups. To study this effect, we conduct an experiment on the Adult dataset, with race = {White, Other} as the additional protected attribute, splitting the data into  8 8 8 8  subgroups. In Table  5 , we present the results when using  race  and  sex & race  (intersection) as protected attributes. In line with our previous results, we observe that the class (ratio) strategy and CART generative method significantly enhance fairness without compromising utility. The non-parametric nature of CART enables consistent generation even from the most under-represented subgroups under extreme data scarcity. For example, in the Adult dataset, the subgroup (sex, race, class) = (Female, Other, 1) accounts for only  0.5 % percent 0.5 0.5\\% 0.5 %  of the total data, i.e. under  200 200 200 200  instances, as seen in Fig.  2 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Results for Dutch census and Credit card clients datasets.",
        "table": "S5.T4.10",
        "footnotes": [],
        "references": [
            "The rest of this paper is organized as follows. We describe all relevant works related to fairness, class imbalance, and generative methods in Section  2 , we present the problem formulation, dataset details, and evaluation metrics in Section  3 , we define all sampling strategies in Section  4 , and include all experiments and results in Section  5 . We conclude the paper with a discussion and opportunities for future work in Section  6 .",
            "Nonetheless, recently several generative models have been proposed for generating synthetic mixed tabular data, for example, based on neural networks  [ 27 ] , classification trees  [ 22 ] , probabilistic approaches  [ 11 ] , or even large language models  [ 2 ] . In this work, we evaluate such synthetic tabular data generation methods (defined in Section  2.4 ) for class imbalance and fairness, while considering different sampling strategies.",
            "In our study, we define and compare various over-sampling methods (Section  4 ) dictated by the generative models and sampling strategies  G , S G S G,S italic_G , italic_S , aiming to correct both class and group imbalance.",
            "All generative methods covered in our study (described in Section  2.4 ) can be trained on a set of tabular data, and then used to generate an arbitrary number of synthetic samples. Given our assumption of a single binary protected attribute and a binary classification task, this results in  4 4 4 4  homogeneous subgroups for each dataset. Additionally, as defined in Section  3 , a sampling strategy dictates the number of synthetic samples to draw from each subgroup, to create the final augmented training set. In this work, we propose four such sampling strategies aimed at addressing class imbalance, group imbalance, or both. Namely,  class  and  protected , sample data to achieve class, and group balance, respectively. Furthermore,  class & protected , and  class (ratio) , sample synthetic data to achieve both class and group parity. We define each sampling strategy in detail hereafter and provide a visual representation of the final distributions of the augmented data for each sampling strategy on all datasets in Figure  1 .",
            "The following Table  3  and Table  4  show the results of our comparative study for the  Adult  and  German credit  datasets, and the  Dutch census  and  Credit card clients  datasets, respectively. As previously mentioned, we present average metrics for all sampling strategies and generative methods. The first two rows in each table (for each dataset) correspond to baselines, i.e. training the classifier using the real data, and synthetic data generated with Tabfairgan  [ 21 ] . Subsequent rows display results for augmented training data generated through various combinations of the five generative methods and four sampling strategies. We highlight in blue the experiments with superior performance compared to training on the real dataset. Testing (evaluation) is always performed on, previously-unseen, test data from the real dataset."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Results for Adult dataset with  race  and  sex & race  (intersectional) protected attributes.",
        "table": "S5.T5.10",
        "footnotes": [],
        "references": [
            "The rest of this paper is organized as follows. We describe all relevant works related to fairness, class imbalance, and generative methods in Section  2 , we present the problem formulation, dataset details, and evaluation metrics in Section  3 , we define all sampling strategies in Section  4 , and include all experiments and results in Section  5 . We conclude the paper with a discussion and opportunities for future work in Section  6 .",
            "We perform experiments for all four datasets, four sampling methods, and five generative models. To ensure robustness, each experiment on the downstream task is 3-fold cross-validated and repeated two times over different random seeds. We report average results over all repetitions, highlighting the best results in bold, and underlining the second-best. For the accumulated results of Section  5.2 , we further shade with  blue color  the experiments on synthetic data, which exhibit better performance than training on the original real data (first row).",
            "In all previous experiments, we assume a single binary protected attribute and class label, leading to  4 4 4 4  subgroups (see Section  3 ). Nonetheless, in some cases multiple protected attributes can exist, partitioning the data into further groups. To study this effect, we conduct an experiment on the Adult dataset, with race = {White, Other} as the additional protected attribute, splitting the data into  8 8 8 8  subgroups. In Table  5 , we present the results when using  race  and  sex & race  (intersection) as protected attributes. In line with our previous results, we observe that the class (ratio) strategy and CART generative method significantly enhance fairness without compromising utility. The non-parametric nature of CART enables consistent generation even from the most under-represented subgroups under extreme data scarcity. For example, in the Adult dataset, the subgroup (sex, race, class) = (Female, Other, 1) accounts for only  0.5 % percent 0.5 0.5\\% 0.5 %  of the total data, i.e. under  200 200 200 200  instances, as seen in Fig.  2 ."
        ]
    }
}