{
    "id_table_1": {
        "caption": "Table 2:  Performance of DPrompt tuning",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "As illustrated in Figure  1 , the reasoning tree consists of 4 layers, and we have retrieved 3 documents that provide information for nodes  u 2 , 0 subscript u 2 0 u_{2,0} italic_u start_POSTSUBSCRIPT 2 , 0 end_POSTSUBSCRIPT ,  u 1 , 1 subscript u 1 1 u_{1,1} italic_u start_POSTSUBSCRIPT 1 , 1 end_POSTSUBSCRIPT  and  u 2 , 2 subscript u 2 2 u_{2,2} italic_u start_POSTSUBSCRIPT 2 , 2 end_POSTSUBSCRIPT , respectively. and with document  D  1 D 1 D1 italic_D 1 , node  u 1 , 0 subscript u 1 0 u_{1,0} italic_u start_POSTSUBSCRIPT 1 , 0 end_POSTSUBSCRIPT  can also be erased because it contributes only to  u 2 , 0 subscript u 2 0 u_{2,0} italic_u start_POSTSUBSCRIPT 2 , 0 end_POSTSUBSCRIPT , and with  D  2 D 2 D2 italic_D 2 ,  u 0 , 1 subscript u 0 1 u_{0,1} italic_u start_POSTSUBSCRIPT 0 , 1 end_POSTSUBSCRIPT  is not also necessary, the same applies to node  u 1 , 2 subscript u 1 2 u_{1,2} italic_u start_POSTSUBSCRIPT 1 , 2 end_POSTSUBSCRIPT  and  u 1 , 3 subscript u 1 3 u_{1,3} italic_u start_POSTSUBSCRIPT 1 , 3 end_POSTSUBSCRIPT  because of  D  3 D 3 D3 italic_D 3 . Consequently, all 4 nodes in layer 1 can be either accessed or reduced through the documents, meaning that all nodes in layer 1 and 0 are unnecessary. This results in a reasoning depth of 2 instead of 4. Thus, with relevant documents, RAG can effectively reduce the reasoning complexity of the problem, enabling the LLM to solve problems of higher complexity.",
            "The proof is detailed in Appendix  A.1 . This theorem indicates that the erased nodes cannot propagate indefinitely like a fission reaction; rather, they will stop at a certain threshold. Specifically, when  p l < 1  1 q l n  ln  q l n subscript p l 1 1 superscript subscript q l n superscript subscript q l n p_{l}<1-\\frac{1}{q_{l}^{n}-\\ln q_{l}^{n}} italic_p start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT < 1 - divide start_ARG 1 end_ARG start_ARG italic_q start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT - roman_ln italic_q start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_ARG  and  t  t ^ t ^ t t\\geq\\hat{t} italic_t  over^ start_ARG italic_t end_ARG , it follows that  f  ( t )  t f t t f(t)\\leq t italic_f ( italic_t )  italic_t . As illustrated in Figure  2 , if  x  t ^ x ^ t x\\leq\\hat{t} italic_x  over^ start_ARG italic_t end_ARG , the probability of erasure increases, whereas for  x > t ^ x ^ t x>\\hat{t} italic_x > over^ start_ARG italic_t end_ARG , it gradually decreases, stabilizing at  t ^ ^ t \\hat{t} over^ start_ARG italic_t end_ARG .",
            "where  L  (  ) L  \\mathcal{L}(\\delta) caligraphic_L ( italic_ )  stands for the loss with    \\delta italic_  percent of relevant documents and  L  ( 1 ) L 1 \\mathcal{L}(1) caligraphic_L ( 1 )  means the loss with all information related to the query, no distraction is involved.  z z {\\bm{z}} bold_italic_z  is the embedding of documents,  H  (  ) H  H(\\cdot) italic_H (  )  and  I  (  ;  ) I   I(\\cdot;\\cdot) italic_I (  ;  )  represents the entropy and mutual information.  c 1 , c 2 subscript c 1 subscript c 2 c_{1},c_{2} italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  are constants and the value is shown in Appendix  B.1 .",
            "The proof is presented in Appendix  B.1 . From Equation  1 , we see that the generalization error decreases as    \\delta italic_  increases. This indicates that if not all information needed to distinguish irrelevant tokens is captured in the embedding, an increasing number of distracting documents will have negative affect on performance.  The term  H  ( Z r ) H subscript Z r H({\\bm{Z}}_{r}) italic_H ( bold_italic_Z start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT )  represents the amount of relevant token information utilized to infer the final result, suggesting that the greater the reliance of the LLM on documents, the more detrimental the impact of noise becomes. Furthermore, performance is primarily determined by  I  ( w ; z ) I w z I({\\bm{w}};{\\bm{z}}) italic_I ( bold_italic_w ; bold_italic_z ) , which quantifies the information in the embedding relevant for assessing the documents relevance.  Recent studies indicate that LLMs effectively perform information compression  (Deletang et al.,  2023 ; Huang et al.,  2024 ; Yu et al.,  2023 ) , retaining only the information pertinent to the downstream task while discarding extraneous noise, specifically  H  ( z | s ) H conditional z s H({\\bm{z}}|{\\bm{s}}) italic_H ( bold_italic_z | bold_italic_s ) . Consequently, only limited information about how to differentiate irrelevant documents is preserved, leading to diminished performance.",
            "We conduct experiments on three models with a subset of NQ  (Kwiatkowski et al.,  2019 ) , more details about the experiment is shown in Appendix  D . As shown in Table  1 , positioning the query ahead significantly enhances performance. The term gold refers to the configuration query, gold-document indicating that the query is placed before the documents, with only the gold document (which contains the answer) included. In contrast, dis represents the configuration query, gold-document, distraction-document where one distracting document is incorporated. The notation gold-r dis-r signifies that the query has been placed both ahead and after of the documents. It is clear that this arrangement improves performance across all three models except for vicuna when faced with distraction, we assume that this is mainly because vicuna is finetuned on a conversation dataset, which makes the model already capable of handling distracting documents, and the reversed query document order have negative impact on the filtering process.",
            "When we implement RAG, it may struggle to reduce many layers, but it effectively helps to decrease numerous nodes, introducing some sparsity to LLMs. In Figure  1 , Document 1 contains information about  u 2 , 2 subscript u 2 2 u_{2,2} italic_u start_POSTSUBSCRIPT 2 , 2 end_POSTSUBSCRIPT , which can aid in reducing the calculations for nodes  u 1 , 2 , u 1 , 3 subscript u 1 2 subscript u 1 3 u_{1,2},u_{1,3} italic_u start_POSTSUBSCRIPT 1 , 2 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT 1 , 3 end_POSTSUBSCRIPT , and so on. This allows for the pruning of certain parameters used to compute these nodes, resulting in a sparser LLM when utilizing RAG."
        ]
    },
    "id_table_2": {
        "caption": "Table 3:  Performance on regular order",
        "table": "S5.T2.1",
        "footnotes": [],
        "references": [
            "In Section  2 , we explore the extent to which RAG can enhance the reasoning of LLMs, demonstrating that RAG has limited potential to improve reasoning capacity. In Section  3 , we discuss the negative impact of noise on performance and the challenges associated with incorporating the filtering process into the reasoning process. In Section  4 , we highlight the complexities involved in judging relevance, illustrating that solving this problem requires multiple layers, and we propose a method to simplify the process.",
            "The proof is detailed in Appendix  A.1 . This theorem indicates that the erased nodes cannot propagate indefinitely like a fission reaction; rather, they will stop at a certain threshold. Specifically, when  p l < 1  1 q l n  ln  q l n subscript p l 1 1 superscript subscript q l n superscript subscript q l n p_{l}<1-\\frac{1}{q_{l}^{n}-\\ln q_{l}^{n}} italic_p start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT < 1 - divide start_ARG 1 end_ARG start_ARG italic_q start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT - roman_ln italic_q start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_ARG  and  t  t ^ t ^ t t\\geq\\hat{t} italic_t  over^ start_ARG italic_t end_ARG , it follows that  f  ( t )  t f t t f(t)\\leq t italic_f ( italic_t )  italic_t . As illustrated in Figure  2 , if  x  t ^ x ^ t x\\leq\\hat{t} italic_x  over^ start_ARG italic_t end_ARG , the probability of erasure increases, whereas for  x > t ^ x ^ t x>\\hat{t} italic_x > over^ start_ARG italic_t end_ARG , it gradually decreases, stabilizing at  t ^ ^ t \\hat{t} over^ start_ARG italic_t end_ARG .",
            "Now we consider the scenario where the fission reaction converges to a certain point and calculate the probability of erasing a layer. As the probability of replacing a node approaches  t l ^ ^ subscript t l \\hat{t_{l}} over^ start_ARG italic_t start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_ARG , we assume that the probability of replacing a node in layer  l l l italic_l  is  t l ^ ^ subscript t l \\hat{t_{l}} over^ start_ARG italic_t start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_ARG , i.e.,  t l = t l ^ subscript t l ^ subscript t l t_{l}=\\hat{t_{l}} italic_t start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = over^ start_ARG italic_t start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_ARG . In Appendix  A.2 , we demonstrate through simulation that  t l  t l ^ subscript t l ^ subscript t l t_{l}\\approx\\hat{t_{l}} italic_t start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT  over^ start_ARG italic_t start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_ARG , which validates our assumption.",
            "The proof is presented in Appendix  A.2 . According to the theorem,  1   1  q   n 1 italic- 1 superscript q italic- n 1-\\frac{\\epsilon}{1-q^{\\epsilon n}} 1 - divide start_ARG italic_ end_ARG start_ARG 1 - italic_q start_POSTSUPERSCRIPT italic_ italic_n end_POSTSUPERSCRIPT end_ARG  is decreasing with   italic- \\epsilon italic_ , so for a feasible solution, we require a large   italic- \\epsilon italic_ , i.e., a small   n n  \\sqrt[n]{\\delta} nth-root start_ARG italic_n end_ARG start_ARG italic_ end_ARG . Considering a similar scenario with  q  [ 0.1 , 0.8 ] q 0.1 0.8 q\\in[0.1,0.8] italic_q  [ 0.1 , 0.8 ]  and  n  [ 2 , 16 ] n 2 16 n\\in[2,16] italic_n  [ 2 , 16 ] , where  q i  q j =   ( n i  n j ) subscript q i subscript q j  subscript n i subscript n j q_{i}-q_{j}=\\eta(n_{i}-n_{j}) italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = italic_ ( italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_n start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) , as shown in Figure  3 , high-probability layer erasure is primarily achievable in lower layers, as we cannot retrieve much information from higher layers. However, we also observe that with a small probability, effectively erasing higher layers is possible, indicating that RAG can significantly enhance LLM reasoning. This suggests that if we are fortunate enough to retrieve documents that directly lead to the answer, we can substantially reduce reasoning complexity. Nonetheless, in typical scenarios, the assistance is limited since most retrieved documents contain only basic information.",
            "The proof is presented in Appendix  B.2 . To effectively fine-tune the model for filtering out irrelevant information in the attention matrix, we need    0  0 \\delta\\approx 0 italic_  0 . This implies that for all tokens to be retained,  x i T  W  x j superscript subscript x i T W subscript x j {\\bm{x}}_{i}^{T}{\\bm{W}}{\\bm{x}}_{j} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_W bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  must remain nearly constant. As a result, approximating the optimal solution proves to be quite challenging.",
            "Furthermore, this method renders Assumption  2.3  unrealistic, as assessing relevance requires a deep understanding of the document, thereby increasing the number of layers necessary for effective information extraction. If we assume that  t t t italic_t  layers are needed to evaluate relevance, then extracting information about node  u l , j subscript u l j u_{l,j} italic_u start_POSTSUBSCRIPT italic_l , italic_j end_POSTSUBSCRIPT  would require    l + t   l t \\lambda\\cdot l+t italic_  italic_l + italic_t  layers. This could potentially exceed  l l l italic_l , and extracting information from documents may necessitate a greater reasoning depth than standard reasoning.  Consequently, nodes located below layer  t 1   t 1  \\frac{t}{1-\\lambda} divide start_ARG italic_t end_ARG start_ARG 1 - italic_ end_ARG  cannot access document information because their reasoning would be finished faster than information extraction of documents. Then the reasoning depth cannot be easily reduced, as it primarily occurs in the lower layers. Therefore, leveraging document information may not enhance reasoning ability; in fact, it could potentially degrade performance if the model becomes overly reliant on document information rather than employing vanilla reasoning.",
            "From Table  2 , we can see that DPrompt tuning achieves significant performance improvements across all three models. This indicates that adding virtual tokens at the beginning enhances the LLMs ability to distinguish irrelevant information. However, there are instances where the LoRA-fine-tuned model performs worse than the vanilla reasoning with RAG, aligning with previous findings that fine-tuned RALM may performs worse  (Lakatos et al.,  2024 ) . We also conduct experiments with regular order (query ahead of documents) and we put the result in Appendix  D.2",
            "Using Theorem  B.2 , we can observe that,"
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "A4.T3.1",
        "footnotes": [],
        "references": [
            "In Section  2 , we explore the extent to which RAG can enhance the reasoning of LLMs, demonstrating that RAG has limited potential to improve reasoning capacity. In Section  3 , we discuss the negative impact of noise on performance and the challenges associated with incorporating the filtering process into the reasoning process. In Section  4 , we highlight the complexities involved in judging relevance, illustrating that solving this problem requires multiple layers, and we propose a method to simplify the process.",
            "The proof is presented in Appendix  A.2 . According to the theorem,  1   1  q   n 1 italic- 1 superscript q italic- n 1-\\frac{\\epsilon}{1-q^{\\epsilon n}} 1 - divide start_ARG italic_ end_ARG start_ARG 1 - italic_q start_POSTSUPERSCRIPT italic_ italic_n end_POSTSUPERSCRIPT end_ARG  is decreasing with   italic- \\epsilon italic_ , so for a feasible solution, we require a large   italic- \\epsilon italic_ , i.e., a small   n n  \\sqrt[n]{\\delta} nth-root start_ARG italic_n end_ARG start_ARG italic_ end_ARG . Considering a similar scenario with  q  [ 0.1 , 0.8 ] q 0.1 0.8 q\\in[0.1,0.8] italic_q  [ 0.1 , 0.8 ]  and  n  [ 2 , 16 ] n 2 16 n\\in[2,16] italic_n  [ 2 , 16 ] , where  q i  q j =   ( n i  n j ) subscript q i subscript q j  subscript n i subscript n j q_{i}-q_{j}=\\eta(n_{i}-n_{j}) italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = italic_ ( italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_n start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) , as shown in Figure  3 , high-probability layer erasure is primarily achievable in lower layers, as we cannot retrieve much information from higher layers. However, we also observe that with a small probability, effectively erasing higher layers is possible, indicating that RAG can significantly enhance LLM reasoning. This suggests that if we are fortunate enough to retrieve documents that directly lead to the answer, we can substantially reduce reasoning complexity. Nonetheless, in typical scenarios, the assistance is limited since most retrieved documents contain only basic information.",
            "The proof is detailed in Appendix  B.3 . From the above theorem, it is evident that achieving better performance requires the input to incorporate information that effectively distinguishes irrelevant tokens (a large  I  ( w ^ ; v ) I ^ w v I(\\hat{{\\bm{w}}};{\\bm{v}}) italic_I ( over^ start_ARG bold_italic_w end_ARG ; bold_italic_v ) ) and the necessary information for inference (a large  I  ( s ; v ) I s v I({\\bm{s}};{\\bm{v}}) italic_I ( bold_italic_s ; bold_italic_v ) ). Additionally, there should be minimal noise incorporated into the input (a large    \\delta italic_ ). As previously noted, the self-attention module struggles to filter out irrelevant information while retaining relevant tokens, which makes it difficult to increase    \\delta italic_ . Furthermore, it is difficult to integrate additional information about  w ^ ^ w \\hat{{\\bm{w}}} over^ start_ARG bold_italic_w end_ARG  (i.e., to increase  I  ( w ^ ; v ) I ^ w v I(\\hat{{\\bm{w}}};{\\bm{v}}) italic_I ( over^ start_ARG bold_italic_w end_ARG ; bold_italic_v ) ) because the input to the MLP layer is given by  v =  j a i , j  x j  W v v subscript j subscript a i j subscript x j subscript W v {\\bm{v}}=\\sum_{j}a_{i,j}{\\bm{x}}_{j}{\\bm{W}}_{v} bold_italic_v =  start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_a start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT bold_italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , resulting in  v v {\\bm{v}} bold_italic_v  being an embedding of dimension  m m m italic_m  and precision  p p p italic_p . This limited dimensionality makes it challenging to encapsulate all the necessary information for filtering out irrelevant documents for all involved tokens. Consequently, the Feed Forward Layer is unlikely to achieve optimal performance when dealing with noisy input.",
            "Furthermore, this method renders Assumption  2.3  unrealistic, as assessing relevance requires a deep understanding of the document, thereby increasing the number of layers necessary for effective information extraction. If we assume that  t t t italic_t  layers are needed to evaluate relevance, then extracting information about node  u l , j subscript u l j u_{l,j} italic_u start_POSTSUBSCRIPT italic_l , italic_j end_POSTSUBSCRIPT  would require    l + t   l t \\lambda\\cdot l+t italic_  italic_l + italic_t  layers. This could potentially exceed  l l l italic_l , and extracting information from documents may necessitate a greater reasoning depth than standard reasoning.  Consequently, nodes located below layer  t 1   t 1  \\frac{t}{1-\\lambda} divide start_ARG italic_t end_ARG start_ARG 1 - italic_ end_ARG  cannot access document information because their reasoning would be finished faster than information extraction of documents. Then the reasoning depth cannot be easily reduced, as it primarily occurs in the lower layers. Therefore, leveraging document information may not enhance reasoning ability; in fact, it could potentially degrade performance if the model becomes overly reliant on document information rather than employing vanilla reasoning.",
            "In Table  3 , we show the performance when the prompt is in regular order, which means that we put the documents ahead of the query. We can observe that although DPrompt tuning performs best, the improvement is not as significant as the reversed order, which shows that the reversed order can actually help when faced with irrelevant information"
        ]
    },
    "global_footnotes": []
}