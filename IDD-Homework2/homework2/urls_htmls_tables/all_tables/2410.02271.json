{
    "id_table_1": {
        "caption": "TABLE I:  Comparison of the statistics of existing text-music retrieval datasets and CoLLAP.",
        "table": "S3.T1.4",
        "footnotes": [],
        "references": [
            "The ability to effectively model temporal characteristics is essential in the representation learning of audio waveforms,  especially for complex and full-length music tracks.  Music information retrieval works  [ 1 ,  2 ]  have studied approaches to extract musical temporal and structural information,  which can be further used to augment models music understanding abilities  [ 3 ] .  The recent contrastive learning approaches  [ 4 ,  5 ,  6 ]  enable to extract such information as latent audio representations,  which are trained to distinguish between matched text-audio pairs and other mismatched pairs by capturing distinctive features in the audio data (illustrated in Figure  1(a) ).  However, such methods have focused on relatively short segments, limiting the models ability to handle longer, more nuanced sequences.",
            "To address these challenges, we introduce Contrastive Long-form Language-audio Pretraining (CoLLAP),  which extends the perception window to handle both long-form audio inputs and detailed language descriptions.  We illustrate the comparison between the conventional CLAP model and our proposed CoLLAP model in Figure  1 .  The CoLLAP model uses a feature extractor to segment music tracks into frames and encode each by a kernel function.  Then kernel-wise and temporal attention mechanisms are employed to measure global and temporal alignment between audio and text.  Finally, the model is optimized with contrastive learning using weighted similarity scores from both kernel-wise and temporal attention.  CoLLAP effectively extends the perception window for both the input audio (up to 5 minutes) and the language descriptions (exceeding 250 words),  which enables retrieval of full-length music tracks with fine-grained music descriptions."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:    The retrieval performance of three variants of Larger-CLAP and two variants of CoLLAP on four evaluation datasets.  We report recall values of rank 5, 20, and 100 for text-to-music (T2M) and music-to-text (M2T) retrieval.  The best values are highlighted in bold fonts, while the second-best values are underlined.",
        "table": "S4.T2.4",
        "footnotes": [],
        "references": [
            "We illustrate our CoLLAP model design in Figure  2 ,  where full-length music track waveform is processed with a dual-feature extractor,  while textual representations are extracted from musical structure augmented captions.  We split music tracks of variable lengths into frames to enable audio temporal attention with texts,  which extracts and measures both the global and temporal multimodal alignment scores.  With the temporal attention augmented alignment scores,  we follow the conventional contrastive learning scheme  [ 9 ,  4 ,  10 ,  6 ] ,  where the contrastive loss will be propagated back to both the temporal attention and the feature extractors."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III:  Speech and audio retrieval on the VCTK dataset  [ 20 ] . We report Recall@ k k k italic_k  metrics for text-to-audio (T2A) and audio-to-text (A2T) retrieval.",
        "table": "S4.T3.10",
        "footnotes": [],
        "references": []
    },
    "id_table_4": {
        "caption": "TABLE IV:  Wikipedia context and audio retrieval on the MusicCaps and SongDescriber datasets. We report Recall@ k k k italic_k  metrics for wiki-to-music (W2M) and music-to-wiki (M2W) retrieval.",
        "table": "S4.T4.7",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}