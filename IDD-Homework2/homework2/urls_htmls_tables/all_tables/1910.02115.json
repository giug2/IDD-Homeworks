{
    "PAPER'S NUMBER OF TABLES": 8,
    "S3.T1": {
        "caption": "TABLE I: Saturated performances of SCBFwP with different update rates",
        "table": "<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Update Rate</span></th>\n<th id=\"S3.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">AUCROC</span></th>\n<th id=\"S3.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">AUCPR</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">10%</td>\n<td id=\"S3.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.9776</td>\n<td id=\"S3.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.9695</td>\n</tr>\n<tr id=\"S3.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.3.2.1\" class=\"ltx_td ltx_align_center\">20%</td>\n<td id=\"S3.T1.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.9772</td>\n<td id=\"S3.T1.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.9686</td>\n</tr>\n<tr id=\"S3.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.4.3.1\" class=\"ltx_td ltx_align_center\">30%</td>\n<td id=\"S3.T1.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.9777</td>\n<td id=\"S3.T1.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.9697</td>\n</tr>\n<tr id=\"S3.T1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.5.4.1\" class=\"ltx_td ltx_align_center\">40%</td>\n<td id=\"S3.T1.1.5.4.2\" class=\"ltx_td ltx_align_center\">0.9768</td>\n<td id=\"S3.T1.1.5.4.3\" class=\"ltx_td ltx_align_center\">0.9604</td>\n</tr>\n<tr id=\"S3.T1.1.6.5\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.6.5.1\" class=\"ltx_td ltx_align_center\">50%</td>\n<td id=\"S3.T1.1.6.5.2\" class=\"ltx_td ltx_align_center\">0.9780</td>\n<td id=\"S3.T1.1.6.5.3\" class=\"ltx_td ltx_align_center\">0.9695</td>\n</tr>\n<tr id=\"S3.T1.1.7.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.7.6.1\" class=\"ltx_td ltx_align_center\">60%</td>\n<td id=\"S3.T1.1.7.6.2\" class=\"ltx_td ltx_align_center\">0.9774</td>\n<td id=\"S3.T1.1.7.6.3\" class=\"ltx_td ltx_align_center\">0.9682</td>\n</tr>\n<tr id=\"S3.T1.1.8.7\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.8.7.1\" class=\"ltx_td ltx_align_center\">70%</td>\n<td id=\"S3.T1.1.8.7.2\" class=\"ltx_td ltx_align_center\">0.9774</td>\n<td id=\"S3.T1.1.8.7.3\" class=\"ltx_td ltx_align_center\">0.9688</td>\n</tr>\n<tr id=\"S3.T1.1.9.8\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.9.8.1\" class=\"ltx_td ltx_align_center\">80%</td>\n<td id=\"S3.T1.1.9.8.2\" class=\"ltx_td ltx_align_center\">0.9781</td>\n<td id=\"S3.T1.1.9.8.3\" class=\"ltx_td ltx_align_center\">0.9703</td>\n</tr>\n<tr id=\"S3.T1.1.10.9\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.10.9.1\" class=\"ltx_td ltx_align_center\">90%</td>\n<td id=\"S3.T1.1.10.9.2\" class=\"ltx_td ltx_align_center\">0.9774</td>\n<td id=\"S3.T1.1.10.9.3\" class=\"ltx_td ltx_align_center\">0.9676</td>\n</tr>\n<tr id=\"S3.T1.1.11.10\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.11.10.1\" class=\"ltx_td ltx_align_center ltx_border_b\">100%</td>\n<td id=\"S3.T1.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_border_b\">0.9775</td>\n<td id=\"S3.T1.1.11.10.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.9685</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "The Stochastic Channel-Based Federated Learning (SCBF) method computes the norms of channels in gradients result from the local training output after each global loop, calculates the ",
                "Œ±",
                "ùõº",
                "\\alpha",
                "-percentile of the channel norms and then sifts out the channels that have greater variation in the gradients than the percentile for the server update. In this method, the server seizes the information from those uploading channels with biggest variation, thus achieving comparative performances with the stat-of-the-art method which has to convey the entire local weights to the server when updating.",
                "Fig ",
                "LABEL:Fig:_SCBF_Model",
                " shows the relationship between the server and clients and demonstrates the process of server update."
            ]
        ]
    },
    "S3.T2": {
        "caption": "TABLE II: Saturated performances of SCBF compared with FA",
        "table": "<table id=\"S3.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th id=\"S3.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T2.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">AUCROC</span></th>\n<th id=\"S3.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T2.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">AUCPR</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">SCBF</td>\n<td id=\"S3.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.9825</td>\n<td id=\"S3.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.9763</td>\n</tr>\n<tr id=\"S3.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_b\">Federated Averaging</td>\n<td id=\"S3.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_b\">0.9821</td>\n<td id=\"S3.T2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.9731</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "The update rate controls how many channels are selected whose non-zero part is uploaded to the server in each global loop, playing a vital role in affecting the final performance. To choose a suitable update rate for our distributed system, we implement SCBF models with different update rates ranging from 10% to 100%. The neural network pruning is used in this step to accelerate the training process. The performances are plotted in the first row of Fig ",
                "3",
                ". The result shows that even with 10% channels uploaded to the server, the SCBF model achieves an AUCROC of 0.9776 and an AUCPR of 0.9695, and this result even outperforms the model when sharing all parameters with the server. It confirms the intuition behind SCBF: the importance of a feature differs when training on different datasets, and we could extract important information from the channels that features with biggest variations go through. We could infer that less than 10% of the channels contain the most fundamental information and ignoring the rest information does little harm to the learning of models. Besides, using a wide range of upload rates only leads to a 0.01319 amplitude in AUCROC and a 0.02739 amplitude in AUCPR, which facilitates the configuration process with a stable high performance.",
                "To show the effectiveness of SCBF method compared to Federated averaging (FA), which implements the federated learning by averaging the gradients obtained from local training processes and is widely used in distributed systems. We set the update rate as 30% for SCBF and conduct both methods for federated learning on the same datasets for 100 global loops without pruning. As shown in Fig ",
                "3",
                ", our model reaches saturation at the ",
                "20",
                "‚Äã",
                "t",
                "‚Äã",
                "h",
                "20",
                "ùë°",
                "‚Ñé",
                "20th",
                " global loop, much faster than the FA which saturates at the ",
                "60",
                "‚Äã",
                "t",
                "‚Äã",
                "h",
                "60",
                "ùë°",
                "‚Ñé",
                "60th",
                " global loop. The performance of SCBF keep exceeding that of FA. In the ",
                "4",
                "‚Äã",
                "t",
                "‚Äã",
                "h",
                "4",
                "ùë°",
                "‚Ñé",
                "4th",
                " global loop, SCBF achieves 0.05388 higher in AUCROC and 0.09695 higher in AUCPR than FA. After 100 global loops, the AUCROC of SCBF is 0.0033 higher than FA and the AUCPR of SCBF is 0.0032 higher than FA. We could conclude that our method achieves comparative performance to the Federated Averaging method with higher saturating speed. What stands out in our method is that our model doesn‚Äôt have to reveal the overall local models to the server and makes it hard for attackers to track the channels we choose.",
                "As shown in Fig ",
                "5",
                ", when the upload rate for channels is set to 30%, the parameters uploaded to the server is 45% using positive selection. With half of the parameters unrevealed to the server, the model achieves better performance and higher saturating speed."
            ]
        ]
    },
    "S3.T3": {
        "caption": "TABLE III: Saturated performances of SCBF and FA with and without pruning",
        "table": "<table id=\"S3.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th id=\"S3.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T3.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">AUCROC</span></th>\n<th id=\"S3.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T3.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">AUCPR</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T3.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">FA</td>\n<td id=\"S3.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.9821</td>\n<td id=\"S3.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.9731</td>\n</tr>\n<tr id=\"S3.T3.1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.3.2.1\" class=\"ltx_td ltx_align_center\">SCBF</td>\n<td id=\"S3.T3.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.9825</td>\n<td id=\"S3.T3.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.9763</td>\n</tr>\n<tr id=\"S3.T3.1.4.3\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.4.3.1\" class=\"ltx_td ltx_align_center\">FAwP</td>\n<td id=\"S3.T3.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.9809</td>\n<td id=\"S3.T3.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.9683</td>\n</tr>\n<tr id=\"S3.T3.1.5.4\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_b\">SCBFwP</td>\n<td id=\"S3.T3.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b\">0.9776</td>\n<td id=\"S3.T3.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.9694</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "To speed up the training process and reduce the size of the neural network, we conduct network pruning for several loops after pre-training the model. It is important to train a small-scaled deep learning model with high processing speed. In our trail, we set the pruning rate for each global loop to 10%, which demonstrates the proportion of neurons to be pruned in the training loop. The total proportion of neurons to be pruned in the first several loops is set to 47%, which decides the final scale of pruned model. Fig ",
                "4",
                " compares the AUCROC and AUCPR values of both models, the SCBF and FA, with and without pruning. The results show that network pruning could speed up training process and accelerate convergence while maintaining higher performances. The results also show, as expected, that pruning 47% neurons from the network will decrease the final performance due to the simplified model structure. The AUCROC for SCBF with pruning is reduced by 0.0048 and the AUCPR for it is reduced by 0.006814. We can observe a reduction of 0.0012 in AUCROC and 0.0047 in AUCPR for Federated Averaging method compared to FA with pruning. The reduction in performances is negligible in many application situations but the acceleration in both saturating speed and training speed is quite beneficial, the latter of which will be discussed in the following section.",
                "Moreover, the best performance is achieved by SCBF after 100 loops of training with 0.9825 for AUCROC and 0.9763 for AUCPR. The highest evaluation in the first 5 loops is obtained by the SCBF model with pruning. The results demonstrate that SCBF is a reliable choice for federated learning and SCBF with pruning method might be a better choice for whom preferring a quicker saturating speed.",
                "In the first row of Fig ",
                "4",
                ", there is an obvious decline in the performance for the SCBF with pruning, which indicates an over-pruned phenomenon for our trail. So there is a tradeoff between time efficiency and the final accuracy. And by tuning the pruning rate for each global loop and the total pruned rate of the model, we could achieve better performance because if only the redundant neurons are pruned, the model could promote its learning efficiency without remembering useless information.",
                "Also as regard to the stability of our model with the pruning rate and total pruned fraction (also called total pruned rate), we execute the models of SCBFwP controlling the variate. Firstly, we fix the total pruned fraction as 50% and run the programs with different pruning rates ranging from 10% to 50%. As shown in the figure, with the pruning rate increasing, the final performance gets better and saturates quicker for most circumstances, but there are also exceptions regarding the high performances of 10% pruning rate for both AUCROC and AUCPR, and lower performance of 40% pruning rate for AUCPR. In the third row of Fig ",
                "4",
                ", we fix the pruning rate to 10% and execute pruning for different times ranging from 1 to 6. The total pruned fractions are calculated and annotated in the corresponding labels. The figure shows that SCBF gets better performance when reducing the times of pruning. The results with a fixed pruning rate is more stable than those with a fixed total pruned rate, indicating that people should pay more attention to the selection of pruning rate for each step when building models and it is stable for a SCBF model to adjust the times of neural network pruning. So after choosing a suitable pruning rate, we could appropriately increase the loops in which the model is pruned to shorten the executing time with little effect on the final performance.",
                "As shown in Fig ",
                "5",
                ", the SCBFwP could save 85% of the transinformation compared to Federated Averaging. And for SCBF, when the upload rate for channels is set to 30%, the parameters uploaded to the server is 45% using positive selection."
            ]
        ]
    },
    "S3.T4": {
        "caption": "TABLE IV: Saturated performances of SCBFwP when total pruned proportion is fixed and pruning rate for each training loop changes",
        "table": "<table id=\"S3.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T4.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Pruning Rate/Loop</span></th>\n<th id=\"S3.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T4.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">AUCROC</span></th>\n<th id=\"S3.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T4.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">AUCPR</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T4.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T4.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">10%</td>\n<td id=\"S3.T4.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.9765</td>\n<td id=\"S3.T4.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.9661</td>\n</tr>\n<tr id=\"S3.T4.1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T4.1.3.2.1\" class=\"ltx_td ltx_align_center\">20%</td>\n<td id=\"S3.T4.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.9730</td>\n<td id=\"S3.T4.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.9568</td>\n</tr>\n<tr id=\"S3.T4.1.4.3\" class=\"ltx_tr\">\n<td id=\"S3.T4.1.4.3.1\" class=\"ltx_td ltx_align_center\">30%</td>\n<td id=\"S3.T4.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.9763</td>\n<td id=\"S3.T4.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.9662</td>\n</tr>\n<tr id=\"S3.T4.1.5.4\" class=\"ltx_tr\">\n<td id=\"S3.T4.1.5.4.1\" class=\"ltx_td ltx_align_center\">40%</td>\n<td id=\"S3.T4.1.5.4.2\" class=\"ltx_td ltx_align_center\">0.9693</td>\n<td id=\"S3.T4.1.5.4.3\" class=\"ltx_td ltx_align_center\">0.9465</td>\n</tr>\n<tr id=\"S3.T4.1.6.5\" class=\"ltx_tr\">\n<td id=\"S3.T4.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_b\">50%</td>\n<td id=\"S3.T4.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_b\">0.9769</td>\n<td id=\"S3.T4.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.9663</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "To speed up the training process and reduce the size of the neural network, we conduct network pruning for several loops after pre-training the model. It is important to train a small-scaled deep learning model with high processing speed. In our trail, we set the pruning rate for each global loop to 10%, which demonstrates the proportion of neurons to be pruned in the training loop. The total proportion of neurons to be pruned in the first several loops is set to 47%, which decides the final scale of pruned model. Fig ",
                "4",
                " compares the AUCROC and AUCPR values of both models, the SCBF and FA, with and without pruning. The results show that network pruning could speed up training process and accelerate convergence while maintaining higher performances. The results also show, as expected, that pruning 47% neurons from the network will decrease the final performance due to the simplified model structure. The AUCROC for SCBF with pruning is reduced by 0.0048 and the AUCPR for it is reduced by 0.006814. We can observe a reduction of 0.0012 in AUCROC and 0.0047 in AUCPR for Federated Averaging method compared to FA with pruning. The reduction in performances is negligible in many application situations but the acceleration in both saturating speed and training speed is quite beneficial, the latter of which will be discussed in the following section.",
                "Moreover, the best performance is achieved by SCBF after 100 loops of training with 0.9825 for AUCROC and 0.9763 for AUCPR. The highest evaluation in the first 5 loops is obtained by the SCBF model with pruning. The results demonstrate that SCBF is a reliable choice for federated learning and SCBF with pruning method might be a better choice for whom preferring a quicker saturating speed.",
                "In the first row of Fig ",
                "4",
                ", there is an obvious decline in the performance for the SCBF with pruning, which indicates an over-pruned phenomenon for our trail. So there is a tradeoff between time efficiency and the final accuracy. And by tuning the pruning rate for each global loop and the total pruned rate of the model, we could achieve better performance because if only the redundant neurons are pruned, the model could promote its learning efficiency without remembering useless information.",
                "Also as regard to the stability of our model with the pruning rate and total pruned fraction (also called total pruned rate), we execute the models of SCBFwP controlling the variate. Firstly, we fix the total pruned fraction as 50% and run the programs with different pruning rates ranging from 10% to 50%. As shown in the figure, with the pruning rate increasing, the final performance gets better and saturates quicker for most circumstances, but there are also exceptions regarding the high performances of 10% pruning rate for both AUCROC and AUCPR, and lower performance of 40% pruning rate for AUCPR. In the third row of Fig ",
                "4",
                ", we fix the pruning rate to 10% and execute pruning for different times ranging from 1 to 6. The total pruned fractions are calculated and annotated in the corresponding labels. The figure shows that SCBF gets better performance when reducing the times of pruning. The results with a fixed pruning rate is more stable than those with a fixed total pruned rate, indicating that people should pay more attention to the selection of pruning rate for each step when building models and it is stable for a SCBF model to adjust the times of neural network pruning. So after choosing a suitable pruning rate, we could appropriately increase the loops in which the model is pruned to shorten the executing time with little effect on the final performance.",
                "As shown in Fig ",
                "5",
                ", the SCBFwP could save 85% of the transinformation compared to Federated Averaging. And for SCBF, when the upload rate for channels is set to 30%, the parameters uploaded to the server is 45% using positive selection."
            ]
        ]
    },
    "S3.T5": {
        "caption": "TABLE V: Saturated performances of SCBFwP when pruning rate for each training loop is fixed and total pruned proportion changes",
        "table": "<table id=\"S3.T5.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T5.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T5.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T5.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Total Pruned Proportion</span></th>\n<th id=\"S3.T5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T5.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">AUCROC</span></th>\n<th id=\"S3.T5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T5.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">AUCPR</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T5.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T5.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">10%</td>\n<td id=\"S3.T5.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.9769</td>\n<td id=\"S3.T5.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.9731</td>\n</tr>\n<tr id=\"S3.T5.1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T5.1.3.2.1\" class=\"ltx_td ltx_align_center\">19%</td>\n<td id=\"S3.T5.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.9797</td>\n<td id=\"S3.T5.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.9722</td>\n</tr>\n<tr id=\"S3.T5.1.4.3\" class=\"ltx_tr\">\n<td id=\"S3.T5.1.4.3.1\" class=\"ltx_td ltx_align_center\">27%</td>\n<td id=\"S3.T5.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.9795</td>\n<td id=\"S3.T5.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.9725</td>\n</tr>\n<tr id=\"S3.T5.1.5.4\" class=\"ltx_tr\">\n<td id=\"S3.T5.1.5.4.1\" class=\"ltx_td ltx_align_center\">34%</td>\n<td id=\"S3.T5.1.5.4.2\" class=\"ltx_td ltx_align_center\">0.9789</td>\n<td id=\"S3.T5.1.5.4.3\" class=\"ltx_td ltx_align_center\">0.9714</td>\n</tr>\n<tr id=\"S3.T5.1.6.5\" class=\"ltx_tr\">\n<td id=\"S3.T5.1.6.5.1\" class=\"ltx_td ltx_align_center\">41%</td>\n<td id=\"S3.T5.1.6.5.2\" class=\"ltx_td ltx_align_center\">0.9781</td>\n<td id=\"S3.T5.1.6.5.3\" class=\"ltx_td ltx_align_center\">0.9703</td>\n</tr>\n<tr id=\"S3.T5.1.7.6\" class=\"ltx_tr\">\n<td id=\"S3.T5.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_border_b\">47%</td>\n<td id=\"S3.T5.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_b\">0.9778</td>\n<td id=\"S3.T5.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.9697</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "To speed up the training process and reduce the size of the neural network, we conduct network pruning for several loops after pre-training the model. It is important to train a small-scaled deep learning model with high processing speed. In our trail, we set the pruning rate for each global loop to 10%, which demonstrates the proportion of neurons to be pruned in the training loop. The total proportion of neurons to be pruned in the first several loops is set to 47%, which decides the final scale of pruned model. Fig ",
                "4",
                " compares the AUCROC and AUCPR values of both models, the SCBF and FA, with and without pruning. The results show that network pruning could speed up training process and accelerate convergence while maintaining higher performances. The results also show, as expected, that pruning 47% neurons from the network will decrease the final performance due to the simplified model structure. The AUCROC for SCBF with pruning is reduced by 0.0048 and the AUCPR for it is reduced by 0.006814. We can observe a reduction of 0.0012 in AUCROC and 0.0047 in AUCPR for Federated Averaging method compared to FA with pruning. The reduction in performances is negligible in many application situations but the acceleration in both saturating speed and training speed is quite beneficial, the latter of which will be discussed in the following section.",
                "Moreover, the best performance is achieved by SCBF after 100 loops of training with 0.9825 for AUCROC and 0.9763 for AUCPR. The highest evaluation in the first 5 loops is obtained by the SCBF model with pruning. The results demonstrate that SCBF is a reliable choice for federated learning and SCBF with pruning method might be a better choice for whom preferring a quicker saturating speed.",
                "In the first row of Fig ",
                "4",
                ", there is an obvious decline in the performance for the SCBF with pruning, which indicates an over-pruned phenomenon for our trail. So there is a tradeoff between time efficiency and the final accuracy. And by tuning the pruning rate for each global loop and the total pruned rate of the model, we could achieve better performance because if only the redundant neurons are pruned, the model could promote its learning efficiency without remembering useless information.",
                "Also as regard to the stability of our model with the pruning rate and total pruned fraction (also called total pruned rate), we execute the models of SCBFwP controlling the variate. Firstly, we fix the total pruned fraction as 50% and run the programs with different pruning rates ranging from 10% to 50%. As shown in the figure, with the pruning rate increasing, the final performance gets better and saturates quicker for most circumstances, but there are also exceptions regarding the high performances of 10% pruning rate for both AUCROC and AUCPR, and lower performance of 40% pruning rate for AUCPR. In the third row of Fig ",
                "4",
                ", we fix the pruning rate to 10% and execute pruning for different times ranging from 1 to 6. The total pruned fractions are calculated and annotated in the corresponding labels. The figure shows that SCBF gets better performance when reducing the times of pruning. The results with a fixed pruning rate is more stable than those with a fixed total pruned rate, indicating that people should pay more attention to the selection of pruning rate for each step when building models and it is stable for a SCBF model to adjust the times of neural network pruning. So after choosing a suitable pruning rate, we could appropriately increase the loops in which the model is pruned to shorten the executing time with little effect on the final performance.",
                "As shown in Fig ",
                "5",
                ", the SCBFwP could save 85% of the transinformation compared to Federated Averaging. And for SCBF, when the upload rate for channels is set to 30%, the parameters uploaded to the server is 45% using positive selection."
            ]
        ]
    },
    "S3.T6": {
        "caption": "TABLE VI: Time consumed by SCBF and FA with and without pruning",
        "table": "<table id=\"S3.T6.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T6.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T6.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span id=\"S3.T6.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th id=\"S3.T6.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T6.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Time (second)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T6.1.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T6.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">Federated Averaging</th>\n<td id=\"S3.T6.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">8679</td>\n</tr>\n<tr id=\"S3.T6.1.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T6.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">Federated Averaging with Pruning</th>\n<td id=\"S3.T6.1.3.2.2\" class=\"ltx_td ltx_align_center\">4508</td>\n</tr>\n<tr id=\"S3.T6.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T6.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">SCBF</th>\n<td id=\"S3.T6.1.4.3.2\" class=\"ltx_td ltx_align_center\">19696</td>\n</tr>\n<tr id=\"S3.T6.1.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T6.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\">SCBFwP</th>\n<td id=\"S3.T6.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b\">8469</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "SCBF preserves the privacy by adding a channel-based upload algorithm, which will lead to an increased burden of calculations when applied to a complex neural network, which, however, could be addressed by introducing pruning process in several global loops. To illustrate this, the time consumed by SCBF and FA before and after pruning described in the last section are listed in Table VI. As could be seen in the table, pruning process could reduce 57% of the time for SCBF and 48% of the time consumed by FA."
        ]
    },
    "S3.T7": {
        "caption": "TABLE VII: Time consumed by SCBFwP with different update rates",
        "table": "<table id=\"S3.T7.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T7.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span id=\"S3.T7.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Update Rate</span></th>\n<th id=\"S3.T7.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T7.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Time (second)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T7.1.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">10%</th>\n<td id=\"S3.T7.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">8339</td>\n</tr>\n<tr id=\"S3.T7.1.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">20%</th>\n<td id=\"S3.T7.1.3.2.2\" class=\"ltx_td ltx_align_center\">8545</td>\n</tr>\n<tr id=\"S3.T7.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">30%</th>\n<td id=\"S3.T7.1.4.3.2\" class=\"ltx_td ltx_align_center\">8469</td>\n</tr>\n<tr id=\"S3.T7.1.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">40%</th>\n<td id=\"S3.T7.1.5.4.2\" class=\"ltx_td ltx_align_center\">7987</td>\n</tr>\n<tr id=\"S3.T7.1.6.5\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">50%</th>\n<td id=\"S3.T7.1.6.5.2\" class=\"ltx_td ltx_align_center\">8359</td>\n</tr>\n<tr id=\"S3.T7.1.7.6\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">60%</th>\n<td id=\"S3.T7.1.7.6.2\" class=\"ltx_td ltx_align_center\">12577</td>\n</tr>\n<tr id=\"S3.T7.1.8.7\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">70%</th>\n<td id=\"S3.T7.1.8.7.2\" class=\"ltx_td ltx_align_center\">9278</td>\n</tr>\n<tr id=\"S3.T7.1.9.8\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">80%</th>\n<td id=\"S3.T7.1.9.8.2\" class=\"ltx_td ltx_align_center\">11462</td>\n</tr>\n<tr id=\"S3.T7.1.10.9\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">90%</th>\n<td id=\"S3.T7.1.10.9.2\" class=\"ltx_td ltx_align_center\">13169</td>\n</tr>\n<tr id=\"S3.T7.1.11.10\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.11.10.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\">100%</th>\n<td id=\"S3.T7.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_border_b\">13030</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table VII shows that models with lower update rates tend to consume less time than those with larger update rates, indicating that choosing a lower rate for update could better preserve the privacy as well as save time."
        ]
    },
    "S3.T8": {
        "caption": "TABLE VIII: Time consumed by SCBFwP with different pruning rates for each loop or different total pruned proportions",
        "table": "<table id=\"S3.T8.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T8.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span id=\"S3.T8.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Pruning Rate/Loop</span></th>\n<th id=\"S3.T8.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S3.T8.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Time (second)</span></th>\n<th id=\"S3.T8.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span id=\"S3.T8.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Total Pruned</span></th>\n<th id=\"S3.T8.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T8.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Time (second)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T8.1.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">10%</th>\n<td id=\"S3.T8.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">11144</td>\n<th id=\"S3.T8.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">10%</th>\n<td id=\"S3.T8.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">25755</td>\n</tr>\n<tr id=\"S3.T8.1.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">20%</th>\n<td id=\"S3.T8.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8561</td>\n<th id=\"S3.T8.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">19%</th>\n<td id=\"S3.T8.1.3.2.4\" class=\"ltx_td ltx_align_center\">22717</td>\n</tr>\n<tr id=\"S3.T8.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">30%</th>\n<td id=\"S3.T8.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">11852</td>\n<th id=\"S3.T8.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">27%</th>\n<td id=\"S3.T8.1.4.3.4\" class=\"ltx_td ltx_align_center\">17579</td>\n</tr>\n<tr id=\"S3.T8.1.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">40%</th>\n<td id=\"S3.T8.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8389</td>\n<th id=\"S3.T8.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">34%</th>\n<td id=\"S3.T8.1.5.4.4\" class=\"ltx_td ltx_align_center\">15909</td>\n</tr>\n<tr id=\"S3.T8.1.6.5\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">50%</th>\n<td id=\"S3.T8.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">12000</td>\n<th id=\"S3.T8.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">41%</th>\n<td id=\"S3.T8.1.6.5.4\" class=\"ltx_td ltx_align_center\">8050</td>\n</tr>\n<tr id=\"S3.T8.1.7.6\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\">-</th>\n<td id=\"S3.T8.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">-</td>\n<th id=\"S3.T8.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\">47%</th>\n<td id=\"S3.T8.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_b\">8469</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "As regard to the time consumed by models with different pruning rates and different total pruned rates, Table VIII shows that different pruning rates for each global loop can equally save the time. And the model will consume more time if too few neurons are pruned due to the executing time of pruning process. With a fixed pruning rate, time consumed by the model tends to decrease by reducing the model size."
        ]
    }
}