{
    "S4.T1": {
        "caption": "Table 1: The training settings.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.1.1\">Settings</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.1.2\">Value</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.2.1.1\">Training Epochs</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.2.1.2\">50</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.3.2.1\">Early stopping epochs</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.3.2.2\">5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.4.3.1\">Batch size</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.4.3.2\">32</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.5.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.5.4.1\">LSTM features in the hidden state</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.5.4.2\">400</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.6.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.6.5.1\">LSTM recurrent layers</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.6.5.2\">2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.7.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.7.6.1\">Dropout probability</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.7.6.2\">15%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.8.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.8.7.1\">Learning rate</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.8.7.2\">0.0008</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.9.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.9.8.1\">Optimizer decay</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.9.8.2\">35%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.10.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.10.9.1\">Node representation padding</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.10.9.2\">40</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.11.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.11.10.1\">Node2Vec walk length</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.11.10.2\">50</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.12.11\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T1.1.12.11.1\">Node2Vec node walks</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T1.1.12.11.2\">50</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "We used PyTorch to train our model. The used hardware consisted of NVIDIA A100-SXM-80GB GPU node, a main memory of 50GB, and AMD EPYC MILAN (3rd gen) CPU processor; we utilized 8 cores of the CPU. Table 1 shows our training settings. We used Glove 300 dimensional word embeddings trained on 6 Billion tokens with a 400,000 words vocabulary. For the Node2Vec training, we used balanced settings in the biased walks. Specifically, we used equal parameters for the breadth-first and the depth-first sampling. For the model training optimization, we employed Adam algorithm. In the following sections we explain the evaluation metrics, the comparison models, the main results analysis, and the model components ablation study."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: The results of our model and the comparison models on the FB15K dataset.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.1.1\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.1.2\">Mean Rank</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.1.3\">Hits@1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.1.4\">Filtered Mean Rank</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.1.5\">Filtered Hits@1</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T2.1.2.1.1\">TransE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2404.16206v1#bib.bib3\" title=\"\">3</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.1.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.1.3\">65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.1.4\">2.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.1.5\">84</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.3.2.1\">TransR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2404.16206v1#bib.bib15\" title=\"\">15</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.2.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.2.3\">42</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.2.4\">2.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.2.5\">91</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.4.3.1\">KG-BERT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2404.16206v1#bib.bib34\" title=\"\">34</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.3.2\">1.69</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.3.3\">69</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.3.4\">1.25</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.4.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.4.3.5.1\">96</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.5.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T2.1.5.4.1\">Shallom <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2404.16206v1#bib.bib7\" title=\"\">7</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.4.2\">1.59</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.4.3\">73</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.4.4\">1.26</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.4.5\">95</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.6.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T2.1.6.5.1\">Ours</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.1.6.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.6.5.2.1\">1.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.1.6.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.6.5.3.1\">74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.1.6.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.6.5.4.1\">1.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.1.6.5.5\">94</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "We show our main results in Table 2. The results show superiority of our relation prediction model compared to the best results in other RP models. However, KG-BERT has equivalent filtered Hits@1 score. Nevertheless, our model beats the scores of the same model on the remaining metrics with good margin. The mean rank scores for TransE and TransR were not reported or found in any other study. We reason our model\u2019s superiority by the combination of the structural and textual details for every node. More precisely, we find the efficient utilization of the textual content leading the performance to this level as we show in the ablation study."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: The results of RPEST variants on the FB15K dataset.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.1\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.2\">Mean Rank</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.3\">Hits@1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.4\">Filtered Mean Rank</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.5\">Filtered Hits@1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1.6\">Epoch Time<sub class=\"ltx_sub\" id=\"S4.T3.1.1.1.6.1\">min</sub>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T3.1.2.1.1\">RPEST<sub class=\"ltx_sub\" id=\"S4.T3.1.2.1.1.1\">Glove</sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.2.1.2\">1.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.2.1.3\">68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.2.1.4\">1.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.2.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.1.5.1\">94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.2.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.1.6.1\">5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T3.1.3.2.1\">RPEST<sub class=\"ltx_sub\" id=\"S4.T3.1.3.2.1.1\">BERT</sub>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.2.2\">1.70</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.2.3\">67</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.2.4\">1.31</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.2.5\">92</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.3.2.6\">13</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.4.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T3.1.4.3.1\">RPEST</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.1.4.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.4.3.2.1\">1.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.1.4.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.4.3.3.1\">74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.1.4.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.4.3.4.1\">1.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.1.4.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.4.3.5.1\">94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.1.4.3.6\">6</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "To evaluate the effectiveness of our model\u2019s units, we created two variants to observe the model results after excluding some of its modules. Particularly, we created a variant that does not employ the nodes\u2019 structural information in the input. Therefore, we excluded the usage of Node2Vec in our model and we only kept Glove text encoder by creating a variant named RPEST<sub class=\"ltx_sub\" id=\"S4.SS4.p1.1.1\">Glove</sub>. Furthermore, we evaluated our choice of language models by replacing Glove with BERT language model in a variant named RPEST\nGlove. Furthermore, we evaluated our choice of language models by replacing Glove with BERT language model in a variant named RPEST<sub class=\"ltx_sub\" id=\"S4.SS4.p1.1.2\">BERT</sub>. We show the evaluation metric scores of the different variants in Table \nBERT. We show the evaluation metric scores of the different variants in Table 3. The table also shows the epoch time in minutes for each variant, and we rely on the time values to emphasize the performance difference."
        ]
    }
}