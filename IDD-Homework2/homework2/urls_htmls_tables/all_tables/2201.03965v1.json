{
    "S4.T1": {
        "caption": "Table 1: VQA accuracy of ViLBERT (Lu et al., 2019) with different number of region proposals. Accuracies are computed over all the question/image pairs in the VQA-HAT (Das et al., 2016) validation set.",
        "table": "<table id=\"S4.T1.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.4.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.4.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Method</th>\n<th id=\"S4.T1.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">VQA Accuracy</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.4.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.4.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">ViLBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2019</a>)</cite> (36 Region Proposals)</td>\n<td id=\"S4.T1.4.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">76.57</td>\n</tr>\n<tr id=\"S4.T1.4.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.4.3.2.1\" class=\"ltx_td ltx_align_center\">ViLBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2019</a>)</cite> (72 Region Proposals)</td>\n<td id=\"S4.T1.4.3.2.2\" class=\"ltx_td ltx_align_center\">79.39</td>\n</tr>\n<tr id=\"S4.T1.4.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.4.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">ViLBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2019</a>)</cite> (108 Region Proposals)</td>\n<td id=\"S4.T1.4.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">80.83</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Increasing the number of region proposals led layers 3-6 of the model to attend to regions more similar to those attended by humans. The increased context due to more region proposals also improved the model’s VQA accuracy (Table 1 and examples in Fig. 4). The region proposals are generated using Faster RCNN (Ren et al., 2016), an object detection architecture. Therefore, even in the first co-attention layer, which has little interaction with the language stream, the rank-correlation of the model’s visual attention with human attention is well above chance. The correlation in the lower layers is likely due to the observation that the majority of the questions in the VQA dataset (Antol et al., 2015) focus either on object categories or object attributes that are salient in terms of basic visual features."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: VQA accuracy of ViLBERT (Lu et al., 2019) in different controls. Note that the reported accuracy is over question/image pairs in VQA-HAT (Das et al., 2016) validation set. Refer section 4.2 for more details.",
        "table": "<table id=\"S4.T2.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.4.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.4.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Method</th>\n<th id=\"S4.T2.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">VQA Accuracy</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.4.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.4.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">ViLBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2019</a>)</cite> (Normal)</th>\n<td id=\"S4.T2.4.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">76.57</td>\n</tr>\n<tr id=\"S4.T2.4.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.4.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">ViLBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2019</a>)</cite> (Shuffled Words)</th>\n<td id=\"S4.T2.4.3.2.2\" class=\"ltx_td ltx_align_center\">60.2</td>\n</tr>\n<tr id=\"S4.T2.4.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.4.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">ViLBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2019</a>)</cite> (Unrelated Question/Image Pair)</th>\n<td id=\"S4.T2.4.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">10.8</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "The model’s VQA accuracy dropped considerably after shuffling the words (Table 2). Thus, while attention seems to be largely independent of grammar and semantics, the ability to answer the questions correctly does require some notion of grammar and/or semantic information."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Accuracy for different VQA models on the VQA test-std set as reported in (Yang et al., 2016; Lu et al., 2016, 2019). Error bars in rank-correlation here show standard error of means.",
        "table": "<table id=\"S4.T3.4\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.4.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">Method</td>\n<td id=\"S4.T3.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">Rank-Correlation</td>\n<td id=\"S4.T3.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">VQA Accuracy</td>\n</tr>\n<tr id=\"S4.T3.4.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Random</td>\n<td id=\"S4.T3.4.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.000 ± 0.001</td>\n<td id=\"S4.T3.4.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr id=\"S4.T3.4.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\">SAN-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al., <a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>\n</td>\n<td id=\"S4.T3.4.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.249 ± 0.004</td>\n<td id=\"S4.T3.4.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">58.9</td>\n</tr>\n<tr id=\"S4.T3.4.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\">HieCoAtt-W <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>\n</td>\n<td id=\"S4.T3.4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.246 ± 0.004</td>\n<td id=\"S4.T3.4.4.4.3\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr id=\"S4.T3.4.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.5.5.1\" class=\"ltx_td ltx_align_center\">HieCoAtt-P <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>\n</td>\n<td id=\"S4.T3.4.5.5.2\" class=\"ltx_td ltx_align_center\">0.256 ± 0.004</td>\n<td id=\"S4.T3.4.5.5.3\" class=\"ltx_td ltx_align_center\">62.1</td>\n</tr>\n<tr id=\"S4.T3.4.6.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.6.6.1\" class=\"ltx_td ltx_align_center\">HieCoAtt-Q <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>\n</td>\n<td id=\"S4.T3.4.6.6.2\" class=\"ltx_td ltx_align_center\">0.264 ± 0.004</td>\n<td id=\"S4.T3.4.6.6.3\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S4.T3.4.7.7\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_t\">ViLBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</td>\n<td id=\"S4.T3.4.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.7.7.2.1\" class=\"ltx_text ltx_font_bold\">0.434 ± 0.006</span></td>\n<td id=\"S4.T3.4.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.7.7.3.1\" class=\"ltx_text ltx_font_bold\">70.92</span></td>\n</tr>\n<tr id=\"S4.T3.4.8.8\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">Human</td>\n<td id=\"S4.T3.4.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.618 ± 0.006</td>\n<td id=\"S4.T3.4.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">-</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Following the example in Fig. 6, row 1, the same image but using the question “Is this singles or doubles?” (instead of “What color is the floor?”), led to the erroneous answer “singles” and ρ=0.02𝜌0.02\\rho=0.02 (cf. ρ=0.548𝜌0.548\\rho=0.548 for the correct question/image pair). The similarity with human attention was largely independent of the layer number but remained well above chance levels in the case of Unrelated Question/Image Pair (Fig. 5). Visual attention alone is sufficient to drive the rank-correlation with humans. Interestingly, even the unrelated question case shows higher similarity than previous benchmarks that combined visual and correct language information (Table 3). For layers 3-6, the similarity with human attention dropped considerably with respect to the correct question condition. Thus, attention is largely dictated by visual information, combined with focused co-attention driven by the presence of specific key words irrespective of their ordering.",
            "In Table 3, we show the VQA accuracy and rank-correlation of the model’s attention maps and human attention maps for the following networks: ViLBERT (Lu et al., 2019), Stacked Attention Network (Yang et al., 2016) with 2 attention layers (SAN-2), Hierarchical Co-Attention Network (Lu et al., 2016) with Word-Level (HieCoAtt-W), Phrase-Level (HieCoAtt-P), and Question-Level (HieCoAtt-Q). ViLBERT (Lu et al., 2019) uses a multi-modal transformer architecture while SAN-2 (Yang et al., 2016) and HieCoAtt (Lu et al., 2016) are based on CNN and LSTM architectures. The rank-correlation for the CNN/LSTM based models is considerably lower than the transformer-based model indicating a superior co-attention mechanism and better fusion of vision and language information in multi-modal transformers. Finally, it’s interesting also to note that an increase in the VQA accuracy is accompanied by a better correlation with human attention."
        ]
    }
}