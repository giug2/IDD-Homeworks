{
    "id_table_1": {
        "caption": "Table 1:  Comparison with existing human motion datasets. More details can be found in our appendix. In the table, B, H, and F refer to body, hand, and face, respectively. part indicates that the text captions include fine-grained descriptions of body parts, while body means the descriptions are not as detailed. multi and single specify whether the dataset contains multi-person scenarios or only single-person data. Our MotionBase is the largest motion generation dataset and benchmark to date, featuring at least 15 more data than previous datasets, along with additional modalities.",
        "table": "S2.T1.1.1",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "Motion generation is an emerging field with diverse applications in video games, filmmaking, and robotics animation. At the forefront of this area is text-to-motion generation (T2M)  (Ahn et al.,  2018 ; Ahuja & Morency,  2019 ) , which plays a crucial role in translating natural language into human motions. State-of-the-art T2M models typically rely on a combination of the motion quantization methods (e.g., VQ  (Van Den Oord et al.,  2017 ) ), along with a text encoder (e.g., CLIP  (Radford et al.,  2021 ) ) and decoder (e.g., GPT-2  (Radford et al.,  2019 ) ) to generate motion sequences from detailed textual instructions. Despite the availability of a few high-quality datasets  (Guo et al.,  2022a ; Lin et al.,  2024 )  curated in recent years, their limited size restricts current methods to a narrow range of scenarios, creating performance bottlenecks when addressing diverse or unseen motions, as illustrated in Figure  1  (RIGHT).",
            "The rapid advancement of large language models (LLMs)  (Touvron et al.,  2023a )  in multimodal learning has been significantly bolstered by the availability of vast data resources  (Zheng et al.,  2024 ; Xu et al.,  2024 ) . In contrast, the volume of motion data remains considerably smaller than that of visual-text data, as illustrated in Figure  1  (LEFT). This disparity primarily arises from the high costs associated with motion data collection, which often requires specialized wearable devices and substantial human labor for annotation. Consequently, developing a state-of-the-art (SoTA) large motion model based on LLMs presents a significant challenge and remains an unresolved issue. While some recent efforts  (Jiang et al.,  2023 )  have explored this direction, the effectiveness of large motion models has yet to be fully demonstrated.",
            "Data is the foundation of large motion models. With advancements in fields like human pose detection, we are now able to extract high-quality motion sequences from vast amounts of online videos, including datasets like InternViD  (Wang et al.,  2023 )  and WebVid  (Bain et al.,  2021 ) . In its initial public release, our MotionBase contains over one million motion clips, each annotated with fine-grained automatic pseudo labels. A comparison with existing benchmarks is presented in Table  1 . Our data collection pipeline involves the following key steps in order.",
            "Similar to previous LLM-based multimodal models, we treat motion as a foreign language. The overall framework is presented in Figure  11  in Appendix  B . Our large motion model, built on a pre-trained LLM, functions as a generative model that connects a motion tokenizer with the LLM backbone    \\Theta roman_ . The motion tokenizer encodes raw motion clip features  M M \\mathcal{M} caligraphic_M  into token embeddings  V = { v 1 , v 2 , ... , v n }  R n  d V subscript v 1 subscript v 2 ... subscript v n superscript R n d \\mathcal{V}=\\{v_{1},v_{2},...,v_{n}\\}\\in\\mathbbm{R}^{n\\times d} caligraphic_V = { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_v start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }  blackboard_R start_POSTSUPERSCRIPT italic_n  italic_d end_POSTSUPERSCRIPT , where  n n n italic_n  denotes the number of motion tokens and  d d d italic_d  represents the dimensionality of each token. To integrate motion tokens into the LLM framework, we incorporate  K K K italic_K  discrete codes in the motion codebook as additional vocabulary for the LLM. Additionally, we introduce two special tokens,  < < < mot > > >  and  < < < /mot > > > , to signify the start and end of motion sequences within the input/output streams. The LLM backbone    \\Theta roman_  is built on a decoder-only architecture using causal transformers. The model generates outputs  Y = { y 1 , y 2 , ... , y m } Y subscript y 1 subscript y 2 ... subscript y m \\mathcal{Y}=\\{y_{1},y_{2},...,y_{m}\\} caligraphic_Y = { italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }  in an auto-regressive manner, where  Y Y \\mathcal{Y} caligraphic_Y  corresponds to the generated motion sequence based on the provided motion-text input tokens. In this work, each motion-text pair in the MotionBase dataset is framed as an instruction-following instance  { X Q , X M } subscript X Q subscript X M \\{\\mathcal{X}_{Q},\\mathcal{X}_{M}\\} { caligraphic_X start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT , caligraphic_X start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT } , representing a question-answer interaction between the user and the motion model. The entire instructional dataset adheres to this unified format. To train our model, we optimize the negative log-likelihood over the predicted tokens which is defined as follows:",
            "Does Static and Synthetic Data help?  Yes, the addition of static image data and synthesized data both contribute to improvements, as illustrated in Table  LABEL:tab:syn_and_static_part , more analysis can be found in Appendix  C.1 .",
            "As mentioned earlier, the FID scores in Table  2  and Table  LABEL:tab:out_of_distribute  yield unexpected results. Specifically, when evaluating on Motion-X and UNSEEN-90K, FID achieves its best performance when trained on Motion-X, significantly outperforming both the smaller HumanML3D and the larger-scale MotionBase. In this section, we aim to investigate this anomaly. FID, a standard metric widely used for generation tasks, is typically measured by a pretrained evaluator. In traditional image generation, FID is calculated using a well-trained, robust visual encoder like InceptionNet  (Szegedy et al.,  2015 ) , which is trained on millions of images. However, the evaluator currently used to compute FID for motion generation is a simple motion autoencoder with a very small parameter scale  (Guo et al.,  2022a ) . Since this motion autoencoder is trained on limited data consisting of only 20K motions, we argue that it may lack the generalization needed for robust performance, leading to difficulties in reliably capturing the complex semantic alignment between text and motion.Similar unexpected results occur in motion reconstruction as well. As show in Table  6 , the FID score on HumanML3D is two orders of magnitude higher when comparing 2D-LFQ and VQ-VAE, despite the former achieving a much lower MPJPE. When tested on MotionBase, 2D-LFQ obtains the highest FID score even while achieving the best MPJPE. We observe the same issue with other metrics like MMDist, as discussed in Appendix  C.1 . Notably,  Voas et al. ( 2023 )  have mentioned that existing metrics are sensitive to the quality of the embedding space and do not always align with human perception. These findings highlight the need for a more robust and fair evaluation metric for large motion models moving forward.",
            "To further explore the annotated motion text, we generate word clouds from the entire text corpus in MotionBase. Since the annotations in MotionBase consist of both whole-body and part-level descriptions, we create separate word clouds for general labels and more detailed annotations, as shown in Figure  9  and Figure  10 , respectively. In Figure  9 , we observe that the whole-body annotations primarily highlight high-level motion activities, such as standing, sitting, and walking. In contrast, Figure  10  shows that part-level annotations focus more on specific body movements, including the torso, shoulders, legs, and arms. We believe that this hierarchical structure of annotations will enhance the understanding of motion.",
            "Due to space limitations in the main paper, we provide the overview of our model architecture in Figure  11  in this appendix. Following most LMMs, our large motion model consists of two stages: pre-training and fine-tuning. During the pre-training stage, we train a motion encoder, a motion decoder, and a motion codebook to represent motions using discrete tokens. With this motion tokenizer, we fine-tune an autoregressive language model to predict motion tokens. In the inference stage, the input text is processed by the language model to generate motion tokens in an autoregressive manner, which are then decoded into natural motion by the pre-trained motion decoder.",
            "First, we provide additional FID results on Motion-X in Figure  12 . It is worth noting that while our motion quantizer performs worse than RQ-VAE on the smaller HumanML3D dataset, it surpasses both VQ and RQ when evaluated on the larger Motion-X and MotionBase benchmarks, as can be seen in Table  6 . This suggests that our approach offers a greater advantage when applied to larger datasets, highlighting its improved generalization compared to previous methods.",
            "To further validate the effectiveness of our 2D quantization strategy, we compare the 2D-LFQ method with its 1D counterpart (which is identical to VQ except for the quantization strategy). The results, shown in Table  11 , demonstrate that 2D quantization in LFQ significantly outperforms the 1D version. This highlights the superior ability of 2D quantization to enhance the representational capacity of the motion tokenizer.",
            "We provide some examples to visualize the human motions predicted by our large motion model trained on MotionBase, as illustrated in Figure  13 . As can be seen, our large motion model is capable of generating motion sequences that align well with the input texts, demonstrating the effectiveness of the MotionBase dataset."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparisons under different model and data sizes. All experiments are conducted using the same pretrained VQ model for consistency. Additionally, we re-train the motion autoencoder and text encoder   (Guo et al.,  2022a )  separately on the Motion-X and MotionBase datasets, using their respective data to train the motion autoencoder for each datasets evaluation.",
        "table": "S5.T2.6.6",
        "footnotes": [
            ""
        ],
        "references": [
            "Does increasing model size benefit motion generation?   Yes. As shown in Table  2 , our results demonstrate that increasing model size leads to significant performance improvements when provided with the same amount of training data. Specifically, Llama2-13b outperforms Llama2-7b, which in turn surpasses GPT2-medium, illustrating a clear trend of performance gains as model capacity increases. This suggests that models with larger size are better equipped to capture diverse, complex patterns and relationships within human motions.",
            "Does increasing data scale benefit motion generation?  Yes. In Table  2 , when using the same foundation model, increasing the scale of training data leads to substantial performance gains on the MotionBase test set, aligning with our expected scaling laws. This improvement is particularly pronounced in the R-precision metric, emphasizing the critical role of data scale in enhancing semantic alignment between generated motions and text prompts. However, contrary to our expectations, we observe a noticeable performance decline on the Motion-X test set if not trained on Motion-X (0.08M). We attribute this to the limitations of the retrieval-based evaluation model, as discussed in Section  5.4 .",
            "Do large motion models outperform in out-of-distribution setup?   Yes. We present the results in Table  LABEL:tab:out_of_distribute . This ablation is essential for further validating the generalization capabilities of large motion models, as the improvements observed in Table  2  may stem from the inclusion of additional in-domain data from Motion-X. In this setup, we select four subsets from MotionBase, comprising 90K samples (UNSEEN-90K), for evaluation, while the remaining 38 subsets are used for training. This ensures that the test set consists entirely of out-of-domain (OOD) samples. We compare the performance of models trained on HumanML3D, MotionX, and Motion-#38, all utilizing the GPT2-medium architecture, where  #  N # N \\#N # italic_N  denotes the number of training subsets. All models are trained using the GPT2-medium. The results on the OOD test set clearly demonstrate that the model trained on MotionBase significantly outperforms those trained on HumanML3D and MotionX, particularly in terms of R@1 and R@3 metrics. These findings strongly highlight the superior generalization ability of large motion models when handling unseen OOD data, especially when trained on diverse, large-scale datasets. However, we once again observe unexpected results with the FID metric, which will be discussed further in Section  5.4 .",
            "As mentioned earlier, the FID scores in Table  2  and Table  LABEL:tab:out_of_distribute  yield unexpected results. Specifically, when evaluating on Motion-X and UNSEEN-90K, FID achieves its best performance when trained on Motion-X, significantly outperforming both the smaller HumanML3D and the larger-scale MotionBase. In this section, we aim to investigate this anomaly. FID, a standard metric widely used for generation tasks, is typically measured by a pretrained evaluator. In traditional image generation, FID is calculated using a well-trained, robust visual encoder like InceptionNet  (Szegedy et al.,  2015 ) , which is trained on millions of images. However, the evaluator currently used to compute FID for motion generation is a simple motion autoencoder with a very small parameter scale  (Guo et al.,  2022a ) . Since this motion autoencoder is trained on limited data consisting of only 20K motions, we argue that it may lack the generalization needed for robust performance, leading to difficulties in reliably capturing the complex semantic alignment between text and motion.Similar unexpected results occur in motion reconstruction as well. As show in Table  6 , the FID score on HumanML3D is two orders of magnitude higher when comparing 2D-LFQ and VQ-VAE, despite the former achieving a much lower MPJPE. When tested on MotionBase, 2D-LFQ obtains the highest FID score even while achieving the best MPJPE. We observe the same issue with other metrics like MMDist, as discussed in Appendix  C.1 . Notably,  Voas et al. ( 2023 )  have mentioned that existing metrics are sensitive to the quality of the embedding space and do not always align with human perception. These findings highlight the need for a more robust and fair evaluation metric for large motion models moving forward.",
            "First, we provide additional FID results on Motion-X in Figure  12 . It is worth noting that while our motion quantizer performs worse than RQ-VAE on the smaller HumanML3D dataset, it surpasses both VQ and RQ when evaluated on the larger Motion-X and MotionBase benchmarks, as can be seen in Table  6 . This suggests that our approach offers a greater advantage when applied to larger datasets, highlighting its improved generalization compared to previous methods."
        ]
    },
    "id_table_3": {
        "caption": "Table 6:  Robustness investigation of the evaluation metrics on the motion reconstruction task.",
        "table": "S5.T3.11.9",
        "footnotes": [],
        "references": [
            "Does the large motion model perform SoTA competitively?  We evaluate our large motion model on the widely adopted HumanML3D benchmark. We compare its performance against a variety of SoTA approaches. This includes diffusion-based methods such as MLD  (Chen et al.,  2023 )  and MotionDiffuse  (Zhang et al.,  2022 ) , as well as the GPT-based T2M-GPT  (Zhang et al.,  2023a ) . We also compare against LLM fine-tuning methods like MotionGPT  (Jiang et al.,  2023 ; Zhang et al.,  2024b ) , MotionLLM  (Wu et al.,  2024 ) , and AvatarGPT  (Zhou et al.,  2024 ) . As shown in Table  3 , our model, which utilizes Llama-2-13B as the decoder and calculates the loss over the entire concatenated sequence of input text, achieves SOTA performance. Our large motion model significantly outperforms other LLM-based methods such as MotionGPT and AvatarGPT, as well as the earlier T2M-GPT. In particular, we observe substantial improvements in key metrics such as R@1, R@3, and MMDist, highlighting our models ability to generate motion sequences that are better aligned with text descriptions and of higher quality.",
            "Slow convergence of large motion models.  To evaluate the convergence speed of large motion models, we train GPT-2, Llama2-7b, and Llama3.1-8b for 300 epochs on Motion-X. The training curve of with R@1 performance is illustrated in Figure  3 . We obverse that all large motion models nearly converge by 200 epochs, with larger models converging faster. Initializing these models with pre-trained weights proves beneficial for speeding up convergence. Compared to large multimodal models like LLaVA  (Liu et al.,  2023 ) , large motion models require more epochs to capture the complex representations of motion sequences. We attribute the slow convergence of these models to the limited representation capacity of the motion tokenizer, which contains only 512 motion tokens. This suggests the need to optimize the motion tokenizer and expand its representation space. To address this, we explore 2D-LFQ quantization method as a promising alternative.",
            "We provide some examples to visualize the human motions predicted by our large motion model trained on MotionBase, as illustrated in Figure  13 . As can be seen, our large motion model is capable of generating motion sequences that align well with the input texts, demonstrating the effectiveness of the MotionBase dataset."
        ]
    },
    "id_table_4": {
        "caption": "Table 7:  Ablation of the effectiveness of synthetic data and static data.",
        "table": "S5.T4.3.3",
        "footnotes": [],
        "references": [
            "Does increasing data scale benefit motion generation?  Yes. In Table  2 , when using the same foundation model, increasing the scale of training data leads to substantial performance gains on the MotionBase test set, aligning with our expected scaling laws. This improvement is particularly pronounced in the R-precision metric, emphasizing the critical role of data scale in enhancing semantic alignment between generated motions and text prompts. However, contrary to our expectations, we observe a noticeable performance decline on the Motion-X test set if not trained on Motion-X (0.08M). We attribute this to the limitations of the retrieval-based evaluation model, as discussed in Section  5.4 .",
            "Do large motion models outperform in out-of-distribution setup?   Yes. We present the results in Table  LABEL:tab:out_of_distribute . This ablation is essential for further validating the generalization capabilities of large motion models, as the improvements observed in Table  2  may stem from the inclusion of additional in-domain data from Motion-X. In this setup, we select four subsets from MotionBase, comprising 90K samples (UNSEEN-90K), for evaluation, while the remaining 38 subsets are used for training. This ensures that the test set consists entirely of out-of-domain (OOD) samples. We compare the performance of models trained on HumanML3D, MotionX, and Motion-#38, all utilizing the GPT2-medium architecture, where  #  N # N \\#N # italic_N  denotes the number of training subsets. All models are trained using the GPT2-medium. The results on the OOD test set clearly demonstrate that the model trained on MotionBase significantly outperforms those trained on HumanML3D and MotionX, particularly in terms of R@1 and R@3 metrics. These findings strongly highlight the superior generalization ability of large motion models when handling unseen OOD data, especially when trained on diverse, large-scale datasets. However, we once again observe unexpected results with the FID metric, which will be discussed further in Section  5.4 .",
            "In this section, we investigate the impact of different motion quantization methods. We compare our proposed 2D lookup-free quantization (2D-LFQ) against two commonly used approaches: residual vector quantization (RVQ) and vector quantization (VQ), across various codebook sizes ranging from  2 8 superscript 2 8 2^{8} 2 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT  to  2 16 superscript 2 16 2^{16} 2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT . The number of parameters for RVQ/VQ and 2D-LFQ are 19.43M and 108.35M, respectively. As shown in Figure  4 , 2D-LFQ demonstrates significant improvements over both RVQ and VQ. Notably, as the codebook size increases, 2D-LFQ continues to enhance performance, while RVQ and VQ experience diminishing returns or performance degradation with larger codebooks. Our deeper analysis attributes these gains to better codebook utilization by 2D-LFQ. Figure  5  illustrates that the utilization rates for VQ and RVQ begin to decline once the codebook size exceeds  2 10 superscript 2 10 2^{10} 2 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT , which corresponds to the peak performance for these methods, whereas the utilization of 2D-LFQ continues to increase with larger codebooks. Additionally, we conduct further experiments to validate the benefits of 2D motion encoding in Appendix  C.5 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 8:  Comparison of evaluations using different encoder models.",
        "table": "S5.T5.7.3",
        "footnotes": [],
        "references": [
            "Does increasing data scale benefit motion generation?  Yes. In Table  2 , when using the same foundation model, increasing the scale of training data leads to substantial performance gains on the MotionBase test set, aligning with our expected scaling laws. This improvement is particularly pronounced in the R-precision metric, emphasizing the critical role of data scale in enhancing semantic alignment between generated motions and text prompts. However, contrary to our expectations, we observe a noticeable performance decline on the Motion-X test set if not trained on Motion-X (0.08M). We attribute this to the limitations of the retrieval-based evaluation model, as discussed in Section  5.4 .",
            "Do large motion models outperform in out-of-distribution setup?   Yes. We present the results in Table  LABEL:tab:out_of_distribute . This ablation is essential for further validating the generalization capabilities of large motion models, as the improvements observed in Table  2  may stem from the inclusion of additional in-domain data from Motion-X. In this setup, we select four subsets from MotionBase, comprising 90K samples (UNSEEN-90K), for evaluation, while the remaining 38 subsets are used for training. This ensures that the test set consists entirely of out-of-domain (OOD) samples. We compare the performance of models trained on HumanML3D, MotionX, and Motion-#38, all utilizing the GPT2-medium architecture, where  #  N # N \\#N # italic_N  denotes the number of training subsets. All models are trained using the GPT2-medium. The results on the OOD test set clearly demonstrate that the model trained on MotionBase significantly outperforms those trained on HumanML3D and MotionX, particularly in terms of R@1 and R@3 metrics. These findings strongly highlight the superior generalization ability of large motion models when handling unseen OOD data, especially when trained on diverse, large-scale datasets. However, we once again observe unexpected results with the FID metric, which will be discussed further in Section  5.4 .",
            "In this section, we investigate the impact of different motion quantization methods. We compare our proposed 2D lookup-free quantization (2D-LFQ) against two commonly used approaches: residual vector quantization (RVQ) and vector quantization (VQ), across various codebook sizes ranging from  2 8 superscript 2 8 2^{8} 2 start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT  to  2 16 superscript 2 16 2^{16} 2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT . The number of parameters for RVQ/VQ and 2D-LFQ are 19.43M and 108.35M, respectively. As shown in Figure  4 , 2D-LFQ demonstrates significant improvements over both RVQ and VQ. Notably, as the codebook size increases, 2D-LFQ continues to enhance performance, while RVQ and VQ experience diminishing returns or performance degradation with larger codebooks. Our deeper analysis attributes these gains to better codebook utilization by 2D-LFQ. Figure  5  illustrates that the utilization rates for VQ and RVQ begin to decline once the codebook size exceeds  2 10 superscript 2 10 2^{10} 2 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT , which corresponds to the peak performance for these methods, whereas the utilization of 2D-LFQ continues to increase with larger codebooks. Additionally, we conduct further experiments to validate the benefits of 2D motion encoding in Appendix  C.5 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 9:  Comparison between fine-tuning and learning from scratch on the Motion-X test set.",
        "table": "S5.T6.2.2",
        "footnotes": [],
        "references": [
            "As mentioned earlier, the FID scores in Table  2  and Table  LABEL:tab:out_of_distribute  yield unexpected results. Specifically, when evaluating on Motion-X and UNSEEN-90K, FID achieves its best performance when trained on Motion-X, significantly outperforming both the smaller HumanML3D and the larger-scale MotionBase. In this section, we aim to investigate this anomaly. FID, a standard metric widely used for generation tasks, is typically measured by a pretrained evaluator. In traditional image generation, FID is calculated using a well-trained, robust visual encoder like InceptionNet  (Szegedy et al.,  2015 ) , which is trained on millions of images. However, the evaluator currently used to compute FID for motion generation is a simple motion autoencoder with a very small parameter scale  (Guo et al.,  2022a ) . Since this motion autoencoder is trained on limited data consisting of only 20K motions, we argue that it may lack the generalization needed for robust performance, leading to difficulties in reliably capturing the complex semantic alignment between text and motion.Similar unexpected results occur in motion reconstruction as well. As show in Table  6 , the FID score on HumanML3D is two orders of magnitude higher when comparing 2D-LFQ and VQ-VAE, despite the former achieving a much lower MPJPE. When tested on MotionBase, 2D-LFQ obtains the highest FID score even while achieving the best MPJPE. We observe the same issue with other metrics like MMDist, as discussed in Appendix  C.1 . Notably,  Voas et al. ( 2023 )  have mentioned that existing metrics are sensitive to the quality of the embedding space and do not always align with human perception. These findings highlight the need for a more robust and fair evaluation metric for large motion models moving forward.",
            "MotionBase contains over 1 million motion sequences from 42 different public datasets and web videos on the Internet. Subsets of MotionX, including Animation, Perform, Dance, Aist, Kungfu, GRAB  (Taheri et al.,  2020 ) , Music, Idea400  (Lin et al.,  2024 ) , HAA500  (Chung et al.,  2021 ) , Game Motion, and Fitness, are included in MotionBase. Recognizing the high cost of collecting and annotating videos, we also see the untapped potential of images for motion understanding. Consequently, MotionBase incorporates image data by repeating each image across 64 frames and treating it as a motion sequence. For the datasets with long-range videos, such as MPI-INF-3DHP  (Mehta et al.,  2017 ) , we segment the footage into sub-clips with random durations ranging from 10 seconds to one minute. Figure  6  and Figure  7  illustrate the scale and length distributions of MotionBase.",
            "First, we provide additional FID results on Motion-X in Figure  12 . It is worth noting that while our motion quantizer performs worse than RQ-VAE on the smaller HumanML3D dataset, it surpasses both VQ and RQ when evaluated on the larger Motion-X and MotionBase benchmarks, as can be seen in Table  6 . This suggests that our approach offers a greater advantage when applied to larger datasets, highlighting its improved generalization compared to previous methods."
        ]
    },
    "id_table_7": {
        "caption": "Table 10:  Results of different loss calculation methods on the HumanML3D test set.",
        "table": "A3.T7.4.4",
        "footnotes": [],
        "references": [
            "MotionBase contains over 1 million motion sequences from 42 different public datasets and web videos on the Internet. Subsets of MotionX, including Animation, Perform, Dance, Aist, Kungfu, GRAB  (Taheri et al.,  2020 ) , Music, Idea400  (Lin et al.,  2024 ) , HAA500  (Chung et al.,  2021 ) , Game Motion, and Fitness, are included in MotionBase. Recognizing the high cost of collecting and annotating videos, we also see the untapped potential of images for motion understanding. Consequently, MotionBase incorporates image data by repeating each image across 64 frames and treating it as a motion sequence. For the datasets with long-range videos, such as MPI-INF-3DHP  (Mehta et al.,  2017 ) , we segment the footage into sub-clips with random durations ranging from 10 seconds to one minute. Figure  6  and Figure  7  illustrate the scale and length distributions of MotionBase."
        ]
    },
    "id_table_8": {
        "caption": "Table 11:  Ablation of 2D motion quantization vs. its 1D version.",
        "table": "A3.T8.6.6",
        "footnotes": [],
        "references": [
            "In this paper, we use Gemini-1.5-pro  (Reid et al.,  2024 )  and GPT-4o-mini  (OpenAI,  2024 )  as large multimodal models (LMM) to generate textual annotations for video and image data, respectively. For each person-centric sample, we first crop and track the persons body using the corresponding bounding box(es). The LMM is then tasked with focusing on the persons physical movements and positions in the global space to generate detailed descriptions. Unlike previous datasets, we provide more granular motion descriptions by dividing the body into upper and lower sections, prompting the LMM to generate part-specific descriptions (part-level). Additionally, an overall summary of the entire bodys movement (whole-body) is also produced. Figure  8  illustrates the prompt used to caption human motion sequences in MotionBase.",
            "Table  8  presents the evaluation results on the HumanML3D test set using different encoder models (EM). We employ the same dual-encoder architecture  (Guo et al.,  2022a )  but trained it on two distinct datasets: HumanML3D and Motion-X, where HumanML3D is a subset of Motion-X. The results highlight the limited generalization ability of the encoder model. When using the model trained on the larger Motion-X dataset, performance metrics on HumanML3D decrease. This suggests that training on the broader Motion-X dataset negatively impacts R-Precision performance on the HumanML3D subset. Furthermore, when the encoder model is trained on Motion-X, increasing the training data size for the text-to-motion model leads to significant performance gains. Conversely, when using the encoder model trained on HumanML3D, the performance of the text-to-motion model degrades as the training data size increases. This might be attributed to inherent limitations in the encoder model itself."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A3.T9.4.4",
        "footnotes": [],
        "references": [
            "To further explore the annotated motion text, we generate word clouds from the entire text corpus in MotionBase. Since the annotations in MotionBase consist of both whole-body and part-level descriptions, we create separate word clouds for general labels and more detailed annotations, as shown in Figure  9  and Figure  10 , respectively. In Figure  9 , we observe that the whole-body annotations primarily highlight high-level motion activities, such as standing, sitting, and walking. In contrast, Figure  10  shows that part-level annotations focus more on specific body movements, including the torso, shoulders, legs, and arms. We believe that this hierarchical structure of annotations will enhance the understanding of motion."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A3.T10.4.4",
        "footnotes": [],
        "references": [
            "To further explore the annotated motion text, we generate word clouds from the entire text corpus in MotionBase. Since the annotations in MotionBase consist of both whole-body and part-level descriptions, we create separate word clouds for general labels and more detailed annotations, as shown in Figure  9  and Figure  10 , respectively. In Figure  9 , we observe that the whole-body annotations primarily highlight high-level motion activities, such as standing, sitting, and walking. In contrast, Figure  10  shows that part-level annotations focus more on specific body movements, including the torso, shoulders, legs, and arms. We believe that this hierarchical structure of annotations will enhance the understanding of motion."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "A3.T11.2.2",
        "footnotes": [],
        "references": [
            "Similar to previous LLM-based multimodal models, we treat motion as a foreign language. The overall framework is presented in Figure  11  in Appendix  B . Our large motion model, built on a pre-trained LLM, functions as a generative model that connects a motion tokenizer with the LLM backbone    \\Theta roman_ . The motion tokenizer encodes raw motion clip features  M M \\mathcal{M} caligraphic_M  into token embeddings  V = { v 1 , v 2 , ... , v n }  R n  d V subscript v 1 subscript v 2 ... subscript v n superscript R n d \\mathcal{V}=\\{v_{1},v_{2},...,v_{n}\\}\\in\\mathbbm{R}^{n\\times d} caligraphic_V = { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_v start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }  blackboard_R start_POSTSUPERSCRIPT italic_n  italic_d end_POSTSUPERSCRIPT , where  n n n italic_n  denotes the number of motion tokens and  d d d italic_d  represents the dimensionality of each token. To integrate motion tokens into the LLM framework, we incorporate  K K K italic_K  discrete codes in the motion codebook as additional vocabulary for the LLM. Additionally, we introduce two special tokens,  < < < mot > > >  and  < < < /mot > > > , to signify the start and end of motion sequences within the input/output streams. The LLM backbone    \\Theta roman_  is built on a decoder-only architecture using causal transformers. The model generates outputs  Y = { y 1 , y 2 , ... , y m } Y subscript y 1 subscript y 2 ... subscript y m \\mathcal{Y}=\\{y_{1},y_{2},...,y_{m}\\} caligraphic_Y = { italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }  in an auto-regressive manner, where  Y Y \\mathcal{Y} caligraphic_Y  corresponds to the generated motion sequence based on the provided motion-text input tokens. In this work, each motion-text pair in the MotionBase dataset is framed as an instruction-following instance  { X Q , X M } subscript X Q subscript X M \\{\\mathcal{X}_{Q},\\mathcal{X}_{M}\\} { caligraphic_X start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT , caligraphic_X start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT } , representing a question-answer interaction between the user and the motion model. The entire instructional dataset adheres to this unified format. To train our model, we optimize the negative log-likelihood over the predicted tokens which is defined as follows:",
            "Due to space limitations in the main paper, we provide the overview of our model architecture in Figure  11  in this appendix. Following most LMMs, our large motion model consists of two stages: pre-training and fine-tuning. During the pre-training stage, we train a motion encoder, a motion decoder, and a motion codebook to represent motions using discrete tokens. With this motion tokenizer, we fine-tune an autoregressive language model to predict motion tokens. In the inference stage, the input text is processed by the language model to generate motion tokens in an autoregressive manner, which are then decoded into natural motion by the pre-trained motion decoder.",
            "To further validate the effectiveness of our 2D quantization strategy, we compare the 2D-LFQ method with its 1D counterpart (which is identical to VQ except for the quantization strategy). The results, shown in Table  11 , demonstrate that 2D quantization in LFQ significantly outperforms the 1D version. This highlights the superior ability of 2D quantization to enhance the representational capacity of the motion tokenizer."
        ]
    }
}