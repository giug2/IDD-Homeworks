{
    "id_table_1": {
        "caption": "Table 1.  Performance of semantic search for text embedding model on ORD-QA.",
        "table": "S5.T1.1.1",
        "footnotes": [],
        "references": [
            "With the exceptional performance and rapid advancement of large language models (LLMs), retrieval augmented generation (RAG) has recently been proven to be an effective method for document grounded QA.  As is shown in  Figure   1 , to prepare for RAG, the entire document is segmented into reasonably-sized chunks, each of which is then embedded into a high-dimensional space by a text embedding model and stored in a vector database.  When a user poses a question, the text embedding model encodes the question into an embedding in the same space.  Through similarity matching, document chunks that most closely match the semantics of the question are retrieved as potential reference candidates.  Then, a reranker model conducts fine filtering and removes irrelevant document chunks from the candidates.  Finally, the relevant document chunks along with the user query are fed to the generator (usually an LLM) for answer generation.",
            "Figure   2  illustrates the overall flow of  RAG-EDA , our customized retrieval augmented generation (RAG) flow for EDA tool documentation QA.  In the preparation phase, the entire EDA tool documentation is segmented into reasonably sized chunks, which are then encoded into vectors by a customized text embedding model (introduced in  Section   3.1 ), forming a vector database.  The first stage is pre-processing, where the user query is tokenized into words and simultaneously encoded into an embedding.  The second stage, retrieval (introduced in  Section   3.2 ), operates in two parts:  lexical retrieval (TF-IDF or BM25) and semantic retrieval using the pre-built vector database.  The results from both searches are combined to form relevant document candidates.  Subsequently, the finetuned reranker model (introduced in  Section   3.3 ) conducts fine filtering on the document candidates and further eliminates the weakly-related ones.  Finally, the fine-filtered relevant document chunks along with the query are fed to the generator model (introduced in  Section   3.4 ) for answer generation.  In the following sub-sections, we will detail the customization and technique for each stage.",
            "Dataset for Text Embedding Model .  To construct the EDA-domain-specific query, positive sample, negative sample triplets mentioned in  Section   3.1 ,  we first collected a comprehensive list of 261 EDA terminologies covering areas like logic synthesis, power analysis, partitioning, floorplanning, placement, routing, STA, DRC, and LVS.  Our dataset collection process involved selecting a terminology  t i subscript t i t_{i} italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT   from this list and using GPT-3.5 to generate a related user query ( s i subscript s i s_{i} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). GPT-3.5 also produced the answer for ( s i subscript s i s_{i} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), which is used as the positive sample ( s i + superscript subscript s i s_{i}^{+} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ).  Simultaneously, we created a negative sample ( s i  superscript subscript s i s_{i}^{-} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ) by replacing  t i subscript t i t_{i} italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  in  s i subscript s i s_{i} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  with a different terminology  t j subscript t j t_{j} italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT   resulting in a data triplet  { s i , s i + , s i  } subscript s i superscript subscript s i superscript subscript s i \\{s_{i},s_{i}^{+},s_{i}^{-}\\} { italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT } .  Overall, we gathered 3,975 query, positive sample, negative sample triplets to fine-tune the text embedding model.",
            "In our customized RAG flow, the text embedding model was fine-tuned on bge-large-en-v1.5 3 3 3 https://huggingface.co/BAAI/bge-large-en-v1.5  for 2 epochs, with a batch size of 16, a maximum sequence length of 128, and a temperature of 0.05.  The reranker, based on the bge-reranker-large model 4 4 4 https://huggingface.co/BAAI/bge-reranker-large  with the XLM-RoBERTa architecture  ( conneau2019unsupervised,  ) , underwent fine-tuning for 30 epochs, with batch size 4 and maximum sequence length of 512.  The generator, powered by the Qwen-14b-chat model  ( bai2023qwen,  ) , was trained in two stages:  For domain knowledge pre-train, we pretrained the model on the textbook chunks dataset described in  section   5.1  for 2 epochs with the learning rate of 1e-5 and batch size of 1.  For task-specific instruction tuning, we finetuned the model using QLoRA ( dettmers2024qlora,  )  for 4 epochs with the learning rate of 2e-4 and batch size of 1. The maximal sequence length is 4096 and the lora rank is 32.  All the models above were fine-tuned on 16xA100 with 40G memory each.",
            "In this sub-section, we evaluate the performance of dense (semantic) search by our finetuned text embedding model, on the benchmark ORD-QA.  We first use the under-test text embedding model to encode the 290 document chunks in  Section   4  into the vector database.  Then, each question  q q q italic_q  in ORD-QA is encoded by the text embedding model to obtain its vector representation  e q superscript e q e^{q} italic_e start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT , and the similarity search library Faiss  ( douze2024faiss,  )  is used to search for the top- k k k italic_k  closest document chunks with  e q superscript e q e^{q} italic_e start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT  in the vector space.  We use the metric recall@ k k k italic_k  (introduced in  Section   2.2 ) to measure the capacity of the text embedding model in retrieving relevant documents.  We adopt the text-embedding-ada-002 model by OpenAI and bge-large-en-v1.5 as baselines, and set  k k k italic_k  to be 5, 10, 15 and 20 for evaluating recall@ k k k italic_k .  Experimental results in  Table   1  show that fine-tuned by our well-designed constrastive learning scheme on the EDA corpus,  our fine-tuned text embedding model dramatically outperforms its base model (bge-large-en-v1.5) and the SOTA commercial text embedding model by OpenAI, on the task of retrieving relevant documents for EDA-tool-involved questions."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Performance of reranker for weakly-related documents filtering on ORD-QA.",
        "table": "S5.T2.1.1",
        "footnotes": [],
        "references": [
            "Figure   2  illustrates the overall flow of  RAG-EDA , our customized retrieval augmented generation (RAG) flow for EDA tool documentation QA.  In the preparation phase, the entire EDA tool documentation is segmented into reasonably sized chunks, which are then encoded into vectors by a customized text embedding model (introduced in  Section   3.1 ), forming a vector database.  The first stage is pre-processing, where the user query is tokenized into words and simultaneously encoded into an embedding.  The second stage, retrieval (introduced in  Section   3.2 ), operates in two parts:  lexical retrieval (TF-IDF or BM25) and semantic retrieval using the pre-built vector database.  The results from both searches are combined to form relevant document candidates.  Subsequently, the finetuned reranker model (introduced in  Section   3.3 ) conducts fine filtering on the document candidates and further eliminates the weakly-related ones.  Finally, the fine-filtered relevant document chunks along with the query are fed to the generator model (introduced in  Section   3.4 ) for answer generation.  In the following sub-sections, we will detail the customization and technique for each stage.",
            "To bridge the gap between existing cross-encoder reranker models and proprietary LLMs in the context of EDA tool QA documents filtering,  we initiate our approach by collecting a set of EDA-tool-related questions, denoted as  Q = { q 1 , q 2 , ... , q n } Q subscript q 1 subscript q 2 ... subscript q n Q=\\{q_{1},q_{2},...,q_{n}\\} italic_Q = { italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_q start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } .  For each  q i  Q subscript q i Q q_{i}\\in Q italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  italic_Q , we apply the hybrid retriever described in  Section   3.2  to retrieve  k k k italic_k  candidate documents  C i subscript C i C_{i} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .  We then employ GPT-4 with task-specific prompt to differentiate relevant documents ( C i + superscript subscript C i C_{i}^{+} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) and weakly-related documents ( C i  superscript subscript C i C_{i}^{-} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ), ensuring  | C i + | + | C i  | = k superscript subscript C i superscript subscript C i k |C_{i}^{+}|+|C_{i}^{-}|=k | italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT | + | italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT | = italic_k .  Next, to enhance the ability of the reranker model in distinguishing between relevant documents and weakly-related documents, even when they share similar semantics or identical keywords with the query,  we introduce a contrastive learning scheme to fine-tune the reranker model using the documents filtered by GPT-4.  Specifically, for a question  q i  Q subscript q i Q q_{i}\\in Q italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  italic_Q , we sample one positive document  s i + superscript subscript s i s_{i}^{+} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT  from  C i + superscript subscript C i C_{i}^{+} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT  and  m m m italic_m  negative documents  { s i , 1  , s i , 2  , ... , s i , m  } superscript subscript s i 1 superscript subscript s i 2 ... superscript subscript s i m \\{s_{i,1}^{-},s_{i,2}^{-},...,s_{i,m}^{-}\\} { italic_s start_POSTSUBSCRIPT italic_i , 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT , italic_s start_POSTSUBSCRIPT italic_i , 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT , ... , italic_s start_POSTSUBSCRIPT italic_i , italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT }  from  C i  superscript subscript C i C_{i}^{-} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT  ( m  k m k m\\leq k italic_m  italic_k ), the contrastive loss for  q i subscript q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  can be then written as:",
            "Dataset for Reranker Model .  To construct the dataset of reranker finetuning, we first randomly sample triplets from the instruction-tuning dataset.  For each question  q i subscript q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , we treat their corresponding relevant document chunks  r i subscript r i r_{i} italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  as the positive documents and denote them by  C i + superscript subscript C i C_{i}^{+} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT .  Then, we apply hybrid retrieval described in  Section   3.2  on  q i subscript q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , and set the top- k k k italic_k  values for both the lexical and semantic searchers to be 10, in result, 20 document chunks are retrieved and denoted as  C i  superscript subscript C i  C_{i}^{\\prime} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT .  Among the chunks in  C i  superscript subscript C i  C_{i}^{\\prime} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , we prompt GPT-4 to detect the ones which are weakly-related/irrelevant with  q i subscript q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , and denote them by  C i  superscript subscript C i C_{i}^{-} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT .  Finally, we get 208  { q i , C i + , C i  } subscript q i superscript subscript C i superscript subscript C i \\{q_{i},C_{i}^{+},C_{i}^{-}\\} { italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT }  triplets as the dataset for reranker finetuning.",
            "In this sub-section, we evaluate the performance of dense (semantic) search by our finetuned text embedding model, on the benchmark ORD-QA.  We first use the under-test text embedding model to encode the 290 document chunks in  Section   4  into the vector database.  Then, each question  q q q italic_q  in ORD-QA is encoded by the text embedding model to obtain its vector representation  e q superscript e q e^{q} italic_e start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT , and the similarity search library Faiss  ( douze2024faiss,  )  is used to search for the top- k k k italic_k  closest document chunks with  e q superscript e q e^{q} italic_e start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT  in the vector space.  We use the metric recall@ k k k italic_k  (introduced in  Section   2.2 ) to measure the capacity of the text embedding model in retrieving relevant documents.  We adopt the text-embedding-ada-002 model by OpenAI and bge-large-en-v1.5 as baselines, and set  k k k italic_k  to be 5, 10, 15 and 20 for evaluating recall@ k k k italic_k .  Experimental results in  Table   1  show that fine-tuned by our well-designed constrastive learning scheme on the EDA corpus,  our fine-tuned text embedding model dramatically outperforms its base model (bge-large-en-v1.5) and the SOTA commercial text embedding model by OpenAI, on the task of retrieving relevant documents for EDA-tool-involved questions.",
            "To evaluate the performance of our fine-tuned reranker model on filtering out weakly-related and irrelevant documents, for each question  q q q italic_q  in ORD-QA,  hybrid search (described in  Section   3.2 ) is first conducted.  We use Faiss and BM25 for the lexical and semantic searches, respectively, and set the search limit of each retriever to be 20.  After hybrid document search, we obtain 40 candidate relevant document chunks for  q q q italic_q .  Then, we de-duplicate the candidates.  Next, the reranker model calculates the similarity score between  q q q italic_q  and each candidate document chunk, and the top  k k k italic_k  document chunks with the highest scores are selected as the relevant document chunks.  Subsequently, we can calculate recall@ k k k italic_k  to measure the fine-filtering ability of the reranker.  The SOTA open-source reranker model bge-reranker-large, based on which our reranker is fine-tuned, is selected as one baseline.  We also compare our reranker model with RRF (introduced in  Section   2 ) since it is used in RAG-fusion, one commonly-used RAG flow.   Table   2  lists the experimental results on ORD-QA: by setting  k k k italic_k  from 1 to 5, our fine-tuned reranker model outperforms the baselines in filtering out weakly-related documents and preserve relevant documents, for EDA-tool-related questions and documents."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Performance of the LLMs as generators on ORD-QA.",
        "table": "S5.T3.1.1",
        "footnotes": [],
        "references": [
            "Figure   2  illustrates the overall flow of  RAG-EDA , our customized retrieval augmented generation (RAG) flow for EDA tool documentation QA.  In the preparation phase, the entire EDA tool documentation is segmented into reasonably sized chunks, which are then encoded into vectors by a customized text embedding model (introduced in  Section   3.1 ), forming a vector database.  The first stage is pre-processing, where the user query is tokenized into words and simultaneously encoded into an embedding.  The second stage, retrieval (introduced in  Section   3.2 ), operates in two parts:  lexical retrieval (TF-IDF or BM25) and semantic retrieval using the pre-built vector database.  The results from both searches are combined to form relevant document candidates.  Subsequently, the finetuned reranker model (introduced in  Section   3.3 ) conducts fine filtering on the document candidates and further eliminates the weakly-related ones.  Finally, the fine-filtered relevant document chunks along with the query are fed to the generator model (introduced in  Section   3.4 ) for answer generation.  In the following sub-sections, we will detail the customization and technique for each stage.",
            "During semantic retrieval, the text embedding model plays a crucial role in transforming both the user query and document chunks into numerical vectors that encapsulate their semantic content.  The accuracy of this semantic representation is essential as it directly impacts the retrieval performance.  Although many proprietary and open-source text embedding models  ( xiao2023c,  ;  chen2024bge,  )  perform well in general information retrieval tasks, they demonstrate poor semantic understanding of the terminologies and concepts in EDA.  As is illustrated in  Figure   3 , the SOTA general-task text embedding model fails to adequately distinguish between basic EDA terms such as Primary Input (PI) and Primary Output (PO).  This deficiency leads to inaccuracies in the semantic vector space it constructs, potentially causing the model to erroneously select less relevant documents. For instance, despite Doc1 being more relevant to the query, Doc2 may be selected as the relevant document candidate due to its perceived semantic closeness, thereby compromising the overall effectiveness of the QA flow.",
            "Motivated by the preceding analysis, our approach deviates from the conventional methodology of solely employing semantic search for information retrieval  ( lewis2020retrieval,  ) .  In our proposed framework, we implement a hybrid search strategy, integrating both semantic and lexical search techniques for the retrieval of documents.  Document chunks retrieved via the hybrid strategy are merged and deduplicated to form a pool of relevant document candidates.  Subsequently, candidate documents that are weakly related to the query will be further differentiated and excluded, as detailed in  Section   3.3 .",
            "To bridge the gap between existing cross-encoder reranker models and proprietary LLMs in the context of EDA tool QA documents filtering,  we initiate our approach by collecting a set of EDA-tool-related questions, denoted as  Q = { q 1 , q 2 , ... , q n } Q subscript q 1 subscript q 2 ... subscript q n Q=\\{q_{1},q_{2},...,q_{n}\\} italic_Q = { italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_q start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } .  For each  q i  Q subscript q i Q q_{i}\\in Q italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  italic_Q , we apply the hybrid retriever described in  Section   3.2  to retrieve  k k k italic_k  candidate documents  C i subscript C i C_{i} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .  We then employ GPT-4 with task-specific prompt to differentiate relevant documents ( C i + superscript subscript C i C_{i}^{+} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) and weakly-related documents ( C i  superscript subscript C i C_{i}^{-} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ), ensuring  | C i + | + | C i  | = k superscript subscript C i superscript subscript C i k |C_{i}^{+}|+|C_{i}^{-}|=k | italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT | + | italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT | = italic_k .  Next, to enhance the ability of the reranker model in distinguishing between relevant documents and weakly-related documents, even when they share similar semantics or identical keywords with the query,  we introduce a contrastive learning scheme to fine-tune the reranker model using the documents filtered by GPT-4.  Specifically, for a question  q i  Q subscript q i Q q_{i}\\in Q italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  italic_Q , we sample one positive document  s i + superscript subscript s i s_{i}^{+} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT  from  C i + superscript subscript C i C_{i}^{+} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT  and  m m m italic_m  negative documents  { s i , 1  , s i , 2  , ... , s i , m  } superscript subscript s i 1 superscript subscript s i 2 ... superscript subscript s i m \\{s_{i,1}^{-},s_{i,2}^{-},...,s_{i,m}^{-}\\} { italic_s start_POSTSUBSCRIPT italic_i , 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT , italic_s start_POSTSUBSCRIPT italic_i , 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT , ... , italic_s start_POSTSUBSCRIPT italic_i , italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT }  from  C i  superscript subscript C i C_{i}^{-} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT  ( m  k m k m\\leq k italic_m  italic_k ), the contrastive loss for  q i subscript q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  can be then written as:",
            "Dataset for Text Embedding Model .  To construct the EDA-domain-specific query, positive sample, negative sample triplets mentioned in  Section   3.1 ,  we first collected a comprehensive list of 261 EDA terminologies covering areas like logic synthesis, power analysis, partitioning, floorplanning, placement, routing, STA, DRC, and LVS.  Our dataset collection process involved selecting a terminology  t i subscript t i t_{i} italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT   from this list and using GPT-3.5 to generate a related user query ( s i subscript s i s_{i} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). GPT-3.5 also produced the answer for ( s i subscript s i s_{i} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), which is used as the positive sample ( s i + superscript subscript s i s_{i}^{+} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ).  Simultaneously, we created a negative sample ( s i  superscript subscript s i s_{i}^{-} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ) by replacing  t i subscript t i t_{i} italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  in  s i subscript s i s_{i} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  with a different terminology  t j subscript t j t_{j} italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT   resulting in a data triplet  { s i , s i + , s i  } subscript s i superscript subscript s i superscript subscript s i \\{s_{i},s_{i}^{+},s_{i}^{-}\\} { italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT } .  Overall, we gathered 3,975 query, positive sample, negative sample triplets to fine-tune the text embedding model.",
            "Dataset For Generator .  There are two training phases for generator: domain knowledge pre-train and task-specific instruction tuning.  For the construction of pre-train dataset,  we split the two textbooks  ( lavagno2016electronic,  ;  alpert2008handbook,  )  mentioned in  Section   3.4  into 4863 chunks,  where the token size of each chunk is limited within 1024.  The procedure of dataset collecting for task-specific instruction tuning is as follows:  Each time we sample 10 document chunks of OpenROAD documentation as the document pool. Then, GPT-4 is prompted to select  n n n italic_n  ( n  10 n 10 n\\leq 10 italic_n  10 ) document chunks from the pool such that the selected chunks are logically related.  Based on the  n n n italic_n  selected document chunks ( r r r italic_r ), we prompt GPT-4 to generate one question ( q q q italic_q ) from the perspective of the OpenROAD users, analyze the question step by step, and give the answer ( a a a italic_a ) referring to  r r r italic_r .  Following the procedure above, we generate 1732  { q , r , a } q r a \\{q,r,a\\} { italic_q , italic_r , italic_a }  triplets as our dataset for instruction tuning.",
            "Dataset for Reranker Model .  To construct the dataset of reranker finetuning, we first randomly sample triplets from the instruction-tuning dataset.  For each question  q i subscript q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , we treat their corresponding relevant document chunks  r i subscript r i r_{i} italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  as the positive documents and denote them by  C i + superscript subscript C i C_{i}^{+} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT .  Then, we apply hybrid retrieval described in  Section   3.2  on  q i subscript q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , and set the top- k k k italic_k  values for both the lexical and semantic searchers to be 10, in result, 20 document chunks are retrieved and denoted as  C i  superscript subscript C i  C_{i}^{\\prime} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT .  Among the chunks in  C i  superscript subscript C i  C_{i}^{\\prime} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , we prompt GPT-4 to detect the ones which are weakly-related/irrelevant with  q i subscript q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , and denote them by  C i  superscript subscript C i C_{i}^{-} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT .  Finally, we get 208  { q i , C i + , C i  } subscript q i superscript subscript C i superscript subscript C i \\{q_{i},C_{i}^{+},C_{i}^{-}\\} { italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT }  triplets as the dataset for reranker finetuning.",
            "To evaluate the performance of our fine-tuned reranker model on filtering out weakly-related and irrelevant documents, for each question  q q q italic_q  in ORD-QA,  hybrid search (described in  Section   3.2 ) is first conducted.  We use Faiss and BM25 for the lexical and semantic searches, respectively, and set the search limit of each retriever to be 20.  After hybrid document search, we obtain 40 candidate relevant document chunks for  q q q italic_q .  Then, we de-duplicate the candidates.  Next, the reranker model calculates the similarity score between  q q q italic_q  and each candidate document chunk, and the top  k k k italic_k  document chunks with the highest scores are selected as the relevant document chunks.  Subsequently, we can calculate recall@ k k k italic_k  to measure the fine-filtering ability of the reranker.  The SOTA open-source reranker model bge-reranker-large, based on which our reranker is fine-tuned, is selected as one baseline.  We also compare our reranker model with RRF (introduced in  Section   2 ) since it is used in RAG-fusion, one commonly-used RAG flow.   Table   2  lists the experimental results on ORD-QA: by setting  k k k italic_k  from 1 to 5, our fine-tuned reranker model outperforms the baselines in filtering out weakly-related documents and preserve relevant documents, for EDA-tool-related questions and documents.",
            "To evaluate the performance of our pre-trained and fine-tuned generator on the task of answering EDA-tool-involved questions, for each question  q q q italic_q  in ORD-QA, we combine  q q q italic_q  and its golden relevant documents  r r r italic_r  into a pre-designed prompt ( Figure   7  shows the template of the prompt). The prompt is then fed to the generator for answer generation.  To quantitatively measure the performance of the generator, for each question, the BLEU, ROUGE-L and UniEval scores between the generated answer and the ground-truth answer are calculated.  We select OpenAI GPT-4 and several SOTA open-source chat LLMs as our baselines, and experimental results in  Table   3  show that being trained by our proposed two-stage EDA-specific training scheme described in  Section   3.4 , our generator is equipped with domain knowledge in EDA and possesses the ability of answering questions related to EDA tool documentation, and outperforms both commercial and academic chat LLMs on the ORD-QA benchmark."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  Performance of the RAG flows on ORD-QA.",
        "table": "S5.T4.1.1",
        "footnotes": [],
        "references": [
            "Figure   2  illustrates the overall flow of  RAG-EDA , our customized retrieval augmented generation (RAG) flow for EDA tool documentation QA.  In the preparation phase, the entire EDA tool documentation is segmented into reasonably sized chunks, which are then encoded into vectors by a customized text embedding model (introduced in  Section   3.1 ), forming a vector database.  The first stage is pre-processing, where the user query is tokenized into words and simultaneously encoded into an embedding.  The second stage, retrieval (introduced in  Section   3.2 ), operates in two parts:  lexical retrieval (TF-IDF or BM25) and semantic retrieval using the pre-built vector database.  The results from both searches are combined to form relevant document candidates.  Subsequently, the finetuned reranker model (introduced in  Section   3.3 ) conducts fine filtering on the document candidates and further eliminates the weakly-related ones.  Finally, the fine-filtered relevant document chunks along with the query are fed to the generator model (introduced in  Section   3.4 ) for answer generation.  In the following sub-sections, we will detail the customization and technique for each stage.",
            "To address the aforementioned issue and enhance the performance of the retriever, we employ supervised contrastive learning to improve the models ability to accurately perceive EDA terminologies and concepts.  The fundamental principle of our contrastive learning approach is as follows:  If two sentences involve the same EDA terminology, they should be positioned closely in the embedded vector space, despite differences in sentence structures.  Conversely, if two sentences have similar structures but pertain to different EDA terminologies or concepts, they should be distanced apart in the embedding space.  Specifically, for one EDA-domain user query  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , we denote its positive and hard negative samples by  x i + superscript subscript x i x_{i}^{+} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT  and  x i  superscript subscript x i x_{i}^{-} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT , respectively.  The positive sample  x i + superscript subscript x i x_{i}^{+} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT  involves the same EDA term with the query  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , meanwhile the negative sample  x i  superscript subscript x i x_{i}^{-} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT  has a similar sentence structure with the query, but pertains to different EDA terminology/concept.   Figure   4  shows one example of the triplet  { x i , x i + , x i  } subscript x i superscript subscript x i superscript subscript x i \\{x_{i},x_{i}^{+},x_{i}^{-}\\} { italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT } :  the query  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and the positive sample  x i + superscript subscript x i x_{i}^{+} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT  are user questions about clock period, while the hard negative sample  x i  superscript subscript x i x_{i}^{-} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT  share the same sentence structure with  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  but contains a different EDA terminology (clock skew).  We construct a triplet dataset  { x i , x i + , x i  } i = 1 N superscript subscript subscript x i superscript subscript x i superscript subscript x i i 1 N \\{x_{i},x_{i}^{+},x_{i}^{-}\\}_{i=1}^{N} { italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT  of size  N N N italic_N .  Then, we apply mini-batch gradient descent and supervised contrastive learning  ( gao2021simcse,  ;  zhang2023contrastive,  )  to finetune the embedding model.  Assume the mini-batch size is  M M M italic_M ,  and for each query  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  in the mini-batch, we apply in-batch sampling. The contrastive loss of  s i subscript s i s_{i} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  can be written as:",
            "Dataset For Generator .  There are two training phases for generator: domain knowledge pre-train and task-specific instruction tuning.  For the construction of pre-train dataset,  we split the two textbooks  ( lavagno2016electronic,  ;  alpert2008handbook,  )  mentioned in  Section   3.4  into 4863 chunks,  where the token size of each chunk is limited within 1024.  The procedure of dataset collecting for task-specific instruction tuning is as follows:  Each time we sample 10 document chunks of OpenROAD documentation as the document pool. Then, GPT-4 is prompted to select  n n n italic_n  ( n  10 n 10 n\\leq 10 italic_n  10 ) document chunks from the pool such that the selected chunks are logically related.  Based on the  n n n italic_n  selected document chunks ( r r r italic_r ), we prompt GPT-4 to generate one question ( q q q italic_q ) from the perspective of the OpenROAD users, analyze the question step by step, and give the answer ( a a a italic_a ) referring to  r r r italic_r .  Following the procedure above, we generate 1732  { q , r , a } q r a \\{q,r,a\\} { italic_q , italic_r , italic_a }  triplets as our dataset for instruction tuning.",
            "In this sub-section, we evaluate the performance of dense (semantic) search by our finetuned text embedding model, on the benchmark ORD-QA.  We first use the under-test text embedding model to encode the 290 document chunks in  Section   4  into the vector database.  Then, each question  q q q italic_q  in ORD-QA is encoded by the text embedding model to obtain its vector representation  e q superscript e q e^{q} italic_e start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT , and the similarity search library Faiss  ( douze2024faiss,  )  is used to search for the top- k k k italic_k  closest document chunks with  e q superscript e q e^{q} italic_e start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT  in the vector space.  We use the metric recall@ k k k italic_k  (introduced in  Section   2.2 ) to measure the capacity of the text embedding model in retrieving relevant documents.  We adopt the text-embedding-ada-002 model by OpenAI and bge-large-en-v1.5 as baselines, and set  k k k italic_k  to be 5, 10, 15 and 20 for evaluating recall@ k k k italic_k .  Experimental results in  Table   1  show that fine-tuned by our well-designed constrastive learning scheme on the EDA corpus,  our fine-tuned text embedding model dramatically outperforms its base model (bge-large-en-v1.5) and the SOTA commercial text embedding model by OpenAI, on the task of retrieving relevant documents for EDA-tool-involved questions.",
            "To evaluate the performance of our pre-trained and fine-tuned generator on the task of answering EDA-tool-involved questions, for each question  q q q italic_q  in ORD-QA, we combine  q q q italic_q  and its golden relevant documents  r r r italic_r  into a pre-designed prompt ( Figure   7  shows the template of the prompt). The prompt is then fed to the generator for answer generation.  To quantitatively measure the performance of the generator, for each question, the BLEU, ROUGE-L and UniEval scores between the generated answer and the ground-truth answer are calculated.  We select OpenAI GPT-4 and several SOTA open-source chat LLMs as our baselines, and experimental results in  Table   3  show that being trained by our proposed two-stage EDA-specific training scheme described in  Section   3.4 , our generator is equipped with domain knowledge in EDA and possesses the ability of answering questions related to EDA tool documentation, and outperforms both commercial and academic chat LLMs on the ORD-QA benchmark.",
            "For the implementation of the baselines, we use Faiss and OpenAI text-embedding-ada-002 model for semantic retrieval, BM25 for lexical retrieval, and GPT-4 as the generator.  To ensure fair comparison, the top- k k k italic_k  values of semantic retrieval for vanilla-RAG, HyDE, Llmlingua and ITER-RETGEN are set to 5. For RAG-fusion, the top- k k k italic_k  values of semantic and lexical retrievers are set to 20, and RRF returns the top- 5 5 5 5  highest-score documents as relevant.  The number of retrieve-generation iterations of ITER-RETGEN is set to 2.  For ablation study, we also replace the generator of RAG-EDA by GPT-4 as one baseline.   Table   4  shows the experimental results on ORD-QA:  The questions in ORD-QA can be divided into four categories as mentioned in  Section   4 , and since the numbers of GUI questions and install & test questions are small, we combine them together as a new category.  Consequently, columns 2 to 4 in  Table   4  list results for the three categories of questions, and the last column shows the average results on the whole benchmark.  Note that the metrics of BLEU, ROUGE-L and UniEval are calculated between the ground-truth answers (provided in ORD-QA) and the generated answers by the RAG flow.  It is observed that compared with the question categories of functionality and vlsi-flow,  the GUI & install & test questions are easier to solve in nature, on which all the RAG flows achieve better performance.  Our proposed RAG flow (RAG-EDA) achieves the best performance on all categories of questions, and the baseline RAG-EDA+GPT-4 (replacing the generator with GPT-4 in our proposed flow) ranks second,  demonstrating effectiveness of our proposed RAG flow and fine-tuned generator in solving documentation QA for EDA tools.  The performances of RAG-fusion, HyDE and ITER-RETGEN are similar and worse than our customized flow,  indicating the necessity of RAG flow customization for EDA tool documentation QA.  The result of Llmlingua is the worst, the reason is that the compression LLM of Llmlingua demonstrates inferior performance on handling EDA-tool related questions and documents. After the process of prompt compression, the key information in both the question and the documents are missing, leading to bad quality for answering."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.  Performance of the RAG flows on QA dataset of the commercial EDA tool.",
        "table": "S5.T5.1.1",
        "footnotes": [],
        "references": [
            "The text embedding model, finetuned through our supervised contrastive learning approach, exhibits improved semantic comprehension of the EDA-related corpus.  This enhancement leads to superior document retrieval performance in the EDA-tool documentation QA task.  Detailed results and analysis will be presented in  Section   5 .",
            "Candidate document chunks obtained by the hybrid retriever  either share semantic similarities or contain the same keywords/phrases as the query.  Based on the relevance to the query, each candidate document chunk can be classified into one of two categories: (1) Relevant document whose content can be directly used to answer the query;  (2) Weakly-related document which may contain the same EDA terminology/concept as the query, but can not be used to answer the query.  Previous research  ( cuconasu2024power,  )  indicates that weakly-related documents may cause augmented LLMs to generate error-prone responses.   Figure   5  demonstrates the impact of a weakly-related document chunk on answer generation for EDA tool queries.  The user inquires the command to place a specific pin. If the relevant document of the command  place_pin  is referred to, the generator gets correct answer. Meanwhile, referring to the weakly-related document, that is, the command to place all pins, the generator leads to wrong answer.",
            "Re-ranking is one common technique to filter out weakly-related and irrelevant documents, and is implemented by many frameworks such as LlamaIndex, LangChain and HayStack.  SOTA rerankers in the RAG framework typically employ a cross-encoder architecture, which processes the query and each candidate document as a pair. It calculates a relevance score for each pair using a cross-attention mechanism. The top  k k k italic_k  documents with the highest relevance scores are selected as the relevant documents.  Although SOTA cross-encoder reranker models such as bge-reranker 1 1 1 https://huggingface.co/BAAI/bge-reranker-large  demonstrate superior performance in general tasks of information retrieval,  they are less effective in distinguishing between relevant and weakly-related documents in the context of EDA tool documentation QA.  As is shown in  Figure   5 , for a query requesting a command to place a specific pin, the reranker incorrectly identifies the command  p  l  a  c  e  _  p  i  n  s p l a c e _ p i n s place\\_pins italic_p italic_l italic_a italic_c italic_e _ italic_p italic_i italic_n italic_s  as relevant.  We observe that with well-designed prompt, the proprietary LLMs such as GPT-4 perform well in filtering out weakly-related documents for EDA-tool-involved QA .  However, the autoregressive nature of GPT-4 makes it slow for document filtering, and proprietary LLMs are also costly and challenging to deploy.",
            "After being fine-tuned on the high-quality dataset, our reranker model exhibits high retrieval recall in EDA-tool QA scenarios. Detailed experiments and their results are discussed in  Section   5 .",
            "In our customized RAG flow, the text embedding model was fine-tuned on bge-large-en-v1.5 3 3 3 https://huggingface.co/BAAI/bge-large-en-v1.5  for 2 epochs, with a batch size of 16, a maximum sequence length of 128, and a temperature of 0.05.  The reranker, based on the bge-reranker-large model 4 4 4 https://huggingface.co/BAAI/bge-reranker-large  with the XLM-RoBERTa architecture  ( conneau2019unsupervised,  ) , underwent fine-tuning for 30 epochs, with batch size 4 and maximum sequence length of 512.  The generator, powered by the Qwen-14b-chat model  ( bai2023qwen,  ) , was trained in two stages:  For domain knowledge pre-train, we pretrained the model on the textbook chunks dataset described in  section   5.1  for 2 epochs with the learning rate of 1e-5 and batch size of 1.  For task-specific instruction tuning, we finetuned the model using QLoRA ( dettmers2024qlora,  )  for 4 epochs with the learning rate of 2e-4 and batch size of 1. The maximal sequence length is 4096 and the lora rank is 32.  All the models above were fine-tuned on 16xA100 with 40G memory each.",
            "To further verify the transferability and universality of our proposed RAG flow for different EDA tools, we collect a documentation QA evaluation dataset for a commercial EDA tool (the tool is a platform for timing ECO). The dataset follows the same standard as ORD-QA and contains 60 question-documents-answer triplets.  Experimental results in  Table   5  shows the outperformance of our proposed RAG flow compared with the baselines, thus certifies that our proposed RAG flow can be transferred to the task of documentation QA for different EDA tools."
        ]
    },
    "global_footnotes": [
        "https://huggingface.co/BAAI/bge-reranker-large",
        "https://openroad.readthedocs.io/en/latest/",
        "https://huggingface.co/BAAI/bge-large-en-v1.5",
        "https://huggingface.co/BAAI/bge-reranker-large"
    ]
}