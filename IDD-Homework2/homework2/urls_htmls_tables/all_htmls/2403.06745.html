<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation</title>
<!--Generated on Mon Mar 11 14:07:23 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.06745v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S1" title="1. Introduction â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S2" title="2. Related Work â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S3" title="3. Problem â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Problem</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S4" title="4. Methods â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S4.SS1" title="4.1. Task-enhanced Constrained Turning â€£ 4. Methods â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Task-enhanced Constrained Turning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S4.SS2" title="4.2. Auto-constriction Turning â€£ 4. Methods â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Auto-constriction Turning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5" title="5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.SS1" title="5.1. Dataset â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.SS2" title="5.2. Evaluation â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.SS3" title="5.3. Implementation Details â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.SS4" title="5.4. Baselines â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.SS5" title="5.5. Experimental Results â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Experimental Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.SS6" title="5.6. Ablation Study â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Ablation Study</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.SS6.SSS1" title="5.6.1. Ablation Experiment on Constrained Template Component â€£ 5.6. Ablation Study â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.1 </span>Ablation Experiment on Constrained Template Component</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.SS6.SSS2" title="5.6.2. Effect of Number of Triggers â€£ 5.6. Ablation Study â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.2 </span>Effect of Number of Triggers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.SS6.SSS3" title="5.6.3. Effect of Model Size â€£ 5.6. Ablation Study â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.3 </span>Effect of Model Size</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.SS6.SSS4" title="5.6.4. Effect of Data Size â€£ 5.6. Ablation Study â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.4 </span>Effect of Data Size</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.SS6.SSS5" title="5.6.5. Over/Under-generation â€£ 5.6. Ablation Study â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6.5 </span>Over/Under-generation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S6" title="6. Conclusion â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S7" title="7. Bibliographical References â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Bibliographical References</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2403.06745v1 [cs.CL] 11 Mar 2024</div></div>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation</h1>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Large language model (LLM) has achieved promising performance in multilingual machine translation tasks through zero/few-shot prompts or prompt-tuning. However, due to the mixture of multilingual data during the pre-training of LLM, the LLM-based translation models face the off-target issue in both prompt-based methods, including a series of phenomena, namely instruction misunderstanding, translation with wrong language and over-generation. For this issue, this paper introduces an <span class="ltx_text ltx_font_bold ltx_framed_underline" id="id1.id1.1">A</span>uto-<span class="ltx_text ltx_font_bold ltx_framed_underline" id="id1.id1.2">C</span>onstriction <span class="ltx_text ltx_font_bold ltx_framed_underline" id="id1.id1.3">T</span>urning mechanism for <span class="ltx_text ltx_font_bold ltx_framed_underline" id="id1.id1.4">M</span>ultilingual <span class="ltx_text ltx_font_bold ltx_framed_underline" id="id1.id1.5">N</span>eural <span class="ltx_text ltx_font_bold ltx_framed_underline" id="id1.id1.6">M</span>achine <span class="ltx_text ltx_font_bold ltx_framed_underline" id="id1.id1.7">T</span>ranslation (ACT-MNMT), which is a novel supervised fine-tuning mechanism and orthogonal to the traditional prompt-based methods.
In this method, ACT-MNMT automatically constructs a constrained template in the target side by adding trigger tokens ahead of the ground truth.
Furthermore, trigger tokens can be arranged and combined freely to represent different task semantics, and they can be iteratively updated to maximize the label likelihood. Experiments are performed on WMT test sets with multiple metrics, and the experimental results demonstrate that ACT-MNMT achieves substantially improved performance across multiple translation directions and reduce the off-target phenomena in the translation. 
<br class="ltx_break"/>
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="id1.id1.8">Keywords:â€‰</span>large language model, multilingual neural machine translation, off-target, auto-constriction tuning</p>
</div>
<span class="ltx_ERROR undefined" id="id1">\NAT@set@cites</span>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text" id="p1.1.1"></span></p>
</div>
<div class="ltx_para ltx_align_center" id="p2">
<p class="ltx_p" id="p2.1"><span class="ltx_text ltx_font_bold" id="p2.1.1" style="font-size:144%;">ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation</span></p>
</div>
<div class="ltx_para ltx_align_center" id="p3">
<table class="ltx_tabular ltx_align_top" id="p3.7">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="p3.4.4">
<td class="ltx_td ltx_align_center" id="p3.4.4.4"><span class="ltx_text ltx_font_bold" id="p3.4.4.4.4" style="font-size:120%;">Shaojie Dai<math alttext="{}^{1,2,3}" class="ltx_Math" display="inline" id="p3.1.1.1.1.m1.3"><semantics id="p3.1.1.1.1.m1.3a"><msup id="p3.1.1.1.1.m1.3.3" xref="p3.1.1.1.1.m1.3.3.cmml"><mi id="p3.1.1.1.1.m1.3.3a" xref="p3.1.1.1.1.m1.3.3.cmml"></mi><mrow id="p3.1.1.1.1.m1.3.3.3.5" xref="p3.1.1.1.1.m1.3.3.3.4.cmml"><mn id="p3.1.1.1.1.m1.1.1.1.1" mathvariant="normal" xref="p3.1.1.1.1.m1.1.1.1.1.cmml">1</mn><mo id="p3.1.1.1.1.m1.3.3.3.5.1" mathvariant="normal" xref="p3.1.1.1.1.m1.3.3.3.4.cmml">,</mo><mn id="p3.1.1.1.1.m1.2.2.2.2" mathvariant="normal" xref="p3.1.1.1.1.m1.2.2.2.2.cmml">2</mn><mo id="p3.1.1.1.1.m1.3.3.3.5.2" mathvariant="normal" xref="p3.1.1.1.1.m1.3.3.3.4.cmml">,</mo><mn id="p3.1.1.1.1.m1.3.3.3.3" mathvariant="normal" xref="p3.1.1.1.1.m1.3.3.3.3.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="p3.1.1.1.1.m1.3b"><apply id="p3.1.1.1.1.m1.3.3.cmml" xref="p3.1.1.1.1.m1.3.3"><list id="p3.1.1.1.1.m1.3.3.3.4.cmml" xref="p3.1.1.1.1.m1.3.3.3.5"><cn id="p3.1.1.1.1.m1.1.1.1.1.cmml" type="integer" xref="p3.1.1.1.1.m1.1.1.1.1">1</cn><cn id="p3.1.1.1.1.m1.2.2.2.2.cmml" type="integer" xref="p3.1.1.1.1.m1.2.2.2.2">2</cn><cn id="p3.1.1.1.1.m1.3.3.3.3.cmml" type="integer" xref="p3.1.1.1.1.m1.3.3.3.3">3</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.1.1.1.1.m1.3c">{}^{1,2,3}</annotation><annotation encoding="application/x-llamapun" id="p3.1.1.1.1.m1.3d">start_FLOATSUPERSCRIPT 1 , 2 , 3 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Xin Liu<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p3.2.2.2.2.m2.1"><semantics id="p3.2.2.2.2.m2.1a"><msup id="p3.2.2.2.2.m2.1.1" xref="p3.2.2.2.2.m2.1.1.cmml"><mi id="p3.2.2.2.2.m2.1.1a" xref="p3.2.2.2.2.m2.1.1.cmml"></mi><mn id="p3.2.2.2.2.m2.1.1.1" mathvariant="normal" xref="p3.2.2.2.2.m2.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p3.2.2.2.2.m2.1b"><apply id="p3.2.2.2.2.m2.1.1.cmml" xref="p3.2.2.2.2.m2.1.1"><cn id="p3.2.2.2.2.m2.1.1.1.cmml" type="integer" xref="p3.2.2.2.2.m2.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.2.2.2.2.m2.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p3.2.2.2.2.m2.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Ping Luo<math alttext="{}^{1,2,3}" class="ltx_Math" display="inline" id="p3.3.3.3.3.m3.3"><semantics id="p3.3.3.3.3.m3.3a"><msup id="p3.3.3.3.3.m3.3.3" xref="p3.3.3.3.3.m3.3.3.cmml"><mi id="p3.3.3.3.3.m3.3.3a" xref="p3.3.3.3.3.m3.3.3.cmml"></mi><mrow id="p3.3.3.3.3.m3.3.3.3.5" xref="p3.3.3.3.3.m3.3.3.3.4.cmml"><mn id="p3.3.3.3.3.m3.1.1.1.1" mathvariant="normal" xref="p3.3.3.3.3.m3.1.1.1.1.cmml">1</mn><mo id="p3.3.3.3.3.m3.3.3.3.5.1" mathvariant="normal" xref="p3.3.3.3.3.m3.3.3.3.4.cmml">,</mo><mn id="p3.3.3.3.3.m3.2.2.2.2" mathvariant="normal" xref="p3.3.3.3.3.m3.2.2.2.2.cmml">2</mn><mo id="p3.3.3.3.3.m3.3.3.3.5.2" mathvariant="normal" xref="p3.3.3.3.3.m3.3.3.3.4.cmml">,</mo><mn id="p3.3.3.3.3.m3.3.3.3.3" mathvariant="normal" xref="p3.3.3.3.3.m3.3.3.3.3.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="p3.3.3.3.3.m3.3b"><apply id="p3.3.3.3.3.m3.3.3.cmml" xref="p3.3.3.3.3.m3.3.3"><list id="p3.3.3.3.3.m3.3.3.3.4.cmml" xref="p3.3.3.3.3.m3.3.3.3.5"><cn id="p3.3.3.3.3.m3.1.1.1.1.cmml" type="integer" xref="p3.3.3.3.3.m3.1.1.1.1">1</cn><cn id="p3.3.3.3.3.m3.2.2.2.2.cmml" type="integer" xref="p3.3.3.3.3.m3.2.2.2.2">2</cn><cn id="p3.3.3.3.3.m3.3.3.3.3.cmml" type="integer" xref="p3.3.3.3.3.m3.3.3.3.3">3</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.3.3.3.3.m3.3c">{}^{1,2,3}</annotation><annotation encoding="application/x-llamapun" id="p3.3.3.3.3.m3.3d">start_FLOATSUPERSCRIPT 1 , 2 , 3 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Yue Yu<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p3.4.4.4.4.m4.1"><semantics id="p3.4.4.4.4.m4.1a"><msup id="p3.4.4.4.4.m4.1.1" xref="p3.4.4.4.4.m4.1.1.cmml"><mi id="p3.4.4.4.4.m4.1.1a" xref="p3.4.4.4.4.m4.1.1.cmml"></mi><mn id="p3.4.4.4.4.m4.1.1.1" mathvariant="normal" xref="p3.4.4.4.4.m4.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p3.4.4.4.4.m4.1b"><apply id="p3.4.4.4.4.m4.1.1.cmml" xref="p3.4.4.4.4.m4.1.1"><cn id="p3.4.4.4.4.m4.1.1.1.cmml" type="integer" xref="p3.4.4.4.4.m4.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.4.4.4.4.m4.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p3.4.4.4.4.m4.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="p3.5.5">
<td class="ltx_td ltx_align_center" id="p3.5.5.1">
<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p3.5.5.1.m1.1"><semantics id="p3.5.5.1.m1.1a"><msup id="p3.5.5.1.m1.1.1" xref="p3.5.5.1.m1.1.1.cmml"><mi id="p3.5.5.1.m1.1.1a" xref="p3.5.5.1.m1.1.1.cmml"></mi><mn id="p3.5.5.1.m1.1.1.1" xref="p3.5.5.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p3.5.5.1.m1.1b"><apply id="p3.5.5.1.m1.1.1.cmml" xref="p3.5.5.1.m1.1.1"><cn id="p3.5.5.1.m1.1.1.1.cmml" type="integer" xref="p3.5.5.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.5.5.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p3.5.5.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>Key Laboratory of Intelligent Information Processing</td>
</tr>
<tr class="ltx_tr" id="p3.7.8.1">
<td class="ltx_td ltx_align_center" id="p3.7.8.1.1">Institute of Computing Technology, Chinese Academy of Sciences</td>
</tr>
<tr class="ltx_tr" id="p3.7.7">
<td class="ltx_td ltx_align_center" id="p3.7.7.2">
<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p3.6.6.1.m1.1"><semantics id="p3.6.6.1.m1.1a"><msup id="p3.6.6.1.m1.1.1" xref="p3.6.6.1.m1.1.1.cmml"><mi id="p3.6.6.1.m1.1.1a" xref="p3.6.6.1.m1.1.1.cmml"></mi><mn id="p3.6.6.1.m1.1.1.1" xref="p3.6.6.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p3.6.6.1.m1.1b"><apply id="p3.6.6.1.m1.1.1.cmml" xref="p3.6.6.1.m1.1.1"><cn id="p3.6.6.1.m1.1.1.1.cmml" type="integer" xref="p3.6.6.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.6.6.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p3.6.6.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>Peng Cheng Laboratory, <math alttext="{}^{3}" class="ltx_Math" display="inline" id="p3.7.7.2.m2.1"><semantics id="p3.7.7.2.m2.1a"><msup id="p3.7.7.2.m2.1.1" xref="p3.7.7.2.m2.1.1.cmml"><mi id="p3.7.7.2.m2.1.1a" xref="p3.7.7.2.m2.1.1.cmml"></mi><mn id="p3.7.7.2.m2.1.1.1" xref="p3.7.7.2.m2.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="p3.7.7.2.m2.1b"><apply id="p3.7.7.2.m2.1.1.cmml" xref="p3.7.7.2.m2.1.1"><cn id="p3.7.7.2.m2.1.1.1.cmml" type="integer" xref="p3.7.7.2.m2.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.7.7.2.m2.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="p3.7.7.2.m2.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math>University of Chinese Academy of Sciences</td>
</tr>
<tr class="ltx_tr" id="p3.7.9.2">
<td class="ltx_td ltx_align_center" id="p3.7.9.2.1">daishaojie22@mails.ucas.ac.cn, hit.liuxin@gmail.com, luop@ict.ac.cn, yuy@pcl.ac.cn</td>
</tr>
</tbody>
</table>
<br class="ltx_break"/>
<p class="ltx_p" id="p3.8"><span class="ltx_text ltx_font_italic" id="p3.8.1">Abstract content</span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.Â Â Â Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models (LLMs) have demonstrated remarkable capabilities in various scenarios, displaying a level of aptitude that approaches or even outperforms human-level intelligence in certain aspects.
Among these diverse competencies of LLMs, the multilingual neural machine translation (MNMT) ability of LLMs has gained significant attention from academia and industry. Some researchesÂ <cite class="ltx_cite ltx_citemacro_cite">Vilar etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib56" title="">2022</a>); Hendy etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib24" title="">2023</a>); Bawden and Yvon (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib3" title="">2023</a>)</cite> have investigated and presented a comprehensive evaluation of numerous LLMs for multilingual neural machine translation performance, such as the performance of different models in comparison with state-of-the-art baselines.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, as reported byÂ <cite class="ltx_cite ltx_citemacro_citet">Bawden and Yvon (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib3" title="">2023</a>)</cite>, in the BLOOM model, MNMT zero-shot prompt seriously suffers from the off-target issue of over-generation and generating in the wrong language. This phenomenon not only appears in generative pre-training (GPT) style models, but also in encoder-decoder style models. FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S1.F1" title="Figure 1 â€£ 1. Introduction â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a> shows the off-target ratio of BLOOMZ-7B1 and mT0-xl (3.7B) models on IWSLT 2017 test sets. In FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S1.F1" title="Figure 1 â€£ 1. Introduction â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, we utilize the <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.1">langdetect</span> package<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/Mimino666/langdetect</span></span></span> to detect the highest probability language of the generated translations.
As shown in the Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S1.F1" title="Figure 1 â€£ 1. Introduction â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, we have the following observations: 1) both BLOOMZ-7B1 and mT0-xlÂ <cite class="ltx_cite ltx_citemacro_cite">Muennighoff etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib37" title="">2022</a>)</cite> exhibit a serious off-target issue under the zero-shot setting with the prompt of "<span class="ltx_text ltx_font_italic" id="S1.p2.1.2">Translate the following text from </span>{<span class="ltx_text ltx_font_italic" id="S1.p2.1.3">sourceLanguage</span>}<span class="ltx_text ltx_font_italic" id="S1.p2.1.4"> to </span>{<span class="ltx_text ltx_font_italic" id="S1.p2.1.5">targetLanguage</span>}<span class="ltx_text ltx_font_italic" id="S1.p2.1.6">: </span>{<span class="ltx_text ltx_font_italic" id="S1.p2.1.7">sourceString</span>}" used in the multitask fine-turning stage.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_flex_size_1 ltx_align_center" id="S1.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="317" id="S1.F1.sf1.g1" src="x1.png" width="423"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_sansserif" id="S1.F1.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_sansserif" id="S1.F1.sf1.3.2" style="font-size:90%;">BLOOMZ-7B1</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_flex_size_1 ltx_align_center" id="S1.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="317" id="S1.F1.sf2.g1" src="x2.png" width="423"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_sansserif" id="S1.F1.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_sansserif" id="S1.F1.sf2.3.2" style="font-size:90%;">mT0-xl (3.7B)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Off-target ratio on IWSLT 2017 test datasets (to evaluate the off-target ratio between any pair of languages, we extract a set of 488 sentences from the IWSLT 2017Â <cite class="ltx_cite ltx_citemacro_cite">Cettolo etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib8" title="">2017</a>)</cite> test dataset that have identical English content, and each sentence has 10 different translations in various languages).</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">2) Low-resource languages present more serious off-target issue, such as Japanese (ja), Korean (ko), and Dutch (nl), especially between non-English language pairs.
3) mT0-xl shows a better translation performance compared to BLOOMZ-7B1, even though the later has more parameters.
We also show the over/under generation results on IWSLT 2017 in section <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.SS6.SSS5" title="5.6.5. Over/Under-generation â€£ 5.6. Ablation Study â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5.6.5</span></a>, Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.F5" title="Figure 5 â€£ 5.6.5. Over/Under-generation â€£ 5.6. Ablation Study â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>.
We exploit the over/under-generation ratio as an indicator of LLMâ€™s comprehension on instructions, where a lower ratio denotes a stronger ability to understand the instructions.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><cite class="ltx_cite ltx_citemacro_citet">Vilar etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib56" title="">2022</a>)</cite> have investigated various strategies for choosing translation demonstrations for few-shot prompting to probe the MNMT ability of PaLM. The research concludes that demonstrations quality is the most important factor, and when there are no demonstrations, it seriously affects the quality of translation.
This is in line with human intuition.
Furthermore, eliciting and probing the translation ability of LLMs still face four major challenges in off-target issue:
1) <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">misunderstanding of instructions (MI)</span>: when we feed a LLM with a translation-related instruction and source sentence in English, the expected gold output should be the target sentence in Chinese. But the LLM may misunderstand the instruction and treat it as a classification-related instruction, leading to an incorrect output.
2) <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">off-target translation (OT)</span>: the LLM outputs a sentence that is not in the target language.
3) <span class="ltx_text ltx_font_italic" id="S1.p4.1.3">source copy (SC)</span>: LLM outputs a sentence that is almost identical to the source sentence, which can also be considered as a type of OT error.
4) <span class="ltx_text ltx_font_italic" id="S1.p4.1.4">over/under generation (OUG)</span>: LLM translations are significantly longer or shorter than references.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address these challenges and explore the translation abilities of LLMs while enhancing their understanding of instructions,
in this work, we focus on automatically constructing constrained template in the target side to assist LLMs to generate the desired outputs.
First, we propose two types of trigger tokens: common trigger tokens and specific trigger tokens. The common trigger tokens are designed to capture shared information across different translation directions. The specific trigger tokens aim to capture target language-specific information.
Second, we design a mapping strategy to establish an association between trigger token sequences and different translation directions.
Trigger tokens can be considered as a continuous latent representation of different translation directions.
</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Overall, our primary contributions can be summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a novel task-enhanced constrained turning method (TECT-MNMT), which utilizes a manually designed hard encoding constrained template to assist LLMs in understanding the prompt and guiding the modelâ€™s output.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We further extent TECT-MNMT to auto-constriction turning (ACT-MNMT), which uses a soft encoding constrained template that do not require manual design to constrain the model output.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We conduct extensive experiments on WMT dataset. Results show that both TECT-MNMT and ACT-MNMT can effectively alleviate four types of translation errors mentioned above and outperforms instruction fine-tuning baseline.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.Â Â Â Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">LLMs have acquired advanced language comprehension skills during the pre-training stage, and shown remarkable abilities in multilingual translationsÂ <cite class="ltx_cite ltx_citemacro_cite">Lin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib33" title="">2022</a>)</cite>.
The traditional method to probe the translation performance of LLM is In-Context Learning (ICL)Â <cite class="ltx_cite ltx_citemacro_cite">Brown etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib4" title="">2020</a>)</cite>, which is powerful and can provide a few demonstrations to guide models on how to perform, even on an unseen task without fine-tuning. Researchers have been devoted to investigating and designing demonstrations selection strategy. <cite class="ltx_cite ltx_citemacro_citet">Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib63" title="">2023</a>)</cite> find that prompt template and demonstration selection both have substantial impact on translation.
<cite class="ltx_cite ltx_citemacro_citet">Agrawal etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib1" title="">2022</a>)</cite> shows that the translation quality and the domain of the in-context examples matter greatly.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">However, LLMs with a parameter size below 7 billion exhibit a weak capacity for ICL, which may underestimate their translation abilitiesÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib32" title="">2023</a>)</cite>. Recently, AutoPromptÂ <cite class="ltx_cite ltx_citemacro_cite">Shin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib50" title="">2020</a>)</cite> employs discrete text prompts to maximize the log-likelihood of true labels. While this technique outperforms manually designed prompts, there is still a gap compared to model tuning, and discrete prompts are usually difficult for humans to understand.
Therefore, P-TuningÂ <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib35" title="">2021</a>)</cite>, Prompt TuningÂ <cite class="ltx_cite ltx_citemacro_cite">Lester etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib30" title="">2021</a>)</cite> and Multitask Prompt TuningÂ <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib59" title="">2023</a>)</cite> have been developed to enhance the LLMâ€™s ability in downstream tasks through optimize prompts in a continuous way.
Recently, some researchers attempt to strike a balance between the costs of fine-tuning, inference, and the translation performance of ICL, such as mFTIÂ <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib32" title="">2023</a>)</cite>, which collects a small portion of high-quality multilingual parallel sentences (1k per language pair) to fine-tune XGLM-7B <cite class="ltx_cite ltx_citemacro_cite">Lin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib33" title="">2022</a>)</cite>, and then explores the multilingual translation performance.
However, all the methods mentioned above focus on making efforts on the input side of the model.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.Â Â Â Problem</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.7"><span class="ltx_text ltx_font_bold" id="S3.p1.7.1">Multilingual Neural Machine Translation</span> is an important task in the field of natural language processing (NLP), which aims to translate a sentence <math alttext="x" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_x</annotation></semantics></math> in source language into a sentence <math alttext="y" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_y</annotation></semantics></math> in target languageÂ <cite class="ltx_cite ltx_citemacro_cite">Huang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib27" title="">2022</a>); Costa-jussÃ  etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib15" title="">2022</a>)</cite>. Formally, given a set of language pairs <math alttext="L" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">italic_L</annotation></semantics></math>, the MNMT model is trained on the combination of <math alttext="|L|" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><mrow id="S3.p1.4.m4.1.2.2" xref="S3.p1.4.m4.1.2.1.cmml"><mo id="S3.p1.4.m4.1.2.2.1" stretchy="false" xref="S3.p1.4.m4.1.2.1.1.cmml">|</mo><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">L</mi><mo id="S3.p1.4.m4.1.2.2.2" stretchy="false" xref="S3.p1.4.m4.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.2.1.cmml" xref="S3.p1.4.m4.1.2.2"><abs id="S3.p1.4.m4.1.2.1.1.cmml" xref="S3.p1.4.m4.1.2.2.1"></abs><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">ğ¿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">|L|</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">| italic_L |</annotation></semantics></math> parallel corpus <math alttext="D=\{D_{l}^{train}\}_{l=1}^{|L|}" class="ltx_Math" display="inline" id="S3.p1.5.m5.2"><semantics id="S3.p1.5.m5.2a"><mrow id="S3.p1.5.m5.2.2" xref="S3.p1.5.m5.2.2.cmml"><mi id="S3.p1.5.m5.2.2.3" xref="S3.p1.5.m5.2.2.3.cmml">D</mi><mo id="S3.p1.5.m5.2.2.2" xref="S3.p1.5.m5.2.2.2.cmml">=</mo><msubsup id="S3.p1.5.m5.2.2.1" xref="S3.p1.5.m5.2.2.1.cmml"><mrow id="S3.p1.5.m5.2.2.1.1.1.1" xref="S3.p1.5.m5.2.2.1.1.1.2.cmml"><mo id="S3.p1.5.m5.2.2.1.1.1.1.2" stretchy="false" xref="S3.p1.5.m5.2.2.1.1.1.2.cmml">{</mo><msubsup id="S3.p1.5.m5.2.2.1.1.1.1.1" xref="S3.p1.5.m5.2.2.1.1.1.1.1.cmml"><mi id="S3.p1.5.m5.2.2.1.1.1.1.1.2.2" xref="S3.p1.5.m5.2.2.1.1.1.1.1.2.2.cmml">D</mi><mi id="S3.p1.5.m5.2.2.1.1.1.1.1.2.3" xref="S3.p1.5.m5.2.2.1.1.1.1.1.2.3.cmml">l</mi><mrow id="S3.p1.5.m5.2.2.1.1.1.1.1.3" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3.cmml"><mi id="S3.p1.5.m5.2.2.1.1.1.1.1.3.2" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3.2.cmml">t</mi><mo id="S3.p1.5.m5.2.2.1.1.1.1.1.3.1" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S3.p1.5.m5.2.2.1.1.1.1.1.3.3" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3.3.cmml">r</mi><mo id="S3.p1.5.m5.2.2.1.1.1.1.1.3.1a" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S3.p1.5.m5.2.2.1.1.1.1.1.3.4" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3.4.cmml">a</mi><mo id="S3.p1.5.m5.2.2.1.1.1.1.1.3.1b" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S3.p1.5.m5.2.2.1.1.1.1.1.3.5" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3.5.cmml">i</mi><mo id="S3.p1.5.m5.2.2.1.1.1.1.1.3.1c" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S3.p1.5.m5.2.2.1.1.1.1.1.3.6" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3.6.cmml">n</mi></mrow></msubsup><mo id="S3.p1.5.m5.2.2.1.1.1.1.3" stretchy="false" xref="S3.p1.5.m5.2.2.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.p1.5.m5.2.2.1.1.3" xref="S3.p1.5.m5.2.2.1.1.3.cmml"><mi id="S3.p1.5.m5.2.2.1.1.3.2" xref="S3.p1.5.m5.2.2.1.1.3.2.cmml">l</mi><mo id="S3.p1.5.m5.2.2.1.1.3.1" xref="S3.p1.5.m5.2.2.1.1.3.1.cmml">=</mo><mn id="S3.p1.5.m5.2.2.1.1.3.3" xref="S3.p1.5.m5.2.2.1.1.3.3.cmml">1</mn></mrow><mrow id="S3.p1.5.m5.1.1.1.3" xref="S3.p1.5.m5.1.1.1.2.cmml"><mo id="S3.p1.5.m5.1.1.1.3.1" stretchy="false" xref="S3.p1.5.m5.1.1.1.2.1.cmml">|</mo><mi id="S3.p1.5.m5.1.1.1.1" xref="S3.p1.5.m5.1.1.1.1.cmml">L</mi><mo id="S3.p1.5.m5.1.1.1.3.2" stretchy="false" xref="S3.p1.5.m5.1.1.1.2.1.cmml">|</mo></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.2b"><apply id="S3.p1.5.m5.2.2.cmml" xref="S3.p1.5.m5.2.2"><eq id="S3.p1.5.m5.2.2.2.cmml" xref="S3.p1.5.m5.2.2.2"></eq><ci id="S3.p1.5.m5.2.2.3.cmml" xref="S3.p1.5.m5.2.2.3">ğ·</ci><apply id="S3.p1.5.m5.2.2.1.cmml" xref="S3.p1.5.m5.2.2.1"><csymbol cd="ambiguous" id="S3.p1.5.m5.2.2.1.2.cmml" xref="S3.p1.5.m5.2.2.1">superscript</csymbol><apply id="S3.p1.5.m5.2.2.1.1.cmml" xref="S3.p1.5.m5.2.2.1"><csymbol cd="ambiguous" id="S3.p1.5.m5.2.2.1.1.2.cmml" xref="S3.p1.5.m5.2.2.1">subscript</csymbol><set id="S3.p1.5.m5.2.2.1.1.1.2.cmml" xref="S3.p1.5.m5.2.2.1.1.1.1"><apply id="S3.p1.5.m5.2.2.1.1.1.1.1.cmml" xref="S3.p1.5.m5.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p1.5.m5.2.2.1.1.1.1.1.1.cmml" xref="S3.p1.5.m5.2.2.1.1.1.1.1">superscript</csymbol><apply id="S3.p1.5.m5.2.2.1.1.1.1.1.2.cmml" xref="S3.p1.5.m5.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p1.5.m5.2.2.1.1.1.1.1.2.1.cmml" xref="S3.p1.5.m5.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.p1.5.m5.2.2.1.1.1.1.1.2.2.cmml" xref="S3.p1.5.m5.2.2.1.1.1.1.1.2.2">ğ·</ci><ci id="S3.p1.5.m5.2.2.1.1.1.1.1.2.3.cmml" xref="S3.p1.5.m5.2.2.1.1.1.1.1.2.3">ğ‘™</ci></apply><apply id="S3.p1.5.m5.2.2.1.1.1.1.1.3.cmml" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3"><times id="S3.p1.5.m5.2.2.1.1.1.1.1.3.1.cmml" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3.1"></times><ci id="S3.p1.5.m5.2.2.1.1.1.1.1.3.2.cmml" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3.2">ğ‘¡</ci><ci id="S3.p1.5.m5.2.2.1.1.1.1.1.3.3.cmml" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3.3">ğ‘Ÿ</ci><ci id="S3.p1.5.m5.2.2.1.1.1.1.1.3.4.cmml" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3.4">ğ‘</ci><ci id="S3.p1.5.m5.2.2.1.1.1.1.1.3.5.cmml" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3.5">ğ‘–</ci><ci id="S3.p1.5.m5.2.2.1.1.1.1.1.3.6.cmml" xref="S3.p1.5.m5.2.2.1.1.1.1.1.3.6">ğ‘›</ci></apply></apply></set><apply id="S3.p1.5.m5.2.2.1.1.3.cmml" xref="S3.p1.5.m5.2.2.1.1.3"><eq id="S3.p1.5.m5.2.2.1.1.3.1.cmml" xref="S3.p1.5.m5.2.2.1.1.3.1"></eq><ci id="S3.p1.5.m5.2.2.1.1.3.2.cmml" xref="S3.p1.5.m5.2.2.1.1.3.2">ğ‘™</ci><cn id="S3.p1.5.m5.2.2.1.1.3.3.cmml" type="integer" xref="S3.p1.5.m5.2.2.1.1.3.3">1</cn></apply></apply><apply id="S3.p1.5.m5.1.1.1.2.cmml" xref="S3.p1.5.m5.1.1.1.3"><abs id="S3.p1.5.m5.1.1.1.2.1.cmml" xref="S3.p1.5.m5.1.1.1.3.1"></abs><ci id="S3.p1.5.m5.1.1.1.1.cmml" xref="S3.p1.5.m5.1.1.1.1">ğ¿</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.2c">D=\{D_{l}^{train}\}_{l=1}^{|L|}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.5.m5.2d">italic_D = { italic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_L | end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="D_{l}^{train}" class="ltx_Math" display="inline" id="S3.p1.6.m6.1"><semantics id="S3.p1.6.m6.1a"><msubsup id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml"><mi id="S3.p1.6.m6.1.1.2.2" xref="S3.p1.6.m6.1.1.2.2.cmml">D</mi><mi id="S3.p1.6.m6.1.1.2.3" xref="S3.p1.6.m6.1.1.2.3.cmml">l</mi><mrow id="S3.p1.6.m6.1.1.3" xref="S3.p1.6.m6.1.1.3.cmml"><mi id="S3.p1.6.m6.1.1.3.2" xref="S3.p1.6.m6.1.1.3.2.cmml">t</mi><mo id="S3.p1.6.m6.1.1.3.1" xref="S3.p1.6.m6.1.1.3.1.cmml">â¢</mo><mi id="S3.p1.6.m6.1.1.3.3" xref="S3.p1.6.m6.1.1.3.3.cmml">r</mi><mo id="S3.p1.6.m6.1.1.3.1a" xref="S3.p1.6.m6.1.1.3.1.cmml">â¢</mo><mi id="S3.p1.6.m6.1.1.3.4" xref="S3.p1.6.m6.1.1.3.4.cmml">a</mi><mo id="S3.p1.6.m6.1.1.3.1b" xref="S3.p1.6.m6.1.1.3.1.cmml">â¢</mo><mi id="S3.p1.6.m6.1.1.3.5" xref="S3.p1.6.m6.1.1.3.5.cmml">i</mi><mo id="S3.p1.6.m6.1.1.3.1c" xref="S3.p1.6.m6.1.1.3.1.cmml">â¢</mo><mi id="S3.p1.6.m6.1.1.3.6" xref="S3.p1.6.m6.1.1.3.6.cmml">n</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><apply id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p1.6.m6.1.1.1.cmml" xref="S3.p1.6.m6.1.1">superscript</csymbol><apply id="S3.p1.6.m6.1.1.2.cmml" xref="S3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p1.6.m6.1.1.2.1.cmml" xref="S3.p1.6.m6.1.1">subscript</csymbol><ci id="S3.p1.6.m6.1.1.2.2.cmml" xref="S3.p1.6.m6.1.1.2.2">ğ·</ci><ci id="S3.p1.6.m6.1.1.2.3.cmml" xref="S3.p1.6.m6.1.1.2.3">ğ‘™</ci></apply><apply id="S3.p1.6.m6.1.1.3.cmml" xref="S3.p1.6.m6.1.1.3"><times id="S3.p1.6.m6.1.1.3.1.cmml" xref="S3.p1.6.m6.1.1.3.1"></times><ci id="S3.p1.6.m6.1.1.3.2.cmml" xref="S3.p1.6.m6.1.1.3.2">ğ‘¡</ci><ci id="S3.p1.6.m6.1.1.3.3.cmml" xref="S3.p1.6.m6.1.1.3.3">ğ‘Ÿ</ci><ci id="S3.p1.6.m6.1.1.3.4.cmml" xref="S3.p1.6.m6.1.1.3.4">ğ‘</ci><ci id="S3.p1.6.m6.1.1.3.5.cmml" xref="S3.p1.6.m6.1.1.3.5">ğ‘–</ci><ci id="S3.p1.6.m6.1.1.3.6.cmml" xref="S3.p1.6.m6.1.1.3.6">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">D_{l}^{train}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.6.m6.1d">italic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t italic_r italic_a italic_i italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> denotes the training data of language pair <math alttext="(X_{l},Y_{l})" class="ltx_Math" display="inline" id="S3.p1.7.m7.2"><semantics id="S3.p1.7.m7.2a"><mrow id="S3.p1.7.m7.2.2.2" xref="S3.p1.7.m7.2.2.3.cmml"><mo id="S3.p1.7.m7.2.2.2.3" stretchy="false" xref="S3.p1.7.m7.2.2.3.cmml">(</mo><msub id="S3.p1.7.m7.1.1.1.1" xref="S3.p1.7.m7.1.1.1.1.cmml"><mi id="S3.p1.7.m7.1.1.1.1.2" xref="S3.p1.7.m7.1.1.1.1.2.cmml">X</mi><mi id="S3.p1.7.m7.1.1.1.1.3" xref="S3.p1.7.m7.1.1.1.1.3.cmml">l</mi></msub><mo id="S3.p1.7.m7.2.2.2.4" xref="S3.p1.7.m7.2.2.3.cmml">,</mo><msub id="S3.p1.7.m7.2.2.2.2" xref="S3.p1.7.m7.2.2.2.2.cmml"><mi id="S3.p1.7.m7.2.2.2.2.2" xref="S3.p1.7.m7.2.2.2.2.2.cmml">Y</mi><mi id="S3.p1.7.m7.2.2.2.2.3" xref="S3.p1.7.m7.2.2.2.2.3.cmml">l</mi></msub><mo id="S3.p1.7.m7.2.2.2.5" stretchy="false" xref="S3.p1.7.m7.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.2b"><interval closure="open" id="S3.p1.7.m7.2.2.3.cmml" xref="S3.p1.7.m7.2.2.2"><apply id="S3.p1.7.m7.1.1.1.1.cmml" xref="S3.p1.7.m7.1.1.1.1"><csymbol cd="ambiguous" id="S3.p1.7.m7.1.1.1.1.1.cmml" xref="S3.p1.7.m7.1.1.1.1">subscript</csymbol><ci id="S3.p1.7.m7.1.1.1.1.2.cmml" xref="S3.p1.7.m7.1.1.1.1.2">ğ‘‹</ci><ci id="S3.p1.7.m7.1.1.1.1.3.cmml" xref="S3.p1.7.m7.1.1.1.1.3">ğ‘™</ci></apply><apply id="S3.p1.7.m7.2.2.2.2.cmml" xref="S3.p1.7.m7.2.2.2.2"><csymbol cd="ambiguous" id="S3.p1.7.m7.2.2.2.2.1.cmml" xref="S3.p1.7.m7.2.2.2.2">subscript</csymbol><ci id="S3.p1.7.m7.2.2.2.2.2.cmml" xref="S3.p1.7.m7.2.2.2.2.2">ğ‘Œ</ci><ci id="S3.p1.7.m7.2.2.2.2.3.cmml" xref="S3.p1.7.m7.2.2.2.2.3">ğ‘™</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.2c">(X_{l},Y_{l})</annotation><annotation encoding="application/x-llamapun" id="S3.p1.7.m7.2d">( italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , italic_Y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT )</annotation></semantics></math>. The MNMT model is commonly trained to minimize the negative log-likelihood loss as follows:</p>
</div>
<div class="ltx_para" id="S3.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}=\sum_{D_{l}\in D}\sum_{(x,y)\in D_{l}}\sum_{i\leq|y|}-\log p(y_{i}%
|x,y_{&lt;i};\theta)," class="ltx_Math" display="block" id="S3.E1.m1.6"><semantics id="S3.E1.m1.6a"><mrow id="S3.E1.m1.6.6.1" xref="S3.E1.m1.6.6.1.1.cmml"><mrow id="S3.E1.m1.6.6.1.1" xref="S3.E1.m1.6.6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.6.6.1.1.3" xref="S3.E1.m1.6.6.1.1.3.cmml">â„’</mi><mo id="S3.E1.m1.6.6.1.1.2" rspace="0.111em" xref="S3.E1.m1.6.6.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.6.6.1.1.1" xref="S3.E1.m1.6.6.1.1.1.cmml"><mrow id="S3.E1.m1.6.6.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.3.cmml"><munder id="S3.E1.m1.6.6.1.1.1.3.1" xref="S3.E1.m1.6.6.1.1.1.3.1.cmml"><mo id="S3.E1.m1.6.6.1.1.1.3.1.2" movablelimits="false" rspace="0em" xref="S3.E1.m1.6.6.1.1.1.3.1.2.cmml">âˆ‘</mo><mrow id="S3.E1.m1.6.6.1.1.1.3.1.3" xref="S3.E1.m1.6.6.1.1.1.3.1.3.cmml"><msub id="S3.E1.m1.6.6.1.1.1.3.1.3.2" xref="S3.E1.m1.6.6.1.1.1.3.1.3.2.cmml"><mi id="S3.E1.m1.6.6.1.1.1.3.1.3.2.2" xref="S3.E1.m1.6.6.1.1.1.3.1.3.2.2.cmml">D</mi><mi id="S3.E1.m1.6.6.1.1.1.3.1.3.2.3" xref="S3.E1.m1.6.6.1.1.1.3.1.3.2.3.cmml">l</mi></msub><mo id="S3.E1.m1.6.6.1.1.1.3.1.3.1" xref="S3.E1.m1.6.6.1.1.1.3.1.3.1.cmml">âˆˆ</mo><mi id="S3.E1.m1.6.6.1.1.1.3.1.3.3" xref="S3.E1.m1.6.6.1.1.1.3.1.3.3.cmml">D</mi></mrow></munder><mrow id="S3.E1.m1.6.6.1.1.1.3.2" xref="S3.E1.m1.6.6.1.1.1.3.2.cmml"><munder id="S3.E1.m1.6.6.1.1.1.3.2.1" xref="S3.E1.m1.6.6.1.1.1.3.2.1.cmml"><mo id="S3.E1.m1.6.6.1.1.1.3.2.1.2" movablelimits="false" rspace="0em" xref="S3.E1.m1.6.6.1.1.1.3.2.1.2.cmml">âˆ‘</mo><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mrow id="S3.E1.m1.2.2.2.4.2" xref="S3.E1.m1.2.2.2.4.1.cmml"><mo id="S3.E1.m1.2.2.2.4.2.1" stretchy="false" xref="S3.E1.m1.2.2.2.4.1.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">x</mi><mo id="S3.E1.m1.2.2.2.4.2.2" xref="S3.E1.m1.2.2.2.4.1.cmml">,</mo><mi id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">y</mi><mo id="S3.E1.m1.2.2.2.4.2.3" stretchy="false" xref="S3.E1.m1.2.2.2.4.1.cmml">)</mo></mrow><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">âˆˆ</mo><msub id="S3.E1.m1.2.2.2.5" xref="S3.E1.m1.2.2.2.5.cmml"><mi id="S3.E1.m1.2.2.2.5.2" xref="S3.E1.m1.2.2.2.5.2.cmml">D</mi><mi id="S3.E1.m1.2.2.2.5.3" xref="S3.E1.m1.2.2.2.5.3.cmml">l</mi></msub></mrow></munder><munder id="S3.E1.m1.6.6.1.1.1.3.2.2" xref="S3.E1.m1.6.6.1.1.1.3.2.2.cmml"><mo id="S3.E1.m1.6.6.1.1.1.3.2.2.2" movablelimits="false" rspace="0em" xref="S3.E1.m1.6.6.1.1.1.3.2.2.2.cmml">âˆ‘</mo><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.cmml"><mi id="S3.E1.m1.3.3.1.3" xref="S3.E1.m1.3.3.1.3.cmml">i</mi><mo id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.2.cmml">â‰¤</mo><mrow id="S3.E1.m1.3.3.1.4.2" xref="S3.E1.m1.3.3.1.4.1.cmml"><mo id="S3.E1.m1.3.3.1.4.2.1" stretchy="false" xref="S3.E1.m1.3.3.1.4.1.1.cmml">|</mo><mi id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml">y</mi><mo id="S3.E1.m1.3.3.1.4.2.2" stretchy="false" xref="S3.E1.m1.3.3.1.4.1.1.cmml">|</mo></mrow></mrow></munder></mrow></mrow><mo id="S3.E1.m1.6.6.1.1.1.2" lspace="0em" xref="S3.E1.m1.6.6.1.1.1.2.cmml">âˆ’</mo><mrow id="S3.E1.m1.6.6.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.cmml"><mrow id="S3.E1.m1.6.6.1.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.1.3.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.3.1" xref="S3.E1.m1.6.6.1.1.1.1.3.1.cmml">log</mi><mo id="S3.E1.m1.6.6.1.1.1.1.3a" lspace="0.167em" xref="S3.E1.m1.6.6.1.1.1.1.3.cmml">â¡</mo><mi id="S3.E1.m1.6.6.1.1.1.1.3.2" xref="S3.E1.m1.6.6.1.1.1.1.3.2.cmml">p</mi></mrow><mo id="S3.E1.m1.6.6.1.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.2.cmml">y</mi><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">x</mi><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.2.cmml">,</mo><msub id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.cmml">y</mi><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.2.cmml">;</mo><mi id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">Î¸</mi></mrow></mrow><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.6.6.1.2" xref="S3.E1.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.6b"><apply id="S3.E1.m1.6.6.1.1.cmml" xref="S3.E1.m1.6.6.1"><eq id="S3.E1.m1.6.6.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.2"></eq><ci id="S3.E1.m1.6.6.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.3">â„’</ci><apply id="S3.E1.m1.6.6.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1"><minus id="S3.E1.m1.6.6.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.2"></minus><apply id="S3.E1.m1.6.6.1.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.1.3"><apply id="S3.E1.m1.6.6.1.1.1.3.1.cmml" xref="S3.E1.m1.6.6.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.1.3.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.3.1">subscript</csymbol><sum id="S3.E1.m1.6.6.1.1.1.3.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.3.1.2"></sum><apply id="S3.E1.m1.6.6.1.1.1.3.1.3.cmml" xref="S3.E1.m1.6.6.1.1.1.3.1.3"><in id="S3.E1.m1.6.6.1.1.1.3.1.3.1.cmml" xref="S3.E1.m1.6.6.1.1.1.3.1.3.1"></in><apply id="S3.E1.m1.6.6.1.1.1.3.1.3.2.cmml" xref="S3.E1.m1.6.6.1.1.1.3.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.1.3.1.3.2.1.cmml" xref="S3.E1.m1.6.6.1.1.1.3.1.3.2">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.1.3.1.3.2.2.cmml" xref="S3.E1.m1.6.6.1.1.1.3.1.3.2.2">ğ·</ci><ci id="S3.E1.m1.6.6.1.1.1.3.1.3.2.3.cmml" xref="S3.E1.m1.6.6.1.1.1.3.1.3.2.3">ğ‘™</ci></apply><ci id="S3.E1.m1.6.6.1.1.1.3.1.3.3.cmml" xref="S3.E1.m1.6.6.1.1.1.3.1.3.3">ğ·</ci></apply></apply><apply id="S3.E1.m1.6.6.1.1.1.3.2.cmml" xref="S3.E1.m1.6.6.1.1.1.3.2"><apply id="S3.E1.m1.6.6.1.1.1.3.2.1.cmml" xref="S3.E1.m1.6.6.1.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.1.3.2.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.3.2.1">subscript</csymbol><sum id="S3.E1.m1.6.6.1.1.1.3.2.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.3.2.1.2"></sum><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><in id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></in><interval closure="open" id="S3.E1.m1.2.2.2.4.1.cmml" xref="S3.E1.m1.2.2.2.4.2"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">ğ‘¥</ci><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">ğ‘¦</ci></interval><apply id="S3.E1.m1.2.2.2.5.cmml" xref="S3.E1.m1.2.2.2.5"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.5.1.cmml" xref="S3.E1.m1.2.2.2.5">subscript</csymbol><ci id="S3.E1.m1.2.2.2.5.2.cmml" xref="S3.E1.m1.2.2.2.5.2">ğ·</ci><ci id="S3.E1.m1.2.2.2.5.3.cmml" xref="S3.E1.m1.2.2.2.5.3">ğ‘™</ci></apply></apply></apply><apply id="S3.E1.m1.6.6.1.1.1.3.2.2.cmml" xref="S3.E1.m1.6.6.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.6.6.1.1.1.3.2.2">subscript</csymbol><sum id="S3.E1.m1.6.6.1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.6.6.1.1.1.3.2.2.2"></sum><apply id="S3.E1.m1.3.3.1.cmml" xref="S3.E1.m1.3.3.1"><leq id="S3.E1.m1.3.3.1.2.cmml" xref="S3.E1.m1.3.3.1.2"></leq><ci id="S3.E1.m1.3.3.1.3.cmml" xref="S3.E1.m1.3.3.1.3">ğ‘–</ci><apply id="S3.E1.m1.3.3.1.4.1.cmml" xref="S3.E1.m1.3.3.1.4.2"><abs id="S3.E1.m1.3.3.1.4.1.1.cmml" xref="S3.E1.m1.3.3.1.4.2.1"></abs><ci id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1">ğ‘¦</ci></apply></apply></apply></apply></apply><apply id="S3.E1.m1.6.6.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1"><times id="S3.E1.m1.6.6.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.2"></times><apply id="S3.E1.m1.6.6.1.1.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.3"><log id="S3.E1.m1.6.6.1.1.1.1.3.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.3.1"></log><ci id="S3.E1.m1.6.6.1.1.1.1.3.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.3.2">ğ‘</ci></apply><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.2">ğ‘¦</ci><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply><list id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1"><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">ğ‘¥</ci><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.2">ğ‘¦</ci><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.3"><lt id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply><ci id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5">ğœƒ</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.6c">\mathcal{L}=\sum_{D_{l}\in D}\sum_{(x,y)\in D_{l}}\sum_{i\leq|y|}-\log p(y_{i}%
|x,y_{&lt;i};\theta),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.6d">caligraphic_L = âˆ‘ start_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT âˆˆ italic_D end_POSTSUBSCRIPT âˆ‘ start_POSTSUBSCRIPT ( italic_x , italic_y ) âˆˆ italic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_POSTSUBSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_i â‰¤ | italic_y | end_POSTSUBSCRIPT - roman_log italic_p ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x , italic_y start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT ; italic_Î¸ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p2.3">where <math alttext="p(\cdot|\cdot;\theta)" class="ltx_math_unparsed" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><mrow id="S3.p2.1.m1.1b"><mi id="S3.p2.1.m1.1.1">p</mi><mrow id="S3.p2.1.m1.1.2"><mo id="S3.p2.1.m1.1.2.1" stretchy="false">(</mo><mo id="S3.p2.1.m1.1.2.2" lspace="0em" rspace="0em">â‹…</mo><mo fence="false" id="S3.p2.1.m1.1.2.3" stretchy="false">|</mo><mo id="S3.p2.1.m1.1.2.4" lspace="0em" rspace="0em">â‹…</mo><mo id="S3.p2.1.m1.1.2.5">;</mo><mi id="S3.p2.1.m1.1.2.6">Î¸</mi><mo id="S3.p2.1.m1.1.2.7" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">p(\cdot|\cdot;\theta)</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">italic_p ( â‹… | â‹… ; italic_Î¸ )</annotation></semantics></math> maps the source sentence and previous generated text <math alttext="y_{&lt;i}" class="ltx_Math" display="inline" id="S3.p2.2.m2.1"><semantics id="S3.p2.2.m2.1a"><msub id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mi id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml">y</mi><mrow id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml"><mi id="S3.p2.2.m2.1.1.3.2" xref="S3.p2.2.m2.1.1.3.2.cmml"></mi><mo id="S3.p2.2.m2.1.1.3.1" xref="S3.p2.2.m2.1.1.3.1.cmml">&lt;</mo><mi id="S3.p2.2.m2.1.1.3.3" xref="S3.p2.2.m2.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2">ğ‘¦</ci><apply id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3"><lt id="S3.p2.2.m2.1.1.3.1.cmml" xref="S3.p2.2.m2.1.1.3.1"></lt><csymbol cd="latexml" id="S3.p2.2.m2.1.1.3.2.cmml" xref="S3.p2.2.m2.1.1.3.2">absent</csymbol><ci id="S3.p2.2.m2.1.1.3.3.cmml" xref="S3.p2.2.m2.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">y_{&lt;i}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.1d">italic_y start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to the next target token, and <math alttext="\theta" class="ltx_Math" display="inline" id="S3.p2.3.m3.1"><semantics id="S3.p2.3.m3.1a"><mi id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.p2.3.m3.1d">italic_Î¸</annotation></semantics></math> denotes the modelâ€™s parameters.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S3.F2.g1" src="x3.png" width="872"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An overview of ACT-MNMT applied to Multilingual Neural Machine Translation (the number of common trigger token for each translation direction is 1, and the number of specific trigger token is 2 in this example).</figcaption>
</figure>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.4">We should note that depending on different MNMT model training strategies, the source sentence <math alttext="x" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">italic_x</annotation></semantics></math> and target sentence <math alttext="y" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">italic_y</annotation></semantics></math> may be combined with other information (<span class="ltx_text ltx_font_italic" id="S3.p3.4.1">e</span>.<span class="ltx_text ltx_font_italic" id="S3.p3.4.2">g</span>.Â , joining the prompt and source sentence together as the model input).
In the following part of this paper, We will continue to use <math alttext="x" class="ltx_Math" display="inline" id="S3.p3.3.m3.1"><semantics id="S3.p3.3.m3.1a"><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.p3.3.m3.1d">italic_x</annotation></semantics></math> to denote the model input and <math alttext="y" class="ltx_Math" display="inline" id="S3.p3.4.m4.1"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.p3.4.m4.1d">italic_y</annotation></semantics></math> to denote the model output.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.Â Â Â Methods</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Intuitively, a natural way to constrain and guide LLM to generate the optimal output is: first, generating the descriptive information related to the task in the prompt, and then generating the output, which is similar to Chain-of-ThoughtÂ <cite class="ltx_cite ltx_citemacro_cite">Wei etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib60" title="">2022</a>)</cite>.
Therefore, we can determine whether LLM understand the prompts based on this auxiliary descriptive information.
However, writing descriptive information is time-consuming and it is also unclear whether the same phrasing will be effective for every model.
To this end, we first propose a task-enhanced constrained turning mechanism for multilingual neural machine translation (TECT-MNMT), which utilises a hard encoding constrained template.
Then, we extend TECT-MNMT to an auto-constriction turning mechanism (ACT-MNMT), which utilises a soft encoding constrained template. The illustration of ACT-MNMT is provided in figureÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S3.F2" title="Figure 2 â€£ 3. Problem â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>.
</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>MT prompts for instruction turning. All prompts specify the target language (L2, in blue and in brackets). Six prompts are also specified the source language (L1, in red and in brackets) in the instruction.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1">Prompt</th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.1.1.1.2">Target</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.2.1">Given the following source text in <span class="ltx_text" id="S4.T1.1.2.2.1.1" style="color:#FF0000;">[L1]</span>: [SRC], a good <span class="ltx_text" id="S4.T1.1.2.2.1.2" style="color:#0000FF;">[L2]</span>
</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.2.2" rowspan="2"><span class="ltx_text" id="S4.T1.1.2.2.2.1">translate from <span class="ltx_text" id="S4.T1.1.2.2.2.1.1" style="color:#FF0000;">[L1]</span> to <span class="ltx_text" id="S4.T1.1.2.2.2.1.2" style="color:#0000FF;">[L2]</span>: [TGT]</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.3.3.1">translation is:</th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.4.4.1">If the original version says [SRC] then the <span class="ltx_text" id="S4.T1.1.4.4.1.1" style="color:#0000FF;">[L2]</span> version should say:</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.4.4.2">translate from <span class="ltx_text" id="S4.T1.1.4.4.2.1" style="color:#FF0000;">[L1]</span> to <span class="ltx_text" id="S4.T1.1.4.4.2.2" style="color:#0000FF;">[L2]</span>: [TGT]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.5.5.1">What is the <span class="ltx_text" id="S4.T1.1.5.5.1.1" style="color:#0000FF;">[L2]</span> translation of the sentence: [SRC]?</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.5.5.2">translate from <span class="ltx_text" id="S4.T1.1.5.5.2.1" style="color:#FF0000;">[L1]</span> to <span class="ltx_text" id="S4.T1.1.5.5.2.2" style="color:#0000FF;">[L2]</span>: [TGT]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.6.6.1">
<span class="ltx_text" id="S4.T1.1.6.6.1.1" style="color:#FF0000;">[L1]</span>: [SRC] = <span class="ltx_text" id="S4.T1.1.6.6.1.2" style="color:#0000FF;">[L2]</span>:</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.6.6.2">translate from <span class="ltx_text" id="S4.T1.1.6.6.2.1" style="color:#FF0000;">[L1]</span> to <span class="ltx_text" id="S4.T1.1.6.6.2.2" style="color:#0000FF;">[L2]</span>: [TGT]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.7.7.1">[SRC] translates into <span class="ltx_text" id="S4.T1.1.7.7.1.1" style="color:#0000FF;">[L2]</span> as:</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.7.7.2">translate from <span class="ltx_text" id="S4.T1.1.7.7.2.1" style="color:#FF0000;">[L1]</span> to <span class="ltx_text" id="S4.T1.1.7.7.2.2" style="color:#0000FF;">[L2]</span>: [TGT]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.8.8.1">How do you say [SRC] in <span class="ltx_text" id="S4.T1.1.8.8.1.1" style="color:#0000FF;">[L2]</span>?</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.8.8.2">translate from <span class="ltx_text" id="S4.T1.1.8.8.2.1" style="color:#FF0000;">[L1]</span> to <span class="ltx_text" id="S4.T1.1.8.8.2.2" style="color:#0000FF;">[L2]</span>: [TGT]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.9.9.1">[SRC] = <span class="ltx_text" id="S4.T1.1.9.9.1.1" style="color:#0000FF;">[L2]</span>:</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.9.9.2">translate from <span class="ltx_text" id="S4.T1.1.9.9.2.1" style="color:#FF0000;">[L1]</span> to <span class="ltx_text" id="S4.T1.1.9.9.2.2" style="color:#0000FF;">[L2]</span>: [TGT]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.10.10.1">Translate this from <span class="ltx_text" id="S4.T1.1.10.10.1.1" style="color:#FF0000;">[L1]</span> into <span class="ltx_text" id="S4.T1.1.10.10.1.2" style="color:#0000FF;">[L2]</span>: [SRC]</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.10.10.2">translate from <span class="ltx_text" id="S4.T1.1.10.10.2.1" style="color:#FF0000;">[L1]</span> to <span class="ltx_text" id="S4.T1.1.10.10.2.2" style="color:#0000FF;">[L2]</span>: [TGT]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.11.11.1">Translate this into <span class="ltx_text" id="S4.T1.1.11.11.1.1" style="color:#0000FF;">[L2]</span>: [SRC]</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.11.11.2">translate from <span class="ltx_text" id="S4.T1.1.11.11.2.1" style="color:#FF0000;">[L1]</span> to <span class="ltx_text" id="S4.T1.1.11.11.2.2" style="color:#0000FF;">[L2]</span>: [TGT]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.12.12.1">Given the following passage: [SRC], a good <span class="ltx_text" id="S4.T1.1.12.12.1.1" style="color:#0000FF;">[L2]</span> translation is:</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.12.12.2">translate from <span class="ltx_text" id="S4.T1.1.12.12.2.1" style="color:#FF0000;">[L1]</span> to <span class="ltx_text" id="S4.T1.1.12.12.2.2" style="color:#0000FF;">[L2]</span>: [TGT]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.13.13.1">
<span class="ltx_text" id="S4.T1.1.13.13.1.1" style="color:#FF0000;">[L1]</span>: [SRC] translates into <span class="ltx_text" id="S4.T1.1.13.13.1.2" style="color:#0000FF;">[L2]</span> as:</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.13.13.2">translate from <span class="ltx_text" id="S4.T1.1.13.13.2.1" style="color:#FF0000;">[L1]</span> to <span class="ltx_text" id="S4.T1.1.13.13.2.2" style="color:#0000FF;">[L2]</span>: [TGT]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.14.14.1">If the <span class="ltx_text" id="S4.T1.1.14.14.1.1" style="color:#FF0000;">[L1]</span> version says: [SRC]; then the <span class="ltx_text" id="S4.T1.1.14.14.1.2" style="color:#0000FF;">[L2]</span> version should say:</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.14.14.2">translate from <span class="ltx_text" id="S4.T1.1.14.14.2.1" style="color:#FF0000;">[L1]</span> to <span class="ltx_text" id="S4.T1.1.14.14.2.2" style="color:#0000FF;">[L2]</span>: [TGT]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.15.15.1">What is the <span class="ltx_text" id="S4.T1.1.15.15.1.1" style="color:#0000FF;">[L2]</span> translation of: [SRC]?</th>
<td class="ltx_td ltx_align_left" id="S4.T1.1.15.15.2">translate from <span class="ltx_text" id="S4.T1.1.15.15.2.1" style="color:#FF0000;">[L1]</span> to <span class="ltx_text" id="S4.T1.1.15.15.2.2" style="color:#0000FF;">[L2]</span>: [TGT]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.16.16.1">What is the <span class="ltx_text" id="S4.T1.1.16.16.1.1" style="color:#0000FF;">[L2]</span> translation of the <span class="ltx_text" id="S4.T1.1.16.16.1.2" style="color:#FF0000;">[L1]</span> sentence: [SRC]?</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.16.16.2">translate from <span class="ltx_text" id="S4.T1.1.16.16.2.1" style="color:#FF0000;">[L1]</span> to <span class="ltx_text" id="S4.T1.1.16.16.2.2" style="color:#0000FF;">[L2]</span>: [TGT]</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.1.Â Â Â Task-enhanced Constrained Turning</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The task-enhanced constrained turning pretends a translation task descriptive prefix to an encoder-decoder style or decoder-only transformer architecture to obtain LLM output <math alttext="z" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">z</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_z</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="z=[P_{des},y]," class="ltx_Math" display="block" id="S4.E2.m1.2"><semantics id="S4.E2.m1.2a"><mrow id="S4.E2.m1.2.2.1" xref="S4.E2.m1.2.2.1.1.cmml"><mrow id="S4.E2.m1.2.2.1.1" xref="S4.E2.m1.2.2.1.1.cmml"><mi id="S4.E2.m1.2.2.1.1.3" xref="S4.E2.m1.2.2.1.1.3.cmml">z</mi><mo id="S4.E2.m1.2.2.1.1.2" xref="S4.E2.m1.2.2.1.1.2.cmml">=</mo><mrow id="S4.E2.m1.2.2.1.1.1.1" xref="S4.E2.m1.2.2.1.1.1.2.cmml"><mo id="S4.E2.m1.2.2.1.1.1.1.2" stretchy="false" xref="S4.E2.m1.2.2.1.1.1.2.cmml">[</mo><msub id="S4.E2.m1.2.2.1.1.1.1.1" xref="S4.E2.m1.2.2.1.1.1.1.1.cmml"><mi id="S4.E2.m1.2.2.1.1.1.1.1.2" xref="S4.E2.m1.2.2.1.1.1.1.1.2.cmml">P</mi><mrow id="S4.E2.m1.2.2.1.1.1.1.1.3" xref="S4.E2.m1.2.2.1.1.1.1.1.3.cmml"><mi id="S4.E2.m1.2.2.1.1.1.1.1.3.2" xref="S4.E2.m1.2.2.1.1.1.1.1.3.2.cmml">d</mi><mo id="S4.E2.m1.2.2.1.1.1.1.1.3.1" xref="S4.E2.m1.2.2.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S4.E2.m1.2.2.1.1.1.1.1.3.3" xref="S4.E2.m1.2.2.1.1.1.1.1.3.3.cmml">e</mi><mo id="S4.E2.m1.2.2.1.1.1.1.1.3.1a" xref="S4.E2.m1.2.2.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S4.E2.m1.2.2.1.1.1.1.1.3.4" xref="S4.E2.m1.2.2.1.1.1.1.1.3.4.cmml">s</mi></mrow></msub><mo id="S4.E2.m1.2.2.1.1.1.1.3" xref="S4.E2.m1.2.2.1.1.1.2.cmml">,</mo><mi id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml">y</mi><mo id="S4.E2.m1.2.2.1.1.1.1.4" stretchy="false" xref="S4.E2.m1.2.2.1.1.1.2.cmml">]</mo></mrow></mrow><mo id="S4.E2.m1.2.2.1.2" xref="S4.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.2b"><apply id="S4.E2.m1.2.2.1.1.cmml" xref="S4.E2.m1.2.2.1"><eq id="S4.E2.m1.2.2.1.1.2.cmml" xref="S4.E2.m1.2.2.1.1.2"></eq><ci id="S4.E2.m1.2.2.1.1.3.cmml" xref="S4.E2.m1.2.2.1.1.3">ğ‘§</ci><interval closure="closed" id="S4.E2.m1.2.2.1.1.1.2.cmml" xref="S4.E2.m1.2.2.1.1.1.1"><apply id="S4.E2.m1.2.2.1.1.1.1.1.cmml" xref="S4.E2.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S4.E2.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S4.E2.m1.2.2.1.1.1.1.1.2.cmml" xref="S4.E2.m1.2.2.1.1.1.1.1.2">ğ‘ƒ</ci><apply id="S4.E2.m1.2.2.1.1.1.1.1.3.cmml" xref="S4.E2.m1.2.2.1.1.1.1.1.3"><times id="S4.E2.m1.2.2.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.2.2.1.1.1.1.1.3.1"></times><ci id="S4.E2.m1.2.2.1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.2.2.1.1.1.1.1.3.2">ğ‘‘</ci><ci id="S4.E2.m1.2.2.1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.2.2.1.1.1.1.1.3.3">ğ‘’</ci><ci id="S4.E2.m1.2.2.1.1.1.1.1.3.4.cmml" xref="S4.E2.m1.2.2.1.1.1.1.1.3.4">ğ‘ </ci></apply></apply><ci id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1">ğ‘¦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.2c">z=[P_{des},y],</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.2d">italic_z = [ italic_P start_POSTSUBSCRIPT italic_d italic_e italic_s end_POSTSUBSCRIPT , italic_y ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.p1.5">where <math alttext="[\cdot,\cdot]" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m1.2"><semantics id="S4.SS1.p1.2.m1.2a"><mrow id="S4.SS1.p1.2.m1.2.3.2" xref="S4.SS1.p1.2.m1.2.3.1.cmml"><mo id="S4.SS1.p1.2.m1.2.3.2.1" stretchy="false" xref="S4.SS1.p1.2.m1.2.3.1.cmml">[</mo><mo id="S4.SS1.p1.2.m1.1.1" lspace="0em" rspace="0em" xref="S4.SS1.p1.2.m1.1.1.cmml">â‹…</mo><mo id="S4.SS1.p1.2.m1.2.3.2.2" rspace="0em" xref="S4.SS1.p1.2.m1.2.3.1.cmml">,</mo><mo id="S4.SS1.p1.2.m1.2.2" lspace="0em" rspace="0em" xref="S4.SS1.p1.2.m1.2.2.cmml">â‹…</mo><mo id="S4.SS1.p1.2.m1.2.3.2.3" stretchy="false" xref="S4.SS1.p1.2.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m1.2b"><interval closure="closed" id="S4.SS1.p1.2.m1.2.3.1.cmml" xref="S4.SS1.p1.2.m1.2.3.2"><ci id="S4.SS1.p1.2.m1.1.1.cmml" xref="S4.SS1.p1.2.m1.1.1">â‹…</ci><ci id="S4.SS1.p1.2.m1.2.2.cmml" xref="S4.SS1.p1.2.m1.2.2">â‹…</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m1.2c">[\cdot,\cdot]</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m1.2d">[ â‹… , â‹… ]</annotation></semantics></math> denotes the contact operation. Here, <math alttext="P_{des}" class="ltx_Math" display="inline" id="S4.SS1.p1.3.m2.1"><semantics id="S4.SS1.p1.3.m2.1a"><msub id="S4.SS1.p1.3.m2.1.1" xref="S4.SS1.p1.3.m2.1.1.cmml"><mi id="S4.SS1.p1.3.m2.1.1.2" xref="S4.SS1.p1.3.m2.1.1.2.cmml">P</mi><mrow id="S4.SS1.p1.3.m2.1.1.3" xref="S4.SS1.p1.3.m2.1.1.3.cmml"><mi id="S4.SS1.p1.3.m2.1.1.3.2" xref="S4.SS1.p1.3.m2.1.1.3.2.cmml">d</mi><mo id="S4.SS1.p1.3.m2.1.1.3.1" xref="S4.SS1.p1.3.m2.1.1.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.3.m2.1.1.3.3" xref="S4.SS1.p1.3.m2.1.1.3.3.cmml">e</mi><mo id="S4.SS1.p1.3.m2.1.1.3.1a" xref="S4.SS1.p1.3.m2.1.1.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.3.m2.1.1.3.4" xref="S4.SS1.p1.3.m2.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m2.1b"><apply id="S4.SS1.p1.3.m2.1.1.cmml" xref="S4.SS1.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m2.1.1.1.cmml" xref="S4.SS1.p1.3.m2.1.1">subscript</csymbol><ci id="S4.SS1.p1.3.m2.1.1.2.cmml" xref="S4.SS1.p1.3.m2.1.1.2">ğ‘ƒ</ci><apply id="S4.SS1.p1.3.m2.1.1.3.cmml" xref="S4.SS1.p1.3.m2.1.1.3"><times id="S4.SS1.p1.3.m2.1.1.3.1.cmml" xref="S4.SS1.p1.3.m2.1.1.3.1"></times><ci id="S4.SS1.p1.3.m2.1.1.3.2.cmml" xref="S4.SS1.p1.3.m2.1.1.3.2">ğ‘‘</ci><ci id="S4.SS1.p1.3.m2.1.1.3.3.cmml" xref="S4.SS1.p1.3.m2.1.1.3.3">ğ‘’</ci><ci id="S4.SS1.p1.3.m2.1.1.3.4.cmml" xref="S4.SS1.p1.3.m2.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m2.1c">P_{des}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.3.m2.1d">italic_P start_POSTSUBSCRIPT italic_d italic_e italic_s end_POSTSUBSCRIPT</annotation></semantics></math> denotes a sequence of task descriptive prefix (<span class="ltx_text ltx_font_italic" id="S4.SS1.p1.5.1">e</span>.<span class="ltx_text ltx_font_italic" id="S4.SS1.p1.5.2">g</span>.Â , <math alttext="P_{des}" class="ltx_Math" display="inline" id="S4.SS1.p1.4.m3.1"><semantics id="S4.SS1.p1.4.m3.1a"><msub id="S4.SS1.p1.4.m3.1.1" xref="S4.SS1.p1.4.m3.1.1.cmml"><mi id="S4.SS1.p1.4.m3.1.1.2" xref="S4.SS1.p1.4.m3.1.1.2.cmml">P</mi><mrow id="S4.SS1.p1.4.m3.1.1.3" xref="S4.SS1.p1.4.m3.1.1.3.cmml"><mi id="S4.SS1.p1.4.m3.1.1.3.2" xref="S4.SS1.p1.4.m3.1.1.3.2.cmml">d</mi><mo id="S4.SS1.p1.4.m3.1.1.3.1" xref="S4.SS1.p1.4.m3.1.1.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.4.m3.1.1.3.3" xref="S4.SS1.p1.4.m3.1.1.3.3.cmml">e</mi><mo id="S4.SS1.p1.4.m3.1.1.3.1a" xref="S4.SS1.p1.4.m3.1.1.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.4.m3.1.1.3.4" xref="S4.SS1.p1.4.m3.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m3.1b"><apply id="S4.SS1.p1.4.m3.1.1.cmml" xref="S4.SS1.p1.4.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m3.1.1.1.cmml" xref="S4.SS1.p1.4.m3.1.1">subscript</csymbol><ci id="S4.SS1.p1.4.m3.1.1.2.cmml" xref="S4.SS1.p1.4.m3.1.1.2">ğ‘ƒ</ci><apply id="S4.SS1.p1.4.m3.1.1.3.cmml" xref="S4.SS1.p1.4.m3.1.1.3"><times id="S4.SS1.p1.4.m3.1.1.3.1.cmml" xref="S4.SS1.p1.4.m3.1.1.3.1"></times><ci id="S4.SS1.p1.4.m3.1.1.3.2.cmml" xref="S4.SS1.p1.4.m3.1.1.3.2">ğ‘‘</ci><ci id="S4.SS1.p1.4.m3.1.1.3.3.cmml" xref="S4.SS1.p1.4.m3.1.1.3.3">ğ‘’</ci><ci id="S4.SS1.p1.4.m3.1.1.3.4.cmml" xref="S4.SS1.p1.4.m3.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m3.1c">P_{des}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.4.m3.1d">italic_P start_POSTSUBSCRIPT italic_d italic_e italic_s end_POSTSUBSCRIPT</annotation></semantics></math>=translate from French to German:).
Since each token in the prefix is in the vocabulary, we can directly initialize the <math alttext="P_{des}" class="ltx_Math" display="inline" id="S4.SS1.p1.5.m4.1"><semantics id="S4.SS1.p1.5.m4.1a"><msub id="S4.SS1.p1.5.m4.1.1" xref="S4.SS1.p1.5.m4.1.1.cmml"><mi id="S4.SS1.p1.5.m4.1.1.2" xref="S4.SS1.p1.5.m4.1.1.2.cmml">P</mi><mrow id="S4.SS1.p1.5.m4.1.1.3" xref="S4.SS1.p1.5.m4.1.1.3.cmml"><mi id="S4.SS1.p1.5.m4.1.1.3.2" xref="S4.SS1.p1.5.m4.1.1.3.2.cmml">d</mi><mo id="S4.SS1.p1.5.m4.1.1.3.1" xref="S4.SS1.p1.5.m4.1.1.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.5.m4.1.1.3.3" xref="S4.SS1.p1.5.m4.1.1.3.3.cmml">e</mi><mo id="S4.SS1.p1.5.m4.1.1.3.1a" xref="S4.SS1.p1.5.m4.1.1.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.5.m4.1.1.3.4" xref="S4.SS1.p1.5.m4.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m4.1b"><apply id="S4.SS1.p1.5.m4.1.1.cmml" xref="S4.SS1.p1.5.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.5.m4.1.1.1.cmml" xref="S4.SS1.p1.5.m4.1.1">subscript</csymbol><ci id="S4.SS1.p1.5.m4.1.1.2.cmml" xref="S4.SS1.p1.5.m4.1.1.2">ğ‘ƒ</ci><apply id="S4.SS1.p1.5.m4.1.1.3.cmml" xref="S4.SS1.p1.5.m4.1.1.3"><times id="S4.SS1.p1.5.m4.1.1.3.1.cmml" xref="S4.SS1.p1.5.m4.1.1.3.1"></times><ci id="S4.SS1.p1.5.m4.1.1.3.2.cmml" xref="S4.SS1.p1.5.m4.1.1.3.2">ğ‘‘</ci><ci id="S4.SS1.p1.5.m4.1.1.3.3.cmml" xref="S4.SS1.p1.5.m4.1.1.3.3">ğ‘’</ci><ci id="S4.SS1.p1.5.m4.1.1.3.4.cmml" xref="S4.SS1.p1.5.m4.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m4.1c">P_{des}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.5.m4.1d">italic_P start_POSTSUBSCRIPT italic_d italic_e italic_s end_POSTSUBSCRIPT</annotation></semantics></math> based on the pre-trained weight matrix of LLMâ€™s vocabulary.
TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S4.T1" title="Table 1 â€£ 4. Methods â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, we provide a detailed list of the input/output format for instruction fine-turning.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.2.Â Â Â Auto-constriction Turning</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Different from task-enhanced constrained turning (TECT-MNMT), auto-constriction turning first generates the trigger tokes related to translation direction, and then generates the ground truth <math alttext="y" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">y</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_y</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}=\sum_{(x,y)\sim D}\sum_{i\leq|z|}-\log p(z_{i}|x,z_{&lt;i};\theta)," class="ltx_Math" display="block" id="S4.E3.m1.6"><semantics id="S4.E3.m1.6a"><mrow id="S4.E3.m1.6.6.1" xref="S4.E3.m1.6.6.1.1.cmml"><mrow id="S4.E3.m1.6.6.1.1" xref="S4.E3.m1.6.6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.6.6.1.1.3" xref="S4.E3.m1.6.6.1.1.3.cmml">â„’</mi><mo id="S4.E3.m1.6.6.1.1.2" rspace="0.111em" xref="S4.E3.m1.6.6.1.1.2.cmml">=</mo><mrow id="S4.E3.m1.6.6.1.1.1" xref="S4.E3.m1.6.6.1.1.1.cmml"><mrow id="S4.E3.m1.6.6.1.1.1.3" xref="S4.E3.m1.6.6.1.1.1.3.cmml"><munder id="S4.E3.m1.6.6.1.1.1.3.1" xref="S4.E3.m1.6.6.1.1.1.3.1.cmml"><mo id="S4.E3.m1.6.6.1.1.1.3.1.2" movablelimits="false" rspace="0em" xref="S4.E3.m1.6.6.1.1.1.3.1.2.cmml">âˆ‘</mo><mrow id="S4.E3.m1.2.2.2" xref="S4.E3.m1.2.2.2.cmml"><mrow id="S4.E3.m1.2.2.2.4.2" xref="S4.E3.m1.2.2.2.4.1.cmml"><mo id="S4.E3.m1.2.2.2.4.2.1" stretchy="false" xref="S4.E3.m1.2.2.2.4.1.cmml">(</mo><mi id="S4.E3.m1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.cmml">x</mi><mo id="S4.E3.m1.2.2.2.4.2.2" xref="S4.E3.m1.2.2.2.4.1.cmml">,</mo><mi id="S4.E3.m1.2.2.2.2" xref="S4.E3.m1.2.2.2.2.cmml">y</mi><mo id="S4.E3.m1.2.2.2.4.2.3" stretchy="false" xref="S4.E3.m1.2.2.2.4.1.cmml">)</mo></mrow><mo id="S4.E3.m1.2.2.2.3" xref="S4.E3.m1.2.2.2.3.cmml">âˆ¼</mo><mi id="S4.E3.m1.2.2.2.5" xref="S4.E3.m1.2.2.2.5.cmml">D</mi></mrow></munder><munder id="S4.E3.m1.6.6.1.1.1.3.2" xref="S4.E3.m1.6.6.1.1.1.3.2.cmml"><mo id="S4.E3.m1.6.6.1.1.1.3.2.2" movablelimits="false" rspace="0em" xref="S4.E3.m1.6.6.1.1.1.3.2.2.cmml">âˆ‘</mo><mrow id="S4.E3.m1.3.3.1" xref="S4.E3.m1.3.3.1.cmml"><mi id="S4.E3.m1.3.3.1.3" xref="S4.E3.m1.3.3.1.3.cmml">i</mi><mo id="S4.E3.m1.3.3.1.2" xref="S4.E3.m1.3.3.1.2.cmml">â‰¤</mo><mrow id="S4.E3.m1.3.3.1.4.2" xref="S4.E3.m1.3.3.1.4.1.cmml"><mo id="S4.E3.m1.3.3.1.4.2.1" stretchy="false" xref="S4.E3.m1.3.3.1.4.1.1.cmml">|</mo><mi id="S4.E3.m1.3.3.1.1" xref="S4.E3.m1.3.3.1.1.cmml">z</mi><mo id="S4.E3.m1.3.3.1.4.2.2" stretchy="false" xref="S4.E3.m1.3.3.1.4.1.1.cmml">|</mo></mrow></mrow></munder></mrow><mo id="S4.E3.m1.6.6.1.1.1.2" lspace="0em" xref="S4.E3.m1.6.6.1.1.1.2.cmml">âˆ’</mo><mrow id="S4.E3.m1.6.6.1.1.1.1" xref="S4.E3.m1.6.6.1.1.1.1.cmml"><mrow id="S4.E3.m1.6.6.1.1.1.1.3" xref="S4.E3.m1.6.6.1.1.1.1.3.cmml"><mi id="S4.E3.m1.6.6.1.1.1.1.3.1" xref="S4.E3.m1.6.6.1.1.1.1.3.1.cmml">log</mi><mo id="S4.E3.m1.6.6.1.1.1.1.3a" lspace="0.167em" xref="S4.E3.m1.6.6.1.1.1.1.3.cmml">â¡</mo><mi id="S4.E3.m1.6.6.1.1.1.1.3.2" xref="S4.E3.m1.6.6.1.1.1.1.3.2.cmml">p</mi></mrow><mo id="S4.E3.m1.6.6.1.1.1.1.2" xref="S4.E3.m1.6.6.1.1.1.1.2.cmml">â¢</mo><mrow id="S4.E3.m1.6.6.1.1.1.1.1.1" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.cmml"><mo id="S4.E3.m1.6.6.1.1.1.1.1.1.2" stretchy="false" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.6.6.1.1.1.1.1.1.1" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.cmml"><msub id="S4.E3.m1.6.6.1.1.1.1.1.1.1.3" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E3.m1.6.6.1.1.1.1.1.1.1.3.2" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.3.2.cmml">z</mi><mi id="S4.E3.m1.6.6.1.1.1.1.1.1.1.3.3" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S4.E3.m1.6.6.1.1.1.1.1.1.1.2" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E3.m1.4.4" xref="S4.E3.m1.4.4.cmml">x</mi><mo id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.2.cmml">,</mo><msub id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.cmml">z</mi><mrow id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.2.cmml">;</mo><mi id="S4.E3.m1.5.5" xref="S4.E3.m1.5.5.cmml">Î¸</mi></mrow></mrow><mo id="S4.E3.m1.6.6.1.1.1.1.1.1.3" stretchy="false" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S4.E3.m1.6.6.1.2" xref="S4.E3.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.6b"><apply id="S4.E3.m1.6.6.1.1.cmml" xref="S4.E3.m1.6.6.1"><eq id="S4.E3.m1.6.6.1.1.2.cmml" xref="S4.E3.m1.6.6.1.1.2"></eq><ci id="S4.E3.m1.6.6.1.1.3.cmml" xref="S4.E3.m1.6.6.1.1.3">â„’</ci><apply id="S4.E3.m1.6.6.1.1.1.cmml" xref="S4.E3.m1.6.6.1.1.1"><minus id="S4.E3.m1.6.6.1.1.1.2.cmml" xref="S4.E3.m1.6.6.1.1.1.2"></minus><apply id="S4.E3.m1.6.6.1.1.1.3.cmml" xref="S4.E3.m1.6.6.1.1.1.3"><apply id="S4.E3.m1.6.6.1.1.1.3.1.cmml" xref="S4.E3.m1.6.6.1.1.1.3.1"><csymbol cd="ambiguous" id="S4.E3.m1.6.6.1.1.1.3.1.1.cmml" xref="S4.E3.m1.6.6.1.1.1.3.1">subscript</csymbol><sum id="S4.E3.m1.6.6.1.1.1.3.1.2.cmml" xref="S4.E3.m1.6.6.1.1.1.3.1.2"></sum><apply id="S4.E3.m1.2.2.2.cmml" xref="S4.E3.m1.2.2.2"><csymbol cd="latexml" id="S4.E3.m1.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.3">similar-to</csymbol><interval closure="open" id="S4.E3.m1.2.2.2.4.1.cmml" xref="S4.E3.m1.2.2.2.4.2"><ci id="S4.E3.m1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1">ğ‘¥</ci><ci id="S4.E3.m1.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2">ğ‘¦</ci></interval><ci id="S4.E3.m1.2.2.2.5.cmml" xref="S4.E3.m1.2.2.2.5">ğ·</ci></apply></apply><apply id="S4.E3.m1.6.6.1.1.1.3.2.cmml" xref="S4.E3.m1.6.6.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E3.m1.6.6.1.1.1.3.2.1.cmml" xref="S4.E3.m1.6.6.1.1.1.3.2">subscript</csymbol><sum id="S4.E3.m1.6.6.1.1.1.3.2.2.cmml" xref="S4.E3.m1.6.6.1.1.1.3.2.2"></sum><apply id="S4.E3.m1.3.3.1.cmml" xref="S4.E3.m1.3.3.1"><leq id="S4.E3.m1.3.3.1.2.cmml" xref="S4.E3.m1.3.3.1.2"></leq><ci id="S4.E3.m1.3.3.1.3.cmml" xref="S4.E3.m1.3.3.1.3">ğ‘–</ci><apply id="S4.E3.m1.3.3.1.4.1.cmml" xref="S4.E3.m1.3.3.1.4.2"><abs id="S4.E3.m1.3.3.1.4.1.1.cmml" xref="S4.E3.m1.3.3.1.4.2.1"></abs><ci id="S4.E3.m1.3.3.1.1.cmml" xref="S4.E3.m1.3.3.1.1">ğ‘§</ci></apply></apply></apply></apply><apply id="S4.E3.m1.6.6.1.1.1.1.cmml" xref="S4.E3.m1.6.6.1.1.1.1"><times id="S4.E3.m1.6.6.1.1.1.1.2.cmml" xref="S4.E3.m1.6.6.1.1.1.1.2"></times><apply id="S4.E3.m1.6.6.1.1.1.1.3.cmml" xref="S4.E3.m1.6.6.1.1.1.1.3"><log id="S4.E3.m1.6.6.1.1.1.1.3.1.cmml" xref="S4.E3.m1.6.6.1.1.1.1.3.1"></log><ci id="S4.E3.m1.6.6.1.1.1.1.3.2.cmml" xref="S4.E3.m1.6.6.1.1.1.1.3.2">ğ‘</ci></apply><apply id="S4.E3.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.6.6.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E3.m1.6.6.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="S4.E3.m1.6.6.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.6.6.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E3.m1.6.6.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.3.2">ğ‘§</ci><ci id="S4.E3.m1.6.6.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply><list id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1"><ci id="S4.E3.m1.4.4.cmml" xref="S4.E3.m1.4.4">ğ‘¥</ci><apply id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.2">ğ‘§</ci><apply id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.3"><lt id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E3.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply><ci id="S4.E3.m1.5.5.cmml" xref="S4.E3.m1.5.5">ğœƒ</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.6c">\mathcal{L}=\sum_{(x,y)\sim D}\sum_{i\leq|z|}-\log p(z_{i}|x,z_{&lt;i};\theta),</annotation><annotation encoding="application/x-llamapun" id="S4.E3.m1.6d">caligraphic_L = âˆ‘ start_POSTSUBSCRIPT ( italic_x , italic_y ) âˆ¼ italic_D end_POSTSUBSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_i â‰¤ | italic_z | end_POSTSUBSCRIPT - roman_log italic_p ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x , italic_z start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT ; italic_Î¸ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.p1.3">where <math alttext="z=[t(x),y]" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m1.3"><semantics id="S4.SS2.p1.2.m1.3a"><mrow id="S4.SS2.p1.2.m1.3.3" xref="S4.SS2.p1.2.m1.3.3.cmml"><mi id="S4.SS2.p1.2.m1.3.3.3" xref="S4.SS2.p1.2.m1.3.3.3.cmml">z</mi><mo id="S4.SS2.p1.2.m1.3.3.2" xref="S4.SS2.p1.2.m1.3.3.2.cmml">=</mo><mrow id="S4.SS2.p1.2.m1.3.3.1.1" xref="S4.SS2.p1.2.m1.3.3.1.2.cmml"><mo id="S4.SS2.p1.2.m1.3.3.1.1.2" stretchy="false" xref="S4.SS2.p1.2.m1.3.3.1.2.cmml">[</mo><mrow id="S4.SS2.p1.2.m1.3.3.1.1.1" xref="S4.SS2.p1.2.m1.3.3.1.1.1.cmml"><mi id="S4.SS2.p1.2.m1.3.3.1.1.1.2" xref="S4.SS2.p1.2.m1.3.3.1.1.1.2.cmml">t</mi><mo id="S4.SS2.p1.2.m1.3.3.1.1.1.1" xref="S4.SS2.p1.2.m1.3.3.1.1.1.1.cmml">â¢</mo><mrow id="S4.SS2.p1.2.m1.3.3.1.1.1.3.2" xref="S4.SS2.p1.2.m1.3.3.1.1.1.cmml"><mo id="S4.SS2.p1.2.m1.3.3.1.1.1.3.2.1" stretchy="false" xref="S4.SS2.p1.2.m1.3.3.1.1.1.cmml">(</mo><mi id="S4.SS2.p1.2.m1.1.1" xref="S4.SS2.p1.2.m1.1.1.cmml">x</mi><mo id="S4.SS2.p1.2.m1.3.3.1.1.1.3.2.2" stretchy="false" xref="S4.SS2.p1.2.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p1.2.m1.3.3.1.1.3" xref="S4.SS2.p1.2.m1.3.3.1.2.cmml">,</mo><mi id="S4.SS2.p1.2.m1.2.2" xref="S4.SS2.p1.2.m1.2.2.cmml">y</mi><mo id="S4.SS2.p1.2.m1.3.3.1.1.4" stretchy="false" xref="S4.SS2.p1.2.m1.3.3.1.2.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m1.3b"><apply id="S4.SS2.p1.2.m1.3.3.cmml" xref="S4.SS2.p1.2.m1.3.3"><eq id="S4.SS2.p1.2.m1.3.3.2.cmml" xref="S4.SS2.p1.2.m1.3.3.2"></eq><ci id="S4.SS2.p1.2.m1.3.3.3.cmml" xref="S4.SS2.p1.2.m1.3.3.3">ğ‘§</ci><interval closure="closed" id="S4.SS2.p1.2.m1.3.3.1.2.cmml" xref="S4.SS2.p1.2.m1.3.3.1.1"><apply id="S4.SS2.p1.2.m1.3.3.1.1.1.cmml" xref="S4.SS2.p1.2.m1.3.3.1.1.1"><times id="S4.SS2.p1.2.m1.3.3.1.1.1.1.cmml" xref="S4.SS2.p1.2.m1.3.3.1.1.1.1"></times><ci id="S4.SS2.p1.2.m1.3.3.1.1.1.2.cmml" xref="S4.SS2.p1.2.m1.3.3.1.1.1.2">ğ‘¡</ci><ci id="S4.SS2.p1.2.m1.1.1.cmml" xref="S4.SS2.p1.2.m1.1.1">ğ‘¥</ci></apply><ci id="S4.SS2.p1.2.m1.2.2.cmml" xref="S4.SS2.p1.2.m1.2.2">ğ‘¦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m1.3c">z=[t(x),y]</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m1.3d">italic_z = [ italic_t ( italic_x ) , italic_y ]</annotation></semantics></math>, and <math alttext="t(\cdot)" class="ltx_Math" display="inline" id="S4.SS2.p1.3.m2.1"><semantics id="S4.SS2.p1.3.m2.1a"><mrow id="S4.SS2.p1.3.m2.1.2" xref="S4.SS2.p1.3.m2.1.2.cmml"><mi id="S4.SS2.p1.3.m2.1.2.2" xref="S4.SS2.p1.3.m2.1.2.2.cmml">t</mi><mo id="S4.SS2.p1.3.m2.1.2.1" xref="S4.SS2.p1.3.m2.1.2.1.cmml">â¢</mo><mrow id="S4.SS2.p1.3.m2.1.2.3.2" xref="S4.SS2.p1.3.m2.1.2.cmml"><mo id="S4.SS2.p1.3.m2.1.2.3.2.1" stretchy="false" xref="S4.SS2.p1.3.m2.1.2.cmml">(</mo><mo id="S4.SS2.p1.3.m2.1.1" lspace="0em" rspace="0em" xref="S4.SS2.p1.3.m2.1.1.cmml">â‹…</mo><mo id="S4.SS2.p1.3.m2.1.2.3.2.2" stretchy="false" xref="S4.SS2.p1.3.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m2.1b"><apply id="S4.SS2.p1.3.m2.1.2.cmml" xref="S4.SS2.p1.3.m2.1.2"><times id="S4.SS2.p1.3.m2.1.2.1.cmml" xref="S4.SS2.p1.3.m2.1.2.1"></times><ci id="S4.SS2.p1.3.m2.1.2.2.cmml" xref="S4.SS2.p1.3.m2.1.2.2">ğ‘¡</ci><ci id="S4.SS2.p1.3.m2.1.1.cmml" xref="S4.SS2.p1.3.m2.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m2.1c">t(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.3.m2.1d">italic_t ( â‹… )</annotation></semantics></math> represents a mapping strategy that establishes an association between trigger token sequences and different translation directions.
Specifically, for translation task with different translation directions, we design two types of trigger tokens: <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.3.1">common trigger token</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.3.2">specific trigger token</span>. The common trigger token is designed to learn shared information in different translation directions, and the specific trigger token is to capture the target language information.
Therefore, each translation direction has common trigger tokens, and only the translation directions with the same target language have the same specific trigger token.
Both types of trigger tokens help the model understand instructions.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Trigger tokens are special tokens added to the vocabulary during fine-turning, which can also be seen as additional supervised information to determine whether the model understands the instructions.
Due to trigger tokens are added as special tokens, we can easily remove these trigger tokens during decoding.
We should note that the trigger tokens are generated on the target side. Therefore, the added trigger tokens can be combined and arranged arbitrarily, allowing them to represent even unseen semantic information during the fine-tuning stage.
As a result, auto-constriction turning performs well under zero-shot setting.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.Â Â Â Experiments</h2>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>The basic statistics of training&amp;validation datasets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.16">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.16.17.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.16.17.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">Lang-Pair</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.16.17.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">Training</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.16.17.1.3" style="padding-left:4.3pt;padding-right:4.3pt;">Validation</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.2.2.3" style="padding-left:4.3pt;padding-right:4.3pt;">EN-CS</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">2,000<math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><times id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.m1.1d">Ã—</annotation></semantics></math>2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">2,000<math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.2.2.2.m1.1"><semantics id="S5.T2.2.2.2.m1.1a"><mo id="S5.T2.2.2.2.m1.1.1" xref="S5.T2.2.2.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.m1.1b"><times id="S5.T2.2.2.2.m1.1.1.cmml" xref="S5.T2.2.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.m1.1d">Ã—</annotation></semantics></math>2</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T2.4.4.3" style="padding-left:4.3pt;padding-right:4.3pt;">EN-DE</th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.3.1" style="padding-left:4.3pt;padding-right:4.3pt;">2,000<math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.3.3.1.m1.1"><semantics id="S5.T2.3.3.1.m1.1a"><mo id="S5.T2.3.3.1.m1.1.1" xref="S5.T2.3.3.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.1.m1.1b"><times id="S5.T2.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.1.m1.1d">Ã—</annotation></semantics></math>2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.4.2" style="padding-left:4.3pt;padding-right:4.3pt;">2,000<math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.4.4.2.m1.1"><semantics id="S5.T2.4.4.2.m1.1a"><mo id="S5.T2.4.4.2.m1.1.1" xref="S5.T2.4.4.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.2.m1.1b"><times id="S5.T2.4.4.2.m1.1.1.cmml" xref="S5.T2.4.4.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.2.m1.1d">Ã—</annotation></semantics></math>2</td>
</tr>
<tr class="ltx_tr" id="S5.T2.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T2.6.6.3" style="padding-left:4.3pt;padding-right:4.3pt;">EN-FR</th>
<td class="ltx_td ltx_align_center" id="S5.T2.5.5.1" style="padding-left:4.3pt;padding-right:4.3pt;">2,000<math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.5.5.1.m1.1"><semantics id="S5.T2.5.5.1.m1.1a"><mo id="S5.T2.5.5.1.m1.1.1" xref="S5.T2.5.5.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.1.m1.1b"><times id="S5.T2.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.5.1.m1.1d">Ã—</annotation></semantics></math>2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.6.6.2" style="padding-left:4.3pt;padding-right:4.3pt;">2,000<math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.6.6.2.m1.1"><semantics id="S5.T2.6.6.2.m1.1a"><mo id="S5.T2.6.6.2.m1.1.1" xref="S5.T2.6.6.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.2.m1.1b"><times id="S5.T2.6.6.2.m1.1.1.cmml" xref="S5.T2.6.6.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.6.6.2.m1.1d">Ã—</annotation></semantics></math>2</td>
</tr>
<tr class="ltx_tr" id="S5.T2.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T2.8.8.3" style="padding-left:4.3pt;padding-right:4.3pt;">EN-ZH</th>
<td class="ltx_td ltx_align_center" id="S5.T2.7.7.1" style="padding-left:4.3pt;padding-right:4.3pt;">2,000<math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.7.7.1.m1.1"><semantics id="S5.T2.7.7.1.m1.1a"><mo id="S5.T2.7.7.1.m1.1.1" xref="S5.T2.7.7.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.7.7.1.m1.1b"><times id="S5.T2.7.7.1.m1.1.1.cmml" xref="S5.T2.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.7.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.7.7.1.m1.1d">Ã—</annotation></semantics></math>2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.8.8.2" style="padding-left:4.3pt;padding-right:4.3pt;">2,000<math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.8.8.2.m1.1"><semantics id="S5.T2.8.8.2.m1.1a"><mo id="S5.T2.8.8.2.m1.1.1" xref="S5.T2.8.8.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.8.8.2.m1.1b"><times id="S5.T2.8.8.2.m1.1.1.cmml" xref="S5.T2.8.8.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.8.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.8.8.2.m1.1d">Ã—</annotation></semantics></math>2</td>
</tr>
<tr class="ltx_tr" id="S5.T2.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T2.10.10.3" style="padding-left:4.3pt;padding-right:4.3pt;">EN-RU</th>
<td class="ltx_td ltx_align_center" id="S5.T2.9.9.1" style="padding-left:4.3pt;padding-right:4.3pt;">2,000<math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.9.9.1.m1.1"><semantics id="S5.T2.9.9.1.m1.1a"><mo id="S5.T2.9.9.1.m1.1.1" xref="S5.T2.9.9.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.9.9.1.m1.1b"><times id="S5.T2.9.9.1.m1.1.1.cmml" xref="S5.T2.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.9.9.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.9.9.1.m1.1d">Ã—</annotation></semantics></math>2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.10.10.2" style="padding-left:4.3pt;padding-right:4.3pt;">2,000<math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.10.10.2.m1.1"><semantics id="S5.T2.10.10.2.m1.1a"><mo id="S5.T2.10.10.2.m1.1.1" xref="S5.T2.10.10.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.10.10.2.m1.1b"><times id="S5.T2.10.10.2.m1.1.1.cmml" xref="S5.T2.10.10.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.10.10.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.10.10.2.m1.1d">Ã—</annotation></semantics></math>2</td>
</tr>
<tr class="ltx_tr" id="S5.T2.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T2.12.12.3" style="padding-left:4.3pt;padding-right:4.3pt;">EN-RO</th>
<td class="ltx_td ltx_align_center" id="S5.T2.11.11.1" style="padding-left:4.3pt;padding-right:4.3pt;">2,000<math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.11.11.1.m1.1"><semantics id="S5.T2.11.11.1.m1.1a"><mo id="S5.T2.11.11.1.m1.1.1" xref="S5.T2.11.11.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.11.11.1.m1.1b"><times id="S5.T2.11.11.1.m1.1.1.cmml" xref="S5.T2.11.11.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.11.11.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.11.11.1.m1.1d">Ã—</annotation></semantics></math>2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.12.12.2" style="padding-left:4.3pt;padding-right:4.3pt;">2,000<math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.12.12.2.m1.1"><semantics id="S5.T2.12.12.2.m1.1a"><mo id="S5.T2.12.12.2.m1.1.1" xref="S5.T2.12.12.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.12.12.2.m1.1b"><times id="S5.T2.12.12.2.m1.1.1.cmml" xref="S5.T2.12.12.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.12.12.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.12.12.2.m1.1d">Ã—</annotation></semantics></math>2</td>
</tr>
<tr class="ltx_tr" id="S5.T2.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T2.14.14.3" style="padding-left:4.3pt;padding-right:4.3pt;">EN-UK</th>
<td class="ltx_td ltx_align_center" id="S5.T2.13.13.1" style="padding-left:4.3pt;padding-right:4.3pt;">2,000<math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.13.13.1.m1.1"><semantics id="S5.T2.13.13.1.m1.1a"><mo id="S5.T2.13.13.1.m1.1.1" xref="S5.T2.13.13.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.13.13.1.m1.1b"><times id="S5.T2.13.13.1.m1.1.1.cmml" xref="S5.T2.13.13.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.13.13.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.13.13.1.m1.1d">Ã—</annotation></semantics></math>2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.14.14.2" style="padding-left:4.3pt;padding-right:4.3pt;">2,000<math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.14.14.2.m1.1"><semantics id="S5.T2.14.14.2.m1.1a"><mo id="S5.T2.14.14.2.m1.1.1" xref="S5.T2.14.14.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.14.14.2.m1.1b"><times id="S5.T2.14.14.2.m1.1.1.cmml" xref="S5.T2.14.14.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.14.14.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.14.14.2.m1.1d">Ã—</annotation></semantics></math>2</td>
</tr>
<tr class="ltx_tr" id="S5.T2.16.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T2.16.16.3" style="padding-left:4.3pt;padding-right:4.3pt;">EN-HI</th>
<td class="ltx_td ltx_align_center" id="S5.T2.15.15.1" style="padding-left:4.3pt;padding-right:4.3pt;">2,000<math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.15.15.1.m1.1"><semantics id="S5.T2.15.15.1.m1.1a"><mo id="S5.T2.15.15.1.m1.1.1" xref="S5.T2.15.15.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.15.15.1.m1.1b"><times id="S5.T2.15.15.1.m1.1.1.cmml" xref="S5.T2.15.15.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.15.15.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.15.15.1.m1.1d">Ã—</annotation></semantics></math>2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.16.16.2" style="padding-left:4.3pt;padding-right:4.3pt;">2,000<math alttext="\times" class="ltx_Math" display="inline" id="S5.T2.16.16.2.m1.1"><semantics id="S5.T2.16.16.2.m1.1a"><mo id="S5.T2.16.16.2.m1.1.1" xref="S5.T2.16.16.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.16.16.2.m1.1b"><times id="S5.T2.16.16.2.m1.1.1.cmml" xref="S5.T2.16.16.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.16.16.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.16.16.2.m1.1d">Ã—</annotation></semantics></math>2</td>
</tr>
<tr class="ltx_tr" id="S5.T2.16.18.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.16.18.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">Total</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T2.16.18.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">32,000</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T2.16.18.1.3" style="padding-left:4.3pt;padding-right:4.3pt;">32,000</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>The basic statistics of test sets. The arrow is the translation direction.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.2.2.3" style="padding-left:4.3pt;padding-right:4.3pt;">Benchmarks</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;"><math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.T3.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.m1.1.1" stretchy="false" xref="S5.T3.1.1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.m1.1d">â†’</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.2.2.2" style="padding-left:4.3pt;padding-right:4.3pt;"><math alttext="\leftarrow" class="ltx_Math" display="inline" id="S5.T3.2.2.2.m1.1"><semantics id="S5.T3.2.2.2.m1.1a"><mo id="S5.T3.2.2.2.m1.1.1" stretchy="false" xref="S5.T3.2.2.2.m1.1.1.cmml">â†</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.m1.1b"><ci id="S5.T3.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1">â†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.m1.1c">\leftarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.2.2.m1.1d">â†</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.2.3.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.3.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">WMT-22 EN-CS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.3.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">2,037</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.3.1.3" style="padding-left:4.3pt;padding-right:4.3pt;">1,448</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.4.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.4.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">WMT-22 EN-DE</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.4.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">2,037</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.4.2.3" style="padding-left:4.3pt;padding-right:4.3pt;">1,984</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.5.3.1" style="padding-left:4.3pt;padding-right:4.3pt;">WMT-22 EN-ZH</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.5.3.2" style="padding-left:4.3pt;padding-right:4.3pt;">2,037</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.5.3.3" style="padding-left:4.3pt;padding-right:4.3pt;">1,875</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.6.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.6.4.1" style="padding-left:4.3pt;padding-right:4.3pt;">WMT-22 EN-UK</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.6.4.2" style="padding-left:4.3pt;padding-right:4.3pt;">2,037</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.6.4.3" style="padding-left:4.3pt;padding-right:4.3pt;">2,018</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.7.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.7.5.1" style="padding-left:4.3pt;padding-right:4.3pt;">WMT-22 EN-RU</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.7.5.2" style="padding-left:4.3pt;padding-right:4.3pt;">2,037</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.7.5.3" style="padding-left:4.3pt;padding-right:4.3pt;">2,016</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.8.6.1" style="padding-left:4.3pt;padding-right:4.3pt;">WMT-22 DE-FR</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.8.6.2" style="padding-left:4.3pt;padding-right:4.3pt;">1,984</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.8.6.3" style="padding-left:4.3pt;padding-right:4.3pt;">2,006</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.9.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.9.7.1" style="padding-left:4.3pt;padding-right:4.3pt;">WMT-22 CS-UK</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.9.7.2" style="padding-left:4.3pt;padding-right:4.3pt;">1,930</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.9.7.3" style="padding-left:4.3pt;padding-right:4.3pt;">2,812</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.10.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.10.8.1" style="padding-left:4.3pt;padding-right:4.3pt;">WMT-16 EN-RO</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.10.8.2" style="padding-left:4.3pt;padding-right:4.3pt;">1,999</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.10.8.3" style="padding-left:4.3pt;padding-right:4.3pt;">1,999</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.11.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.11.9.1" style="padding-left:4.3pt;padding-right:4.3pt;">WMT-14 EN-HI</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.11.9.2" style="padding-left:4.3pt;padding-right:4.3pt;">2,507</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.11.9.3" style="padding-left:4.3pt;padding-right:4.3pt;">2,507</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.12.10">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.2.12.10.1" style="padding-left:4.3pt;padding-right:4.3pt;">WMT-14 EN-FR</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.2.12.10.2" style="padding-left:4.3pt;padding-right:4.3pt;">3,003</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T3.2.12.10.3" style="padding-left:4.3pt;padding-right:4.3pt;">3,003</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.1.Â Â Â Dataset</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Our training data always has either the source or the target in EnglishÂ <cite class="ltx_cite ltx_citemacro_cite">Schioppa etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib48" title="">2023</a>)</cite>. We select 8 language pairs in OPUS-100<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://object.pouta.csc.fi/OPUS-100/v1.0/opus-100-corpus-v1.0.tar.gz</span></span></span> for training and evaluating. To guarantee the quality of the training data, we use the LABSE <cite class="ltx_cite ltx_citemacro_cite">Feng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib21" title="">2020</a>)</cite> tool to select sentences with similarity scores above 0.75.
Since our goal is to explore the translation ability of LLM and reduce the impact of parallel corpus on LLM, we limit the number of parallel sentences for training and validation to 2,000 per language pair, which follows the same settings asÂ <cite class="ltx_cite ltx_citemacro_citet">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib32" title="">2023</a>)</cite>.
In order to minimize the possibility of overlap with mT0â€™s training corpus and enable comparisons with state-of-the-art (SOTA) systems, we use the WMT 14, 16 and 22 benchmarks for evaluation. We evaluate our model and baselines on news data from the WMT 2022Â <cite class="ltx_cite ltx_citemacro_cite">Vilar etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib56" title="">2022</a>); Kocmi etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib29" title="">2022</a>)</cite> evaluation campaign. WMT 22 moves away from testing only on the news domain like in previous years and shifts to focus on the general scenario covering news, social, conversational, and e-commerce.
Since Hindi, French and Romanian are common evaluated and not included in WMT 22, we use test data from WMT 14 and WMT 16.
TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.T2" title="Table 2 â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a> and TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.T3" title="Table 3 â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a> show statistics for our training, validation, and test data (since each language pair contains two directions, we multiply each cell by 2).</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">In the experiment, we find that although the selected training dataset for zh-en language pair has a high LABSE similarity score.
It often appears in both English and Chinese within the same sentence, which seriously damages the quality of training data.
Hence, We translate this low-quality en-zh language pair, from English to Chinese, using the official ChatGPT API<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://platform.openai.com/docs/api-reference/chat</span></span></span> provided by OpenAI <cite class="ltx_cite ltx_citemacro_cite">Ouyang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib38" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">To generalize instruction tuning, we collect publicly available multilingual translation prompts fromÂ <cite class="ltx_cite ltx_citemacro_citet">Bawden and Yvon (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib3" title="">2023</a>)</cite> and PromptSourceÂ <cite class="ltx_cite ltx_citemacro_citet">Bach etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib2" title="">2022</a>)</cite> in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S4.T1" title="Table 1 â€£ 4. Methods â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, and randomly select prompt and parallel sentence to combine as model input. Accordingly, the instruction tuning dataset is scrambled to ensure diversity and randomness. When doing the inference, we also randomly select the aforementioned translation prompts, in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S4.T1" title="Table 1 â€£ 4. Methods â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, to generalize the evaluation results.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.2.Â Â Â Evaluation</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">To comprehensively evaluate the performance of multilingual machine translation, we utilize four widely-used metrics<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>If a source sentence has multiple references, we report the average score</span></span></span>: scaleBLEU <cite class="ltx_cite ltx_citemacro_cite">Post (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib43" title="">2018</a>)</cite>, chrF <cite class="ltx_cite ltx_citemacro_cite">PopoviÄ‡ (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib41" title="">2015</a>)</cite>, comet-da<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/Unbabel/COMET</span></span></span>, and perplexity (we restrict perplexity to the X-EN direction since GPT-2-Large <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib45" title="">2019</a>)</cite> has only been trained on English text).
Furthermore, to qualitatively determine the four types of translation errors mentioned in section <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S1" title="1. Introduction â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, we apply the following metrics.</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">OT ratio:</span> we utilize the <span class="ltx_text ltx_font_typewriter" id="S5.I1.i1.p1.1.2">langdetect</span> package to detect the highest probability language of the generated translations. We calculate the proportion of the number of sentences that are translated into the wrong language out of all sentences.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">SC ratio:</span> We detect source copy (SC) error by computing scareBLEU score between predictions and the source sentences. If scareBLEU score exceeds 80, we consider it to be a SC error.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">OUG ratio:</span> We consider translations with a length ratio, compared to reference, exceeding 2 or falling below 0.5 as over/under generation (OUG) issue. Meanwhile, misunderstanding of instructions (MI) issue can also be reflected by sentence length.
</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.3.Â Â Â Implementation Details</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We train the baselines and ACT-MNMT on 8 Tesla-V100-32G GPUs.
For all the baselines, the random seed, learning rate, learning schedule, dropout rate, and gradient clipping, number beams, no repeat ngram size, number of common trigger tokens per translation direction, and number of specific trigger tokens per translation direction are set as 42, 2e-5, cosine, 0.1, 1.0, 5, 3, 1 and 1, respectively.
Due to memory limitations, the batch size is set to 2, the gradient accumulation step is set to 2, and training is performed with FP16.
The ratio of validation set used for evaluating is set to 0.1 empirically.
We adopt early stopping strategy, and the patience is set to 5.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">mT0Â <cite class="ltx_cite ltx_citemacro_cite">Muennighoff etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib37" title="">2022</a>)</cite> releases 5 variants of different sizes, ranging from 300M to 13B, referred to as "mT0-small" and "mT0-xxl" respectively. In this paper (where our goal is to design the recipe through extensive experimentation), we use mT0-xl to reduce computational costs. For all models and experiments, we use Hugging Face Transformers<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://github.com/huggingface/transformers</span></span></span>.
</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Main results for mT0-xl on test datasets. Averages over different sets of translation directions (There are a total of 20 translation directions, including 8 from other languages to English, 8 from English to other languages, and 4 directions that do not involve English at all. Please refer to Table <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.T3" title="Table 3 â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a> for details). </figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.1.1.2" style="padding-left:4.3pt;padding-right:4.3pt;"></th>
<td class="ltx_td ltx_border_t" id="S5.T4.1.1.3" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td class="ltx_td ltx_border_t" id="S5.T4.1.1.4" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td class="ltx_td ltx_border_t" id="S5.T4.1.1.5" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td class="ltx_td ltx_border_t" id="S5.T4.1.1.6" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td class="ltx_td ltx_border_t" id="S5.T4.1.1.7" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S5.T4.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">Error type (in <math alttext="\%" class="ltx_Math" display="inline" id="S5.T4.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.m1.1.1" xref="S5.T4.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T4.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.m1.1d">%</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.2.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">Baselines</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.2.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">Size</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.2.1.3" style="padding-left:4.3pt;padding-right:4.3pt;">scareBLEU</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.2.1.4" style="padding-left:4.3pt;padding-right:4.3pt;">chrF</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.2.1.5" style="padding-left:4.3pt;padding-right:4.3pt;">comet-da</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.2.1.6" style="padding-left:4.3pt;padding-right:4.3pt;">perplexity</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.2.1.7" style="padding-left:4.3pt;padding-right:4.3pt;">OT ratio</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.2.1.8" style="padding-left:4.3pt;padding-right:4.3pt;">SC ratio</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.2.1.9" style="padding-left:4.3pt;padding-right:4.3pt;">OUG ratio</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.1.3.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">mT0-xl 0-shot</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">3.7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.2.3" style="padding-left:4.3pt;padding-right:4.3pt;">16.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.2.4" style="padding-left:4.3pt;padding-right:4.3pt;">35.59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.2.5" style="padding-left:4.3pt;padding-right:4.3pt;">72.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.2.6" style="padding-left:4.3pt;padding-right:4.3pt;">175.57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.2.7" style="padding-left:4.3pt;padding-right:4.3pt;">32.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.2.8" style="padding-left:4.3pt;padding-right:4.3pt;">12.52</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.2.9" style="padding-left:4.3pt;padding-right:4.3pt;">6.06</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.4.3.1" style="padding-left:4.3pt;padding-right:4.3pt;">mT0-xl 1-shot</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.2" style="padding-left:4.3pt;padding-right:4.3pt;">3.7B</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.3" style="padding-left:4.3pt;padding-right:4.3pt;">7.29</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.4" style="padding-left:4.3pt;padding-right:4.3pt;">21.28</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.5" style="padding-left:4.3pt;padding-right:4.3pt;">55.65</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.6" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.4.3.6.1">83.60</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.7" style="padding-left:4.3pt;padding-right:4.3pt;">39.58</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.8" style="padding-left:4.3pt;padding-right:4.3pt;">23.60</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.9" style="padding-left:4.3pt;padding-right:4.3pt;">22.18</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.5.4.1" style="padding-left:4.3pt;padding-right:4.3pt;">mT0-xxl 0-shot</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.2" style="padding-left:4.3pt;padding-right:4.3pt;">13B</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.3" style="padding-left:4.3pt;padding-right:4.3pt;">21.06</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.4" style="padding-left:4.3pt;padding-right:4.3pt;">43.08</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.5" style="padding-left:4.3pt;padding-right:4.3pt;">77.70</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.6" style="padding-left:4.3pt;padding-right:4.3pt;">270.00</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.7" style="padding-left:4.3pt;padding-right:4.3pt;">17.77</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.8" style="padding-left:4.3pt;padding-right:4.3pt;">4.82</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.9" style="padding-left:4.3pt;padding-right:4.3pt;">2.77</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.6.5.1" style="padding-left:4.3pt;padding-right:4.3pt;">mFTI</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.6.5.2" style="padding-left:4.3pt;padding-right:4.3pt;">3.7B</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.6.5.3" style="padding-left:4.3pt;padding-right:4.3pt;">25.50</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.6.5.4" style="padding-left:4.3pt;padding-right:4.3pt;">50.17</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.6.5.5" style="padding-left:4.3pt;padding-right:4.3pt;">82.13</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.6.5.6" style="padding-left:4.3pt;padding-right:4.3pt;">144.49</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.6.5.7" style="padding-left:4.3pt;padding-right:4.3pt;">6.26</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.6.5.8" style="padding-left:4.3pt;padding-right:4.3pt;">0.70</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.6.5.9" style="padding-left:4.3pt;padding-right:4.3pt;">0.88</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.1.7.6.1" style="padding-left:4.3pt;padding-right:4.3pt;">TECT-MNMT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.7.6.2" style="padding-left:4.3pt;padding-right:4.3pt;">3.7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.7.6.3" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.7.6.3.1">25.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.7.6.4" style="padding-left:4.3pt;padding-right:4.3pt;">50.61</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.7.6.5" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.7.6.5.1">82.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.7.6.6" style="padding-left:4.3pt;padding-right:4.3pt;">242.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.7.6.7" style="padding-left:4.3pt;padding-right:4.3pt;">4.19</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.7.6.8" style="padding-left:4.3pt;padding-right:4.3pt;">0.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.7.6.9" style="padding-left:4.3pt;padding-right:4.3pt;">0.75</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T4.1.8.7.1" style="padding-left:4.3pt;padding-right:4.3pt;">ACT-MNMT</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.8.7.2" style="padding-left:4.3pt;padding-right:4.3pt;">3.7B</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.8.7.3" style="padding-left:4.3pt;padding-right:4.3pt;">25.87</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.8.7.4" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.8.7.4.1">50.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.8.7.5" style="padding-left:4.3pt;padding-right:4.3pt;">82.39</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.8.7.6" style="padding-left:4.3pt;padding-right:4.3pt;">126.25</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.8.7.7" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.8.7.7.1">3.91</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.8.7.8" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.8.7.8.1">0.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.8.7.9" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.8.7.9.1">0.58</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.4.Â Â Â Baselines</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">We compare our method with following baselines:</p>
<ul class="ltx_itemize" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i1.p1.1.1">0-shot:</span> In-context learning (ICL) <cite class="ltx_cite ltx_citemacro_cite">Brown etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib4" title="">2020</a>)</cite> is a powerful method for harnessing the capabilities of LLM, which provides a few demonstrations to guide models on how to perform even on an unseen task without fine-tuning.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i2.p1.1.1">1-shot:</span> To guarantee the quality of demonstrations, we randomly select demonstrations from the FLORES-200 <cite class="ltx_cite ltx_citemacro_cite">Costa-jussÃ  etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib15" title="">2022</a>)</cite> validation dataset for the 1-shot setting.
We do not apply more demonstrations, since more demonstrations do not lead to higher performance in case of mT0 <cite class="ltx_cite ltx_citemacro_cite">Muennighoff etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib37" title="">2022</a>)</cite>.
</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I2.i3.p1">
<p class="ltx_p" id="S5.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i3.p1.1.1">mFTI:</span> <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib32" title="">2023</a>)</cite> is a full fine-tuning method, where all model parameters are tuned during adaptation on multilingual machine translation task with a limited training samples.
It can also be regarded as instruction fine-tuning.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I2.i4.p1">
<p class="ltx_p" id="S5.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i4.p1.1.1">TECT-MNMT:</span> is a variant of our proposed model mentioned in Equation <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S4.E2" title="2 â€£ 4.1. Task-enhanced Constrained Turning â€£ 4. Methods â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>.
It utilizes a hard encoding constrained template in the target side to guide the generation of the LLM (hard encoding constrained template is empirically set to <span class="ltx_text ltx_font_italic" id="S5.I2.i4.p1.1.2">translate from [L1] to [L2]</span>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.5.Â Â Â Experimental Results</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">The average results on both zero-shot and supervised directions, along with other baselines, are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.T4" title="Table 4 â€£ 5.3. Implementation Details â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>, where the best result is displayed in bold.
According to Table <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.T4" title="Table 4 â€£ 5.3. Implementation Details â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>, we can find that both TECT-MNMT and ACT-MNMT outperform all baselines in terms of all evaluation metrics at all levels (except the perplexity), based on the average results for all translation directions, and even outperforms larger models, such as mT0-xxl 13B.
Specifically, TECT-MNMT improves 0.44, and ACT-MNMT improves 0.37 in scareBLEU compared to the bested performed baseline, mFTI.
Furthermore, ACT-MNMT achieves the minimum OT ratio, SC ratio and OUG ratio, as well as the maximum chrF-style score.
These results indicate that the auto-constriction template can assist LLM in better understanding instructions and minimize translation issues (<span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.1">e</span>.<span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.2">g</span>.Â , misunderstanding of instructions issue, off-target translation issue, source copy issue and over/under generation issue).
The main reason is that our designed constrained template on the target side can extract trigger tokens as additional supervised information.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">In addition, we can observe that mT0-xl exhibits a weak capacity for in-context learning. 1-shot baseline achieves lower performance compared to 0-shot baseline, which is consistent withÂ <cite class="ltx_cite ltx_citemacro_citet">Muennighoff etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib37" title="">2022</a>)</cite>.
We also find that ACT-MNMT shows slightly weaker but comparable performance, <span class="ltx_text ltx_font_italic" id="S5.SS5.p2.1.1">e</span>.<span class="ltx_text ltx_font_italic" id="S5.SS5.p2.1.2">g</span>.Â on semantic-level metrics, compared to TECT-MNMT.
We consider that hard encoding provides a clearer guidance. However, designing an optimal constraint template requires profound insights into the task information.
</p>
</div>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Five constrained templates used in the ablation experiment.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.1.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">Variants</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">Constrained Template</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.2.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">TECT-MNMT-1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">translate from [L1] to [L2]:</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.3.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">TECT-MNMT-2</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.3.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">translate to [L2]:</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.4.3.1" style="padding-left:4.3pt;padding-right:4.3pt;">TECT-MNMT-3</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.3.2" style="padding-left:4.3pt;padding-right:4.3pt;">translate from [L1]:</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.5.4.1" style="padding-left:4.3pt;padding-right:4.3pt;">TECT-MNMT-4</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.5.4.2" style="padding-left:4.3pt;padding-right:4.3pt;">from [L1] to [L2]:</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T5.1.6.5.1" style="padding-left:4.3pt;padding-right:4.3pt;">TECT-MNMT-5</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T5.1.6.5.2" style="padding-left:4.3pt;padding-right:4.3pt;">[L2]:</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Averages over different sets of translation directions with five different constrained templates.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T6.1.1.2" style="padding-left:4.3pt;padding-right:4.3pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T6.1.1.3" style="padding-left:4.3pt;padding-right:4.3pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T6.1.1.4" style="padding-left:4.3pt;padding-right:4.3pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T6.1.1.5" style="padding-left:4.3pt;padding-right:4.3pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T6.1.1.6" style="padding-left:4.3pt;padding-right:4.3pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S5.T6.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">Error type (in <math alttext="\%" class="ltx_Math" display="inline" id="S5.T6.1.1.1.m1.1"><semantics id="S5.T6.1.1.1.m1.1a"><mo id="S5.T6.1.1.1.m1.1.1" xref="S5.T6.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.T6.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T6.1.1.1.m1.1.1.cmml" xref="S5.T6.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.1.1.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.T6.1.1.1.m1.1d">%</annotation></semantics></math>)</th>
</tr>
<tr class="ltx_tr" id="S5.T6.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S5.T6.1.2.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">baselines</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.1.2.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">scareBLEU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.1.2.1.3" style="padding-left:4.3pt;padding-right:4.3pt;">chrF</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.1.2.1.4" style="padding-left:4.3pt;padding-right:4.3pt;">comet-da</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.1.2.1.5" style="padding-left:4.3pt;padding-right:4.3pt;">perplexity</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.1.2.1.6" style="padding-left:4.3pt;padding-right:4.3pt;">OT ratio</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.1.2.1.7" style="padding-left:4.3pt;padding-right:4.3pt;">SC ratio</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.1.2.1.8" style="padding-left:4.3pt;padding-right:4.3pt;">OUG ratio</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T6.1.3.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">TECT-MNMT-1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.3.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">25.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.3.1.3" style="padding-left:4.3pt;padding-right:4.3pt;">50.61</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.3.1.4" style="padding-left:4.3pt;padding-right:4.3pt;">82.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.3.1.5" style="padding-left:4.3pt;padding-right:4.3pt;">242.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.3.1.6" style="padding-left:4.3pt;padding-right:4.3pt;">4.19</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.3.1.7" style="padding-left:4.3pt;padding-right:4.3pt;">0.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.3.1.8" style="padding-left:4.3pt;padding-right:4.3pt;">0.75</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.1.4.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">TECT-MNMT-2</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.2.2" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.4.2.2.1">26.11</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.2.3" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.4.2.3.1">50.83</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.2.4" style="padding-left:4.3pt;padding-right:4.3pt;">82.49</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.2.5" style="padding-left:4.3pt;padding-right:4.3pt;">134.80</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.2.6" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.4.2.6.1">4.17</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.2.7" style="padding-left:4.3pt;padding-right:4.3pt;">0.32</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.2.8" style="padding-left:4.3pt;padding-right:4.3pt;">0.71</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.1.5.3.1" style="padding-left:4.3pt;padding-right:4.3pt;">TECT-MNMT-3</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.5.3.2" style="padding-left:4.3pt;padding-right:4.3pt;">26.02</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.5.3.3" style="padding-left:4.3pt;padding-right:4.3pt;">50.76</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.5.3.4" style="padding-left:4.3pt;padding-right:4.3pt;">82.45</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.5.3.5" style="padding-left:4.3pt;padding-right:4.3pt;">128.62</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.5.3.6" style="padding-left:4.3pt;padding-right:4.3pt;">4.18</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.5.3.7" style="padding-left:4.3pt;padding-right:4.3pt;">0.33</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.5.3.8" style="padding-left:4.3pt;padding-right:4.3pt;">0.72</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.1.6.4.1" style="padding-left:4.3pt;padding-right:4.3pt;">TECT-MNMT-4</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.6.4.2" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.6.4.2.1">26.11</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.6.4.3" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.6.4.3.1">50.83</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.6.4.4" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.6.4.4.1">82.57</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.6.4.5" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.6.4.5.1">127.62</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.6.4.6" style="padding-left:4.3pt;padding-right:4.3pt;">4.27</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.6.4.7" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.6.4.7.1">0.21</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.6.4.8" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.6.4.8.1">0.66</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.7.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T6.1.7.5.1" style="padding-left:4.3pt;padding-right:4.3pt;">TECT-MNMT-5</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.1.7.5.2" style="padding-left:4.3pt;padding-right:4.3pt;">25.98</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.1.7.5.3" style="padding-left:4.3pt;padding-right:4.3pt;">50.65</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.1.7.5.4" style="padding-left:4.3pt;padding-right:4.3pt;">82.45</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.1.7.5.5" style="padding-left:4.3pt;padding-right:4.3pt;">129.67</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.1.7.5.6" style="padding-left:4.3pt;padding-right:4.3pt;">4.42</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.1.7.5.7" style="padding-left:4.3pt;padding-right:4.3pt;">0.31</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.1.7.5.8" style="padding-left:4.3pt;padding-right:4.3pt;">0.72</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T7">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Parameter sensitivity <span class="ltx_text ltx_font_italic" id="S5.T7.6.1">w</span>.<span class="ltx_text ltx_font_italic" id="S5.T7.7.2">r</span>.<span class="ltx_text ltx_font_italic" id="S5.T7.8.3">t</span>Â number of trigger tokens.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T7.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T7.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" colspan="2" id="S5.T7.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">
<math alttext="\#" class="ltx_Math" display="inline" id="S5.T7.1.1.1.m1.1"><semantics id="S5.T7.1.1.1.m1.1a"><mi id="S5.T7.1.1.1.m1.1.1" mathvariant="normal" xref="S5.T7.1.1.1.m1.1.1.cmml">#</mi><annotation-xml encoding="MathML-Content" id="S5.T7.1.1.1.m1.1b"><ci id="S5.T7.1.1.1.m1.1.1.cmml" xref="S5.T7.1.1.1.m1.1.1">#</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.1.1.1.m1.1c">\#</annotation><annotation encoding="application/x-llamapun" id="S5.T7.1.1.1.m1.1d">#</annotation></semantics></math> trigger tokens</th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T7.2.2.3" style="padding-left:4.3pt;padding-right:4.3pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T7.2.2.4" style="padding-left:4.3pt;padding-right:4.3pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T7.2.2.5" style="padding-left:4.3pt;padding-right:4.3pt;"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T7.2.2.6" style="padding-left:4.3pt;padding-right:4.3pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S5.T7.2.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">Error type (in <math alttext="\%" class="ltx_Math" display="inline" id="S5.T7.2.2.2.m1.1"><semantics id="S5.T7.2.2.2.m1.1a"><mo id="S5.T7.2.2.2.m1.1.1" xref="S5.T7.2.2.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.T7.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T7.2.2.2.m1.1.1.cmml" xref="S5.T7.2.2.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.2.2.2.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.T7.2.2.2.m1.1d">%</annotation></semantics></math>)</th>
</tr>
<tr class="ltx_tr" id="S5.T7.2.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S5.T7.2.3.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">common</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S5.T7.2.3.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">specific</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T7.2.3.1.3" style="padding-left:4.3pt;padding-right:4.3pt;">scareBLEU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T7.2.3.1.4" style="padding-left:4.3pt;padding-right:4.3pt;">chrF</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T7.2.3.1.5" style="padding-left:4.3pt;padding-right:4.3pt;">comet-da</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T7.2.3.1.6" style="padding-left:4.3pt;padding-right:4.3pt;">perplexity</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T7.2.3.1.7" style="padding-left:4.3pt;padding-right:4.3pt;">OT ratio</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T7.2.3.1.8" style="padding-left:4.3pt;padding-right:4.3pt;">SC ratio</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T7.2.3.1.9" style="padding-left:4.3pt;padding-right:4.3pt;">OUG ratio</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T7.2.4.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T7.2.4.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T7.2.4.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.2.4.1.3" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T7.2.4.1.3.1">25.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.2.4.1.4" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T7.2.4.1.4.1">50.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.2.4.1.5" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T7.2.4.1.5.1">82.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.2.4.1.6" style="padding-left:4.3pt;padding-right:4.3pt;">126.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.2.4.1.7" style="padding-left:4.3pt;padding-right:4.3pt;">3.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.2.4.1.8" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T7.2.4.1.8.1">0.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.2.4.1.9" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T7.2.4.1.9.1">0.58</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.5.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T7.2.5.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T7.2.5.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">3</th>
<td class="ltx_td ltx_align_center" id="S5.T7.2.5.2.3" style="padding-left:4.3pt;padding-right:4.3pt;">25.80</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.5.2.4" style="padding-left:4.3pt;padding-right:4.3pt;">50.44</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.5.2.5" style="padding-left:4.3pt;padding-right:4.3pt;">82.23</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.5.2.6" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T7.2.5.2.6.1">119.26</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.5.2.7" style="padding-left:4.3pt;padding-right:4.3pt;">3.96</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.5.2.8" style="padding-left:4.3pt;padding-right:4.3pt;">0.40</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.5.2.9" style="padding-left:4.3pt;padding-right:4.3pt;">0.69</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.6.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T7.2.6.3.1" style="padding-left:4.3pt;padding-right:4.3pt;">1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T7.2.6.3.2" style="padding-left:4.3pt;padding-right:4.3pt;">6</th>
<td class="ltx_td ltx_align_center" id="S5.T7.2.6.3.3" style="padding-left:4.3pt;padding-right:4.3pt;">25.78</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.6.3.4" style="padding-left:4.3pt;padding-right:4.3pt;">50.52</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.6.3.5" style="padding-left:4.3pt;padding-right:4.3pt;">82.25</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.6.3.6" style="padding-left:4.3pt;padding-right:4.3pt;">128.73</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.6.3.7" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S5.T7.2.6.3.7.1">3.67</span></td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.6.3.8" style="padding-left:4.3pt;padding-right:4.3pt;">0.23</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.6.3.9" style="padding-left:4.3pt;padding-right:4.3pt;">0.71</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.7.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T7.2.7.4.1" style="padding-left:4.3pt;padding-right:4.3pt;">1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T7.2.7.4.2" style="padding-left:4.3pt;padding-right:4.3pt;">9</th>
<td class="ltx_td ltx_align_center" id="S5.T7.2.7.4.3" style="padding-left:4.3pt;padding-right:4.3pt;">25.31</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.7.4.4" style="padding-left:4.3pt;padding-right:4.3pt;">49.56</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.7.4.5" style="padding-left:4.3pt;padding-right:4.3pt;">81.73</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.7.4.6" style="padding-left:4.3pt;padding-right:4.3pt;">124.52</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.7.4.7" style="padding-left:4.3pt;padding-right:4.3pt;">4.94</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.7.4.8" style="padding-left:4.3pt;padding-right:4.3pt;">0.24</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.7.4.9" style="padding-left:4.3pt;padding-right:4.3pt;">1.96</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.8.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T7.2.8.5.1" style="padding-left:4.3pt;padding-right:4.3pt;">2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T7.2.8.5.2" style="padding-left:4.3pt;padding-right:4.3pt;">6</th>
<td class="ltx_td ltx_align_center" id="S5.T7.2.8.5.3" style="padding-left:4.3pt;padding-right:4.3pt;">25.55</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.8.5.4" style="padding-left:4.3pt;padding-right:4.3pt;">50.25</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.8.5.5" style="padding-left:4.3pt;padding-right:4.3pt;">82.05</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.8.5.6" style="padding-left:4.3pt;padding-right:4.3pt;">121.29</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.8.5.7" style="padding-left:4.3pt;padding-right:4.3pt;">3.74</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.8.5.8" style="padding-left:4.3pt;padding-right:4.3pt;">0.24</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.8.5.9" style="padding-left:4.3pt;padding-right:4.3pt;">0.79</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.9.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T7.2.9.6.1" style="padding-left:4.3pt;padding-right:4.3pt;">3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T7.2.9.6.2" style="padding-left:4.3pt;padding-right:4.3pt;">3</th>
<td class="ltx_td ltx_align_center" id="S5.T7.2.9.6.3" style="padding-left:4.3pt;padding-right:4.3pt;">25.32</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.9.6.4" style="padding-left:4.3pt;padding-right:4.3pt;">49.96</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.9.6.5" style="padding-left:4.3pt;padding-right:4.3pt;">81.79</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.9.6.6" style="padding-left:4.3pt;padding-right:4.3pt;">121.80</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.9.6.7" style="padding-left:4.3pt;padding-right:4.3pt;">3.98</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.9.6.8" style="padding-left:4.3pt;padding-right:4.3pt;">0.24</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.9.6.9" style="padding-left:4.3pt;padding-right:4.3pt;">0.85</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.10.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T7.2.10.7.1" style="padding-left:4.3pt;padding-right:4.3pt;">3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T7.2.10.7.2" style="padding-left:4.3pt;padding-right:4.3pt;">9</th>
<td class="ltx_td ltx_align_center" id="S5.T7.2.10.7.3" style="padding-left:4.3pt;padding-right:4.3pt;">24.09</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.10.7.4" style="padding-left:4.3pt;padding-right:4.3pt;">46.90</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.10.7.5" style="padding-left:4.3pt;padding-right:4.3pt;">79.81</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.10.7.6" style="padding-left:4.3pt;padding-right:4.3pt;">121.78</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.10.7.7" style="padding-left:4.3pt;padding-right:4.3pt;">8.75</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.10.7.8" style="padding-left:4.3pt;padding-right:4.3pt;">0.34</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.10.7.9" style="padding-left:4.3pt;padding-right:4.3pt;">5.60</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.11.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T7.2.11.8.1" style="padding-left:4.3pt;padding-right:4.3pt;">6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T7.2.11.8.2" style="padding-left:4.3pt;padding-right:4.3pt;">6</th>
<td class="ltx_td ltx_align_center" id="S5.T7.2.11.8.3" style="padding-left:4.3pt;padding-right:4.3pt;">24.18</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.11.8.4" style="padding-left:4.3pt;padding-right:4.3pt;">47.23</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.11.8.5" style="padding-left:4.3pt;padding-right:4.3pt;">79.88</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.11.8.6" style="padding-left:4.3pt;padding-right:4.3pt;">134.70</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.11.8.7" style="padding-left:4.3pt;padding-right:4.3pt;">8.37</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.11.8.8" style="padding-left:4.3pt;padding-right:4.3pt;">0.49</td>
<td class="ltx_td ltx_align_center" id="S5.T7.2.11.8.9" style="padding-left:4.3pt;padding-right:4.3pt;">5.06</td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.12.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S5.T7.2.12.9.1" style="padding-left:4.3pt;padding-right:4.3pt;">9</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T7.2.12.9.2" style="padding-left:4.3pt;padding-right:4.3pt;">9</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T7.2.12.9.3" style="padding-left:4.3pt;padding-right:4.3pt;">23.95</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T7.2.12.9.4" style="padding-left:4.3pt;padding-right:4.3pt;">46.44</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T7.2.12.9.5" style="padding-left:4.3pt;padding-right:4.3pt;">78.83</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T7.2.12.9.6" style="padding-left:4.3pt;padding-right:4.3pt;">122.77</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T7.2.12.9.7" style="padding-left:4.3pt;padding-right:4.3pt;">9.83</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T7.2.12.9.8" style="padding-left:4.3pt;padding-right:4.3pt;">0.48</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T7.2.12.9.9" style="padding-left:4.3pt;padding-right:4.3pt;">6.84</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.6.Â Â Â Ablation Study</h3>
<section class="ltx_subsubsection" id="S5.SS6.SSS1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">5.6.1.Â Â Â Ablation Experiment on Constrained Template Component</h4>
<div class="ltx_para" id="S5.SS6.SSS1.p1">
<p class="ltx_p" id="S5.SS6.SSS1.p1.1">To determine the impact of each part of the constrained template on the translation performance, we design five variations of the constrained template elaborately, as shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.T5" title="Table 5 â€£ 5.5. Experimental Results â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>.
The results of ablation study are presented in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.T6" title="Table 6 â€£ 5.5. Experimental Results â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS6.SSS1.p2">
<p class="ltx_p" id="S5.SS6.SSS1.p2.1">First, as we can see that all TECT-MNMT based methods outperform the instruction fine-tuning baseline, mFTI, which indicates that the constrained template can help LLM understand instructions, alleviate the four translation errors mentioned above, and guide the generation of the optimal outputs.
Second, TECT-MNMT-2 achieves better performance than TECT-MNMT-3.
It shows that the target language information plays a more important role than the source language information, which in line withÂ <cite class="ltx_cite ltx_citemacro_citet">Wu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib61" title="">2021</a>)</cite>.
Third, TECT-MNMT-4 achieves better performance than TECT-MNMT-5, which indicates that including only target language information is not sufficient, and understanding how to perform translation tasks is equally important.
Overall, we have the conclusion that providing information about task execution helps guide the modelâ€™s output and improves performance.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="261" id="S5.F3.g1" src="x4.png" width="348"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Parameter sensitivity <span class="ltx_text ltx_font_italic" id="S5.F3.4.1">w</span>.<span class="ltx_text ltx_font_italic" id="S5.F3.5.2">r</span>.<span class="ltx_text ltx_font_italic" id="S5.F3.6.3">t</span>Â  model size.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS6.SSS2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">5.6.2.Â Â Â Effect of Number of Triggers</h4>
<div class="ltx_para" id="S5.SS6.SSS2.p1">
<p class="ltx_p" id="S5.SS6.SSS2.p1.1">We show the result in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.T7" title="Table 7 â€£ 5.5. Experimental Results â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7</span></a>.
We observe that as the number of common trigger tokens and specific trigger tokens increases, the performance of the model gradually decreases.
In addition, when analyzing the experimental results in depth, we observe that perplexity is highly sensitive in the translation direction from Chinese to English.
Specifically, when the number of common trigger token and specific trigger token change from 1 to 3, the perplexity in the Chinese to English direction changes from 234.89 changes 180.75.
This greatly reduces the average improvement in other translation directions.</p>
</div>
<figure class="ltx_figure" id="S5.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S5.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="205" id="S5.F4.sf1.g1" src="x5.png" width="274"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_sansserif" id="S5.F4.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_sansserif" id="S5.F4.sf1.3.2" style="font-size:90%;">average</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S5.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="205" id="S5.F4.sf2.g1" src="x6.png" width="274"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_sansserif" id="S5.F4.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_sansserif" id="S5.F4.sf2.3.2" style="font-size:90%;">DE to EN</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S5.F4.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="205" id="S5.F4.sf3.g1" src="x7.png" width="274"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_sansserif" id="S5.F4.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text ltx_font_sansserif" id="S5.F4.sf3.3.2" style="font-size:90%;">EN to DE</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Parameter sensitivity <span class="ltx_text ltx_font_italic" id="S5.F4.4.1">w</span>.<span class="ltx_text ltx_font_italic" id="S5.F4.5.2">r</span>.<span class="ltx_text ltx_font_italic" id="S5.F4.6.3">t</span>Â  number of DE-EN language pair for training.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS6.SSS3">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">5.6.3.Â Â Â Effect of Model Size</h4>
<div class="ltx_para" id="S5.SS6.SSS3.p1">
<p class="ltx_p" id="S5.SS6.SSS3.p1.1">Since there exist different versions of mT0 with differing numbers of parameters, we perform the experiment to determine whether the ACT-MNMT has a good scalability and how model size impacts the performance. In this experiment, we report the average scaleBLEU score and off-target ratio across 20 translation directions in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.F3" title="Figure 3 â€£ 5.6.1. Ablation Experiment on Constrained Template Component â€£ 5.6. Ablation Study â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS6.SSS3.p2">
<p class="ltx_p" id="S5.SS6.SSS3.p2.4">Some observations are summarized from the experimental results:
(1) Our approach, ACT-MNMT, can be applied to different model sizes and achieve higher scaleBLEU and lower off-target ratio compared to traditional instruction fine-tuning. (2) ACT-MNMT can effectively alleviate off-target issue. Specifically, ACT-MNMT obtains 12.71<math alttext="\%" class="ltx_Math" display="inline" id="S5.SS6.SSS3.p2.1.m1.1"><semantics id="S5.SS6.SSS3.p2.1.m1.1a"><mo id="S5.SS6.SSS3.p2.1.m1.1.1" xref="S5.SS6.SSS3.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS3.p2.1.m1.1b"><csymbol cd="latexml" id="S5.SS6.SSS3.p2.1.m1.1.1.cmml" xref="S5.SS6.SSS3.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS3.p2.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS3.p2.1.m1.1d">%</annotation></semantics></math>, 14.48<math alttext="\%" class="ltx_Math" display="inline" id="S5.SS6.SSS3.p2.2.m2.1"><semantics id="S5.SS6.SSS3.p2.2.m2.1a"><mo id="S5.SS6.SSS3.p2.2.m2.1.1" xref="S5.SS6.SSS3.p2.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS3.p2.2.m2.1b"><csymbol cd="latexml" id="S5.SS6.SSS3.p2.2.m2.1.1.cmml" xref="S5.SS6.SSS3.p2.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS3.p2.2.m2.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS3.p2.2.m2.1d">%</annotation></semantics></math>, 21.58<math alttext="\%" class="ltx_Math" display="inline" id="S5.SS6.SSS3.p2.3.m3.1"><semantics id="S5.SS6.SSS3.p2.3.m3.1a"><mo id="S5.SS6.SSS3.p2.3.m3.1.1" xref="S5.SS6.SSS3.p2.3.m3.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS3.p2.3.m3.1b"><csymbol cd="latexml" id="S5.SS6.SSS3.p2.3.m3.1.1.cmml" xref="S5.SS6.SSS3.p2.3.m3.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS3.p2.3.m3.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS3.p2.3.m3.1d">%</annotation></semantics></math> and 37.55<math alttext="\%" class="ltx_Math" display="inline" id="S5.SS6.SSS3.p2.4.m4.1"><semantics id="S5.SS6.SSS3.p2.4.m4.1a"><mo id="S5.SS6.SSS3.p2.4.m4.1.1" xref="S5.SS6.SSS3.p2.4.m4.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS3.p2.4.m4.1b"><csymbol cd="latexml" id="S5.SS6.SSS3.p2.4.m4.1.1.cmml" xref="S5.SS6.SSS3.p2.4.m4.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS3.p2.4.m4.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS3.p2.4.m4.1d">%</annotation></semantics></math> reduction compared to mFTI in terms of Small-300M, Base-580M, Large-1.2B and Extra Large-3.7B on off-target ratio metric, respectively, and achieves equal or even better scareBLE score.
Furthermore, as the model parameters increase, ACT-MNMT consistently outperforms mFTI under the same parameters, which shows the robustness of ACT-MNMT.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS6.SSS4">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">5.6.4.Â Â Â Effect of Data Size</h4>
<div class="ltx_para" id="S5.SS6.SSS4.p1">
<p class="ltx_p" id="S5.SS6.SSS4.p1.1">In this subsection, we will investigate how the number of training examples affects the model.
We only change the number of training language pairs for DE-EN (German-English), and keep the number of other language pairs unchanged.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.F4" title="Figure 4 â€£ 5.6.2. Effect of Number of Triggers â€£ 5.6. Ablation Study â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a> presents scareBLEU and off-target ratio (OTR) in terms of average over all directions, DE to EN direction and EN to DE direction, respectively.</p>
</div>
<div class="ltx_para" id="S5.SS6.SSS4.p2">
<p class="ltx_p" id="S5.SS6.SSS4.p2.1">Firstly, we observe a common phenomenon in the three subgraphs of Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.F4" title="Figure 4 â€£ 5.6.2. Effect of Number of Triggers â€£ 5.6. Ablation Study â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>, where the OTR shows a trend of initially decreasing and then increasing with the increase in the number of DE-EN language pairs (the knee point may be at 1,000 or 2,000).
This phenomenon can be explained by the fact that when the number of German examples exceeds 2,000, it can cause an imbalance in multilingual language pairs, leading to an increased probability of translating into German.
Secondly, ACT-MNMT exhibits strong data scalability, which significantly improves the translation quality, especially when there are limited DE-EN language pairs for training.
For instance, in the case of 100 English German language pairs, ACT-MNMT obtains 5.29 scaleBLEU improvement, and achieve 86.24<math alttext="\%" class="ltx_Math" display="inline" id="S5.SS6.SSS4.p2.1.m1.1"><semantics id="S5.SS6.SSS4.p2.1.m1.1a"><mo id="S5.SS6.SSS4.p2.1.m1.1.1" xref="S5.SS6.SSS4.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS6.SSS4.p2.1.m1.1b"><csymbol cd="latexml" id="S5.SS6.SSS4.p2.1.m1.1.1.cmml" xref="S5.SS6.SSS4.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.SSS4.p2.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS6.SSS4.p2.1.m1.1d">%</annotation></semantics></math> decrease on OTR metric compared to mFTI.
In addition, as English and German increased from 100 to 20,000, DE to EN increases by 1.39 scareBLEU and EN to DE increased by 8.92 scareBLEU, which indicates that translating from other languages to German is more sensitive than translate from German to other languages.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS6.SSS5">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">5.6.5.Â Â Â Over/Under-generation</h4>
<figure class="ltx_figure" id="S5.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_flex_size_1 ltx_align_center" id="S5.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="317" id="S5.F5.sf1.g1" src="x8.png" width="423"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_sansserif" id="S5.F5.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text ltx_font_sansserif" id="S5.F5.sf1.3.2" style="font-size:90%;">BLOOMZ-7B1</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_flex_size_1 ltx_align_center" id="S5.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="317" id="S5.F5.sf2.g1" src="x9.png" width="424"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_sansserif" id="S5.F5.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text ltx_font_sansserif" id="S5.F5.sf2.3.2" style="font-size:90%;">mT0-xl (3.7B)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Over/Under-generation ratio on IWSLT 2017 test datasets.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS6.SSS5.p1">
<p class="ltx_p" id="S5.SS6.SSS5.p1.1">FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.F5" title="Figure 5 â€£ 5.6.5. Over/Under-generation â€£ 5.6. Ablation Study â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a> reports the over/under-generation ratio of any pair of languages on IWSLT 2017 test datasets.
Since BLOOMZ and mT0 have trained on FLORES-200Â <cite class="ltx_cite ltx_citemacro_cite">Muennighoff etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#bib.bib37" title="">2022</a>)</cite>, we do not evaluate on this standard multilingual translation dataset. As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.06745v1#S5.F5" title="Figure 5 â€£ 5.6.5. Over/Under-generation â€£ 5.6. Ablation Study â€£ 5. Experiments â€£ ACT-MNMT: Auto-Constriction Turning for Multilingual Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>, we have the following observations: 1) Over/Under-generation (OUG) ratio and off-target (OT) ratio have the similar distributions among language pairs. For instance, both the OUG and OT ratio exhibit small values among rich-resource language pairs, and present large values among low-resource language pairs. 2) Compared to OT ratio, OUG ratio is lower among all directions, which means OT issue is more severe than OUG issue.
</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.Â Â Â Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we first propose a novel task-enhanced constrained turning (TECT-MNMT) method, which utilises a manually designed hard encoding constrained template to guide the output of the model.
Then, we extend TECT-MNMT to an auto-constriction turning method (ACT-MNMT), which applies a sequence of trigger token as a soft-encoding constrained template to guide the output of the model.
Auto-constriction template can effectively learn patterns in different translation directions, alleviate OT problems, OUG problems, and SC problems.
Both TECT-MNMT and ACT-MNMT significantly outperforms instruction fine-tuning baseline.
We demonstrate that the ACT-MNMT model has good scalability and robustness through extensive experiments, and conclude that providing information about task execution helps guide the modelâ€™s generation and improves performance.
Although we focus only on mT0 model in this work, our method is trivially extendable to auto-regressive language models.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">The main limitation of our work is that although we reduce the cost of training by reducing the number of training samples, we still need to fine-tune the weights of the original model, which can be troublesome in some scenarios.
In the future, we will be devoted to using more efficient methods to solve off-target issue in multilingual machine translation.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">7.Â Â Â Bibliographical References</h2>
<span class="ltx_ERROR undefined" id="S7.1">\c@NAT@ctr</span>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography"></h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal etÂ al. (2022)</span>
<span class="ltx_bibblock">
Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan
Ghazvininejad. 2022.

</span>
<span class="ltx_bibblock">In-context examples selection for machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2212.02437</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bach etÂ al. (2022)</span>
<span class="ltx_bibblock">
StephenÂ H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel,
NihalÂ V. Nayak, Abheesht Sharma, Taewoon Kim, MÂ Saiful Bari, Thibault Fevry,
Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David,
Canwen Xu, Gunjan Chhablani, Han Wang, JasonÂ Alan Fries, MagedÂ S.
Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang,
Xiangru Tang, Mike Tian-Jian Jiang, and AlexanderÂ M. Rush. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2202.01279" title="">Promptsource: An integrated
development environment and repository for natural language prompts</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bawden and Yvon (2023)</span>
<span class="ltx_bibblock">
Rachel Bawden and FranÃ§ois Yvon. 2023.

</span>
<span class="ltx_bibblock">Investigating the translation performance of a large multilingual
language model: the case of bloom.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2303.01911</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
etÂ al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Advances in neural information processing systems</em>,
33:1877â€“1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BSI (1973a)</span>
<span class="ltx_bibblock">
BSI. 1973a.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Natural Fibre Twines</em>, 3rd edition.

</span>
<span class="ltx_bibblock">British Standards Institution, London.

</span>
<span class="ltx_bibblock">BS 2570.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BSI (1973b)</span>
<span class="ltx_bibblock">
BSI. 1973b.

</span>
<span class="ltx_bibblock">Natural fibre twines.

</span>
<span class="ltx_bibblock">BS 2570, British Standards Institution, London.

</span>
<span class="ltx_bibblock">3rd. edn.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Castor and Pollux (1992)</span>
<span class="ltx_bibblock">
A.Â Castor and L.Â E. Pollux. 1992.

</span>
<span class="ltx_bibblock">The use of user modelling to guide inference and learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Applied Intelligence</em>, 2(1):37â€“53.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cettolo etÂ al. (2017)</span>
<span class="ltx_bibblock">
Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Niehues Jan, StÃ¼ker
Sebastian, Sudoh Katsuitho, Yoshino Koichiro, and Federmann Christian. 2017.

</span>
<span class="ltx_bibblock">Overview of the iwslt 2017 evaluation campaign.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 14th International Workshop on Spoken
Language Translation</em>, pages 2â€“14.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Liang Chen, Shuming Ma, Dongdong Zhang, Furu Wei, and Baobao Chang.
2023a.

</span>
<span class="ltx_bibblock">On the off-target problem of zero-shot multilingual neural machine
translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Liang Chen, Shuming Ma, Dongdong Zhang, Furu Wei, and Baobao Chang.
2023b.

</span>
<span class="ltx_bibblock">On the off-target problem of zero-shot multilingual neural machine
translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2305.10930</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chercheur (1994)</span>
<span class="ltx_bibblock">
J.L. Chercheur. 1994.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Case-Based Reasoning</em>, 2nd edition.

</span>
<span class="ltx_bibblock">Morgan Kaufman Publishers, San Mateo, CA.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chomsky (1973)</span>
<span class="ltx_bibblock">
N.Â Chomsky. 1973.

</span>
<span class="ltx_bibblock">Conditions on transformations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">A festschrift for Morris Halle</em>, New York. Holt, Rinehart
&amp; Winston.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery etÂ al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
Adam Roberts, Paul Barham, HyungÂ Won Chung, Charles Sutton, Sebastian
Gehrmann, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2204.02311</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung etÂ al. (2022)</span>
<span class="ltx_bibblock">
HyungÂ Won Chung, LeÂ Hou, Shayne Longpre, Barret Zoph, YiÂ Tay, William Fedus,
Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2210.11416</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costa-jussÃ  etÂ al. (2022)</span>
<span class="ltx_bibblock">
MartaÂ R Costa-jussÃ , James Cross, Onur Ã‡elebi, Maha Elbayad, Kenneth
Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean
Maillard, etÂ al. 2022.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2207.04672</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du etÂ al. (2021)</span>
<span class="ltx_bibblock">
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
Jie Tang. 2021.

</span>
<span class="ltx_bibblock">Glm: General language model pretraining with autoregressive blank
infilling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2103.10360</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ebrahimi etÂ al. (2017)</span>
<span class="ltx_bibblock">
Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2017.

</span>
<span class="ltx_bibblock">Hotflip: White-box adversarial examples for text classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:1712.06751</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eco (1990)</span>
<span class="ltx_bibblock">
Umberto Eco. 1990.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">The Limits of Interpretation</em>.

</span>
<span class="ltx_bibblock">Indian University Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan etÂ al. (2021)</span>
<span class="ltx_bibblock">
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky,
Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav
Chaudhary, etÂ al. 2021.

</span>
<span class="ltx_bibblock">Beyond english-centric multilingual machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">The Journal of Machine Learning Research</em>, 22(1):4839â€“4886.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Farhad etÂ al. (2021)</span>
<span class="ltx_bibblock">
Akhbardeh Farhad, Arkhangorodsky Arkady, Biesialska Magdalena, Bojar
OndÅ™ej, Chatterjee Rajen, Chaudhary Vishrav, MartaÂ R Costa-jussa,
EspaÃ±a-Bonet Cristina, Fan Angela, Federmann Christian, etÂ al. 2021.

</span>
<span class="ltx_bibblock">Findings of the 2021 conference on machine translation (wmt21).

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the Sixth Conference on Machine Translation</em>,
pages 1â€“88. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng etÂ al. (2020)</span>
<span class="ltx_bibblock">
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang.
2020.

</span>
<span class="ltx_bibblock">Language-agnostic bert sentence embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2007.01852</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag etÂ al. (2022)</span>
<span class="ltx_bibblock">
Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart,
Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and AndrÃ©Â FT
Martins. 2022.

</span>
<span class="ltx_bibblock">Results of wmt22 metrics shared task: Stop using bleuâ€“neural metrics
are better and more robust.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the Seventh Conference on Machine Translation
(WMT)</em>, pages 46â€“68.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. (2022)</span>
<span class="ltx_bibblock">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek,
DaÂ Ju, Sanjana Krishnan, Marcâ€™Aurelio Ranzato, Francisco GuzmÃ¡n, and
Angela Fan. 2022.

</span>
<span class="ltx_bibblock">The flores-101 evaluation benchmark for low-resource and multilingual
machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Transactions of the Association for Computational Linguistics</em>,
10:522â€“538.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendy etÂ al. (2023)</span>
<span class="ltx_bibblock">
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu
Matsushita, YoungÂ Jin Kim, Mohamed Afify, and HanyÂ Hassan Awadalla. 2023.

</span>
<span class="ltx_bibblock">How good are gpt models at machine translation? a comprehensive
evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2302.09210</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoel (1971a)</span>
<span class="ltx_bibblock">
PaulÂ Gerhard Hoel. 1971a.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Elementary Statistics</em>, 3rd edition.

</span>
<span class="ltx_bibblock">Wiley series in probability and mathematical statistics. Wiley, New
York, Chichester.

</span>
<span class="ltx_bibblock">ISBN 0Â 471Â 40300.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoel (1971b)</span>
<span class="ltx_bibblock">
PaulÂ Gerhard Hoel. 1971b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Elementary Statistics</em>, 3rd edition, Wiley series in
probability and mathematical statistics, pages 19â€“33. Wiley, New York,
Chichester.

</span>
<span class="ltx_bibblock">ISBN 0Â 471Â 40300.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yichong Huang, Xiaocheng Feng, Xinwei Geng, and Bing Qin. 2022.

</span>
<span class="ltx_bibblock">Unifying the convergences in multilingual neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 6822â€“6835.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jespersen (1922)</span>
<span class="ltx_bibblock">
Otto Jespersen. 1922.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Language: Its Nature, Development, and Origin</em>.

</span>
<span class="ltx_bibblock">Allen and Unwin.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi etÂ al. (2022)</span>
<span class="ltx_bibblock">
Tom Kocmi, Rachel Bawden, OndÅ™ej Bojar, Anton Dvorkovich, Christian
Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz,
Barry Haddow, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Findings of the 2022 conference on machine translation (wmt22).

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the Seventh Conference on Machine Translation
(WMT)</em>, pages 1â€“45.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester etÂ al. (2021)</span>
<span class="ltx_bibblock">
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.

</span>
<span class="ltx_bibblock">The power of scale for parameter-efficient prompt tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2104.08691</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis etÂ al. (2019)</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.

</span>
<span class="ltx_bibblock">Bart: Denoising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:1910.13461</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Chen, and Jiajun Chen. 2023.

</span>
<span class="ltx_bibblock">Eliciting the translation ability of large language models via
multilingual finetuning with translation instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2305.15083</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2022)</span>
<span class="ltx_bibblock">
XiÂ Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen,
Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Few-shot learning with multilingual generative language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 9019â€“9052.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit
Bansal, and ColinÂ A Raffel. 2022.

</span>
<span class="ltx_bibblock">Few-shot parameter-efficient fine-tuning is better and cheaper than
in-context learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Advances in Neural Information Processing Systems</em>,
35:1950â€“1965.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Xiao Liu, Kaixuan Ji, Yicheng Fu, WengÂ Lam Tam, Zhengxiao Du, Zhilin Yang, and
Jie Tang. 2021.

</span>
<span class="ltx_bibblock">P-tuning v2: Prompt tuning can be comparable to fine-tuning
universally across scales and tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2110.07602</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and
Jie Tang. 2023.

</span>
<span class="ltx_bibblock">Gpt understands, too.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">AI Open</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff etÂ al. (2022)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella
Biderman, TevenÂ Le Scao, MÂ Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey
Schoelkopf, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Crosslingual generalization through multitask finetuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2211.01786</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, XuÂ Jiang, Diogo Almeida, Carroll Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, etÂ al.
2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Advances in Neural Information Processing Systems</em>,
35:27730â€“27744.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni etÂ al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 40th annual meeting of the Association
for Computational Linguistics</em>, pages 311â€“318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pires etÂ al. (2023)</span>
<span class="ltx_bibblock">
TelmoPessoa Pires, RobinM. Schmidt, Yi-Hsiu Liao, and Stephan Peitz. 2023.

</span>
<span class="ltx_bibblock">Learning language-specific layers for multilingual machine
translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">PopoviÄ‡ (2015)</span>
<span class="ltx_bibblock">
Maja PopoviÄ‡. 2015.

</span>
<span class="ltx_bibblock">chrf: character n-gram f-score for automatic mt evaluation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the tenth workshop on statistical machine
translation</em>, pages 392â€“395.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">PopoviÄ‡ (2017)</span>
<span class="ltx_bibblock">
Maja PopoviÄ‡. 2017.

</span>
<span class="ltx_bibblock">chrf++: words helping character n-grams.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the second conference on machine
translation</em>, pages 612â€“618.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.aclweb.org/anthology/W18-6319" title="">A call for clarity
in reporting BLEU scores</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the Third Conference on Machine Translation:
Research Papers</em>, pages 186â€“191, Belgium, Brussels. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2018)</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, etÂ al. 2018.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
Sutskever, etÂ al. 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">OpenAI blog</em>, 1(8):9.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel etÂ al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and PeterÂ J Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">The Journal of Machine Learning Research</em>, 21(1):5485â€“5551.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao etÂ al. (2022)</span>
<span class="ltx_bibblock">
TevenÂ Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana IliÄ‡,
Daniel Hesslow, Roman CastagnÃ©, AlexandraÂ Sasha Luccioni, FranÃ§ois
Yvon, Matthias GallÃ©, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2211.05100</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schioppa etÂ al. (2023)</span>
<span class="ltx_bibblock">
Andrea Schioppa, Xavier Garcia, and Orhan Firat. 2023.

</span>
<span class="ltx_bibblock">Cross-lingual supervision improves large language models
pre-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2305.11778</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sellam etÂ al. (2020)</span>
<span class="ltx_bibblock">
Thibault Sellam, Dipanjan Das, and AnkurÂ P Parikh. 2020.

</span>
<span class="ltx_bibblock">Bleurt: Learning robust metrics for text generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2004.04696</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin etÂ al. (2020)</span>
<span class="ltx_bibblock">
Taylor Shin, Yasaman Razeghi, RobertÂ L LoganÂ IV, Eric Wallace, and Sameer
Singh. 2020.

</span>
<span class="ltx_bibblock">Autoprompt: Eliciting knowledge from language models with
automatically generated prompts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2010.15980</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singer etÂ al. (1954â€“58)</span>
<span class="ltx_bibblock">
CharlesÂ Joseph Singer, E.Â J. Holmyard, and A.Â R. Hall, editors. 1954â€“58.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">A history of technology</em>.

</span>
<span class="ltx_bibblock">Oxford University Press, London.

</span>
<span class="ltx_bibblock">5 vol.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">StrÃ¶tgen and Gertz (2012)</span>
<span class="ltx_bibblock">
Jannik StrÃ¶tgen and Michael Gertz. 2012.

</span>
<span class="ltx_bibblock">Temporal tagging on different domains: Challenges, strategies, and
gold standards.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Proceedings of the Eight International Conference on
Language Resources and Evaluation (LRECâ€™12)</em>, pages 3746â€“3753, Istanbul,
Turkey. European Language Resource Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Superman etÂ al. (2000)</span>
<span class="ltx_bibblock">
S.Â Superman, B.Â Batman, C.Â Catwoman, and S.Â Spiderman. 2000.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Superheroes experiences with books</em>, 20th edition.

</span>
<span class="ltx_bibblock">The Phantom Editors Associates, Gotham City.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric
Hambro, Faisal Azhar, etÂ al. 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilar etÂ al. (2022)</span>
<span class="ltx_bibblock">
David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and
George Foster. 2022.

</span>
<span class="ltx_bibblock">Prompting palm for translation: Assessing strategies and performance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2211.09102</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wallace etÂ al. (2019)</span>
<span class="ltx_bibblock">
Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019.

</span>
<span class="ltx_bibblock">Universal adversarial triggers for attacking and analyzing nlp.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:1908.07125</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Thomas Wang, Adam Roberts, Daniel Hesslow, Teven LeÂ Scao, HyungÂ Won Chung,
IzÂ Beltagy, Julien Launay, and Colin Raffel. 2022.

</span>
<span class="ltx_bibblock">What language model architecture and pretraining objective works best
for zero-shot generalization?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">International Conference on Machine Learning</em>, pages
22964â€“22984. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, and Yoon
Kim. 2023.

</span>
<span class="ltx_bibblock">Multitask prompt tuning enables parameter-efficient transfer
learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">arXiv preprint arXiv:2303.02861</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, EdÂ Chi, QuocÂ V
Le, Denny Zhou, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Advances in Neural Information Processing Systems</em>,
35:24824â€“24837.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Liwei Wu, Shanbo Cheng, Mingxuan Wang, and Lei Li. 2021.

</span>
<span class="ltx_bibblock">Language tags matter for zero-shot neural machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2106.07930</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue etÂ al. (2020)</span>
<span class="ltx_bibblock">
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, and Colin Raffel. 2020.

</span>
<span class="ltx_bibblock">mt5: A massively multilingual pre-trained text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:2010.11934</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Biao Zhang, Barry Haddow, and Alexandra Birch. 2023.

</span>
<span class="ltx_bibblock">Prompting large language model for machine translation: A case study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2301.07069</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2020)</span>
<span class="ltx_bibblock">
Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. 2020.

</span>
<span class="ltx_bibblock">Improving massively multilingual neural machine translation and
zero-shot translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">arXiv preprint arXiv:2004.11867</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, XiÂ Victoria Lin, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2205.01068</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_para" id="p4">
<p class="ltx_p" id="p4.1"></p>
</div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Mar 11 14:07:23 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
