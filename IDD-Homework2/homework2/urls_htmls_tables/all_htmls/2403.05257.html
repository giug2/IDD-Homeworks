<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity</title>
<!--Generated on Fri Mar  8 08:46:02 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.05257v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S1" title="1. Introduction ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S2" title="2. Data Augmentation Techniques ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Data Augmentation Techniques</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S3" title="3. Experiments ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S3.SS1" title="3.1. Setup ‣ 3. Experiments ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S3.SS2" title="3.2. Datasets ‣ 3. Experiments ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S3.SS2.SSS0.Px1" title="Training ‣ 3.2. Datasets ‣ 3. Experiments ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title">Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S3.SS2.SSS0.Px2" title="Evaluation ‣ 3.2. Datasets ‣ 3. Experiments ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title">Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S3.SS3" title="3.3. Evaluation Protocol ‣ 3. Experiments ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Evaluation Protocol</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S3.SS4" title="3.4. Results ‣ 3. Experiments ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S4" title="4. Discussion ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S4.SS0.SSS0.Px1" title="Between cross-lingual transfer or machine translation, which is better? ‣ 4. Discussion ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title">Between cross-lingual transfer or machine translation, which is better?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S4.SS0.SSS0.Px2" title="Do these approaches yield performance on par with that of a state-of-the-art multilingual model? ‣ 4. Discussion ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title">Do these approaches yield performance on par with that of a state-of-the-art multilingual model?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S5" title="5. Related Work ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S5.SS0.SSS0.Px1" title="Learning Sentence Embeddings ‣ 5. Related Work ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title">Learning Sentence Embeddings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S5.SS0.SSS0.Px2" title="Cross-lingual Transferability of Multilingual Models ‣ 5. Related Work ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title">Cross-lingual Transferability of Multilingual Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S5.SS0.SSS0.Px3" title="Cross-lingual Transferability of Monolingual Models ‣ 5. Related Work ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title">Cross-lingual Transferability of Monolingual Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S6" title="6. Conclusion ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S7" title="7. Bibliographical References ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Bibliographical References</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A1" title="Appendix A Implementation Details ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A2" title="Appendix B Data Preprocessing ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Data Preprocessing</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A2.SS0.SSS0.Px1" title="NLI datasets ‣ Appendix B Data Preprocessing ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title">NLI datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A2.SS0.SSS0.Px2" title="Wikipedia datasets ‣ Appendix B Data Preprocessing ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title">Wikipedia datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A3" title="Appendix C Analysis of Sentence Embeddings ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Analysis of Sentence Embeddings</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A3.SS0.SSS0.Px1" title="Visualization ‣ Appendix C Analysis of Sentence Embeddings ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title">Visualization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A3.SS0.SSS0.Px2" title="Alignment and Uniformity ‣ Appendix C Analysis of Sentence Embeddings ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title">Alignment and Uniformity</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A4" title="Appendix D Replication Study ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Replication Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A5" title="Appendix E Ablation Study ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Ablation Study</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY-NC-ND 4.0</div><div id="watermark-tr">arXiv:2403.05257v1 [cs.CL] 08 Mar 2024</div></div>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Cross-lingual Transfer or Machine Translation? 
<br class="ltx_break"/>On Data Augmentation for Monolingual Semantic Textual Similarity</h1>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Learning better sentence embeddings leads to improved performance for natural language understanding tasks including semantic textual similarity (STS) and natural language inference (NLI).
As prior studies leverage large-scale labeled NLI datasets for fine-tuning masked language models to yield sentence embeddings, task performance for languages other than English is often left behind.
In this study, we directly compared two data augmentation techniques as potential solutions for monolingual STS:

<span class="ltx_inline-enumerate" id="S0.I1">
<span class="ltx_inline-item" id="S0.I1.i1"><span class="ltx_tag ltx_tag_inline-item">(a)</span> <span class="ltx_text ltx_font_italic" id="S0.I1.i1.1">cross-lingual transfer</span><span class="ltx_text" id="S0.I1.i1.2">that exploits English resources alone as training data to yield non-English sentence embeddings as zero-shot inference, and
</span></span>
<span class="ltx_inline-item" id="S0.I1.i2"><span class="ltx_tag ltx_tag_inline-item">(b)</span> <span class="ltx_text ltx_font_italic" id="S0.I1.i2.1">machine translation</span><span class="ltx_text" id="S0.I1.i2.2">that coverts English data into pseudo non-English training data in advance.
</span></span>
</span>
In our experiments on monolingual STS in Japanese and Korean, we find that the two data techniques yield performance on par.
Rather, we find a superiority of the Wikipedia domain over the NLI domain for these languages, in contrast to prior studies that focused on NLI as training data.
Combining our findings, we demonstrate that the cross-lingual transfer of Wikipedia data exhibits improved performance, and that native Wikipedia data can further improve performance for monolingual STS.

<br class="ltx_break"/>
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="id1.id1.1">Keywords: </span>sentence embeddings, cross-lingual transfer, machine translation</p>
</div>
<span class="ltx_ERROR undefined" id="id1">\NAT@set@cites</span>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text" id="p1.1.1"></span></p>
</div>
<div class="ltx_para ltx_align_center" id="p2">
<p class="ltx_p" id="p2.1"><span class="ltx_text ltx_font_bold" id="p2.1.1" style="font-size:144%;">Cross-lingual Transfer or Machine Translation?</span></p>
<p class="ltx_p" id="p2.2"><span class="ltx_text ltx_font_bold" id="p2.2.1" style="font-size:144%;">On Data Augmentation for Monolingual Semantic Textual Similarity</span></p>
</div>
<div class="ltx_para ltx_align_center" id="p3">
<table class="ltx_tabular ltx_align_top" id="p3.1">
<tr class="ltx_tr" id="p3.1.2">
<td class="ltx_td ltx_align_center" id="p3.1.2.1"><span class="ltx_text ltx_font_bold" id="p3.1.2.1.1" style="font-size:120%;">Sho Hoshino, Akihiko Kato, Soichiro Murakami, Peinan Zhang</span></td>
</tr>
<tr class="ltx_tr" id="p3.1.1">
<td class="ltx_td ltx_align_center" id="p3.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="20" id="p3.1.1.1.g1" src="extracted/5457243/affiliation.png" width="90"/></td>
</tr>
<tr class="ltx_tr" id="p3.1.3">
<td class="ltx_td ltx_align_center" id="p3.1.3.1">{hoshino_sho, kato_akihiko, murakami_soichiro, zhang_peinan}@cyberagent.co.jp</td>
</tr>
</table>
<br class="ltx_break"/>
<p class="ltx_p" id="p3.2"><span class="ltx_text ltx_font_italic" id="p3.2.1">Abstract content</span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Monolingual semantic textual similarity <cite class="ltx_cite ltx_citemacro_citep">(STS; Agirre et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib1" title="">2016</a>)</cite> has been used as a progress milestone for the learning of sentence embeddings <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib22" title="">2019</a>); Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib12" title="">2021</a>)</cite>.
Given two sentences in a target language, our task was to predict the similarity between the two sentences.
Monolingual STS is a core part of natural language understanding tasks <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib26" title="">2018</a>)</cite> and is related to natural language inference <cite class="ltx_cite ltx_citemacro_citep">(NLI; Dagan et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib8" title="">2005</a>)</cite> where the task was to predict whether one sentence entails another.
NLI data have been used for unsupervised STS without using any STS-specific data and are therefore considered to be suitable training data for monolingual STS.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Because such a quantity of training data is not always available in languages other than English, data augmentation techniques including

<span class="ltx_inline-enumerate" id="S1.I1">
<span class="ltx_inline-item" id="S1.I1.i1"><span class="ltx_tag ltx_tag_inline-item">(a)</span> <span class="ltx_text ltx_font_italic" id="S1.I1.i1.1">cross-lingual transfer</span><cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib7" title="">2018</a>); Gogoulou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib13" title="">2022</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib28" title="">2022</a>)</cite><span class="ltx_text" id="S1.I1.i1.2">and
</span></span>
<span class="ltx_inline-item" id="S1.I1.i2"><span class="ltx_tag ltx_tag_inline-item">(b)</span> <span class="ltx_text ltx_font_italic" id="S1.I1.i2.1">machine translation</span><cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib7" title="">2018</a>); Ham et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib15" title="">2020</a>); Yanaka and Mineshima (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib31" title="">2022</a>)</cite></span>
</span>
have been extensively applied.
However, aside from the monumental prior study on NLI <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib7" title="">2018</a>)</cite>, there have been few to no comprehensive studies that directly compared the two different data augmentation techniques particularly on monolingual STS, which differs from multilingual STS <cite class="ltx_cite ltx_citemacro_cite">Cer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib5" title="">2017</a>)</cite> in that the given two sentences are in the same language.
Therefore, we investigated the following research questions to search for the most suitable data augmentation technique for monolingual STS, which would have the greatest need for monolingual data:

<span class="ltx_inline-description" id="S1.I2">
<span class="ltx_inline-item" id="S1.I2.ix1"><span class="ltx_tag ltx_tag_inline-item"><span class="ltx_text ltx_font_bold" id="S1.I2.ix1.1.1.1">RQ1</span></span> <span class="ltx_text ltx_font_italic" id="S1.I2.ix1.2">Between cross-lingual transfer or machine translation, which is better?</span></span>
<span class="ltx_inline-item" id="S1.I2.ix2"><span class="ltx_tag ltx_tag_inline-item"><span class="ltx_text ltx_font_bold" id="S1.I2.ix2.1.1.1">RQ2</span></span> <span class="ltx_text ltx_font_italic" id="S1.I2.ix2.2">Do these approaches yield performance on par with that of a state-of-the-art multilingual model?</span></span>
</span></p>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S1.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="185" id="S1.F1.sf1.g1" src="extracted/5457243/figure/eye-catch1.png" width="269"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>cross-lingual transfer</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S1.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="243" id="S1.F1.sf2.g1" src="extracted/5457243/figure/eye-catch2.png" width="269"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>machine translation</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of the two different data augmentation techniques applied from English to non-English.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this study, we empirically evaluated the two different data augmentation techniques using Japanese and Korean, which can be seen as relatively low-resourced and linguistically dissimilar languages when compared with English, and thus are challenging for the two techniques.
We used unsupervised multilingual SimCSE <cite class="ltx_cite ltx_citemacro_citep">(mSimCSE; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib28" title="">2022</a>)</cite> as our test bed, which is a multilingual extension of unsupervised SimCSE <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib12" title="">2021</a>)</cite> that uses sentence-pair contrastive learning for self-supervised learning of unlabeled data.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Specifically, we trained mSimCSE models in two ways, each corresponding to the two data augmentation techniques:

<span class="ltx_inline-enumerate" id="S1.I3">
<span class="ltx_inline-item" id="S1.I3.i1"><span class="ltx_tag ltx_tag_inline-item">(a)</span> <span class="ltx_text" id="S1.I3.i1.1">by using English training data alone as cross-lingual transfer, and
</span></span>
<span class="ltx_inline-item" id="S1.I3.i2"><span class="ltx_tag ltx_tag_inline-item">(b)</span> <span class="ltx_text" id="S1.I3.i2.1">by using machine-translated data from English into Korean and Japanese.
</span></span>
</span>
We also compared trained mSimCSE models with LaBSE <cite class="ltx_cite ltx_citemacro_cite">Feng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib10" title="">2022</a>)</cite>, a state-of-the-art multilingual model.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In our experiments on monolingual STS, we demonstrate that cross-lingual transfer achieves performance on par with that of machine translation (<span class="ltx_text ltx_font_bold" id="S1.p5.1.1">RQ1</span>).
We further demonstrate that the combination of cross-lingual transfer and Wikipedia domain data exhibits the best performance outperforming or comparable to that of LaBSE (<span class="ltx_text ltx_font_bold" id="S1.p5.1.2">RQ2</span>).
In contrast to prior studies, we also observed that Wikipedia domain data can be used as an alternative drop-in replacement for NLI domain data when used as unlabeled training data for monolingual STS.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Since Wikipedia domain data is easier to obtain than NLI domain data, our results suggest a recipe for training better sentence embeddings using a large-scale multilingual Wikipedia dataset.
As such a pilot study, we actually report improved results using a Japanese portion of the Wikipedia data.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   Data Augmentation Techniques</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Data augmentation <cite class="ltx_cite ltx_citemacro_cite">Feng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib11" title="">2021</a>)</cite> is a generic strategy to deal with relatively low-resourced situations in a target language to increase the number of training examples (Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">1</span></a>).
We explain two different data augmentation techniques as follows.</p>
<dl class="ltx_description" id="S2.I1">
<dt class="ltx_item" id="S2.I1.ix1"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S2.I1.ix1.1.1.1">Cross-lingual transfer</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S2.I1.ix1.p1">
<p class="ltx_p" id="S2.I1.ix1.p1.1">leverages training data in other languages but not in the target language.
We conduct fine-tuning of pretrained masked language models using the available data, without leveraging target language resources.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>In this context, the pretrained models must be capable of processing a target language and its vocabulary, unlike cross-lingual transfer to unseen languages <cite class="ltx_cite ltx_citemacro_cite">Artetxe et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib2" title="">2020</a>)</cite>.</span></span></span>
After that, we use the trained model to perform zero-shot inference in the target language.
This cost-effective approach has virtually no cost for obtaining data.
<cite class="ltx_cite ltx_citemacro_citet">Conneau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib7" title="">2018</a>)</cite> studied the performance of such cross-lingual transfer from English to other languages using a multilingual NLI dataset (XNLI).</p>
</div>
</dd>
<dt class="ltx_item" id="S2.I1.ix2"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S2.I1.ix2.1.1.1">Machine translation</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S2.I1.ix2.p1">
<p class="ltx_p" id="S2.I1.ix2.p1.1">leverages the same data, but this time by using a machine translation system to translate it from other languages into the target language.
We then perform fine-tuning of pretrained masked language models using the machine-translated data.
After that, we use the trained model to perform normal inference in the target language, unlike cross-lingual transfer.
This pay-as-you-go approach has two variations.
<cite class="ltx_cite ltx_citemacro_citet">Conneau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib7" title="">2018</a>)</cite> created the training portion of the XNLI dataset using a neural machine translation system (<span class="ltx_text ltx_font_smallcaps" id="S2.I1.ix2.p1.1.1">Translate Train</span>).
They also attained comparable results by translating test data only at runtime (<span class="ltx_text ltx_font_smallcaps" id="S2.I1.ix2.p1.1.2">Translate Test</span>).</p>
</div>
</dd>
</dl>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Experiments</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We compared the performance of cross-lingual transfer and machine translation on unsupervised learning for monolingual STS <cite class="ltx_cite ltx_citemacro_cite">Agirre et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib1" title="">2016</a>)</cite>.
Given two sentences in a target language, our task was to predict the similarity between the two sentences using unlabeled training data.
This task differs from supervised learning that exploits labeled data, because our focus is on the relatively low-resourced situations in Japanese and Korean.
The task also differs from multilingual STS in that the given two sentences are in the same language.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.1.   Setup</h3>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.1">
<tr class="ltx_tr" id="S3.T1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1"><span class="ltx_text" id="S3.T1.1.1.1.1" style="font-size:90%;">Hyperparameter</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.2"><span class="ltx_text" id="S3.T1.1.1.2.1" style="font-size:90%;">Values</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1"><span class="ltx_text" id="S3.T1.1.2.1.1" style="font-size:90%;"># Epochs</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2"><span class="ltx_text" id="S3.T1.1.2.2.1" style="font-size:90%;">1</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3">
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.1"><span class="ltx_text" id="S3.T1.1.3.1.1" style="font-size:90%;">Learning rate</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2"><span class="ltx_text" id="S3.T1.1.3.2.1" style="font-size:90%;">5e-5</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4">
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.1"><span class="ltx_text" id="S3.T1.1.4.1.1" style="font-size:90%;">Warmup rate</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.2"><span class="ltx_text" id="S3.T1.1.4.2.1" style="font-size:90%;">10%</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.5.1"><span class="ltx_text" id="S3.T1.1.5.1.1" style="font-size:90%;">Batch size</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.5.2"><span class="ltx_text" id="S3.T1.1.5.2.1" style="font-size:90%;">{32, 64, 128}</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Hyperparameters.
</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">More specifically, we investigated the performance of sentence embeddings obtained as a result of unsupervised learning for monolingual STS.
Following mSimCSE <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib28" title="">2022</a>)</cite>, we fine-tuned multilingual pretrained masked language models by feeding various training data as different data augmentation techniques.
Since XLM-R <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib6" title="">2020</a>)</cite> is used as a common baseline in the relevant literature, we used XLM-R as the multilingual base model and performed fine-tuning using unsupervised SimCSE <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib12" title="">2021</a>)</cite>.
We compared our models with LaBSE <cite class="ltx_cite ltx_citemacro_cite">Feng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib10" title="">2022</a>)</cite> without fine-tuning as a strong fully supervised baseline.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">We used random hardware available at Google Colaboratory<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://colab.research.google.com/" title="">https://colab.research.google.com/</a></span></span></span> and Sentence-Transformers 2.2.0 <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib22" title="">2019</a>)</cite> for our implementation.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The details of our implementation are described in the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A1" title="Appendix A Implementation Details ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">A</span></a>, followed by the replication study of unsupervised SimCSE reported in the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A4" title="Appendix D Replication Study ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">D</span></a>.</span></span></span>
We followed the hyperparameters used in Sentence-Transformers as best practices, unless listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S3.T1" title="Table 1 ‣ 3.1. Setup ‣ 3. Experiments ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.2.   Datasets</h3>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T2.1">
<tr class="ltx_tr" id="S3.T2.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.1"><span class="ltx_text" id="S3.T2.1.1.1.1" style="font-size:90%;">Name</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.2"><span class="ltx_text" id="S3.T2.1.1.2.1" style="font-size:90%;">Size</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.3"><span class="ltx_text" id="S3.T2.1.1.3.1" style="font-size:90%;">Domain</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.4"><span class="ltx_text" id="S3.T2.1.1.4.1" style="font-size:90%;">Lang.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.5"><span class="ltx_text" id="S3.T2.1.1.5.1" style="font-size:90%;">Sources</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1"><span class="ltx_text" id="S3.T2.1.2.1.1" style="font-size:90%;">SNLI</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.2"><span class="ltx_text" id="S3.T2.1.2.2.1" style="font-size:90%;">0.5M</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.3"><span class="ltx_text" id="S3.T2.1.2.3.1" style="font-size:90%;">NLI</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.4"><span class="ltx_text" id="S3.T2.1.2.4.1" style="font-size:90%;">en</span></td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.2.5"></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3">
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.1"><span class="ltx_text" id="S3.T2.1.3.1.1" style="font-size:90%;">MNLI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2"><span class="ltx_text" id="S3.T2.1.3.2.1" style="font-size:90%;">0.5M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.3"><span class="ltx_text" id="S3.T2.1.3.3.1" style="font-size:90%;">NLI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.4"><span class="ltx_text" id="S3.T2.1.3.4.1" style="font-size:90%;">en</span></td>
<td class="ltx_td" id="S3.T2.1.3.5"></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4">
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.1"><span class="ltx_text" id="S3.T2.1.4.1.1" style="font-size:90%;">JSNLI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.2"><span class="ltx_text" id="S3.T2.1.4.2.1" style="font-size:90%;">0.5M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.3"><span class="ltx_text" id="S3.T2.1.4.3.1" style="font-size:90%;">NLI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.4">
<span class="ltx_text" id="S3.T2.1.4.4.1" style="font-size:90%;">MT</span><sub class="ltx_sub" id="S3.T2.1.4.4.2"><span class="ltx_text" id="S3.T2.1.4.4.2.1" style="font-size:90%;">ja</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.5"><span class="ltx_text" id="S3.T2.1.4.5.1" style="font-size:90%;">SNLI</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5">
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.1"><span class="ltx_text" id="S3.T2.1.5.1.1" style="font-size:90%;">KorNLI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.2"><span class="ltx_text" id="S3.T2.1.5.2.1" style="font-size:90%;">1.0M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.3"><span class="ltx_text" id="S3.T2.1.5.3.1" style="font-size:90%;">NLI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.4">
<span class="ltx_text" id="S3.T2.1.5.4.1" style="font-size:90%;">MT</span><sub class="ltx_sub" id="S3.T2.1.5.4.2"><span class="ltx_text" id="S3.T2.1.5.4.2.1" style="font-size:90%;">ko</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.5"><span class="ltx_text" id="S3.T2.1.5.5.1" style="font-size:90%;">SNLI, MNLI</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6">
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.1"><span class="ltx_text" id="S3.T2.1.6.1.1" style="font-size:90%;">Wiki</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.2"><span class="ltx_text" id="S3.T2.1.6.2.1" style="font-size:90%;">1.0M</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.3"><span class="ltx_text" id="S3.T2.1.6.3.1" style="font-size:90%;">Wikipedia</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.4"><span class="ltx_text" id="S3.T2.1.6.4.1" style="font-size:90%;">en</span></td>
<td class="ltx_td" id="S3.T2.1.6.5"></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.7">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.7.1"><span class="ltx_text" id="S3.T2.1.7.1.1" style="font-size:90%;">Wiki-40B</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.7.2"><span class="ltx_text" id="S3.T2.1.7.2.1" style="font-size:90%;">1.0M</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.7.3"><span class="ltx_text" id="S3.T2.1.7.3.1" style="font-size:90%;">Wikipedia</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.7.4"><span class="ltx_text" id="S3.T2.1.7.4.1" style="font-size:90%;">ja</span></td>
<td class="ltx_td ltx_border_bb" id="S3.T2.1.7.5"></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>
The statistics of the various training datasets we used.
The size column reports the approximate number of training examples.
MT denotes pseudo data machine-translated from the corresponding resources shown in the last column.
</figcaption>
</figure>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Training</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S3.T2" title="Table 2 ‣ 3.2. Datasets ‣ 3. Experiments ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes various datasets in English, Japanese, and Korean that are primarily used as our training data.
Following prior studies on monolingual STS <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib22" title="">2019</a>; Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib12" title="">2021</a>)</cite>, we used NLI datasets that contain premises, hypotheses, and their relationship labels such as entailment, contradiction, and neutral <cite class="ltx_cite ltx_citemacro_cite">Dagan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib8" title="">2005</a>)</cite>.
Specifically, we used premises and hypotheses as training examples of unsupervised learning, while simply discarding NLI labels.
We also used English and Japanese Wikipedia.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The details of data preprocessing are described in the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A2" title="Appendix B Data Preprocessing ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">B</span></a>.</span></span></span></p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p2.1">For fine-tuning in English, we used the Stanford Natural Language Inference corpus <cite class="ltx_cite ltx_citemacro_citep">(SNLI; Bowman et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib4" title="">2015</a>)</cite> and the Multi-Genre Natural Language Inference corpus <cite class="ltx_cite ltx_citemacro_citep">(MNLI; Williams et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib29" title="">2018</a>)</cite> as well as English Wikipedia <cite class="ltx_cite ltx_citemacro_citep">(Wiki; Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib12" title="">2021</a>)</cite>.
To match the data size of NLI datasets with its Wikipedia counterpart (1 million sentences), we used them in two ways<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>An ablation study with different data sizes is reported in the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A5" title="Appendix E Ablation Study ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">E</span></a>.</span></span></span>:

<span class="ltx_inline-enumerate" id="S3.I1">
<span class="ltx_inline-item" id="S3.I1.i1"><span class="ltx_tag ltx_tag_inline-item">(i)</span> <span class="ltx_text" id="S3.I1.i1.1">only the premises portions of SNLI and MNLI (<span class="ltx_text ltx_font_bold" id="S3.I1.i1.1.1">NLI</span>; 942,854 sentences), and
</span></span>
<span class="ltx_inline-item" id="S3.I1.i2"><span class="ltx_tag ltx_tag_inline-item">(ii)</span> <span class="ltx_text" id="S3.I1.i2.1">both the premise and hypothesis portions of SNLI (<span class="ltx_text ltx_font_bold" id="S3.I1.i2.1.1">SNLI</span>; 1,100,304 sentences).
</span></span>
</span></p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p3">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p3.1">For fine-tuning in Japanese and Korean, machine-translated English datasets are used instead.
For instance, the JSNLI dataset <cite class="ltx_cite ltx_citemacro_cite">Yoshikoshi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib32" title="">2020</a>)</cite> contains machine-translated SNLI and the KorNLI dataset <cite class="ltx_cite ltx_citemacro_cite">Ham et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib15" title="">2020</a>)</cite> contains machine-translated SNLI and MNLI, respectively.
We also used Japanese Wikipedia data created from multilingual Wikipedia <cite class="ltx_cite ltx_citemacro_citep">(Wiki-40B; Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib14" title="">2020</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Evaluation</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">We used the following labeled STS datasets as our evaluation data: STS-B <cite class="ltx_cite ltx_citemacro_cite">Cer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib5" title="">2017</a>)</cite> for English; JSICK-STS <cite class="ltx_cite ltx_citemacro_cite">Yanaka and Mineshima (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib31" title="">2022</a>)</cite>, a human-translated SICK <cite class="ltx_cite ltx_citemacro_cite">Marelli et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib20" title="">2014</a>)</cite>, and JGLUE-JSTS <cite class="ltx_cite ltx_citemacro_cite">Kurihara et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib18" title="">2022</a>)</cite> for Japanese; and KorSTS <cite class="ltx_cite ltx_citemacro_cite">Ham et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib15" title="">2020</a>)</cite>, a human-translated STS-B <cite class="ltx_cite ltx_citemacro_cite">Cer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib5" title="">2017</a>)</cite>, and KLUE-STS <cite class="ltx_cite ltx_citemacro_cite">Park et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib21" title="">2021</a>)</cite> for Korean.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.3.   Evaluation Protocol</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Following <cite class="ltx_cite ltx_citemacro_citet">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib12" title="">2021</a>)</cite>, we evaluate sentence embeddings by measuring the correlation between the human labels <math alttext="[0,5]" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.2"><semantics id="S3.SS3.p1.1.m1.2a"><mrow id="S3.SS3.p1.1.m1.2.3.2" xref="S3.SS3.p1.1.m1.2.3.1.cmml"><mo id="S3.SS3.p1.1.m1.2.3.2.1" stretchy="false" xref="S3.SS3.p1.1.m1.2.3.1.cmml">[</mo><mn id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">0</mn><mo id="S3.SS3.p1.1.m1.2.3.2.2" xref="S3.SS3.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS3.p1.1.m1.2.2" xref="S3.SS3.p1.1.m1.2.2.cmml">5</mn><mo id="S3.SS3.p1.1.m1.2.3.2.3" stretchy="false" xref="S3.SS3.p1.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.2b"><interval closure="closed" id="S3.SS3.p1.1.m1.2.3.1.cmml" xref="S3.SS3.p1.1.m1.2.3.2"><cn id="S3.SS3.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS3.p1.1.m1.1.1">0</cn><cn id="S3.SS3.p1.1.m1.2.2.cmml" type="integer" xref="S3.SS3.p1.1.m1.2.2">5</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.2c">[0,5]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.2d">[ 0 , 5 ]</annotation></semantics></math> and the cosine distance between two given sentences, using Spearman’s rank correlation coefficient <cite class="ltx_cite ltx_citemacro_cite">Spearman (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib24" title="">1904</a>)</cite>.
However, instead of using test portions of the labeled STS datasets, we used development portions as our evaluation data to avoid the unnecessary overfitting to English development data <cite class="ltx_cite ltx_citemacro_cite">Keung et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib16" title="">2020</a>)</cite>, which would caused by applying the standard evaluation protocol to cross-lingual transfer experiments.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T3.1">
<tr class="ltx_tr" id="S3.T3.1.1">
<td class="ltx_td ltx_border_tt" id="S3.T3.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S3.T3.1.1.2"><span class="ltx_text" id="S3.T3.1.1.2.1" style="font-size:90%;">Japanese</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S3.T3.1.1.3"><span class="ltx_text" id="S3.T3.1.1.3.1" style="font-size:90%;">Korean</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.2">
<td class="ltx_td ltx_align_center" id="S3.T3.1.2.1"><span class="ltx_text" id="S3.T3.1.2.1.1" style="font-size:90%;">Models</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.2.2">
<span class="ltx_text" id="S3.T3.1.2.2.1"></span><span class="ltx_text" id="S3.T3.1.2.2.2" style="font-size:90%;"> </span><span class="ltx_text" id="S3.T3.1.2.2.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.2.2.3.1">
<span class="ltx_tr" id="S3.T3.1.2.2.3.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.1.2.2.3.1.1.1">JSICK-</span></span>
<span class="ltx_tr" id="S3.T3.1.2.2.3.1.2">
<span class="ltx_td ltx_align_center" id="S3.T3.1.2.2.3.1.2.1">STS</span></span>
</span></span><span class="ltx_text" id="S3.T3.1.2.2.4" style="font-size:90%;"> </span><span class="ltx_text" id="S3.T3.1.2.2.5"></span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.2.3">
<span class="ltx_text" id="S3.T3.1.2.3.1"></span><span class="ltx_text" id="S3.T3.1.2.3.2" style="font-size:90%;"> </span><span class="ltx_text" id="S3.T3.1.2.3.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.2.3.3.1">
<span class="ltx_tr" id="S3.T3.1.2.3.3.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.1.2.3.3.1.1.1">JGLUE-</span></span>
<span class="ltx_tr" id="S3.T3.1.2.3.3.1.2">
<span class="ltx_td ltx_align_center" id="S3.T3.1.2.3.3.1.2.1">JSTS</span></span>
</span></span><span class="ltx_text" id="S3.T3.1.2.3.4" style="font-size:90%;"> </span><span class="ltx_text" id="S3.T3.1.2.3.5"></span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.2.4"><span class="ltx_text" id="S3.T3.1.2.4.1" style="font-size:90%;">KorSTS</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.2.5">
<span class="ltx_text" id="S3.T3.1.2.5.1"></span><span class="ltx_text" id="S3.T3.1.2.5.2" style="font-size:90%;"> </span><span class="ltx_text" id="S3.T3.1.2.5.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.2.5.3.1">
<span class="ltx_tr" id="S3.T3.1.2.5.3.1.1">
<span class="ltx_td ltx_align_center" id="S3.T3.1.2.5.3.1.1.1">KLUE-</span></span>
<span class="ltx_tr" id="S3.T3.1.2.5.3.1.2">
<span class="ltx_td ltx_align_center" id="S3.T3.1.2.5.3.1.2.1">STS</span></span>
</span></span><span class="ltx_text" id="S3.T3.1.2.5.4" style="font-size:90%;"> </span><span class="ltx_text" id="S3.T3.1.2.5.5"></span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="5" id="S3.T3.1.3.1">
<span class="ltx_text ltx_font_italic" id="S3.T3.1.3.1.1" style="font-size:90%;">Cross-lingual transfer</span><span class="ltx_text" id="S3.T3.1.3.1.2" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.4">
<td class="ltx_td ltx_align_left" id="S3.T3.1.4.1">
<span class="ltx_text" id="S3.T3.1.4.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="S3.T3.1.4.1.2"><span class="ltx_text" id="S3.T3.1.4.1.2.1" style="font-size:90%;">en</span></sub>
</td>
<td class="ltx_td" id="S3.T3.1.4.2"></td>
<td class="ltx_td" id="S3.T3.1.4.3"></td>
<td class="ltx_td" id="S3.T3.1.4.4"></td>
<td class="ltx_td" id="S3.T3.1.4.5"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.5">
<td class="ltx_td ltx_align_left" id="S3.T3.1.5.1"><span class="ltx_text" id="S3.T3.1.5.1.1" style="font-size:90%;">+ NLI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.5.2"><span class="ltx_text" id="S3.T3.1.5.2.1" style="font-size:90%;">79.55</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.5.3"><span class="ltx_text" id="S3.T3.1.5.3.1" style="font-size:90%;">74.12</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.5.4"><span class="ltx_text" id="S3.T3.1.5.4.1" style="font-size:90%;">74.68</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.5.5"><span class="ltx_text" id="S3.T3.1.5.5.1" style="font-size:90%;">72.87</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.6">
<td class="ltx_td ltx_align_left" id="S3.T3.1.6.1"><span class="ltx_text" id="S3.T3.1.6.1.1" style="font-size:90%;">+ SNLI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.6.2"><span class="ltx_text ltx_font_bold" id="S3.T3.1.6.2.1" style="font-size:90%;">80.31</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.6.3"><span class="ltx_text" id="S3.T3.1.6.3.1" style="font-size:90%;">74.73</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.6.4"><span class="ltx_text" id="S3.T3.1.6.4.1" style="font-size:90%;">75.21</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.6.5"><span class="ltx_text" id="S3.T3.1.6.5.1" style="font-size:90%;">65.83</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.7">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="5" id="S3.T3.1.7.1">
<span class="ltx_text ltx_font_italic" id="S3.T3.1.7.1.1" style="font-size:90%;">Machine translation</span><span class="ltx_text" id="S3.T3.1.7.1.2" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.8">
<td class="ltx_td ltx_align_left" id="S3.T3.1.8.1">
<span class="ltx_text" id="S3.T3.1.8.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="S3.T3.1.8.1.2"><span class="ltx_text" id="S3.T3.1.8.1.2.1" style="font-size:90%;">MT</span><sub class="ltx_sub" id="S3.T3.1.8.1.2.2"><span class="ltx_text" id="S3.T3.1.8.1.2.2.1" style="font-size:90%;">ja</span></sub></sub>
</td>
<td class="ltx_td" id="S3.T3.1.8.2"></td>
<td class="ltx_td" id="S3.T3.1.8.3"></td>
<td class="ltx_td" id="S3.T3.1.8.4"></td>
<td class="ltx_td" id="S3.T3.1.8.5"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.9">
<td class="ltx_td ltx_align_left" id="S3.T3.1.9.1"><span class="ltx_text" id="S3.T3.1.9.1.1" style="font-size:90%;">+ JSNLI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.9.2"><span class="ltx_text" id="S3.T3.1.9.2.1" style="font-size:90%;">78.41</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.9.3"><span class="ltx_text ltx_font_bold" id="S3.T3.1.9.3.1" style="font-size:90%;">75.55</span></td>
<td class="ltx_td" id="S3.T3.1.9.4"></td>
<td class="ltx_td" id="S3.T3.1.9.5"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.10">
<td class="ltx_td ltx_align_left" id="S3.T3.1.10.1">
<span class="ltx_text" id="S3.T3.1.10.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="S3.T3.1.10.1.2"><span class="ltx_text" id="S3.T3.1.10.1.2.1" style="font-size:90%;">MT</span><sub class="ltx_sub" id="S3.T3.1.10.1.2.2"><span class="ltx_text" id="S3.T3.1.10.1.2.2.1" style="font-size:90%;">kr</span></sub></sub>
</td>
<td class="ltx_td" id="S3.T3.1.10.2"></td>
<td class="ltx_td" id="S3.T3.1.10.3"></td>
<td class="ltx_td" id="S3.T3.1.10.4"></td>
<td class="ltx_td" id="S3.T3.1.10.5"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.11">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T3.1.11.1"><span class="ltx_text" id="S3.T3.1.11.1.1" style="font-size:90%;">+ KorNLI</span></td>
<td class="ltx_td ltx_border_bb" id="S3.T3.1.11.2"></td>
<td class="ltx_td ltx_border_bb" id="S3.T3.1.11.3"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.11.4"><span class="ltx_text ltx_font_bold" id="S3.T3.1.11.4.1" style="font-size:90%;">75.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.11.5"><span class="ltx_text ltx_font_bold" id="S3.T3.1.11.5.1" style="font-size:90%;">74.45</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>
Comparisons of cross-lingual transfer and machine translation as data augmentation.
We report Spearman’s correlation [%].
Higher is better.
</figcaption>
</figure>
<figure class="ltx_table" id="S3.T4">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_flex_size_2 ltx_align_center" id="S3.T4.sf1">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.sf1.1">
<tr class="ltx_tr" id="S3.T4.sf1.1.1">
<td class="ltx_td ltx_border_tt" id="S3.T4.sf1.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.sf1.1.1.2"><span class="ltx_text" id="S3.T4.sf1.1.1.2.1" style="font-size:90%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.sf1.1.2">
<td class="ltx_td ltx_align_center" id="S3.T4.sf1.1.2.1"><span class="ltx_text" id="S3.T4.sf1.1.2.1.1" style="font-size:90%;">Models</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.sf1.1.2.2"><span class="ltx_text" id="S3.T4.sf1.1.2.2.1" style="font-size:90%;">STS-B</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.sf1.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S3.T4.sf1.1.3.1">
<span class="ltx_text ltx_font_italic" id="S3.T4.sf1.1.3.1.1" style="font-size:90%;">Unsupervised</span><span class="ltx_text" id="S3.T4.sf1.1.3.1.2" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.sf1.1.4">
<td class="ltx_td ltx_align_left" id="S3.T4.sf1.1.4.1">
<span class="ltx_text" id="S3.T4.sf1.1.4.1.1" style="font-size:90%;">RoBERTa</span><sub class="ltx_sub" id="S3.T4.sf1.1.4.1.2"><span class="ltx_text" id="S3.T4.sf1.1.4.1.2.1" style="font-size:90%;">large</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.sf1.1.4.2"><span class="ltx_text" id="S3.T4.sf1.1.4.2.1" style="font-size:90%;">56.29</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.sf1.1.5">
<td class="ltx_td ltx_align_left" id="S3.T4.sf1.1.5.1"><span class="ltx_text" id="S3.T4.sf1.1.5.1.1" style="font-size:90%;">SimCSE</span></td>
<td class="ltx_td" id="S3.T4.sf1.1.5.2"></td>
</tr>
<tr class="ltx_tr" id="S3.T4.sf1.1.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T4.sf1.1.6.1"><span class="ltx_text" id="S3.T4.sf1.1.6.1.1" style="font-size:90%;">+ Wiki</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.sf1.1.6.2"><span class="ltx_text ltx_font_bold" id="S3.T4.sf1.1.6.2.1" style="font-size:90%;">85.95</span></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">(a) </span>Monolingual models.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_flex_size_2 ltx_align_center" id="S3.T4.sf2">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.sf2.1">
<tr class="ltx_tr" id="S3.T4.sf2.1.1">
<td class="ltx_td ltx_border_tt" id="S3.T4.sf2.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.sf2.1.1.2"><span class="ltx_text" id="S3.T4.sf2.1.1.2.1" style="font-size:90%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.sf2.1.2">
<td class="ltx_td ltx_align_center" id="S3.T4.sf2.1.2.1"><span class="ltx_text" id="S3.T4.sf2.1.2.1.1" style="font-size:90%;">Models</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.sf2.1.2.2"><span class="ltx_text" id="S3.T4.sf2.1.2.2.1" style="font-size:90%;">STS-B</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.sf2.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S3.T4.sf2.1.3.1">
<span class="ltx_text ltx_font_italic" id="S3.T4.sf2.1.3.1.1" style="font-size:90%;">Unsupervised</span><span class="ltx_text" id="S3.T4.sf2.1.3.1.2" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.sf2.1.4">
<td class="ltx_td ltx_align_left" id="S3.T4.sf2.1.4.1">
<span class="ltx_text" id="S3.T4.sf2.1.4.1.1" style="font-size:90%;">XLM-R</span><sub class="ltx_sub" id="S3.T4.sf2.1.4.1.2"><span class="ltx_text" id="S3.T4.sf2.1.4.1.2.1" style="font-size:90%;">large</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.sf2.1.4.2"><span class="ltx_text" id="S3.T4.sf2.1.4.2.1" style="font-size:90%;">43.54</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.sf2.1.5">
<td class="ltx_td ltx_align_left" id="S3.T4.sf2.1.5.1">
<span class="ltx_text" id="S3.T4.sf2.1.5.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="S3.T4.sf2.1.5.1.2"><span class="ltx_text" id="S3.T4.sf2.1.5.1.2.1" style="font-size:90%;">en</span></sub>
</td>
<td class="ltx_td" id="S3.T4.sf2.1.5.2"></td>
</tr>
<tr class="ltx_tr" id="S3.T4.sf2.1.6">
<td class="ltx_td ltx_align_left" id="S3.T4.sf2.1.6.1"><span class="ltx_text" id="S3.T4.sf2.1.6.1.1" style="font-size:90%;">+ Wiki</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.sf2.1.6.2"><span class="ltx_text ltx_font_bold" id="S3.T4.sf2.1.6.2.1" style="font-size:90%;">83.54</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.sf2.1.7">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T4.sf2.1.7.1"><span class="ltx_text" id="S3.T4.sf2.1.7.1.1" style="font-size:90%;">+ NLI</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.sf2.1.7.2"><span class="ltx_text" id="S3.T4.sf2.1.7.2.1" style="font-size:90%;">76.98</span></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">(b) </span>Multilingual models.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparisons of different data domains.
</figcaption>
</figure>
<figure class="ltx_table" id="S3.T5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T5.1">
<tr class="ltx_tr" id="S3.T5.1.1">
<td class="ltx_td ltx_border_tt" id="S3.T5.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S3.T5.1.1.2"><span class="ltx_text" id="S3.T5.1.1.2.1" style="font-size:90%;">Japanese</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S3.T5.1.1.3"><span class="ltx_text" id="S3.T5.1.1.3.1" style="font-size:90%;">Korean</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.2">
<td class="ltx_td ltx_align_center" id="S3.T5.1.2.1"><span class="ltx_text" id="S3.T5.1.2.1.1" style="font-size:90%;">Models</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.1.2.2">
<span class="ltx_text" id="S3.T5.1.2.2.1"></span><span class="ltx_text" id="S3.T5.1.2.2.2" style="font-size:90%;"> </span><span class="ltx_text" id="S3.T5.1.2.2.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S3.T5.1.2.2.3.1">
<span class="ltx_tr" id="S3.T5.1.2.2.3.1.1">
<span class="ltx_td ltx_align_center" id="S3.T5.1.2.2.3.1.1.1">JSICK-</span></span>
<span class="ltx_tr" id="S3.T5.1.2.2.3.1.2">
<span class="ltx_td ltx_align_center" id="S3.T5.1.2.2.3.1.2.1">STS</span></span>
</span></span><span class="ltx_text" id="S3.T5.1.2.2.4" style="font-size:90%;"> </span><span class="ltx_text" id="S3.T5.1.2.2.5"></span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.1.2.3">
<span class="ltx_text" id="S3.T5.1.2.3.1"></span><span class="ltx_text" id="S3.T5.1.2.3.2" style="font-size:90%;"> </span><span class="ltx_text" id="S3.T5.1.2.3.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S3.T5.1.2.3.3.1">
<span class="ltx_tr" id="S3.T5.1.2.3.3.1.1">
<span class="ltx_td ltx_align_center" id="S3.T5.1.2.3.3.1.1.1">JGLUE-</span></span>
<span class="ltx_tr" id="S3.T5.1.2.3.3.1.2">
<span class="ltx_td ltx_align_center" id="S3.T5.1.2.3.3.1.2.1">JSTS</span></span>
</span></span><span class="ltx_text" id="S3.T5.1.2.3.4" style="font-size:90%;"> </span><span class="ltx_text" id="S3.T5.1.2.3.5"></span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.1.2.4"><span class="ltx_text" id="S3.T5.1.2.4.1" style="font-size:90%;">KorSTS</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.1.2.5">
<span class="ltx_text" id="S3.T5.1.2.5.1"></span><span class="ltx_text" id="S3.T5.1.2.5.2" style="font-size:90%;"> </span><span class="ltx_text" id="S3.T5.1.2.5.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S3.T5.1.2.5.3.1">
<span class="ltx_tr" id="S3.T5.1.2.5.3.1.1">
<span class="ltx_td ltx_align_center" id="S3.T5.1.2.5.3.1.1.1">KLUE-</span></span>
<span class="ltx_tr" id="S3.T5.1.2.5.3.1.2">
<span class="ltx_td ltx_align_center" id="S3.T5.1.2.5.3.1.2.1">STS</span></span>
</span></span><span class="ltx_text" id="S3.T5.1.2.5.4" style="font-size:90%;"> </span><span class="ltx_text" id="S3.T5.1.2.5.5"></span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="5" id="S3.T5.1.3.1">
<span class="ltx_text ltx_font_italic" id="S3.T5.1.3.1.1" style="font-size:90%;">Unsupervised</span><span class="ltx_text" id="S3.T5.1.3.1.2" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.4">
<td class="ltx_td ltx_align_left" id="S3.T5.1.4.1">
<span class="ltx_text" id="S3.T5.1.4.1.1" style="font-size:90%;">XLM-R</span><sub class="ltx_sub" id="S3.T5.1.4.1.2"><span class="ltx_text" id="S3.T5.1.4.1.2.1" style="font-size:90%;">large</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.1.4.2"><span class="ltx_text" id="S3.T5.1.4.2.1" style="font-size:90%;">61.87</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.1.4.3"><span class="ltx_text" id="S3.T5.1.4.3.1" style="font-size:90%;">54.19</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.1.4.4"><span class="ltx_text" id="S3.T5.1.4.4.1" style="font-size:90%;">49.26</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.1.4.5"><span class="ltx_text" id="S3.T5.1.4.5.1" style="font-size:90%;">20.13</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.5">
<td class="ltx_td ltx_align_left" id="S3.T5.1.5.1">
<span class="ltx_text" id="S3.T5.1.5.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="S3.T5.1.5.1.2"><span class="ltx_text" id="S3.T5.1.5.1.2.1" style="font-size:90%;">en</span></sub>
</td>
<td class="ltx_td" id="S3.T5.1.5.2"></td>
<td class="ltx_td" id="S3.T5.1.5.3"></td>
<td class="ltx_td" id="S3.T5.1.5.4"></td>
<td class="ltx_td" id="S3.T5.1.5.5"></td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.6">
<td class="ltx_td ltx_align_left" id="S3.T5.1.6.1"><span class="ltx_text" id="S3.T5.1.6.1.1" style="font-size:90%;">+ Wiki</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.1.6.2"><span class="ltx_text ltx_font_bold" id="S3.T5.1.6.2.1" style="font-size:90%;">81.02</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.1.6.3"><span class="ltx_text ltx_font_bold" id="S3.T5.1.6.3.1" style="font-size:90%;">77.62</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.1.6.4"><span class="ltx_text ltx_font_bold" id="S3.T5.1.6.4.1" style="font-size:90%;">80.38</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.1.6.5"><span class="ltx_text" id="S3.T5.1.6.5.1" style="font-size:90%;">81.42</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.7">
<td class="ltx_td ltx_align_left" id="S3.T5.1.7.1"><span class="ltx_text" id="S3.T5.1.7.1.1" style="font-size:90%;">+ SNLI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.1.7.2"><span class="ltx_text" id="S3.T5.1.7.2.1" style="font-size:90%;">80.31</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.1.7.3"><span class="ltx_text" id="S3.T5.1.7.3.1" style="font-size:90%;">74.73</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.1.7.4"><span class="ltx_text" id="S3.T5.1.7.4.1" style="font-size:90%;">75.21</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.1.7.5"><span class="ltx_text" id="S3.T5.1.7.5.1" style="font-size:90%;">65.83</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.8">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S3.T5.1.8.1">
<span class="ltx_text ltx_font_italic" id="S3.T5.1.8.1.1" style="font-size:90%;">Fully supervised</span><span class="ltx_text" id="S3.T5.1.8.1.2" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T5.1.9.1"><span class="ltx_text" id="S3.T5.1.9.1.1" style="font-size:90%;">LaBSE</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.1.9.2"><span class="ltx_text" id="S3.T5.1.9.2.1" style="font-size:90%;">76.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.1.9.3"><span class="ltx_text" id="S3.T5.1.9.3.1" style="font-size:90%;">76.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.1.9.4"><span class="ltx_text" id="S3.T5.1.9.4.1" style="font-size:90%;">73.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.1.9.5"><span class="ltx_text ltx_font_bold" id="S3.T5.1.9.5.1" style="font-size:90%;">82.81</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>
Comparisons of the best-performing cross-lingual transfer of Wikipedia data and LaBSE.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.4.   Results</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S3.T3" title="Table 3 ‣ 3.3. Evaluation Protocol ‣ 3. Experiments ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">3</span></a> summarizes the comparisons of the two data augmentation techniques under a controlled setting within the same NLI domain.
In this fair setting, machine translation (mSimCSE<sub class="ltx_sub" id="S3.SS4.p1.1.1">MT</sub>) slightly outperformed cross-lingual transfer (mSimCSE<sub class="ltx_sub" id="S3.SS4.p1.1.2">en</sub>).
Specifically, in the case of Korean, mSimCSE<sub class="ltx_sub" id="S3.SS4.p1.1.3">MT<sub class="ltx_sub" id="S3.SS4.p1.1.3.1">kr</sub></sub> almost outperformed mSimCSE<sub class="ltx_sub" id="S3.SS4.p1.1.4">en</sub>.
Similarly, in the case of Japanese, mSimCSE<sub class="ltx_sub" id="S3.SS4.p1.1.5">MT<sub class="ltx_sub" id="S3.SS4.p1.1.5.1">ja</sub></sub> outperformed mSimCSE<sub class="ltx_sub" id="S3.SS4.p1.1.6">en</sub> with the exception on JSICK-STS.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">However, we are yet to conclude that machine translation is better than cross-lingual transfer.
Table <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S3.T4" title="Table 4 ‣ 3.3. Evaluation Protocol ‣ 3. Experiments ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">4</span></a> summarizes the comparisons between NLI and Wikipedia domains in English.
Unlike prior studies focusing on NLI as training data, surprisingly, we obtained different results in this domain-aware setting.
Specifically, the mSimCSE<sub class="ltx_sub" id="S3.SS4.p2.1.1">en</sub> model trained using Wikipedia data outperformed its counterpart using NLI data, suggesting a superiority of Wikipedia as unlabeled training data.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">These new results led us to the combination of our findings, namely the cross-lingual transfer of Wikipedia domain data.
Table <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S3.T5" title="Table 5 ‣ 3.3. Evaluation Protocol ‣ 3. Experiments ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">5</span></a> summarizes the comparisons involving both the Wikipedia and NLI domains.
In this best-performing setting following <cite class="ltx_cite ltx_citemacro_citet">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib12" title="">2021</a>)</cite>, the cross-lingual transfer of Wikpedia outperformed the machine translation of NLI, resulting in performance almost outperforming that of LaBSE.
Specifically, the mSimCSE<sub class="ltx_sub" id="S3.SS4.p3.1.1">en</sub> model trained using Wikipedia data outperformed LaBSE with the only exception on KLUE-STS.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>We also performed an analysis of the obtained sentence embeddings using KLUE-STS, which is reported in the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A3" title="Appendix C Analysis of Sentence Embeddings ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">C</span></a></span></span></span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Discussion</h2>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Between cross-lingual transfer or machine translation, which is better?</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">It depends.
On one hand, machine translation can outperform cross-lingual transfer if we used the same NLI domain as training data.
On the other hand, the cross-lingual transfer of Wikipedia domain data outperformed the machine-translated NLI domain data.
These results rather suggest the effectiveness of Wikipedia as unlabeled training data.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Do these approaches yield performance on par with that of a state-of-the-art multilingual model?</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">Yes, we found that the cross-lingual transfer of Wikipedia data can outperform LaBSE.
This posed us a few more questions:
Should we pursue the direction of English as training data proxy, similar to mSimCSE?
Are there some benefit from using native multilingual data, if we could create it without using machine translation, similar to LaBSE?</p>
</div>
<figure class="ltx_table" id="S4.T6">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T6.1">
<tr class="ltx_tr" id="S4.T6.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T6.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T6.1.1.2"><span class="ltx_text" id="S4.T6.1.1.2.1" style="font-size:90%;">English</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T6.1.1.3"><span class="ltx_text" id="S4.T6.1.1.3.1" style="font-size:90%;">Japanese</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.2">
<td class="ltx_td ltx_align_center" id="S4.T6.1.2.1"><span class="ltx_text" id="S4.T6.1.2.1.1" style="font-size:90%;">Models</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.2.2"><span class="ltx_text" id="S4.T6.1.2.2.1" style="font-size:90%;">STS-B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.2.3">
<span class="ltx_text" id="S4.T6.1.2.3.1"></span><span class="ltx_text" id="S4.T6.1.2.3.2" style="font-size:90%;"> </span><span class="ltx_text" id="S4.T6.1.2.3.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T6.1.2.3.3.1">
<span class="ltx_tr" id="S4.T6.1.2.3.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T6.1.2.3.3.1.1.1">JSICK-</span></span>
<span class="ltx_tr" id="S4.T6.1.2.3.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T6.1.2.3.3.1.2.1">STS</span></span>
</span></span><span class="ltx_text" id="S4.T6.1.2.3.4" style="font-size:90%;"> </span><span class="ltx_text" id="S4.T6.1.2.3.5"></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.2.4">
<span class="ltx_text" id="S4.T6.1.2.4.1"></span><span class="ltx_text" id="S4.T6.1.2.4.2" style="font-size:90%;"> </span><span class="ltx_text" id="S4.T6.1.2.4.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T6.1.2.4.3.1">
<span class="ltx_tr" id="S4.T6.1.2.4.3.1.1">
<span class="ltx_td ltx_align_center" id="S4.T6.1.2.4.3.1.1.1">JGLUE-</span></span>
<span class="ltx_tr" id="S4.T6.1.2.4.3.1.2">
<span class="ltx_td ltx_align_center" id="S4.T6.1.2.4.3.1.2.1">JSTS</span></span>
</span></span><span class="ltx_text" id="S4.T6.1.2.4.4" style="font-size:90%;"> </span><span class="ltx_text" id="S4.T6.1.2.4.5"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S4.T6.1.3.1">
<span class="ltx_text ltx_font_italic" id="S4.T6.1.3.1.1" style="font-size:90%;">Cross-lingual transfer</span><span class="ltx_text" id="S4.T6.1.3.1.2" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.4">
<td class="ltx_td ltx_align_left" id="S4.T6.1.4.1">
<span class="ltx_text" id="S4.T6.1.4.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="S4.T6.1.4.1.2"><span class="ltx_text" id="S4.T6.1.4.1.2.1" style="font-size:90%;">en</span></sub>
</td>
<td class="ltx_td" id="S4.T6.1.4.2"></td>
<td class="ltx_td" id="S4.T6.1.4.3"></td>
<td class="ltx_td" id="S4.T6.1.4.4"></td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.5">
<td class="ltx_td ltx_align_left" id="S4.T6.1.5.1"><span class="ltx_text" id="S4.T6.1.5.1.1" style="font-size:90%;">+ Wiki</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.2"><span class="ltx_text ltx_font_bold" id="S4.T6.1.5.2.1" style="font-size:90%;">83.54</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.3"><span class="ltx_text" id="S4.T6.1.5.3.1" style="font-size:90%;">81.02</span></td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4"><span class="ltx_text" id="S4.T6.1.5.4.1" style="font-size:90%;">77.62</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.6">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S4.T6.1.6.1">
<span class="ltx_text ltx_font_italic" id="S4.T6.1.6.1.1" style="font-size:90%;">Native data</span><span class="ltx_text" id="S4.T6.1.6.1.2" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.7">
<td class="ltx_td ltx_align_left" id="S4.T6.1.7.1">
<span class="ltx_text" id="S4.T6.1.7.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="S4.T6.1.7.1.2"><span class="ltx_text" id="S4.T6.1.7.1.2.1" style="font-size:90%;">ja</span></sub>
</td>
<td class="ltx_td" id="S4.T6.1.7.2"></td>
<td class="ltx_td" id="S4.T6.1.7.3"></td>
<td class="ltx_td" id="S4.T6.1.7.4"></td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T6.1.8.1"><span class="ltx_text" id="S4.T6.1.8.1.1" style="font-size:90%;">+ Wiki-40B</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.2"><span class="ltx_text" id="S4.T6.1.8.2.1" style="font-size:90%;">81.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.3"><span class="ltx_text ltx_font_bold" id="S4.T6.1.8.3.1" style="font-size:90%;">81.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.4"><span class="ltx_text ltx_font_bold" id="S4.T6.1.8.4.1" style="font-size:90%;">78.71</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 6: </span>
Comparisons of English Wikipedia data cross-lingual transfer and native Japanese data.
</figcaption>
</figure>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#S4.T6" title="Table 6 ‣ Do these approaches yield performance on par with that of a state-of-the-art multilingual model? ‣ 4. Discussion ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">6</span></a> summarizes additional results using native Japanese Wikipedia data.
This pilot study suggests that using native data can indeed improve performance over cross-lingual transfer of English data.
However, we are yet to answer the ultimate question of English as training data proxy.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Related Work</h2>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Learning Sentence Embeddings</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">Sentence embeddings are learned representations of sentences within a single dense matrix, unlike word embeddings, which are represented in multiple dense matrices.
Several methods for fine-tuning pretrained masked language models have been proposed, including

<span class="ltx_inline-enumerate" id="S5.I1">
<span class="ltx_inline-item" id="S5.I1.i1"><span class="ltx_tag ltx_tag_inline-item">(a)</span> <span class="ltx_text" id="S5.I1.i1.1">SBERT <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib22" title="">2019</a>)</cite> using sentence-pair regression for supervised learning of labeled STS data, and
</span></span>
<span class="ltx_inline-item" id="S5.I1.i2"><span class="ltx_tag ltx_tag_inline-item">(b)</span> <span class="ltx_text" id="S5.I1.i2.1">unsupervised SimCSE <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib12" title="">2021</a>)</cite> using sentence-pair contrastive learning for self-supervised learning of unlabeled data.
</span></span>
</span>
Both methods yield good sentence embeddings in terms of cosine distance in STS.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Cross-lingual Transferability of Multilingual Models</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Artetxe and Schwenk (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib3" title="">2019</a>)</cite> proposed LASER, language-agnostic representations of sentence embeddings learned from dedicated parallel data.
They studied cross-lingual transferability of LASER on XNLI <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib7" title="">2018</a>)</cite> and found improved performance in various languages.
<cite class="ltx_cite ltx_citemacro_citet">Feng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib10" title="">2022</a>)</cite> proposed LaBSE, which outperformed LASER in downstream tasks by using additional monolingual data with parallel data.
<cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib28" title="">2022</a>)</cite> proposed mSimCSE, a multilingual extension of SimCSE, which is perhaps the most related work.
They investigated cross-lingual transferability of mSimCSE by using various tasks including multilingual STS.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Cross-lingual Transferability of Monolingual Models</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Artetxe et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib2" title="">2020</a>)</cite> studied cross-lingual transferability from one language to unseen languages.
They showed that the transfer learning of monolingual BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib9" title="">2019</a>)</cite> at the lexical level outperformed multilingual BERT on XQuAD.
<cite class="ltx_cite ltx_citemacro_citet">Reimers and Gurevych (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib23" title="">2020</a>)</cite> studied transfer learning of SBERT at the sentence level, using multilingual knowledge distillation in English and Korean as monolingual and multilingual STS.
They observed performance better than that of monolingual fine-tuning in Korean.
<cite class="ltx_cite ltx_citemacro_citet">Gogoulou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib13" title="">2022</a>)</cite> studied cross-lingual transferability from non-English languages into English by using various GLUE tasks <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib26" title="">2018</a>)</cite> including monolingual STS.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this study, we empirically compared cross-lingual transfer and machine translation in terms of monolingual STS performance.
We chose Japanese and Korean as our test bed, because these languages are relatively low-resourced and linguistically dissimilar compared with English, and thus are challenging for the two data augmentation techniques.
We found that the cross-lingual transfer exhibits performance comparable to that of the machine translation depending on data domain.
We also found that, in contrast to prior studies, cross-lingual transfer of Wikipedia data achieved the best performance, outperforming or comparable to that of the state-of-the-art LaBSE.
Our future work will include fine-grained analysis of which types of data are better suitable for monolingual STS.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section"><span class="ltx_text ltx_font_bold ltx_align_center" id="Sx1.1.1" style="font-size:120%;">Limitations</span></h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Our study focuses primarily on Japanese and Korean, which are often considered high-resource or mid-resource languages, but, at the same time, are relatively low-resourced with respect to STS training data and therefore suitable for data augmentation.
For this reason, our results are not directly applicable to other low-resource and regional languages, as even human-labeled STS evaluation data are lacking.
In addition, we did not conduct experiments using Wikipedia data machine-translated from English into Japanese and Korean, which will be part of our future work.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">7.   Bibliographical References</h2>
<span class="ltx_ERROR undefined" id="S7.1">\c@NAT@ctr</span>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography"></h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agirre et al. (2016)</span>
<span class="ltx_bibblock">
Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/S16-1081" title="">SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 10th International Workshop on Semantic Evaluation</em>, pages 497–511.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Artetxe et al. (2020)</span>
<span class="ltx_bibblock">
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.421" title="">On the cross-lingual transferability of monolingual representations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 4623–4637.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Artetxe and Schwenk (2019)</span>
<span class="ltx_bibblock">
Mikel Artetxe and Holger Schwenk. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00288" title="">Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Transactions of the Association for Computational Linguistics</em>, 7:597–610.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bowman et al. (2015)</span>
<span class="ltx_bibblock">
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D15-1075" title="">A large annotated corpus for learning natural language inference</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</em>, pages 632–642.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cer et al. (2017)</span>
<span class="ltx_bibblock">
Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/S17-2001" title="">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 11th International Workshop on Semantic Evaluation</em>, pages 1–14.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.747" title="">Unsupervised cross-lingual representation learning at scale</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 8440–8451.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2018)</span>
<span class="ltx_bibblock">
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1269" title="">XNLI: Evaluating cross-lingual sentence representations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 2475–2485.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dagan et al. (2005)</span>
<span class="ltx_bibblock">
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/11736790_9" title="">The PASCAL recognising textual entailment challenge</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 1st PASCAL Machine Learning Challenges Workshop</em>, pages 177–190.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1423" title="">BERT: Pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 4171–4186.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2022)</span>
<span class="ltx_bibblock">
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.acl-long.62" title="">Language-agnostic BERT sentence embedding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</em>, pages 878–891.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2021)</span>
<span class="ltx_bibblock">
Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.findings-acl.84" title="">A survey of data augmentation approaches for NLP</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</em>, pages 968–988.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2021)</span>
<span class="ltx_bibblock">
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-main.552" title="">SimCSE: Simple contrastive learning of sentence embeddings</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 6894–6910.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gogoulou et al. (2022)</span>
<span class="ltx_bibblock">
Evangelia Gogoulou, Ariel Ekgren, Tim Isbister, and Magnus Sahlgren. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.lrec-1.100" title="">Cross-lingual transfer of monolingual models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 13th International Conference on Language Resources and Evaluation</em>, pages 948–955.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2020)</span>
<span class="ltx_bibblock">
Mandy Guo, Zihang Dai, Denny Vrandečić, and Rami Al-Rfou. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.lrec-1.297" title="">Wiki-40B: Multilingual language model dataset</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the Twelfth Language Resources and Evaluation Conference</em>, pages 2440–2452.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ham et al. (2020)</span>
<span class="ltx_bibblock">
Jiyeon Ham, Yo Joong Choe, Kyubyong Park, Ilji Choi, and Hyungjoon Soh. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.findings-emnlp.39" title="">KorNLI and KorSTS: New benchmark datasets for Korean natural language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Findings of the Association for Computational Linguistics: EMNLP 2020</em>, pages 422–430.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Keung et al. (2020)</span>
<span class="ltx_bibblock">
Phillip Keung, Yichao Lu, Julian Salazar, and Vikas Bhardwaj. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.40" title="">Don’t use English dev: On the zero-shot cross-lingual evaluation of contextual embeddings</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</em>, pages 549–554.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-2012" title="">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, pages 66–71.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kurihara et al. (2022)</span>
<span class="ltx_bibblock">
Kentaro Kurihara, Daisuke Kawahara, and Tomohide Shibata. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.lrec-1.317" title="">JGLUE: Japanese general language understanding evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 13th Language Resources and Evaluation Conference</em>, pages 2957–2966.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.1907.11692" title="">RoBERTa: A robustly optimized BERT pretraining approach</a>.

</span>
<span class="ltx_bibblock">Preprint, arXiv:1907.11692.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marelli et al. (2014)</span>
<span class="ltx_bibblock">
Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/L14-1314/" title="">A SICK cure for the evaluation of compositional distributional semantic models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 9th International Conference on Language Resources and Evaluation</em>, pages 216–223.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2021)</span>
<span class="ltx_bibblock">
Sungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik Cho, Ji Yoon Han, Jangwon Park, Chisung Song, Junseong Kim, Youngsook Song, Taehwan Oh, Joohong Lee, Juhyun Oh, Sungwon Lyu, Younghoon Jeong, Inkwon Lee, Sangwoo Seo, Dongjun Lee, Hyunwoo Kim, Myeonghwa Lee, Seongbo Jang, Seungwon Do, Sunkyoung Kim, Kyungtae Lim, Jongwon Lee, Kyumin Park, Jamin Shin, Seonghyun Kim, Lucy Park, Alice Oh, Jung-Woo Ha, and Kyunghyun Cho. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/98dce83da57b0395e163467c9dae521b-Paper-round2.pdf" title="">KLUE: Korean language understanding evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1410" title="">Sentence-BERT: Sentence embeddings using Siamese BERT-networks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</em>, pages 3982–3992.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2020)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.365" title="">Making monolingual sentence embeddings multilingual using knowledge distillation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</em>, pages 4512–4525.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spearman (1904)</span>
<span class="ltx_bibblock">
Charles Spearman. 1904.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://www.jstor.org/stable/1412159" title="">The proof and measurement of association between two things</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">The American Journal of Psychology</em>, 15(1):72–101.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van der Maaten and Hinton (2008)</span>
<span class="ltx_bibblock">
Laurens van der Maaten and Geoffrey Hinton. 2008.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://jmlr.org/papers/v9/vandermaaten08a.html" title="">Visualizing data using t-SNE</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Journal of Machine Learning Research</em>, 9(86):2579–2605.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2018)</span>
<span class="ltx_bibblock">
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-5446" title="">GLUE: A multi-task benchmark and analysis platform for natural language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 2018 EMNLP Workshop BlackboxNLP</em>, pages 353–355.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Isola (2020)</span>
<span class="ltx_bibblock">
Tongzhou Wang and Phillip Isola. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v119/wang20k.html" title="">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 37th International Conference on Machine Learning</em>, pages 9929–9939.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Yaushian Wang, Ashley Wu, and Graham Neubig. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.emnlp-main.621" title="">English contrastive learning can learn universal cross-lingual sentence embeddings</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 9122–9133.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williams et al. (2018)</span>
<span class="ltx_bibblock">
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N18-1101" title="">A broad-coverage challenge corpus for sentence understanding through inference</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 1112–1122.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2020)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-demos.6" title="">Transformers: State-of-the-art natural language processing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, pages 38–45.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yanaka and Mineshima (2022)</span>
<span class="ltx_bibblock">
Hitomi Yanaka and Koji Mineshima. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00518" title="">Compositional evaluation on Japanese textual entailment and similarity</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Transactions of the Association for Computational Linguistics</em>, 10:1266–1284.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoshikoshi et al. (2020)</span>
<span class="ltx_bibblock">
Takumi Yoshikoshi, Daisuke Kawahara, and Sadao Kurohashi. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://nlp.ist.i.kyoto-u.ac.jp/?%E6%97%A5%E6%9C%AC%E8%AA%9ESNLI%28JSNLI%29%E3%83%87%E3%83%BC%E3%82%BF%E3%82%BB%E3%83%83%E3%83%88" title="">Multilingualization of a natural language inference dataset using machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">IPSJ SIG Technical Report</em>, NL-244(6):1–8.

</span>
<span class="ltx_bibblock">In Japanese.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Implementation Details</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">In this study, we reimplemented the unsupervised SimCSE <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib12" title="">2021</a>)</cite> instead of utilizing the original implementation provided by the authors<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/princeton-nlp/SimCSE" title="">https://github.com/princeton-nlp/SimCSE</a></span></span></span>.
We used Sentence-Transformers 2.2.0 <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib22" title="">2019</a>)</cite>, Hugging Face Transformers 4.18.0 <cite class="ltx_cite ltx_citemacro_cite">Wolf et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib30" title="">2020</a>)</cite>, and SentencePiece 0.1.96 <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib17" title="">2018</a>)</cite>.
We also used the publicly available models of XLM-R<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/xlm-roberta-large" title="">https://huggingface.co/xlm-roberta-large</a></span></span></span>, LaBSE<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/sentence-transformers/LaBSE" title="">https://huggingface.co/sentence-transformers/LaBSE</a></span></span></span>, RoBERTa<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/roberta-large" title="">https://huggingface.co/roberta-large</a></span></span></span>, KLUE-RoBERTa<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/klue/roberta-large" title="">https://huggingface.co/klue/roberta-large</a></span></span></span>, and SBERT<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/sentence-transformers/roberta-large-nli-mean-tokens" title="">https://huggingface.co/sentence-transformers/roberta-large-nli-mean-tokens</a></span></span></span>, as well as the unsupervised SimCSE models distributed by the original authors<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/princeton-nlp/unsup-simcse-roberta-large" title="">https://huggingface.co/princeton-nlp/unsup-simcse-roberta-large</a></span></span></span>.
We report results obtained in a single run with the random seed fixed, while freezing hyperparameters for the sake of reproducibility <cite class="ltx_cite ltx_citemacro_cite">Keung et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib16" title="">2020</a>)</cite>.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Data Preprocessing</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Here, we describe the details of our data preprocessing to make the data used in this study reproducible.
We have removed empty lines from our NLI and Wikipedia datasets and applied additional preprocessing as follows.
</p>
</div>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">NLI datasets</h4>
<div class="ltx_para" id="A2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px1.p1.1">For JSNLI, we roughly detokenized the already-tokenized dataset by applying NFKC normalization and eliminating white spaces.
For JSICK-STS, we used its test portion to make it comparable with the other datasets.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Wikipedia datasets</h4>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p1.1">For English Wikipedia, we used the data<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse" title="">https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse</a></span></span></span> distributed with the original implementation of SimCSE <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib12" title="">2021</a>)</cite>, which contains exactly 1 million sentences but the authors did not report its details.</p>
</div>
<div class="ltx_para" id="A2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="A2.SS0.SSS0.Px2.p2.1">Therefore, we created our own Japanese Wikipedia data from the multilingual Wiki-40B dataset<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/wiki40b" title="">https://huggingface.co/datasets/wiki40b</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib14" title="">2020</a>)</cite> as a replication study.
Specifically, we extracted Japanese paragraphs from Wiki-40B and applied sentence splitting using new line symbols and Japanese period markers.
To match the data size of the Japanese Wikipedia data with its English counterpart, we used only the first 1 million sentences.</p>
</div>
<figure class="ltx_figure" id="A2.F2"><span class="ltx_inline-para ltx_minipage ltx_align_center ltx_align_middle" id="A2.F2.1" style="width:227.6pt;">
<span class="ltx_figure" id="A2.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="267" id="A2.F2.sf1.g1" src="extracted/5457243/figure/roberta.png" width="252"/>
<span class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>KLUE-RoBERTa<sub class="ltx_sub" id="A2.F2.sf1.2.1">large</sub></span>
</span>
<span class="ltx_figure" id="A2.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="274" id="A2.F2.sf2.g1" src="extracted/5457243/figure/roberta-simcse.png" width="252"/>
<span class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>SimCSE<sub class="ltx_sub" id="A2.F2.sf2.2.1">MT<sub class="ltx_sub" id="A2.F2.sf2.2.1.1">kr</sub></sub>+KorNLI</span>
</span>
<span class="ltx_figure" id="A2.F2.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="274" id="A2.F2.sf3.g1" src="extracted/5457243/figure/xlmr.png" width="252"/>
<span class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>XLM-R<sub class="ltx_sub" id="A2.F2.sf3.2.1">large</sub></span>
</span>
<span class="ltx_figure" id="A2.F2.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="271" id="A2.F2.sf4.g1" src="extracted/5457243/figure/xlmr-simcse.png" width="252"/>
<span class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>mSimCSE<sub class="ltx_sub" id="A2.F2.sf4.2.1">en</sub> + Wiki</span>
</span></span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
2D visualization of sentence embeddings on KLUE-STS.
Different scales are used to better illustrate the isotropic nature of SimCSE fine-tuning.
</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Analysis of Sentence Embeddings</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">We performed an analysis of the obtained sentence embeddings using visualization and metrics.</p>
</div>
<section class="ltx_paragraph" id="A3.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Visualization</h4>
<div class="ltx_para" id="A3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A3.SS0.SSS0.Px1.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A2.F2" title="Figure 2 ‣ Wikipedia datasets ‣ Appendix B Data Preprocessing ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">2</span></a> provides a visualization of the sentence embeddings on KLUE-STS, which were obtained by applying t-distributed stochastic neighbor embedding <cite class="ltx_cite ltx_citemacro_citep">(t-SNE; van der Maaten and Hinton, <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib25" title="">2008</a>)</cite> for two-dimensional reduction.
We compared cross-lingual transfer (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A2.F2.sf4" title="2d ‣ Figure 2 ‣ Wikipedia datasets ‣ Appendix B Data Preprocessing ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">2d</span></a>) with a Korean model (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A2.F2.sf2" title="2b ‣ Figure 2 ‣ Wikipedia datasets ‣ Appendix B Data Preprocessing ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">2b</span></a>), which was fine-tuned using KLUE-RoBERTa <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib19" title="">2019</a>); Park et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib21" title="">2021</a>)</cite>, unsupervised SimCSE <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib12" title="">2021</a>)</cite>, and KorNLI.
We observe that cross-lingual transfer can maintain the isotropic nature of SimCSE fine-tuning.</p>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Alignment and Uniformity</h4>
<div class="ltx_para" id="A3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A3.SS0.SSS0.Px2.p1.4">We further confirmed the aforementioned trend by using alignment and uniformity metrics <cite class="ltx_cite ltx_citemacro_cite">Wang and Isola (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib27" title="">2020</a>)</cite> to quantify the isotropic nature of SimCSE fine-tuning.
The monolingual model (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A2.F2.sf2" title="2b ‣ Figure 2 ‣ Wikipedia datasets ‣ Appendix B Data Preprocessing ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">2b</span></a>) scored <math alttext="\ell_{\mathrm{align}}=0.3540" class="ltx_Math" display="inline" id="A3.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="A3.SS0.SSS0.Px2.p1.1.m1.1a"><mrow id="A3.SS0.SSS0.Px2.p1.1.m1.1.1" xref="A3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml"><msub id="A3.SS0.SSS0.Px2.p1.1.m1.1.1.2" xref="A3.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml"><mi id="A3.SS0.SSS0.Px2.p1.1.m1.1.1.2.2" mathvariant="normal" xref="A3.SS0.SSS0.Px2.p1.1.m1.1.1.2.2.cmml">ℓ</mi><mi id="A3.SS0.SSS0.Px2.p1.1.m1.1.1.2.3" xref="A3.SS0.SSS0.Px2.p1.1.m1.1.1.2.3.cmml">align</mi></msub><mo id="A3.SS0.SSS0.Px2.p1.1.m1.1.1.1" xref="A3.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml">=</mo><mn id="A3.SS0.SSS0.Px2.p1.1.m1.1.1.3" xref="A3.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml">0.3540</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="A3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.1.1"><eq id="A3.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.1.1.1"></eq><apply id="A3.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="A3.SS0.SSS0.Px2.p1.1.m1.1.1.2.1.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.1.1.2">subscript</csymbol><ci id="A3.SS0.SSS0.Px2.p1.1.m1.1.1.2.2.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.1.1.2.2">ℓ</ci><ci id="A3.SS0.SSS0.Px2.p1.1.m1.1.1.2.3.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.1.1.2.3">align</ci></apply><cn id="A3.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml" type="float" xref="A3.SS0.SSS0.Px2.p1.1.m1.1.1.3">0.3540</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px2.p1.1.m1.1c">\ell_{\mathrm{align}}=0.3540</annotation><annotation encoding="application/x-llamapun" id="A3.SS0.SSS0.Px2.p1.1.m1.1d">roman_ℓ start_POSTSUBSCRIPT roman_align end_POSTSUBSCRIPT = 0.3540</annotation></semantics></math> and <math alttext="\ell_{\mathrm{uniform}}=-3.3486" class="ltx_Math" display="inline" id="A3.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="A3.SS0.SSS0.Px2.p1.2.m2.1a"><mrow id="A3.SS0.SSS0.Px2.p1.2.m2.1.1" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><msub id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml"><mi id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.2.2" mathvariant="normal" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.cmml">ℓ</mi><mi id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.cmml">uniform</mi></msub><mo id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.1" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml">=</mo><mrow id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml"><mo id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.3a" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml">−</mo><mn id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.3.2" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.3.2.cmml">3.3486</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1"><eq id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.1"></eq><apply id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.2.1.cmml" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.2">subscript</csymbol><ci id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.cmml" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.2.2">ℓ</ci><ci id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.cmml" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.2.3">uniform</ci></apply><apply id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.3"><minus id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.3.1.cmml" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.3"></minus><cn id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.3.2.cmml" type="float" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.3.2">3.3486</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px2.p1.2.m2.1c">\ell_{\mathrm{uniform}}=-3.3486</annotation><annotation encoding="application/x-llamapun" id="A3.SS0.SSS0.Px2.p1.2.m2.1d">roman_ℓ start_POSTSUBSCRIPT roman_uniform end_POSTSUBSCRIPT = - 3.3486</annotation></semantics></math>, while cross-lingual transfer (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A2.F2.sf4" title="2d ‣ Figure 2 ‣ Wikipedia datasets ‣ Appendix B Data Preprocessing ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">2d</span></a>) scored similar values of <math alttext="\ell_{\mathrm{align}}=0.2684" class="ltx_Math" display="inline" id="A3.SS0.SSS0.Px2.p1.3.m3.1"><semantics id="A3.SS0.SSS0.Px2.p1.3.m3.1a"><mrow id="A3.SS0.SSS0.Px2.p1.3.m3.1.1" xref="A3.SS0.SSS0.Px2.p1.3.m3.1.1.cmml"><msub id="A3.SS0.SSS0.Px2.p1.3.m3.1.1.2" xref="A3.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml"><mi id="A3.SS0.SSS0.Px2.p1.3.m3.1.1.2.2" mathvariant="normal" xref="A3.SS0.SSS0.Px2.p1.3.m3.1.1.2.2.cmml">ℓ</mi><mi id="A3.SS0.SSS0.Px2.p1.3.m3.1.1.2.3" xref="A3.SS0.SSS0.Px2.p1.3.m3.1.1.2.3.cmml">align</mi></msub><mo id="A3.SS0.SSS0.Px2.p1.3.m3.1.1.1" xref="A3.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml">=</mo><mn id="A3.SS0.SSS0.Px2.p1.3.m3.1.1.3" xref="A3.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml">0.2684</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px2.p1.3.m3.1b"><apply id="A3.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.3.m3.1.1"><eq id="A3.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.3.m3.1.1.1"></eq><apply id="A3.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="A3.SS0.SSS0.Px2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="A3.SS0.SSS0.Px2.p1.3.m3.1.1.2.1.cmml" xref="A3.SS0.SSS0.Px2.p1.3.m3.1.1.2">subscript</csymbol><ci id="A3.SS0.SSS0.Px2.p1.3.m3.1.1.2.2.cmml" xref="A3.SS0.SSS0.Px2.p1.3.m3.1.1.2.2">ℓ</ci><ci id="A3.SS0.SSS0.Px2.p1.3.m3.1.1.2.3.cmml" xref="A3.SS0.SSS0.Px2.p1.3.m3.1.1.2.3">align</ci></apply><cn id="A3.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml" type="float" xref="A3.SS0.SSS0.Px2.p1.3.m3.1.1.3">0.2684</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px2.p1.3.m3.1c">\ell_{\mathrm{align}}=0.2684</annotation><annotation encoding="application/x-llamapun" id="A3.SS0.SSS0.Px2.p1.3.m3.1d">roman_ℓ start_POSTSUBSCRIPT roman_align end_POSTSUBSCRIPT = 0.2684</annotation></semantics></math> and <math alttext="\ell_{\mathrm{uniform}}=-3.2484" class="ltx_Math" display="inline" id="A3.SS0.SSS0.Px2.p1.4.m4.1"><semantics id="A3.SS0.SSS0.Px2.p1.4.m4.1a"><mrow id="A3.SS0.SSS0.Px2.p1.4.m4.1.1" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.cmml"><msub id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml"><mi id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.2" mathvariant="normal" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.2.cmml">ℓ</mi><mi id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.3" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.3.cmml">uniform</mi></msub><mo id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.1" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml">=</mo><mrow id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.3" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml"><mo id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.3a" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml">−</mo><mn id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.3.2" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.3.2.cmml">3.2484</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px2.p1.4.m4.1b"><apply id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1"><eq id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.1"></eq><apply id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.1.cmml" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2">subscript</csymbol><ci id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.2.cmml" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.2">ℓ</ci><ci id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.3.cmml" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.3">uniform</ci></apply><apply id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.3"><minus id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.3.1.cmml" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.3"></minus><cn id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.3.2.cmml" type="float" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.3.2">3.2484</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px2.p1.4.m4.1c">\ell_{\mathrm{uniform}}=-3.2484</annotation><annotation encoding="application/x-llamapun" id="A3.SS0.SSS0.Px2.p1.4.m4.1d">roman_ℓ start_POSTSUBSCRIPT roman_uniform end_POSTSUBSCRIPT = - 3.2484</annotation></semantics></math>.</p>
</div>
<figure class="ltx_table" id="A3.T7">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_flex_size_2 ltx_align_center" id="A3.T7.sf1">
<table class="ltx_tabular ltx_align_middle" id="A3.T7.sf1.1">
<tr class="ltx_tr" id="A3.T7.sf1.1.1">
<td class="ltx_td ltx_border_tt" id="A3.T7.sf1.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T7.sf1.1.1.2"><span class="ltx_text" id="A3.T7.sf1.1.1.2.1" style="font-size:90%;">English</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.2">
<td class="ltx_td ltx_align_center" id="A3.T7.sf1.1.2.1"><span class="ltx_text" id="A3.T7.sf1.1.2.1.1" style="font-size:90%;">Models</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.sf1.1.2.2"><span class="ltx_text" id="A3.T7.sf1.1.2.2.1" style="font-size:90%;">STS-B</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A3.T7.sf1.1.3.1">
<span class="ltx_text ltx_font_italic" id="A3.T7.sf1.1.3.1.1" style="font-size:90%;">Unsupervised</span><span class="ltx_text" id="A3.T7.sf1.1.3.1.2" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.4">
<td class="ltx_td ltx_align_center" id="A3.T7.sf1.1.4.1">
<span class="ltx_text" id="A3.T7.sf1.1.4.1.1" style="font-size:90%;">RoBERTa</span><sub class="ltx_sub" id="A3.T7.sf1.1.4.1.2"><span class="ltx_text" id="A3.T7.sf1.1.4.1.2.1" style="font-size:90%;">base</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="A3.T7.sf1.1.4.2"><span class="ltx_text" id="A3.T7.sf1.1.4.2.1" style="font-size:90%;">64.70</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.5">
<td class="ltx_td ltx_align_center" id="A3.T7.sf1.1.5.1">
<span class="ltx_text" id="A3.T7.sf1.1.5.1.1" style="font-size:90%;">SimCSE</span><sub class="ltx_sub" id="A3.T7.sf1.1.5.1.2"><span class="ltx_text" id="A3.T7.sf1.1.5.1.2.1" style="font-size:90%;">orig</span></sub>
</td>
<td class="ltx_td" id="A3.T7.sf1.1.5.2"></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.6">
<td class="ltx_td ltx_align_left" id="A3.T7.sf1.1.6.1"><span class="ltx_text" id="A3.T7.sf1.1.6.1.1" style="font-size:90%;">+ Wiki</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.sf1.1.6.2"><span class="ltx_text" id="A3.T7.sf1.1.6.2.1" style="font-size:90%;">84.26</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.7">
<td class="ltx_td ltx_align_center" id="A3.T7.sf1.1.7.1">
<span class="ltx_text" id="A3.T7.sf1.1.7.1.1" style="font-size:90%;">SimCSE</span><sub class="ltx_sub" id="A3.T7.sf1.1.7.1.2"><span class="ltx_text" id="A3.T7.sf1.1.7.1.2.1" style="font-size:90%;">repro</span></sub>
</td>
<td class="ltx_td" id="A3.T7.sf1.1.7.2"></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.8">
<td class="ltx_td ltx_align_left" id="A3.T7.sf1.1.8.1"><span class="ltx_text" id="A3.T7.sf1.1.8.1.1" style="font-size:90%;">+ Wiki</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.sf1.1.8.2"><span class="ltx_text" id="A3.T7.sf1.1.8.2.1" style="font-size:90%;">83.90</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.9">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.sf1.1.9.1">
<span class="ltx_text" id="A3.T7.sf1.1.9.1.1" style="font-size:90%;">RoBERTa</span><sub class="ltx_sub" id="A3.T7.sf1.1.9.1.2"><span class="ltx_text" id="A3.T7.sf1.1.9.1.2.1" style="font-size:90%;">large</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.sf1.1.9.2"><span class="ltx_text" id="A3.T7.sf1.1.9.2.1" style="font-size:90%;">56.29</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.10">
<td class="ltx_td ltx_align_center" id="A3.T7.sf1.1.10.1">
<span class="ltx_text" id="A3.T7.sf1.1.10.1.1" style="font-size:90%;">SimCSE</span><sub class="ltx_sub" id="A3.T7.sf1.1.10.1.2"><span class="ltx_text" id="A3.T7.sf1.1.10.1.2.1" style="font-size:90%;">orig</span></sub>
</td>
<td class="ltx_td" id="A3.T7.sf1.1.10.2"></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.11">
<td class="ltx_td ltx_align_left" id="A3.T7.sf1.1.11.1"><span class="ltx_text" id="A3.T7.sf1.1.11.1.1" style="font-size:90%;">+ Wiki</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.sf1.1.11.2"><span class="ltx_text" id="A3.T7.sf1.1.11.2.1" style="font-size:90%;">85.52</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.12">
<td class="ltx_td ltx_align_center" id="A3.T7.sf1.1.12.1">
<span class="ltx_text" id="A3.T7.sf1.1.12.1.1" style="font-size:90%;">SimCSE</span><sub class="ltx_sub" id="A3.T7.sf1.1.12.1.2"><span class="ltx_text" id="A3.T7.sf1.1.12.1.2.1" style="font-size:90%;">repro</span></sub>
</td>
<td class="ltx_td" id="A3.T7.sf1.1.12.2"></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.13">
<td class="ltx_td ltx_align_left" id="A3.T7.sf1.1.13.1"><span class="ltx_text" id="A3.T7.sf1.1.13.1.1" style="font-size:90%;">+ Wiki</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.sf1.1.13.2"><span class="ltx_text ltx_font_bold" id="A3.T7.sf1.1.13.2.1" style="font-size:90%;">85.95</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.14">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A3.T7.sf1.1.14.1">
<span class="ltx_text ltx_font_italic" id="A3.T7.sf1.1.14.1.1" style="font-size:90%;">English NLI Supervised</span><span class="ltx_text" id="A3.T7.sf1.1.14.1.2" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.15">
<td class="ltx_td ltx_align_center" id="A3.T7.sf1.1.15.1">
<span class="ltx_text" id="A3.T7.sf1.1.15.1.1" style="font-size:90%;">SBERT</span><sub class="ltx_sub" id="A3.T7.sf1.1.15.1.2"><span class="ltx_text" id="A3.T7.sf1.1.15.1.2.1" style="font-size:90%;">base</span></sub>
</td>
<td class="ltx_td" id="A3.T7.sf1.1.15.2"></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.16">
<td class="ltx_td ltx_align_left" id="A3.T7.sf1.1.16.1"><span class="ltx_text" id="A3.T7.sf1.1.16.1.1" style="font-size:90%;">+ NLI</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.sf1.1.16.2"><span class="ltx_text" id="A3.T7.sf1.1.16.2.1" style="font-size:90%;">80.73</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.17">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.sf1.1.17.1">
<span class="ltx_text" id="A3.T7.sf1.1.17.1.1" style="font-size:90%;">SBERT</span><sub class="ltx_sub" id="A3.T7.sf1.1.17.1.2"><span class="ltx_text" id="A3.T7.sf1.1.17.1.2.1" style="font-size:90%;">large</span></sub>
</td>
<td class="ltx_td ltx_border_t" id="A3.T7.sf1.1.17.2"></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf1.1.18">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T7.sf1.1.18.1"><span class="ltx_text" id="A3.T7.sf1.1.18.1.1" style="font-size:90%;">+ NLI</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.sf1.1.18.2"><span class="ltx_text" id="A3.T7.sf1.1.18.2.1" style="font-size:90%;">82.51</span></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">(a) </span>Monolingual models.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_flex_size_2 ltx_align_center" id="A3.T7.sf2">
<table class="ltx_tabular ltx_align_middle" id="A3.T7.sf2.1">
<tr class="ltx_tr" id="A3.T7.sf2.1.1">
<td class="ltx_td ltx_border_tt" id="A3.T7.sf2.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T7.sf2.1.1.2"><span class="ltx_text" id="A3.T7.sf2.1.1.2.1" style="font-size:90%;">English</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf2.1.2">
<td class="ltx_td ltx_align_center" id="A3.T7.sf2.1.2.1"><span class="ltx_text" id="A3.T7.sf2.1.2.1.1" style="font-size:90%;">Models</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.sf2.1.2.2"><span class="ltx_text" id="A3.T7.sf2.1.2.2.1" style="font-size:90%;">STS-B</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf2.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A3.T7.sf2.1.3.1">
<span class="ltx_text ltx_font_italic" id="A3.T7.sf2.1.3.1.1" style="font-size:90%;">Unsupervised</span><span class="ltx_text" id="A3.T7.sf2.1.3.1.2" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf2.1.4">
<td class="ltx_td ltx_align_center" id="A3.T7.sf2.1.4.1">
<span class="ltx_text" id="A3.T7.sf2.1.4.1.1" style="font-size:90%;">XLM-R</span><sub class="ltx_sub" id="A3.T7.sf2.1.4.1.2"><span class="ltx_text" id="A3.T7.sf2.1.4.1.2.1" style="font-size:90%;">base</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="A3.T7.sf2.1.4.2"><span class="ltx_text" id="A3.T7.sf2.1.4.2.1" style="font-size:90%;">53.62</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf2.1.5">
<td class="ltx_td ltx_align_center" id="A3.T7.sf2.1.5.1">
<span class="ltx_text" id="A3.T7.sf2.1.5.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="A3.T7.sf2.1.5.1.2"><span class="ltx_text" id="A3.T7.sf2.1.5.1.2.1" style="font-size:90%;">en</span></sub>
</td>
<td class="ltx_td" id="A3.T7.sf2.1.5.2"></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf2.1.6">
<td class="ltx_td ltx_align_left" id="A3.T7.sf2.1.6.1"><span class="ltx_text" id="A3.T7.sf2.1.6.1.1" style="font-size:90%;">+ Wiki</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.sf2.1.6.2"><span class="ltx_text" id="A3.T7.sf2.1.6.2.1" style="font-size:90%;">76.44</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf2.1.7">
<td class="ltx_td ltx_align_left" id="A3.T7.sf2.1.7.1"><span class="ltx_text" id="A3.T7.sf2.1.7.1.1" style="font-size:90%;">+ NLI</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.sf2.1.7.2"><span class="ltx_text" id="A3.T7.sf2.1.7.2.1" style="font-size:90%;">75.79</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf2.1.8">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.sf2.1.8.1">
<span class="ltx_text" id="A3.T7.sf2.1.8.1.1" style="font-size:90%;">XLM-R</span><sub class="ltx_sub" id="A3.T7.sf2.1.8.1.2"><span class="ltx_text" id="A3.T7.sf2.1.8.1.2.1" style="font-size:90%;">large</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.sf2.1.8.2"><span class="ltx_text" id="A3.T7.sf2.1.8.2.1" style="font-size:90%;">43.54</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf2.1.9">
<td class="ltx_td ltx_align_center" id="A3.T7.sf2.1.9.1">
<span class="ltx_text" id="A3.T7.sf2.1.9.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="A3.T7.sf2.1.9.1.2"><span class="ltx_text" id="A3.T7.sf2.1.9.1.2.1" style="font-size:90%;">en</span></sub>
</td>
<td class="ltx_td" id="A3.T7.sf2.1.9.2"></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf2.1.10">
<td class="ltx_td ltx_align_left" id="A3.T7.sf2.1.10.1"><span class="ltx_text" id="A3.T7.sf2.1.10.1.1" style="font-size:90%;">+ Wiki</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.sf2.1.10.2"><span class="ltx_text ltx_font_bold" id="A3.T7.sf2.1.10.2.1" style="font-size:90%;">83.54</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf2.1.11">
<td class="ltx_td ltx_align_left" id="A3.T7.sf2.1.11.1"><span class="ltx_text" id="A3.T7.sf2.1.11.1.1" style="font-size:90%;">+ NLI</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.sf2.1.11.2"><span class="ltx_text" id="A3.T7.sf2.1.11.2.1" style="font-size:90%;">76.98</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf2.1.12">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A3.T7.sf2.1.12.1">
<span class="ltx_text ltx_font_italic" id="A3.T7.sf2.1.12.1.1" style="font-size:90%;">Fully supervised</span><span class="ltx_text" id="A3.T7.sf2.1.12.1.2" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T7.sf2.1.13">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.sf2.1.13.1"><span class="ltx_text" id="A3.T7.sf2.1.13.1.1" style="font-size:90%;">LaBSE</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.sf2.1.13.2"><span class="ltx_text" id="A3.T7.sf2.1.13.2.1" style="font-size:90%;">74.13</span></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">(b) </span>Multilingual models.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 7: </span>Comparison of unsupervised SimCSE in English on different training data and base models.
SimCSE<sub class="ltx_sub" id="A3.T7.7.1">orig</sub> denotes the models distributed by the original authors, whereas SimCSE<sub class="ltx_sub" id="A3.T7.8.2">repro</sub> and mSimCSE denote our implementation used in this study.
</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Replication Study</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">We conducted a replication study of unsupervised SimCSE in English <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib12" title="">2021</a>)</cite> by using various training data that have been extensively used in the relevant literature but not directly compared in the unsupervised SimCSE setup.
We used RoBERTa <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib19" title="">2019</a>)</cite> as the monolingual base model and performed monolingual fine-tuning using unsupervised SimCSE.</p>
</div>
<div class="ltx_para" id="A4.p2">
<p class="ltx_p" id="A4.p2.1">As summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A3.T7" title="Table 7 ‣ Alignment and Uniformity ‣ Appendix C Analysis of Sentence Embeddings ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">7</span></a>, our implementation (SimCSE<sub class="ltx_sub" id="A4.p2.1.1">repro</sub> and mSimCSE) exhibited performance comparable to that of the models distributed by the original authors (SimCSE<sub class="ltx_sub" id="A4.p2.1.2">orig</sub>).</p>
</div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Ablation Study</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">We conducted an ablation study on different sizes of training data in English, Japanese, and Korean.
In this comparison, we used NLI datasets, combining them in two ways:

<span class="ltx_inline-enumerate" id="A5.I1">
<span class="ltx_inline-item" id="A5.I1.i1"><span class="ltx_tag ltx_tag_inline-item">(a)</span> <span class="ltx_text" id="A5.I1.i1.1">only the premise portion, and
</span></span>
<span class="ltx_inline-item" id="A5.I1.i2"><span class="ltx_tag ltx_tag_inline-item">(b)</span> <span class="ltx_text" id="A5.I1.i2.1">both the premise and hypothesis portions.
</span></span>
</span>
As a result, our data sizes varied from roughly 0.5M examples to up to 2.0M examples.</p>
</div>
<div class="ltx_para" id="A5.p2">
<p class="ltx_p" id="A5.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A5.T8" title="Table 8 ‣ Appendix E Ablation Study ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">8</span></a> summarizes the results of the ablation study.
Our findings are twofold.

<span class="ltx_inline-enumerate" id="A5.I2">
<span class="ltx_inline-item" id="A5.I2.i1"><span class="ltx_tag ltx_tag_inline-item">(i)</span> <span class="ltx_text" id="A5.I2.i1.1">There was a minimum practical size of 1.0M examples.
All models trained using lower sizes suffered serious performance degradation.
</span></span>
<span class="ltx_inline-item" id="A5.I2.i2"><span class="ltx_tag ltx_tag_inline-item">(ii)</span> <span class="ltx_text" id="A5.I2.i2.1">We also observed that when the data size was already close to 1.0M examples, the case using the premise portion alone (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A5.T8.sf1" title="8a ‣ Table 8 ‣ Appendix E Ablation Study ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">8a</span></a>) outperformed that using both the premise and hypothesis (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#A5.T8.sf2" title="8b ‣ Table 8 ‣ Appendix E Ablation Study ‣ Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity"><span class="ltx_text ltx_ref_tag">8b</span></a>).
This result is convincing, as the hypothesis portion is artificially created from the premise portion <cite class="ltx_cite ltx_citemacro_cite">Dagan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.05257v1#bib.bib8" title="">2005</a>)</cite> and there is not much variation in the relation between them.
</span></span>
</span></p>
</div>
<figure class="ltx_table" id="A5.T8">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_flex_size_2 ltx_align_center" id="A5.T8.sf1">
<table class="ltx_tabular ltx_align_middle" id="A5.T8.sf1.1">
<tr class="ltx_tr" id="A5.T8.sf1.1.1">
<td class="ltx_td ltx_border_tt" id="A5.T8.sf1.1.1.1"></td>
<td class="ltx_td ltx_border_tt" id="A5.T8.sf1.1.1.2"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A5.T8.sf1.1.1.3"><span class="ltx_text" id="A5.T8.sf1.1.1.3.1" style="font-size:90%;">English</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A5.T8.sf1.1.1.4"><span class="ltx_text" id="A5.T8.sf1.1.1.4.1" style="font-size:90%;">Japanese</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A5.T8.sf1.1.1.5"><span class="ltx_text" id="A5.T8.sf1.1.1.5.1" style="font-size:90%;">Korean</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf1.1.2">
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.2.1"><span class="ltx_text" id="A5.T8.sf1.1.2.1.1" style="font-size:90%;">Models</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.2.2"><span class="ltx_text" id="A5.T8.sf1.1.2.2.1" style="font-size:90%;">Size</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.2.3"><span class="ltx_text" id="A5.T8.sf1.1.2.3.1" style="font-size:90%;">STS-B</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.2.4">
<span class="ltx_text" id="A5.T8.sf1.1.2.4.1"></span><span class="ltx_text" id="A5.T8.sf1.1.2.4.2" style="font-size:90%;"> </span><span class="ltx_text" id="A5.T8.sf1.1.2.4.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.sf1.1.2.4.3.1">
<span class="ltx_tr" id="A5.T8.sf1.1.2.4.3.1.1">
<span class="ltx_td ltx_align_center" id="A5.T8.sf1.1.2.4.3.1.1.1">JGLUE-</span></span>
<span class="ltx_tr" id="A5.T8.sf1.1.2.4.3.1.2">
<span class="ltx_td ltx_align_center" id="A5.T8.sf1.1.2.4.3.1.2.1">JSTS</span></span>
</span></span><span class="ltx_text" id="A5.T8.sf1.1.2.4.4" style="font-size:90%;"> </span><span class="ltx_text" id="A5.T8.sf1.1.2.4.5"></span>
</td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.2.5"><span class="ltx_text" id="A5.T8.sf1.1.2.5.1" style="font-size:90%;">KorSTS</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.2.6">
<span class="ltx_text" id="A5.T8.sf1.1.2.6.1"></span><span class="ltx_text" id="A5.T8.sf1.1.2.6.2" style="font-size:90%;"> </span><span class="ltx_text" id="A5.T8.sf1.1.2.6.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.sf1.1.2.6.3.1">
<span class="ltx_tr" id="A5.T8.sf1.1.2.6.3.1.1">
<span class="ltx_td ltx_align_center" id="A5.T8.sf1.1.2.6.3.1.1.1">KLUE-</span></span>
<span class="ltx_tr" id="A5.T8.sf1.1.2.6.3.1.2">
<span class="ltx_td ltx_align_center" id="A5.T8.sf1.1.2.6.3.1.2.1">STS</span></span>
</span></span><span class="ltx_text" id="A5.T8.sf1.1.2.6.4" style="font-size:90%;"> </span><span class="ltx_text" id="A5.T8.sf1.1.2.6.5"></span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf1.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="6" id="A5.T8.sf1.1.3.1">
<span class="ltx_text ltx_font_italic" id="A5.T8.sf1.1.3.1.1" style="font-size:90%;">Unsupervised</span><span class="ltx_text" id="A5.T8.sf1.1.3.1.2" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf1.1.4">
<td class="ltx_td ltx_align_left" id="A5.T8.sf1.1.4.1">
<span class="ltx_text" id="A5.T8.sf1.1.4.1.1" style="font-size:90%;">XLM-R</span><sub class="ltx_sub" id="A5.T8.sf1.1.4.1.2"><span class="ltx_text" id="A5.T8.sf1.1.4.1.2.1" style="font-size:90%;">base</span></sub>
</td>
<td class="ltx_td" id="A5.T8.sf1.1.4.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.4.3"><span class="ltx_text" id="A5.T8.sf1.1.4.3.1" style="font-size:90%;">53.62</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.4.4"><span class="ltx_text" id="A5.T8.sf1.1.4.4.1" style="font-size:90%;">59.28</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.4.5"><span class="ltx_text" id="A5.T8.sf1.1.4.5.1" style="font-size:90%;">57.98</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.4.6"><span class="ltx_text" id="A5.T8.sf1.1.4.6.1" style="font-size:90%;">30.69</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf1.1.5">
<td class="ltx_td ltx_align_left" id="A5.T8.sf1.1.5.1">
<span class="ltx_text" id="A5.T8.sf1.1.5.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="A5.T8.sf1.1.5.1.2"><span class="ltx_text" id="A5.T8.sf1.1.5.1.2.1" style="font-size:90%;">en</span></sub>
</td>
<td class="ltx_td" id="A5.T8.sf1.1.5.2"></td>
<td class="ltx_td" id="A5.T8.sf1.1.5.3"></td>
<td class="ltx_td" id="A5.T8.sf1.1.5.4"></td>
<td class="ltx_td" id="A5.T8.sf1.1.5.5"></td>
<td class="ltx_td" id="A5.T8.sf1.1.5.6"></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf1.1.6">
<td class="ltx_td ltx_align_left" id="A5.T8.sf1.1.6.1"><span class="ltx_text" id="A5.T8.sf1.1.6.1.1" style="font-size:90%;">+ SNLI</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.6.2"><span class="ltx_text" id="A5.T8.sf1.1.6.2.1" style="font-size:90%;">0.5M</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.6.3"><span class="ltx_text" id="A5.T8.sf1.1.6.3.1" style="font-size:90%;">72.61</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.6.4"><span class="ltx_text" id="A5.T8.sf1.1.6.4.1" style="font-size:90%;">71.56</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.6.5"><span class="ltx_text" id="A5.T8.sf1.1.6.5.1" style="font-size:90%;">69.52</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.6.6"><span class="ltx_text" id="A5.T8.sf1.1.6.6.1" style="font-size:90%;">52.13</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf1.1.7">
<td class="ltx_td ltx_align_left" id="A5.T8.sf1.1.7.1"><span class="ltx_text" id="A5.T8.sf1.1.7.1.1" style="font-size:90%;">+ NLI</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.7.2"><span class="ltx_text" id="A5.T8.sf1.1.7.2.1" style="font-size:90%;">1.0M</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.7.3"><span class="ltx_text" id="A5.T8.sf1.1.7.3.1" style="font-size:90%;">75.79</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.7.4"><span class="ltx_text" id="A5.T8.sf1.1.7.4.1" style="font-size:90%;">71.83</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.7.5"><span class="ltx_text" id="A5.T8.sf1.1.7.5.1" style="font-size:90%;">73.61</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.7.6"><span class="ltx_text" id="A5.T8.sf1.1.7.6.1" style="font-size:90%;">60.66</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf1.1.8">
<td class="ltx_td ltx_align_left" id="A5.T8.sf1.1.8.1">
<span class="ltx_text" id="A5.T8.sf1.1.8.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="A5.T8.sf1.1.8.1.2"><span class="ltx_text" id="A5.T8.sf1.1.8.1.2.1" style="font-size:90%;">MT</span><sub class="ltx_sub" id="A5.T8.sf1.1.8.1.2.2"><span class="ltx_text" id="A5.T8.sf1.1.8.1.2.2.1" style="font-size:90%;">ja</span></sub></sub>
</td>
<td class="ltx_td" id="A5.T8.sf1.1.8.2"></td>
<td class="ltx_td" id="A5.T8.sf1.1.8.3"></td>
<td class="ltx_td" id="A5.T8.sf1.1.8.4"></td>
<td class="ltx_td" id="A5.T8.sf1.1.8.5"></td>
<td class="ltx_td" id="A5.T8.sf1.1.8.6"></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf1.1.9">
<td class="ltx_td ltx_align_left" id="A5.T8.sf1.1.9.1"><span class="ltx_text" id="A5.T8.sf1.1.9.1.1" style="font-size:90%;">+ JSNLI</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.9.2"><span class="ltx_text" id="A5.T8.sf1.1.9.2.1" style="font-size:90%;">0.5M</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.9.3"><span class="ltx_text" id="A5.T8.sf1.1.9.3.1" style="font-size:90%;">72.87</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.9.4"><span class="ltx_text" id="A5.T8.sf1.1.9.4.1" style="font-size:90%;">71.16</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.9.5"><span class="ltx_text" id="A5.T8.sf1.1.9.5.1" style="font-size:90%;">73.48</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.9.6"><span class="ltx_text" id="A5.T8.sf1.1.9.6.1" style="font-size:90%;">63.69</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf1.1.10">
<td class="ltx_td ltx_align_left" id="A5.T8.sf1.1.10.1">
<span class="ltx_text" id="A5.T8.sf1.1.10.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="A5.T8.sf1.1.10.1.2"><span class="ltx_text" id="A5.T8.sf1.1.10.1.2.1" style="font-size:90%;">MT</span><sub class="ltx_sub" id="A5.T8.sf1.1.10.1.2.2"><span class="ltx_text" id="A5.T8.sf1.1.10.1.2.2.1" style="font-size:90%;">kr</span></sub></sub>
</td>
<td class="ltx_td" id="A5.T8.sf1.1.10.2"></td>
<td class="ltx_td" id="A5.T8.sf1.1.10.3"></td>
<td class="ltx_td" id="A5.T8.sf1.1.10.4"></td>
<td class="ltx_td" id="A5.T8.sf1.1.10.5"></td>
<td class="ltx_td" id="A5.T8.sf1.1.10.6"></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf1.1.11">
<td class="ltx_td ltx_align_left" id="A5.T8.sf1.1.11.1"><span class="ltx_text" id="A5.T8.sf1.1.11.1.1" style="font-size:90%;">+ KorNLI</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.11.2"><span class="ltx_text" id="A5.T8.sf1.1.11.2.1" style="font-size:90%;">1.0M</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.11.3"><span class="ltx_text" id="A5.T8.sf1.1.11.3.1" style="font-size:90%;">75.39</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.11.4"><span class="ltx_text" id="A5.T8.sf1.1.11.4.1" style="font-size:90%;">73.02</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.11.5"><span class="ltx_text" id="A5.T8.sf1.1.11.5.1" style="font-size:90%;">73.34</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.11.6"><span class="ltx_text" id="A5.T8.sf1.1.11.6.1" style="font-size:90%;">67.13</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf1.1.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T8.sf1.1.12.1">
<span class="ltx_text" id="A5.T8.sf1.1.12.1.1" style="font-size:90%;">XLM-R</span><sub class="ltx_sub" id="A5.T8.sf1.1.12.1.2"><span class="ltx_text" id="A5.T8.sf1.1.12.1.2.1" style="font-size:90%;">large</span></sub>
</td>
<td class="ltx_td ltx_border_t" id="A5.T8.sf1.1.12.2"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.sf1.1.12.3"><span class="ltx_text" id="A5.T8.sf1.1.12.3.1" style="font-size:90%;">43.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.sf1.1.12.4"><span class="ltx_text" id="A5.T8.sf1.1.12.4.1" style="font-size:90%;">54.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.sf1.1.12.5"><span class="ltx_text" id="A5.T8.sf1.1.12.5.1" style="font-size:90%;">49.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.sf1.1.12.6"><span class="ltx_text" id="A5.T8.sf1.1.12.6.1" style="font-size:90%;">20.13</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf1.1.13">
<td class="ltx_td ltx_align_left" id="A5.T8.sf1.1.13.1">
<span class="ltx_text" id="A5.T8.sf1.1.13.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="A5.T8.sf1.1.13.1.2"><span class="ltx_text" id="A5.T8.sf1.1.13.1.2.1" style="font-size:90%;">en</span></sub>
</td>
<td class="ltx_td" id="A5.T8.sf1.1.13.2"></td>
<td class="ltx_td" id="A5.T8.sf1.1.13.3"></td>
<td class="ltx_td" id="A5.T8.sf1.1.13.4"></td>
<td class="ltx_td" id="A5.T8.sf1.1.13.5"></td>
<td class="ltx_td" id="A5.T8.sf1.1.13.6"></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf1.1.14">
<td class="ltx_td ltx_align_left" id="A5.T8.sf1.1.14.1"><span class="ltx_text" id="A5.T8.sf1.1.14.1.1" style="font-size:90%;">+ NLI</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.14.2"><span class="ltx_text" id="A5.T8.sf1.1.14.2.1" style="font-size:90%;">1.0M</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.14.3"><span class="ltx_text" id="A5.T8.sf1.1.14.3.1" style="font-size:90%;">78.66</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.14.4"><span class="ltx_text" id="A5.T8.sf1.1.14.4.1" style="font-size:90%;">74.12</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.14.5"><span class="ltx_text" id="A5.T8.sf1.1.14.5.1" style="font-size:90%;">74.68</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf1.1.14.6"><span class="ltx_text" id="A5.T8.sf1.1.14.6.1" style="font-size:90%;">72.87</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf1.1.15">
<td class="ltx_td ltx_align_left" id="A5.T8.sf1.1.15.1">
<span class="ltx_text" id="A5.T8.sf1.1.15.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="A5.T8.sf1.1.15.1.2"><span class="ltx_text" id="A5.T8.sf1.1.15.1.2.1" style="font-size:90%;">MT</span><sub class="ltx_sub" id="A5.T8.sf1.1.15.1.2.2"><span class="ltx_text" id="A5.T8.sf1.1.15.1.2.2.1" style="font-size:90%;">kr</span></sub></sub>
</td>
<td class="ltx_td" id="A5.T8.sf1.1.15.2"></td>
<td class="ltx_td" id="A5.T8.sf1.1.15.3"></td>
<td class="ltx_td" id="A5.T8.sf1.1.15.4"></td>
<td class="ltx_td" id="A5.T8.sf1.1.15.5"></td>
<td class="ltx_td" id="A5.T8.sf1.1.15.6"></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf1.1.16">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A5.T8.sf1.1.16.1"><span class="ltx_text" id="A5.T8.sf1.1.16.1.1" style="font-size:90%;">+ KorNLI</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T8.sf1.1.16.2"><span class="ltx_text" id="A5.T8.sf1.1.16.2.1" style="font-size:90%;">1.0M</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T8.sf1.1.16.3"><span class="ltx_text ltx_font_bold" id="A5.T8.sf1.1.16.3.1" style="font-size:90%;">80.29</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T8.sf1.1.16.4"><span class="ltx_text" id="A5.T8.sf1.1.16.4.1" style="font-size:90%;">74.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T8.sf1.1.16.5"><span class="ltx_text ltx_font_bold" id="A5.T8.sf1.1.16.5.1" style="font-size:90%;">75.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T8.sf1.1.16.6"><span class="ltx_text ltx_font_bold" id="A5.T8.sf1.1.16.6.1" style="font-size:90%;">74.45</span></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">(a) </span>The premise alone.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_table ltx_flex_size_2 ltx_align_center" id="A5.T8.sf2">
<table class="ltx_tabular ltx_align_middle" id="A5.T8.sf2.1">
<tr class="ltx_tr" id="A5.T8.sf2.1.1">
<td class="ltx_td ltx_border_tt" id="A5.T8.sf2.1.1.1"></td>
<td class="ltx_td ltx_border_tt" id="A5.T8.sf2.1.1.2"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A5.T8.sf2.1.1.3"><span class="ltx_text" id="A5.T8.sf2.1.1.3.1" style="font-size:90%;">English</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A5.T8.sf2.1.1.4"><span class="ltx_text" id="A5.T8.sf2.1.1.4.1" style="font-size:90%;">Japanese</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A5.T8.sf2.1.1.5"><span class="ltx_text" id="A5.T8.sf2.1.1.5.1" style="font-size:90%;">Korean</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf2.1.2">
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.2.1"><span class="ltx_text" id="A5.T8.sf2.1.2.1.1" style="font-size:90%;">Models</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.2.2"><span class="ltx_text" id="A5.T8.sf2.1.2.2.1" style="font-size:90%;">Size</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.2.3"><span class="ltx_text" id="A5.T8.sf2.1.2.3.1" style="font-size:90%;">STS-B</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.2.4">
<span class="ltx_text" id="A5.T8.sf2.1.2.4.1"></span><span class="ltx_text" id="A5.T8.sf2.1.2.4.2" style="font-size:90%;"> </span><span class="ltx_text" id="A5.T8.sf2.1.2.4.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.sf2.1.2.4.3.1">
<span class="ltx_tr" id="A5.T8.sf2.1.2.4.3.1.1">
<span class="ltx_td ltx_align_center" id="A5.T8.sf2.1.2.4.3.1.1.1">JGLUE-</span></span>
<span class="ltx_tr" id="A5.T8.sf2.1.2.4.3.1.2">
<span class="ltx_td ltx_align_center" id="A5.T8.sf2.1.2.4.3.1.2.1">JSTS</span></span>
</span></span><span class="ltx_text" id="A5.T8.sf2.1.2.4.4" style="font-size:90%;"> </span><span class="ltx_text" id="A5.T8.sf2.1.2.4.5"></span>
</td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.2.5"><span class="ltx_text" id="A5.T8.sf2.1.2.5.1" style="font-size:90%;">KorSTS</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.2.6">
<span class="ltx_text" id="A5.T8.sf2.1.2.6.1"></span><span class="ltx_text" id="A5.T8.sf2.1.2.6.2" style="font-size:90%;"> </span><span class="ltx_text" id="A5.T8.sf2.1.2.6.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.sf2.1.2.6.3.1">
<span class="ltx_tr" id="A5.T8.sf2.1.2.6.3.1.1">
<span class="ltx_td ltx_align_center" id="A5.T8.sf2.1.2.6.3.1.1.1">KLUE-</span></span>
<span class="ltx_tr" id="A5.T8.sf2.1.2.6.3.1.2">
<span class="ltx_td ltx_align_center" id="A5.T8.sf2.1.2.6.3.1.2.1">STS</span></span>
</span></span><span class="ltx_text" id="A5.T8.sf2.1.2.6.4" style="font-size:90%;"> </span><span class="ltx_text" id="A5.T8.sf2.1.2.6.5"></span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf2.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="6" id="A5.T8.sf2.1.3.1">
<span class="ltx_text ltx_font_italic" id="A5.T8.sf2.1.3.1.1" style="font-size:90%;">Unsupervised</span><span class="ltx_text" id="A5.T8.sf2.1.3.1.2" style="font-size:90%;"></span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf2.1.4">
<td class="ltx_td ltx_align_left" id="A5.T8.sf2.1.4.1">
<span class="ltx_text" id="A5.T8.sf2.1.4.1.1" style="font-size:90%;">XLM-R</span><sub class="ltx_sub" id="A5.T8.sf2.1.4.1.2"><span class="ltx_text" id="A5.T8.sf2.1.4.1.2.1" style="font-size:90%;">base</span></sub>
</td>
<td class="ltx_td" id="A5.T8.sf2.1.4.2"></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.4.3"><span class="ltx_text" id="A5.T8.sf2.1.4.3.1" style="font-size:90%;">53.62</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.4.4"><span class="ltx_text" id="A5.T8.sf2.1.4.4.1" style="font-size:90%;">59.28</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.4.5"><span class="ltx_text" id="A5.T8.sf2.1.4.5.1" style="font-size:90%;">57.98</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.4.6"><span class="ltx_text" id="A5.T8.sf2.1.4.6.1" style="font-size:90%;">30.69</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf2.1.5">
<td class="ltx_td ltx_align_left" id="A5.T8.sf2.1.5.1">
<span class="ltx_text" id="A5.T8.sf2.1.5.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="A5.T8.sf2.1.5.1.2"><span class="ltx_text" id="A5.T8.sf2.1.5.1.2.1" style="font-size:90%;">en</span></sub>
</td>
<td class="ltx_td" id="A5.T8.sf2.1.5.2"></td>
<td class="ltx_td" id="A5.T8.sf2.1.5.3"></td>
<td class="ltx_td" id="A5.T8.sf2.1.5.4"></td>
<td class="ltx_td" id="A5.T8.sf2.1.5.5"></td>
<td class="ltx_td" id="A5.T8.sf2.1.5.6"></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf2.1.6">
<td class="ltx_td ltx_align_left" id="A5.T8.sf2.1.6.1"><span class="ltx_text" id="A5.T8.sf2.1.6.1.1" style="font-size:90%;">+ SNLI</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.6.2"><span class="ltx_text" id="A5.T8.sf2.1.6.2.1" style="font-size:90%;">1.0M</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.6.3"><span class="ltx_text" id="A5.T8.sf2.1.6.3.1" style="font-size:90%;">75.03</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.6.4"><span class="ltx_text" id="A5.T8.sf2.1.6.4.1" style="font-size:90%;">72.69</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.6.5"><span class="ltx_text" id="A5.T8.sf2.1.6.5.1" style="font-size:90%;">73.71</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.6.6"><span class="ltx_text" id="A5.T8.sf2.1.6.6.1" style="font-size:90%;">58.75</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf2.1.7">
<td class="ltx_td ltx_align_left" id="A5.T8.sf2.1.7.1"><span class="ltx_text" id="A5.T8.sf2.1.7.1.1" style="font-size:90%;">+ NLI</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.7.2"><span class="ltx_text" id="A5.T8.sf2.1.7.2.1" style="font-size:90%;">2.0M</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.7.3"><span class="ltx_text" id="A5.T8.sf2.1.7.3.1" style="font-size:90%;">75.83</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.7.4"><span class="ltx_text" id="A5.T8.sf2.1.7.4.1" style="font-size:90%;">73.12</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.7.5"><span class="ltx_text" id="A5.T8.sf2.1.7.5.1" style="font-size:90%;">74.18</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.7.6"><span class="ltx_text" id="A5.T8.sf2.1.7.6.1" style="font-size:90%;">61.75</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf2.1.8">
<td class="ltx_td ltx_align_left" id="A5.T8.sf2.1.8.1">
<span class="ltx_text" id="A5.T8.sf2.1.8.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="A5.T8.sf2.1.8.1.2"><span class="ltx_text" id="A5.T8.sf2.1.8.1.2.1" style="font-size:90%;">MT</span><sub class="ltx_sub" id="A5.T8.sf2.1.8.1.2.2"><span class="ltx_text" id="A5.T8.sf2.1.8.1.2.2.1" style="font-size:90%;">ja</span></sub></sub>
</td>
<td class="ltx_td" id="A5.T8.sf2.1.8.2"></td>
<td class="ltx_td" id="A5.T8.sf2.1.8.3"></td>
<td class="ltx_td" id="A5.T8.sf2.1.8.4"></td>
<td class="ltx_td" id="A5.T8.sf2.1.8.5"></td>
<td class="ltx_td" id="A5.T8.sf2.1.8.6"></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf2.1.9">
<td class="ltx_td ltx_align_left" id="A5.T8.sf2.1.9.1"><span class="ltx_text" id="A5.T8.sf2.1.9.1.1" style="font-size:90%;">+ JSNLI</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.9.2"><span class="ltx_text" id="A5.T8.sf2.1.9.2.1" style="font-size:90%;">1.0M</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.9.3"><span class="ltx_text" id="A5.T8.sf2.1.9.3.1" style="font-size:90%;">75.26</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.9.4"><span class="ltx_text" id="A5.T8.sf2.1.9.4.1" style="font-size:90%;">71.69</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.9.5"><span class="ltx_text" id="A5.T8.sf2.1.9.5.1" style="font-size:90%;">74.20</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.9.6"><span class="ltx_text" id="A5.T8.sf2.1.9.6.1" style="font-size:90%;">65.39</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf2.1.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="A5.T8.sf2.1.10.1">
<span class="ltx_text" id="A5.T8.sf2.1.10.1.1" style="font-size:90%;">XLM-R</span><sub class="ltx_sub" id="A5.T8.sf2.1.10.1.2"><span class="ltx_text" id="A5.T8.sf2.1.10.1.2.1" style="font-size:90%;">large</span></sub>
</td>
<td class="ltx_td ltx_border_t" id="A5.T8.sf2.1.10.2"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.sf2.1.10.3"><span class="ltx_text" id="A5.T8.sf2.1.10.3.1" style="font-size:90%;">43.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.sf2.1.10.4"><span class="ltx_text" id="A5.T8.sf2.1.10.4.1" style="font-size:90%;">54.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.sf2.1.10.5"><span class="ltx_text" id="A5.T8.sf2.1.10.5.1" style="font-size:90%;">49.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.sf2.1.10.6"><span class="ltx_text" id="A5.T8.sf2.1.10.6.1" style="font-size:90%;">20.13</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf2.1.11">
<td class="ltx_td ltx_align_left" id="A5.T8.sf2.1.11.1">
<span class="ltx_text" id="A5.T8.sf2.1.11.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="A5.T8.sf2.1.11.1.2"><span class="ltx_text" id="A5.T8.sf2.1.11.1.2.1" style="font-size:90%;">en</span></sub>
</td>
<td class="ltx_td" id="A5.T8.sf2.1.11.2"></td>
<td class="ltx_td" id="A5.T8.sf2.1.11.3"></td>
<td class="ltx_td" id="A5.T8.sf2.1.11.4"></td>
<td class="ltx_td" id="A5.T8.sf2.1.11.5"></td>
<td class="ltx_td" id="A5.T8.sf2.1.11.6"></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf2.1.12">
<td class="ltx_td ltx_align_left" id="A5.T8.sf2.1.12.1"><span class="ltx_text" id="A5.T8.sf2.1.12.1.1" style="font-size:90%;">+ SNLI</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.12.2"><span class="ltx_text" id="A5.T8.sf2.1.12.2.1" style="font-size:90%;">1.0M</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.12.3"><span class="ltx_text" id="A5.T8.sf2.1.12.3.1" style="font-size:90%;">75.36</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.12.4"><span class="ltx_text" id="A5.T8.sf2.1.12.4.1" style="font-size:90%;">74.73</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.12.5"><span class="ltx_text" id="A5.T8.sf2.1.12.5.1" style="font-size:90%;">75.21</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.12.6"><span class="ltx_text" id="A5.T8.sf2.1.12.6.1" style="font-size:90%;">65.83</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf2.1.13">
<td class="ltx_td ltx_align_left" id="A5.T8.sf2.1.13.1"><span class="ltx_text" id="A5.T8.sf2.1.13.1.1" style="font-size:90%;">+ NLI</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.13.2"><span class="ltx_text" id="A5.T8.sf2.1.13.2.1" style="font-size:90%;">2.0M</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.13.3"><span class="ltx_text" id="A5.T8.sf2.1.13.3.1" style="font-size:90%;">76.87</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.13.4"><span class="ltx_text" id="A5.T8.sf2.1.13.4.1" style="font-size:90%;">75.19</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.13.5"><span class="ltx_text" id="A5.T8.sf2.1.13.5.1" style="font-size:90%;">74.19</span></td>
<td class="ltx_td ltx_align_center" id="A5.T8.sf2.1.13.6"><span class="ltx_text" id="A5.T8.sf2.1.13.6.1" style="font-size:90%;">70.60</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf2.1.14">
<td class="ltx_td ltx_align_left" id="A5.T8.sf2.1.14.1">
<span class="ltx_text" id="A5.T8.sf2.1.14.1.1" style="font-size:90%;">mSimCSE</span><sub class="ltx_sub" id="A5.T8.sf2.1.14.1.2"><span class="ltx_text" id="A5.T8.sf2.1.14.1.2.1" style="font-size:90%;">MT</span><sub class="ltx_sub" id="A5.T8.sf2.1.14.1.2.2"><span class="ltx_text" id="A5.T8.sf2.1.14.1.2.2.1" style="font-size:90%;">ja</span></sub></sub>
</td>
<td class="ltx_td" id="A5.T8.sf2.1.14.2"></td>
<td class="ltx_td" id="A5.T8.sf2.1.14.3"></td>
<td class="ltx_td" id="A5.T8.sf2.1.14.4"></td>
<td class="ltx_td" id="A5.T8.sf2.1.14.5"></td>
<td class="ltx_td" id="A5.T8.sf2.1.14.6"></td>
</tr>
<tr class="ltx_tr" id="A5.T8.sf2.1.15">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A5.T8.sf2.1.15.1"><span class="ltx_text" id="A5.T8.sf2.1.15.1.1" style="font-size:90%;">+ JSNLI</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T8.sf2.1.15.2"><span class="ltx_text" id="A5.T8.sf2.1.15.2.1" style="font-size:90%;">1.0M</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T8.sf2.1.15.3"><span class="ltx_text" id="A5.T8.sf2.1.15.3.1" style="font-size:90%;">79.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T8.sf2.1.15.4"><span class="ltx_text ltx_font_bold" id="A5.T8.sf2.1.15.4.1" style="font-size:90%;">75.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T8.sf2.1.15.5"><span class="ltx_text" id="A5.T8.sf2.1.15.5.1" style="font-size:90%;">75.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T8.sf2.1.15.6"><span class="ltx_text" id="A5.T8.sf2.1.15.6.1" style="font-size:90%;">73.97</span></td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">(b) </span>Both the premise and hypothesis.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 8: </span>
Ablation study on different data sizes, data combinations, and base models.
<span class="ltx_text ltx_font_bold" id="A5.T8.5.1">Boldface</span> only highlights the best performance values over the two combinations of the premise alone and both the premise and hypothesis.
</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Mar  8 08:46:02 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
