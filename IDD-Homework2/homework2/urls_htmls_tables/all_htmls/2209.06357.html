<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2209.06357] DASH: Visual Analytics for Debiasing Image Classification via User-Driven Synthetic Data Augmentation</title><meta property="og:description" content="Image classification models often learn to predict a class based on irrelevant co-occurrences between input features and an output class in training data. We call the unwanted correlations “data biases,” and the visual…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DASH: Visual Analytics for Debiasing Image Classification via User-Driven Synthetic Data Augmentation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="DASH: Visual Analytics for Debiasing Image Classification via User-Driven Synthetic Data Augmentation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2209.06357">

<!--Generated on Thu Mar 14 00:12:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\EuroVisShort</span><span id="p1.2" class="ltx_ERROR undefined">\BibtexOrBiblatex</span><span id="p1.3" class="ltx_ERROR undefined">\electronicVersion</span><span id="p1.4" class="ltx_ERROR undefined">\PrintedOrElectronic</span>
</div>
<div id="p2" class="ltx_para">
<span id="p2.2" class="ltx_ERROR undefined">\teaser</span>
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;">
<img src="/html/2209.06357/assets/x1.png" id="p2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="239" alt="[Uncaptioned image]">
</span>
<span id="p2.1.2" class="ltx_text ltx_caption ltx_align_center">An overview of DASH: (A) Projection View shows the latent space representation of images using t-SNE; (B) Mosaic View summarizes the performance differences between two previously trained classifiers; (C) Trace View shows how the two classifiers predict individual images differently (red: incorrect to correct, blue: correct to incorrect); (D) Grad-CAM View shows the feature importance of images as heatmaps; (E) Image View shows a list of selected images; (F) Cluster GAN View shows clustering results which can be used to translate visual features of images to other images using XploreGAN; (G) Augmented Image View shows newly created images for retraining and (H) shows the summary of the new images; (I) Classifier Board shows the performance of different classifiers.</span></p>
</div>
<h1 class="ltx_title ltx_title_document">DASH: Visual Analytics for Debiasing Image Classification
<br class="ltx_break">via User-Driven Synthetic Data Augmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id6.6.6" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:433.6pt;">
<span id="id6.6.6.6" class="ltx_p">Bum Chul Kwon<sup id="id6.6.6.6.1" class="ltx_sup"><span id="id6.6.6.6.1.1" class="ltx_text ltx_font_italic">1</span></sup><span id="id6.6.6.6.2" class="ltx_ERROR undefined">\orcid</span>0000-0002-9391-6274, Jungsoo Lee<sup id="id6.6.6.6.3" class="ltx_sup"><span id="id6.6.6.6.3.1" class="ltx_text ltx_font_italic">2</span></sup>, Chaeyeon Chung<sup id="id6.6.6.6.4" class="ltx_sup"><span id="id6.6.6.6.4.1" class="ltx_text ltx_font_italic">2</span></sup>, Nyoungwoo Lee<sup id="id6.6.6.6.5" class="ltx_sup"><span id="id6.6.6.6.5.1" class="ltx_text ltx_font_italic">2</span></sup><span id="id6.6.6.6.6" class="ltx_ERROR undefined">\orcid</span>0000-0001-5660-133X, Ho-Jin Choi<sup id="id6.6.6.6.7" class="ltx_sup"><span id="id6.6.6.6.7.1" class="ltx_text ltx_font_italic">2</span></sup><span id="id6.6.6.6.8" class="ltx_ERROR undefined">\orcid</span>0000-0002-3398-9543, Jaegul Choo<sup id="id6.6.6.6.9" class="ltx_sup"><span id="id6.6.6.6.9.1" class="ltx_text ltx_font_italic">2</span></sup><span id="id6.6.6.6.10" class="ltx_ERROR undefined">\orcid</span>0000-0003-1071-4835</span>
</span>

<br class="ltx_break">
<span id="id8.8.8" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:433.6pt;">
<span id="id7.7.7.1" class="ltx_p"><sup id="id7.7.7.1.1" class="ltx_sup">1</sup>IBM Research, Cambridge, Massachusetts, United States</span>
<span id="id8.8.8.2" class="ltx_p ltx_align_center"><sup id="id8.8.8.2.1" class="ltx_sup">2</sup> KAIST, Daejeon, Republic of Korea
</span>
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="9.1" class="ltx_p">Image classification models often learn to predict a class based on irrelevant co-occurrences between input features and an output class in training data. We call the unwanted correlations “data biases,” and the visual features causing data biases “bias factors.” It is challenging to identify and mitigate biases automatically without human intervention. Therefore, we conducted a design study to find a human-in-the-loop solution. First, we identified user tasks that capture the bias mitigation process for image classification models with three experts. Then, to support the tasks, we developed a visual analytics system called DASH that allows users to visually identify bias factors, to iteratively generate synthetic images using a state-of-the-art image-to-image translation model, and to supervise the model training process for improving the classification accuracy. Our quantitative evaluation and qualitative study with ten participants demonstrate the usefulness of DASH and provide lessons for future work.</p>
<span id="10.2" class="ltx_ERROR undefined">{CCSXML}</span>
<p id="11.3" class="ltx_p">&lt;ccs2012&gt;
&lt;concept&gt;
&lt;concept_id&gt;10003120.10003145.10003147.10010365&lt;/concept_id&gt;
&lt;concept_desc&gt;Human-centered computing Visual analytics&lt;/concept_desc&gt;
&lt;concept_significance&gt;500&lt;/concept_significance&gt;
&lt;/concept&gt;
&lt;/ccs2012&gt;</p>
<span id="12.4" class="ltx_ERROR undefined">\ccsdesc</span>
<p id="13.5" class="ltx_p">[500]Human-centered computing Visual analytics</p>
<span id="14.6" class="ltx_ERROR undefined">\printccsdesc</span>
</div>
<span id="15" class="ltx_note ltx_note_frontmatter ltx_role_volume"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">volume: </span>41</span></span></span><span id="16" class="ltx_note ltx_note_frontmatter ltx_role_issue"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">issue: </span>3</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Image classification models often learn to predict an output class based on irrelevant features co-occurring with the class within images in training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">SMG<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>20</a>]</cite>.
We call the undesirable correlation between some visual features and class labels in training data as “data biases,” and refer to such visual features causing the biases as “bias factors.”
For example, as illustrated in the previous literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">BCY<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>20</a>]</cite>, many images of ‘frogs’ in training data are taken with ‘swamps’ in the background.
Image classification models often make mistakes by predicting the class of frogs based on swamps in the background.
In this example, swamp is a bias factor that causes the image classification model to be biased for the class label frogs.
Though biased models may provide high accuracy in training data, they can result in fatal errors on unseen data beyond training data.
Therefore, it is important for data scientists to identify and mitigate biases in models before deploying them.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To resolve data biases, image classification models need to unlearn irrelevant features and learn more important features that are related to the output class.
In this context, data augmentation can be a viable solution that can generate synthetic images by artificially combining existing ones into new images.
However, it is difficult to automatically identify bias factors of given models and generate images that can effectively target and remove the unwanted correlations.
The process is often iterative and labor-intensive because data scientists need to inspect the models to discover bias factors among many potential features, generate images by augmenting existing images, and re-train and evaluate the model so that it can achieve a higher performance in testing data.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we conducted a design study with thirteen experts in image classification to develop a visual analytics system for the model debiasing problem.
First, we analyzed the user tasks with three experts to understand the model debiasing process.
Second, based on the user tasks, we developed a visual analytics system called DASH (<span id="S1.p3.1.1" class="ltx_text ltx_font_bold">D</span>ata <span id="S1.p3.1.2" class="ltx_text ltx_font_bold">A</span>ugmentation <span id="S1.p3.1.3" class="ltx_text ltx_font_bold">S</span>ystem for <span id="S1.p3.1.4" class="ltx_text ltx_font_bold">H</span>uman-in-the-loop).
DASH allows data scientists to visually identify bias factors among non-trivial visual attributes of images (e.g., colors and object presence).
Using DASH, they can also synthesize new images by translating target attributes using a state-of-the-art image-to-image translation technique called XploreGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">BCYC20</a>]</cite> and evaluate the quality of the generated images.
Finally, DASH allows them to retrain the model with the newly created images and evaluate the performance of the revised model against previous models.
To evaluate DASH, we conducted a user study with ten machine learning experts on two real-world datasets.
The results demonstrate that DASH helps data scientists discover and mitigate biases of image classification models.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our main contributions include:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We identify user tasks with three experts that capture the user-driven debiasing process for image classification models.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We present a visual analytics system called DASH that allows users to identify the bias factors, to synthesize new images using image-to-image translation, and to visually supervise the model retraining process.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We conduct a qualitative evaluation with ten machine experts to show the effectiveness of using DASH for debiasing image classification models.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2209.06357/assets/x2.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="283" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>In Projection View, users find clusters of images. Circles and crosses represent correctly classified and misclassified images, respectively. The color inside each point shows its true class label, and the color of the stroke (border) shows its predicted class. Users can zoom in to view actual images.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>User Tasks: Model Debiasing Process</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We derived the tasks based on discussions among the co-authors of this work, who are experts in the fields of computer vision.
The following tasks represent high-level objects that users need to perform in order to mitigate biases in image classification models.
We assume that users already have trained an image classification model with a training dataset.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">T1: <span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Discover data biases in training data.</span>
With the initial model, users generate the accuracy of the model on test (unseen) data.
The model generates lower accuracy, so the users investigate the source of errors.
The errors usually originate from homogeneous distributions of training data, which include unintended correlation between visual attributes of images (swamp) and classes (frog) in training data.
Users filter data by the specific class labels that cause errors in test data and then derive irregularities.
They often use activation maps like GradCAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">SCD<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>17</a>]</cite> to highlight important regions of images that the model uses for the classification task.
After iterative exploration, they hypothesize the unwanted correlation between some visual features and class labels.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">T2: <span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Mitigate bias through data augmentation.</span>
Once users identified the sources of errors (target visual features to unlearn), users need to generate new images.
Users need to generate images of frogs with diverse backgrounds like street, house, and tree.
Users can use an image translation technique to change an attribute (swamp) of an image to another attribute (street) from other images without altering other attributes (frog).
Image translation models require a “source” image including the class label and a “style” image containing diverse attributes (e.g., street, house, tree) to be fused into the source image.
After training the image translation models, users evaluate how realistic the resulting image is.
Once satisfied with the quality, users generate new images with diverse backgrounds and label them with the target class for retraining.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">T3: <span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Retrain, evaluate, and steer the classifier.</span>
With the newly generated images, users retrain the image classification model.
In this process, users adjust model parameters and hyperparameters (e.g., epoch number, batch size, learning rate) to maximize the learning outcome.
After retraining the classifier, users evaluate the performance of classification model.
In addition to accuracy, users need to assess whether the revised model correctly classify images of the target label.
In particular, it is important to test whether the fused visual feature was helpful to resolve the target biases in the model.
This process cannot be done at once.
Some images that users generated may not help the model to improve their accuracy.
In the worst case, some images may decrease the accuracy of the model.
Then, users can discard the model instance and retrain the previous version of the model with newly generated images.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>DASH: Visual Analytics for Data Augmentation</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Based on the user tasks, we developed DASH which includes multiple, coordinated views.
The following sections describe how the system supports the bias mitigation tasks.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Projection View</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Projection View presents the overall distribution of the latent space in the training and validation sets with a two-dimensional scatter plot where each data point indicates each image.
Following the recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">WGYS18</a>, <a href="#bib.bibx6" title="" class="ltx_ref">CPY<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>19</a>, <a href="#bib.bibx25" title="" class="ltx_ref">STN<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>16</a>, <a href="#bib.bibx7" title="" class="ltx_ref">CRHC18</a>, <a href="#bib.bibx12" title="" class="ltx_ref">KEV<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>18</a>]</cite>, we created a two-dimensional scatter plot of images using t-SNE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">vdMH08</a>]</cite>.
We used each image’s latent space representation extracted from the last convolutional layer of the image classification model.
By doing so, the representation of each image captures its high-level semantics (e.g., background colors, objects, texture) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">BZK<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>17</a>]</cite>.
<a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ DASH: Visual Analytics for Debiasing Image Classification via User-Driven Synthetic Data Augmentation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a> (a) shows that Projection View separates images by the fruit types in the training set based on colors.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">By exploring Projection View, users can discover a noticeable difference in the data distributions between the training and validation sets (<span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">T1</span>).
A contour plot of Projection View indicates the estimated density of image clouds.
In addition, users can check whether each data point is predicted correctly or incorrectly with the marker shape, a circle (correct) or a cross (incorrect) respectively, which are colored differently according to its ground truth class (<span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_bold">T1</span>).
We allow users to zoom into a cluster, where each point turns into the actual image at the maximum zoom level.
They can also gain the additional information of an image, such as its class label, predicted label, and prediction loss value, in a popup.
They can also select a group of data points by using lasso-selection to load the actual images on Image View right below Projection View.
The validation set of <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ DASH: Visual Analytics for Debiasing Image Classification via User-Driven Synthetic Data Augmentation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a> (a) shows misclassified items, which include images of fruits in their unusual colors (e.g., green bananas).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Grad-CAM View</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Grad-CAM View helps users to understand to which areas of an image the model attributes more importance while making decisions (<span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">T1</span>).
One can also consider using other existing explanation methods, such as saliency maps, Guided BackProp <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">SDBR15</a>]</cite>, and Guided Grad-CAM View <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">RDV<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>16</a>]</cite>.
As <span class="ltx_ref ltx_refmacro_autoref ltx_ref_self"><span class="ltx_text ltx_ref_title">DASH: Visual Analytics for Debiasing Image Classification<span class="ltx_text"> </span>via User-Driven Synthetic Data Augmentation</span></span> (D) shows, Grad-CAM View shows the heatmap.
By interpreting Grad-CAM heatmaps over the images, users can estimate the regions that are correlated with its predicted class label.
For example, as the first column of <a href="#S3.F2" title="Figure 2 ‣ 3.2 Grad-CAM View ‣ 3 DASH: Visual Analytics for Data Augmentation ‣ DASH: Visual Analytics for Debiasing Image Classification via User-Driven Synthetic Data Augmentation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> (a) shows, the model accurately classifies an image of an orange with the focus on the orange.
On the other hand, the model misclassifies another image of an orange as an apple in the second column of <a href="#S3.F2" title="Figure 2 ‣ 3.2 Grad-CAM View ‣ 3 DASH: Visual Analytics for Data Augmentation ‣ DASH: Visual Analytics for Debiasing Image Classification via User-Driven Synthetic Data Augmentation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> (a).
Grad-CAM View shows that the model focuses on the peripheral region of the image instead of the green orange in the center.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2209.06357/assets/x3.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="365" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Grad-CAM View shows the areas that the model gives the most attention as heatmaps (Red: High; Blue: Low). The three rows indicate i) original image; ii) heatmap; iii) original <math id="S3.F2.2.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S3.F2.2.m1.1b"><mo id="S3.F2.2.m1.1.1" xref="S3.F2.2.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S3.F2.2.m1.1c"><plus id="S3.F2.2.m1.1.1.cmml" xref="S3.F2.2.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.m1.1d">+</annotation></semantics></math> heatmap. Users can infer where the model should focus on by comparing the correctly classified and misclassified images in Grad-CAM View.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Cluster GAN View</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Cluster GAN View presents groups of images by discovering clusters of the latent space vectors (<span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">T2</span>).
We chose XploreGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">BCYC20</a>]</cite> over other image translation models (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">CCK<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>18</a>, <a href="#bib.bibx8" title="" class="ltx_ref">HLBK18</a>, <a href="#bib.bibx16" title="" class="ltx_ref">LTH<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>18</a>, <a href="#bib.bibx10" title="" class="ltx_ref">IZZE17</a>, <a href="#bib.bibx29" title="" class="ltx_ref">ZPIE17</a>, <a href="#bib.bibx15" title="" class="ltx_ref">LDX<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>19</a>]</cite>) because the model can generate new images without predefined labels for style features (e.g., swamp).
XploreGAN clusters images into <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">K</annotation></semantics></math> groups using K-Means clustering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">HW79</a>]</cite>, then uses the cluster indices as pseudo labels for style features shared by the images within each cluster.
Then, XploreGAN can transfer visual features present in the images of a cluster to other images.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">As <span class="ltx_ref ltx_refmacro_autoref ltx_ref_self"><span class="ltx_text ltx_ref_title">DASH: Visual Analytics for Debiasing Image Classification<span class="ltx_text"> </span>via User-Driven Synthetic Data Augmentation</span></span> (F) presents, users can run clustering by adjusting the target number of clusters ranging from 2 to 20.
Once the clustering completes, it shows a table with columns (clusters) of <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">N</annotation></semantics></math> representative images that are the closest to the centroid of their respective clusters.
Users choose a column (highlighted in orange in <span class="ltx_ref ltx_refmacro_autoref ltx_ref_self"><span class="ltx_text ltx_ref_title">DASH: Visual Analytics for Debiasing Image Classification<span class="ltx_text"> </span>via User-Driven Synthetic Data Augmentation</span></span> (F)) to use the images in the cluster as a target style images and choose an image from Image View as a source image in <span class="ltx_ref ltx_refmacro_autoref ltx_ref_self"><span class="ltx_text ltx_ref_title">DASH: Visual Analytics for Debiasing Image Classification<span class="ltx_text"> </span>via User-Driven Synthetic Data Augmentation</span></span> (E).
After generating new images, users can validate the quality of the generated images on Augmented Image View as <span class="ltx_ref ltx_refmacro_autoref ltx_ref_self"><span class="ltx_text ltx_ref_title">DASH: Visual Analytics for Debiasing Image Classification<span class="ltx_text"> </span>via User-Driven Synthetic Data Augmentation</span></span> (G) shows (<span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">T2</span>).</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Classifier Board, Mosaic View, and Trace View</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Classifier Board allows users to visually supervise the retraining process and to navigate the results (<span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_bold">T3</span>).
While retraining, Classifier Board shows loss values of training and validation sets for every epoch in a line chart (red: training, blue: validation).
Moreover, Classifier Board enables users to switch back and forth among the previously trained models (<span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_bold">T3</span>).
By doing so, users can discard unsuccessful retraining attempts if necessary.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Mosaic View shows the differences in classification results between two different models (<span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_bold">T3</span>).
Inspired by prior studies on confusion matrix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">KLTH10</a>, <a href="#bib.bibx26" title="" class="ltx_ref">Tor13</a>, <a href="#bib.bibx1" title="" class="ltx_ref">AHH<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>14</a>, <a href="#bib.bibx18" title="" class="ltx_ref">RAL<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>17</a>]</cite>, Mosaic View highlights the cell-level differences using a confusion matrix.
Each cell in Mosaic View changes its size in proportion to the number of images in the corresponding cell.
This allows users to understand the overall model performance at a glance (<span id="S3.SS4.p2.1.2" class="ltx_text ltx_font_bold">T1</span>).
Users can click on cells of interest by using CTRL <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mo id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><plus id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">+</annotation></semantics></math> Click, and it filters other views by the images in the cells.
For instance, users can select the two cells in (2,1) and (2,3) of <span class="ltx_ref ltx_refmacro_autoref ltx_ref_self"><span class="ltx_text ltx_ref_title">DASH: Visual Analytics for Debiasing Image Classification<span class="ltx_text"> </span>via User-Driven Synthetic Data Augmentation</span></span> (B) to inspect the images of ‘banana’, which are misclassified as ‘apple’ and ‘orange’, respectively.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">Trace View summarizes how individual images are predicted differently from different model instances.
In Trace View, points in the upper row are correctly classified images while those in the lower row are incorrect ones, as <span class="ltx_ref ltx_refmacro_autoref ltx_ref_self"><span class="ltx_text ltx_ref_title">DASH: Visual Analytics for Debiasing Image Classification<span class="ltx_text"> </span>via User-Driven Synthetic Data Augmentation</span></span> (C) presents.
Red lines indicate that the items were previously predicted incorrectly; blue lines show that the items were previously predicted correctly.
By browsing across multiple models from different iterations of training, users can gain insights about 1) the changes between different iterations and 2) edge cases, where models constantly make mistakes (<span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_bold">T3</span>).
The insights can lead users to select the image group of interest and to analyze them in more detail to understand why they are predicted differently during different iterations of retraining.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>User Study</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We conducted a user study, where participants perform bias mitigation tasks on two datasets, 1) fruit dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Kal18</a>, <a href="#bib.bibx17" title="" class="ltx_ref">mes20</a>]</cite> (450 images) and 2) cartoon dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">RBG<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>20</a>]</cite> (680 images), and provide their insights about DASH through interviews at the end.
We initially trained models with low accuracy less than 55% with biases on colors of fruits and sunglasses of cartoon characters, respectively.
Participants were asked to identify the source of biases and mitigate them using DASH.
We recruited ten participants (9 graduate students and a recent graduate; 6 males and 4 females; mean age of 24.5), who are studying/working in computer vision for at least 6 months (10.7 months of working experiences in average). They were instructed with a video and provided with a tool on a toy dataset o that they can learn how to use the tool.
Each participant took two hours to complete the study and received $12.5 per hour for the reward.
All users performed their tasks with a Macbook Pro (16-inch, 2019) monitor with a screen resolution of 2560<math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.p1.1.m1.1a"><mo id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><times id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\times</annotation></semantics></math>1600.
We also used one GPU (NVIDIA TITAN Xp 12GB VRAM) for the computation and Chrome (v. 85.0.4183.102) for the browser.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">All participants successfully achieved the test accuracy of 90% for the cartoon dataset, and seven out of ten participants (P2, P4, P6-10) reached the test accuracy of 65% for the fruit dataset.
In the process, participants retrained 2.1 and 3.3 times on average for cartoon and fruit datasets, respectively.
Wilcoxon Signed Rank Test indicates that there was a statistically significant difference in retraining iterations between the cartoon dataset and the fruit dataset (<span id="S4.p2.1.1" class="ltx_text ltx_font_italic">p<math id="S4.p2.1.1.m1.1" class="ltx_Math" alttext="=" display="inline"><semantics id="S4.p2.1.1.m1.1a"><mo id="S4.p2.1.1.m1.1.1" xref="S4.p2.1.1.m1.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S4.p2.1.1.m1.1b"><eq id="S4.p2.1.1.m1.1.1.cmml" xref="S4.p2.1.1.m1.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.1.m1.1c">=</annotation></semantics></math></span>0.048).
Understandably, participants perceived the fruit dataset more difficult and did more poorly on it than the cartoon dataset because the fruit dataset includes real images of fruits.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Participants shared their experiences with DASH.
Here we provide some areas for improvements based on their comments.
Three participants (P2, P4, P5) pointed out that DASH requires domain expertise and prior experiences in deep learning and data augmentation.
P2 added, “Novice users would take more time to learn how to use DASH. They may need hands-on tutorials for a longer period of time. The instructional video was useful to understand the tool.”
Participants also described why some views were difficult to use.
P6 reported “While exploring the images, I observed only subtle differences in visual characteristics between clustering results with different <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.p3.1.m1.1a"><mi id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><ci id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">K</annotation></semantics></math>s.”
Additionally, we observed that prior knowledge about particular views in DASH may prevent participants from using the views.
For example, P7 finished his tasks for both datasets without using Grad-CAM View. He said “I do not trust the robustness and usefulness of GradCAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">SCD<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>17</a>]</cite> because I didn’t find the technique useful in the past.
This prior knowledge prevented me from using Grad-CAM View and I trusted my own judgment when I inspected individual images.”</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Participants also shared future ideas to improve the bias mitigation processes using DASH.
First, three participants (P1, P3, P4) wanted to keep the data augmentation history.
While repeatedly generating and discarding images, the participants easily forgot what they already did or what they should do.
Thus, the participants wanted to keep track of their previous attempts in order to save time and efforts.
Three participants (P3-5) also reported that they wanted to separately analyze images which were frequently misclassified over previously trained models.
In that way, they can further investigate why the model keeps making mistakes and derive a strategy to mitigate the specific biases.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Our work studies a visual analytic approach to tackle the problem of debiasing image classification models through data augmentation.
We designed DASH by conducting a design study with experts in deep learning.
DASH integrates the state-of-the-art image translation technique with various views as a unified system which saves time and cognitive efforts of users.
In particular, various views of DASH lead users to gain key insights that are required for debiasing.
The user study and the quantitative evaluation demonstrate that DASH can provide users with capabilities to effectively solve real-world biases in image data.
Future work can investigate ways to help novice users learn how to use the tool.
Our experiment is limited because we used a small dataset with relatively simple biases due to time constraints.
Future work can also investigate the use of bias mitigation tools like DASH on a large-scale dataset like ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">KSH12</a>]</cite> for a longer period of time through a long-term case study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">SP06</a>]</cite>.
Future studies can embed such tools in notebook environments like Jupyter Notebook so that data scientists can develop their own models.
Users aim to achieve high quality for translated images, so it will be useful to develop a user interface to retrain XploreGAN interactively.
The study shows task analysis, tool design, and user experiments which can be useful to conduct future studies on developing visual analytics tools for bias mitigation in image classification models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[AHH<sup id="bib.bibx1.4.4.1" class="ltx_sup"><span id="bib.bibx1.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>14]</span>
<span class="ltx_bibblock">
<span id="bib.bibx1.7.1" class="ltx_text ltx_font_smallcaps">Alsallakh B., Hanbury A., Hauser H., Miksch S., Rauber A.</span>:

</span>
<span class="ltx_bibblock">Visual Methods for Analyzing Probabilistic Classification
Data.

</span>
<span class="ltx_bibblock"><em id="bib.bibx1.8.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics 20</em>,
12 (Dec. 2014), 1703–1712.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[BCY<sup id="bib.bibx2.4.4.1" class="ltx_sup"><span id="bib.bibx2.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>20]</span>
<span class="ltx_bibblock">
<span id="bib.bibx2.7.1" class="ltx_text ltx_font_smallcaps">Bahng H., Chun S., Yun S., Choo J., Oh S. J.</span>:

</span>
<span class="ltx_bibblock">Learning de-biased representations with biased representations, 2020.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[BCYC20]</span>
<span class="ltx_bibblock">
<span id="bib.bibx3.1.1" class="ltx_text ltx_font_smallcaps">Bahng H., Chung S., Yoo S., Choo J.</span>:

</span>
<span class="ltx_bibblock">Exploring unlabeled faces for novel attribute discovery.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx3.2.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em> (June 2020).

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[BZK<sup id="bib.bibx4.4.4.1" class="ltx_sup"><span id="bib.bibx4.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>17]</span>
<span class="ltx_bibblock">
<span id="bib.bibx4.7.1" class="ltx_text ltx_font_smallcaps">Bau D., Zhou B., Khosla A., Oliva A., Torralba A.</span>:

</span>
<span class="ltx_bibblock">Network dissection: Quantifying interpretability of deep visual
representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx4.8.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition</em> (2017).

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[CCK<sup id="bib.bibx5.4.4.1" class="ltx_sup"><span id="bib.bibx5.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>18]</span>
<span class="ltx_bibblock">
<span id="bib.bibx5.7.1" class="ltx_text ltx_font_smallcaps">Choi Y., Choi M., Kim M., Ha J.-W., Kim S., Choo J.</span>:

</span>
<span class="ltx_bibblock">Stargan: Unified generative adversarial networks for multi-domain
image-to-image translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx5.8.1" class="ltx_emph ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em> (June 2018).

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[CPY<sup id="bib.bibx6.4.4.1" class="ltx_sup"><span id="bib.bibx6.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>19]</span>
<span class="ltx_bibblock">
<span id="bib.bibx6.7.1" class="ltx_text ltx_font_smallcaps">Choi M., Park C., Yang S., Kim Y., Choo J., Hong S. R.</span>:

</span>
<span class="ltx_bibblock">Aila: Attentive interactive labeling assistant for document
classification through attention-based deep neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx6.8.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 CHI Conference on Human Factors in
Computing Systems</em> (New York, NY, USA, 2019), CHI ’19, Association for
Computing Machinery.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3290605.3300460" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1145/3290605.3300460</span></a>.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[CRHC18]</span>
<span class="ltx_bibblock">
<span id="bib.bibx7.1.1" class="ltx_text ltx_font_smallcaps">Chan D. M., Rao R., Huang F., Canny J. F.</span>:

</span>
<span class="ltx_bibblock">T-sne-cuda: Gpu-accelerated t-sne and its applications to modern
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx7.2.1" class="ltx_emph ltx_font_italic">2018 30th International Symposium on Computer Architecture
and High Performance Computing (SBAC-PAD)</em> (2018), pp. 330–338.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[HLBK18]</span>
<span class="ltx_bibblock">
<span id="bib.bibx8.1.1" class="ltx_text ltx_font_smallcaps">Huang X., Liu M.-Y., Belongie S., Kautz J.</span>:

</span>
<span class="ltx_bibblock">Multimodal unsupervised image-to-image translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx8.2.1" class="ltx_emph ltx_font_italic">ECCV</em> (2018).

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[HW79]</span>
<span class="ltx_bibblock">
<span id="bib.bibx9.1.1" class="ltx_text ltx_font_smallcaps">Hartigan J. A., Wong M. A.</span>:

</span>
<span class="ltx_bibblock">A k-means clustering algorithm.

</span>
<span class="ltx_bibblock"><em id="bib.bibx9.2.1" class="ltx_emph ltx_font_italic">JSTOR: Applied Statistics 28</em>, 1 (1979), 100–108.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[IZZE17]</span>
<span class="ltx_bibblock">
<span id="bib.bibx10.1.1" class="ltx_text ltx_font_smallcaps">Isola P., Zhu J.-Y., Zhou T., Efros A. A.</span>:

</span>
<span class="ltx_bibblock">Image-to-image translation with conditional adversarial networks.

</span>
<span class="ltx_bibblock"><em id="bib.bibx10.2.1" class="ltx_emph ltx_font_italic">CVPR</em> (2017).

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Kal18]</span>
<span class="ltx_bibblock">
<span id="bib.bibx11.1.1" class="ltx_text ltx_font_smallcaps">Kalluri S. R.</span>:

</span>
<span class="ltx_bibblock">Fruits fresh and rotten for classification: Apples, oranges, bananas.

</span>
<span class="ltx_bibblock">Retrieved from
<a target="_blank" href="https://www.kaggle.com/sriramr/fruits-fresh-and-rotten-for-classification" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/sriramr/fruits-fresh-and-rotten-for-classification</a>,
2018.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[KEV<sup id="bib.bibx12.4.4.1" class="ltx_sup"><span id="bib.bibx12.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>18]</span>
<span class="ltx_bibblock">
<span id="bib.bibx12.7.1" class="ltx_text ltx_font_smallcaps">Kwon B. C., Eysenbach B., Verma J., Ng K., Filippi C. D., Stewart
W. F., Perer A.</span>:

</span>
<span class="ltx_bibblock">Clustervision: Visual supervision of unsupervised clustering.

</span>
<span class="ltx_bibblock"><em id="bib.bibx12.8.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics 24</em>, 1
(Jan 2018), 142–151.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[KLTH10]</span>
<span class="ltx_bibblock">
<span id="bib.bibx13.1.1" class="ltx_text ltx_font_smallcaps">Kapoor A., Lee B., Tan D., Horvitz E.</span>:

</span>
<span class="ltx_bibblock">Interactive optimization for steering machine classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx13.2.1" class="ltx_emph ltx_font_italic">ACM SIGCHI Conference on Human Factors in
Computing System</em> (2010), ACM Press, p. 1343.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[KSH12]</span>
<span class="ltx_bibblock">
<span id="bib.bibx14.1.1" class="ltx_text ltx_font_smallcaps">Krizhevsky A., Sutskever I., Hinton G. E.</span>:

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx14.2.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 25</em>,
Pereira F., Burges C. J. C., Bottou L., Weinberger K. Q., (Eds.). Curran
Associates, Inc., 2012, pp. 1097–1105.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[LDX<sup id="bib.bibx15.4.4.1" class="ltx_sup"><span id="bib.bibx15.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>19]</span>
<span class="ltx_bibblock">
<span id="bib.bibx15.7.1" class="ltx_text ltx_font_smallcaps">Liu M., Ding Y., Xia M., Liu X., Ding E., Zuo W., Wen S.</span>:

</span>
<span class="ltx_bibblock">Stgan: A unified selective transfer network for arbitrary image
attribute editing.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx15.8.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</em> (2019).

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[LTH<sup id="bib.bibx16.4.4.1" class="ltx_sup"><span id="bib.bibx16.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>18]</span>
<span class="ltx_bibblock">
<span id="bib.bibx16.7.1" class="ltx_text ltx_font_smallcaps">Lee H.-Y., Tseng H.-Y., Huang J.-B., Singh M. K., Yang M.-H.</span>:

</span>
<span class="ltx_bibblock">Diverse image-to-image translation via disentangled representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx16.8.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em> (2018).

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[mes20]</span>
<span class="ltx_bibblock">
<span id="bib.bibx17.1.1" class="ltx_text ltx_font_smallcaps">meshram v.</span>:

</span>
<span class="ltx_bibblock">Fruitsgb: Top indian fruits with quality.

</span>
<span class="ltx_bibblock"><em id="bib.bibx17.2.1" class="ltx_emph ltx_font_italic">IEEE Dataport</em> (2020).

</span>
<span class="ltx_bibblock">https://ieee-dataport.org/open-access/fruitsgb-top-indian-fruits-quality.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[RAL<sup id="bib.bibx18.4.4.1" class="ltx_sup"><span id="bib.bibx18.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>17]</span>
<span class="ltx_bibblock">
<span id="bib.bibx18.7.1" class="ltx_text ltx_font_smallcaps">Ren D., Amershi S., Lee B., Suh J., Williams J. D.</span>:

</span>
<span class="ltx_bibblock">Squares: Supporting Interactive Performance Analysis for
Multiclass Classifiers.

</span>
<span class="ltx_bibblock"><em id="bib.bibx18.8.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics 23</em>, 1
(Jan. 2017), 61–70.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[RBG<sup id="bib.bibx19.4.4.1" class="ltx_sup"><span id="bib.bibx19.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>20]</span>
<span class="ltx_bibblock">
<span id="bib.bibx19.7.1" class="ltx_text ltx_font_smallcaps">Royer A., Bousmalis K., Gouws S., Bertsch F., Mosseri I., Cole F.,
Murphy K.</span>:

</span>
<span class="ltx_bibblock">XGAN: Unsupervised Image-to-Image Translation for
Many-to-Many Mappings.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx19.8.1" class="ltx_emph ltx_font_italic">Domain Adaptation for Visual Understanding</em>, Singh R.,
Vatsa M., Patel V. M., Ratha N., (Eds.). Springer International Publishing,
Cham, 2020, pp. 33–49.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[RDV<sup id="bib.bibx20.4.4.1" class="ltx_sup"><span id="bib.bibx20.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>16]</span>
<span class="ltx_bibblock">
<span id="bib.bibx20.7.1" class="ltx_text ltx_font_smallcaps">Rs R., Das A., Vedantam R., Cogswell M., Parikh D., Batra D.</span>:

</span>
<span class="ltx_bibblock">Grad-cam: Why did you say that?

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[SCD<sup id="bib.bibx21.4.4.1" class="ltx_sup"><span id="bib.bibx21.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>17]</span>
<span class="ltx_bibblock">
<span id="bib.bibx21.7.1" class="ltx_text ltx_font_smallcaps">Selvaraju R. R., Cogswell M., Das A., Vedantam R., Parikh D., Batra
D.</span>:

</span>
<span class="ltx_bibblock">Grad-cam: Visual explanations from deep networks via gradient-based
localization.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx21.8.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision (ICCV)</em> (Oct 2017).

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[SDBR15]</span>
<span class="ltx_bibblock">
<span id="bib.bibx22.1.1" class="ltx_text ltx_font_smallcaps">Springenberg J., Dosovitskiy A., Brox T., Riedmiller M.</span>:

</span>
<span class="ltx_bibblock">Striving for simplicity: The all convolutional net.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx22.2.1" class="ltx_emph ltx_font_italic">ICLR (workshop track)</em> (2015).

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[SMG<sup id="bib.bibx23.4.4.1" class="ltx_sup"><span id="bib.bibx23.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>20]</span>
<span class="ltx_bibblock">
<span id="bib.bibx23.7.1" class="ltx_text ltx_font_smallcaps">Singh K. K., Mahajan D., Grauman K., Lee Y. J., Feiszli M., Ghadiyaram
D.</span>:

</span>
<span class="ltx_bibblock">Don’t Judge an Object by Its Context: Learning to
Overcome Contextual Bias.

</span>
<span class="ltx_bibblock">pp. 11070–11078.

</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[SP06]</span>
<span class="ltx_bibblock">
<span id="bib.bibx24.1.1" class="ltx_text ltx_font_smallcaps">Shneiderman B., Plaisant C.</span>:

</span>
<span class="ltx_bibblock">Strategies for Evaluating Information Visualization Tools:
Multi-dimensional In-depth Long-term Case Studies.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx24.2.1" class="ltx_emph ltx_font_italic">Proceedings of the 2006 AVI Workshop on BEyond Time
and Errors: Novel Evaluation Methods for Information
Visualization</em> (New York, NY, USA, 2006), BELIV ’06, ACM, pp. 1–7.

</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[STN<sup id="bib.bibx25.4.4.1" class="ltx_sup"><span id="bib.bibx25.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>16]</span>
<span class="ltx_bibblock">
<span id="bib.bibx25.7.1" class="ltx_text ltx_font_smallcaps">Smilkov D., Thorat N., Nicholson C., Reif E., Viégas F., Wattenberg
M.</span>:

</span>
<span class="ltx_bibblock">Embedding projector: Interactive visualization and interpretation of
embeddings.

</span>
<span class="ltx_bibblock"><em id="bib.bibx25.8.1" class="ltx_emph ltx_font_italic">NIPS 2016 Workshop</em> (11 2016).

</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Tor13]</span>
<span class="ltx_bibblock">
<span id="bib.bibx26.1.1" class="ltx_text ltx_font_smallcaps">Torkildson M. K.</span>:

</span>
<span class="ltx_bibblock">Visualizing the performance of classification algorithms with
additional re-annotated data.

</span>
<span class="ltx_bibblock">ACM Press, p. 2767.

</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[vdMH08]</span>
<span class="ltx_bibblock">
<span id="bib.bibx27.1.1" class="ltx_text ltx_font_smallcaps">van der Maaten L., Hinton G.</span>:

</span>
<span class="ltx_bibblock">Visualizing data using t-SNE.

</span>
<span class="ltx_bibblock"><em id="bib.bibx27.2.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research 9</em> (2008), 2579–2605.

</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[WGYS18]</span>
<span class="ltx_bibblock">
<span id="bib.bibx28.1.1" class="ltx_text ltx_font_smallcaps">Wang J., Gou L., Yang H., Shen H.</span>:

</span>
<span class="ltx_bibblock">Ganviz: A visual analytics approach to understand the adversarial
game.

</span>
<span class="ltx_bibblock"><em id="bib.bibx28.2.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics 24</em>, 6
(2018), 1905–1917.

</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[ZPIE17]</span>
<span class="ltx_bibblock">
<span id="bib.bibx29.1.1" class="ltx_text ltx_font_smallcaps">Zhu J., Park T., Isola P., Efros A. A.</span>:

</span>
<span class="ltx_bibblock">Unpaired image-to-image translation using cycle-consistent
adversarial networks.

</span>
<span class="ltx_bibblock"><em id="bib.bibx29.2.1" class="ltx_emph ltx_font_italic">CoRR abs/1703.10593</em> (2017).

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1703.10593" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1703.10593</span></a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_rdf" about="" property="dcterms:creator" content="{\@shortauthor}"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="{Computer Graphics Forum"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="{\pdf@Subject}"></div>
<div class="ltx_rdf" about="" property="dcterms:title" content="{\@shorttitle}"></div>

<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2209.06356" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2209.06357" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2209.06357">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2209.06357" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2209.06358" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 00:12:31 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
