<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors</title>
<!--Generated on Tue Sep 24 19:49:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Deep Learning,  Neural Networks,  CNN,  Video Classification,  Gesture Recognition,  Musculoskeletal Ultrasound
" lang="en" name="keywords"/>
<base href="/html/2409.16431v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S1" title="In Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S2" title="In Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S2.SS1" title="In II Methods ‣ Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Data Pre-Processing</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S2.SS1.SSS1" title="In II-A Data Pre-Processing ‣ II Methods ‣ Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>1 </span>Joint angle calculation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S2.SS1.SSS2" title="In II-A Data Pre-Processing ‣ II Methods ‣ Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>2 </span>Ultrasound Images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S2.SS1.SSS3" title="In II-A Data Pre-Processing ‣ II Methods ‣ Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>3 </span>Obtaining video segments</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S2.SS2" title="In II Methods ‣ Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Classifiers</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S2.SS2.SSS1" title="In II-B Classifiers ‣ II Methods ‣ Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>1 </span>2D CNN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S2.SS2.SSS2" title="In II-B Classifiers ‣ II Methods ‣ Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>2 </span>3D CNN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S2.SS2.SSS3" title="In II-B Classifiers ‣ II Methods ‣ Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>3 </span>(2+1)D CNN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S2.SS2.SSS4" title="In II-B Classifiers ‣ II Methods ‣ Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>4 </span>Proposed Network</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S3" title="In Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Experimental Design</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S3.SS1" title="In III Experimental Design ‣ Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Model Training</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S3.SS2" title="In III Experimental Design ‣ Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Evaluation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S3.SS3" title="In III Experimental Design ‣ Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Hyperparameters</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S4" title="In Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S5" title="In Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusions</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks
<br class="ltx_break"/><span class="ltx_note ltx_role_thanks" id="id3.id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>*Co-first authors</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Keshav Bimbraw
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id4.1.id1">Robotics Engineering</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id5.2.id2">Worcester Polytechnic Institute
<br class="ltx_break"/></span>Worcester, USA 
<br class="ltx_break"/>kbimbraw@wpi.edu
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ankit Talele
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id6.1.id1">Robotics Engineering</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id7.2.id2">Worcester Polytechnic Institute
<br class="ltx_break"/></span>Worcester, USA 
<br class="ltx_break"/>amtalele@wpi.edu
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haichong K. Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id8.1.id1">Robotics and Biomedical Engineering</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id9.2.id2">Worcester Polytechnic Institute
<br class="ltx_break"/></span>Worcester, USA 
<br class="ltx_break"/>hzhang10@wpi.edu
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.2">Ultrasound based hand movement estimation is a crucial area of research with applications in human-machine interaction. Forearm ultrasound offers detailed information about muscle morphology changes during hand movement which can be used to estimate hand gestures. Previous work has focused on analyzing 2-Dimensional (2D) ultrasound image frames using techniques such as convolutional neural networks (CNNs). However, such 2D techniques do not capture temporal features from segments of ultrasound data corresponding to continuous hand movements. This study uses 3D CNN based techniques to capture spatio-temporal patterns within ultrasound video segments for gesture recognition. We compared the performance of a 2D convolution-based network with (2+1)D convolution-based, 3D convolution-based, and our proposed network. Our methodology enhanced the gesture classification accuracy to 98.8 <math alttext="\pm" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><csymbol cd="latexml" id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">±</annotation></semantics></math> 0.9%, from 96.5 <math alttext="\pm" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><csymbol cd="latexml" id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">±</annotation></semantics></math> 2.3% compared to a network trained with 2D convolution layers. These results demonstrate the advantages of using ultrasound video snippets for improving hand gesture classification performance.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Deep Learning, Neural Networks, CNN, Video Classification, Gesture Recognition, Musculoskeletal Ultrasound

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Brightness Mode (B-Mode) ultrasound data from the forearm provides a visualization of the physiological mechanisms underlying hand movements and force generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib1" title="">1</a>]</cite>. This has been used to estimate hand gestures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib2" title="">2</a>]</cite>, finger angles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib3" title="">3</a>]</cite> and finger forces <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib4" title="">4</a>]</cite>. It has been used for controlling robots <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib5" title="">5</a>]</cite>, prosthetics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib6" title="">6</a>]</cite> and virtual reality interfaces <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib7" title="">7</a>]</cite>. As ultrasound sensing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib8" title="">8</a>]</cite> and processing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib9" title="">9</a>]</cite> becomes smaller and smaller, there is a need to further improve the performance of ultrasound-based hand gesture classification. Most prior research has focused on processing 2-Dimensional (2D) B-mode ultrasound data for this purpose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib10" title="">10</a>]</cite>. Notably, convolutional neural networks (CNNs) have been used for forearm ultrasound based gesture classification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib3" title="">3</a>]</cite>. These networks extract spatial features from the ultrasound images during training to optimize their parameters. When the gestures are acquired dynamically, as in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib3" title="">3</a>]</cite>, processing data in a 2D fashion doesn’t leverage the advantages of the hand movement undertaken over time.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Spatiotemporal convolutions (referred to as (2+1)D convolutions) have been used to design neural networks for spatial and temporal feature based action classification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib11" title="">11</a>]</cite>. Such convolutions have been used to design neural networks used for classification and segmentation tasks. Rehman et al. used 3D CNN for brain tumor detection and classification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib12" title="">12</a>]</cite>. They have also been used for lung cancer screening based on computed tomography (CT) data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib13" title="">13</a>]</cite>. Chen et al. used 3D CNN for segmentation of tumor based on magnetic resonance imaging (MRI) data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib14" title="">14</a>]</cite>. Ebadi et al. classified lung ultrasound video segments to detect pneumonia <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib15" title="">15</a>]</cite>. Rasheed et al. used ultrasound video segments for automated fetal head classification and segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib16" title="">16</a>]</cite>. However, such spatiotemporal techniques have not been used for forearm ultrasound data based gesture classification.These time-varying features can potentially improve gesture detection accuracy.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This paper proposes a modified (2+1)D convolution neural network model. Its performance is compared with 2D, (2+1)D and 3D convolution based neural network models. By using forearm ultrasound data for 12 gestures acquired from 3 subjects, we show that the proposed approach is superior to 2D, (2+1)D and 3D convolution based networks. Section II describes the data preprocessing and the classifier. Section III describes the experimental design and Section IV describes the results.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Methods</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Forearm ultrasound data from 3 subjects performing 12 hand gestures was used in this study. The subjects alternated between a rest position and the hand gestures. The Vicon motion capture system was used to acquire ground truth finger angle data. Additional information about the data acquisition can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib3" title="">3</a>]</cite>.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Data Pre-Processing</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The metacarpophalangeal joint angles were calculated from the raw motion capture data for index, middle, ring and pinky fingers. The ultrasound data acquired using a Verasonics system was also preprocessed before training. The joint angles and ultrasound data were used to extract video segments corresponding to each gesture.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS1.4.1.1">II-A</span>1 </span>Joint angle calculation</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">Motion capture data from the Vicon system tracked the positions of markers placed on the fingers, and this data was used to calculate the angles between the finger joints like in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib2" title="">2</a>]</cite>. This raw data was processed and the necessary frames were extracted from the motion capture data. NumPy arrays (.npy files) were created that contained the finger angles for each gesture. This step was crucial for converting raw motion capture data into a format suitable for model training and hand gesture prediction based on finger angles.</p>
</div>
<figure class="ltx_figure" id="S2.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="276" id="S2.F1.sf1.g1" src="extracted/5877282/angles.png" width="276"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S2.F1.sf1.3.2" style="font-size:90%;">Plot of finger angles. </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="208" id="S2.F1.sf2.g1" src="extracted/5877282/image_2.png" width="207"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S2.F1.sf2.3.2" style="font-size:90%;">Cropped ultrasound image frame.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">Data visualization: (a) Ground truth for video segment extraction. Plot of finger angles for 500 frames for index (Finger 0), middle (Finger 1), ring (Finger 2), and pinky (Finger 3) fingers, and (b) 224x224 pixel ultrasound image.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS2.4.1.1">II-A</span>2 </span>Ultrasound Images</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">For each gesture and subject, 1,400 ultrasound frames were acquired and stored in a single .mat file. The .mat files were converted to .npy arrays and grayscaled, to obtain a final shape of (1400, 636, 256), meaning there were 1,400 frames, each with a resolution of 636x256 pixels per gesture, and subject. The images were cropped to 224x224 pixels to facilitate training by removing redundant information.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS3.4.1.1">II-A</span>3 </span>Obtaining video segments</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">The video segments were obtained by first calculating the peaks in the finger angle data. This helped estimate the terminal hand position for each gesture. Then, a window of frames surrounding each peak was taken to extract the video segments. This was done for each subject and gesture.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="250" id="S2.F2.g1" src="extracted/5877282/ankit_network.png" width="621"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">The proposed network with convolution, batch normalization, residual, dense, dropout and pooling layers. Additional operations are indicated.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Classifiers</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">2D, 3D and (2+1)D convolutions were used to design neural network based classifiers for this study. These are described as follows.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS1.4.1.1">II-B</span>1 </span>2D CNN</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">A 2D CNN processes two-dimensional data, such as individual images or image slices. It applies 2D convolutional filters that slide across the height and width of the input, extracting spatial features like edges, textures, and patterns. This architecture is suited for tasks like image classification, object detection, and recognition, where temporal information is irrelevant. However, 2D CNNs cannot capture temporal or depth information, limiting their effectiveness for analyzing sequences or volumetric data.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS2.4.1.1">II-B</span>2 </span>3D CNN</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">A 3D CNN is designed to handle three-dimensional data, such as video clips or volumetric datasets like MRI scans <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib14" title="">14</a>]</cite>. It uses 3D convolutional filters that slide across height, width, and depth (time or spatial depth), capturing both spatial and temporal features simultaneously. This makes 3D CNNs effective for tasks involving spatiotemporal data, including action recognition, gesture classification, and 3D object detection. However, they are computationally demanding and more prone to overfitting due to the large number of parameters.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS3.4.1.1">II-B</span>3 </span>(2+1)D CNN</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.3">The (2+1)D CNN processes 3D data similarly to a 3D CNN but decomposes the process into separate spatial and temporal steps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib11" title="">11</a>]</cite>. Instead of applying a 3D convolution, it uses a 2D spatial convolution followed by a 1D temporal convolution. This decomposition reduces the number of parameters and improves efficiency. For instance, a 3D convolution with a <math alttext="3\times 3\times 3" class="ltx_Math" display="inline" id="S2.SS2.SSS3.p1.1.m1.1"><semantics id="S2.SS2.SSS3.p1.1.m1.1a"><mrow id="S2.SS2.SSS3.p1.1.m1.1.1" xref="S2.SS2.SSS3.p1.1.m1.1.1.cmml"><mn id="S2.SS2.SSS3.p1.1.m1.1.1.2" xref="S2.SS2.SSS3.p1.1.m1.1.1.2.cmml">3</mn><mo id="S2.SS2.SSS3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS2.SSS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS2.SSS3.p1.1.m1.1.1.3" xref="S2.SS2.SSS3.p1.1.m1.1.1.3.cmml">3</mn><mo id="S2.SS2.SSS3.p1.1.m1.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S2.SS2.SSS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS2.SSS3.p1.1.m1.1.1.4" xref="S2.SS2.SSS3.p1.1.m1.1.1.4.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.1.m1.1b"><apply id="S2.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1"><times id="S2.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1.1"></times><cn id="S2.SS2.SSS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S2.SS2.SSS3.p1.1.m1.1.1.2">3</cn><cn id="S2.SS2.SSS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.SS2.SSS3.p1.1.m1.1.1.3">3</cn><cn id="S2.SS2.SSS3.p1.1.m1.1.1.4.cmml" type="integer" xref="S2.SS2.SSS3.p1.1.m1.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.1.m1.1c">3\times 3\times 3</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS3.p1.1.m1.1d">3 × 3 × 3</annotation></semantics></math> kernel has significantly more parameters than the (2+1)D version, which uses <math alttext="1\times 3\times 3" class="ltx_Math" display="inline" id="S2.SS2.SSS3.p1.2.m2.1"><semantics id="S2.SS2.SSS3.p1.2.m2.1a"><mrow id="S2.SS2.SSS3.p1.2.m2.1.1" xref="S2.SS2.SSS3.p1.2.m2.1.1.cmml"><mn id="S2.SS2.SSS3.p1.2.m2.1.1.2" xref="S2.SS2.SSS3.p1.2.m2.1.1.2.cmml">1</mn><mo id="S2.SS2.SSS3.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS2.SSS3.p1.2.m2.1.1.1.cmml">×</mo><mn id="S2.SS2.SSS3.p1.2.m2.1.1.3" xref="S2.SS2.SSS3.p1.2.m2.1.1.3.cmml">3</mn><mo id="S2.SS2.SSS3.p1.2.m2.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S2.SS2.SSS3.p1.2.m2.1.1.1.cmml">×</mo><mn id="S2.SS2.SSS3.p1.2.m2.1.1.4" xref="S2.SS2.SSS3.p1.2.m2.1.1.4.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.2.m2.1b"><apply id="S2.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.1"><times id="S2.SS2.SSS3.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS3.p1.2.m2.1.1.1"></times><cn id="S2.SS2.SSS3.p1.2.m2.1.1.2.cmml" type="integer" xref="S2.SS2.SSS3.p1.2.m2.1.1.2">1</cn><cn id="S2.SS2.SSS3.p1.2.m2.1.1.3.cmml" type="integer" xref="S2.SS2.SSS3.p1.2.m2.1.1.3">3</cn><cn id="S2.SS2.SSS3.p1.2.m2.1.1.4.cmml" type="integer" xref="S2.SS2.SSS3.p1.2.m2.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.2.m2.1c">1\times 3\times 3</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS3.p1.2.m2.1d">1 × 3 × 3</annotation></semantics></math> for spatial convolution and <math alttext="3\times 1\times 1" class="ltx_Math" display="inline" id="S2.SS2.SSS3.p1.3.m3.1"><semantics id="S2.SS2.SSS3.p1.3.m3.1a"><mrow id="S2.SS2.SSS3.p1.3.m3.1.1" xref="S2.SS2.SSS3.p1.3.m3.1.1.cmml"><mn id="S2.SS2.SSS3.p1.3.m3.1.1.2" xref="S2.SS2.SSS3.p1.3.m3.1.1.2.cmml">3</mn><mo id="S2.SS2.SSS3.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS2.SSS3.p1.3.m3.1.1.1.cmml">×</mo><mn id="S2.SS2.SSS3.p1.3.m3.1.1.3" xref="S2.SS2.SSS3.p1.3.m3.1.1.3.cmml">1</mn><mo id="S2.SS2.SSS3.p1.3.m3.1.1.1a" lspace="0.222em" rspace="0.222em" xref="S2.SS2.SSS3.p1.3.m3.1.1.1.cmml">×</mo><mn id="S2.SS2.SSS3.p1.3.m3.1.1.4" xref="S2.SS2.SSS3.p1.3.m3.1.1.4.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.3.m3.1b"><apply id="S2.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS3.p1.3.m3.1.1"><times id="S2.SS2.SSS3.p1.3.m3.1.1.1.cmml" xref="S2.SS2.SSS3.p1.3.m3.1.1.1"></times><cn id="S2.SS2.SSS3.p1.3.m3.1.1.2.cmml" type="integer" xref="S2.SS2.SSS3.p1.3.m3.1.1.2">3</cn><cn id="S2.SS2.SSS3.p1.3.m3.1.1.3.cmml" type="integer" xref="S2.SS2.SSS3.p1.3.m3.1.1.3">1</cn><cn id="S2.SS2.SSS3.p1.3.m3.1.1.4.cmml" type="integer" xref="S2.SS2.SSS3.p1.3.m3.1.1.4">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.3.m3.1c">3\times 1\times 1</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS3.p1.3.m3.1d">3 × 1 × 1</annotation></semantics></math> for temporal convolution.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p2">
<p class="ltx_p" id="S2.SS2.SSS3.p2.1">This architecture is particularly useful for tasks that require capturing both spatial and temporal features, such as video-based action recognition or gesture analysis. The reduced computational complexity and enhanced optimization make (2+1)D CNNs more efficient than traditional 3D CNNs. They also allow for better expressiveness by introducing nonlinearities between spatial and temporal convolutions. However, despite reducing parameters, they still require careful tuning and substantial computational resources, especially in deep architectures or large datasets.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS4.4.1.1">II-B</span>4 </span>Proposed Network</h4>
<div class="ltx_para" id="S2.SS2.SSS4.p1">
<p class="ltx_p" id="S2.SS2.SSS4.p1.1">The proposed architecture is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S2.F2" title="Figure 2 ‣ II-A3 Obtaining video segments ‣ II-A Data Pre-Processing ‣ II Methods ‣ Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_tag">2</span></a>, and is based on <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib11" title="">11</a>]</cite>. It uses the Conv2Plus1D block, which decomposes 3D convolutions into a 2D spatial convolution followed by a 1D temporal convolution. Initially, the video segment dimensions (depth, height, width) are adjusted using trilinear interpolation. This allows for spatial and temporal resolution throughout the network, balancing computational efficiency with feature preservation. Residual layers consist of pairs of convolution blocks with batch normalization and activation functions. An optional projection is included when input and output dimensions differ, improving gradient flow and stabilizing training in deeper networks.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS4.p2">
<p class="ltx_p" id="S2.SS2.SSS4.p2.1">The network architecture consists of sequential convolution layers with batch normalization, resizing, and ReLU activations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib17" title="">17</a>]</cite>. The residual blocks are applied at specific filter sizes, such as 16 and 64 filters, to improve feature learning. The architecture concludes with global average pooling, flattening, and dropout to reduce dimensions and prevent overfitting before the final classification layer. The output layer is a fully connected layer that generates class predictions based on the features extracted by the preceding layers, tailored to the number of target classes. This architecture is designed to capture spatiotemporal features efficiently from video data while maintaining parameter efficiency and robust training dynamics through the use of residual and resizing strategies.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Experimental Design</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The model was trained using data from three subjects, each performing 12 distinct gestures. We used a 20% test-train split to evaluate the model’s ability to generalize across different gestures.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Model Training</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Initially, we used a TensorFlow-based video classification model from the original (2+1)D CNN paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib11" title="">11</a>]</cite>. However, the TensorFlow data loader was inefficient for our large and complex ultrasound dataset, causing significant performance bottlenecks. To resolve this, we transitioned to PyTorch, enabling the development of a more efficient, customized data loader. This transition improved data throughput and resource utilization, significantly enhancing the training pipeline for large-scale video data.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Evaluation</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We compared a slightly modified (2+1)D CNN to the 2D design in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib3" title="">3</a>]</cite>, a standard 3D model, and the base (2+1)D model in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#bib.bib11" title="">11</a>]</cite>. Classification accuracy was used as the primary metric for performance evaluation. Confusion matrices were generated to visualize model performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">Hyperparameters</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Our model’s convolution blocks use a (3, 3, 3) kernel size to decompose spatial and temporal dimensions, capturing spatiotemporal features. Padding is set to ’same’ to maintain input dimensions. The model uses varying filter sizes, starting from 8 and increasing to 64, to progressively deepen feature extraction. Batch normalization is applied after each convolution block to stabilize learning, and ReLU activations introduce non-linearity.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">The model is trained with a batch size of 8, while validation and testing are performed with a batch size of 1. A dropout rate of 0.5 is applied before the final classification layer to reduce overfitting. The training process uses an Adam optimizer with a learning rate of 1e-4 and categorical cross-entropy as the loss function. Data is split into 80% for training and 20% for testing, ensuring robust performance evaluation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Results</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We evaluated the proposed model’s performance against three baseline architectures: a 2D CNN, (2+1)D CNN, and 3D CNN, using a dataset of 12 hand gestures captured from forearm ultrasound video segments. The classification accuracy for each model is summarized in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16431v1#S4.F3" title="Figure 3 ‣ IV Results ‣ Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks *Co-first authors"><span class="ltx_text ltx_ref_tag">3</span></a>. The 2D CNN achieved a classification accuracy of 96.5 ± 2.3%, showing strong spatial feature extraction but lacking the capacity to capture temporal dynamics. The (2+1)D CNN, which decomposes spatial and temporal convolutions, achieved an accuracy of 86.0 ± 6.1%. This lower performance likely stems from the model’s limited ability to capture the temporal intricacies in ultrasound data. The 3D CNN, which directly models spatiotemporal relationships, outperformed the (2+1)D CNN with a classification accuracy of 92.8 ± 3.1%, highlighting the significance of temporal feature modeling.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="291" id="S4.F3.g1" src="extracted/5877282/results.png" width="276"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.3.2" style="font-size:90%;">Comparison of classification accuracy across different models. The proposed model achieves the highest accuracy, outperforming both spatial and spatiotemporal baseline architectures.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Our proposed model outperformed all baselines, reaching a classification accuracy of 98.8 ± 0.9%. This superior performance underscores the effectiveness of our spatiotemporal feature extraction approach, combining 2D spatial convolutions with 1D temporal processing while maintaining parameter efficiency. These results emphasize the model’s strong generalization across gestures and subjects, showcasing its potential for robust hand gesture classification from ultrasound video data.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusions</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This study demonstrates the effectiveness of spatiotemporal convolution-based neural networks for hand gesture classification using forearm ultrasound video snippets. By incorporating spatiotemporal feature extraction, our proposed model achieved an impressive accuracy of 98.8 ± 0.9%, significantly outperforming traditional 2D, (2+1)D, and 3D CNN architectures. This advancement highlights the importance of capturing dynamic features in continuous hand movements, suggesting that spatiotemporal approaches can significantly improve gesture classification for human-machine interaction applications. Future work will focus on further optimizing the network architecture and exploring its applicability in real-time gesture recognition systems.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. A. Jacobson, <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Fundamentals of Musculoskeletal Ultrasound</span>, Elsevier Health Sciences, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
K. Bimbraw, C. J. Nycz, M. J. Schueler, Z. Ziming, and H. K. Zhang, “Prediction of metacarpophalangeal joint angles and classification of hand configurations based on ultrasound imaging of the forearm,” in <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">2022 International Conference on Robotics and Automation (ICRA)</span>, pp. 91–97, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
K. Bimbraw, C. J. Nycz, M. Schueler, Z. Zhang, and H. K. Zhang, “Simultaneous estimation of hand configurations and finger joint angles using forearm ultrasound,” <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">IEEE Transactions on Medical Robotics and Bionics</span>, vol. 5, no. 1, pp. 120–132, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
K. Bimbraw and H. K. Zhang, “Estimating Force Exerted by the Fingers Based on Forearm Ultrasound,” in <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">2023 IEEE International Ultrasonics Symposium (IUS)</span>, pp. 1–4, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
K. Bimbraw, E. Fox, G. Weinberg, and F. L. Hammond, “Towards sonomyography-based real-time control of powered prosthesis grasp synergies,” in <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">2020 42nd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</span>, pp. 4753–4757, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
N. Hettiarachchi, Z. Ju, and H. Liu, “A new wearable ultrasound muscle activity sensing system for dexterous prosthetic control,” in <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">2015 IEEE International Conference on Systems, Man, and Cybernetics</span>, pp. 1415–1420, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
K. Bimbraw, J. Rothenberg, and H. Zhang, “Leveraging Ultrasound Sensing for Virtual Object Manipulation in Immersive Environments,” in <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">2023 IEEE 19th International Conference on Body Sensor Networks (BSN)</span>, pp. 1–4, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S. Frey, S. Vostrikov, L. Benini, and A. Cossettini, “WULPUS: a Wearable Ultra Low-Power Ultrasound probe for multi-day monitoring of carotid artery and muscle activity,” in <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">2022 IEEE International Ultrasonics Symposium (IUS)</span>, pp. 1–4, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
K. Bimbraw, H. K. Zhang, and B. Islam, “Forearm Ultrasound based Gesture Recognition on Edge,” <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2409.09915</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Jess McIntosh, Asier Marzo, Mike Fraser, Carol Phillips. ”Echoflex: Hand gesture recognition using ultrasound imaging.” In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, pp. 1923–1934. 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri, “A closer look at spatiotemporal convolutions for action recognition,” in <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</span>, pp. 6450–6459, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Rehman, M. A. Khan, T. Saba, Z. Mehmood, U. Tariq, and N. Ayesha, “Microscopic brain tumor detection and classification using 3D CNN and feature selection architecture,” <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Microscopy Research and Technique</span>, vol. 84, no. 1, pp. 133–149, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Yu, B. Yang, J. Wang, J. Leader, D. Wilson, and J. Pu, “2D CNN versus 3D CNN for false-positive reduction in lung cancer screening,” <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Journal of Medical Imaging</span>, vol. 7, no. 5, pp. 051202–051202, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
L. Chen, Y. Wu, A. M. DSouza, A. Z. Abidin, A. Wismüller, and C. Xu, “MRI tumor segmentation with densely connected 3D CNN,” in <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Medical Imaging 2018: Image Processing</span>, vol. 10574, pp. 357–364, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S. E. Ebadi, D. Krishnaswamy, S. E. Bolouri, D. Zonoobi, R. Greiner, N. Meuser-Herr, J. L. Jaremko, J. Kapur, M. Noga, and K. Punithakumar, “Automated detection of pneumonia in lung ultrasound using deep video classification for COVID-19,” <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Informatics in Medicine Unlocked</span>, vol. 25, p. 100687, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
K. Rasheed, F. Junejo, A. Malik, and M. Saqib, “Automated fetal head classification and segmentation using ultrasound video,” <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">IEEE Access</span>, vol. 9, pp. 160249–160267, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A. F. Agarap, “Deep learning using rectified linear units (relu),” <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:1803.08375</span>, 2018.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 24 19:49:25 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
