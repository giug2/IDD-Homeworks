<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance</title>
<!--Generated on Mon Sep 16 20:39:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
shared situation awareness,  human autonomy teaming,  AI-advised decision making
" lang="en" name="keywords"/>
<base href="/html/2409.10717v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S1" title="In Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S2" title="In Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Background &amp; Prior Similar Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S2.SS1" title="In II Background &amp; Prior Similar Work ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Naturalistic Decision Making (NDM) &amp; Role of Judgement</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S2.SS2" title="In II Background &amp; Prior Similar Work ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Model-Specific Contributions to Mitigate Challenges</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S2.SS3" title="In II Background &amp; Prior Similar Work ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Contextual Information Improving Team Performance with Recommender Systems</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S3" title="In Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S3.SS1" title="In III Methodology ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Task Domain &amp; Decision Environment</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S3.SS2" title="In III Methodology ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Experiment Design and Task Procedure</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S3.SS3" title="In III Methodology ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Measures and Dependent Variables</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S3.SS4" title="In III Methodology ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Experiment Procedure</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4" title="In Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.SS1" title="In IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Objective Metrics</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.SS1.SSS1" title="In IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>1 </span>Final Decision Agreement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.SS1.SSS2" title="In IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>2 </span>Task Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.SS1.SSS3" title="In IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>3 </span>Sway, the AI’s Influence on the Participant</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.SS1.SSS4" title="In IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>4 </span>Average Completion Time</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.SS2" title="In IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Subjective Metrics</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.SS2.SSS1" title="In IV-B Subjective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>1 </span>NASA TLX</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.SS2.SSS2" title="In IV-B Subjective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>2 </span>i-THAu</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S5" title="In Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S6" title="In Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Divya Srivastava, 
and Karen M. Feigh
</span><span class="ltx_author_notes">D. Srivastava is with the Department
of Mechanical Engineering, Georgia Institute of Technology, Atlanta,
GA, 30332 USA e-mail: (divya.srivastava@gatech.edu).</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Recommender systems, while a powerful decision making tool, are often operationalized as black box models, such that their AI algorithms are not accessible or interpretable by human operators. This in turn can cause confusion and frustration for the operator and result in unsatisfactory outcomes. While the field of explainable AI has made remarkable strides in addressing this challenge by focusing on interpreting and explaining the algorithms to human operators, there are remaining gaps in the human’s understanding of the recommender system. This paper investigates the relative impact of using context, properties of the decision making task and environment, to align human and AI algorithm understanding of the state of the world, i.e. judgment, to improve joint human-recommender performance as compared to utilizing post-hoc algorithmic explanations.
We conducted an empirical, between-subjects experiment in which participants were asked to work with an automated recommender system to complete a decision making task. We manipulated the method of transparency (shared contextual information to support shared judgment vs algorithmic explanations) and record the human’s understanding of the task, the recommender system, and their overall performance. We found that both techniques yielded equivalent agreement on final decisions. However, those who saw task context had less tendency to over-rely on the recommender system and were able to better pinpoint in what conditions the AI erred. Both methods improved participants’ confidence in their own decision making, and increased mental demand equally and frustration negligibly.
These results present an alternative approach to improving team performance to post-hoc explanations and illustrate the impact of judgment on human cognition in working with recommender systems.
</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
shared situation awareness, human autonomy teaming, AI-advised decision making

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recommender
systems offer immense value in guiding human decision-making across critical domains. Traditionally, these systems employed algorithms designed to suggest optimal courses of action to users.
The latest iteration of recommender systems use pre-trained AI models to form the foundation of their automated agents and recommendation systems. These cutting-edge systems leverage advanced artificial intelligence to provide highly informed and tailored guidance, empowering humans to make well-informed choices in low-stakes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib1" title="">1</a>]</cite> and high-stakes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib2" title="">2</a>]</cite> scenarios.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">While recommender systems offer immense potential, their increasing sophistication, particularly those powered by AI-trained algorithms, has resulted in a significant challenge – opacity. Unlike their predecessors, these advanced systems are virtually impenetrable to human comprehension <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib3" title="">3</a>]</cite>, utilizing algorithms that are either not available to or not easily interpretable by humans. These systems are often characterized as “black boxes” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib4" title="">4</a>]</cite>. Further, this opaqueness is purposefully implemented and valued as it seemingly facilitates faster decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib6" title="">6</a>]</cite>. However, this approach can be problematic. In well-defined, simple decision environments or when the human operator is an expert, opaque AI systems may yield satisfactory results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib7" title="">7</a>]</cite>. However, in complex scenarios or challenging decision landscapes, these systems are prone to failure <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib8" title="">8</a>]</cite>. When the AI’s recommendation is unsatisfactory or even catastrophic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib10" title="">10</a>]</cite>, users are left with little to no insight into the reasoning behind the suggested course of action, making it difficult to recognize <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib12" title="">12</a>]</cite> or rectify errors. And in many safety critical domains, these random failures represent unacceptable risks which must be managed.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Recognizing these challenges, significant efforts have been made to improve the transparency and interpretability of recommender systems. These include extensive training for human operators<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib13" title="">13</a>]</cite>, enhanced visualizations of proposed solutions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib15" title="">15</a>]</cite>, ranking candidate solutions for comparison <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib16" title="">16</a>]</cite>, and attempting to explain the underlying reasoning behind recommendations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib17" title="">17</a>]</cite>. While these approaches have improved specific types of recommender systems, identifying best practices for effective human-AI interaction in recommender systems remains an open challenge.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Much of the work thus far focuses on “opening” the black box in order to make the automation explainable or more interpretable by humans. However, predominant XAI work focuses on providing <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">post-hoc</span> explanations of algorithmic models, which is may not be useful in real-time decision making. While <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">post-hoc</span> XAI has been successful with simple automation in previous generations of technology, autonomous systems without explanatory mechanisms are being utilized in present day anyway, which can be detrimental to current users.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib20" title="">20</a>]</cite> has shown that supporting user judgment by providing contextual information about the decision environment has a positive effect on the human working with the AI-driven recommender systems and their overall combined performance. This contextual information can be any relevant information outside of the black box AI that may serve to increase understanding of the decision environment. This transparency technique yields several benefits such as improved human judgement, improved team performance with the AI partner, regardless of the accuracy of the partner, and more accurately calibrated trust in the AI partner. Follow-up studies indicate that the provision of this contextual information produces these benefits even when the information is presented at different levels of abstraction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib21" title="">21</a>]</cite>. Since this transparency technique is effective in its goals and robust to abstraction, we next aim to see if it can produce equivalent performance as the popular transparency technique of providing explanations.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">This paper investigates the effectiveness of aligning team judgement using contextual information with respect to the effectiveness of using explanations to support a human working with black box recommender systems. The driving force behind providing relevant context information about the decision environment is to support the holistic judgement of the human decision maker, whereas explanations provide insights into the decision making process of AI algorithms for specific instances or inputs. There are several limitations of trying to extract explanations of the AI algorithms:</p>
</div>
<div class="ltx_para" id="S1.p7">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Complexity</span>: AI algorithms, especially deep learning models, can be highly complex with numerous layers and parameters. Extracting explanations from such models can be challenging due to their intricate structure and the difficulty of interpreting the relationships between inputs and outputs.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Interpretability</span>: Explanations often rely on simplified models or techniques to approximate the behavior of the AI algorithm. While these approximations can provide insights, they may not capture the full complexity of the original model, leading to potential discrepancies and limitations in the explanations. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib22" title="">22</a>]</cite></p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Scalability</span>: Generating explanations for large datasets or complex models can be computationally expensive and time-consuming. As the size and complexity of the data increase, the process of generating explanations may become impractical or infeasible.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.p1.1.1">Sensitivity to input variations</span>: Explanations can be sensitive to small changes in inputs, resulting in different explanations for similar instances, making it challenging to rely solely on explanations for understanding the overall behavior of the AI algorithm. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib23" title="">23</a>]</cite></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">There is substantial ongoing research in the field of <span class="ltx_glossaryref" title="">explainable AI (XAI)</span> to address these limitations. However, providing contextual information should bypass these issues because the information that is relevant to the task is already being used as input to the AI. Providing contextual information to the human decision maker aligns the human and AI judgement and provides a shared situation awareness that may prove as valuable as post-hoc explanations. We hypothesize that providing contextual information will yield comparable team performance and calibrated trust in the AI partner as using local explanations would. If this is the case, then this model-agnostic method of increasing transparency may prove less time-consuming and costly than current XAI techniques that are model-specific (i.e. aimed at explaining or interpreting the specific algorithms of the AI). This would be beneficial for current and future advanced technology, especially for parties concerned about model confidentiality.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background &amp; Prior Similar Work</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Decision Support Systems (DSS) have been a prevalent application of <span class="ltx_glossaryref" title="">artificial intelligence (AI)</span> across various industries for decades, predating the integration of <span class="ltx_glossaryref" title="">AI</span> methods into their underlying algorithms.
These systems are designed to assist operators in decision-making tasks with the goal of enabling better and faster decisions.
They achieve this by either simplifying the decision space or generating potential solutions, thereby reducing the cognitive burden on human decision-makers.
A core advantage of decision support systems lies in their ability to encode expert-level knowledge and insights, thus empowering non-expert users with access to specialized information.
AI-driven recommender systems represent a subset of DSS, wherein an <span class="ltx_glossaryref" title="">AI</span>-based algorithm is used to recommend a specific course of action.
Specific examples of <span class="ltx_glossaryref" title="">AI</span> algorithms include machine learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib24" title="">24</a>]</cite>, reinforcement learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib26" title="">26</a>]</cite>, and neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib28" title="">28</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">DSS and the more specific recommender systems leverage standard process models of cognition and decision making which have been modeled in several ways. Most involve cyclic 4 step cognitive processes such as:
SEAL model (Sense-Explore-Act-Learn) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib29" title="">29</a>]</cite>, SIDA model (Sense-Interpret-Decide-Act) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib30" title="">30</a>]</cite>, or the OODA loop (Observe-Orient-Decide-Act) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib31" title="">31</a>]</cite>. Others are more complex such as Hollnagel’s COCOM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib32" title="">32</a>]</cite>, ABC Group’s Adaptive toolbox and contingency models. Most decision making models will converge on a few key elements: observation of the external world, integration of that information into the decision maker’s understanding of present and potential future states, consideration and selection of possible courses of action, followed by implementation of that course of action.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">The use of recommender systems in these decision making processes is primarily to speed up the process of choosing a course of action, which is typically done by minimizing or removing altogether the need for a human to observe or take in information about the state of the world, or to process that information to form some sort of judgement of the current state of that world.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Naturalistic Decision Making (NDM) &amp; Role of Judgement</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The field of <span class="ltx_glossaryref" title="">naturalistic decision making (NDM)</span> focuses on comprehending, modeling, and enhancing human decision-making and cognitive function performance in demanding real-world scenarios.
Several works within <span class="ltx_glossaryref" title="">NDM</span> have demonstrated that context and adequate time allocation for the orientation and judgment of relevant information significantly influences the decision-making process. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib35" title="">35</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">In AI-advised decision-making tasks, the allocation of responsibility for each cognitive function in the decision-making process is as follows: <span class="ltx_glossaryref" title="">AI</span> assumes responsibility for the Observing and Orienting phases, and can contribute to the Decision phase. <span class="ltx_glossaryref" title="">AI</span> collects pertinent data (Observe/Sense), and then utilizes this information to generate potential solutions (Orient/Judge/Interpret), or to suggest a decision (Decide). The human operator primarily serves as a safety checkpoint, retaining the responsibilities of making the final decision (Decide) and then implementing it (Act). However, the opaque nature of many systems restricts appropriate understanding of the <span class="ltx_glossaryref" title="">AI</span>’s information observation and orientation processes, thus impeding the development of team shared situation awareness (SSA) due to intentional limitation of human involvement in the processes that build situation awareness.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Furthermore, the absence of a judgment phase for humans in the decision making process often leads to overreliance on system recommendations. This is a common issue observed across various domains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib36" title="">36</a>]</cite>, with both embodied <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib38" title="">38</a>]</cite> and nonembodied automated systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib40" title="">40</a>]</cite>.
This can be attributed to a variety of explanations, though Wagner et al conclude that “people might assume [automation] has knowledge [they] do not possess, viewing [automation]’s actions, at times mistakenly, as a direct reflection of the intentions of the developer, when in fact the [automation] may be malfunctioning or users may be misinterpreting its abilities. Even when presented with evidence of a system’s bad behavior or failure” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib36" title="">36</a>]</cite>.
Whatever the underlying cause, overreliance can be mitigated through accurate mutual understanding of tasks, roles, and responsibilities between human and AI teammates, such that each can make accurate predictions of each other’s decision making capabilities.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Model-Specific Contributions to Mitigate Challenges</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The field of <span class="ltx_glossaryref" title="">XAI</span> seeks to address challenges posed by the opaque nature of AI systems.
Some approaches aim to provide algorithmic transparency by presenting insights into how the AI processes information.
Much work in this area focuses on providing explanations for isolated decision points <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib17" title="">17</a>]</cite>.
The most prevalent types of explanation are global explanations and local explanations. Global explanations explain the overall model behavior, e.g. a full set of heuristics or rules used to classify inputs into outputs, whereas local explanations offer a specific explanation for each specific set of decision parameters.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">While common, this approach is not without its challenges.
Explanations can be presented in various formats, ranging from textual descriptions to visual representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib41" title="">41</a>]</cite>, and there is no consensus on what constitutes a good or effective explanation.
For simpler <span class="ltx_glossaryref" title="">AI</span> algorithms (e.g., decision trees), explanations can be relatively straightforward, often involving the visualization of the decision structure or providing the features that triggered the decision.
In contrast, explanations for more complex algorithms, such as deep neural networks, are challenging to generate as the sheer volume of parameters and nodes used by the algorithm makes extracting a straightforward human-like explanation from them very difficult.
Moreover, the explanation’s effectiveness depends on its relevance to the user’s existing mental models and its ability to increase the human’s understanding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib17" title="">17</a>]</cite>.
The explanation must be communicated at an appropriate level of abstraction for the individual user.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">While successful implementation of this type of algorithmic transparency can provide post-hoc context for outputs, it remains that the decision making process is truncated for the decision maker, as they do not directly <span class="ltx_text ltx_font_italic" id="S2.SS2.p3.1.1">Observe</span> any information and must work backwards to <span class="ltx_text ltx_font_italic" id="S2.SS2.p3.1.2">Orient</span> the explanation with the AI’s suggestion.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">Other work has attempted to assist the human in evaluating the AI’s suggestion by providing more interaction with the suggestion, or providing an alternate visualization of the suggestion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib15" title="">15</a>]</cite>.
While these techniques can increase understanding of simple algorithms, they alone may not be sufficient to support the human’s judgment in complex decision-making tasks.
All of these approaches make valid contributions to the push to make human-AI decision making teams better, but as stand-alone techniques, they all truncate the decision making process which necessarily creates knowledge gaps for the decision maker, and therefore, skews their judgment.</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">Recent advancements in transparency methods have aimed to make complex machine learning models more understandable without uncovering their internal algorithms.
These methods do not rely on the specifics of the machine learning model, and as such, can be applied to any machine learning model after it has been trained.
Two prominent examples of these methods are LIME and SHAP.
Local Interpretable Model-agnostic Explanations (LIME) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib42" title="">42</a>]</cite> works by approximating the behavior of a complex model locally around a specific prediction using a simpler, interpretable model.
This allows for explanations of individual predictions in terms that humans can understand.
However, this technique faces the same aforementioned challenges associated with explanations.
Shapley Additive Explanations (SHAP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib23" title="">23</a>]</cite> are based on game theory concepts and provide post-hoc global explanations of how much influence each input variable has on the target output variable. While insight into the relative importance of input variables within the model can be useful, this technique falls short in conveying what specifically the input variables are. Without a comprehensive understanding of the specific input variables and their real-world implications, the decision maker’s grasp of the situation remains incomplete. SHAP also requires exponential computational time <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib43" title="">43</a>]</cite>.
Additionally, these methods are not very robust– small changes in input features can lead to dramatic changes in explanations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib44" title="">44</a>]</cite>.
While useful for fine-tuning algorithms, these methods are not suited to real-time, high-stakes decision making tasks.</p>
</div>
<div class="ltx_para" id="S2.SS2.p6">
<p class="ltx_p" id="S2.SS2.p6.1">Given these limitations, there is a growing recognition that supporting the human’s real time decision making process may require approaches that circumvent the need to understand the specific algorithms used by AI recommender systems.
To this end, one promising direction is to provide contextual information about the task environment. Promising results have shown that this technique is effective in supporting the human’s Orientation process when making a high-stakes decision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib21" title="">21</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">Contextual Information Improving Team Performance with Recommender Systems</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">To address the concerns, we conducted a series of experiments to provide an alternative to explaining or interpreting complex AI models by investigating the use of a model-agnostic method of supporting operators’ cognitive process of judgement. We provided the decision maker with relevant information that the AI used to generate possible courses of action in an attempt to align the user and AI understanding of the state of the decision environment. Our underlying hypothesis was that if the user and the AI had a joint understanding of the decision environment that a well-functioning AI would provide a recommendation that should be understandable by the user.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">We began by, testing the basic hypothesis that providing the human decision maker with contextual information about the decision environment would align their situation awareness with that of a perfect AI recommender system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib18" title="">18</a>]</cite>. Results showed that providing contextual information about the decision environment boosted the team’s shared situation awareness. The relevant contextual information aided the human’s judgement which whas shown by increased agreement between the human’s final decision and the perfect AI’s suggestion.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">Next we investigated if supporting the human’s judgement via contextual information would be effective with an imperfect AI<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib19" title="">19</a>]</cite>. The results of the second experiment indicate that supporting the human’s judgement via contextual information helped participants determine the AI’s error boundary (both when it is wrong and in what ways/situations it is wrong). It also reduced participants’ overreliance on the AI’s suggestion, as well as accurately calibrated the participant’s trust in the AI’s capabilities.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">Encouraged by the findings from these initial experiments, the technique of supporting human situation awareness by providing them with relevant decision environment information seemed valid, but we wanted to verify that these findings are not due to the specific representations or visualizations of the contextual information shown to participants. Here the specific representations of the contextual information was varied to both increase and decrease the level of abstraction of the information. Results from the next experiment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib21" title="">21</a>]</cite> validated that crucial metrics in team decision-making, such as task performance, shared situation awareness, and trust in AI’s capabilities, are robust to the level of abstraction at which information is displayed. This means that the supporting the human’s judgement is important and helpful even if the information available to them is abstract or in overwhelming levels of detail.</p>
</div>
<div class="ltx_para" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1">Finally, in this work, we seek to compare the effectiveness of supporting human judgement via contextual information versus the popular transparency technique of providing local decision point explanations. This will inform whether or not this technique holds up to the current standard in automation transparency and support it as a valid transparency technique.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methodology</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We conducted a between-subjects experiment in which participants were asked to work with an automated recommender system to complete a decision making task.
We manipulated the type of transparency technique the participants were provided with, and we measured the effects on final task performance, trust in the recommender system, user workload, and the shared situation awareness between participants and the recommender.
There are four treatments groups: 1) Absent (the Baseline condition with no contextual information or explanation), 2) Contextual Information, 3) Local Explanation, and 4) Contextual Information AND Local Explanation.
Each treatment incorporated a different transparency structure intended to increase the human’s understanding of the task and autonomous partner.
This experiment uses data from 180 participants.
Data for the baseline (Absent, in which there is no explanation or contextual information) performance and the data for Contextual Information are from previous experiments, the results of which are published in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib19" title="">19</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">For the last two conditions, we utilized the same scenarios with a different transparency method in which participants are assigned to either the third treatment group (Local Explanation) or the fourth treatment group (Contextual Information AND Local Explanation).
Instead of or in addition to providing contextual information, we provide a local explanation (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S3.F2" title="Figure 2 ‣ III-A Task Domain &amp; Decision Environment ‣ III Methodology ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">2</span></a>) at the time of the decision point (see Fig.<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S3.F1" title="Figure 1 ‣ III Methodology ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">1</span></a>).
The explanation was a simple text box displaying a one-sentence explanation. The explanation was developed using a the heuristic which explained why the AI made its decision.
The explanation was always aligned with the AI’s (correct or incorrect) suggestion, such that, in situations in which the AI’s decision was correct and corresponded to the ground truth, the explanation provided 2-3 corresponding figures of merit to cite as the reason why. Similarly, in the situations in which the AI’s decision was incorrect, the explanation provided 2-3 contradicting or ambiguous figures of merit to cite.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We additionally include Trajectory Awareness as an independent variable that manipulates the amount of interaction the human has with the AI’s generated solutions (Observation-Only or Interactive).</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">Participants were recruited through an online recruitment platform (Prolific, www.prolific.co).
Individuals who were under 18, located outside of the USA, not proficient in English, and/or did not have normal or corrected-to-normal vision were excluded from the studies.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="209" id="S3.F1.1.g1" src="extracted/5858843/images/exp4/exp5_task_outline.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.4.2" style="font-size:90%;">Task Outline and Metrics for EDL Trajectory Planning</span></figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Task Domain &amp; Decision Environment</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">This work uses the same experiment platform and task developed by Srivastava et al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib21" title="">21</a>]</cite>. This section describes the main components that are necessary to the development and understanding of this specific body of research work.
Participants played the role of commander of a spacecraft in Mars orbit, tasked with finalizing the <span class="ltx_glossaryref" title="">entry, descent, and landing (EDL)</span> trajectory of a probe to a landing site.
Participants were told that they were aided by an AI Mission Computer that made a parallel evaluation of the proposed trajectory and offered agreement or disagreement with the participant’s decision.
After seeing the AI’s recommendation, the participant had the final call on whether to execute or abort the mission.
We did not impose time constraints on the task.
For control and reproducibility, though the system was presented to participants as “intelligent”, its responses, the contextual information it used, and the explanations it provided in each scenario were predetermined and fixed.
The scenarios and tasks were also constant across participants, though the order in which they were shown was balanced.
These were all originally generated from a physics based simulation, where specific scenarios which possessed the correct mix of characteristics were chosen.
The task is outlined in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S3.F1" title="Figure 1 ‣ III Methodology ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">First, the AI Mission Computer generated a possible trajectory for the probe’s landing (using algorithms unavailable to the human participant).
The team was shown a set of six figures of merit (FoM) (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S3.F2" title="Figure 2 ‣ III-A Task Domain &amp; Decision Environment ‣ III Methodology ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">2</span></a>) that characterized the proposed flight plan: velocity vs. altitude; heating rate, heat load, and acceleration vs. time; latitude vs. longitude; and landing confidence.
The FoM charts were shaded to indicate safe, risky, and dangerous thresholds, and the participants were instructed on how to interpret each.
Using the FoMs, the participant and the AI individually evaluated whether or not to execute the landing trajectory (second judgement point represented by yellow diamond #2).
The AI Mission Computer’s decision was then made known to the participant, along with an explanation as to why the AI made the suggestion it did in regard to that specific mission .</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="181" id="S3.F2.g1" src="extracted/5858843/images/exp4/local_exp.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Example of AI’s Local Explanation with the generated suggestion</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">After seeing the explanation, the participant made the final decision on whether to execute or abort the mission in light of the AI’s recommendation (third decision point represented by yellow diamond #3).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Experiment Design and Task Procedure</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">This between-subject experiment includes two independent variables: Transparency Technique (4 levels) and Trajectory Awareness (2 levels). The “AI” behavior utilized in these studies is fixed and static to ensure that certain inputs (world state conditions) mapped to outputs (trajectory charts), thus allowing repeatability in the experiment.
To simplify the analysis, the decision to execute or abort is pre-determined for each scenario, allowing for repeatable manipulation of the accuracy of the AI’s suggestion. For all of the data analyzed in this study, the AI is correct 60% of the time, and fails in specific weather conditions.
The decision to have the AI fail in a predictable way mimics real world situations in which algorithms that are trained on clearly labeled data and well-defined problems are successful when encountering similar situations, but fail when faced with noisy, degraded, or unfamiliar data.
Additionally, by having the AI fail in a predictable manner, we were able to discern if providing transparency by way of environment information or by way of explanation helps the human understand the AI’s limitations holistically (does the human pick up on the fact that the AI is giving inaccurate suggestions?) and specifically (under what circumstances does the AI fail?).
These answers will inform the human’s understanding of their AI partner, and thus their collaboration with it. Trajanctory Awareness indicates whether or not the participants were required to classify each of the FoMs as good or bad (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S3.F2" title="Figure 2 ‣ III-A Task Domain &amp; Decision Environment ‣ III Methodology ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">2</span></a>), or if they were simply presented with the FoMs with no requirement for classification. This provided an additional control to understand if the participant adequately understood the FoMs and if this added task would improve performance.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Participants were assigned to a single treatment that was one of eight combinations of the Transparency Technique and Trajectory Awareness levels. Participants completed 10 tasks within their treatment group.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S3.F3.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="398" id="S3.F3.1.g1" src="extracted/5858843/images/gps_questions.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S3.F3.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="410" id="S3.F3.2.g1" src="extracted/5858843/images/weather_questions.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S3.F3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="379" id="S3.F3.3.g1" src="extracted/5858843/images/entryangle_questions.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.5.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.6.2" style="font-size:90%;">World State Information from Left to Right: GPS, Atmosphere/Weather, Anticipated Entry Angle</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">The first transparency technique level is the Absent group which was the control group, in which participants were not given any relevant information about the world state.
This lack of contextual information mimics current black box systems, and participants in this group started directly at the Trajectory Evaluation part of the experiment.
In the Contextual Information mode, participants viewed three information screens about the world state (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S3.F3" title="Figure 3 ‣ III-B Experiment Design and Task Procedure ‣ III Methodology ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">3</span></a>). The agents separately evaluated how risky the world state was (risky conditions vs safe conditions) in that scenario, and were asked if the world state conditions were risky or safe enough to attempt a landing.
The AI Mission Computer prepared its own answer to the same question.
For implementation purposes, for all phases, these responses were always correct. This was the first assessment of the participant and AI having a shared understanding, i.e. the world state.
The participant was then informed of the AI’s judgment of the world state conditions (and thus whether they are in agreement).</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="151" id="S3.F4.g1" src="extracted/5858843/images/trajectories_questions.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">AI-generated trajectory with interactive questions</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">Following the assessment of the world state context information to align situation awareness, a landing trajectory was presented, and the agents (both human and AI) evaluated whether or not to execute it.
For the Absent Group, and the Local Explanation Group, the experiment started here as there was no contextual information to review.
Participants were shown a set of six figures of merit that described the proposed flight plan.
The Trajectory Awareness variable had two levels: Observation-Only or Interactive.
In the Observation-Only Mode, participants were able to view the six figures of merit for the suggested trajectory and made an execute/abort decision.
In Interactive Mode, participants were additionally asked to mark each of the six figures of merit as “Good”, “Bad”, or “Maybe” according to that chart’s specific risk factors (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S3.F4" title="Figure 4 ‣ III-B Experiment Design and Task Procedure ‣ III Methodology ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">4</span></a>).
After observing or interacting with each of the trajectory charts, participants were asked to decide whether to execute the trajectory.
This decision point allowed us to get a second measure of shared understanding between the agents.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">The AI Mission Computer’s decision was revealed to the participant after their own decision was entered. For the groups that included a Local Explanation, the decision came along with an explanation that described why the AI made the suggestion it did (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S3.F2" title="Figure 2 ‣ III-A Task Domain &amp; Decision Environment ‣ III Methodology ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1">Participants were then asked to make a final decision, allowing us to understand how much the AI’s own assessment influenced the participant.</p>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.1">To summarize, a total of 2 decision points were recorded for the participants in the Absent and Local Explanation groups (their initial decision before seeing the AI’s recommendation, and then final decision). A total of 3 decision points were recorded for the participants in the Contextual Information, and the Contextual Information AND Local Explanation groups, as they also had to make a judgement about the environment before moving on to the actual task decision.
These decisions could be compared in several ways providing us with insights into the alignment between human and AI, consistency of participant decisions, the influence of the AI’s analysis , the influence of the contextual information and the influence of the explanation on the participant.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">Measures and Dependent Variables</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">From the participant decisions recorded, we computed the following metrics of interest:</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">Final Agreement: </span>We first recorded the Final Agreement between the participant and the AI after the AI’s recommendation to execute/abort was revealed (yellow diamond 3 Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S3.F1" title="Figure 1 ‣ III Methodology ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">1</span></a>, as at this point all participants have experienced some transparency technique. We computed the Final Agreement on a per participant basis across all 10 scenarios they experienced.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p3.1.1">Task Performance: </span>We assessed the final decision of the human participant against the ground truth. This was a binary metric of whether the human made the right decision on whether to abort or execute the trajectory.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.1">Sway, the AI’s Influence on the Participant: </span> We recorded the number of decisions per participant that changed from initial (yellow diamond 1) to final decisions (yellow diamond 3) and from there computed the percentage of initial disagreements that were resolved. As the AI did not change its decision, and the situations in which the AI made decisions that were in error were known, this allowed us to understand the appropriate or detrimental influence that the AI had on the participant.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p5.1.1">Time: </span> We recorded the completion time for participants to complete the mission planning task. This metric allowed us to understand the additional burden that contextual information and explanations would add to the baseline task. Additional time is important as often contextual information is excluded as it is considered too temporally costly or mentally taxing to be included.</p>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1">Additionally, responses were recorded from the subjective questionnaires given to participants.</p>
</div>
<div class="ltx_para" id="S3.SS3.p7">
<p class="ltx_p" id="S3.SS3.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p7.1.1">Trust: </span>The pre-experiment questionnaire was an i-THAu trust assessment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#bib.bib45" title="">45</a>]</cite> in which participants were asked to answer a series of statements about their Faith in Persons and Faith in Technology on a seven-point Likert scale from [-3:3].
A composite average of their answers informs their overall dispositional trust in these two categories. Following the experiment the rest of the the i-THAu trust assessment in which participants responded to a series of statements about their experience of working with the automated system on a seven-point Likert scale from [-3:3] was administered.
For both i-THAu assessments, a rating of -3 indicated a lack of trust – the subject didn’t trust people/technology or depend on the AI to help them with the <span class="ltx_glossaryref" title="">EDL</span> task, or they didn’t understand the role of the AI.
Conversely, a rating of 3 indicated a high level of trust in people/technology, as well as high trust in and understanding of the AI.</p>
</div>
<div class="ltx_para" id="S3.SS3.p8">
<p class="ltx_p" id="S3.SS3.p8.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p8.1.1">Workload: </span>
We measured participant workload using the NASA TLX workload assessment which measured overall cognitive workload.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.4.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.5.2">Experiment Procedure</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Participants were given briefing documents and asked to complete a consent form upon starting the study. Each study had four main components: pre-experiment questionnaires, training session, data collection, and post-experiment questionnaires.
The pre-experiment questionnaires measured dispositional trust of the participant in two areas: Faith in Technology and Faith in Persons. These two factors were measured to see if they could predict the accuracy of the human’s mental model and overall task performance.
Participants were assigned randomly to a treatment group, and the order of scenarios shown to participants was balanced. Instructional videos, tailored to the specific treatment, were used to train participants on the task and the AI Mission Computer.
No task-relevant experience was assumed.
Participants then completed 6 practice rounds of the mission planning task.
Participants who had correct task performance in 5 out of 6 practice trials were allowed to proceed past the practice rounds.
Passing participants then proceeded to complete 10 trials of the mission planning task.
Participants were given feedback on whether their decision was correct or not during the practice rounds, but they were not given any feedback on their performance during the actual data collection rounds.
The post-experiment questionnaires measured workload and learned trust. </p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Results</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">For all objective metrics, we performed a linear mixed effects analysis to determine the significance of the relationship between the metric and our independent variables.
The fixed effects were the Transparency Technique (Absent, Contextual Information, Explanation, Contextual Information and Explanation), Trajectory Awareness (observation or interaction), the average Dispositional Trust in People and the average Dispositional Trust in Technology.
We also included an interaction effect between Transparency Technique and Trajectory Awareness and intercepts for participants as a random effect.
We performed an ANOVA for each fitted linear mixed-effects model.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Objective Metrics</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS1.4.1.1">IV-A</span>1 </span>Final Decision Agreement</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">The team’s Final Decision Agreement (after the AI reveals its decision, shown as yellow diamond #3 in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S3.F1" title="Figure 1 ‣ III Methodology ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">1</span></a>) is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.F5" title="Figure 5 ‣ IV-A1 Final Decision Agreement ‣ IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">5</span></a>. All transparency techniques (providing contextual information, a local explanation, or both) result in a team agreement trending closer to the ideal 60% agreement than having no alignment or explanation technique (Absent). However, a linear mixed effects analysis (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.T1" title="TABLE I ‣ IV-A1 Final Decision Agreement ‣ IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">I</span></a>) for this metric revealed that none of the fixed effects were statistically significant in predicting the Final Agreement between the team.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="153" id="S4.F5.g1" src="extracted/5858843/images/exp4/exp4_final_agr.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.3.2" style="font-size:90%;">Final Agreement [%] Between Human and 60% Accurate AI</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.2.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S4.T1.3.2" style="font-size:90%;">ANOVA for Final Decision Agreement</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.4.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T1.4.1.1.1"></td>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.4.1.1.2">df, Error</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.4.1.1.3">F</th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T1.4.1.1.4">P</th>
</tr>
<tr class="ltx_tr" id="S4.T1.4.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.4.2.2.1">Transparency Technique</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.4.2.2.2">3, 172</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.4.2.2.3">2.024</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T1.4.2.2.4">0.1123</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.3.3">
<td class="ltx_td ltx_align_left" id="S4.T1.4.3.3.1">Trajectory Awareness</td>
<td class="ltx_td ltx_align_left" id="S4.T1.4.3.3.2">1, 172</td>
<td class="ltx_td ltx_align_left" id="S4.T1.4.3.3.3">0.232</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T1.4.3.3.4">0.6305</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T1.4.4.4.1">Transp. Techique x Trajectory Aw.</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T1.4.4.4.2">3, 172</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T1.4.4.4.3">1.337</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_b" id="S4.T1.4.4.4.4">0.2639</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1">Separating out the influence of the AI reliability, Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.F6" title="Figure 6 ‣ IV-A1 Final Decision Agreement ‣ IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">6</span></a> depicts how the trends for the team’s Final Decision Agreement were influenced by the reliability of the AI. In this figure, we collapsed across trajectory awareness for clarity as it had not previously shown to influence this metric.
Ideally, when the AI is correct, the human would agree with it 100% of the time; conversely when the AI is incorrect, that agreement would drop to 0%.
The average final agreement between the participant and the AI when the AI was correct (purple line in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.F6" title="Figure 6 ‣ IV-A1 Final Decision Agreement ‣ IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">6</span></a>) trended upward toward 100% when the participant worked with an AI that was transparent in some way.
We also see that the average Final Agreement between the participant and the AI when the AI was incorrect (brown line in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.F6" title="Figure 6 ‣ IV-A1 Final Decision Agreement ‣ IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">6</span></a>) trended downward toward 40% if the participant was aided by a technique to understand the AI Mission Computer.
This indicates that providing contextual information or a local explanation to the human improves their mental model of their AI partner (understanding that their AI teammate is limited in some capacity).
In turn, this improvement in the human’s mental model helps them determine when (and when not) to align with the AI’s suggestion for the final decision.
Further this indicates a bias toward AI agreement, particularly in the incorrect cases, that can be reduced, but not eliminated, with the introduction of transparency techniques.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="152" id="S4.F6.g1" src="extracted/5858843/images/exp4/final_agr_accuracy.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.3.2" style="font-size:90%;">Final Decision Agreement [%] Between Human and 60% Accurate AI for both Trajectory Awareness Levels</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS2.4.1.1">IV-A</span>2 </span>Task Performance</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.F7" title="Figure 7 ‣ IV-A2 Task Performance ‣ IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">7</span></a> shows the percentage of trials in which the human’s final decision was correct (Task Performance). The Absent level of working with the black box AI results in an average of  70% Task Performance, whereas having some transparency technique increased task performance to an average of  80%. A linear mixed effects analysis revealed a statistically significant difference between the baseline and each of the transparency techniques, but not between techniques. This means that having at least some sort of decision-making support yields higher team performance, as expected. Additionally, it was noted that explanation performed similarly to contextual information.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="147" id="S4.F7.g1" src="extracted/5858843/images/exp4/exp4_tp.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S4.F7.3.2" style="font-size:90%;">Percentage of trials in which the Human’s Final Decision is Correct (Task Performance)</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text" id="S4.T2.4.2" style="font-size:90%;"> ANOVA for Task Performance</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<td class="ltx_td ltx_border_tt" id="S4.T2.1.2.1.1"></td>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.2.1.2">df, error</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.2.1.3">F</th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.2.1.4">P</th>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.2">Transparency Technique</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.3">3, 172</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.4">8.787</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.1.1.1">
<math alttext="&lt;" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><lt id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">&lt;</annotation></semantics></math>0.0001</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<td class="ltx_td ltx_align_left" id="S4.T2.1.3.2.1">Trajectory Awareness</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.3.2.2">1, 172</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.3.2.3">6.199</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T2.1.3.2.4">0.0137</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T2.1.4.3.1">Transp. Techique x Trajectory Aw.</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T2.1.4.3.2">3, 172</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T2.1.4.3.3">1.862</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_b" id="S4.T2.1.4.3.4">0.1378</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">The results from the ANOVA for this metric (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.T2" title="TABLE II ‣ IV-A2 Task Performance ‣ IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">II</span></a>) indicate that access to a transparency technique is statistically significant in influencing how correct the participant’s final decision is. No other effects were significant.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS3.4.1.1">IV-A</span>3 </span>Sway, the AI’s Influence on the Participant</h4>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="154" id="S4.F8.g1" src="extracted/5858843/images/exp4/exp4_sway.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S4.F8.3.2" style="font-size:90%;">Gap in Human’s and AI’s Final Judgement</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.F8" title="Figure 8 ‣ IV-A3 Sway, the AI’s Influence on the Participant ‣ IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">8</span></a> shows the gap in the participant’s and AI’s final judgement, operationalized as the percentage of decisions per participant that changed from initial to final decisions to match the AI’s suggestion (positive being changes toward the AI stance).
Providing just the local explanation resulted in slightly higher deference to the AI’s suggestion than providing only or additional contextual information.
This indicates that, while having the local explanation yields equivalent Task Performance as having contextual information, it increases reliance on the AI partner, whereas having contextual information supports the human decision maker’s judgment to be on aligned with the AI partner when appropriate and to be informed sufficiently to contribute to a joint decision.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.3.1.1" style="font-size:90%;">TABLE III</span>: </span><span class="ltx_text" id="S4.T3.4.2" style="font-size:90%;"> ANOVA for Resolved Team Disagreements</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<td class="ltx_td ltx_border_t" id="S4.T3.1.2.1.1"></td>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.1.2">df, error</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.1.3">F</th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.1.4">P</th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.2">Transparency Technique</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.3">3, 172</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.4">3.469</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T3.1.1.1">
<math alttext="&lt;" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><lt id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">&lt;</annotation></semantics></math>0.0001</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<td class="ltx_td ltx_align_left" id="S4.T3.1.3.2.1">Trajectory Awareness</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.3.2.2">1, 172</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.3.2.3">2.345</td>
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T3.1.3.2.4">0.1275</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T3.1.4.3.1">Transp. Technique x Trajectory Aw.</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T3.1.4.3.2">3, 172</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T3.1.4.3.3">0.558</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_b" id="S4.T3.1.4.3.4">0.6436</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.SSS3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.p2.1">The results from the ANOVA for this metric (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.T3" title="TABLE III ‣ IV-A3 Sway, the AI’s Influence on the Participant ‣ IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">III</span></a>) indicate that having access to a transparency technique is statistically significant in predicting when the participant will change their decision to align with the AI’s suggestion.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS4.4.1.1">IV-A</span>4 </span>Average Completion Time</h4>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="152" id="S4.F9.g1" src="extracted/5858843/images/exp4/avg_time.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S4.F9.3.2" style="font-size:90%;">Average Time to Complete a Mission-Planning Trial</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">Finally, we observed the additional objective metric of time. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.F9" title="Figure 9 ‣ IV-A4 Average Completion Time ‣ IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">9</span></a> shows the average time it took for a participant to complete the mission planning task.
In our control group, where participants had no additional information regarding the mission planning task or their AI partner, participants took roughly 3 seconds to complete the task.
Subsequently, adding more transparency to the system, and thus more for the decision maker to contend with, resulted in higher completion times.
However, we see from Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.F10" title="Figure 10 ‣ IV-A4 Average Completion Time ‣ IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">10</span></a> that interacting with the figures of merit was the real driver of additional completion time. Individually assessing each FoM had a significant impact on the average completion time for those who saw contextual information as well.</p>
</div>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="152" id="S4.F10.g1" src="extracted/5858843/images/exp4/avg_time_traj.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F10.2.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="S4.F10.3.2" style="font-size:90%;">Average Time to Complete a Mission-Planning Trial</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Subjective Metrics</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS1.4.1.1">IV-B</span>1 </span>NASA TLX</h4>
<figure class="ltx_figure" id="S4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="457" id="S4.F11.g1" src="extracted/5858843/images/exp4/tlx_exp5.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F11.2.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text" id="S4.F11.3.2" style="font-size:90%;">Composite Results of NASA TLX Questionnaire</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.F11" title="Figure 11 ‣ IV-B1 NASA TLX ‣ IV-B Subjective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">11</span></a> shows the subjective mental workload of each participant using the NASA TLX assessment at the end of the experiment. While all six subscales were recorded as there was little physical activity and no temporal constraints we have chosen only to present the remaining four subscales. Having any transparency technique yields more effort, mental demand, and frustration on the user’s part, it also yields a slightly higher perception of their own performance.
Only Mental demand indicated any significant impact from the Transparency Techniques and only when compared to the baseline of nothing.
Additionally, while contextual information generated slightly more mental demand and effort than providing only a local explanation, both techniques elicited the same amount of frustration and perceived performance, while contextual information also bridged the gap between the human and AI’s judgement (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.F8" title="Figure 8 ‣ IV-A3 Sway, the AI’s Influence on the Participant ‣ IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">8</span></a>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS2.4.1.1">IV-B</span>2 </span>i-THAu</h4>
<figure class="ltx_figure" id="S4.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="167" id="S4.F12.g1" src="extracted/5858843/images/exp4/exp4_trust.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F12.2.1.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text" id="S4.F12.3.2" style="font-size:90%;">Post-Experiment i-THAu Metric of Human’s Trust in AI’s Capabilities</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.F12" title="Figure 12 ‣ IV-B2 i-THAu ‣ IV-B Subjective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">12</span></a> shows the human’s trust in the AI’s capabilities to complete the task measured in the post-experiment i-THAu assessment.
Across all treatments trust remained in the low positive range with scores close to 1 on a scale from -3 to 3.
Participants who worked with a black box AI (Transparency Technique = Absent) tended to have a slightly higher trust in the AI’s capabilities than participants who had more insight into the AI’s decision making.
However, this metric is not statistically significant for differences in trust in capability between transparency techniques.
Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.F12" title="Figure 12 ‣ IV-B2 i-THAu ‣ IV-B Subjective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">12</span></a> also indicates that having some sort of guided interaction/information enforcement in the form of interaction increases trust in the AI when given a local explanation only.
This may be because participants who only see a local explanation tend to defer to the AI’s suggestion more than those who only or additionally see contextual information (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.F8" title="Figure 8 ‣ IV-A3 Sway, the AI’s Influence on the Participant ‣ IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">8</span></a>), and interacting with relevant questions forces participants to absorb domain information more, therefore increasing their understanding of the AI’s suggestion.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Discussion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The purpose of this series of experiments was to empower and most effectively utilize human decision makers when working with AI recommender systems.
Rather than attempting to explain or interpret the model-specific workings of complex AI models, we examined an alternative approach designed to support the human decision-maker’s cognitive process of judgment, i.e. assessment of the state of the world, by providing a model-agnostic alternative method. We evaluated equipping the decision-maker with the relevant information that the AI system utilized to generate potential courses of action. Our hypothesis was that this technique would align the particpant’s situational awareness with that of the world in which the AI is operating.
By establishing a shared understanding of the underlying information, e.g. shared situation awareness, we expected that the overall performance of the human-AI team would improve as the human would be grounded sufficiently to make an informed and independent decision.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">This experiment was the last in a series of four that systematically evaluated the impact of contextual information to improve human judgment and align the human-AI situation awareness.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Here we sought to compare the effectiveness of supporting human judgement via contextual information versus the popular transparency technique of providing local decision point explanations. We found that providing contextual information was a viable alternate solution to providing explanations, as overall performance was very similar. This means that, based on the context of the problem space, this novel approach may be less costly to implement and yield as effective results as understanding the AI’s inner algorithms would.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">This work has some limitations.
This body of research work was conducted in a single domain, potentially restricting the broader applicability of its conclusions in other fields.
Nonetheless, we believe that supporting human judgment in decision-making scenarios where operators bear responsibility for their choices, especially in high-stakes cases, will likely serve to bolster performance across various fields.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">Furthermore, this study did not evaluate the efficacy of providing contextual information against other methods of enhancing human judgment. Moreover, the AI mission planner employed in our experiments was a basic heuristics-based model, and we did not explore alternative AI explanation techniques beyond local explanations.
Subsequent studies could investigate the effect different representations of contextual information could have on the human’s judgment, or compare this technique to other XAI techniques.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">These subjective metrics in conjunction with the objective metrics indicate that having at least some sort of transparency technique yields relatively equivalent, improved team performance with respect to having no transparency technique at all (Figs. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.F5" title="Figure 5 ‣ IV-A1 Final Decision Agreement ‣ IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.F7" title="Figure 7 ‣ IV-A2 Task Performance ‣ IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">7</span></a>).
However, having contextual information is more effective in reducing overreliance on AI (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10717v1#S4.F8" title="Figure 8 ‣ IV-A3 Sway, the AI’s Influence on the Participant ‣ IV-A Objective Metrics ‣ IV Results ‣ Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance"><span class="ltx_text ltx_ref_tag">8</span></a>) and determining in what ways the AI is limited/where its error boundary is.
Both transparency techniques were also effective in boosting the human decision maker’s perception of their own performance while negligibly increasing participant frustration.
Additionally, we see that, although those who see world state information take more time to complete the task, they achieve significantly better objective performance than those who have no insight into the system. These findings are important because they emphasize the need for transparency in AI systems to achieve improved team performance, and this technique enables various solutions for increasing transparency for human-autonomy teams that can be determined based on system design and environment criteria.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was funded by Sandia National Laboratory (SNL) with Dr. Paul Schutte serving as Program Manager.
This work is solely that of the authors and does not represent an official SNL position.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y. Yedidi, “Appstek corp,” https://appstekcorp.com/blog/10-remarkable-real-world-examples-of-recommender-systems/, April 6, 2023, accessed: September 1, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Shaikh, “Capital numbers,” https://www.capitalnumbers.com/blog/ai-driven-recommendation-systems-finance/, March 19, 2024, accessed: September 1, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Z. C. Lipton, “The Mythos of Model Interpretability,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv:1606.03490 [cs, stat]</em>, Mar. 2017, arXiv: 1606.03490.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
W. J. von Eschenbach, “Transparency and the black box problem: Why we do not trust ai,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Philosophy and Technology</em>, vol. 34, no. 4, pp. 1607–1622, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. A. Baker, P. J. Kornguth, J. Y. Lo, M. E. Williford, and C. E. Floyd, “Breast cancer: prediction with artificial neural network based on bi-rads standardized lexicon.” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Radiology</em>, vol. 196 3, pp. 817–22, 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Parmar, D. A. Illingworth, and R. P. Thomas, “Model blindness: A framework for understanding how model-based decision support systems can lead to performance degradation,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the Human Factors and Ergonomics Society Annual Meeting</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
L. Martignon and U. Hoffrage, “Fast, frugal, and fit: Simple heuristics for paired comparison,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Theory and Decision</em>, vol. 52, pp. 29–71, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. C. Canellas, K. M. Feigh, and Z. K. Chua, “Accuracy and effort of decision-making strategies with incomplete information: Implications for decision support system design,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">IEEE Transactions on Human-Machine Systems</em>, vol. 45, no. 6, pp. 686–701, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
U. S. N. T. S. Board, <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Descent Below Visual Glidepath and Impact with Seawall, Asiana Airlines Flight 214, Boeing 777-200ER, HL7742, San Francisco, California, July 6, 2013</em>, ser. Aircraft accident report.   National Transportation Safety Board, 2014. [Online]. Available: https://books.google.com/books?id=b4zrjgEACAAJ

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
R. Williams and R. Yampolskiy, “Understanding and avoiding ai failures: A practical guide,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Philosophies</em>, vol. 6, no. 3, 2021. [Online]. Available: https://www.mdpi.com/2409-9287/6/3/53

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
R. Parasuraman and D. H. Manzey, “Complacency and bias in human use of automation: An attentional integration,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Human factors</em>, vol. 52, no. 3, pp. 381–410, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
C. Zsambok and G. Klein, <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Naturalistic Decision Making</em>, ser. Expertise, research and applications.   L. Erlbaum Associates, 1997. [Online]. Available: https://books.google.com/books?id=0P0kAQAAMAAJ

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Y. Yang, E. Kandogan, Y. Li, P. Sen, and W. S. Lasecki, “A study on interaction in human-in-the-loop machine learning for text analytics.” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">IUI Workshops</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
B. Becker, R. Kohavi, and D. Sommerfield, “Visualizing the simple baysian classifier,” in <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Information visualization in data mining and knowledge discovery</em>, 2001, pp. 237–249.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
U. Erra, B. Frola, and V. Scarano, “An interactive bio-inspired approach to clustering and visualizing datasets,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">2011 15th International Conference on Information Visualisation</em>.   IEEE, 2011, pp. 440–447.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C. C. Monroe, “Optimizing military planners course of action decision making,” Master’s thesis, Atlanta GA, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
R. Dazeley, P. Vamplew, C. Foale, C. Young, S. Aryal, and F. Cruz, “Levels of explainable artificial intelligence for human-aligned conversational explanations,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Artificial Intelligence</em>, vol. 299, oct 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
D. K. Srivastava, J. M. Lilly, and K. M. Feigh, “Improving human situation awareness in ai-advised decision making,” in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">2022 IEEE 3rd International Conference on Human-Machine Systems (ICHMS)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
D. Srivastava, J. M. Lilly, and K. M. Feigh, “Exploring the role of judgement and shared situation awareness when working with ai recommender systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Cognition, Technology &amp; Work</em>, Jul 2024. [Online]. Available: https://doi.org/10.1007/s10111-024-00771-9

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J. Kolb, D. Srivastava, and K. M. Feigh, “The effects of inaccurate decision-support systems on structured shared decision-making for human-robot teams,” in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)</em>.   IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
D. Srivastava, J. Kolb, and K. Feigh, “Impact of abstraction levels of context information on ai-advised decision making for an entry descent and landing task,” 01 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
H. Lakkaraju, E. Kamar, R. Caruana, and J. Leskovec, “Interpretable &amp; explorable approximations of black box models,” <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">CoRR</em>, vol. abs/1707.01154, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S. M. Lundberg and S.-I. Lee, “A unified approach to interpreting model predictions,” in <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Advances in Neural Information Processing Systems</em>, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., vol. 30.   Curran Associates, Inc., 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Q. Zhang, J. Lu, and Y. Jin, <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Artificial intelligence in recommender systems</em>, 2021, pp. 439–457.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y. Hu, Q. Da, A. Zeng, Y. Yu, and Y. Xu, “Reinforcement learning to rank in e-commerce search engine: Formalization, analysis, and application,” 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
E. Ie, V. Jain, J. Wang, S. Narvekar, R. Agarwal, R. Wu, H.-T. Cheng, T. Chandra, and C. Boutilier, “Slateq: A tractable decomposition for reinforcement learning with recommendation sets,” in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the Twenty-eighth International Joint Conference on Artificial Intelligence (IJCAI-19)</em>, Macau, China, 2019, pp. 2592–2599, see arXiv:1905.12767 for a related and expanded paper (with additional material and authors).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Y. Gong and Q. Zhang, “Hashtag recommendation using attention-based convolutional neural network,” in <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</em>, ser. IJCAI’16.   AAAI Press, 2016, p. 2782–2788.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
H. Jing and A. J. Smola, “Neural survival recommender,” in <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</em>, ser. WSDM ’17.   New York, NY, USA: Association for Computing Machinery, 2017, p. 515–524. [Online]. Available: https://doi.org/10.1145/3018661.3018719

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
S. Rana, “Decision intelligence frameworks — ooda loop vs sealtm by diwo,” May 2020. [Online]. Available: https://satyendra-p-rana.medium.com/decision-intelligence-frameworks-ooda-loop-vs-seal-by-diwo-cffb511dffe2

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
S. H. Haeckel, “Adaptive enterprise design: The sense-and-respond model,” <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Planning Review</em>, vol. 23, no. 3, p. 6, May 1995, copyright - Copyright Planning Forum May/Jun 1995; Last updated - 2023-12-05. [Online]. Available: https://www.proquest.com/scholarly-journals/adaptive-enterprise-design-sense-respond-model/docview/194374874/se-2

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
D. Hendrick, “Complexity theory and conflict transformation: An exploration of potential and implications,” 01 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
N. B. Sarter, R. Amalberti, and E. Hollnagel, <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Modeling the Orderliness of Human Action</em>.   Lawrence Erlbaum Associates, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
R. J. Harrington and M. C. Ottenbacher, “Decision-making tactics and contextual features: Strategic, tactical and operational implications,” <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">International Journal of Hospitality &amp; Tourism Administration</em>, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
M. Ben-Akiva, A. de Palma, D. Mcfadden, M. Abou-Zeid, P. Chiappori, M. de Lapparent, S. Durlauf, M. Fosgerau, D. Fukuda, S. Hess, C. Manski, A. Pakes, N. Picard, and J. Walker, “Process and context in choice models,” <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Marketing Letters</em>, vol. 23, pp. 439–456, 06 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
J. Vazquez-Diz, J. Morillo Baro, R. Reigal Garrido, V. Morales Sanchez, and A. Hernández Mendo, “Contextual factors and decision-making in the behavior of finalization in the positional attack in beach handball: Differences by gender through polar coordinates analysis,” <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Frontiers in Psychology</em>, vol. 10, 06 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
A. R. Wagner, J. Borenstein, and A. Howard, “Overtrust in the robotic age,” <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Commun. ACM</em>, vol. 61, no. 9, p. 22–24, aug 2018. [Online]. Available: https://doi.org/10.1145/3241365

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
P. Robinette, W. Li, R. Allen, A. M. Howard, and A. R. Wagner, “Overtrust of robots in emergency evacuation scenarios,” in <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</em>, 2016, pp. 101–108.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
S. Booth, J. Tomokin, H. Pfister, J. Waldo, K. Gajos, and R. Nagpal, “Piggybacking robots: Human-robot overtrust in university dormitory security,” in <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI</em>, 2017, pp. 426–434.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
J. D. Lee and K. A. See, “Trust in automation: Designing for appropriate reliance,” <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Human Factors</em>, vol. 46, no. 1, pp. 50–80, 2004, pMID: 15151155. [Online]. Available: https://doi.org/10.1518/hfes.46.1.50_30392

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
A. Bussone, S. Stumpf, and D. O’Sullivan, “The role of explanations on trust and reliance in clinical decision support systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">2015 International Conference on Healthcare Informatics</em>, 2015, pp. 160–169.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi, “A survey of methods for explaining black box models,” <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">ACM Comput. Surv.</em>, vol. 51, no. 5, aug 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
M. T. Ribeiro, S. Singh, and C. Guestrin, “”why should i trust you?”: Explaining the predictions of any classifier,” 2016. [Online]. Available: https://arxiv.org/abs/1602.04938

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
K. Aas, M. Jullum, and A. Løland, “Explaining individual predictions when features are dependent: More accurate approximations to shapley values,” 2019. [Online]. Available: https://arxiv.org/abs/1903.10464

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
D. Alvarez-Melis and T. S. Jaakkola, “On the robustness of interpretability methods,” 2018. [Online]. Available: https://arxiv.org/abs/1806.08049

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Y. Razin, “Interdependent trust for humans and automation survey,” Available at https://sites.gatech.edu/feigh-lab/publications/ (2022).

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 16 20:39:41 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
