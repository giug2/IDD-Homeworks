<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2001.02501] Table Structure Extraction with Bi-directional Gated Recurrent Unit Networks</title><meta property="og:description" content="Tables present summarized and structured information to the reader, which makes table’s structure extraction an important part of document understanding applications. However, table structure identification is a hard p…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Table Structure Extraction with Bi-directional Gated Recurrent Unit Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Table Structure Extraction with Bi-directional Gated Recurrent Unit Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2001.02501">

<!--Generated on Fri Mar  1 21:15:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Table Structure Extraction with Bi-directional 
<br class="ltx_break">Gated Recurrent Unit Networks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Saqib Ali Khan<sup id="id7.7.id1" class="ltx_sup"><span id="id7.7.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Syed Muhammad Daniyal Khalid<sup id="id8.8.id2" class="ltx_sup"><span id="id8.8.id2.1" class="ltx_text ltx_font_italic">1</span></sup>, Muhammad Ali Shahzad<sup id="id9.9.id3" class="ltx_sup"><span id="id9.9.id3.1" class="ltx_text ltx_font_italic">1,2</span></sup> and Faisal Shafait<sup id="id10.10.id4" class="ltx_sup"><span id="id10.10.id4.1" class="ltx_text ltx_font_italic">1,2</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><sup id="id11.11.id1" class="ltx_sup"><span id="id11.11.id1.1" class="ltx_text ltx_font_italic">1</span></sup> School of Electrical Engineering and Computer Science (SEECS),
<br class="ltx_break">National University of Sciences and Technology (NUST),
Islamabad, Pakistan
<br class="ltx_break"><sup id="id12.12.id2" class="ltx_sup"><span id="id12.12.id2.1" class="ltx_text ltx_font_italic">2</span></sup> Deep Learning Laboratory, National Center of Artificial Intelligence (NCAI), Islamabad, Pakistan
<br class="ltx_break">Email: faisal.shafait@seecs.edu.pk
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id13.id1" class="ltx_p"><span id="id13.id1.1" class="ltx_text">Tables present summarized and structured information to the reader, which makes table’s structure extraction an important part of document understanding applications. However, table structure identification is a hard problem not only because of the large variation in the table layouts and styles, but also owing to the variations in the page layouts and the noise contamination levels. A lot of research has been done to identify table structure, most of which is based on applying heuristics with the aid of optical character recognition (OCR) to hand pick layout features of the tables. These methods fail to generalize well because of the variations in the table layouts and the errors generated by OCR. In this paper, we have proposed a robust deep learning based approach to extract rows and columns from a detected table in document images with a high precision. In the proposed solution, the table images are first pre-processed and then fed to a bi-directional Recurrent Neural Network with Gated Recurrent Units (GRU) followed by a fully-connected layer with softmax activation. The network scans the images from top-to-bottom as well as left-to-right and classifies each input as either a row-separator or a column-separator. We have benchmarked our system on publicly available UNLV as well as ICDAR 2013 datasets on which it outperformed the state-of-the-art table structure extraction systems by a significant margin.</span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
<span id="id14.id1" class="ltx_text">
component; bi directional GRU, table-layouts, UNLV
</span>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">A table contains an ordered arrangement of rows and columns that are widely used to present a set of facts about some information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. They are widely used in research articles, data analysis, newspapers, magazines, invoices and financial documents. Tables present multiple information points for a large number of items in rows and columns that are easy to perceive and analyze. They structure the information to provide a visual summary of the most valuable information contained in the document. It is for this reason that the table recognition systems have captured the interest of a large number of researchers to make contributions in this domain over the past two decades.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Tables have numerous layouts which makes it very hard for conventional feature engineering approaches to decode table structures generically. These approaches generally rely on visual features like ruling lines, spacing between different columns, type of data in the table cells, their relationships with overlapping neighbors or color encoded cell blocks. They perform reasonably well on the tables of a particular layout or a business case but fail to scale across multiple domains.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In the recent years, researchers have greatly improved the results of computer vision problems by applying deep learning techniques. Schreiber et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> proposed a deep learning based approach for recognizing rows and columns of tables in document images. Their proposed system employs a semantic segmentation (FCN-Xs architectures) model with custom tweaking to the hyper-features as well as skip pooling to enhance the segmentation results. The major limitation of this method is the way FCN processes the table. Each stride of an FCN filter maps a portion of the input image pixels to an output pixel. This fails to capture the fact that the rows and columns in a table follow a unique repetitive sequence of in-between spacing and data length as the information of the next and the previous row-column elements is not taken into account. Also, the receptive field of the CNN based models does not process the entire row or column in a single stride. In this paper, we overcome this limitation by using a sequential modeling approach. Specifically two bi-directional GRUs are used. One bi-directional GRU identifies the row boundaries while the other identifies the column boundaries. Each bi-direction GRU has its own fully connected layer to classify the input as either a row-boundary or a column-boundary. Our approach successfully overcomes the limitations of a CNN based model and provides a data-driven approach towards a general, layout independent table structure extraction system.
We have benchmarked our system on publicly available UNLV dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> where it outperformed T-Recs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> table structure recognition system. It is to be noted that no part of the UNLV dataset has been used in the training process.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The rest of this paper is organized in the following sections: Section II consists of the related work in table structure recognition domain. Section III elaborates our proposed methodology that consists of a pre-processing module and a classification module. Section IV presents the evaluation metrics while benchmarking and evaluation of the proposed algorithm is detailed in Section V. Section VI provides the conclusive remarks as a guideline for the future work in this domain.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">A substantial amount of work has been done to identify the structure of a table both using heuristic-based methods as well as using deep learning. Kieninger et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> proposed a system which was one of the earliest successful attempts on table structure extraction problem called T-Recs. The input to this system is the word bounding boxes. These boxes are then grouped into rows and columns using a bottom-up approach by evaluating the vertical and horizontal overlaps between the boxes to form a segmentation graph. The major problem in this approach is that the output depends on a large number of parameters values that are heuristically set. Besides, the algorithm fails if the preceding OCR step does not correctly identify words bounding boxes (for example if the character recognizer misses dots and commas in numeric data).</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> proposed a data-driven approach similar to the X-Y cut algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> that is based on probability optimization technique to solve table structure extraction problem. This statistical algorithm uses probabilities that are derived from a large training corpus. This method also takes into account the distances between adjacent words and it works on single column, double column and mixed column layouts.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Shigarov et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> proposed a method that relies on PDF metadata with information including font and text bounding boxes. The algorithm uses ad-hoc heuristics for recovering table cells from text chunks and ruling lines. The algorithm combines these text chunks into text blocks through a text block recovery algorithm and then uses a threshold to configure the block vertically or horizontally.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Zanibbi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> presented a survey for table recognition systems in terms of interactions of table models, observations, transformations, and inferences. Their survey answers questions about what and when some decisions are made by table structure recognition systems. Furthermore, this survey outlines the dataset used for the training and evaluation of these systems.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Jianying et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> proposed a general algorithm for table structure extraction from an already detected table region. In their proposed methodology, they have used hierarchical clustering for column detection. Additionally, the system uses lexical and spatial criteria to classify headers of tables. They have used a directed acyclic attribute graph or DAG for evaluation of table structure extraction.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> proposed an automatic ground truth generation system which can generate a large amount of accurate ground truth for table recognition systems. They use novel background analysis table recognition algorithms and an X-Y cut algorithm for identifying table regions in the document images. This system takes line and word segmentation results as input and outputs table cell detection results.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Kasar et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> proposed a technique for table structure extraction based on query-patterns. This approach is a client-driven approach in which the client will provide the query pattern based on the location of key fields in the document. The input query pattern is then converted into a relational graph in which the nodes represent the features and the edges represent the spatial relationship between these nodes.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p">Shamilian et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> proposed a system that reads layout of the tables in machine printed form. They have provided a graphical user interface (GUI) for users to define contextual rules to identify key fields inside a table. The system can also be manually retargeted to new layouts by the user. This system has been applied to more than 400 distinct tabular layouts.</p>
</div>
<div id="S2.p9" class="ltx_para">
<p id="S2.p9.1" class="ltx_p">Schreiber et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> proposed a deep learning based approach for table structure recognition. This system uses semantic segmentation model with FCN-8 architecture and skip pooling features to detect rows and columns of a table. Additionally, they have vertically stretched the table images in order to increase the precision on row detection. Furthermore, they have used CRF to improve the results of semantic segmentation model. Siddiqui el al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> also proposed a deep learning based method based on Deep Deformable Convolutional Neural Network (CNN) for table detection.</p>
</div>
<div id="S2.p10" class="ltx_para">
<p id="S2.p10.1" class="ltx_p">In this paper, we have proposed a novel solution for table structure extraction using a sequential model, assuming that the table has already been detected using an existing algorithm (e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. In the proposed methodology, the table images are first pre-processed by applying binarization, noise removal, and morphological transformation. These transformed images are then passed to a bi-directional Gated Recurrent Unit (GRU) recurrent neural network that detects rows and columns in the table.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Proposed Methodology</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The proposed method is divided into three modules: Image pre-processing, a row-column classifier and post-processing. The pre-processing step plays a crucial role in converting the table images containing text to natural images that do not contain textual features. These images are then passed to the classifier that uses rows and columns as time steps to classify each row and column. In the post-processing step, the segmentation space generated by the classifier is parsed to give a single line prediction of rows and columns. This section explains each module in greater detail.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Image Pre-processing</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The first and the foremost step is pre-processing the table images. This step plays a preliminary role in converting the raw table images to a simpler form so that the layout or structure of the table is more apparent. The goal of this transformation is to increase the efficiency of our classifier by removing unnecessary detail from the input images.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The images are first cleaned up by removing the ruling lines and other non-text foreground objects. The cleaned image is then run through adaptive binarization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> so that the pixel intensities are uniform. Once the images have been binarized, they are resized to a fixed dimension of <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="1600\times 512" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mn id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">1600</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><times id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">1600</cn><cn type="integer" id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">1600\times 512</annotation></semantics></math> as the neural network is designed to process fixed size inputs.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2001.02501/assets/images/row_network.jpeg" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="244" height="130" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Neural Architecture for row classification: Passing a <math id="S3.F1.4.m1.1" class="ltx_Math" alttext="(1600\times 512)" display="inline"><semantics id="S3.F1.4.m1.1b"><mrow id="S3.F1.4.m1.1.1.1" xref="S3.F1.4.m1.1.1.1.1.cmml"><mo stretchy="false" id="S3.F1.4.m1.1.1.1.2" xref="S3.F1.4.m1.1.1.1.1.cmml">(</mo><mrow id="S3.F1.4.m1.1.1.1.1" xref="S3.F1.4.m1.1.1.1.1.cmml"><mn id="S3.F1.4.m1.1.1.1.1.2" xref="S3.F1.4.m1.1.1.1.1.2.cmml">1600</mn><mo lspace="0.222em" rspace="0.222em" id="S3.F1.4.m1.1.1.1.1.1" xref="S3.F1.4.m1.1.1.1.1.1.cmml">×</mo><mn id="S3.F1.4.m1.1.1.1.1.3" xref="S3.F1.4.m1.1.1.1.1.3.cmml">512</mn></mrow><mo stretchy="false" id="S3.F1.4.m1.1.1.1.3" xref="S3.F1.4.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F1.4.m1.1c"><apply id="S3.F1.4.m1.1.1.1.1.cmml" xref="S3.F1.4.m1.1.1.1"><times id="S3.F1.4.m1.1.1.1.1.1.cmml" xref="S3.F1.4.m1.1.1.1.1.1"></times><cn type="integer" id="S3.F1.4.m1.1.1.1.1.2.cmml" xref="S3.F1.4.m1.1.1.1.1.2">1600</cn><cn type="integer" id="S3.F1.4.m1.1.1.1.1.3.cmml" xref="S3.F1.4.m1.1.1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.4.m1.1d">(1600\times 512)</annotation></semantics></math> pre-processed image to a bi-directional GRU with an input of size <math id="S3.F1.5.m2.1" class="ltx_Math" alttext="(1600\times 1)" display="inline"><semantics id="S3.F1.5.m2.1b"><mrow id="S3.F1.5.m2.1.1.1" xref="S3.F1.5.m2.1.1.1.1.cmml"><mo stretchy="false" id="S3.F1.5.m2.1.1.1.2" xref="S3.F1.5.m2.1.1.1.1.cmml">(</mo><mrow id="S3.F1.5.m2.1.1.1.1" xref="S3.F1.5.m2.1.1.1.1.cmml"><mn id="S3.F1.5.m2.1.1.1.1.2" xref="S3.F1.5.m2.1.1.1.1.2.cmml">1600</mn><mo lspace="0.222em" rspace="0.222em" id="S3.F1.5.m2.1.1.1.1.1" xref="S3.F1.5.m2.1.1.1.1.1.cmml">×</mo><mn id="S3.F1.5.m2.1.1.1.1.3" xref="S3.F1.5.m2.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.F1.5.m2.1.1.1.3" xref="S3.F1.5.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F1.5.m2.1c"><apply id="S3.F1.5.m2.1.1.1.1.cmml" xref="S3.F1.5.m2.1.1.1"><times id="S3.F1.5.m2.1.1.1.1.1.cmml" xref="S3.F1.5.m2.1.1.1.1.1"></times><cn type="integer" id="S3.F1.5.m2.1.1.1.1.2.cmml" xref="S3.F1.5.m2.1.1.1.1.2">1600</cn><cn type="integer" id="S3.F1.5.m2.1.1.1.1.3.cmml" xref="S3.F1.5.m2.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.5.m2.1d">(1600\times 1)</annotation></semantics></math> at each timestep. The bi-directional GRU outputs a <math id="S3.F1.6.m3.1" class="ltx_Math" alttext="(512\times 2)" display="inline"><semantics id="S3.F1.6.m3.1b"><mrow id="S3.F1.6.m3.1.1.1" xref="S3.F1.6.m3.1.1.1.1.cmml"><mo stretchy="false" id="S3.F1.6.m3.1.1.1.2" xref="S3.F1.6.m3.1.1.1.1.cmml">(</mo><mrow id="S3.F1.6.m3.1.1.1.1" xref="S3.F1.6.m3.1.1.1.1.cmml"><mn id="S3.F1.6.m3.1.1.1.1.2" xref="S3.F1.6.m3.1.1.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S3.F1.6.m3.1.1.1.1.1" xref="S3.F1.6.m3.1.1.1.1.1.cmml">×</mo><mn id="S3.F1.6.m3.1.1.1.1.3" xref="S3.F1.6.m3.1.1.1.1.3.cmml">2</mn></mrow><mo stretchy="false" id="S3.F1.6.m3.1.1.1.3" xref="S3.F1.6.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F1.6.m3.1c"><apply id="S3.F1.6.m3.1.1.1.1.cmml" xref="S3.F1.6.m3.1.1.1"><times id="S3.F1.6.m3.1.1.1.1.1.cmml" xref="S3.F1.6.m3.1.1.1.1.1"></times><cn type="integer" id="S3.F1.6.m3.1.1.1.1.2.cmml" xref="S3.F1.6.m3.1.1.1.1.2">512</cn><cn type="integer" id="S3.F1.6.m3.1.1.1.1.3.cmml" xref="S3.F1.6.m3.1.1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.6.m3.1d">(512\times 2)</annotation></semantics></math> vector which is post-processed to get a single regressed row segmentation boundary.</figcaption>
</figure>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.2" class="ltx_p">After binarization, three iterations of dilation transform are applied to the resized image using a rectangular kernel. In the case of column detection, the dilation kernel is a vertical dilation filter of dimensions <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="3\times 5" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mrow id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mn id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.1.m1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><times id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">3</cn><cn type="integer" id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">3\times 5</annotation></semantics></math> and in the case of row detection, it is a horizontal dilation filter of dimensions <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="5\times 3" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mn id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.2.m2.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><times id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1"></times><cn type="integer" id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">5</cn><cn type="integer" id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">5\times 3</annotation></semantics></math>. These dilation operations join the adjacent rows and columns, which helps the model to pick up the pattern of the row and the column separators. The transformed images are then normalized to have values between 0 and 1 to be fed to the subsequent recurrent neural network.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Model</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">This section provides details of the proposed methodology and it is further divided into two parts i) Column Classification ii) Row Classification. These two tasks are not very different by nature yet they require different model organization.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The crux of our approach is to identify segmentation space between the rows and the columns using recurrent neural networks. Different architectures of recurrent neural networks are proposed in the literature. We have selected Gated Recurrent Unit (GRU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and Long Short-Term Memory (LSTM) networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> for our algorithm because of their ability to incorporate contextual information without vanishing gradient problem. The results demonstrate (see Section <a href="#S4.F3" title="Figure 3 ‣ IV Performance Measures ‣ Table Structure Extraction with Bi-directional Gated Recurrent Unit Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> that GRUs outperform the LSTMs by a significant margin for both row and column classification. An analysis of the results showed that the LSTM networks, due to their inherent complexity, tend to overfit on the simpler data. The later sections in this paper will only discuss the approach with GRUs for brevity, as the approach with LSTMs is quite similar.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The bi-directional GRU takes rows and columns as timesteps and use the information of previous row-column elements to predict future ones. This approach provides a significant improvement over the CNN based models because of the memory cells in GRUs that learns the pattern of inter-row and inter-column spacing and the sequence of repetition of row-column elements effectively. This approach outperformed Schreiber et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> table structure extraction system based on semantic segmentation by a significant margin. The architectures for row and column classification are detailed in the following two sections.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2001.02501/assets/images/column_network.jpeg" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="244" height="129" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Neural Architecture for column classification: Passing a <math id="S3.F2.4.m1.1" class="ltx_Math" alttext="(1600\times 512)" display="inline"><semantics id="S3.F2.4.m1.1b"><mrow id="S3.F2.4.m1.1.1.1" xref="S3.F2.4.m1.1.1.1.1.cmml"><mo stretchy="false" id="S3.F2.4.m1.1.1.1.2" xref="S3.F2.4.m1.1.1.1.1.cmml">(</mo><mrow id="S3.F2.4.m1.1.1.1.1" xref="S3.F2.4.m1.1.1.1.1.cmml"><mn id="S3.F2.4.m1.1.1.1.1.2" xref="S3.F2.4.m1.1.1.1.1.2.cmml">1600</mn><mo lspace="0.222em" rspace="0.222em" id="S3.F2.4.m1.1.1.1.1.1" xref="S3.F2.4.m1.1.1.1.1.1.cmml">×</mo><mn id="S3.F2.4.m1.1.1.1.1.3" xref="S3.F2.4.m1.1.1.1.1.3.cmml">512</mn></mrow><mo stretchy="false" id="S3.F2.4.m1.1.1.1.3" xref="S3.F2.4.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.4.m1.1c"><apply id="S3.F2.4.m1.1.1.1.1.cmml" xref="S3.F2.4.m1.1.1.1"><times id="S3.F2.4.m1.1.1.1.1.1.cmml" xref="S3.F2.4.m1.1.1.1.1.1"></times><cn type="integer" id="S3.F2.4.m1.1.1.1.1.2.cmml" xref="S3.F2.4.m1.1.1.1.1.2">1600</cn><cn type="integer" id="S3.F2.4.m1.1.1.1.1.3.cmml" xref="S3.F2.4.m1.1.1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.m1.1d">(1600\times 512)</annotation></semantics></math> pre-processed image to a bi-directional GRU with an input of size <math id="S3.F2.5.m2.1" class="ltx_Math" alttext="(512\times 1)" display="inline"><semantics id="S3.F2.5.m2.1b"><mrow id="S3.F2.5.m2.1.1.1" xref="S3.F2.5.m2.1.1.1.1.cmml"><mo stretchy="false" id="S3.F2.5.m2.1.1.1.2" xref="S3.F2.5.m2.1.1.1.1.cmml">(</mo><mrow id="S3.F2.5.m2.1.1.1.1" xref="S3.F2.5.m2.1.1.1.1.cmml"><mn id="S3.F2.5.m2.1.1.1.1.2" xref="S3.F2.5.m2.1.1.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S3.F2.5.m2.1.1.1.1.1" xref="S3.F2.5.m2.1.1.1.1.1.cmml">×</mo><mn id="S3.F2.5.m2.1.1.1.1.3" xref="S3.F2.5.m2.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.F2.5.m2.1.1.1.3" xref="S3.F2.5.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.5.m2.1c"><apply id="S3.F2.5.m2.1.1.1.1.cmml" xref="S3.F2.5.m2.1.1.1"><times id="S3.F2.5.m2.1.1.1.1.1.cmml" xref="S3.F2.5.m2.1.1.1.1.1"></times><cn type="integer" id="S3.F2.5.m2.1.1.1.1.2.cmml" xref="S3.F2.5.m2.1.1.1.1.2">512</cn><cn type="integer" id="S3.F2.5.m2.1.1.1.1.3.cmml" xref="S3.F2.5.m2.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.5.m2.1d">(512\times 1)</annotation></semantics></math> at each timestep. The bi-directional GRU outputs a <math id="S3.F2.6.m3.1" class="ltx_Math" alttext="(1600\times 2)" display="inline"><semantics id="S3.F2.6.m3.1b"><mrow id="S3.F2.6.m3.1.1.1" xref="S3.F2.6.m3.1.1.1.1.cmml"><mo stretchy="false" id="S3.F2.6.m3.1.1.1.2" xref="S3.F2.6.m3.1.1.1.1.cmml">(</mo><mrow id="S3.F2.6.m3.1.1.1.1" xref="S3.F2.6.m3.1.1.1.1.cmml"><mn id="S3.F2.6.m3.1.1.1.1.2" xref="S3.F2.6.m3.1.1.1.1.2.cmml">1600</mn><mo lspace="0.222em" rspace="0.222em" id="S3.F2.6.m3.1.1.1.1.1" xref="S3.F2.6.m3.1.1.1.1.1.cmml">×</mo><mn id="S3.F2.6.m3.1.1.1.1.3" xref="S3.F2.6.m3.1.1.1.1.3.cmml">2</mn></mrow><mo stretchy="false" id="S3.F2.6.m3.1.1.1.3" xref="S3.F2.6.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.6.m3.1c"><apply id="S3.F2.6.m3.1.1.1.1.cmml" xref="S3.F2.6.m3.1.1.1"><times id="S3.F2.6.m3.1.1.1.1.1.cmml" xref="S3.F2.6.m3.1.1.1.1.1"></times><cn type="integer" id="S3.F2.6.m3.1.1.1.1.2.cmml" xref="S3.F2.6.m3.1.1.1.1.2">1600</cn><cn type="integer" id="S3.F2.6.m3.1.1.1.1.3.cmml" xref="S3.F2.6.m3.1.1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.6.m3.1d">(1600\times 2)</annotation></semantics></math> vector which is post-processed to get a single regressed column segmentation boundary.</figcaption>
</figure>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.4.1.1" class="ltx_text">III-B</span>1 </span>Column Classification</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.5" class="ltx_p">The neural architecture for column recognition classifies each column of the image as either a column or a whitespace between two columns. The images are passed one at a time and each image is considered to be a batch like in stochastic gradient descent (SGD). The pre-processed input image of dimension <math id="S3.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="1600\times 512" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mrow id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS1.p1.1.m1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml">1600</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.1.m1.1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS1.p1.1.m1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><apply id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"><times id="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2">1600</cn><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">1600\times 512</annotation></semantics></math> within a single batch is split into <math id="S3.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="1600" display="inline"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mn id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml">1600</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><cn type="integer" id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1">1600</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">1600</annotation></semantics></math> sequences (columns), each consisting of <math id="S3.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><mn id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><cn type="integer" id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">512</annotation></semantics></math> pixel values. We have used a hidden dimension of size <math id="S3.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><mn id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><cn type="integer" id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">512</annotation></semantics></math>. The two-layer GRU is initialized with hidden dimensions <math id="S3.SS2.SSS1.p1.5.m5.1" class="ltx_Math" alttext="(4\times 1\times 512)" display="inline"><semantics id="S3.SS2.SSS1.p1.5.m5.1a"><mrow id="S3.SS2.SSS1.p1.5.m5.1.1.1" xref="S3.SS2.SSS1.p1.5.m5.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p1.5.m5.1.1.1.2" xref="S3.SS2.SSS1.p1.5.m5.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS1.p1.5.m5.1.1.1.1" xref="S3.SS2.SSS1.p1.5.m5.1.1.1.1.cmml"><mn id="S3.SS2.SSS1.p1.5.m5.1.1.1.1.2" xref="S3.SS2.SSS1.p1.5.m5.1.1.1.1.2.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.5.m5.1.1.1.1.1" xref="S3.SS2.SSS1.p1.5.m5.1.1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS1.p1.5.m5.1.1.1.1.3" xref="S3.SS2.SSS1.p1.5.m5.1.1.1.1.3.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p1.5.m5.1.1.1.1.1a" xref="S3.SS2.SSS1.p1.5.m5.1.1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS1.p1.5.m5.1.1.1.1.4" xref="S3.SS2.SSS1.p1.5.m5.1.1.1.1.4.cmml">512</mn></mrow><mo stretchy="false" id="S3.SS2.SSS1.p1.5.m5.1.1.1.3" xref="S3.SS2.SSS1.p1.5.m5.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.5.m5.1b"><apply id="S3.SS2.SSS1.p1.5.m5.1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.1"><times id="S3.SS2.SSS1.p1.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS1.p1.5.m5.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.1.1.2">4</cn><cn type="integer" id="S3.SS2.SSS1.p1.5.m5.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.1.1.3">1</cn><cn type="integer" id="S3.SS2.SSS1.p1.5.m5.1.1.1.1.4.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.1.1.4">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.5.m5.1c">(4\times 1\times 512)</annotation></semantics></math> corresponding to 2 * number of layers * batch size * hidden dimension size.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.2" class="ltx_p">The GRU processes the image as <math id="S3.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="1600" display="inline"><semantics id="S3.SS2.SSS1.p2.1.m1.1a"><mn id="S3.SS2.SSS1.p2.1.m1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.cmml">1600</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.1b"><cn type="integer" id="S3.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1">1600</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.1c">1600</annotation></semantics></math> “timesteps”, each timestep corresponding to a column with <math id="S3.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S3.SS2.SSS1.p2.2.m2.1a"><mn id="S3.SS2.SSS1.p2.2.m2.1.1" xref="S3.SS2.SSS1.p2.2.m2.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.2.m2.1b"><cn type="integer" id="S3.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.2.m2.1c">512</annotation></semantics></math> input pixel values. At each timestep, the GRU has the information about all the columns to the left and the right (if any) of the current column, as well as the pixel values contained within the current column being evaluated. Using this information, the GRU can learn to identify the gap between the columns as those columns containing mostly white pixels and having two column regions on their left and right sides.</p>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.4" class="ltx_p">The GRU outputs a tensor of shape <math id="S3.SS2.SSS1.p3.1.m1.1" class="ltx_Math" alttext="1600\times 512" display="inline"><semantics id="S3.SS2.SSS1.p3.1.m1.1a"><mrow id="S3.SS2.SSS1.p3.1.m1.1.1" xref="S3.SS2.SSS1.p3.1.m1.1.1.cmml"><mn id="S3.SS2.SSS1.p3.1.m1.1.1.2" xref="S3.SS2.SSS1.p3.1.m1.1.1.2.cmml">1600</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p3.1.m1.1.1.1" xref="S3.SS2.SSS1.p3.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS1.p3.1.m1.1.1.3" xref="S3.SS2.SSS1.p3.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.1.m1.1b"><apply id="S3.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1"><times id="S3.SS2.SSS1.p3.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS1.p3.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.2">1600</cn><cn type="integer" id="S3.SS2.SSS1.p3.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.1.m1.1c">1600\times 512</annotation></semantics></math> corresponding to sequence length times hidden dimension. This tensor is then passed through a fully connected layer which outputs a <math id="S3.SS2.SSS1.p3.2.m2.1" class="ltx_Math" alttext="1600\times 2" display="inline"><semantics id="S3.SS2.SSS1.p3.2.m2.1a"><mrow id="S3.SS2.SSS1.p3.2.m2.1.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.cmml"><mn id="S3.SS2.SSS1.p3.2.m2.1.1.2" xref="S3.SS2.SSS1.p3.2.m2.1.1.2.cmml">1600</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p3.2.m2.1.1.1" xref="S3.SS2.SSS1.p3.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS1.p3.2.m2.1.1.3" xref="S3.SS2.SSS1.p3.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.2.m2.1b"><apply id="S3.SS2.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1"><times id="S3.SS2.SSS1.p3.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.1"></times><cn type="integer" id="S3.SS2.SSS1.p3.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.2">1600</cn><cn type="integer" id="S3.SS2.SSS1.p3.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.2.m2.1c">1600\times 2</annotation></semantics></math> shaped tensor. This output is finally passed through a softmax layer which gives the final output of shape <math id="S3.SS2.SSS1.p3.3.m3.1" class="ltx_Math" alttext="1600\times 2" display="inline"><semantics id="S3.SS2.SSS1.p3.3.m3.1a"><mrow id="S3.SS2.SSS1.p3.3.m3.1.1" xref="S3.SS2.SSS1.p3.3.m3.1.1.cmml"><mn id="S3.SS2.SSS1.p3.3.m3.1.1.2" xref="S3.SS2.SSS1.p3.3.m3.1.1.2.cmml">1600</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.p3.3.m3.1.1.1" xref="S3.SS2.SSS1.p3.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS1.p3.3.m3.1.1.3" xref="S3.SS2.SSS1.p3.3.m3.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.3.m3.1b"><apply id="S3.SS2.SSS1.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1"><times id="S3.SS2.SSS1.p3.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1.1"></times><cn type="integer" id="S3.SS2.SSS1.p3.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1.2">1600</cn><cn type="integer" id="S3.SS2.SSS1.p3.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p3.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.3.m3.1c">1600\times 2</annotation></semantics></math>, consisting of binary class probabilities for each of the <math id="S3.SS2.SSS1.p3.4.m4.1" class="ltx_Math" alttext="1600" display="inline"><semantics id="S3.SS2.SSS1.p3.4.m4.1a"><mn id="S3.SS2.SSS1.p3.4.m4.1.1" xref="S3.SS2.SSS1.p3.4.m4.1.1.cmml">1600</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.4.m4.1b"><cn type="integer" id="S3.SS2.SSS1.p3.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p3.4.m4.1.1">1600</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.4.m4.1c">1600</annotation></semantics></math> columns.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.4.1.1" class="ltx_text">III-B</span>2 </span>Row Classification</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.5" class="ltx_p">The neural architecture for row detection is a transpose of the column classifier and it classifies each row of the image as either a row or a whitespace between two rows. The images are fed one at a time and each image is considered to be a batch. The pre-processed input image of dimension <math id="S3.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="1600\times 512" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><mrow id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS2.p1.1.m1.1.1.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml">1600</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p1.1.m1.1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS2.p1.1.m1.1.1.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><apply id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1"><times id="S3.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.2">1600</cn><cn type="integer" id="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">1600\times 512</annotation></semantics></math> within a single batch is split into <math id="S3.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S3.SS2.SSS2.p1.2.m2.1a"><mn id="S3.SS2.SSS2.p1.2.m2.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.2.m2.1b"><cn type="integer" id="S3.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.2.m2.1c">512</annotation></semantics></math> sequences (rows), each consisting of <math id="S3.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="1600" display="inline"><semantics id="S3.SS2.SSS2.p1.3.m3.1a"><mn id="S3.SS2.SSS2.p1.3.m3.1.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.cmml">1600</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.3.m3.1b"><cn type="integer" id="S3.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1">1600</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.3.m3.1c">1600</annotation></semantics></math> pixel values. We have used a hidden dimension of size <math id="S3.SS2.SSS2.p1.4.m4.1" class="ltx_Math" alttext="1024" display="inline"><semantics id="S3.SS2.SSS2.p1.4.m4.1a"><mn id="S3.SS2.SSS2.p1.4.m4.1.1" xref="S3.SS2.SSS2.p1.4.m4.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.4.m4.1b"><cn type="integer" id="S3.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.4.m4.1c">1024</annotation></semantics></math>. The 2-layer GRU is initialized with hidden dimensions <math id="S3.SS2.SSS2.p1.5.m5.1" class="ltx_Math" alttext="(4\times 1\times 1024)" display="inline"><semantics id="S3.SS2.SSS2.p1.5.m5.1a"><mrow id="S3.SS2.SSS2.p1.5.m5.1.1.1" xref="S3.SS2.SSS2.p1.5.m5.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p1.5.m5.1.1.1.2" xref="S3.SS2.SSS2.p1.5.m5.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS2.p1.5.m5.1.1.1.1" xref="S3.SS2.SSS2.p1.5.m5.1.1.1.1.cmml"><mn id="S3.SS2.SSS2.p1.5.m5.1.1.1.1.2" xref="S3.SS2.SSS2.p1.5.m5.1.1.1.1.2.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p1.5.m5.1.1.1.1.1" xref="S3.SS2.SSS2.p1.5.m5.1.1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS2.p1.5.m5.1.1.1.1.3" xref="S3.SS2.SSS2.p1.5.m5.1.1.1.1.3.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p1.5.m5.1.1.1.1.1a" xref="S3.SS2.SSS2.p1.5.m5.1.1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS2.p1.5.m5.1.1.1.1.4" xref="S3.SS2.SSS2.p1.5.m5.1.1.1.1.4.cmml">1024</mn></mrow><mo stretchy="false" id="S3.SS2.SSS2.p1.5.m5.1.1.1.3" xref="S3.SS2.SSS2.p1.5.m5.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.5.m5.1b"><apply id="S3.SS2.SSS2.p1.5.m5.1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.5.m5.1.1.1"><times id="S3.SS2.SSS2.p1.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.5.m5.1.1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS2.p1.5.m5.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.5.m5.1.1.1.1.2">4</cn><cn type="integer" id="S3.SS2.SSS2.p1.5.m5.1.1.1.1.3.cmml" xref="S3.SS2.SSS2.p1.5.m5.1.1.1.1.3">1</cn><cn type="integer" id="S3.SS2.SSS2.p1.5.m5.1.1.1.1.4.cmml" xref="S3.SS2.SSS2.p1.5.m5.1.1.1.1.4">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.5.m5.1c">(4\times 1\times 1024)</annotation></semantics></math> corresponding to 2 * number of (layers * batch size * hidden dimension size).</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.2" class="ltx_p">In the case of row classification, there are <math id="S3.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><mn id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><cn type="integer" id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">512</annotation></semantics></math> timesteps with <math id="S3.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="1600" display="inline"><semantics id="S3.SS2.SSS2.p2.2.m2.1a"><mn id="S3.SS2.SSS2.p2.2.m2.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.cmml">1600</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.2.m2.1b"><cn type="integer" id="S3.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1">1600</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.2.m2.1c">1600</annotation></semantics></math> inputs each. At each timestep, the GRU has information about all the rows above and below the current row as well as the pixel values within the current row.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.4" class="ltx_p">The GRU outputs a tensor of shape <math id="S3.SS2.SSS2.p3.1.m1.1" class="ltx_Math" alttext="512\times 1600" display="inline"><semantics id="S3.SS2.SSS2.p3.1.m1.1a"><mrow id="S3.SS2.SSS2.p3.1.m1.1.1" xref="S3.SS2.SSS2.p3.1.m1.1.1.cmml"><mn id="S3.SS2.SSS2.p3.1.m1.1.1.2" xref="S3.SS2.SSS2.p3.1.m1.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p3.1.m1.1.1.1" xref="S3.SS2.SSS2.p3.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS2.p3.1.m1.1.1.3" xref="S3.SS2.SSS2.p3.1.m1.1.1.3.cmml">1600</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.1.m1.1b"><apply id="S3.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1"><times id="S3.SS2.SSS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.2">512</cn><cn type="integer" id="S3.SS2.SSS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1.3">1600</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.1.m1.1c">512\times 1600</annotation></semantics></math> corresponding to sequence length x hidden dimension. This tensor is then passed through a fully connected layer which outputs a <math id="S3.SS2.SSS2.p3.2.m2.1" class="ltx_Math" alttext="512\times 2" display="inline"><semantics id="S3.SS2.SSS2.p3.2.m2.1a"><mrow id="S3.SS2.SSS2.p3.2.m2.1.1" xref="S3.SS2.SSS2.p3.2.m2.1.1.cmml"><mn id="S3.SS2.SSS2.p3.2.m2.1.1.2" xref="S3.SS2.SSS2.p3.2.m2.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p3.2.m2.1.1.1" xref="S3.SS2.SSS2.p3.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS2.p3.2.m2.1.1.3" xref="S3.SS2.SSS2.p3.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.2.m2.1b"><apply id="S3.SS2.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1"><times id="S3.SS2.SSS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.1"></times><cn type="integer" id="S3.SS2.SSS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.2">512</cn><cn type="integer" id="S3.SS2.SSS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.2.m2.1c">512\times 2</annotation></semantics></math> shaped tensor. This output is finally passed through a softmax layer which gives an output of shape <math id="S3.SS2.SSS2.p3.3.m3.1" class="ltx_Math" alttext="512\times 2" display="inline"><semantics id="S3.SS2.SSS2.p3.3.m3.1a"><mrow id="S3.SS2.SSS2.p3.3.m3.1.1" xref="S3.SS2.SSS2.p3.3.m3.1.1.cmml"><mn id="S3.SS2.SSS2.p3.3.m3.1.1.2" xref="S3.SS2.SSS2.p3.3.m3.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.p3.3.m3.1.1.1" xref="S3.SS2.SSS2.p3.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS2.p3.3.m3.1.1.3" xref="S3.SS2.SSS2.p3.3.m3.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.3.m3.1b"><apply id="S3.SS2.SSS2.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1"><times id="S3.SS2.SSS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.1"></times><cn type="integer" id="S3.SS2.SSS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.2">512</cn><cn type="integer" id="S3.SS2.SSS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.3.m3.1c">512\times 2</annotation></semantics></math>, consisting of binary class probabilities for each of the <math id="S3.SS2.SSS2.p3.4.m4.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S3.SS2.SSS2.p3.4.m4.1a"><mn id="S3.SS2.SSS2.p3.4.m4.1.1" xref="S3.SS2.SSS2.p3.4.m4.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.4.m4.1b"><cn type="integer" id="S3.SS2.SSS2.p3.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.4.m4.1c">512</annotation></semantics></math> rows.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p id="S3.SS2.SSS2.p4.1" class="ltx_p">The last step in the classification is parsing the segmentation space predicted by the classifier. We take the midpoint of the segmentation space and applying the logic to drop the leftmost and the rightmost predictions in the case of columns and the top and the bottom predictions in the case of rows. This step regresses the output to a single line prediction of rows and column separators.</p>
</div>
<div id="S3.SS2.SSS2.p5" class="ltx_para">
<p id="S3.SS2.SSS2.p5.1" class="ltx_p">The complete model architectures for row and column classification are shown in Figure <a href="#S3.F2" title="Figure 2 ‣ III-B Model ‣ III Proposed Methodology ‣ Table Structure Extraction with Bi-directional Gated Recurrent Unit Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S3.F1" title="Figure 1 ‣ III-A Image Pre-processing ‣ III Proposed Methodology ‣ Table Structure Extraction with Bi-directional Gated Recurrent Unit Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Training</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We used Adam optimizer paired with binary cross entropy loss function to train our models. A typical table image contains more rows and columns than the whitespace between them. Initial attempts at training resulted in a model that always predicted a row-column element and failed to detect the whitespace due to this class imbalance problem. So, we took measures to reduce this class imbalance problem and applied weighting to our loss function to penalize an incorrectly predicted row-column element only <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="66\%" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mn id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">66</mn><mo id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">66</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">66\%</annotation></semantics></math> as much as an incorrectly predicted whitespace element.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.5" class="ltx_p">The dataset used for training consisted of freely available document images downloaded from various sources. The tables, rows and columns were manually labelled using custom tools. With a fixed learning rate of <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="0.0005" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mn id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">0.0005</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><cn type="float" id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">0.0005</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">0.0005</annotation></semantics></math>, we trained the column classifier for <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mn id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><cn type="integer" id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">10</annotation></semantics></math> iterations over <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="323" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mn id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">323</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><cn type="integer" id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">323</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">323</annotation></semantics></math> images and the row classifier for <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="35" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mn id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">35</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><cn type="integer" id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">35</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">35</annotation></semantics></math> iterations for <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="286" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mn id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">286</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><cn type="integer" id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">286</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">286</annotation></semantics></math> images.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Performance Measures</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Various researchers have used different evaluation metrics ranging from simple precision and recall to more complex evaluation algorithms. In this paper, we have used the performance evaluation algorithm described in Shahab et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> to evaluate the performance of our model for two main reasons: i) This metric paints the detailed picture of how the algorithm performs using six different measures. ii) It is a general purpose metric that can be applied to any type of segments such as tables, rows, columns and cells.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.10" class="ltx_p">The methodology proposed by Shahab et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, starts with numbering the ground truth segments and the detected segments. A correspondence matrix is then created with <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">m</annotation></semantics></math> rows and <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.p2.2.m2.1a"><mi id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><ci id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">n</annotation></semantics></math> columns where <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S4.p2.3.m3.1a"><mi id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><ci id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">m</annotation></semantics></math> is the number of ground truth segments and <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.p2.4.m4.1a"><mi id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><ci id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">n</annotation></semantics></math> is the number of detected segments in an image. The [G<sub id="S4.p2.10.1" class="ltx_sub">i</sub>,S<sub id="S4.p2.10.2" class="ltx_sub">j</sub>] entry in the matrix represents the number of pixels in the <math id="S4.p2.5.m5.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S4.p2.5.m5.1a"><msup id="S4.p2.5.m5.1.1" xref="S4.p2.5.m5.1.1.cmml"><mi id="S4.p2.5.m5.1.1.2" xref="S4.p2.5.m5.1.1.2.cmml">i</mi><mrow id="S4.p2.5.m5.1.1.3" xref="S4.p2.5.m5.1.1.3.cmml"><mi id="S4.p2.5.m5.1.1.3.2" xref="S4.p2.5.m5.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p2.5.m5.1.1.3.1" xref="S4.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="S4.p2.5.m5.1.1.3.3" xref="S4.p2.5.m5.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p2.5.m5.1b"><apply id="S4.p2.5.m5.1.1.cmml" xref="S4.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.p2.5.m5.1.1.1.cmml" xref="S4.p2.5.m5.1.1">superscript</csymbol><ci id="S4.p2.5.m5.1.1.2.cmml" xref="S4.p2.5.m5.1.1.2">𝑖</ci><apply id="S4.p2.5.m5.1.1.3.cmml" xref="S4.p2.5.m5.1.1.3"><times id="S4.p2.5.m5.1.1.3.1.cmml" xref="S4.p2.5.m5.1.1.3.1"></times><ci id="S4.p2.5.m5.1.1.3.2.cmml" xref="S4.p2.5.m5.1.1.3.2">𝑡</ci><ci id="S4.p2.5.m5.1.1.3.3.cmml" xref="S4.p2.5.m5.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m5.1c">i^{th}</annotation></semantics></math> ground truth segment that overlap with the <math id="S4.p2.6.m6.1" class="ltx_Math" alttext="j^{th}" display="inline"><semantics id="S4.p2.6.m6.1a"><msup id="S4.p2.6.m6.1.1" xref="S4.p2.6.m6.1.1.cmml"><mi id="S4.p2.6.m6.1.1.2" xref="S4.p2.6.m6.1.1.2.cmml">j</mi><mrow id="S4.p2.6.m6.1.1.3" xref="S4.p2.6.m6.1.1.3.cmml"><mi id="S4.p2.6.m6.1.1.3.2" xref="S4.p2.6.m6.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p2.6.m6.1.1.3.1" xref="S4.p2.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.p2.6.m6.1.1.3.3" xref="S4.p2.6.m6.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p2.6.m6.1b"><apply id="S4.p2.6.m6.1.1.cmml" xref="S4.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.p2.6.m6.1.1.1.cmml" xref="S4.p2.6.m6.1.1">superscript</csymbol><ci id="S4.p2.6.m6.1.1.2.cmml" xref="S4.p2.6.m6.1.1.2">𝑗</ci><apply id="S4.p2.6.m6.1.1.3.cmml" xref="S4.p2.6.m6.1.1.3"><times id="S4.p2.6.m6.1.1.3.1.cmml" xref="S4.p2.6.m6.1.1.3.1"></times><ci id="S4.p2.6.m6.1.1.3.2.cmml" xref="S4.p2.6.m6.1.1.3.2">𝑡</ci><ci id="S4.p2.6.m6.1.1.3.3.cmml" xref="S4.p2.6.m6.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.6.m6.1c">j^{th}</annotation></semantics></math> detected segment. <math id="S4.p2.7.m7.1" class="ltx_Math" alttext="\lvert G_{i}\rvert" display="inline"><semantics id="S4.p2.7.m7.1a"><mrow id="S4.p2.7.m7.1.1.1" xref="S4.p2.7.m7.1.1.2.cmml"><mo stretchy="false" id="S4.p2.7.m7.1.1.1.2" xref="S4.p2.7.m7.1.1.2.1.cmml">|</mo><msub id="S4.p2.7.m7.1.1.1.1" xref="S4.p2.7.m7.1.1.1.1.cmml"><mi id="S4.p2.7.m7.1.1.1.1.2" xref="S4.p2.7.m7.1.1.1.1.2.cmml">G</mi><mi id="S4.p2.7.m7.1.1.1.1.3" xref="S4.p2.7.m7.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.p2.7.m7.1.1.1.3" xref="S4.p2.7.m7.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.7.m7.1b"><apply id="S4.p2.7.m7.1.1.2.cmml" xref="S4.p2.7.m7.1.1.1"><abs id="S4.p2.7.m7.1.1.2.1.cmml" xref="S4.p2.7.m7.1.1.1.2"></abs><apply id="S4.p2.7.m7.1.1.1.1.cmml" xref="S4.p2.7.m7.1.1.1.1"><csymbol cd="ambiguous" id="S4.p2.7.m7.1.1.1.1.1.cmml" xref="S4.p2.7.m7.1.1.1.1">subscript</csymbol><ci id="S4.p2.7.m7.1.1.1.1.2.cmml" xref="S4.p2.7.m7.1.1.1.1.2">𝐺</ci><ci id="S4.p2.7.m7.1.1.1.1.3.cmml" xref="S4.p2.7.m7.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.7.m7.1c">\lvert G_{i}\rvert</annotation></semantics></math> represents the total number of pixels in the <math id="S4.p2.8.m8.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S4.p2.8.m8.1a"><msup id="S4.p2.8.m8.1.1" xref="S4.p2.8.m8.1.1.cmml"><mi id="S4.p2.8.m8.1.1.2" xref="S4.p2.8.m8.1.1.2.cmml">i</mi><mrow id="S4.p2.8.m8.1.1.3" xref="S4.p2.8.m8.1.1.3.cmml"><mi id="S4.p2.8.m8.1.1.3.2" xref="S4.p2.8.m8.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p2.8.m8.1.1.3.1" xref="S4.p2.8.m8.1.1.3.1.cmml">​</mo><mi id="S4.p2.8.m8.1.1.3.3" xref="S4.p2.8.m8.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p2.8.m8.1b"><apply id="S4.p2.8.m8.1.1.cmml" xref="S4.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S4.p2.8.m8.1.1.1.cmml" xref="S4.p2.8.m8.1.1">superscript</csymbol><ci id="S4.p2.8.m8.1.1.2.cmml" xref="S4.p2.8.m8.1.1.2">𝑖</ci><apply id="S4.p2.8.m8.1.1.3.cmml" xref="S4.p2.8.m8.1.1.3"><times id="S4.p2.8.m8.1.1.3.1.cmml" xref="S4.p2.8.m8.1.1.3.1"></times><ci id="S4.p2.8.m8.1.1.3.2.cmml" xref="S4.p2.8.m8.1.1.3.2">𝑡</ci><ci id="S4.p2.8.m8.1.1.3.3.cmml" xref="S4.p2.8.m8.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.8.m8.1c">i^{th}</annotation></semantics></math> ground truth segment and <math id="S4.p2.9.m9.1" class="ltx_Math" alttext="\lvert{S_{j}}\rvert" display="inline"><semantics id="S4.p2.9.m9.1a"><mrow id="S4.p2.9.m9.1.1.1" xref="S4.p2.9.m9.1.1.2.cmml"><mo stretchy="false" id="S4.p2.9.m9.1.1.1.2" xref="S4.p2.9.m9.1.1.2.1.cmml">|</mo><msub id="S4.p2.9.m9.1.1.1.1" xref="S4.p2.9.m9.1.1.1.1.cmml"><mi id="S4.p2.9.m9.1.1.1.1.2" xref="S4.p2.9.m9.1.1.1.1.2.cmml">S</mi><mi id="S4.p2.9.m9.1.1.1.1.3" xref="S4.p2.9.m9.1.1.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S4.p2.9.m9.1.1.1.3" xref="S4.p2.9.m9.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.9.m9.1b"><apply id="S4.p2.9.m9.1.1.2.cmml" xref="S4.p2.9.m9.1.1.1"><abs id="S4.p2.9.m9.1.1.2.1.cmml" xref="S4.p2.9.m9.1.1.1.2"></abs><apply id="S4.p2.9.m9.1.1.1.1.cmml" xref="S4.p2.9.m9.1.1.1.1"><csymbol cd="ambiguous" id="S4.p2.9.m9.1.1.1.1.1.cmml" xref="S4.p2.9.m9.1.1.1.1">subscript</csymbol><ci id="S4.p2.9.m9.1.1.1.1.2.cmml" xref="S4.p2.9.m9.1.1.1.1.2">𝑆</ci><ci id="S4.p2.9.m9.1.1.1.1.3.cmml" xref="S4.p2.9.m9.1.1.1.1.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.9.m9.1c">\lvert{S_{j}}\rvert</annotation></semantics></math> represents the total number of pixels in the <math id="S4.p2.10.m10.1" class="ltx_Math" alttext="j^{th}" display="inline"><semantics id="S4.p2.10.m10.1a"><msup id="S4.p2.10.m10.1.1" xref="S4.p2.10.m10.1.1.cmml"><mi id="S4.p2.10.m10.1.1.2" xref="S4.p2.10.m10.1.1.2.cmml">j</mi><mrow id="S4.p2.10.m10.1.1.3" xref="S4.p2.10.m10.1.1.3.cmml"><mi id="S4.p2.10.m10.1.1.3.2" xref="S4.p2.10.m10.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p2.10.m10.1.1.3.1" xref="S4.p2.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.p2.10.m10.1.1.3.3" xref="S4.p2.10.m10.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p2.10.m10.1b"><apply id="S4.p2.10.m10.1.1.cmml" xref="S4.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S4.p2.10.m10.1.1.1.cmml" xref="S4.p2.10.m10.1.1">superscript</csymbol><ci id="S4.p2.10.m10.1.1.2.cmml" xref="S4.p2.10.m10.1.1.2">𝑗</ci><apply id="S4.p2.10.m10.1.1.3.cmml" xref="S4.p2.10.m10.1.1.3"><times id="S4.p2.10.m10.1.1.3.1.cmml" xref="S4.p2.10.m10.1.1.3.1"></times><ci id="S4.p2.10.m10.1.1.3.2.cmml" xref="S4.p2.10.m10.1.1.3.2">𝑡</ci><ci id="S4.p2.10.m10.1.1.3.3.cmml" xref="S4.p2.10.m10.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.10.m10.1c">j^{th}</annotation></semantics></math> detection. Once the correspondence matrix for an image has been created, we can define the following measures:</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2001.02501/assets/images/output.jpeg" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="448" height="294" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Results of our proposed table structure extraction approach on a few images from UNLV dataset showing examples of correct, over-segmented and under-segmented detections</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.7.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.8.2" class="ltx_text ltx_font_italic">Correct Detections</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p">This measure shows the total number of ground truth segments that have a large intersection with a detected segment and the detected segment does not have a significant overlap with any other ground truth segment. That is, for a detected segment <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="S_{j}" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><msub id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">S</mi><mi id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">𝑆</ci><ci id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">S_{j}</annotation></semantics></math> and ground truth segments <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="G_{i}" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><msub id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">G</mi><mi id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">𝐺</ci><ci id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">G_{i}</annotation></semantics></math>:</p>
</div>
<div id="S4.SS1.3" class="ltx_para">
<p id="S4.SS1.3.3" class="ltx_p ltx_align_center"><math id="S4.SS1.1.1.m1.2" class="ltx_Math" alttext="\frac{\lvert G_{i}\cap S_{j}\rvert}{\lvert G_{i}\rvert}&gt;0.9" display="inline"><semantics id="S4.SS1.1.1.m1.2a"><mrow id="S4.SS1.1.1.m1.2.3" xref="S4.SS1.1.1.m1.2.3.cmml"><mfrac id="S4.SS1.1.1.m1.2.2" xref="S4.SS1.1.1.m1.2.2.cmml"><mrow id="S4.SS1.1.1.m1.1.1.1.1" xref="S4.SS1.1.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS1.1.1.m1.1.1.1.1.2" xref="S4.SS1.1.1.m1.1.1.1.2.1.cmml">|</mo><mrow id="S4.SS1.1.1.m1.1.1.1.1.1" xref="S4.SS1.1.1.m1.1.1.1.1.1.cmml"><msub id="S4.SS1.1.1.m1.1.1.1.1.1.2" xref="S4.SS1.1.1.m1.1.1.1.1.1.2.cmml"><mi id="S4.SS1.1.1.m1.1.1.1.1.1.2.2" xref="S4.SS1.1.1.m1.1.1.1.1.1.2.2.cmml">G</mi><mi id="S4.SS1.1.1.m1.1.1.1.1.1.2.3" xref="S4.SS1.1.1.m1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS1.1.1.m1.1.1.1.1.1.1" xref="S4.SS1.1.1.m1.1.1.1.1.1.1.cmml">∩</mo><msub id="S4.SS1.1.1.m1.1.1.1.1.1.3" xref="S4.SS1.1.1.m1.1.1.1.1.1.3.cmml"><mi id="S4.SS1.1.1.m1.1.1.1.1.1.3.2" xref="S4.SS1.1.1.m1.1.1.1.1.1.3.2.cmml">S</mi><mi id="S4.SS1.1.1.m1.1.1.1.1.1.3.3" xref="S4.SS1.1.1.m1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo stretchy="false" id="S4.SS1.1.1.m1.1.1.1.1.3" xref="S4.SS1.1.1.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S4.SS1.1.1.m1.2.2.2.1" xref="S4.SS1.1.1.m1.2.2.2.2.cmml"><mo stretchy="false" id="S4.SS1.1.1.m1.2.2.2.1.2" xref="S4.SS1.1.1.m1.2.2.2.2.1.cmml">|</mo><msub id="S4.SS1.1.1.m1.2.2.2.1.1" xref="S4.SS1.1.1.m1.2.2.2.1.1.cmml"><mi id="S4.SS1.1.1.m1.2.2.2.1.1.2" xref="S4.SS1.1.1.m1.2.2.2.1.1.2.cmml">G</mi><mi id="S4.SS1.1.1.m1.2.2.2.1.1.3" xref="S4.SS1.1.1.m1.2.2.2.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS1.1.1.m1.2.2.2.1.3" xref="S4.SS1.1.1.m1.2.2.2.2.1.cmml">|</mo></mrow></mfrac><mo id="S4.SS1.1.1.m1.2.3.1" xref="S4.SS1.1.1.m1.2.3.1.cmml">&gt;</mo><mn id="S4.SS1.1.1.m1.2.3.2" xref="S4.SS1.1.1.m1.2.3.2.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.1.1.m1.2b"><apply id="S4.SS1.1.1.m1.2.3.cmml" xref="S4.SS1.1.1.m1.2.3"><gt id="S4.SS1.1.1.m1.2.3.1.cmml" xref="S4.SS1.1.1.m1.2.3.1"></gt><apply id="S4.SS1.1.1.m1.2.2.cmml" xref="S4.SS1.1.1.m1.2.2"><divide id="S4.SS1.1.1.m1.2.2.3.cmml" xref="S4.SS1.1.1.m1.2.2"></divide><apply id="S4.SS1.1.1.m1.1.1.1.2.cmml" xref="S4.SS1.1.1.m1.1.1.1.1"><abs id="S4.SS1.1.1.m1.1.1.1.2.1.cmml" xref="S4.SS1.1.1.m1.1.1.1.1.2"></abs><apply id="S4.SS1.1.1.m1.1.1.1.1.1.cmml" xref="S4.SS1.1.1.m1.1.1.1.1.1"><intersect id="S4.SS1.1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS1.1.1.m1.1.1.1.1.1.1"></intersect><apply id="S4.SS1.1.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS1.1.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.1.1.m1.1.1.1.1.1.2.1.cmml" xref="S4.SS1.1.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS1.1.1.m1.1.1.1.1.1.2.2.cmml" xref="S4.SS1.1.1.m1.1.1.1.1.1.2.2">𝐺</ci><ci id="S4.SS1.1.1.m1.1.1.1.1.1.2.3.cmml" xref="S4.SS1.1.1.m1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.SS1.1.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS1.1.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.1.1.m1.1.1.1.1.1.3.1.cmml" xref="S4.SS1.1.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.SS1.1.1.m1.1.1.1.1.1.3.2.cmml" xref="S4.SS1.1.1.m1.1.1.1.1.1.3.2">𝑆</ci><ci id="S4.SS1.1.1.m1.1.1.1.1.1.3.3.cmml" xref="S4.SS1.1.1.m1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply><apply id="S4.SS1.1.1.m1.2.2.2.2.cmml" xref="S4.SS1.1.1.m1.2.2.2.1"><abs id="S4.SS1.1.1.m1.2.2.2.2.1.cmml" xref="S4.SS1.1.1.m1.2.2.2.1.2"></abs><apply id="S4.SS1.1.1.m1.2.2.2.1.1.cmml" xref="S4.SS1.1.1.m1.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS1.1.1.m1.2.2.2.1.1.1.cmml" xref="S4.SS1.1.1.m1.2.2.2.1.1">subscript</csymbol><ci id="S4.SS1.1.1.m1.2.2.2.1.1.2.cmml" xref="S4.SS1.1.1.m1.2.2.2.1.1.2">𝐺</ci><ci id="S4.SS1.1.1.m1.2.2.2.1.1.3.cmml" xref="S4.SS1.1.1.m1.2.2.2.1.1.3">𝑖</ci></apply></apply></apply><cn type="float" id="S4.SS1.1.1.m1.2.3.2.cmml" xref="S4.SS1.1.1.m1.2.3.2">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.1.1.m1.2c">\frac{\lvert G_{i}\cap S_{j}\rvert}{\lvert G_{i}\rvert}&gt;0.9</annotation></semantics></math> and <math id="S4.SS1.2.2.m2.2" class="ltx_Math" alttext="\frac{\lvert G_{k}\cap S_{j}\rvert}{\lvert S_{j}\rvert}&lt;0.1" display="inline"><semantics id="S4.SS1.2.2.m2.2a"><mrow id="S4.SS1.2.2.m2.2.3" xref="S4.SS1.2.2.m2.2.3.cmml"><mfrac id="S4.SS1.2.2.m2.2.2" xref="S4.SS1.2.2.m2.2.2.cmml"><mrow id="S4.SS1.2.2.m2.1.1.1.1" xref="S4.SS1.2.2.m2.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS1.2.2.m2.1.1.1.1.2" xref="S4.SS1.2.2.m2.1.1.1.2.1.cmml">|</mo><mrow id="S4.SS1.2.2.m2.1.1.1.1.1" xref="S4.SS1.2.2.m2.1.1.1.1.1.cmml"><msub id="S4.SS1.2.2.m2.1.1.1.1.1.2" xref="S4.SS1.2.2.m2.1.1.1.1.1.2.cmml"><mi id="S4.SS1.2.2.m2.1.1.1.1.1.2.2" xref="S4.SS1.2.2.m2.1.1.1.1.1.2.2.cmml">G</mi><mi id="S4.SS1.2.2.m2.1.1.1.1.1.2.3" xref="S4.SS1.2.2.m2.1.1.1.1.1.2.3.cmml">k</mi></msub><mo id="S4.SS1.2.2.m2.1.1.1.1.1.1" xref="S4.SS1.2.2.m2.1.1.1.1.1.1.cmml">∩</mo><msub id="S4.SS1.2.2.m2.1.1.1.1.1.3" xref="S4.SS1.2.2.m2.1.1.1.1.1.3.cmml"><mi id="S4.SS1.2.2.m2.1.1.1.1.1.3.2" xref="S4.SS1.2.2.m2.1.1.1.1.1.3.2.cmml">S</mi><mi id="S4.SS1.2.2.m2.1.1.1.1.1.3.3" xref="S4.SS1.2.2.m2.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo stretchy="false" id="S4.SS1.2.2.m2.1.1.1.1.3" xref="S4.SS1.2.2.m2.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S4.SS1.2.2.m2.2.2.2.1" xref="S4.SS1.2.2.m2.2.2.2.2.cmml"><mo stretchy="false" id="S4.SS1.2.2.m2.2.2.2.1.2" xref="S4.SS1.2.2.m2.2.2.2.2.1.cmml">|</mo><msub id="S4.SS1.2.2.m2.2.2.2.1.1" xref="S4.SS1.2.2.m2.2.2.2.1.1.cmml"><mi id="S4.SS1.2.2.m2.2.2.2.1.1.2" xref="S4.SS1.2.2.m2.2.2.2.1.1.2.cmml">S</mi><mi id="S4.SS1.2.2.m2.2.2.2.1.1.3" xref="S4.SS1.2.2.m2.2.2.2.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S4.SS1.2.2.m2.2.2.2.1.3" xref="S4.SS1.2.2.m2.2.2.2.2.1.cmml">|</mo></mrow></mfrac><mo id="S4.SS1.2.2.m2.2.3.1" xref="S4.SS1.2.2.m2.2.3.1.cmml">&lt;</mo><mn id="S4.SS1.2.2.m2.2.3.2" xref="S4.SS1.2.2.m2.2.3.2.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.2.2.m2.2b"><apply id="S4.SS1.2.2.m2.2.3.cmml" xref="S4.SS1.2.2.m2.2.3"><lt id="S4.SS1.2.2.m2.2.3.1.cmml" xref="S4.SS1.2.2.m2.2.3.1"></lt><apply id="S4.SS1.2.2.m2.2.2.cmml" xref="S4.SS1.2.2.m2.2.2"><divide id="S4.SS1.2.2.m2.2.2.3.cmml" xref="S4.SS1.2.2.m2.2.2"></divide><apply id="S4.SS1.2.2.m2.1.1.1.2.cmml" xref="S4.SS1.2.2.m2.1.1.1.1"><abs id="S4.SS1.2.2.m2.1.1.1.2.1.cmml" xref="S4.SS1.2.2.m2.1.1.1.1.2"></abs><apply id="S4.SS1.2.2.m2.1.1.1.1.1.cmml" xref="S4.SS1.2.2.m2.1.1.1.1.1"><intersect id="S4.SS1.2.2.m2.1.1.1.1.1.1.cmml" xref="S4.SS1.2.2.m2.1.1.1.1.1.1"></intersect><apply id="S4.SS1.2.2.m2.1.1.1.1.1.2.cmml" xref="S4.SS1.2.2.m2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.2.2.m2.1.1.1.1.1.2.1.cmml" xref="S4.SS1.2.2.m2.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS1.2.2.m2.1.1.1.1.1.2.2.cmml" xref="S4.SS1.2.2.m2.1.1.1.1.1.2.2">𝐺</ci><ci id="S4.SS1.2.2.m2.1.1.1.1.1.2.3.cmml" xref="S4.SS1.2.2.m2.1.1.1.1.1.2.3">𝑘</ci></apply><apply id="S4.SS1.2.2.m2.1.1.1.1.1.3.cmml" xref="S4.SS1.2.2.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.2.2.m2.1.1.1.1.1.3.1.cmml" xref="S4.SS1.2.2.m2.1.1.1.1.1.3">subscript</csymbol><ci id="S4.SS1.2.2.m2.1.1.1.1.1.3.2.cmml" xref="S4.SS1.2.2.m2.1.1.1.1.1.3.2">𝑆</ci><ci id="S4.SS1.2.2.m2.1.1.1.1.1.3.3.cmml" xref="S4.SS1.2.2.m2.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply><apply id="S4.SS1.2.2.m2.2.2.2.2.cmml" xref="S4.SS1.2.2.m2.2.2.2.1"><abs id="S4.SS1.2.2.m2.2.2.2.2.1.cmml" xref="S4.SS1.2.2.m2.2.2.2.1.2"></abs><apply id="S4.SS1.2.2.m2.2.2.2.1.1.cmml" xref="S4.SS1.2.2.m2.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS1.2.2.m2.2.2.2.1.1.1.cmml" xref="S4.SS1.2.2.m2.2.2.2.1.1">subscript</csymbol><ci id="S4.SS1.2.2.m2.2.2.2.1.1.2.cmml" xref="S4.SS1.2.2.m2.2.2.2.1.1.2">𝑆</ci><ci id="S4.SS1.2.2.m2.2.2.2.1.1.3.cmml" xref="S4.SS1.2.2.m2.2.2.2.1.1.3">𝑗</ci></apply></apply></apply><cn type="float" id="S4.SS1.2.2.m2.2.3.2.cmml" xref="S4.SS1.2.2.m2.2.3.2">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.2.2.m2.2c">\frac{\lvert G_{k}\cap S_{j}\rvert}{\lvert S_{j}\rvert}&lt;0.1</annotation></semantics></math>    <math id="S4.SS1.3.3.m3.1" class="ltx_Math" alttext="\forall k\neq i" display="inline"><semantics id="S4.SS1.3.3.m3.1a"><mrow id="S4.SS1.3.3.m3.1.1" xref="S4.SS1.3.3.m3.1.1.cmml"><mrow id="S4.SS1.3.3.m3.1.1.2" xref="S4.SS1.3.3.m3.1.1.2.cmml"><mo rspace="0.167em" id="S4.SS1.3.3.m3.1.1.2.1" xref="S4.SS1.3.3.m3.1.1.2.1.cmml">∀</mo><mi id="S4.SS1.3.3.m3.1.1.2.2" xref="S4.SS1.3.3.m3.1.1.2.2.cmml">k</mi></mrow><mo id="S4.SS1.3.3.m3.1.1.1" xref="S4.SS1.3.3.m3.1.1.1.cmml">≠</mo><mi id="S4.SS1.3.3.m3.1.1.3" xref="S4.SS1.3.3.m3.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.3.3.m3.1b"><apply id="S4.SS1.3.3.m3.1.1.cmml" xref="S4.SS1.3.3.m3.1.1"><neq id="S4.SS1.3.3.m3.1.1.1.cmml" xref="S4.SS1.3.3.m3.1.1.1"></neq><apply id="S4.SS1.3.3.m3.1.1.2.cmml" xref="S4.SS1.3.3.m3.1.1.2"><csymbol cd="latexml" id="S4.SS1.3.3.m3.1.1.2.1.cmml" xref="S4.SS1.3.3.m3.1.1.2.1">for-all</csymbol><ci id="S4.SS1.3.3.m3.1.1.2.2.cmml" xref="S4.SS1.3.3.m3.1.1.2.2">𝑘</ci></apply><ci id="S4.SS1.3.3.m3.1.1.3.cmml" xref="S4.SS1.3.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.3.3.m3.1c">\forall k\neq i</annotation></semantics></math></p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.7.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.8.2" class="ltx_text ltx_font_italic">Partial Detections</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The total number of ground truth segments that have a significant intersection with a single detected segment. However, the intersection is not large enough to be counted as a correct detection. That is,</p>
</div>
<div id="S4.SS2.3" class="ltx_para">
<p id="S4.SS2.3.3" class="ltx_p ltx_align_center"><math id="S4.SS2.1.1.m1.2" class="ltx_Math" alttext="0.1&lt;\frac{\lvert G_{i}\cap S_{j}\rvert}{\lvert G_{i}\rvert}&lt;0.9" display="inline"><semantics id="S4.SS2.1.1.m1.2a"><mrow id="S4.SS2.1.1.m1.2.3" xref="S4.SS2.1.1.m1.2.3.cmml"><mn id="S4.SS2.1.1.m1.2.3.2" xref="S4.SS2.1.1.m1.2.3.2.cmml">0.1</mn><mo id="S4.SS2.1.1.m1.2.3.3" xref="S4.SS2.1.1.m1.2.3.3.cmml">&lt;</mo><mfrac id="S4.SS2.1.1.m1.2.2" xref="S4.SS2.1.1.m1.2.2.cmml"><mrow id="S4.SS2.1.1.m1.1.1.1.1" xref="S4.SS2.1.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.1.1.m1.1.1.1.1.2" xref="S4.SS2.1.1.m1.1.1.1.2.1.cmml">|</mo><mrow id="S4.SS2.1.1.m1.1.1.1.1.1" xref="S4.SS2.1.1.m1.1.1.1.1.1.cmml"><msub id="S4.SS2.1.1.m1.1.1.1.1.1.2" xref="S4.SS2.1.1.m1.1.1.1.1.1.2.cmml"><mi id="S4.SS2.1.1.m1.1.1.1.1.1.2.2" xref="S4.SS2.1.1.m1.1.1.1.1.1.2.2.cmml">G</mi><mi id="S4.SS2.1.1.m1.1.1.1.1.1.2.3" xref="S4.SS2.1.1.m1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS2.1.1.m1.1.1.1.1.1.1" xref="S4.SS2.1.1.m1.1.1.1.1.1.1.cmml">∩</mo><msub id="S4.SS2.1.1.m1.1.1.1.1.1.3" xref="S4.SS2.1.1.m1.1.1.1.1.1.3.cmml"><mi id="S4.SS2.1.1.m1.1.1.1.1.1.3.2" xref="S4.SS2.1.1.m1.1.1.1.1.1.3.2.cmml">S</mi><mi id="S4.SS2.1.1.m1.1.1.1.1.1.3.3" xref="S4.SS2.1.1.m1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo stretchy="false" id="S4.SS2.1.1.m1.1.1.1.1.3" xref="S4.SS2.1.1.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S4.SS2.1.1.m1.2.2.2.1" xref="S4.SS2.1.1.m1.2.2.2.2.cmml"><mo stretchy="false" id="S4.SS2.1.1.m1.2.2.2.1.2" xref="S4.SS2.1.1.m1.2.2.2.2.1.cmml">|</mo><msub id="S4.SS2.1.1.m1.2.2.2.1.1" xref="S4.SS2.1.1.m1.2.2.2.1.1.cmml"><mi id="S4.SS2.1.1.m1.2.2.2.1.1.2" xref="S4.SS2.1.1.m1.2.2.2.1.1.2.cmml">G</mi><mi id="S4.SS2.1.1.m1.2.2.2.1.1.3" xref="S4.SS2.1.1.m1.2.2.2.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS2.1.1.m1.2.2.2.1.3" xref="S4.SS2.1.1.m1.2.2.2.2.1.cmml">|</mo></mrow></mfrac><mo id="S4.SS2.1.1.m1.2.3.4" xref="S4.SS2.1.1.m1.2.3.4.cmml">&lt;</mo><mn id="S4.SS2.1.1.m1.2.3.5" xref="S4.SS2.1.1.m1.2.3.5.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.1.1.m1.2b"><apply id="S4.SS2.1.1.m1.2.3.cmml" xref="S4.SS2.1.1.m1.2.3"><and id="S4.SS2.1.1.m1.2.3a.cmml" xref="S4.SS2.1.1.m1.2.3"></and><apply id="S4.SS2.1.1.m1.2.3b.cmml" xref="S4.SS2.1.1.m1.2.3"><lt id="S4.SS2.1.1.m1.2.3.3.cmml" xref="S4.SS2.1.1.m1.2.3.3"></lt><cn type="float" id="S4.SS2.1.1.m1.2.3.2.cmml" xref="S4.SS2.1.1.m1.2.3.2">0.1</cn><apply id="S4.SS2.1.1.m1.2.2.cmml" xref="S4.SS2.1.1.m1.2.2"><divide id="S4.SS2.1.1.m1.2.2.3.cmml" xref="S4.SS2.1.1.m1.2.2"></divide><apply id="S4.SS2.1.1.m1.1.1.1.2.cmml" xref="S4.SS2.1.1.m1.1.1.1.1"><abs id="S4.SS2.1.1.m1.1.1.1.2.1.cmml" xref="S4.SS2.1.1.m1.1.1.1.1.2"></abs><apply id="S4.SS2.1.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.1.1.m1.1.1.1.1.1"><intersect id="S4.SS2.1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS2.1.1.m1.1.1.1.1.1.1"></intersect><apply id="S4.SS2.1.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS2.1.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.1.1.m1.1.1.1.1.1.2.1.cmml" xref="S4.SS2.1.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS2.1.1.m1.1.1.1.1.1.2.2.cmml" xref="S4.SS2.1.1.m1.1.1.1.1.1.2.2">𝐺</ci><ci id="S4.SS2.1.1.m1.1.1.1.1.1.2.3.cmml" xref="S4.SS2.1.1.m1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.SS2.1.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS2.1.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.1.1.m1.1.1.1.1.1.3.1.cmml" xref="S4.SS2.1.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.SS2.1.1.m1.1.1.1.1.1.3.2.cmml" xref="S4.SS2.1.1.m1.1.1.1.1.1.3.2">𝑆</ci><ci id="S4.SS2.1.1.m1.1.1.1.1.1.3.3.cmml" xref="S4.SS2.1.1.m1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply><apply id="S4.SS2.1.1.m1.2.2.2.2.cmml" xref="S4.SS2.1.1.m1.2.2.2.1"><abs id="S4.SS2.1.1.m1.2.2.2.2.1.cmml" xref="S4.SS2.1.1.m1.2.2.2.1.2"></abs><apply id="S4.SS2.1.1.m1.2.2.2.1.1.cmml" xref="S4.SS2.1.1.m1.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS2.1.1.m1.2.2.2.1.1.1.cmml" xref="S4.SS2.1.1.m1.2.2.2.1.1">subscript</csymbol><ci id="S4.SS2.1.1.m1.2.2.2.1.1.2.cmml" xref="S4.SS2.1.1.m1.2.2.2.1.1.2">𝐺</ci><ci id="S4.SS2.1.1.m1.2.2.2.1.1.3.cmml" xref="S4.SS2.1.1.m1.2.2.2.1.1.3">𝑖</ci></apply></apply></apply></apply><apply id="S4.SS2.1.1.m1.2.3c.cmml" xref="S4.SS2.1.1.m1.2.3"><lt id="S4.SS2.1.1.m1.2.3.4.cmml" xref="S4.SS2.1.1.m1.2.3.4"></lt><share href="#S4.SS2.1.1.m1.2.2.cmml" id="S4.SS2.1.1.m1.2.3d.cmml" xref="S4.SS2.1.1.m1.2.3"></share><cn type="float" id="S4.SS2.1.1.m1.2.3.5.cmml" xref="S4.SS2.1.1.m1.2.3.5">0.9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.1.1.m1.2c">0.1&lt;\frac{\lvert G_{i}\cap S_{j}\rvert}{\lvert G_{i}\rvert}&lt;0.9</annotation></semantics></math> and <math id="S4.SS2.2.2.m2.2" class="ltx_Math" alttext="\frac{\lvert G_{i}\cap S_{k}\rvert}{\lvert G_{i}\rvert}&lt;0.1" display="inline"><semantics id="S4.SS2.2.2.m2.2a"><mrow id="S4.SS2.2.2.m2.2.3" xref="S4.SS2.2.2.m2.2.3.cmml"><mfrac id="S4.SS2.2.2.m2.2.2" xref="S4.SS2.2.2.m2.2.2.cmml"><mrow id="S4.SS2.2.2.m2.1.1.1.1" xref="S4.SS2.2.2.m2.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.2.2.m2.1.1.1.1.2" xref="S4.SS2.2.2.m2.1.1.1.2.1.cmml">|</mo><mrow id="S4.SS2.2.2.m2.1.1.1.1.1" xref="S4.SS2.2.2.m2.1.1.1.1.1.cmml"><msub id="S4.SS2.2.2.m2.1.1.1.1.1.2" xref="S4.SS2.2.2.m2.1.1.1.1.1.2.cmml"><mi id="S4.SS2.2.2.m2.1.1.1.1.1.2.2" xref="S4.SS2.2.2.m2.1.1.1.1.1.2.2.cmml">G</mi><mi id="S4.SS2.2.2.m2.1.1.1.1.1.2.3" xref="S4.SS2.2.2.m2.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS2.2.2.m2.1.1.1.1.1.1" xref="S4.SS2.2.2.m2.1.1.1.1.1.1.cmml">∩</mo><msub id="S4.SS2.2.2.m2.1.1.1.1.1.3" xref="S4.SS2.2.2.m2.1.1.1.1.1.3.cmml"><mi id="S4.SS2.2.2.m2.1.1.1.1.1.3.2" xref="S4.SS2.2.2.m2.1.1.1.1.1.3.2.cmml">S</mi><mi id="S4.SS2.2.2.m2.1.1.1.1.1.3.3" xref="S4.SS2.2.2.m2.1.1.1.1.1.3.3.cmml">k</mi></msub></mrow><mo stretchy="false" id="S4.SS2.2.2.m2.1.1.1.1.3" xref="S4.SS2.2.2.m2.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S4.SS2.2.2.m2.2.2.2.1" xref="S4.SS2.2.2.m2.2.2.2.2.cmml"><mo stretchy="false" id="S4.SS2.2.2.m2.2.2.2.1.2" xref="S4.SS2.2.2.m2.2.2.2.2.1.cmml">|</mo><msub id="S4.SS2.2.2.m2.2.2.2.1.1" xref="S4.SS2.2.2.m2.2.2.2.1.1.cmml"><mi id="S4.SS2.2.2.m2.2.2.2.1.1.2" xref="S4.SS2.2.2.m2.2.2.2.1.1.2.cmml">G</mi><mi id="S4.SS2.2.2.m2.2.2.2.1.1.3" xref="S4.SS2.2.2.m2.2.2.2.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS2.2.2.m2.2.2.2.1.3" xref="S4.SS2.2.2.m2.2.2.2.2.1.cmml">|</mo></mrow></mfrac><mo id="S4.SS2.2.2.m2.2.3.1" xref="S4.SS2.2.2.m2.2.3.1.cmml">&lt;</mo><mn id="S4.SS2.2.2.m2.2.3.2" xref="S4.SS2.2.2.m2.2.3.2.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.2.2.m2.2b"><apply id="S4.SS2.2.2.m2.2.3.cmml" xref="S4.SS2.2.2.m2.2.3"><lt id="S4.SS2.2.2.m2.2.3.1.cmml" xref="S4.SS2.2.2.m2.2.3.1"></lt><apply id="S4.SS2.2.2.m2.2.2.cmml" xref="S4.SS2.2.2.m2.2.2"><divide id="S4.SS2.2.2.m2.2.2.3.cmml" xref="S4.SS2.2.2.m2.2.2"></divide><apply id="S4.SS2.2.2.m2.1.1.1.2.cmml" xref="S4.SS2.2.2.m2.1.1.1.1"><abs id="S4.SS2.2.2.m2.1.1.1.2.1.cmml" xref="S4.SS2.2.2.m2.1.1.1.1.2"></abs><apply id="S4.SS2.2.2.m2.1.1.1.1.1.cmml" xref="S4.SS2.2.2.m2.1.1.1.1.1"><intersect id="S4.SS2.2.2.m2.1.1.1.1.1.1.cmml" xref="S4.SS2.2.2.m2.1.1.1.1.1.1"></intersect><apply id="S4.SS2.2.2.m2.1.1.1.1.1.2.cmml" xref="S4.SS2.2.2.m2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.2.2.m2.1.1.1.1.1.2.1.cmml" xref="S4.SS2.2.2.m2.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS2.2.2.m2.1.1.1.1.1.2.2.cmml" xref="S4.SS2.2.2.m2.1.1.1.1.1.2.2">𝐺</ci><ci id="S4.SS2.2.2.m2.1.1.1.1.1.2.3.cmml" xref="S4.SS2.2.2.m2.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.SS2.2.2.m2.1.1.1.1.1.3.cmml" xref="S4.SS2.2.2.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.2.2.m2.1.1.1.1.1.3.1.cmml" xref="S4.SS2.2.2.m2.1.1.1.1.1.3">subscript</csymbol><ci id="S4.SS2.2.2.m2.1.1.1.1.1.3.2.cmml" xref="S4.SS2.2.2.m2.1.1.1.1.1.3.2">𝑆</ci><ci id="S4.SS2.2.2.m2.1.1.1.1.1.3.3.cmml" xref="S4.SS2.2.2.m2.1.1.1.1.1.3.3">𝑘</ci></apply></apply></apply><apply id="S4.SS2.2.2.m2.2.2.2.2.cmml" xref="S4.SS2.2.2.m2.2.2.2.1"><abs id="S4.SS2.2.2.m2.2.2.2.2.1.cmml" xref="S4.SS2.2.2.m2.2.2.2.1.2"></abs><apply id="S4.SS2.2.2.m2.2.2.2.1.1.cmml" xref="S4.SS2.2.2.m2.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS2.2.2.m2.2.2.2.1.1.1.cmml" xref="S4.SS2.2.2.m2.2.2.2.1.1">subscript</csymbol><ci id="S4.SS2.2.2.m2.2.2.2.1.1.2.cmml" xref="S4.SS2.2.2.m2.2.2.2.1.1.2">𝐺</ci><ci id="S4.SS2.2.2.m2.2.2.2.1.1.3.cmml" xref="S4.SS2.2.2.m2.2.2.2.1.1.3">𝑖</ci></apply></apply></apply><cn type="float" id="S4.SS2.2.2.m2.2.3.2.cmml" xref="S4.SS2.2.2.m2.2.3.2">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.2.2.m2.2c">\frac{\lvert G_{i}\cap S_{k}\rvert}{\lvert G_{i}\rvert}&lt;0.1</annotation></semantics></math>    <math id="S4.SS2.3.3.m3.1" class="ltx_Math" alttext="\forall k\neq j" display="inline"><semantics id="S4.SS2.3.3.m3.1a"><mrow id="S4.SS2.3.3.m3.1.1" xref="S4.SS2.3.3.m3.1.1.cmml"><mrow id="S4.SS2.3.3.m3.1.1.2" xref="S4.SS2.3.3.m3.1.1.2.cmml"><mo rspace="0.167em" id="S4.SS2.3.3.m3.1.1.2.1" xref="S4.SS2.3.3.m3.1.1.2.1.cmml">∀</mo><mi id="S4.SS2.3.3.m3.1.1.2.2" xref="S4.SS2.3.3.m3.1.1.2.2.cmml">k</mi></mrow><mo id="S4.SS2.3.3.m3.1.1.1" xref="S4.SS2.3.3.m3.1.1.1.cmml">≠</mo><mi id="S4.SS2.3.3.m3.1.1.3" xref="S4.SS2.3.3.m3.1.1.3.cmml">j</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.3.3.m3.1b"><apply id="S4.SS2.3.3.m3.1.1.cmml" xref="S4.SS2.3.3.m3.1.1"><neq id="S4.SS2.3.3.m3.1.1.1.cmml" xref="S4.SS2.3.3.m3.1.1.1"></neq><apply id="S4.SS2.3.3.m3.1.1.2.cmml" xref="S4.SS2.3.3.m3.1.1.2"><csymbol cd="latexml" id="S4.SS2.3.3.m3.1.1.2.1.cmml" xref="S4.SS2.3.3.m3.1.1.2.1">for-all</csymbol><ci id="S4.SS2.3.3.m3.1.1.2.2.cmml" xref="S4.SS2.3.3.m3.1.1.2.2">𝑘</ci></apply><ci id="S4.SS2.3.3.m3.1.1.3.cmml" xref="S4.SS2.3.3.m3.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.3.3.m3.1c">\forall k\neq j</annotation></semantics></math></p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.6.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.7.2" class="ltx_text ltx_font_italic">Over segmentation</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The total number of ground truth segments that have a large overlap with two or more detected segments. An over-segmented detection means that multiple detected segments span over a single ground truth. Mathematically,</p>
</div>
<div id="S4.SS3.2" class="ltx_para">
<p id="S4.SS3.2.2" class="ltx_p ltx_align_center"><math id="S4.SS3.1.1.m1.2" class="ltx_Math" alttext="0.1&lt;\frac{\lvert G_{i}\cap S_{j}\rvert}{\lvert G_{i}\rvert}&lt;0.9" display="inline"><semantics id="S4.SS3.1.1.m1.2a"><mrow id="S4.SS3.1.1.m1.2.3" xref="S4.SS3.1.1.m1.2.3.cmml"><mn id="S4.SS3.1.1.m1.2.3.2" xref="S4.SS3.1.1.m1.2.3.2.cmml">0.1</mn><mo id="S4.SS3.1.1.m1.2.3.3" xref="S4.SS3.1.1.m1.2.3.3.cmml">&lt;</mo><mfrac id="S4.SS3.1.1.m1.2.2" xref="S4.SS3.1.1.m1.2.2.cmml"><mrow id="S4.SS3.1.1.m1.1.1.1.1" xref="S4.SS3.1.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS3.1.1.m1.1.1.1.1.2" xref="S4.SS3.1.1.m1.1.1.1.2.1.cmml">|</mo><mrow id="S4.SS3.1.1.m1.1.1.1.1.1" xref="S4.SS3.1.1.m1.1.1.1.1.1.cmml"><msub id="S4.SS3.1.1.m1.1.1.1.1.1.2" xref="S4.SS3.1.1.m1.1.1.1.1.1.2.cmml"><mi id="S4.SS3.1.1.m1.1.1.1.1.1.2.2" xref="S4.SS3.1.1.m1.1.1.1.1.1.2.2.cmml">G</mi><mi id="S4.SS3.1.1.m1.1.1.1.1.1.2.3" xref="S4.SS3.1.1.m1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS3.1.1.m1.1.1.1.1.1.1" xref="S4.SS3.1.1.m1.1.1.1.1.1.1.cmml">∩</mo><msub id="S4.SS3.1.1.m1.1.1.1.1.1.3" xref="S4.SS3.1.1.m1.1.1.1.1.1.3.cmml"><mi id="S4.SS3.1.1.m1.1.1.1.1.1.3.2" xref="S4.SS3.1.1.m1.1.1.1.1.1.3.2.cmml">S</mi><mi id="S4.SS3.1.1.m1.1.1.1.1.1.3.3" xref="S4.SS3.1.1.m1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo stretchy="false" id="S4.SS3.1.1.m1.1.1.1.1.3" xref="S4.SS3.1.1.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S4.SS3.1.1.m1.2.2.2.1" xref="S4.SS3.1.1.m1.2.2.2.2.cmml"><mo stretchy="false" id="S4.SS3.1.1.m1.2.2.2.1.2" xref="S4.SS3.1.1.m1.2.2.2.2.1.cmml">|</mo><msub id="S4.SS3.1.1.m1.2.2.2.1.1" xref="S4.SS3.1.1.m1.2.2.2.1.1.cmml"><mi id="S4.SS3.1.1.m1.2.2.2.1.1.2" xref="S4.SS3.1.1.m1.2.2.2.1.1.2.cmml">G</mi><mi id="S4.SS3.1.1.m1.2.2.2.1.1.3" xref="S4.SS3.1.1.m1.2.2.2.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS3.1.1.m1.2.2.2.1.3" xref="S4.SS3.1.1.m1.2.2.2.2.1.cmml">|</mo></mrow></mfrac><mo id="S4.SS3.1.1.m1.2.3.4" xref="S4.SS3.1.1.m1.2.3.4.cmml">&lt;</mo><mn id="S4.SS3.1.1.m1.2.3.5" xref="S4.SS3.1.1.m1.2.3.5.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.1.1.m1.2b"><apply id="S4.SS3.1.1.m1.2.3.cmml" xref="S4.SS3.1.1.m1.2.3"><and id="S4.SS3.1.1.m1.2.3a.cmml" xref="S4.SS3.1.1.m1.2.3"></and><apply id="S4.SS3.1.1.m1.2.3b.cmml" xref="S4.SS3.1.1.m1.2.3"><lt id="S4.SS3.1.1.m1.2.3.3.cmml" xref="S4.SS3.1.1.m1.2.3.3"></lt><cn type="float" id="S4.SS3.1.1.m1.2.3.2.cmml" xref="S4.SS3.1.1.m1.2.3.2">0.1</cn><apply id="S4.SS3.1.1.m1.2.2.cmml" xref="S4.SS3.1.1.m1.2.2"><divide id="S4.SS3.1.1.m1.2.2.3.cmml" xref="S4.SS3.1.1.m1.2.2"></divide><apply id="S4.SS3.1.1.m1.1.1.1.2.cmml" xref="S4.SS3.1.1.m1.1.1.1.1"><abs id="S4.SS3.1.1.m1.1.1.1.2.1.cmml" xref="S4.SS3.1.1.m1.1.1.1.1.2"></abs><apply id="S4.SS3.1.1.m1.1.1.1.1.1.cmml" xref="S4.SS3.1.1.m1.1.1.1.1.1"><intersect id="S4.SS3.1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS3.1.1.m1.1.1.1.1.1.1"></intersect><apply id="S4.SS3.1.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS3.1.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.1.1.m1.1.1.1.1.1.2.1.cmml" xref="S4.SS3.1.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS3.1.1.m1.1.1.1.1.1.2.2.cmml" xref="S4.SS3.1.1.m1.1.1.1.1.1.2.2">𝐺</ci><ci id="S4.SS3.1.1.m1.1.1.1.1.1.2.3.cmml" xref="S4.SS3.1.1.m1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.SS3.1.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS3.1.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.1.1.m1.1.1.1.1.1.3.1.cmml" xref="S4.SS3.1.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.SS3.1.1.m1.1.1.1.1.1.3.2.cmml" xref="S4.SS3.1.1.m1.1.1.1.1.1.3.2">𝑆</ci><ci id="S4.SS3.1.1.m1.1.1.1.1.1.3.3.cmml" xref="S4.SS3.1.1.m1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply><apply id="S4.SS3.1.1.m1.2.2.2.2.cmml" xref="S4.SS3.1.1.m1.2.2.2.1"><abs id="S4.SS3.1.1.m1.2.2.2.2.1.cmml" xref="S4.SS3.1.1.m1.2.2.2.1.2"></abs><apply id="S4.SS3.1.1.m1.2.2.2.1.1.cmml" xref="S4.SS3.1.1.m1.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS3.1.1.m1.2.2.2.1.1.1.cmml" xref="S4.SS3.1.1.m1.2.2.2.1.1">subscript</csymbol><ci id="S4.SS3.1.1.m1.2.2.2.1.1.2.cmml" xref="S4.SS3.1.1.m1.2.2.2.1.1.2">𝐺</ci><ci id="S4.SS3.1.1.m1.2.2.2.1.1.3.cmml" xref="S4.SS3.1.1.m1.2.2.2.1.1.3">𝑖</ci></apply></apply></apply></apply><apply id="S4.SS3.1.1.m1.2.3c.cmml" xref="S4.SS3.1.1.m1.2.3"><lt id="S4.SS3.1.1.m1.2.3.4.cmml" xref="S4.SS3.1.1.m1.2.3.4"></lt><share href="#S4.SS3.1.1.m1.2.2.cmml" id="S4.SS3.1.1.m1.2.3d.cmml" xref="S4.SS3.1.1.m1.2.3"></share><cn type="float" id="S4.SS3.1.1.m1.2.3.5.cmml" xref="S4.SS3.1.1.m1.2.3.5">0.9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.1.1.m1.2c">0.1&lt;\frac{\lvert G_{i}\cap S_{j}\rvert}{\lvert G_{i}\rvert}&lt;0.9</annotation></semantics></math>    </p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.2" class="ltx_p">holds for more than one detected segments <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="S_{j}" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><msub id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">S</mi><mi id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">𝑆</ci><ci id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">S_{j}</annotation></semantics></math> for a particular <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="G_{i}" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><msub id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mi id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">G</mi><mi id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">𝐺</ci><ci id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">G_{i}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.6.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.7.2" class="ltx_text ltx_font_italic">Under segmentation</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">This is the inverse of over-segmentation i.e. the number of detected segments that have a large intersection with more than one ground truth segment. An under-segmented detection means that a single detection spans over multiple ground truth segments. Mathematically,</p>
</div>
<div id="S4.SS4.2" class="ltx_para">
<p id="S4.SS4.2.2" class="ltx_p ltx_align_center"><math id="S4.SS4.1.1.m1.2" class="ltx_Math" alttext="0.1&lt;\frac{\lvert G_{i}\cap S_{j}\rvert}{\lvert S_{j}\rvert}&lt;0.9" display="inline"><semantics id="S4.SS4.1.1.m1.2a"><mrow id="S4.SS4.1.1.m1.2.3" xref="S4.SS4.1.1.m1.2.3.cmml"><mn id="S4.SS4.1.1.m1.2.3.2" xref="S4.SS4.1.1.m1.2.3.2.cmml">0.1</mn><mo id="S4.SS4.1.1.m1.2.3.3" xref="S4.SS4.1.1.m1.2.3.3.cmml">&lt;</mo><mfrac id="S4.SS4.1.1.m1.2.2" xref="S4.SS4.1.1.m1.2.2.cmml"><mrow id="S4.SS4.1.1.m1.1.1.1.1" xref="S4.SS4.1.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS4.1.1.m1.1.1.1.1.2" xref="S4.SS4.1.1.m1.1.1.1.2.1.cmml">|</mo><mrow id="S4.SS4.1.1.m1.1.1.1.1.1" xref="S4.SS4.1.1.m1.1.1.1.1.1.cmml"><msub id="S4.SS4.1.1.m1.1.1.1.1.1.2" xref="S4.SS4.1.1.m1.1.1.1.1.1.2.cmml"><mi id="S4.SS4.1.1.m1.1.1.1.1.1.2.2" xref="S4.SS4.1.1.m1.1.1.1.1.1.2.2.cmml">G</mi><mi id="S4.SS4.1.1.m1.1.1.1.1.1.2.3" xref="S4.SS4.1.1.m1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS4.1.1.m1.1.1.1.1.1.1" xref="S4.SS4.1.1.m1.1.1.1.1.1.1.cmml">∩</mo><msub id="S4.SS4.1.1.m1.1.1.1.1.1.3" xref="S4.SS4.1.1.m1.1.1.1.1.1.3.cmml"><mi id="S4.SS4.1.1.m1.1.1.1.1.1.3.2" xref="S4.SS4.1.1.m1.1.1.1.1.1.3.2.cmml">S</mi><mi id="S4.SS4.1.1.m1.1.1.1.1.1.3.3" xref="S4.SS4.1.1.m1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo stretchy="false" id="S4.SS4.1.1.m1.1.1.1.1.3" xref="S4.SS4.1.1.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S4.SS4.1.1.m1.2.2.2.1" xref="S4.SS4.1.1.m1.2.2.2.2.cmml"><mo stretchy="false" id="S4.SS4.1.1.m1.2.2.2.1.2" xref="S4.SS4.1.1.m1.2.2.2.2.1.cmml">|</mo><msub id="S4.SS4.1.1.m1.2.2.2.1.1" xref="S4.SS4.1.1.m1.2.2.2.1.1.cmml"><mi id="S4.SS4.1.1.m1.2.2.2.1.1.2" xref="S4.SS4.1.1.m1.2.2.2.1.1.2.cmml">S</mi><mi id="S4.SS4.1.1.m1.2.2.2.1.1.3" xref="S4.SS4.1.1.m1.2.2.2.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S4.SS4.1.1.m1.2.2.2.1.3" xref="S4.SS4.1.1.m1.2.2.2.2.1.cmml">|</mo></mrow></mfrac><mo id="S4.SS4.1.1.m1.2.3.4" xref="S4.SS4.1.1.m1.2.3.4.cmml">&lt;</mo><mn id="S4.SS4.1.1.m1.2.3.5" xref="S4.SS4.1.1.m1.2.3.5.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.1.1.m1.2b"><apply id="S4.SS4.1.1.m1.2.3.cmml" xref="S4.SS4.1.1.m1.2.3"><and id="S4.SS4.1.1.m1.2.3a.cmml" xref="S4.SS4.1.1.m1.2.3"></and><apply id="S4.SS4.1.1.m1.2.3b.cmml" xref="S4.SS4.1.1.m1.2.3"><lt id="S4.SS4.1.1.m1.2.3.3.cmml" xref="S4.SS4.1.1.m1.2.3.3"></lt><cn type="float" id="S4.SS4.1.1.m1.2.3.2.cmml" xref="S4.SS4.1.1.m1.2.3.2">0.1</cn><apply id="S4.SS4.1.1.m1.2.2.cmml" xref="S4.SS4.1.1.m1.2.2"><divide id="S4.SS4.1.1.m1.2.2.3.cmml" xref="S4.SS4.1.1.m1.2.2"></divide><apply id="S4.SS4.1.1.m1.1.1.1.2.cmml" xref="S4.SS4.1.1.m1.1.1.1.1"><abs id="S4.SS4.1.1.m1.1.1.1.2.1.cmml" xref="S4.SS4.1.1.m1.1.1.1.1.2"></abs><apply id="S4.SS4.1.1.m1.1.1.1.1.1.cmml" xref="S4.SS4.1.1.m1.1.1.1.1.1"><intersect id="S4.SS4.1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS4.1.1.m1.1.1.1.1.1.1"></intersect><apply id="S4.SS4.1.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS4.1.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS4.1.1.m1.1.1.1.1.1.2.1.cmml" xref="S4.SS4.1.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS4.1.1.m1.1.1.1.1.1.2.2.cmml" xref="S4.SS4.1.1.m1.1.1.1.1.1.2.2">𝐺</ci><ci id="S4.SS4.1.1.m1.1.1.1.1.1.2.3.cmml" xref="S4.SS4.1.1.m1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.SS4.1.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS4.1.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.1.1.m1.1.1.1.1.1.3.1.cmml" xref="S4.SS4.1.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.SS4.1.1.m1.1.1.1.1.1.3.2.cmml" xref="S4.SS4.1.1.m1.1.1.1.1.1.3.2">𝑆</ci><ci id="S4.SS4.1.1.m1.1.1.1.1.1.3.3.cmml" xref="S4.SS4.1.1.m1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply><apply id="S4.SS4.1.1.m1.2.2.2.2.cmml" xref="S4.SS4.1.1.m1.2.2.2.1"><abs id="S4.SS4.1.1.m1.2.2.2.2.1.cmml" xref="S4.SS4.1.1.m1.2.2.2.1.2"></abs><apply id="S4.SS4.1.1.m1.2.2.2.1.1.cmml" xref="S4.SS4.1.1.m1.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS4.1.1.m1.2.2.2.1.1.1.cmml" xref="S4.SS4.1.1.m1.2.2.2.1.1">subscript</csymbol><ci id="S4.SS4.1.1.m1.2.2.2.1.1.2.cmml" xref="S4.SS4.1.1.m1.2.2.2.1.1.2">𝑆</ci><ci id="S4.SS4.1.1.m1.2.2.2.1.1.3.cmml" xref="S4.SS4.1.1.m1.2.2.2.1.1.3">𝑗</ci></apply></apply></apply></apply><apply id="S4.SS4.1.1.m1.2.3c.cmml" xref="S4.SS4.1.1.m1.2.3"><lt id="S4.SS4.1.1.m1.2.3.4.cmml" xref="S4.SS4.1.1.m1.2.3.4"></lt><share href="#S4.SS4.1.1.m1.2.2.cmml" id="S4.SS4.1.1.m1.2.3d.cmml" xref="S4.SS4.1.1.m1.2.3"></share><cn type="float" id="S4.SS4.1.1.m1.2.3.5.cmml" xref="S4.SS4.1.1.m1.2.3.5">0.9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.1.1.m1.2c">0.1&lt;\frac{\lvert G_{i}\cap S_{j}\rvert}{\lvert S_{j}\rvert}&lt;0.9</annotation></semantics></math>    </p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.2" class="ltx_p">holds for more than one ground truth segment <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="G_{i}" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><msub id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml"><mi id="S4.SS4.p2.1.m1.1.1.2" xref="S4.SS4.p2.1.m1.1.1.2.cmml">G</mi><mi id="S4.SS4.p2.1.m1.1.1.3" xref="S4.SS4.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><apply id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.1.m1.1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p2.1.m1.1.1.2.cmml" xref="S4.SS4.p2.1.m1.1.1.2">𝐺</ci><ci id="S4.SS4.p2.1.m1.1.1.3.cmml" xref="S4.SS4.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">G_{i}</annotation></semantics></math> for a particular <math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="S_{j}" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><msub id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml"><mi id="S4.SS4.p2.2.m2.1.1.2" xref="S4.SS4.p2.2.m2.1.1.2.cmml">S</mi><mi id="S4.SS4.p2.2.m2.1.1.3" xref="S4.SS4.p2.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><apply id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.2.m2.1.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p2.2.m2.1.1.2.cmml" xref="S4.SS4.p2.2.m2.1.1.2">𝑆</ci><ci id="S4.SS4.p2.2.m2.1.1.3.cmml" xref="S4.SS4.p2.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">S_{j}</annotation></semantics></math></p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.4.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.5.2" class="ltx_text ltx_font_italic">Missed segments</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.3" class="ltx_p">These are the number of ground truth segments that do not have a large overlap with any of the detected segments. These are segments that our algorithm should have detected but failed to do so.</p>
<p id="S4.SS5.p1.2.2" class="ltx_p ltx_align_center"><math id="S4.SS5.p1.1.1.m1.2" class="ltx_Math" alttext="\frac{\lvert G_{i}\cap S_{j}\rvert}{\lvert G_{i}\rvert}&lt;0.1" display="inline"><semantics id="S4.SS5.p1.1.1.m1.2a"><mrow id="S4.SS5.p1.1.1.m1.2.3" xref="S4.SS5.p1.1.1.m1.2.3.cmml"><mfrac id="S4.SS5.p1.1.1.m1.2.2" xref="S4.SS5.p1.1.1.m1.2.2.cmml"><mrow id="S4.SS5.p1.1.1.m1.1.1.1.1" xref="S4.SS5.p1.1.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS5.p1.1.1.m1.1.1.1.1.2" xref="S4.SS5.p1.1.1.m1.1.1.1.2.1.cmml">|</mo><mrow id="S4.SS5.p1.1.1.m1.1.1.1.1.1" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.cmml"><msub id="S4.SS5.p1.1.1.m1.1.1.1.1.1.2" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.2.cmml"><mi id="S4.SS5.p1.1.1.m1.1.1.1.1.1.2.2" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.2.2.cmml">G</mi><mi id="S4.SS5.p1.1.1.m1.1.1.1.1.1.2.3" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS5.p1.1.1.m1.1.1.1.1.1.1" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.1.cmml">∩</mo><msub id="S4.SS5.p1.1.1.m1.1.1.1.1.1.3" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.3.cmml"><mi id="S4.SS5.p1.1.1.m1.1.1.1.1.1.3.2" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.3.2.cmml">S</mi><mi id="S4.SS5.p1.1.1.m1.1.1.1.1.1.3.3" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo stretchy="false" id="S4.SS5.p1.1.1.m1.1.1.1.1.3" xref="S4.SS5.p1.1.1.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S4.SS5.p1.1.1.m1.2.2.2.1" xref="S4.SS5.p1.1.1.m1.2.2.2.2.cmml"><mo stretchy="false" id="S4.SS5.p1.1.1.m1.2.2.2.1.2" xref="S4.SS5.p1.1.1.m1.2.2.2.2.1.cmml">|</mo><msub id="S4.SS5.p1.1.1.m1.2.2.2.1.1" xref="S4.SS5.p1.1.1.m1.2.2.2.1.1.cmml"><mi id="S4.SS5.p1.1.1.m1.2.2.2.1.1.2" xref="S4.SS5.p1.1.1.m1.2.2.2.1.1.2.cmml">G</mi><mi id="S4.SS5.p1.1.1.m1.2.2.2.1.1.3" xref="S4.SS5.p1.1.1.m1.2.2.2.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS5.p1.1.1.m1.2.2.2.1.3" xref="S4.SS5.p1.1.1.m1.2.2.2.2.1.cmml">|</mo></mrow></mfrac><mo id="S4.SS5.p1.1.1.m1.2.3.1" xref="S4.SS5.p1.1.1.m1.2.3.1.cmml">&lt;</mo><mn id="S4.SS5.p1.1.1.m1.2.3.2" xref="S4.SS5.p1.1.1.m1.2.3.2.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.1.m1.2b"><apply id="S4.SS5.p1.1.1.m1.2.3.cmml" xref="S4.SS5.p1.1.1.m1.2.3"><lt id="S4.SS5.p1.1.1.m1.2.3.1.cmml" xref="S4.SS5.p1.1.1.m1.2.3.1"></lt><apply id="S4.SS5.p1.1.1.m1.2.2.cmml" xref="S4.SS5.p1.1.1.m1.2.2"><divide id="S4.SS5.p1.1.1.m1.2.2.3.cmml" xref="S4.SS5.p1.1.1.m1.2.2"></divide><apply id="S4.SS5.p1.1.1.m1.1.1.1.2.cmml" xref="S4.SS5.p1.1.1.m1.1.1.1.1"><abs id="S4.SS5.p1.1.1.m1.1.1.1.2.1.cmml" xref="S4.SS5.p1.1.1.m1.1.1.1.1.2"></abs><apply id="S4.SS5.p1.1.1.m1.1.1.1.1.1.cmml" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1"><intersect id="S4.SS5.p1.1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.1"></intersect><apply id="S4.SS5.p1.1.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS5.p1.1.1.m1.1.1.1.1.1.2.1.cmml" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS5.p1.1.1.m1.1.1.1.1.1.2.2.cmml" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.2.2">𝐺</ci><ci id="S4.SS5.p1.1.1.m1.1.1.1.1.1.2.3.cmml" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.SS5.p1.1.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS5.p1.1.1.m1.1.1.1.1.1.3.1.cmml" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.SS5.p1.1.1.m1.1.1.1.1.1.3.2.cmml" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.3.2">𝑆</ci><ci id="S4.SS5.p1.1.1.m1.1.1.1.1.1.3.3.cmml" xref="S4.SS5.p1.1.1.m1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply><apply id="S4.SS5.p1.1.1.m1.2.2.2.2.cmml" xref="S4.SS5.p1.1.1.m1.2.2.2.1"><abs id="S4.SS5.p1.1.1.m1.2.2.2.2.1.cmml" xref="S4.SS5.p1.1.1.m1.2.2.2.1.2"></abs><apply id="S4.SS5.p1.1.1.m1.2.2.2.1.1.cmml" xref="S4.SS5.p1.1.1.m1.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.1.1.m1.2.2.2.1.1.1.cmml" xref="S4.SS5.p1.1.1.m1.2.2.2.1.1">subscript</csymbol><ci id="S4.SS5.p1.1.1.m1.2.2.2.1.1.2.cmml" xref="S4.SS5.p1.1.1.m1.2.2.2.1.1.2">𝐺</ci><ci id="S4.SS5.p1.1.1.m1.2.2.2.1.1.3.cmml" xref="S4.SS5.p1.1.1.m1.2.2.2.1.1.3">𝑖</ci></apply></apply></apply><cn type="float" id="S4.SS5.p1.1.1.m1.2.3.2.cmml" xref="S4.SS5.p1.1.1.m1.2.3.2">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.1.m1.2c">\frac{\lvert G_{i}\cap S_{j}\rvert}{\lvert G_{i}\rvert}&lt;0.1</annotation></semantics></math>    <math id="S4.SS5.p1.2.2.m2.1" class="ltx_Math" alttext="\forall j" display="inline"><semantics id="S4.SS5.p1.2.2.m2.1a"><mrow id="S4.SS5.p1.2.2.m2.1.1" xref="S4.SS5.p1.2.2.m2.1.1.cmml"><mo rspace="0.167em" id="S4.SS5.p1.2.2.m2.1.1.1" xref="S4.SS5.p1.2.2.m2.1.1.1.cmml">∀</mo><mi id="S4.SS5.p1.2.2.m2.1.1.2" xref="S4.SS5.p1.2.2.m2.1.1.2.cmml">j</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.2.m2.1b"><apply id="S4.SS5.p1.2.2.m2.1.1.cmml" xref="S4.SS5.p1.2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS5.p1.2.2.m2.1.1.1.cmml" xref="S4.SS5.p1.2.2.m2.1.1.1">for-all</csymbol><ci id="S4.SS5.p1.2.2.m2.1.1.2.cmml" xref="S4.SS5.p1.2.2.m2.1.1.2">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.2.m2.1c">\forall j</annotation></semantics></math></p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS6.6.1.1" class="ltx_text">IV-F</span> </span><span id="S4.SS6.7.2" class="ltx_text ltx_font_italic">False Positive Detections</span>
</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">The inverse of missed segments, these are segments detected by the algorithm but which are not actually present in the ground truth. These are foreground pixels that algorithm has mistakenly detected as segments.</p>
</div>
<div id="S4.SS6.2" class="ltx_para">
<p id="S4.SS6.2.2" class="ltx_p ltx_align_center"><math id="S4.SS6.1.1.m1.2" class="ltx_Math" alttext="\frac{\lvert G_{i}\cap S_{j}\rvert}{\lvert S_{j}\rvert}&lt;0.1" display="inline"><semantics id="S4.SS6.1.1.m1.2a"><mrow id="S4.SS6.1.1.m1.2.3" xref="S4.SS6.1.1.m1.2.3.cmml"><mfrac id="S4.SS6.1.1.m1.2.2" xref="S4.SS6.1.1.m1.2.2.cmml"><mrow id="S4.SS6.1.1.m1.1.1.1.1" xref="S4.SS6.1.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS6.1.1.m1.1.1.1.1.2" xref="S4.SS6.1.1.m1.1.1.1.2.1.cmml">|</mo><mrow id="S4.SS6.1.1.m1.1.1.1.1.1" xref="S4.SS6.1.1.m1.1.1.1.1.1.cmml"><msub id="S4.SS6.1.1.m1.1.1.1.1.1.2" xref="S4.SS6.1.1.m1.1.1.1.1.1.2.cmml"><mi id="S4.SS6.1.1.m1.1.1.1.1.1.2.2" xref="S4.SS6.1.1.m1.1.1.1.1.1.2.2.cmml">G</mi><mi id="S4.SS6.1.1.m1.1.1.1.1.1.2.3" xref="S4.SS6.1.1.m1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS6.1.1.m1.1.1.1.1.1.1" xref="S4.SS6.1.1.m1.1.1.1.1.1.1.cmml">∩</mo><msub id="S4.SS6.1.1.m1.1.1.1.1.1.3" xref="S4.SS6.1.1.m1.1.1.1.1.1.3.cmml"><mi id="S4.SS6.1.1.m1.1.1.1.1.1.3.2" xref="S4.SS6.1.1.m1.1.1.1.1.1.3.2.cmml">S</mi><mi id="S4.SS6.1.1.m1.1.1.1.1.1.3.3" xref="S4.SS6.1.1.m1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo stretchy="false" id="S4.SS6.1.1.m1.1.1.1.1.3" xref="S4.SS6.1.1.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S4.SS6.1.1.m1.2.2.2.1" xref="S4.SS6.1.1.m1.2.2.2.2.cmml"><mo stretchy="false" id="S4.SS6.1.1.m1.2.2.2.1.2" xref="S4.SS6.1.1.m1.2.2.2.2.1.cmml">|</mo><msub id="S4.SS6.1.1.m1.2.2.2.1.1" xref="S4.SS6.1.1.m1.2.2.2.1.1.cmml"><mi id="S4.SS6.1.1.m1.2.2.2.1.1.2" xref="S4.SS6.1.1.m1.2.2.2.1.1.2.cmml">S</mi><mi id="S4.SS6.1.1.m1.2.2.2.1.1.3" xref="S4.SS6.1.1.m1.2.2.2.1.1.3.cmml">j</mi></msub><mo stretchy="false" id="S4.SS6.1.1.m1.2.2.2.1.3" xref="S4.SS6.1.1.m1.2.2.2.2.1.cmml">|</mo></mrow></mfrac><mo id="S4.SS6.1.1.m1.2.3.1" xref="S4.SS6.1.1.m1.2.3.1.cmml">&lt;</mo><mn id="S4.SS6.1.1.m1.2.3.2" xref="S4.SS6.1.1.m1.2.3.2.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.1.1.m1.2b"><apply id="S4.SS6.1.1.m1.2.3.cmml" xref="S4.SS6.1.1.m1.2.3"><lt id="S4.SS6.1.1.m1.2.3.1.cmml" xref="S4.SS6.1.1.m1.2.3.1"></lt><apply id="S4.SS6.1.1.m1.2.2.cmml" xref="S4.SS6.1.1.m1.2.2"><divide id="S4.SS6.1.1.m1.2.2.3.cmml" xref="S4.SS6.1.1.m1.2.2"></divide><apply id="S4.SS6.1.1.m1.1.1.1.2.cmml" xref="S4.SS6.1.1.m1.1.1.1.1"><abs id="S4.SS6.1.1.m1.1.1.1.2.1.cmml" xref="S4.SS6.1.1.m1.1.1.1.1.2"></abs><apply id="S4.SS6.1.1.m1.1.1.1.1.1.cmml" xref="S4.SS6.1.1.m1.1.1.1.1.1"><intersect id="S4.SS6.1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS6.1.1.m1.1.1.1.1.1.1"></intersect><apply id="S4.SS6.1.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS6.1.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS6.1.1.m1.1.1.1.1.1.2.1.cmml" xref="S4.SS6.1.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS6.1.1.m1.1.1.1.1.1.2.2.cmml" xref="S4.SS6.1.1.m1.1.1.1.1.1.2.2">𝐺</ci><ci id="S4.SS6.1.1.m1.1.1.1.1.1.2.3.cmml" xref="S4.SS6.1.1.m1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.SS6.1.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS6.1.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS6.1.1.m1.1.1.1.1.1.3.1.cmml" xref="S4.SS6.1.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.SS6.1.1.m1.1.1.1.1.1.3.2.cmml" xref="S4.SS6.1.1.m1.1.1.1.1.1.3.2">𝑆</ci><ci id="S4.SS6.1.1.m1.1.1.1.1.1.3.3.cmml" xref="S4.SS6.1.1.m1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply><apply id="S4.SS6.1.1.m1.2.2.2.2.cmml" xref="S4.SS6.1.1.m1.2.2.2.1"><abs id="S4.SS6.1.1.m1.2.2.2.2.1.cmml" xref="S4.SS6.1.1.m1.2.2.2.1.2"></abs><apply id="S4.SS6.1.1.m1.2.2.2.1.1.cmml" xref="S4.SS6.1.1.m1.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS6.1.1.m1.2.2.2.1.1.1.cmml" xref="S4.SS6.1.1.m1.2.2.2.1.1">subscript</csymbol><ci id="S4.SS6.1.1.m1.2.2.2.1.1.2.cmml" xref="S4.SS6.1.1.m1.2.2.2.1.1.2">𝑆</ci><ci id="S4.SS6.1.1.m1.2.2.2.1.1.3.cmml" xref="S4.SS6.1.1.m1.2.2.2.1.1.3">𝑗</ci></apply></apply></apply><cn type="float" id="S4.SS6.1.1.m1.2.3.2.cmml" xref="S4.SS6.1.1.m1.2.3.2">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.1.1.m1.2c">\frac{\lvert G_{i}\cap S_{j}\rvert}{\lvert S_{j}\rvert}&lt;0.1</annotation></semantics></math>    <math id="S4.SS6.2.2.m2.1" class="ltx_Math" alttext="\forall i" display="inline"><semantics id="S4.SS6.2.2.m2.1a"><mrow id="S4.SS6.2.2.m2.1.1" xref="S4.SS6.2.2.m2.1.1.cmml"><mo rspace="0.167em" id="S4.SS6.2.2.m2.1.1.1" xref="S4.SS6.2.2.m2.1.1.1.cmml">∀</mo><mi id="S4.SS6.2.2.m2.1.1.2" xref="S4.SS6.2.2.m2.1.1.2.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.2.2.m2.1b"><apply id="S4.SS6.2.2.m2.1.1.cmml" xref="S4.SS6.2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS6.2.2.m2.1.1.1.cmml" xref="S4.SS6.2.2.m2.1.1.1">for-all</csymbol><ci id="S4.SS6.2.2.m2.1.1.2.cmml" xref="S4.SS6.2.2.m2.1.1.2">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.2.2.m2.1c">\forall i</annotation></semantics></math></p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.1" class="ltx_p">Figure <a href="#S4.F3" title="Figure 3 ‣ IV Performance Measures ‣ Table Structure Extraction with Bi-directional Gated Recurrent Unit Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> show our model output for some sample images on UNLV dataset showing correct, over-segmented and under-segmented detections.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experiments and Results</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.3" class="ltx_p">We have used publicly available UNLV dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> for the evaluation of our approach. The dataset spans a number of documents of varying layouts and domains including newspapers, research articles, magazines, technical reports etc. There are <math id="S5.p1.1.m1.2" class="ltx_Math" alttext="2,889" display="inline"><semantics id="S5.p1.1.m1.2a"><mrow id="S5.p1.1.m1.2.3.2" xref="S5.p1.1.m1.2.3.1.cmml"><mn id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">2</mn><mo id="S5.p1.1.m1.2.3.2.1" xref="S5.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.p1.1.m1.2.2" xref="S5.p1.1.m1.2.2.cmml">889</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.2b"><list id="S5.p1.1.m1.2.3.1.cmml" xref="S5.p1.1.m1.2.3.2"><cn type="integer" id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">2</cn><cn type="integer" id="S5.p1.1.m1.2.2.cmml" xref="S5.p1.1.m1.2.2">889</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.2c">2,889</annotation></semantics></math> images in UNLV dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, out of which there are <math id="S5.p1.2.m2.1" class="ltx_Math" alttext="427" display="inline"><semantics id="S5.p1.2.m2.1a"><mn id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml">427</mn><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><cn type="integer" id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">427</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">427</annotation></semantics></math> images containing at least one table. The dataset also includes accompanying, manually drawn ground truths for table boundaries. The ground-truth for columns, rows and cells was presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Since our work is focused on table structure extraction rather than table detection, we cropped the tables from the images using UNLV ground truth files, resulting in <math id="S5.p1.3.m3.1" class="ltx_Math" alttext="557" display="inline"><semantics id="S5.p1.3.m3.1a"><mn id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml">557</mn><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><cn type="integer" id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1">557</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">557</annotation></semantics></math> tables. We then evaluated our model’s outputs against the ground truths provided in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. We did not use any image from the UNLV dataset in the training and validation process of our model and thus all the images were unseen by the model.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.4" class="ltx_p">We have benchmarked our approach with T-Recs system by Kieninger et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, a non-deep-learning based algorithm for table structure extraction. Our approach provided a significant improvement in correct column detections, from <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="40.51\%" display="inline"><semantics id="S5.p2.1.m1.1a"><mrow id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml"><mn id="S5.p2.1.m1.1.1.2" xref="S5.p2.1.m1.1.1.2.cmml">40.51</mn><mo id="S5.p2.1.m1.1.1.1" xref="S5.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><apply id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.p2.1.m1.1.1.1.cmml" xref="S5.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.p2.1.m1.1.1.2.cmml" xref="S5.p2.1.m1.1.1.2">40.51</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">40.51\%</annotation></semantics></math> to <math id="S5.p2.2.m2.1" class="ltx_Math" alttext="55.31\%" display="inline"><semantics id="S5.p2.2.m2.1a"><mrow id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml"><mn id="S5.p2.2.m2.1.1.2" xref="S5.p2.2.m2.1.1.2.cmml">55.31</mn><mo id="S5.p2.2.m2.1.1.1" xref="S5.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><apply id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1"><csymbol cd="latexml" id="S5.p2.2.m2.1.1.1.cmml" xref="S5.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S5.p2.2.m2.1.1.2.cmml" xref="S5.p2.2.m2.1.1.2">55.31</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">55.31\%</annotation></semantics></math> and from <math id="S5.p2.3.m3.1" class="ltx_Math" alttext="54.98\%" display="inline"><semantics id="S5.p2.3.m3.1a"><mrow id="S5.p2.3.m3.1.1" xref="S5.p2.3.m3.1.1.cmml"><mn id="S5.p2.3.m3.1.1.2" xref="S5.p2.3.m3.1.1.2.cmml">54.98</mn><mo id="S5.p2.3.m3.1.1.1" xref="S5.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.3.m3.1b"><apply id="S5.p2.3.m3.1.1.cmml" xref="S5.p2.3.m3.1.1"><csymbol cd="latexml" id="S5.p2.3.m3.1.1.1.cmml" xref="S5.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.p2.3.m3.1.1.2.cmml" xref="S5.p2.3.m3.1.1.2">54.98</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.3.m3.1c">54.98\%</annotation></semantics></math> to <math id="S5.p2.4.m4.1" class="ltx_Math" alttext="58.45\%" display="inline"><semantics id="S5.p2.4.m4.1a"><mrow id="S5.p2.4.m4.1.1" xref="S5.p2.4.m4.1.1.cmml"><mn id="S5.p2.4.m4.1.1.2" xref="S5.p2.4.m4.1.1.2.cmml">58.45</mn><mo id="S5.p2.4.m4.1.1.1" xref="S5.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.4.m4.1b"><apply id="S5.p2.4.m4.1.1.cmml" xref="S5.p2.4.m4.1.1"><csymbol cd="latexml" id="S5.p2.4.m4.1.1.1.cmml" xref="S5.p2.4.m4.1.1.1">percent</csymbol><cn type="float" id="S5.p2.4.m4.1.1.2.cmml" xref="S5.p2.4.m4.1.1.2">58.45</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.4.m4.1c">58.45\%</annotation></semantics></math> in the case of row detection. On the other hand, the number of partial detections has gone down which is explained by the higher number of over-segmentations and under-segmentations in our approach as compared to T-Recs (see Table <a href="#S5.T1" title="Table I ‣ V Experiments and Results ‣ Table Structure Extraction with Bi-directional Gated Recurrent Unit Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> and <a href="#S5.T2" title="Table II ‣ V Experiments and Results ‣ Table Structure Extraction with Bi-directional Gated Recurrent Unit Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> for comparison results).</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table I: </span>The results of evaluating our system on 427 binary 300-dpi scanned UNLV dataset pages containing table zones. The following benchmark is for column segmentation.</figcaption>
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S5.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3"><span id="S5.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy%</span></th>
</tr>
<tr id="S5.T1.1.2.2" class="ltx_tr">
<th id="S5.T1.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S5.T1.1.2.2.2" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t"></th>
<th id="S5.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2"><span id="S5.T1.1.2.2.3.1" class="ltx_text ltx_font_bold">Our Approach</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.3.1" class="ltx_tr">
<th id="S5.T1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S5.T1.1.3.1.1.1" class="ltx_text ltx_font_bold">Performance Measures</span></th>
<td id="S5.T1.1.3.1.2" class="ltx_td ltx_align_right ltx_border_r"><span id="S5.T1.1.3.1.2.1" class="ltx_text ltx_font_bold">T-Recs</span></td>
<td id="S5.T1.1.3.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T1.1.3.1.3.1" class="ltx_text ltx_font_bold">Bi-directional LSTM</span></td>
<td id="S5.T1.1.3.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T1.1.3.1.4.1" class="ltx_text ltx_font_bold">Bi-directional GRU</span></td>
</tr>
<tr id="S5.T1.1.4.2" class="ltx_tr">
<th id="S5.T1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Correct Detections</th>
<td id="S5.T1.1.4.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">40.51</td>
<td id="S5.T1.1.4.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">49.05</td>
<td id="S5.T1.1.4.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">55.31</td>
</tr>
<tr id="S5.T1.1.5.3" class="ltx_tr">
<th id="S5.T1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Partial Detections</th>
<td id="S5.T1.1.5.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">18.57</td>
<td id="S5.T1.1.5.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">15.13</td>
<td id="S5.T1.1.5.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">12.13</td>
</tr>
<tr id="S5.T1.1.6.4" class="ltx_tr">
<th id="S5.T1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Missed Detections</th>
<td id="S5.T1.1.6.4.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">13.50</td>
<td id="S5.T1.1.6.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">6.99</td>
<td id="S5.T1.1.6.4.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">3.12</td>
</tr>
<tr id="S5.T1.1.7.5" class="ltx_tr">
<th id="S5.T1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Over Segmented Detections</th>
<td id="S5.T1.1.7.5.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">13.50</td>
<td id="S5.T1.1.7.5.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">18.44</td>
<td id="S5.T1.1.7.5.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">12.14</td>
</tr>
<tr id="S5.T1.1.8.6" class="ltx_tr">
<th id="S5.T1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Under Segmented Detections</th>
<td id="S5.T1.1.8.6.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">5.11</td>
<td id="S5.T1.1.8.6.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">20.55</td>
<td id="S5.T1.1.8.6.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">16.75</td>
</tr>
<tr id="S5.T1.1.9.7" class="ltx_tr">
<th id="S5.T1.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">False Positive Detections</th>
<td id="S5.T1.1.9.7.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">0.88</td>
<td id="S5.T1.1.9.7.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">1.20</td>
<td id="S5.T1.1.9.7.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">0.08</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table II: </span>Results of evaluating our system on 427 binary 300-dpi scanned UNLV dataset pages containing table zones. The following benchmark is for row segmentation.</figcaption>
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3"><span id="S5.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy%</span></th>
</tr>
<tr id="S5.T2.1.2.2" class="ltx_tr">
<th id="S5.T2.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S5.T2.1.2.2.2" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t"></th>
<th id="S5.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2"><span id="S5.T2.1.2.2.3.1" class="ltx_text ltx_font_bold">Our Approach</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.3.1" class="ltx_tr">
<th id="S5.T2.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S5.T2.1.3.1.1.1" class="ltx_text ltx_font_bold">Performance Measures</span></th>
<td id="S5.T2.1.3.1.2" class="ltx_td ltx_align_right ltx_border_r"><span id="S5.T2.1.3.1.2.1" class="ltx_text ltx_font_bold">T-Recs</span></td>
<td id="S5.T2.1.3.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T2.1.3.1.3.1" class="ltx_text ltx_font_bold">Bi-directional LSTM</span></td>
<td id="S5.T2.1.3.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T2.1.3.1.4.1" class="ltx_text ltx_font_bold">Bi-directional GRU</span></td>
</tr>
<tr id="S5.T2.1.4.2" class="ltx_tr">
<th id="S5.T2.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Correct Detections</th>
<td id="S5.T2.1.4.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">54.98</td>
<td id="S5.T2.1.4.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">51.62</td>
<td id="S5.T2.1.4.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">58.45</td>
</tr>
<tr id="S5.T2.1.5.3" class="ltx_tr">
<th id="S5.T2.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Partial Detections</th>
<td id="S5.T2.1.5.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">12.45</td>
<td id="S5.T2.1.5.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">17.13</td>
<td id="S5.T2.1.5.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">13.35</td>
</tr>
<tr id="S5.T2.1.6.4" class="ltx_tr">
<th id="S5.T2.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Missed Detections</th>
<td id="S5.T2.1.6.4.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">10.69</td>
<td id="S5.T2.1.6.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">8.39</td>
<td id="S5.T2.1.6.4.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">2.50</td>
</tr>
<tr id="S5.T2.1.7.5" class="ltx_tr">
<th id="S5.T2.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Over Segmented Detections</th>
<td id="S5.T2.1.7.5.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">6.27</td>
<td id="S5.T2.1.7.5.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">4.24</td>
<td id="S5.T2.1.7.5.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">8.33</td>
</tr>
<tr id="S5.T2.1.8.6" class="ltx_tr">
<th id="S5.T2.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Under Segmented Detections</th>
<td id="S5.T2.1.8.6.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">7.70</td>
<td id="S5.T2.1.8.6.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">5.30</td>
<td id="S5.T2.1.8.6.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">14.67</td>
</tr>
<tr id="S5.T2.1.9.7" class="ltx_tr">
<th id="S5.T2.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">False Positive Detections</th>
<td id="S5.T2.1.9.7.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">0.12</td>
<td id="S5.T2.1.9.7.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">0.59</td>
<td id="S5.T2.1.9.7.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">0.15</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table III: </span>Comaprison with Schreiber et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> on ICDAR 2013 dataset using the same methods for calculating precision, recall and F1 score as described in Schreiber et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite></figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2"><span id="S5.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy%</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<th id="S5.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S5.T3.1.2.1.1.1" class="ltx_text ltx_font_bold">Performance Measures</span></th>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T3.1.2.1.2.1" class="ltx_text ltx_font_bold">Schreiber et al.</span></td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S5.T3.1.2.1.3.1" class="ltx_text ltx_font_bold">Our Approach</span></td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<th id="S5.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Precision</th>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">95.93</td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">96.92</td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<th id="S5.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Recall</th>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">87.36</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">90.12</td>
</tr>
<tr id="S5.T3.1.5.4" class="ltx_tr">
<th id="S5.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">F1 Score</th>
<td id="S5.T3.1.5.4.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">91.44</td>
<td id="S5.T3.1.5.4.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">93.39</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.2" class="ltx_p">Our proposed solution is also compared with Schreiber et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> which is the state-of-the-art deep learning based approach towards table structure recognition. For that purpose, we used the publicly available ICDAR 2013 table competition dataset containing <math id="S5.p3.1.m1.1" class="ltx_Math" alttext="67" display="inline"><semantics id="S5.p3.1.m1.1a"><mn id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml">67</mn><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><cn type="integer" id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1">67</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">67</annotation></semantics></math> documents with <math id="S5.p3.2.m2.1" class="ltx_Math" alttext="238" display="inline"><semantics id="S5.p3.2.m2.1a"><mn id="S5.p3.2.m2.1.1" xref="S5.p3.2.m2.1.1.cmml">238</mn><annotation-xml encoding="MathML-Content" id="S5.p3.2.m2.1b"><cn type="integer" id="S5.p3.2.m2.1.1.cmml" xref="S5.p3.2.m2.1.1">238</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.2.m2.1c">238</annotation></semantics></math> pages, since this dataset was used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The results of this comparison are shown in Table <a href="#S5.T3" title="Table III ‣ V Experiments and Results ‣ Table Structure Extraction with Bi-directional Gated Recurrent Unit Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">The benchmarking results exhibit that our approach outperforms the existing approaches by a significant margin. There is an increase in the overall correct detections and a decrease in segmentation errors and missed detections. From Table <a href="#S3.F2" title="Figure 2 ‣ III-B Model ‣ III Proposed Methodology ‣ Table Structure Extraction with Bi-directional Gated Recurrent Unit Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S3.F1" title="Figure 1 ‣ III-A Image Pre-processing ‣ III Proposed Methodology ‣ Table Structure Extraction with Bi-directional Gated Recurrent Unit Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, GRUs outperform LSTMs because of its simpler architecture that is less prone to overfitting.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper proposed a novel approach for table structure extraction using GRU based sequential models for deep learning. This approach provides a significant improvement over heuristic algorithms and CNN based models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, owing to the powerful representation of the sequence models that capture the repetitive row/column structures in tables. In the future, we plan to extend this work to develop a coherent framework for information extraction from table cells.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. Lewandowsky and I. Spence, “The perception of statistical graphs,” <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Sociological Methods &amp; Research</span>, vol. 18, pp. 200–242, 1989.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Schreiber, S. Agne, I. Wolf, A. Dengel, and S. Ahmed, “Deepdesrt: Deep
learning for detection and structure recognition of tables in document
images,” in <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Fourteenth International Conference on Document Analysis
and Recognition</span>, vol. 1, pp. 1162–1167, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A. Shahab, “Table ground truth for the UW3 and UNLV datasets.”
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://www.iapr-tc11.org/mediawiki/index.php?title=Table\_Ground\_Truth\_for\_the\_UW3\_and\_UNLV\_datasets</span>, 2010.

</span>
<span class="ltx_bibblock">[Online; accessed 7-April-2017].

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
T. Kieninger and A. Dengel, “A paper-to-html table converting system,” in
<span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proceedings of document analysis systems</span>, pp. 356–365, 1998.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
T. Kieninger and A. Dengel, “Applying the T-RECS table recognition system to
the business letter domain,” in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">International Conference on Document
Analysis and Recognition</span>, p. 0518, 2001.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
T. Kieninger and A. Dengel, “The T-Recs table recognition and analysis
system,” in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Document Analysis Systems: Theory and Practice</span>,
pp. 255–270, 1999.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
T. Kieninger and A. Dengel, “Table recognition and labeling using intrinsic
layout features,” in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">International Conference on Advances in Pattern
Recognition</span>, pp. 307–316, 1999.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
W. Yalin, I. T. Phillips, and R. M. Haralick, “Table structure understanding
and its performance evaluation,” <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Pattern Recognition</span>, vol. 37,
pp. 1479–1497, 2004.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
F. Shafait, D. Keysers, and T. M. Breuel, “Performance evaluation and
benchmarking of six-page segmentation algorithms,” <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on
Pattern Analysis and Machine Intelligence</span>, vol. 30, no. 6, pp. 941–954,
2008.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Shigarov, A. Mikhailov, and A. Altaev, “Configurable table structure
recognition in untagged pdf documents,” in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">ACM Symposium on Document
Engineering</span>, 2016.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
R. Zanibbi, D. Blostein, and R. Cordy, “A survey of table recognition: Models,
observations, transformations, and inferences,” <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">International Journal
on Document Analysis and Recognition</span>, vol. 7, no. 1, pp. 1–16, 2004.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. Hu, R. S. Kashi, D. P. Lopresti, and G. Wilfong, “Table structure
recognition and its evaluation,” in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Document Recognition and
Retrieval</span>, pp. 44–55, 2001.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Y. Wang, I. T. Phillips, and R. Haralick, “Automatic table ground truth
generation and a background-analysis-based table structure extraction
method,” in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Sixth International Conference on Document Analysis and
Recognition</span>, pp. 528–532, 2001.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T. Kasar, T. K. Bhowmik, and A. Belaïd, “Table information extraction and
structure recognition using query patterns,” in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">13th International
Conference on Document Analysis and Recognition</span>, pp. 1086–1090, 2015.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. H. Shamilian, H. S. Baird, and T. L. Wood, “A retargetable table reader,”
in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of the Fourth International Conference on Document
Analysis and Recognition</span>, vol. 1, pp. 158–163, 1997.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S. Siddiqui, M. I. Malik, S. Agne, A. Dengel, and S. Ahmed, “Decnt: Deep
deformable cnn for table detection,” <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 6,
pp. 74151–74161, 2018.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A. Gilani, S. R. Qasim, M. I. Malik, and F. Shafait, “Table detection using
deep learning,” in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">14th International Conference on Document Analysis
and Recognition</span>, pp. 771–776, 2017.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
F. Shafait, D. Keysers, and T. M. Breuel, “Efficient implementation of local
adaptive thresholding techniques using integral images,” in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">SPIE
Document recognition and retrieval XV</span>, vol. 6815, p. 681510, 2008.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural networks,”
<span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Signal Processing</span>, vol. 45, no. 11,
pp. 2673–2681, 1997.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of gated
recurrent neural networks on sequence modeling,” <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">CoRR</span>,
vol. abs/1412.3555, 2014.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Shahab, F. Shafait, T. Kieninger, and A. Dengel, “An open approach towards
the benchmarking of table structure recognition systems,” in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Document
Analysis Systems</span>, pp. 113–120, 2010.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2001.02500" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2001.02501" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2001.02501">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2001.02501" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2001.02502" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 21:15:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
