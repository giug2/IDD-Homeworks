<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.05007] Audio-Guided Fusion Techniques for Multimodal Emotion Analysis</title><meta property="og:description" content="In this paper, we propose a solution for the semi-supervised learning track (MER-SEMI) in MER2024. First, in order to enhance the performance of the feature extractor on sentiment classification tasks,
we fine-tuned vi…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Audio-Guided Fusion Techniques for Multimodal Emotion Analysis">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Audio-Guided Fusion Techniques for Multimodal Emotion Analysis">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.05007">

<!--Generated on Sun Oct  6 00:05:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Multimodal emotion recognition;Multimodal feature fusion;Self-supervised learning">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Audio-Guided Fusion Techniques for Multimodal Emotion Analysis</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pujin Shi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">
State Key Laboratory of Networking and Switching Technology 
<br class="ltx_break">School of Cyberspace Security 
<br class="ltx_break">Beijing University of Posts and Telecommunications</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:shipujin@bupt.edu.cn">shipujin@bupt.edu.cn</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fei Gao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_affiliation_institution">
State Key Laboratory of Networking and Switching Technology 
<br class="ltx_break">School of Cyberspace Security 
<br class="ltx_break">Beijing University of Posts and Telecommunications
</span><span id="id5.2.id2" class="ltx_text ltx_affiliation_city">Beijing</span><span id="id6.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:gaof@bupt.edu.cn">gaof@bupt.edu.cn</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id7.id1" class="ltx_p">In this paper, we propose a solution for the semi-supervised learning track (MER-SEMI) in MER2024. First, in order to enhance the performance of the feature extractor on sentiment classification tasks,
we fine-tuned video and text feature extractors, specifically CLIP-vit-large and Baichuan-13B, using labeled data. This approach effectively preserves the original emotional information conveyed in the videos. Second, we propose an Audio-Guided Transformer (AGT) fusion mechanism, which leverages the robustness of Hubert-large, showing superior effectiveness in fusing both inter-channel and intra-channel information. Third, To enhance the accuracy of the model, we iteratively apply self-supervised learning by using high-confidence unlabeled data as pseudo-labels. Finally, through black-box probing, we discovered an imbalanced data distribution between the training and test sets. Therefore, We adopt a prior-knowledge-based voting mechanism. The results demonstrate the effectiveness of our strategy, ultimately earning us third place in the MER-SEMI track.</p>
</div>
<div class="ltx_keywords">Multimodal emotion recognition;Multimodal feature fusion;Self-supervised learning
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing; November 1, 2024; Melbourne, VIC, Australia.</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing (MRAC ’24), November 1, 2024, Melbourne, VIC, Australia</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-1203-6/24/11</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3689092.3689414</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computer systems organization Neural networks</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Sentiment analysis</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Multi-task learning</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recently, emotion recognition has garnered attention due to its wide range of applications, such as affective computing<cite class="ltx_cite ltx_citemacro_citep">(Dahl and Harvey, <a href="#bib.bib6" title="" class="ltx_ref">2007</a>)</cite>, healthcare<cite class="ltx_cite ltx_citemacro_citep">(Feinberg et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">1986</a>)</cite>, human-computer interaction<cite class="ltx_cite ltx_citemacro_citep">(Mauss et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2013</a>; Ba and Hu, <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>, and market research<cite class="ltx_cite ltx_citemacro_citep">(Torres et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite>. Traditional emotion recognition methods utilize physical and physiological signals<cite class="ltx_cite ltx_citemacro_citep">(Kamble and Sengupta, <a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite>. These methods rely on specialized instruments, which can be time-consuming and labor-intensive to implement. Multimodal emotion recognition<cite class="ltx_cite ltx_citemacro_citep">(Abdullah et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>, on the other hand, extracts emotional representations from speech, visual and text modalities. It employs fusion mechanisms to integrate multimodal features for downstream emotion classification tasks, offering a more flexible and efficient approach<cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2022b</a>)</cite>.In studying emotions, two primary approaches are commonly adopted: the dimensional approach<cite class="ltx_cite ltx_citemacro_citep">(Mehrabian, <a href="#bib.bib23" title="" class="ltx_ref">1996</a>)</cite> and the discrete approach<cite class="ltx_cite ltx_citemacro_citep">(Plutchik and Kellerman, <a href="#bib.bib26" title="" class="ltx_ref">2013</a>)</cite>. The objective of the MER-SEMI track (MER 2024<cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite>) is to map samples to six correct emotion labels (worried, happy, neutral, angry, surprise, and sad), which fundamentally falls under the task of discrete emotion label classification. This requires us to maximize the retention of emotional information from each modality during feature fusion and simultaneously minimizing the interference of irrelevant information between different modalities<cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2022</a>, <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.05007/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="203" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Overview of our multimodal emotional feature fusion framework. (CBT: Context-based Transformer. AMF: Adaptive Multimodal Fusion.Stage 1: Train using labeled data; Stage 2: Generate pseudo-labels using unlabeled data and add them back to the training set for further training.)</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In the MER-SEMI track, there are two primary challenges: <span id="S1.p2.1.1" class="ltx_text ltx_font_bold">(1)</span> How to enhance the robustness ability of models using unsupervised or semi-supervised learning methods in the context of scarce labeled data; <span id="S1.p2.1.2" class="ltx_text ltx_font_bold">(2)</span> How to maximally retain the complementary emotional information across different modalities during multimodal feature fusion; To address <span id="S1.p2.1.3" class="ltx_text ltx_font_bold">(1)</span>, various methods have been proposed. For instance, Ding et al. <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite> introduced the Video-Audio Transformer, which aligns visual and audio modalities and incorporates contrastive loss for semi-supervised learning, achieving significant results<cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>. Similarly, Chen et al.<cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite> proposed a class-balanced pseudo-labeling strategy to select high-confidence pseudo-labeled samples for each category from unlabeled data. Moreover, You et al.<cite class="ltx_cite ltx_citemacro_citep">(You et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite> designed a Temporal-Alignment Attention mechanism to align speech and textual cues in a common space and used auxiliary self-supervised tasks to ensure the consistency and coherence of unlabeled data. Additionally, Yang et al.<cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2023b</a>)</cite> integrated inter-sample contrastive learning and intra-sample modality decomposition into a simple unified loss function, thereby simplifying the training process. To tackle <span id="S1.p2.1.4" class="ltx_text ltx_font_bold">(2)</span>, Mittal et al.<cite class="ltx_cite ltx_citemacro_citep">(Mittal et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite> proposed a learning model based on Long Short-Term Memory (LSTM) networks for emotion perception. In Chen et al.’s work <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>, large-scale unlabeled emotional video data were used to train a mask autoencoder (Expression MAE<cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2022a</a>)</cite>). Further, Wang et al.<cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite> designed three different structures based on Attention-Guided Feature Aggregation (AFG)<cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite> for deep feature fusion. Lastly, Praveen et al.<cite class="ltx_cite ltx_citemacro_citep">(Praveen et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite> proposed a Joint Cross-Attention model that extracts key emotional features. Overall, these methods aim to enhance the accuracy of emotion recognition models from the perspectives of pseudo-label construction and feature fusion.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Recently, researchers have noted that the contribution of the auditory modality is relatively more significant compared to visual and textual modalities<cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>; Pastor et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>.
Based on this conclusion, our main contributions are as follows:</p>
</div>
<div id="S1.p4" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We fine-tuned CLIP-vit-large<cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite> and Baichuan-13B<cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2023a</a>)</cite> using labeled data, allowing the feature extractors to focus on emotional information rather than unrelated background noise. We refer to these fine-tuned feature extractors as Fine-tuned-Clip-large and Fine-tuned-Baichuan-13B.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We improved CAMFNET<cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> and proposed Audio-Guided Transformer fusion mechanism (AGT) . This method uses audio modality features as the leading input for tri-modal fusion. It leverages the generalization of Hubert-large and has been found to more effectively integrate both inter-channel and intra-channel complementary information.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">To leverage unlabeled data as pseudo-labels , inspired by <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2023a</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2023</a>; Arazo et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2020</a>; Fang et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>, We select soft pseudo labels with a confidence higher than 0.9 for self-supervised training. Considering the imbalanced data distribution between training and test sets (Figure <a href="#S2.F3" title="Figure 3 ‣ 2.3. Pseudo-label and Voting Mechanism ‣ 2. Method ‣ Audio-Guided Fusion Techniques for Multimodal Emotion Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), we implemented a prior-knowledge-based voting mechanism <cite class="ltx_cite ltx_citemacro_citep">(Tanha et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2020</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2017</a>)</cite> that takes advantage of prediction distribution inconsistencies across different architectures, with special handling for imbalanced labels.</p>
</div>
</li>
</ul>
<p id="S1.p4.1" class="ltx_p">The overall framework of our model is illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Audio-Guided Fusion Techniques for Multimodal Emotion Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Method</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we will discuss our proposed multimodal emotion recognition system in three subsections. The selection and fine-tuning of feature extractors constitute the first part, the AGT fusion mechanism constitutes the second part, and the pseudo-label and voting mechanism constitutes the third part.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Fine-tuning the feature extractor</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Feature extractors, such as Hubert-large, CLIP-vit-large, and Baichuan-13B, perform well in single-modality scenarios<cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite>. However, they tend to exhibit poor generalization in multi-modality settings. We hypothesize that this may be due to the fact that these feature extractors are trained on public datasets, where they may learn redundant information that is not conducive to multimodal sentiment recognition<cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite>.Therefore, for the audio modality, we ultimately selected Hubert-large due to its superior generalization ability. This model effectively extracts semantic information from raw audio that complements other modalities. Additionally, for the visual and textual modalities, we initially selected CLIP-large and Baichuan-13B as the feature extractors. However, since both of these large models are trained on extensive wild datasets, their use for multimodal emotion recognition introduces a substantial amount of irrelevant information (e.g., background information in videos, emotion-irrelevant information in text). Consequently, we fine-tuned these two feature extractors using the labeled data<cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2024</a>; Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2022</a>; Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2022</a>; Xiao et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2024</a>)</cite>. The specific fine-tuning steps are illustrated in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.1. Fine-tuning the feature extractor ‣ 2. Method ‣ Audio-Guided Fusion Techniques for Multimodal Emotion Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.05007/assets/x2.png" id="S2.F2.sf1.g1" class="ltx_graphics ltx_img_square" width="161" height="177" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S2.F2.sf1.3.2" class="ltx_text" style="font-size:80%;">Fine-tuning CLIP-vit-large</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.05007/assets/x3.png" id="S2.F2.sf2.g1" class="ltx_graphics ltx_img_portrait" width="143" height="181" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S2.F2.sf2.3.2" class="ltx_text" style="font-size:80%;">Fine-tuning Baichuan-13B</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Fine-tuning the feature extractors for both visual and textual modalities</figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.8" class="ltx_p">Regarding the visual modality, based on <cite class="ltx_cite ltx_citemacro_citep">(Rasheed et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite>, better alignment of visual and textual representations in video tasks provides stronger generalization capabilities. Therefore, we performed alignment operations on 5030 labeled data samples using CLIP-vit-large. As shown in Figure<a href="#S2.F2.sf1" title="In Figure 2 ‣ 2.1. Fine-tuning the feature extractor ‣ 2. Method ‣ Audio-Guided Fusion Techniques for Multimodal Emotion Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a>, given a video sample <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="V_{i}\in\mathbb{R}^{T\times H\times W\times C}" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><msub id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2.2" xref="S2.SS1.p2.1.m1.1.1.2.2.cmml">V</mi><mi id="S2.SS1.p2.1.m1.1.1.2.3" xref="S2.SS1.p2.1.m1.1.1.2.3.cmml">i</mi></msub><mo id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml"><mi id="S2.SS1.p2.1.m1.1.1.3.3.2" xref="S2.SS1.p2.1.m1.1.1.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p2.1.m1.1.1.3.3.1" xref="S2.SS1.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p2.1.m1.1.1.3.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.3.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p2.1.m1.1.1.3.3.1a" xref="S2.SS1.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p2.1.m1.1.1.3.3.4" xref="S2.SS1.p2.1.m1.1.1.3.3.4.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p2.1.m1.1.1.3.3.1b" xref="S2.SS1.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p2.1.m1.1.1.3.3.5" xref="S2.SS1.p2.1.m1.1.1.3.3.5.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><in id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1"></in><apply id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.2.1.cmml" xref="S2.SS1.p2.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.2.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2.2">𝑉</ci><ci id="S2.SS1.p2.1.m1.1.1.2.3.cmml" xref="S2.SS1.p2.1.m1.1.1.2.3">𝑖</ci></apply><apply id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">ℝ</ci><apply id="S2.SS1.p2.1.m1.1.1.3.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3"><times id="S2.SS1.p2.1.m1.1.1.3.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3.1"></times><ci id="S2.SS1.p2.1.m1.1.1.3.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3.2">𝑇</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3.3">𝐻</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.4.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3.4">𝑊</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.5.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3.5">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">V_{i}\in\mathbb{R}^{T\times H\times W\times C}</annotation></semantics></math> with <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">T</annotation></semantics></math> frames and its corresponding text label Y, the CLIP image encoder independently encodes each of the <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mi id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">T</annotation></semantics></math> frames into a batch of images, resulting in frame-level embeddings <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="X_{i}\in\mathbb{R}^{T\times D}" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><mrow id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml"><msub id="S2.SS1.p2.4.m4.1.1.2" xref="S2.SS1.p2.4.m4.1.1.2.cmml"><mi id="S2.SS1.p2.4.m4.1.1.2.2" xref="S2.SS1.p2.4.m4.1.1.2.2.cmml">X</mi><mi id="S2.SS1.p2.4.m4.1.1.2.3" xref="S2.SS1.p2.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S2.SS1.p2.4.m4.1.1.1" xref="S2.SS1.p2.4.m4.1.1.1.cmml">∈</mo><msup id="S2.SS1.p2.4.m4.1.1.3" xref="S2.SS1.p2.4.m4.1.1.3.cmml"><mi id="S2.SS1.p2.4.m4.1.1.3.2" xref="S2.SS1.p2.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p2.4.m4.1.1.3.3" xref="S2.SS1.p2.4.m4.1.1.3.3.cmml"><mi id="S2.SS1.p2.4.m4.1.1.3.3.2" xref="S2.SS1.p2.4.m4.1.1.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p2.4.m4.1.1.3.3.1" xref="S2.SS1.p2.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p2.4.m4.1.1.3.3.3" xref="S2.SS1.p2.4.m4.1.1.3.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><apply id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1"><in id="S2.SS1.p2.4.m4.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1.1"></in><apply id="S2.SS1.p2.4.m4.1.1.2.cmml" xref="S2.SS1.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.1.1.2.1.cmml" xref="S2.SS1.p2.4.m4.1.1.2">subscript</csymbol><ci id="S2.SS1.p2.4.m4.1.1.2.2.cmml" xref="S2.SS1.p2.4.m4.1.1.2.2">𝑋</ci><ci id="S2.SS1.p2.4.m4.1.1.2.3.cmml" xref="S2.SS1.p2.4.m4.1.1.2.3">𝑖</ci></apply><apply id="S2.SS1.p2.4.m4.1.1.3.cmml" xref="S2.SS1.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.1.1.3.1.cmml" xref="S2.SS1.p2.4.m4.1.1.3">superscript</csymbol><ci id="S2.SS1.p2.4.m4.1.1.3.2.cmml" xref="S2.SS1.p2.4.m4.1.1.3.2">ℝ</ci><apply id="S2.SS1.p2.4.m4.1.1.3.3.cmml" xref="S2.SS1.p2.4.m4.1.1.3.3"><times id="S2.SS1.p2.4.m4.1.1.3.3.1.cmml" xref="S2.SS1.p2.4.m4.1.1.3.3.1"></times><ci id="S2.SS1.p2.4.m4.1.1.3.3.2.cmml" xref="S2.SS1.p2.4.m4.1.1.3.3.2">𝑇</ci><ci id="S2.SS1.p2.4.m4.1.1.3.3.3.cmml" xref="S2.SS1.p2.4.m4.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">X_{i}\in\mathbb{R}^{T\times D}</annotation></semantics></math>. The CLIP text encoder encodes class Y, wrapped in a prompt template such as ‘the person is ¡emo-label¿’, to produce a text embedding <math id="S2.SS1.p2.5.m5.1" class="ltx_Math" alttext="t\in\mathbb{R}^{D}" display="inline"><semantics id="S2.SS1.p2.5.m5.1a"><mrow id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml"><mi id="S2.SS1.p2.5.m5.1.1.2" xref="S2.SS1.p2.5.m5.1.1.2.cmml">t</mi><mo id="S2.SS1.p2.5.m5.1.1.1" xref="S2.SS1.p2.5.m5.1.1.1.cmml">∈</mo><msup id="S2.SS1.p2.5.m5.1.1.3" xref="S2.SS1.p2.5.m5.1.1.3.cmml"><mi id="S2.SS1.p2.5.m5.1.1.3.2" xref="S2.SS1.p2.5.m5.1.1.3.2.cmml">ℝ</mi><mi id="S2.SS1.p2.5.m5.1.1.3.3" xref="S2.SS1.p2.5.m5.1.1.3.3.cmml">D</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><apply id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1"><in id="S2.SS1.p2.5.m5.1.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1.1"></in><ci id="S2.SS1.p2.5.m5.1.1.2.cmml" xref="S2.SS1.p2.5.m5.1.1.2">𝑡</ci><apply id="S2.SS1.p2.5.m5.1.1.3.cmml" xref="S2.SS1.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p2.5.m5.1.1.3.1.cmml" xref="S2.SS1.p2.5.m5.1.1.3">superscript</csymbol><ci id="S2.SS1.p2.5.m5.1.1.3.2.cmml" xref="S2.SS1.p2.5.m5.1.1.3.2">ℝ</ci><ci id="S2.SS1.p2.5.m5.1.1.3.3.cmml" xref="S2.SS1.p2.5.m5.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">t\in\mathbb{R}^{D}</annotation></semantics></math>. For a batch of videos, the cosine similarity sim(.) maximizes the information between all video-level embeddings <math id="S2.SS1.p2.6.m6.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="S2.SS1.p2.6.m6.1a"><msub id="S2.SS1.p2.6.m6.1.1" xref="S2.SS1.p2.6.m6.1.1.cmml"><mi id="S2.SS1.p2.6.m6.1.1.2" xref="S2.SS1.p2.6.m6.1.1.2.cmml">v</mi><mi id="S2.SS1.p2.6.m6.1.1.3" xref="S2.SS1.p2.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m6.1b"><apply id="S2.SS1.p2.6.m6.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.6.m6.1.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.p2.6.m6.1.1.2.cmml" xref="S2.SS1.p2.6.m6.1.1.2">𝑣</ci><ci id="S2.SS1.p2.6.m6.1.1.3.cmml" xref="S2.SS1.p2.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.m6.1c">v_{i}</annotation></semantics></math> and their corresponding text embeddings <math id="S2.SS1.p2.7.m7.1" class="ltx_Math" alttext="t_{i}" display="inline"><semantics id="S2.SS1.p2.7.m7.1a"><msub id="S2.SS1.p2.7.m7.1.1" xref="S2.SS1.p2.7.m7.1.1.cmml"><mi id="S2.SS1.p2.7.m7.1.1.2" xref="S2.SS1.p2.7.m7.1.1.2.cmml">t</mi><mi id="S2.SS1.p2.7.m7.1.1.3" xref="S2.SS1.p2.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.7.m7.1b"><apply id="S2.SS1.p2.7.m7.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.7.m7.1.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1">subscript</csymbol><ci id="S2.SS1.p2.7.m7.1.1.2.cmml" xref="S2.SS1.p2.7.m7.1.1.2">𝑡</ci><ci id="S2.SS1.p2.7.m7.1.1.3.cmml" xref="S2.SS1.p2.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.7.m7.1c">t_{i}</annotation></semantics></math>, fine-tuning the CLIP model using cross-entropy (CE,(<a href="#S2.E1" title="In 2.1. Fine-tuning the feature extractor ‣ 2. Method ‣ Audio-Guided Fusion Techniques for Multimodal Emotion Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>)) objective with a temperature parameter <math id="S2.SS1.p2.8.m8.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S2.SS1.p2.8.m8.1a"><mi id="S2.SS1.p2.8.m8.1.1" xref="S2.SS1.p2.8.m8.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.8.m8.1b"><ci id="S2.SS1.p2.8.m8.1.1.cmml" xref="S2.SS1.p2.8.m8.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.8.m8.1c">\tau</annotation></semantics></math>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.5" class="ltx_Math" alttext="L=-\sum_{i}\log\left(\frac{\exp(sim(v_{i},t_{i})/\tau)}{\sum_{j}\exp(sim(v_{i},t_{j})/\tau)}\right)" display="block"><semantics id="S2.E1.m1.5a"><mrow id="S2.E1.m1.5.6" xref="S2.E1.m1.5.6.cmml"><mi id="S2.E1.m1.5.6.2" xref="S2.E1.m1.5.6.2.cmml">L</mi><mo id="S2.E1.m1.5.6.1" xref="S2.E1.m1.5.6.1.cmml">=</mo><mrow id="S2.E1.m1.5.6.3" xref="S2.E1.m1.5.6.3.cmml"><mo id="S2.E1.m1.5.6.3a" xref="S2.E1.m1.5.6.3.cmml">−</mo><mrow id="S2.E1.m1.5.6.3.2" xref="S2.E1.m1.5.6.3.2.cmml"><munder id="S2.E1.m1.5.6.3.2.1" xref="S2.E1.m1.5.6.3.2.1.cmml"><mo movablelimits="false" id="S2.E1.m1.5.6.3.2.1.2" xref="S2.E1.m1.5.6.3.2.1.2.cmml">∑</mo><mi id="S2.E1.m1.5.6.3.2.1.3" xref="S2.E1.m1.5.6.3.2.1.3.cmml">i</mi></munder><mrow id="S2.E1.m1.5.6.3.2.2.2" xref="S2.E1.m1.5.6.3.2.2.1.cmml"><mi id="S2.E1.m1.5.5" xref="S2.E1.m1.5.5.cmml">log</mi><mo id="S2.E1.m1.5.6.3.2.2.2a" xref="S2.E1.m1.5.6.3.2.2.1.cmml">⁡</mo><mrow id="S2.E1.m1.5.6.3.2.2.2.1" xref="S2.E1.m1.5.6.3.2.2.1.cmml"><mo id="S2.E1.m1.5.6.3.2.2.2.1.1" xref="S2.E1.m1.5.6.3.2.2.1.cmml">(</mo><mfrac id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml"><mrow id="S2.E1.m1.2.2.2.2" xref="S2.E1.m1.2.2.2.3.cmml"><mi id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">exp</mi><mo id="S2.E1.m1.2.2.2.2a" xref="S2.E1.m1.2.2.2.3.cmml">⁡</mo><mrow id="S2.E1.m1.2.2.2.2.1" xref="S2.E1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.2.1.2" xref="S2.E1.m1.2.2.2.3.cmml">(</mo><mrow id="S2.E1.m1.2.2.2.2.1.1" xref="S2.E1.m1.2.2.2.2.1.1.cmml"><mrow id="S2.E1.m1.2.2.2.2.1.1.2" xref="S2.E1.m1.2.2.2.2.1.1.2.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.2.4" xref="S2.E1.m1.2.2.2.2.1.1.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.2.1.1.2.3" xref="S2.E1.m1.2.2.2.2.1.1.2.3.cmml">​</mo><mi id="S2.E1.m1.2.2.2.2.1.1.2.5" xref="S2.E1.m1.2.2.2.2.1.1.2.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.2.1.1.2.3a" xref="S2.E1.m1.2.2.2.2.1.1.2.3.cmml">​</mo><mi id="S2.E1.m1.2.2.2.2.1.1.2.6" xref="S2.E1.m1.2.2.2.2.1.1.2.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.2.1.1.2.3b" xref="S2.E1.m1.2.2.2.2.1.1.2.3.cmml">​</mo><mrow id="S2.E1.m1.2.2.2.2.1.1.2.2.2" xref="S2.E1.m1.2.2.2.2.1.1.2.2.3.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.2.1.1.2.2.2.3" xref="S2.E1.m1.2.2.2.2.1.1.2.2.3.cmml">(</mo><msub id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.2.cmml">v</mi><mi id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E1.m1.2.2.2.2.1.1.2.2.2.4" xref="S2.E1.m1.2.2.2.2.1.1.2.2.3.cmml">,</mo><msub id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.2.cmml">t</mi><mi id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.3" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E1.m1.2.2.2.2.1.1.2.2.2.5" xref="S2.E1.m1.2.2.2.2.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.2.2.1.1.3" xref="S2.E1.m1.2.2.2.2.1.1.3.cmml">/</mo><mi id="S2.E1.m1.2.2.2.2.1.1.4" xref="S2.E1.m1.2.2.2.2.1.1.4.cmml">τ</mi></mrow><mo stretchy="false" id="S2.E1.m1.2.2.2.2.1.3" xref="S2.E1.m1.2.2.2.3.cmml">)</mo></mrow></mrow><mrow id="S2.E1.m1.4.4.4" xref="S2.E1.m1.4.4.4.cmml"><msub id="S2.E1.m1.4.4.4.3" xref="S2.E1.m1.4.4.4.3.cmml"><mo id="S2.E1.m1.4.4.4.3.2" xref="S2.E1.m1.4.4.4.3.2.cmml">∑</mo><mi id="S2.E1.m1.4.4.4.3.3" xref="S2.E1.m1.4.4.4.3.3.cmml">j</mi></msub><mrow id="S2.E1.m1.4.4.4.2.1" xref="S2.E1.m1.4.4.4.2.2.cmml"><mi id="S2.E1.m1.3.3.3.1" xref="S2.E1.m1.3.3.3.1.cmml">exp</mi><mo id="S2.E1.m1.4.4.4.2.1a" xref="S2.E1.m1.4.4.4.2.2.cmml">⁡</mo><mrow id="S2.E1.m1.4.4.4.2.1.1" xref="S2.E1.m1.4.4.4.2.2.cmml"><mo stretchy="false" id="S2.E1.m1.4.4.4.2.1.1.2" xref="S2.E1.m1.4.4.4.2.2.cmml">(</mo><mrow id="S2.E1.m1.4.4.4.2.1.1.1" xref="S2.E1.m1.4.4.4.2.1.1.1.cmml"><mrow id="S2.E1.m1.4.4.4.2.1.1.1.2" xref="S2.E1.m1.4.4.4.2.1.1.1.2.cmml"><mi id="S2.E1.m1.4.4.4.2.1.1.1.2.4" xref="S2.E1.m1.4.4.4.2.1.1.1.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.4.2.1.1.1.2.3" xref="S2.E1.m1.4.4.4.2.1.1.1.2.3.cmml">​</mo><mi id="S2.E1.m1.4.4.4.2.1.1.1.2.5" xref="S2.E1.m1.4.4.4.2.1.1.1.2.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.4.2.1.1.1.2.3a" xref="S2.E1.m1.4.4.4.2.1.1.1.2.3.cmml">​</mo><mi id="S2.E1.m1.4.4.4.2.1.1.1.2.6" xref="S2.E1.m1.4.4.4.2.1.1.1.2.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.4.2.1.1.1.2.3b" xref="S2.E1.m1.4.4.4.2.1.1.1.2.3.cmml">​</mo><mrow id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.3" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.3.cmml">(</mo><msub id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.2" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.2.cmml">v</mi><mi id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.4" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.3.cmml">,</mo><msub id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.cmml"><mi id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.2" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.2.cmml">t</mi><mi id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3.cmml">j</mi></msub><mo stretchy="false" id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.5" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.4.4.4.2.1.1.1.3" xref="S2.E1.m1.4.4.4.2.1.1.1.3.cmml">/</mo><mi id="S2.E1.m1.4.4.4.2.1.1.1.4" xref="S2.E1.m1.4.4.4.2.1.1.1.4.cmml">τ</mi></mrow><mo stretchy="false" id="S2.E1.m1.4.4.4.2.1.1.3" xref="S2.E1.m1.4.4.4.2.2.cmml">)</mo></mrow></mrow></mrow></mfrac><mo id="S2.E1.m1.5.6.3.2.2.2.1.2" xref="S2.E1.m1.5.6.3.2.2.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.5b"><apply id="S2.E1.m1.5.6.cmml" xref="S2.E1.m1.5.6"><eq id="S2.E1.m1.5.6.1.cmml" xref="S2.E1.m1.5.6.1"></eq><ci id="S2.E1.m1.5.6.2.cmml" xref="S2.E1.m1.5.6.2">𝐿</ci><apply id="S2.E1.m1.5.6.3.cmml" xref="S2.E1.m1.5.6.3"><minus id="S2.E1.m1.5.6.3.1.cmml" xref="S2.E1.m1.5.6.3"></minus><apply id="S2.E1.m1.5.6.3.2.cmml" xref="S2.E1.m1.5.6.3.2"><apply id="S2.E1.m1.5.6.3.2.1.cmml" xref="S2.E1.m1.5.6.3.2.1"><csymbol cd="ambiguous" id="S2.E1.m1.5.6.3.2.1.1.cmml" xref="S2.E1.m1.5.6.3.2.1">subscript</csymbol><sum id="S2.E1.m1.5.6.3.2.1.2.cmml" xref="S2.E1.m1.5.6.3.2.1.2"></sum><ci id="S2.E1.m1.5.6.3.2.1.3.cmml" xref="S2.E1.m1.5.6.3.2.1.3">𝑖</ci></apply><apply id="S2.E1.m1.5.6.3.2.2.1.cmml" xref="S2.E1.m1.5.6.3.2.2.2"><log id="S2.E1.m1.5.5.cmml" xref="S2.E1.m1.5.5"></log><apply id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.4"><divide id="S2.E1.m1.4.4.5.cmml" xref="S2.E1.m1.4.4"></divide><apply id="S2.E1.m1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2"><exp id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1"></exp><apply id="S2.E1.m1.2.2.2.2.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1"><divide id="S2.E1.m1.2.2.2.2.1.1.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3"></divide><apply id="S2.E1.m1.2.2.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2"><times id="S2.E1.m1.2.2.2.2.1.1.2.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.3"></times><ci id="S2.E1.m1.2.2.2.2.1.1.2.4.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.4">𝑠</ci><ci id="S2.E1.m1.2.2.2.2.1.1.2.5.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.5">𝑖</ci><ci id="S2.E1.m1.2.2.2.2.1.1.2.6.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.6">𝑚</ci><interval closure="open" id="S2.E1.m1.2.2.2.2.1.1.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2"><apply id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.2">𝑣</ci><ci id="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.2">𝑡</ci><ci id="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2.2.2.3">𝑖</ci></apply></interval></apply><ci id="S2.E1.m1.2.2.2.2.1.1.4.cmml" xref="S2.E1.m1.2.2.2.2.1.1.4">𝜏</ci></apply></apply><apply id="S2.E1.m1.4.4.4.cmml" xref="S2.E1.m1.4.4.4"><apply id="S2.E1.m1.4.4.4.3.cmml" xref="S2.E1.m1.4.4.4.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.4.3.1.cmml" xref="S2.E1.m1.4.4.4.3">subscript</csymbol><sum id="S2.E1.m1.4.4.4.3.2.cmml" xref="S2.E1.m1.4.4.4.3.2"></sum><ci id="S2.E1.m1.4.4.4.3.3.cmml" xref="S2.E1.m1.4.4.4.3.3">𝑗</ci></apply><apply id="S2.E1.m1.4.4.4.2.2.cmml" xref="S2.E1.m1.4.4.4.2.1"><exp id="S2.E1.m1.3.3.3.1.cmml" xref="S2.E1.m1.3.3.3.1"></exp><apply id="S2.E1.m1.4.4.4.2.1.1.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1"><divide id="S2.E1.m1.4.4.4.2.1.1.1.3.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.3"></divide><apply id="S2.E1.m1.4.4.4.2.1.1.1.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2"><times id="S2.E1.m1.4.4.4.2.1.1.1.2.3.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.3"></times><ci id="S2.E1.m1.4.4.4.2.1.1.1.2.4.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.4">𝑠</ci><ci id="S2.E1.m1.4.4.4.2.1.1.1.2.5.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.5">𝑖</ci><ci id="S2.E1.m1.4.4.4.2.1.1.1.2.6.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.6">𝑚</ci><interval closure="open" id="S2.E1.m1.4.4.4.2.1.1.1.2.2.3.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2"><apply id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.2">𝑣</ci><ci id="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.1.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2">subscript</csymbol><ci id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.2.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.2">𝑡</ci><ci id="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.2.2.2.2.3">𝑗</ci></apply></interval></apply><ci id="S2.E1.m1.4.4.4.2.1.1.1.4.cmml" xref="S2.E1.m1.4.4.4.2.1.1.1.4">𝜏</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.5c">L=-\sum_{i}\log\left(\frac{\exp(sim(v_{i},t_{i})/\tau)}{\sum_{j}\exp(sim(v_{i},t_{j})/\tau)}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">For the textual modality,instead of fine-tuning the model, we adopted the approach of prompt tuning<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2023b</a>)</cite> to generate refined prompts.Specifically, we utilized GPT-4 to perform emotion recognition on text using labeled data within six predefined emotion categories (0: worry, 1: happiness, 2: neutral, 3: anger, 4: surprise, and 5: sadness), outputting the classifications in order of descending likelihood as shown in Figure <a href="#S2.F2.sf2" title="In Figure 2 ‣ 2.1. Fine-tuning the feature extractor ‣ 2. Method ‣ Audio-Guided Fusion Techniques for Multimodal Emotion Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>. Subsequently, we concatenated the original text information with the output labels and fed this combined input into Baichuan-13B to obtain fine-tuned textual features.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>AGT Fusion Mechanism</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.5" class="ltx_p">As illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Audio-Guided Fusion Techniques for Multimodal Emotion Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we extract features from each modality using the selected feature extractors. Subsequently, we use the semantic features extracted by Hubert to guide visual features and textual features independently, using the Context-Based Transformer (CBT) module <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite>. This module primarily extracts deep information within channels. it effectively integrates both global and local information using a multi-head self-attention mechanism, leveraging skip connections and feedforward layers to prevent overfitting and enhance the model’s linear representation capabilities.
During the fusion process, we employed Adaptive Multimodal Fusion (AMF) <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite>, which addresses the issue of different modalities conveying disparate emotions. This method utilizes a multi-head self-attention mechanism to calculate the similarity between different modalities. AMF can determine whether a particular modality expresses an emotion distinct from the others. If a specific modality exhibits very low similarity with all other modalities, it is considered to express a different emotion, and its features are thus zeroed out to eliminate interfering information.</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.1" class="ltx_Math" alttext="X_{f}=f_{AMF}{(f_{CBT}{(cat[A_{i},V_{i}])}+f_{CBT}{(cat[A_{i},T_{i}])})}" display="block"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml"><msub id="S2.E2.m1.1.1.3" xref="S2.E2.m1.1.1.3.cmml"><mi id="S2.E2.m1.1.1.3.2" xref="S2.E2.m1.1.1.3.2.cmml">X</mi><mi id="S2.E2.m1.1.1.3.3" xref="S2.E2.m1.1.1.3.3.cmml">f</mi></msub><mo id="S2.E2.m1.1.1.2" xref="S2.E2.m1.1.1.2.cmml">=</mo><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.cmml"><msub id="S2.E2.m1.1.1.1.3" xref="S2.E2.m1.1.1.1.3.cmml"><mi id="S2.E2.m1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.3.2.cmml">f</mi><mrow id="S2.E2.m1.1.1.1.3.3" xref="S2.E2.m1.1.1.1.3.3.cmml"><mi id="S2.E2.m1.1.1.1.3.3.2" xref="S2.E2.m1.1.1.1.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.3.3.1" xref="S2.E2.m1.1.1.1.3.3.1.cmml">​</mo><mi id="S2.E2.m1.1.1.1.3.3.3" xref="S2.E2.m1.1.1.1.3.3.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.3.3.1a" xref="S2.E2.m1.1.1.1.3.3.1.cmml">​</mo><mi id="S2.E2.m1.1.1.1.3.3.4" xref="S2.E2.m1.1.1.1.3.3.4.cmml">F</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.2" xref="S2.E2.m1.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml"><mrow id="S2.E2.m1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.cmml"><msub id="S2.E2.m1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.1.1.1.1.3.2.cmml">f</mi><mrow id="S2.E2.m1.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.1.3.3.2" xref="S2.E2.m1.1.1.1.1.1.1.1.3.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.1.1.1.1.3.3.1" xref="S2.E2.m1.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S2.E2.m1.1.1.1.1.1.1.1.3.3.3" xref="S2.E2.m1.1.1.1.1.1.1.1.3.3.3.cmml">B</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.1.1.1.1.3.3.1a" xref="S2.E2.m1.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S2.E2.m1.1.1.1.1.1.1.1.3.3.4" xref="S2.E2.m1.1.1.1.1.1.1.1.3.3.4.cmml">T</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.4" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">​</mo><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.5" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.3a" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">​</mo><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.6" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.3b" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">​</mo><mrow id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">[</mo><msub id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">A</mi><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.4" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">V</mi><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.5" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">]</mo></mrow></mrow><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.3.cmml">+</mo><mrow id="S2.E2.m1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.2.cmml"><msub id="S2.E2.m1.1.1.1.1.1.1.2.3" xref="S2.E2.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.2.3.2" xref="S2.E2.m1.1.1.1.1.1.1.2.3.2.cmml">f</mi><mrow id="S2.E2.m1.1.1.1.1.1.1.2.3.3" xref="S2.E2.m1.1.1.1.1.1.1.2.3.3.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.2.3.3.2" xref="S2.E2.m1.1.1.1.1.1.1.2.3.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.1.1.1.2.3.3.1" xref="S2.E2.m1.1.1.1.1.1.1.2.3.3.1.cmml">​</mo><mi id="S2.E2.m1.1.1.1.1.1.1.2.3.3.3" xref="S2.E2.m1.1.1.1.1.1.1.2.3.3.3.cmml">B</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.1.1.1.2.3.3.1a" xref="S2.E2.m1.1.1.1.1.1.1.2.3.3.1.cmml">​</mo><mi id="S2.E2.m1.1.1.1.1.1.1.2.3.3.4" xref="S2.E2.m1.1.1.1.1.1.1.2.3.3.4.cmml">T</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.1.1.1.2.2" xref="S2.E2.m1.1.1.1.1.1.1.2.2.cmml">​</mo><mrow id="S2.E2.m1.1.1.1.1.1.1.2.1.1" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.1.2.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.4" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.3.cmml">​</mo><mi id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.5" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.3a" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.3.cmml">​</mo><mi id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.6" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.3b" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.3.cmml">​</mo><mrow id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.3.cmml"><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.3" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.3.cmml">[</mo><msub id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.cmml">A</mi><mi id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.4" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.3.cmml">,</mo><msub id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.2" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.2.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.2.2" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.2.2.cmml">T</mi><mi id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.2.3" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.5" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.3.cmml">]</mo></mrow></mrow><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.1.2.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1"><eq id="S2.E2.m1.1.1.2.cmml" xref="S2.E2.m1.1.1.2"></eq><apply id="S2.E2.m1.1.1.3.cmml" xref="S2.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.3.2">𝑋</ci><ci id="S2.E2.m1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.3.3">𝑓</ci></apply><apply id="S2.E2.m1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><times id="S2.E2.m1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.2"></times><apply id="S2.E2.m1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.3.2">𝑓</ci><apply id="S2.E2.m1.1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.1.3.3"><times id="S2.E2.m1.1.1.1.3.3.1.cmml" xref="S2.E2.m1.1.1.1.3.3.1"></times><ci id="S2.E2.m1.1.1.1.3.3.2.cmml" xref="S2.E2.m1.1.1.1.3.3.2">𝐴</ci><ci id="S2.E2.m1.1.1.1.3.3.3.cmml" xref="S2.E2.m1.1.1.1.3.3.3">𝑀</ci><ci id="S2.E2.m1.1.1.1.3.3.4.cmml" xref="S2.E2.m1.1.1.1.3.3.4">𝐹</ci></apply></apply><apply id="S2.E2.m1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1"><plus id="S2.E2.m1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3"></plus><apply id="S2.E2.m1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1"><times id="S2.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.2"></times><apply id="S2.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.3.2">𝑓</ci><apply id="S2.E2.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.3.3"><times id="S2.E2.m1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.3.3.1"></times><ci id="S2.E2.m1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.3.3.2">𝐶</ci><ci id="S2.E2.m1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.3.3.3">𝐵</ci><ci id="S2.E2.m1.1.1.1.1.1.1.1.3.3.4.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.3.3.4">𝑇</ci></apply></apply><apply id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1"><times id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.3"></times><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.4">𝑐</ci><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.5.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.5">𝑎</ci><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.6.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.6">𝑡</ci><interval closure="closed" id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2"><apply id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝐴</ci><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.2">𝑉</ci><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.2.2.2.3">𝑖</ci></apply></interval></apply></apply><apply id="S2.E2.m1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2"><times id="S2.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.2"></times><apply id="S2.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.3.2">𝑓</ci><apply id="S2.E2.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.3.3"><times id="S2.E2.m1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.3.3.1"></times><ci id="S2.E2.m1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.3.3.2">𝐶</ci><ci id="S2.E2.m1.1.1.1.1.1.1.2.3.3.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.3.3.3">𝐵</ci><ci id="S2.E2.m1.1.1.1.1.1.1.2.3.3.4.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.3.3.4">𝑇</ci></apply></apply><apply id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1"><times id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.3"></times><ci id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.4.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.4">𝑐</ci><ci id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.5.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.5">𝑎</ci><ci id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.6.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.6">𝑡</ci><interval closure="closed" id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2"><apply id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.2">𝐴</ci><ci id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.2">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.2.2">𝑇</ci><ci id="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.1.1.1.2.2.2.3">𝑖</ci></apply></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">X_{f}=f_{AMF}{(f_{CBT}{(cat[A_{i},V_{i}])}+f_{CBT}{(cat[A_{i},T_{i}])})}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS2.p1.4" class="ltx_p">where <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="X_{f}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><msub id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">X</mi><mi id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">𝑋</ci><ci id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">X_{f}</annotation></semantics></math> represents the fused feature, while <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="A_{i}" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><msub id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">A</mi><mi id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">𝐴</ci><ci id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">A_{i}</annotation></semantics></math>, <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="V_{i}" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><msub id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml"><mi id="S2.SS2.p1.3.m3.1.1.2" xref="S2.SS2.p1.3.m3.1.1.2.cmml">V</mi><mi id="S2.SS2.p1.3.m3.1.1.3" xref="S2.SS2.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.p1.3.m3.1.1.2">𝑉</ci><ci id="S2.SS2.p1.3.m3.1.1.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">V_{i}</annotation></semantics></math>, and <math id="S2.SS2.p1.4.m4.1" class="ltx_Math" alttext="T_{i}" display="inline"><semantics id="S2.SS2.p1.4.m4.1a"><msub id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml"><mi id="S2.SS2.p1.4.m4.1.1.2" xref="S2.SS2.p1.4.m4.1.1.2.cmml">T</mi><mi id="S2.SS2.p1.4.m4.1.1.3" xref="S2.SS2.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><apply id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.4.m4.1.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.p1.4.m4.1.1.2.cmml" xref="S2.SS2.p1.4.m4.1.1.2">𝑇</ci><ci id="S2.SS2.p1.4.m4.1.1.3.cmml" xref="S2.SS2.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">T_{i}</annotation></semantics></math> denote the audio, visual, and textual modality features of the (i)-th sample, respectively.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Pseudo-label and Voting Mechanism</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In order to verify if the distributions of the training and testing sets are consistent (which has implications for the necessity of targeted data augmentation), we calculated the quantity of data for each label in the training set (worried: 616, happy: 1038, neutral: 1248, sad: 730, angry: 1208, surprise: 190), and estimated the approximate distribution of the test dataset through F1-scores obtained by black-box probing (worried: 0.0326, happy: 0.0732, neutral: 0.0505, sad: 0.1157, angry: 0.03412, surprise: 0.0094). As shown in Figure<a href="#S2.F3" title="Figure 3 ‣ 2.3. Pseudo-label and Voting Mechanism ‣ 2. Method ‣ Audio-Guided Fusion Techniques for Multimodal Emotion Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the statistical results indicate a significant discrepancy between the distributions of the test set and training set, particularly for the ’sad’ and ’worried’ labels. Consequently, low-confidence labels are prone to misclassification during pseudo-label generation (e.g., The ground truth label is ’sad’, but the predicted result is ’worried).</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2409.05007/assets/leida.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="402" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Distribution of the training and testing sets across the six emotional labels</figcaption>
</figure>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Based on this, we first generated pseudo-labels with high confidence ( p ¿ 0.9 ) on the test set using Hubert-large, the baseline model, and AGT, and then added the intersection of these results to the training set for iterative training. To fully leverage the generalization potential of Hubert-large, we developed a prior-knowledge-based regularized voting mechanism: when the three models make predictions for ’happy’, ’neutral’, ’surprise’, and ’angry’, we adopt a majority voting principle. However, when ’worried’ and ’sad’ appear in the predictions, to mitigate the impact of overfitting, we use a probabilistic approach to control the weight of predictions from each model. Specifically, 80% of the final prediction is based on the Hubert-large results, and the remaining 20% is from either the baseline model or AGT. This prior-knowledge-based regularized voting mechanism contributed significantly to the improvement of the F1-score.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Experiments and Results</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we conduct ablation experiments and analyze the results. First, we perform comparative experiments on features from two and three modalities, using different feature extractors and fusion mechanisms. Second, we validate the effectiveness of various strategies by incorporating them into the experiments.Note that the evaluation metric for the following experiments is the F1-score.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">In Table <a href="#S4.T1" title="Table 1 ‣ 4. Conclusion ‣ Audio-Guided Fusion Techniques for Multimodal Emotion Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we observe that the fine-tuned feature extractors exhibit significant effectiveness when used in combination, but their performance declines when used individually. We propose a possible reason for this phenomenon: the fine-tuning alignment process may enhance the complementary capabilities between the video and text modalities, but it could also result in some modality competition between the audio and video modalities. Additionally, for the same combinations of feature extractors, the AGT fusion mechanism consistently achieves higher F1-scores on both Train&amp;Val and MER-SEMI datasets compared to the baseline fusion mechanism. This indicates that the AGT fusion mechanism is more effective in capturing complementary information across different modalities during cross-modal feature fusion.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Furthermore, the results in Table <a href="#S4.T2" title="Table 2 ‣ 4. Conclusion ‣ Audio-Guided Fusion Techniques for Multimodal Emotion Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> indicate that the use of self-supervised pseudo-labels (which were obtained by filtering soft labels with a confidence level greater than 0.9 after applying softmax.) training methods can effectively improve the accuracy of model predictions, both in the baseline model and AGT. The proposed prior-knowledge-based voting mechanism has also demonstrated its effectiveness in ablation experiments. This method leverages the imbalanced data distribution between the training and test sets as well as the robustness of Hubert-large to regularize the prediction results with a certain probability.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we fine-tune CLIP-large and Baichuan-13B as feature extractors and propose an Audio-Guided Transformer (AGT) architecture. This fusion mechanism eliminates redundant information between modalities while capturing deeper emotional representations. Additionally, we employ self-supervised learning using pseudo-labels. Given the prior knowledge of the imbalanced distribution between training and test data, we assign higher weights to the more generalizable Hubert-large for imbalanced label predictions and use a voting mechanism to improve model prediction accuracy. Ultimately, these methods helped us achieve a score of 89.83% in the MER-SEMI 2024 track.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Performance of the baseline model and AGT across various feature combinations used as input. Specifically,
\scriptsize{0}⃝,
\scriptsize{1}⃝,
\scriptsize{2}⃝,
\scriptsize{3}⃝ and
\scriptsize{4}⃝ correspond to Hubert-large, Clip-ViT-large, Baichuan-13B, Finetuned-Clip, and Finetuned-Baichuan-13B, respectively.</figcaption>
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.3.1" class="ltx_tr">
<th id="S4.T1.2.3.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="width:42.7pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.3.1.1.1.1" class="ltx_p">Model</span>
</span>
</th>
<th id="S4.T1.2.3.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.3.1.2.1.1" class="ltx_p">Features</span>
</span>
</th>
<th id="S4.T1.2.3.1.3" class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2">Performance</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2" class="ltx_tr">
<td id="S4.T1.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r" style="width:42.7pt;padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T1.2.2.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:56.9pt;padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.1" class="ltx_p"><span id="S4.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Train&amp;Val<math id="S4.T1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></span>
</span>
</td>
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.2.2.1.1" class="ltx_p"><span id="S4.T1.2.2.2.1.1.1" class="ltx_text ltx_font_bold">MER-SEMI<math id="S4.T1.2.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.2.2.2.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.2.2.2.1.1.1.m1.1.1" xref="S4.T1.2.2.2.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.1.1.1.m1.1b"><ci id="S4.T1.2.2.2.1.1.1.m1.1.1.cmml" xref="S4.T1.2.2.2.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></span>
</span>
</td>
</tr>
<tr id="S4.T1.2.4.1" class="ltx_tr">
<td id="S4.T1.2.4.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:42.7pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.4.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.4.1.1.1.1" class="ltx_p">BASELINE</span>
</span>
</td>
<td id="S4.T1.2.4.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.4.1.2.1.1" class="ltx_p">\scriptsize{0}⃝+\scriptsize{1}⃝</span>
</span>
</td>
<td id="S4.T1.2.4.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.4.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.4.1.3.1.1" class="ltx_p">78.77</span>
</span>
</td>
<td id="S4.T1.2.4.1.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.4.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.4.1.4.1.1" class="ltx_p">84.34</span>
</span>
</td>
</tr>
<tr id="S4.T1.2.5.2" class="ltx_tr">
<td id="S4.T1.2.5.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r" style="width:42.7pt;padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T1.2.5.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.5.2.2.1.1" class="ltx_p">\scriptsize{0}⃝+\scriptsize{3}⃝</span>
</span>
</td>
<td id="S4.T1.2.5.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.5.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.5.2.3.1.1" class="ltx_p">78.85</span>
</span>
</td>
<td id="S4.T1.2.5.2.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.5.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.5.2.4.1.1" class="ltx_p">82.96</span>
</span>
</td>
</tr>
<tr id="S4.T1.2.6.3" class="ltx_tr">
<td id="S4.T1.2.6.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r" style="width:42.7pt;padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T1.2.6.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.6.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.6.3.2.1.1" class="ltx_p">\scriptsize{0}⃝+\scriptsize{1}⃝+\scriptsize{2}⃝</span>
</span>
</td>
<td id="S4.T1.2.6.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.6.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.6.3.3.1.1" class="ltx_p">79.34</span>
</span>
</td>
<td id="S4.T1.2.6.3.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.6.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.6.3.4.1.1" class="ltx_p">86.86</span>
</span>
</td>
</tr>
<tr id="S4.T1.2.7.4" class="ltx_tr">
<td id="S4.T1.2.7.4.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r" style="width:42.7pt;padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T1.2.7.4.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.7.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.7.4.2.1.1" class="ltx_p">\scriptsize{0}⃝+\scriptsize{3}⃝+\scriptsize{4}⃝</span>
</span>
</td>
<td id="S4.T1.2.7.4.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.7.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.7.4.3.1.1" class="ltx_p">80.68</span>
</span>
</td>
<td id="S4.T1.2.7.4.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.7.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.7.4.4.1.1" class="ltx_p">87.66</span>
</span>
</td>
</tr>
<tr id="S4.T1.2.8.5" class="ltx_tr">
<td id="S4.T1.2.8.5.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:42.7pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.8.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.8.5.1.1.1" class="ltx_p">AGT</span>
</span>
</td>
<td id="S4.T1.2.8.5.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.8.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.8.5.2.1.1" class="ltx_p">\scriptsize{0}⃝+\scriptsize{1}⃝</span>
</span>
</td>
<td id="S4.T1.2.8.5.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.8.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.8.5.3.1.1" class="ltx_p">79.12</span>
</span>
</td>
<td id="S4.T1.2.8.5.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.8.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.8.5.4.1.1" class="ltx_p">83.43</span>
</span>
</td>
</tr>
<tr id="S4.T1.2.9.6" class="ltx_tr">
<td id="S4.T1.2.9.6.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r" style="width:42.7pt;padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T1.2.9.6.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.9.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.9.6.2.1.1" class="ltx_p">\scriptsize{0}⃝+\scriptsize{3}⃝</span>
</span>
</td>
<td id="S4.T1.2.9.6.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.9.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.9.6.3.1.1" class="ltx_p">79.55</span>
</span>
</td>
<td id="S4.T1.2.9.6.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.9.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.9.6.4.1.1" class="ltx_p">83.73</span>
</span>
</td>
</tr>
<tr id="S4.T1.2.10.7" class="ltx_tr">
<td id="S4.T1.2.10.7.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r" style="width:42.7pt;padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T1.2.10.7.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.10.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.10.7.2.1.1" class="ltx_p">\scriptsize{0}⃝+\scriptsize{1}⃝+\scriptsize{2}⃝</span>
</span>
</td>
<td id="S4.T1.2.10.7.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.10.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.10.7.3.1.1" class="ltx_p">82.89</span>
</span>
</td>
<td id="S4.T1.2.10.7.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.10.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.10.7.4.1.1" class="ltx_p">87.51</span>
</span>
</td>
</tr>
<tr id="S4.T1.2.11.8" class="ltx_tr">
<td id="S4.T1.2.11.8.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r" style="width:42.7pt;padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S4.T1.2.11.8.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.11.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.11.8.2.1.1" class="ltx_p">\scriptsize{0}⃝+\scriptsize{3}⃝+\scriptsize{4}⃝</span>
</span>
</td>
<td id="S4.T1.2.11.8.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.11.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.11.8.3.1.1" class="ltx_p"><span id="S4.T1.2.11.8.3.1.1.1" class="ltx_text ltx_font_bold">83.22</span></span>
</span>
</td>
<td id="S4.T1.2.11.8.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T1.2.11.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.11.8.4.1.1" class="ltx_p"><span id="S4.T1.2.11.8.4.1.1.1" class="ltx_text ltx_font_bold">88.28</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Results of different strategies. Please note that the audio modality consistently utilizes Hubert-large as the feature extractor. Meanwhile, the video and text modalities employ finetuned CLIP and finetuned Baichuan-13b as feature extractors respectively. ‘N’ represents no additional strategy, ‘P’ represents the use of pseudo labels, and ‘V’ represents the use of the voting mechanism.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" rowspan="2"><span id="S4.T2.1.1.1.1.1" class="ltx_text">Strategy</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="3">BASELINE</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="3">AGT</th>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<td id="S4.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">N</td>
<td id="S4.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">P</td>
<td id="S4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">P + V</td>
<td id="S4.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">N</td>
<td id="S4.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">P</td>
<td id="S4.T2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">P + V</td>
</tr>
<tr id="S4.T2.1.3.3" class="ltx_tr">
<td id="S4.T2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.3.3.1.1" class="ltx_text">MER-SEMI</span></td>
<td id="S4.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">87.61</td>
<td id="S4.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">88.64</td>
<td id="S4.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">89.02</td>
<td id="S4.T2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">88.28</td>
<td id="S4.T2.1.3.3.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">89.26</td>
<td id="S4.T2.1.3.3.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.1.3.3.7.1" class="ltx_text ltx_font_bold">89.83</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdullah et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Sharmeen M Saleem Abdullah Abdullah, Siddeeq Y Ameen Ameen, Mohammed AM Sadeeq, and Subhi Zeebaree. 2021.

</span>
<span class="ltx_bibblock">Multimodal emotion recognition using deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">Journal of Applied Science and Technology Trends</em> 2, 01 (2021), 73–79.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arazo et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness. 2020.

</span>
<span class="ltx_bibblock">Pseudo-labeling and confirmation bias in deep semi-supervised learning. In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">2020 International joint conference on neural networks (IJCNN)</em>. IEEE, 1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ba and Hu (2023)</span>
<span class="ltx_bibblock">
Shen Ba and Xiao Hu. 2023.

</span>
<span class="ltx_bibblock">Measuring emotions in education using wearable devices: A systematic review.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Computers &amp; Education</em> 200 (2023), 104797.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Haifeng Chen, Chujia Guo, Yan Li, Peng Zhang, and Dongmei Jiang. 2023.

</span>
<span class="ltx_bibblock">Semi-Supervised Multimodal Emotion Recognition with Class-Balanced Pseudo-labeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st ACM International Conference on Multimedia</em> (2023).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:264492319" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:264492319</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dahl and Harvey (2007)</span>
<span class="ltx_bibblock">
Ronald E Dahl and Allison G Harvey. 2007.

</span>
<span class="ltx_bibblock">Sleep in children and adolescents with behavioral and emotional disorders.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Sleep medicine clinics</em> 2, 3 (2007), 501–511.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Chaoyue Ding, Daoming Zong, Baoxiang Li, Ken Zheng, Dinghao Zhou, Jiakui Li, and Qunyan Zhou. 2023.

</span>
<span class="ltx_bibblock">Learning Aligned Audiovisual Representations for Multimodal Sentiment Analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st International Workshop on Multimodal and Responsible Affective Computing</em> (2023).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:264171011" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:264171011</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Bei Fang, Xian Li, Guangxin Han, and Juhou He. 2023.

</span>
<span class="ltx_bibblock">Rethinking pseudo-labeling for semi-supervised facial expression recognition with contrastive self-supervised learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">IEEE Access</em> 11 (2023), 45547–45558.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feinberg et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (1986)</span>
<span class="ltx_bibblock">
Todd E Feinberg, Arthur Rifkin, Carrie Schaffer, and Elaine Walker. 1986.

</span>
<span class="ltx_bibblock">Facial discrimination and emotional recognition in schizophrenia and affective disorders.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">Archives of general psychiatry</em> 43, 3 (1986), 276–279.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2022a.

</span>
<span class="ltx_bibblock">Masked autoencoders are scalable vision learners. In <em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 16000–16009.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Yu He, Licai Sun, Zheng Lian, Bin Liu, Jianhua Tao, Meng Wang, and Yuan Cheng. 2022b.

</span>
<span class="ltx_bibblock">Multimodal temporal attention in sentiment analysis. In <em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge</em>. 61–66.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Maochun Huang, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. 2023.

</span>
<span class="ltx_bibblock">Context-Based Adaptive Multimodal Fusion Network for Continuous Frame-Level Sentiment Prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> 31 (2023), 3468–3477.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:263693806" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:263693806</a>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yu Huang, Junyang Lin, Chang Zhou, Hongxia Yang, and Longbo Huang. 2022.

</span>
<span class="ltx_bibblock">Modality competition: What makes joint training of multi-modal network fail in deep learning?(provably). In <em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>. PMLR, 9226–9259.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamble and Sengupta (2023)</span>
<span class="ltx_bibblock">
Kranti Kamble and Joydeep Sengupta. 2023.

</span>
<span class="ltx_bibblock">A comprehensive survey on emotion recognition based on electroencephalograph (EEG) signals.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Multimedia Tools and Applications</em> 82, 18 (2023), 27269–27304.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Lei Li, Yongfeng Zhang, and Li Chen. 2023b.

</span>
<span class="ltx_bibblock">Prompt distillation for efficient llm-based recommendation. In <em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management</em>. 1348–1357.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Shan Li, Weihong Deng, and JunPing Du. 2017.

</span>
<span class="ltx_bibblock">Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild. In <em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2852–2861.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Sunan Li, Hailun Lian, Cheng Lu, Yan Zhao, Chuangao Tang, Yuan Zong, and Wenming Zheng. 2023a.

</span>
<span class="ltx_bibblock">Multimodal emotion recognition in noisy environment based on progressive label revision. In <em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st ACM International Conference on Multimedia</em>. 9571–9575.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mngyu Xu, Kexin Wang, Ke Xu, Yu He, Ying Li, Jinming Zhao, et al<span id="bib.bib18.3.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">Mer 2023: Multi-label learning, modality robustness, and semi-supervised learning. In <em id="bib.bib18.4.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st ACM International Conference on Multimedia</em>. 9610–9614.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Zheng Lian, Haiyang Sun, Licai Sun, Zhuofan Wen, Siyuan Zhang, Shun Chen, Hao Gu, Jinming Zhao, Ziyang Ma, Xie Chen, et al<span id="bib.bib19.3.1" class="ltx_text">.</span> 2024.

</span>
<span class="ltx_bibblock">MER 2024: Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.17113</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Zheng Lian, Jianhua Tao, Bin Liu, and Jian Huang. 2019.

</span>
<span class="ltx_bibblock">Conversational emotion analysis via attention mechanisms.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.11263</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard De Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. 2022.

</span>
<span class="ltx_bibblock">Frozen clip models are efficient video learners. In <em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>. Springer, 388–404.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mauss et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Iris B Mauss, Allison S Troy, and Monique K LeBourgeois. 2013.

</span>
<span class="ltx_bibblock">Poorer sleep quality is associated with lower emotion-regulation ability in a laboratory paradigm.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">Cognition &amp; emotion</em> 27, 3 (2013), 567–576.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehrabian (1996)</span>
<span class="ltx_bibblock">
Albert Mehrabian. 1996.

</span>
<span class="ltx_bibblock">Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Current Psychology</em> 14 (1996), 261–292.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mittal et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Trisha Mittal, Puneet Mathur, Aniket Bera, and Dinesh Manocha. 2021.

</span>
<span class="ltx_bibblock">Affect2MM: Affective Analysis of Multimedia Content Using Emotion Causality.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> (2021), 5657–5667.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:232185153" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:232185153</a>

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pastor et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Miguel A Pastor, Dayana Ribas, Alfonso Ortega, Antonio Miguel, and Eduardo Lleida. 2022.

</span>
<span class="ltx_bibblock">Cross-corpus speech emotion recognition with HuBERT self-supervised representation. In <em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">IberSPEECH 2022</em>. ISCA, 76–80.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plutchik and Kellerman (2013)</span>
<span class="ltx_bibblock">
Robert Plutchik and Henry Kellerman. 2013.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Theories of emotion</em>. Vol. 1.

</span>
<span class="ltx_bibblock">Academic press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Praveen et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
R Gnana Praveen, Wheidima Carneiro de Melo, Nasib Ullah, Haseeb Aslam, Osama Zeeshan, Th’eo Denorme, Marco Pedersoli, Alessandro Lameiras Koerich, Patrick Cardinal, and Eric Granger. 2022.

</span>
<span class="ltx_bibblock">A Joint Cross-Attention Model for Audio-Visual Fusion in Dimensional Emotion Recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em> (2022), 2485–2494.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:247762186" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:247762186</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al<span id="bib.bib28.3.1" class="ltx_text">.</span> 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision. In <em id="bib.bib28.4.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>. PMLR, 8748–8763.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasheed et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Hanoona Rasheed, Muhammad Uzair Khattak, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. 2023.

</span>
<span class="ltx_bibblock">Fine-tuned clip models are efficient video learners. In <em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 6545–6554.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tanha et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jafar Tanha, Yousef Abdi, Negin Samadi, Nazila Razzaghi, and Mohammad Asadpour. 2020.

</span>
<span class="ltx_bibblock">Boosting methods for multi-class imbalanced data classification: an experimental review.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">Journal of Big data</em> 7 (2020), 1–47.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Torres et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Edgar P Torres, Edgar Alejandro Torres, Myriam Hernández-Álvarez, and Sang Guun Yoo. 2020.

</span>
<span class="ltx_bibblock">Emotion recognition related to stock trading using machine learning algorithms with feature selection.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">Ieee Access</em> 8 (2020), 199719–199732.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Haotian Wang, Yuxuan Xi, Hang Chen, Jun Du, Yan Song, Qing Wang, Hengshun Zhou, Chenxi Wang, Jie Ma, Pengfei Hu, Ya Jiang, Shi Cheng, Jie Zhang, and Yuzhe Weng. 2023.

</span>
<span class="ltx_bibblock">Hierarchical Audio-Visual Information Fusion with Multi-label Joint Decoding for MER 2023.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st ACM International Conference on Multimedia</em> (2023).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:262012761" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:262012761</a>

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, and Xing Xie. 2022.

</span>
<span class="ltx_bibblock">NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better. In <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Annual Meeting of the Association for Computational Linguistics</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:247084085" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:247084085</a>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Jianfei Xiao, Yancan Chen, Yimin Ou, Hanyi Yu, and Yiyong Xiao. 2024.

</span>
<span class="ltx_bibblock">Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">ArXiv</em> abs/2401.15496 (2024).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:267312232" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:267312232</a>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al<span id="bib.bib35.3.1" class="ltx_text">.</span> 2023a.

</span>
<span class="ltx_bibblock">Baichuan 2: Open large-scale language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.10305</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jiuding Yang, Yakun Yu, Di Niu, Weidong Guo, and Yu Xu. 2023b.

</span>
<span class="ltx_bibblock">Confede: Contrastive feature decomposition for multimodal sentiment analysis. In <em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 7617–7630.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">You et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Chenyu You, Nuo Chen, and Yuexian Zou. 2021.

</span>
<span class="ltx_bibblock">Self-supervised Contrastive Cross-Modality Representation Learning for Spoken Question Answering. In <em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">Conference on Empirical Methods in Natural Language Processing</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:237439566" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:237439566</a>

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.05006" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.05007" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.05007">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.05007" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.05008" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 00:05:23 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
