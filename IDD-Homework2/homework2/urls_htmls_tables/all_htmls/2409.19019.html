<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>RAGProbe: An Automated Approach for Evaluating RAG Applications</title>
<!--Generated on Tue Sep 24 23:28:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Retrieval Augmented Generation,  Large Language Models,  Software Evaluation" lang="en" name="keywords"/>
<base href="/html/2409.19019v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S1" title="In RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S2" title="In RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Motivating Example</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S3" title="In RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Evaluation Scenarios</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S3.SS1" title="In 3. Evaluation Scenarios ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>A schema for Evaluation Scenarios</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S3.SS2" title="In 3. Evaluation Scenarios ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>A set of Evaluation Scenarios</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S4" title="In RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>RAGProbe</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S4.SS1" title="In 4. RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Our Approach</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S4.SS2" title="In 4. RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Usage Example</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S5" title="In RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Evaluation of RAGProbe</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S5.SS1" title="In 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S5.SS1.SSS1" title="In 5.1. Datasets ‣ 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Corpus Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S5.SS1.SSS2" title="In 5.1. Datasets ‣ 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Open-source RAG Repositories</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S5.SS2" title="In 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S5.SS3" title="In 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S5.SS3.SSS1" title="In 5.3. Results ‣ 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.1 </span>How effective are the evaluation scenarios in exposing failure rates in open-source RAG pipelines?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S5.SS3.SSS2" title="In 5.3. Results ‣ 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.2 </span>How does our approach compare to existing state-of-the-art approaches in terms of failure rate and validity?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S5.SS3.SSS3" title="In 5.3. Results ‣ 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.3 </span>What is the impact of different domains on the effectiveness of automatically generated question-answer pairs?</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S6" title="In RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S6.SS1" title="In 6. Discussion ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Evaluating RAG Pipelines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S6.SS2" title="In 6. Discussion ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Threats to Validity</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S6.SS2.SSS1" title="In 6.2. Threats to Validity ‣ 6. Discussion ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span>Internal Validity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S6.SS2.SSS2" title="In 6.2. Threats to Validity ‣ 6. Discussion ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span>External Validity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S6.SS2.SSS3" title="In 6.2. Threats to Validity ‣ 6. Discussion ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.3 </span>Construct Validity</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S6.SS3" title="In 6. Discussion ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Implications</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S6.SS3.SSS1" title="In 6.3. Implications ‣ 6. Discussion ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.1 </span>Researchers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S6.SS3.SSS2" title="In 6.3. Implications ‣ 6. Discussion ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.2 </span>Developers</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S7" title="In RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#S8" title="In RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion and Future Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">RAGProbe: An Automated Approach for Evaluating RAG Applications</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shangeetha Sivasothy, Scott Barnett, Stefanus Kurniawan, Zafaryab Rasool, Rajesh Vasa
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Applied Artificial Intelligence Institute, Deakin University</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Geelong</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">Australia</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:s.sivasothy,%20scott.barnett,%20stefanus.kurniawan,%20zafaryab.rasool,%20rajesh.vasa@deakin.edu.au">s.sivasothy, scott.barnett, stefanus.kurniawan, zafaryab.rasool, rajesh.vasa@deakin.edu.au</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id4.id1">Retrieval Augmented Generation (RAG) is increasingly being used when building Generative AI applications. Evaluating these applications and RAG pipelines is mostly done manually, via a trial and error process. Automating evaluation of RAG pipelines requires overcoming challenges such as context misunderstanding, wrong format, incorrect specificity, and missing content. Prior works therefore focused on improving evaluation metrics as well as enhancing components within the pipeline using available question and answer datasets. However, they have not focused on 1) providing a schema for capturing different types of question-answer pairs or 2) creating a set of templates for generating question-answer pairs that can support automation of RAG pipeline evaluation. In this paper, we present a technique for generating variations in question-answer pairs to trigger failures in RAG pipelines. We validate 5 open-source RAG pipelines using 3 datasets. Our approach revealed the highest failure rates when prompts combine multiple questions: 91% for questions when spanning multiple documents and 78% for questions from a single document; indicating a need for developers to prioritise handling these combined questions. 60% failure rate was observed in academic domain dataset and 53% and 62% failure rates were observed in open-domain datasets. Our automated approach outperforms the existing state-of-the-art methods, by increasing the failure rate by 51% on average per dataset. Our work presents an automated approach for continuously monitoring the health of RAG pipelines, which can be integrated into existing CI/CD pipelines, allowing for improved quality.</p>
</div>
<div class="ltx_keywords">Retrieval Augmented Generation, Large Language Models, Software Evaluation
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Conference acronym ’XX; June 03–05, 2024; Woodstock, NY</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Software and its engineering Empirical software validation</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Retrieval Augmented Generation (RAG) has recently gained popularity for use in question and answer systems, as they better understand query context and meaning <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib16" title="">2024</a>; Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib37" title="">2024</a>; Asai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib3" title="">2023</a>; Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib11" title="">2023</a>; Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib28" title="">2023</a>)</cite>. However, RAG has specific limitations and failure points <cite class="ltx_cite ltx_citemacro_citep">(Barnett et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib5" title="">2024</a>; Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib11" title="">2023</a>; Jeong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib16" title="">2024</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib36" title="">2024</a>)</cite> due to imprecise embedding models and uncertainties introduced when generating results with large language models. Thus, questions a human takes for granted, a RAG pipeline may struggle with i.e. answering multiple questions from either a single document or a set of documents (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S1.T1" title="Table 1 ‣ 1. Introduction ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Table 1</span></a> for more examples). These limitations are uncovered and potentially addressed through an evaluation of the a) quality of answers, b) impact of domain, and c) architectural choices. Improving RAG pipelines currently is often a manual and iterative process, that can be better supported by automation of the evaluation process - similar to automated test case generation.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S1.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S1.T1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.1.1.1">
<span class="ltx_p" id="S1.T1.1.1.1.1.1.1" style="width:8.7pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.1.1.1.1">ID</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.1.2.1">
<span class="ltx_p" id="S1.T1.1.1.1.2.1.1" style="width:238.5pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.2.1.1.1">Evaluation Scenario</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.1.3.1">
<span class="ltx_p" id="S1.T1.1.1.1.3.1.1" style="width:17.3pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.3.1.1.1">Quivr</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.1.4.1">
<span class="ltx_p" id="S1.T1.1.1.1.4.1.1" style="width:28.2pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.4.1.1.1">Danswer</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.1.5.1">
<span class="ltx_p" id="S1.T1.1.1.1.5.1.1" style="width:28.2pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.5.1.1.1">Ragflow</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.1.1.1.6">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.1.6.1">
<span class="ltx_p" id="S1.T1.1.1.1.6.1.1" style="width:21.7pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.6.1.1.1">Verba</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.1.1.1.7">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.1.1.7.1">
<span class="ltx_p" id="S1.T1.1.1.1.7.1.1" style="width:34.7pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.7.1.1.1">Rag-stack</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S1.T1.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.1.1.1">
<span class="ltx_p" id="S1.T1.1.2.1.1.1.1" style="width:8.7pt;">S1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S1.T1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.1.2.1">
<span class="ltx_p" id="S1.T1.1.2.1.2.1.1" style="width:238.5pt;">A question to retrieve number for which answer is in a single document</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S1.T1.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.1.3.1">
<span class="ltx_p" id="S1.T1.1.2.1.3.1.1" style="width:17.3pt;">Pass</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S1.T1.1.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.1.4.1">
<span class="ltx_p" id="S1.T1.1.2.1.4.1.1" style="width:28.2pt;">Pass</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S1.T1.1.2.1.5">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.1.5.1">
<span class="ltx_p" id="S1.T1.1.2.1.5.1.1" style="width:28.2pt;">Pass</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S1.T1.1.2.1.6">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.1.6.1">
<span class="ltx_p" id="S1.T1.1.2.1.6.1.1" style="width:21.7pt;">Pass</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S1.T1.1.2.1.7">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.2.1.7.1">
<span class="ltx_p" id="S1.T1.1.2.1.7.1.1" style="width:34.7pt;">Fail</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S1.T1.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.2.1.1">
<span class="ltx_p" id="S1.T1.1.3.2.1.1.1" style="width:8.7pt;">S2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.2.2.1">
<span class="ltx_p" id="S1.T1.1.3.2.2.1.1" style="width:238.5pt;">A question to retrieve date/time for which answer is in a single document</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.2.3.1">
<span class="ltx_p" id="S1.T1.1.3.2.3.1.1" style="width:17.3pt;">Pass</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.2.4.1">
<span class="ltx_p" id="S1.T1.1.3.2.4.1.1" style="width:28.2pt;">Pass</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.3.2.5">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.2.5.1">
<span class="ltx_p" id="S1.T1.1.3.2.5.1.1" style="width:28.2pt;">Pass</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.3.2.6">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.2.6.1">
<span class="ltx_p" id="S1.T1.1.3.2.6.1.1" style="width:21.7pt;">Pass</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.3.2.7">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.3.2.7.1">
<span class="ltx_p" id="S1.T1.1.3.2.7.1.1" style="width:34.7pt;">Pass</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S1.T1.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.3.1.1">
<span class="ltx_p" id="S1.T1.1.4.3.1.1.1" style="width:8.7pt;">S3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.3.2.1">
<span class="ltx_p" id="S1.T1.1.4.3.2.1.1" style="width:238.5pt;">A multiple-choice question for which answer is in a single document</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.3.3.1">
<span class="ltx_p" id="S1.T1.1.4.3.3.1.1" style="width:17.3pt;">Fail</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.4.3.4">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.3.4.1">
<span class="ltx_p" id="S1.T1.1.4.3.4.1.1" style="width:28.2pt;">Pass</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.4.3.5">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.3.5.1">
<span class="ltx_p" id="S1.T1.1.4.3.5.1.1" style="width:28.2pt;">Pass</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.4.3.6">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.3.6.1">
<span class="ltx_p" id="S1.T1.1.4.3.6.1.1" style="width:21.7pt;">Fail</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.4.3.7">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.4.3.7.1">
<span class="ltx_p" id="S1.T1.1.4.3.7.1.1" style="width:34.7pt;">Pass</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S1.T1.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.4.1.1">
<span class="ltx_p" id="S1.T1.1.5.4.1.1.1" style="width:8.7pt;">S4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.4.2.1">
<span class="ltx_p" id="S1.T1.1.5.4.2.1.1" style="width:238.5pt;">A question combining multiple questions for which answers are in a single document</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.4.3.1">
<span class="ltx_p" id="S1.T1.1.5.4.3.1.1" style="width:17.3pt;">Fail</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.5.4.4">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.4.4.1">
<span class="ltx_p" id="S1.T1.1.5.4.4.1.1" style="width:28.2pt;">Pass</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.5.4.5">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.4.5.1">
<span class="ltx_p" id="S1.T1.1.5.4.5.1.1" style="width:28.2pt;">Fail</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.5.4.6">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.4.6.1">
<span class="ltx_p" id="S1.T1.1.5.4.6.1.1" style="width:21.7pt;">Fail</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.5.4.7">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.5.4.7.1">
<span class="ltx_p" id="S1.T1.1.5.4.7.1.1" style="width:34.7pt;">Fail</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S1.T1.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.5.1.1">
<span class="ltx_p" id="S1.T1.1.6.5.1.1.1" style="width:8.7pt;">S5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.5.2.1">
<span class="ltx_p" id="S1.T1.1.6.5.2.1.1" style="width:238.5pt;">A question combining multiple questions for which answers are in a set of documents</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.5.3.1">
<span class="ltx_p" id="S1.T1.1.6.5.3.1.1" style="width:17.3pt;">Fail</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.6.5.4">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.5.4.1">
<span class="ltx_p" id="S1.T1.1.6.5.4.1.1" style="width:28.2pt;">Partial</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.6.5.5">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.5.5.1">
<span class="ltx_p" id="S1.T1.1.6.5.5.1.1" style="width:28.2pt;">Fail</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.6.5.6">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.5.6.1">
<span class="ltx_p" id="S1.T1.1.6.5.6.1.1" style="width:21.7pt;">Fail</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S1.T1.1.6.5.7">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.6.5.7.1">
<span class="ltx_p" id="S1.T1.1.6.5.7.1.1" style="width:34.7pt;">Fail</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r" id="S1.T1.1.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.7.6.1.1">
<span class="ltx_p" id="S1.T1.1.7.6.1.1.1" style="width:8.7pt;">S6</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S1.T1.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.7.6.2.1">
<span class="ltx_p" id="S1.T1.1.7.6.2.1.1" style="width:238.5pt;">A question for which answer is not in the document corpus</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S1.T1.1.7.6.3">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.7.6.3.1">
<span class="ltx_p" id="S1.T1.1.7.6.3.1.1" style="width:17.3pt;">Fail</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S1.T1.1.7.6.4">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.7.6.4.1">
<span class="ltx_p" id="S1.T1.1.7.6.4.1.1" style="width:28.2pt;">Partial</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S1.T1.1.7.6.5">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.7.6.5.1">
<span class="ltx_p" id="S1.T1.1.7.6.5.1.1" style="width:28.2pt;">Fail</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S1.T1.1.7.6.6">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.7.6.6.1">
<span class="ltx_p" id="S1.T1.1.7.6.6.1.1" style="width:21.7pt;">Fail</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S1.T1.1.7.6.7">
<span class="ltx_inline-block ltx_align_top" id="S1.T1.1.7.6.7.1">
<span class="ltx_p" id="S1.T1.1.7.6.7.1.1" style="width:34.7pt;">Fail</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Effectiveness of manually created evaluation scenarios against open-source Retrieval Augmented Generation (RAG) solutions. Pass - evaluation scenario succeeded, Fail - evaluation scenario produced a defect, and Partial - answer correct but references incorrect.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Prior work on exposing issues and addressing them focused on selecting better evaluation metrics <cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib11" title="">2023</a>; Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib28" title="">2023</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib20" title="">2023</a>; Fadnis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib12" title="">2024</a>)</cite> or improving RAG components by running them against existing question and answer datasets <cite class="ltx_cite ltx_citemacro_citep">(Mao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib22" title="">2020</a>; Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib13" title="">2024</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib38" title="">2024</a>; Salemi and Zamani, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib29" title="">2024</a>; Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib37" title="">2024</a>; Jeong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib16" title="">2024</a>; Asai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib3" title="">2023</a>)</cite>. A recent work, RAGAS <cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib11" title="">2023</a>)</cite> provides one such end-to-end pipeline for evaluating RAG pipelines, spanning data generation and evaluation. However, RAGAS does not provide 1) a schema for capturing different types of question-answer pairs, or 2) a technique to generate question-answer pairs as templates (inspired by traditional test scenario templates) for evaluating a RAG pipeline. Our work is complimentary to RAGAS by addressing these limitations. The research gap identified is that there is no automated process for creating domain specific questions and answers covering a diverse set of variations. In this study, we address this.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we present, RAGProbe for automating the evaluation of RAG pipelines. We introduce the core concept of <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">evaluation scenarios</span> to represent how to evaluate a RAG pipeline. An evaluation scenario when executed produces a question-answer pair and is designed to represent different types of variation found in question-answer pairs. An evaluation scenario includes a) a document corpus sampling and chunking strategy, b) scenario specific prompts and prompting strategy (for generation), and c) a set of evaluation metrics. To the best of our knowledge, this is the first study to identify and investigate variations in question-answer pairs for evaluation of RAG pipelines.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We carried out an evaluation of RAGProbe across 5 open-source RAG pipelines, 3 datasets (Qasper, Google NQ, and MS Marco) and 180 question-answer pairs. Our study involved a) evaluating the effectiveness of our evaluation scenarios, b) comparing our approach with state-of-the-art, and c) analysing the impact of the domain. We found that the evaluation scenarios related to multiple questions posed in a single hit (S4 &amp; S5) resulted in the most failure rates (91% and 78%). We compared RAGProbe to RAGAS<cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib11" title="">2023</a>)</cite> and found that our approach produced more valid question-answer pairs (our approach produced 90%, 98%, and 92% whereas the state-of-the -art approach resulted in 87%, 93%, and 85% for Qasper, Google NQ, and MS Marco datasets respectively), and produced more failures across the RAG pipelines. This shows that our approach produces data that is more effective at evaluating RAG pipelines.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We summarise our contributions as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">An evaluation scenario schema for describing the constructs for evaluating RAG pipelines.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">A set of evaluation scenarios that expose limitations in RAG pipelines.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">A novel approach, RAGProbe for synthesising domain specific instances of evaluation scenarios based on a corpus.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">An evaluation of RAGProbe using 3 public datasets and 5 open-source RAG pipelines.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">A set of recommendations from the literature on how to mitigate each of the identified evaluation scenarios.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Motivating Example</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Imagine Jack, a developer, building a RAG pipeline for a financial institution for question answering. This RAG pipeline enables users to ask questions based on a given corpus (a set of documents). RAG pipelines are required because the corpus includes proprietary documents and large language models (LLM) cannot reliably answer questions using their learned knowledge alone. To build a RAG pipeline, Jack needs to 1) load, parse and chunk the given corpus of documents, 2) index the chunks, 3) store chunks in a vector database (embedding store), and 4) write a prompt which instructs the large language model on how to generate an answer based on the retrieved chunks. To build an initial version of this RAG pipeline, Jack searches for open-source RAG pipelines and evaluates the functionality of existing RAG pipelines by indexing documents from publicly available datasets.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Jack asks different variations of questions (e.g. number, date/time, multiple-choice questions, multiple questions combined in a single question) from 5 open-source RAG pipelines: 1) Quivr<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/QuivrHQ/quivr</span></span></span>, 2) Danswer<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/danswer-ai/danswer</span></span></span>, 3) Ragflow<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/infiniflow/ragflow</span></span></span>, 4) Verba<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://github.com/weaviate/Verba</span></span></span>, and 5) Rag-stack<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/psychic-api/rag-stack</span></span></span>. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S1.T1" title="Table 1 ‣ 1. Introduction ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Table 1</span></a> shows the results of manual execution of each question. All RAG pipelines failed to provide accurate responses for all evaluation scenarios (except S2). Jack realises the critical need to evaluate and improve the robustness of these pipelines to ensure they can handle a variety of evaluation scenarios successfully. Jack was wondering how to automatically generate question-answer pairs related to his domain, as the number of questions that can be asked from a given corpus is infinite and there is no systematic way to evaluate the developed RAG pipeline. Also, the manual approach is time-consuming. Therefore, Jack started looking at existing tools/approaches to generate question-answer pairs from documents and to consider variations of questions. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S2.T2" title="Table 2 ‣ 2. Motivating Example ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Table 2</span></a> compares existing tools. Also, tracing failures within the RAG pipeline is challenging for Jack, particularly when determining which component — retrieval, indexer, prompt, LLM or generator — contributed to the failure. Lack of clear visibility into the system’s internal workings complicates the process of identifying and resolving failures effectively.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T2.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.1.1.1">
<span class="ltx_p" id="S2.T2.1.1.1.1.1.1" style="width:138.8pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.1.1.1.1">Features</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T2.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.1.2.1">
<span class="ltx_p" id="S2.T2.1.1.1.2.1.1" style="width:34.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.2.1.1.1">Corrective-RAG <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib37" title="">2024</a>)</cite></span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T2.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.1.3.1">
<span class="ltx_p" id="S2.T2.1.1.1.3.1.1" style="width:34.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.3.1.1.1">Adaptive-RAG <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib16" title="">2024</a>)</cite></span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T2.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.1.4.1">
<span class="ltx_p" id="S2.T2.1.1.1.4.1.1" style="width:21.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.4.1.1.1">ARES <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib28" title="">2023</a>)</cite></span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T2.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.1.5.1">
<span class="ltx_p" id="S2.T2.1.1.1.5.1.1" style="width:39.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.5.1.1.1">Inspector
<br class="ltx_break"/>RAGet <cite class="ltx_cite ltx_citemacro_citep">(Fadnis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib12" title="">2024</a>)</cite></span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T2.1.1.1.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.1.6.1">
<span class="ltx_p" id="S2.T2.1.1.1.6.1.1" style="width:30.4pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.6.1.1.1">Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib3" title="">2023</a>)</cite></span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T2.1.1.1.7">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.1.7.1">
<span class="ltx_p" id="S2.T2.1.1.1.7.1.1" style="width:26.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.7.1.1.1">RAGAS <cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib11" title="">2023</a>)</cite></span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T2.1.1.1.8">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.1.1.8.1">
<span class="ltx_p" id="S2.T2.1.1.1.8.1.1" style="width:34.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.8.1.1.1">Our 
<br class="ltx_break"/>approach</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.1.1.1">
<span class="ltx_p" id="S2.T2.1.2.1.1.1.1" style="width:138.8pt;">Generate question-answer pairs from documents</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.1.2.1">
<span class="ltx_p" id="S2.T2.1.2.1.2.1.1" style="width:34.7pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.1.3.1">
<span class="ltx_p" id="S2.T2.1.2.1.3.1.1" style="width:34.7pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.1.4.1">
<span class="ltx_p" id="S2.T2.1.2.1.4.1.1" style="width:21.7pt;">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.2.1.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.1.5.1">
<span class="ltx_p" id="S2.T2.1.2.1.5.1.1" style="width:39.0pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.2.1.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.1.6.1">
<span class="ltx_p" id="S2.T2.1.2.1.6.1.1" style="width:30.4pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.2.1.7">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.1.7.1">
<span class="ltx_p" id="S2.T2.1.2.1.7.1.1" style="width:26.0pt;">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T2.1.2.1.8">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.2.1.8.1">
<span class="ltx_p" id="S2.T2.1.2.1.8.1.1" style="width:34.7pt;">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S2.T2.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.2.1.1">
<span class="ltx_p" id="S2.T2.1.3.2.1.1.1" style="width:138.8pt;">Do not require an initial set of question-answer pairs</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.2.2.1">
<span class="ltx_p" id="S2.T2.1.3.2.2.1.1" style="width:34.7pt;">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.2.3.1">
<span class="ltx_p" id="S2.T2.1.3.2.3.1.1" style="width:34.7pt;">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.2.4.1">
<span class="ltx_p" id="S2.T2.1.3.2.4.1.1" style="width:21.7pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.3.2.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.2.5.1">
<span class="ltx_p" id="S2.T2.1.3.2.5.1.1" style="width:39.0pt;">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.3.2.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.2.6.1">
<span class="ltx_p" id="S2.T2.1.3.2.6.1.1" style="width:30.4pt;">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.3.2.7">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.2.7.1">
<span class="ltx_p" id="S2.T2.1.3.2.7.1.1" style="width:26.0pt;">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.3.2.8">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.3.2.8.1">
<span class="ltx_p" id="S2.T2.1.3.2.8.1.1" style="width:34.7pt;">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S2.T2.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.3.1.1">
<span class="ltx_p" id="S2.T2.1.4.3.1.1.1" style="width:138.8pt;">Generate &amp; evaluate question-answer pairs automatically</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.3.2.1">
<span class="ltx_p" id="S2.T2.1.4.3.2.1.1" style="width:34.7pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.3.3.1">
<span class="ltx_p" id="S2.T2.1.4.3.3.1.1" style="width:34.7pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.4.3.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.3.4.1">
<span class="ltx_p" id="S2.T2.1.4.3.4.1.1" style="width:21.7pt;">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.4.3.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.3.5.1">
<span class="ltx_p" id="S2.T2.1.4.3.5.1.1" style="width:39.0pt;">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.4.3.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.3.6.1">
<span class="ltx_p" id="S2.T2.1.4.3.6.1.1" style="width:30.4pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.4.3.7">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.3.7.1">
<span class="ltx_p" id="S2.T2.1.4.3.7.1.1" style="width:26.0pt;">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.4.3.8">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.4.3.8.1">
<span class="ltx_p" id="S2.T2.1.4.3.8.1.1" style="width:34.7pt;">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S2.T2.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.4.1.1">
<span class="ltx_p" id="S2.T2.1.5.4.1.1.1" style="width:138.8pt;">Show variations of questions</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.4.2.1">
<span class="ltx_p" id="S2.T2.1.5.4.2.1.1" style="width:34.7pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.4.3.1">
<span class="ltx_p" id="S2.T2.1.5.4.3.1.1" style="width:34.7pt;">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.5.4.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.4.4.1">
<span class="ltx_p" id="S2.T2.1.5.4.4.1.1" style="width:21.7pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.5.4.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.4.5.1">
<span class="ltx_p" id="S2.T2.1.5.4.5.1.1" style="width:39.0pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.5.4.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.4.6.1">
<span class="ltx_p" id="S2.T2.1.5.4.6.1.1" style="width:30.4pt;">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.5.4.7">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.4.7.1">
<span class="ltx_p" id="S2.T2.1.5.4.7.1.1" style="width:26.0pt;">✓</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.5.4.8">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.5.4.8.1">
<span class="ltx_p" id="S2.T2.1.5.4.8.1.1" style="width:34.7pt;">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S2.T2.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.5.1.1">
<span class="ltx_p" id="S2.T2.1.6.5.1.1.1" style="width:138.8pt;">Include schema for evaluation scenarios</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.5.2.1">
<span class="ltx_p" id="S2.T2.1.6.5.2.1.1" style="width:34.7pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.5.3.1">
<span class="ltx_p" id="S2.T2.1.6.5.3.1.1" style="width:34.7pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.6.5.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.5.4.1">
<span class="ltx_p" id="S2.T2.1.6.5.4.1.1" style="width:21.7pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.6.5.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.5.5.1">
<span class="ltx_p" id="S2.T2.1.6.5.5.1.1" style="width:39.0pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.6.5.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.5.6.1">
<span class="ltx_p" id="S2.T2.1.6.5.6.1.1" style="width:30.4pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.6.5.7">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.5.7.1">
<span class="ltx_p" id="S2.T2.1.6.5.7.1.1" style="width:26.0pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T2.1.6.5.8">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.6.5.8.1">
<span class="ltx_p" id="S2.T2.1.6.5.8.1.1" style="width:34.7pt;">✓</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r" id="S2.T2.1.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.6.1.1">
<span class="ltx_p" id="S2.T2.1.7.6.1.1.1" style="width:138.8pt;">Use scenarios for question-answer generation</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S2.T2.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.6.2.1">
<span class="ltx_p" id="S2.T2.1.7.6.2.1.1" style="width:34.7pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S2.T2.1.7.6.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.6.3.1">
<span class="ltx_p" id="S2.T2.1.7.6.3.1.1" style="width:34.7pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S2.T2.1.7.6.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.6.4.1">
<span class="ltx_p" id="S2.T2.1.7.6.4.1.1" style="width:21.7pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S2.T2.1.7.6.5">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.6.5.1">
<span class="ltx_p" id="S2.T2.1.7.6.5.1.1" style="width:39.0pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S2.T2.1.7.6.6">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.6.6.1">
<span class="ltx_p" id="S2.T2.1.7.6.6.1.1" style="width:30.4pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S2.T2.1.7.6.7">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.6.7.1">
<span class="ltx_p" id="S2.T2.1.7.6.7.1.1" style="width:26.0pt;">✗</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S2.T2.1.7.6.8">
<span class="ltx_inline-block ltx_align_top" id="S2.T2.1.7.6.8.1">
<span class="ltx_p" id="S2.T2.1.7.6.8.1.1" style="width:34.7pt;">✓</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Comparison of existing tools for evaluating RAG pipelines</figcaption>
</figure>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">The motivating example highlights the following features that Jack requires for evaluating his application:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">Generate domain and context specific questions and answers from a corpus for evaluating a RAG pipeline.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">A schema for describing an evaluation scenario for a RAG pipeline.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">A set of evaluation scenarios that cover a variety of question and answers used in a RAG pipeline.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1">Automated evaluation of the answers produced by a RAG pipeline.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Evaluation Scenarios</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this paper, we define an evaluation scenario distinct from unit tests and test scenarios for the following reasons: a) unit tests and test scenarios, assume either a pass or failure outcome, b) neither cover the variance and nuance of natural language, and c) neither require custom evaluation metrics. The necessity for evaluation scenarios emerged from the process of implementing multiple RAG use cases, assessing open-source RAG implementations, and reviewing the relevant literature <cite class="ltx_cite ltx_citemacro_citep">(Barnett et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib5" title="">2024</a>; Rasool et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib26" title="">2024</a>)</cite>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>A schema for Evaluation Scenarios</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">An evaluation scenario schema consists of 6 constructs: 1) document sampling strategy, 2) chunking strategy, 3) chunk sampling strategy, 4) scenario specific prompts, 5) a prompting strategy, and 6) acceptable evaluation metrics.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i1.p1.1.1">Document sampling strategy:</span> Document sampling strategy indicates how documents need to be sampled from the corpus. These strategies can be random or iterative. Random sampling involves selecting documents from the corpus without a specific pattern. Iterative sampling means systematically going through every document in the corpus.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.1">Chunking strategy:</span> This indicates how chunks are created from documents by specifying a particular strategy, separator, chunk size, and chunk overlap size.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i3.p1.1.1">Chunk sampling strategy:</span> This indicates how chunks are selected to generate questions. These strategies can be random or iterative. Random sampling involves selecting chunks from a document without a specific pattern. Iterative sampling means systematically going through every chunk of a document in the corpus.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i4.p1.1.1">Scenario specific prompts:</span> Each evaluation scenario includes a specific prompt to generate question-answer pairs using LLMs. These prompts outline guidelines on dos and don’ts and specify the output format, such as a JSON object containing the question-answer pair.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i5.p1.1.1">Prompting strategy:</span> Prompting strategy is the technique on how the prompt is written. These strategies can be one-shot (i.e. by specifying one example), three-shot (i.e. by specifying three examples), or sequential (i.e. by executing one prompt first and then execute the next prompt by passing the results of the first prompt into the context).</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i6.p1">
<p class="ltx_p" id="S3.I1.i6.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i6.p1.1.1">Acceptable evaluation metrics:</span> Different evaluation metrics are used to compare RAG generated answers against the expected response <cite class="ltx_cite ltx_citemacro_citep">(Balaguer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib4" title="">2024</a>; Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib11" title="">2023</a>; Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib28" title="">2023</a>; Fadnis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib12" title="">2024</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib20" title="">2023</a>)</cite>. These metrics are: 1) correctness, 2) relevance, 3) completeness, 4) consistency, 5) explicitness, 6) contradiction, and 7) no-question related information. <span class="ltx_text ltx_font_italic" id="S3.I1.i6.p1.1.2">Correctness –</span> indicates the RAG generated response should match the expected answer exactly or be a close paraphrase, reflecting the same information. <span class="ltx_text ltx_font_italic" id="S3.I1.i6.p1.1.3">Relevance –</span> indicates the RAG generated response should directly address the question without deviating into unrelated topics. <span class="ltx_text ltx_font_italic" id="S3.I1.i6.p1.1.4">Completeness –</span> is where the RAG generated response covers all parts of the expected response, providing a full response. <span class="ltx_text ltx_font_italic" id="S3.I1.i6.p1.1.5">Consistency –</span> measures whether the RAG generated response aligns with the expected response in terms of detail and context, maintaining logical coherence. <span class="ltx_text ltx_font_italic" id="S3.I1.i6.p1.1.6">Explicitness –</span> indicates the RAG generated response should clearly state that the system does not know or cannot answer the question. <span class="ltx_text ltx_font_italic" id="S3.I1.i6.p1.1.7">Contradiction –</span> shows that the RAG generated response should not provide information or attempt to answer the question after stating the system cannot answer the question. <span class="ltx_text ltx_font_italic" id="S3.I1.i6.p1.1.8">No-question related information –</span> means that the RAG generated response should not include any information that attempts to address the question.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>A set of Evaluation Scenarios</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We identify 6 illustrative scenarios to demonstrate the end-to-end automation of our approach. Three scenarios were selected based on the literature: 1) questions to retrieve numbers <cite class="ltx_cite ltx_citemacro_citep">(Rasool et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib25" title="">2023</a>)</cite>, 2) questions to retrieve date/time <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib36" title="">2024</a>)</cite>, and 3) multiple-choice questions <cite class="ltx_cite ltx_citemacro_citep">(Rasool et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib25" title="">2023</a>)</cite>. The remaining three scenarios were identified from a recent work that highlighted 7 failure points of RAG systems <cite class="ltx_cite ltx_citemacro_citep">(Barnett et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib5" title="">2024</a>)</cite>. These are: 4) combining multiple questions for which answers are in a single document, 5) combining multiple questions for which answers are in a set of documents, and 6) asking a question for which answer is not in the corpus. Examples of each scenario are shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S3.T3" title="Table 3 ‣ 3.2. A set of Evaluation Scenarios ‣ 3. Evaluation Scenarios ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Table 3</span></a> and we describe each scenario below.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1">ID</span></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.2.1">
<span class="ltx_p" id="S3.T3.1.1.1.2.1.1" style="width:195.1pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.2.1.1.1">Question</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.3.1">
<span class="ltx_p" id="S3.T3.1.1.1.3.1.1" style="width:195.1pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.3.1.1.1">Answer</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.2.1.1">S1</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.2.1.2.1">
<span class="ltx_p" id="S3.T3.1.2.1.2.1.1" style="width:195.1pt;">What was the total attendance for the 1982 World Series?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.2.1.3.1">
<span class="ltx_p" id="S3.T3.1.2.1.3.1.1" style="width:195.1pt;">384,570</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T3.1.3.2.1">S2</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T3.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.3.2.2.1">
<span class="ltx_p" id="S3.T3.1.3.2.2.1.1" style="width:195.1pt;">When was the National Insurance Scheme introduced in Jamaica?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T3.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.3.2.3.1">
<span class="ltx_p" id="S3.T3.1.3.2.3.1.1" style="width:195.1pt;">1966</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T3.1.4.3.1">S3</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T3.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.4.3.2.1">
<span class="ltx_p" id="S3.T3.1.4.3.2.1.1" style="width:195.1pt;">Which nation hosted the 2014 Winter Paralympics? A): United States B): Canada C): Russia D): Germany</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T3.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.4.3.3.1">
<span class="ltx_p" id="S3.T3.1.4.3.3.1.1" style="width:195.1pt;">C</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T3.1.5.4.1">S4</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T3.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.5.4.2.1">
<span class="ltx_p" id="S3.T3.1.5.4.2.1.1" style="width:195.1pt;">What does Charles Hockett identify as a core feature of human language? What is the method used to quantify the property of a communication system to combine and re-use elementary forms in a lexicon? What coding was applied to subdivide each word of the lexicon in the phonetic analysis?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T3.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.5.4.3.1">
<span class="ltx_p" id="S3.T3.1.5.4.3.1.1" style="width:195.1pt;">Charles Hockett identified duality of patterning as a core feature of human language. A measure called ‘combinatoriality’, which is a real-valued quantity ranging in the interval [0 : 1] that quantifies how frequently forms recur in a lexicon, is used to quantify this property. The International Phonetic Association (IPA) coding was applied to subdivide each word of the lexicon in the phonetic analysis.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T3.1.6.5.1">S5</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T3.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.6.5.2.1">
<span class="ltx_p" id="S3.T3.1.6.5.2.1.1" style="width:195.1pt;">Where did the 1924 Winter Olympics take place? Who produced track 3 of the album? Who was the first member of the 50 home run club?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T3.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.6.5.3.1">
<span class="ltx_p" id="S3.T3.1.6.5.3.1.1" style="width:195.1pt;">The 1924 Winter Olympics took place in Chamonix. Track 3 of the album was produced by Vinylz, Boi-1da and Velous. Babe Ruth was the first member of the 50 home run club.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S3.T3.1.7.6.1">S6</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S3.T3.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.7.6.2.1">
<span class="ltx_p" id="S3.T3.1.7.6.2.1.1" style="width:195.1pt;">Explain quantum mechanics?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S3.T3.1.7.6.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.7.6.3.1">
<span class="ltx_p" id="S3.T3.1.7.6.3.1.1" style="width:195.1pt;">Sorry, the system doesn’t know the answer.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Overview of evaluation scenarios with example question-answer pairs</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">S1: A question to retrieve number for which answer is in a single document</span></p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">In this scenario, a question is formulated with the expectation of receiving a numerical value as a response. The RAG pipeline is expected to extract relevant numerical information from a document to provide an accurate numeric response. The evaluation scenario schema is as follows:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">Document sampling strategy - Random N documents (e.g. N=10)</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">Chunking strategy - Character text splitting, separator=newline, chunk size=3000, chunk overlapping size=150</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1">Chunk sampling strategy - Random one chunk per document after filtering chunks containing numbers</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1">Scenario specific prompts - Specific prompt</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i5.p1">
<p class="ltx_p" id="S3.I2.i5.p1.1">Prompting strategy - One-shot</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i6.p1">
<p class="ltx_p" id="S3.I2.i6.p1.1">Acceptable evaluation metrics - Correctness, relevance, completeness, consistency</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">S2: A question to retrieve date/time for which answer is in a single document</span></p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">In this scenario, a question is asked to obtain temporal information, specifically a date or time, as the response. The system is expected to retrieve relevant date or time-related details from the document corpus to provide an accurate temporal response. The evaluation scenario schema is as follows:</p>
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1">Document sampling strategy - Random N documents (e.g. N=10)</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1">Chunking strategy - Character text splitting, separator=newline, chunk size=3000, chunk overlapping size=150</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i3.p1">
<p class="ltx_p" id="S3.I3.i3.p1.1">Chunk sampling strategy - Random one chunk per document after filtering chunks containing date/time</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i4.p1">
<p class="ltx_p" id="S3.I3.i4.p1.1">Scenario specific prompts - Specific prompt</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i5.p1">
<p class="ltx_p" id="S3.I3.i5.p1.1">Prompting strategy - One-shot</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i6.p1">
<p class="ltx_p" id="S3.I3.i6.p1.1">Acceptable evaluation metrics - Correctness, relevance, completeness, consistency</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p6.1.1">S3: A multiple-choice question for which answer is in a single document</span></p>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.1">In this scenario, a question is created along with several predetermined answer options. The system is expected to select the correct answer option from the provided options based on its understanding of the question and the context provided in the document corpus. Multiple-choice questions often require the system to analyze and interpret information to discern the most appropriate response among the given choices. The evaluation scenario schema is as follows:</p>
<ul class="ltx_itemize" id="S3.I4">
<li class="ltx_item" id="S3.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i1.p1">
<p class="ltx_p" id="S3.I4.i1.p1.1">Document sampling strategy - Random N documents (e.g. N=10)</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i2.p1">
<p class="ltx_p" id="S3.I4.i2.p1.1">Chunking strategy - Character text splitting, separator=newline, chunk size=3000, chunk overlapping size=150</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i3.p1">
<p class="ltx_p" id="S3.I4.i3.p1.1">Chunk sampling strategy - Random one chunk per document</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i4.p1">
<p class="ltx_p" id="S3.I4.i4.p1.1">Scenario specific prompts - Specific prompt</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i5.p1">
<p class="ltx_p" id="S3.I4.i5.p1.1">Prompting strategy - Three-shot</p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i6.p1">
<p class="ltx_p" id="S3.I4.i6.p1.1">Acceptable evaluation metrics - Correctness, relevance, completeness, consistency</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p8">
<p class="ltx_p" id="S3.SS2.p8.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p8.1.1">S4: A question combining multiple questions for which answers are in a single document</span></p>
</div>
<div class="ltx_para" id="S3.SS2.p9">
<p class="ltx_p" id="S3.SS2.p9.1">This scenario involves formulating a single question by combining multiple individual questions that all relate to information contained within a single document. The combined question requires the system to synthesise answers from different parts of the same document to provide a response. The system response is considered to be complete if all questions inside a single question have been answered. The evaluation scenario schema is as follows:</p>
</div>
<div class="ltx_para" id="S3.SS2.p10">
<ul class="ltx_itemize" id="S3.I5">
<li class="ltx_item" id="S3.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I5.i1.p1">
<p class="ltx_p" id="S3.I5.i1.p1.1">Document sampling strategy - Random N documents (e.g. N=10)</p>
</div>
</li>
<li class="ltx_item" id="S3.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I5.i2.p1">
<p class="ltx_p" id="S3.I5.i2.p1.1">Chunking strategy - Character text splitting, separator=newline, chunk size=3000, chunk overlapping size=150</p>
</div>
</li>
<li class="ltx_item" id="S3.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I5.i3.p1">
<p class="ltx_p" id="S3.I5.i3.p1.1">Chunk sampling strategy - Random three chunks from a single document combined</p>
</div>
</li>
<li class="ltx_item" id="S3.I5.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I5.i4.p1">
<p class="ltx_p" id="S3.I5.i4.p1.1">Scenario specific prompts - Specific prompt</p>
</div>
</li>
<li class="ltx_item" id="S3.I5.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I5.i5.p1">
<p class="ltx_p" id="S3.I5.i5.p1.1">Prompting strategy - One-shot</p>
</div>
</li>
<li class="ltx_item" id="S3.I5.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I5.i6.p1">
<p class="ltx_p" id="S3.I5.i6.p1.1">Acceptable evaluation metrics - Correctness, relevance, completeness, consistency</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="204" id="S3.F1.g1" src="extracted/5877563/Figures/Proposed_Approach.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>RAGProbe: Our automated approach to generate question-answer pairs. Our approach is extensible by adding different evaluation scenarios and different evaluation metrics.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p11">
<p class="ltx_p" id="S3.SS2.p11.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p11.1.1">S5: A question combining multiple questions for which answers are in a set of documents</span></p>
</div>
<div class="ltx_para" id="S3.SS2.p12">
<p class="ltx_p" id="S3.SS2.p12.1">This scenario involves creating a single question by combining multiple individual questions, each sourced from different documents. The consolidated question requires the system to combine information from various sources to provide a response. The system response is considered to be complete if all questions inside a single question have been answered. The evaluation scenario schema is as follows:</p>
<ul class="ltx_itemize" id="S3.I6">
<li class="ltx_item" id="S3.I6.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I6.i1.p1">
<p class="ltx_p" id="S3.I6.i1.p1.1">Document sampling strategy - Random N documents (e.g. N=10)</p>
</div>
</li>
<li class="ltx_item" id="S3.I6.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I6.i2.p1">
<p class="ltx_p" id="S3.I6.i2.p1.1">Chunking strategy - Character text splitting, separator=newline, chunk size=3000, chunk overlapping size=150</p>
</div>
</li>
<li class="ltx_item" id="S3.I6.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I6.i3.p1">
<p class="ltx_p" id="S3.I6.i3.p1.1">Chunk sampling strategy - Random three chunks from a set of documents combined</p>
</div>
</li>
<li class="ltx_item" id="S3.I6.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I6.i4.p1">
<p class="ltx_p" id="S3.I6.i4.p1.1">Scenario specific prompts - Specific prompt</p>
</div>
</li>
<li class="ltx_item" id="S3.I6.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I6.i5.p1">
<p class="ltx_p" id="S3.I6.i5.p1.1">Prompting strategy - One-shot</p>
</div>
</li>
<li class="ltx_item" id="S3.I6.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I6.i6.p1">
<p class="ltx_p" id="S3.I6.i6.p1.1">Acceptable evaluation metrics - Correctness, relevance, completeness, consistency</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p13">
<p class="ltx_p" id="S3.SS2.p13.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p13.1.1">S6: A question for which answer is not in the document corpus</span></p>
</div>
<div class="ltx_para" id="S3.SS2.p14">
<p class="ltx_p" id="S3.SS2.p14.1">In this scenario, a question is posed to the system for which there is no corresponding answer available within the corpus of documents that the system has access to. Therefore, the system is expected to respond that it doesn’t know the response. The evaluation scenario schema is as follows:</p>
<ul class="ltx_itemize" id="S3.I7">
<li class="ltx_item" id="S3.I7.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I7.i1.p1">
<p class="ltx_p" id="S3.I7.i1.p1.1">Document sampling strategy - Random N documents (e.g. N=10)</p>
</div>
</li>
<li class="ltx_item" id="S3.I7.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I7.i2.p1">
<p class="ltx_p" id="S3.I7.i2.p1.1">Chunking strategy - Character text splitting, separator=newline, chunk size=3000, chunk overlapping size=150</p>
</div>
</li>
<li class="ltx_item" id="S3.I7.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I7.i3.p1">
<p class="ltx_p" id="S3.I7.i3.p1.1">Chunk sampling strategy - Iterative throughout the entire corpus</p>
</div>
</li>
<li class="ltx_item" id="S3.I7.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I7.i4.p1">
<p class="ltx_p" id="S3.I7.i4.p1.1">Scenario specific prompts - Specific prompt</p>
</div>
</li>
<li class="ltx_item" id="S3.I7.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I7.i5.p1">
<p class="ltx_p" id="S3.I7.i5.p1.1">Prompting strategy - Sequential (i.e. one prompt executed after the other prompt)</p>
</div>
</li>
<li class="ltx_item" id="S3.I7.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I7.i6.p1">
<p class="ltx_p" id="S3.I7.i6.p1.1">Acceptable evaluation metrics - Explicitness, contradiction, no-question related information</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>RAGProbe </h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we describe our proposed approach, RAGProbe along with a usage example.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Our Approach</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To address the issues discussed in the motivating example (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S2" title="2. Motivating Example ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">section 2</span></a>), we propose a novel approach, RAGProbe, that leverages LLMs for generating context specific evaluation scenarios and evaluates a given RAG pipeline. As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S3.F1" title="Figure 1 ‣ 3.2. A set of Evaluation Scenarios ‣ 3. Evaluation Scenarios ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>, our approach has three key components: 1) Q&amp;A Generator, 2) RAG Evaluation Runner, and 3) Semantic Answer Evaluator. The Q&amp;A Generator takes an existing document corpus as an input, and then applies evaluation scenario schemas to generate questions and answers. The RAG Evaluation Runner is responsible for adapting to the RAG implementation (handle authentication and mapping to API) and collecting answers to all generated questions from the RAG pipeline under evaluation. Semantic Answer Evaluator compares generated answers by Q&amp;A Generator against RAG pipeline generated answers. To handle the ambiguity in natural language responses, we extend OpenAI evals ClosedQA template<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://github.com/openai/evals/blob/main/evals/registry/modelgraded/closedqa.yaml</span></span></span> to determine whether the RAG generated response is correct based on the expected response.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">RAGProbe is implemented in Python and is available online<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>https://figshare.com/s/e0d74c0d346fd2e05d59</span></span></span>. We use GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib24" title="">2023</a>)</cite> to generate question-answer pairs for scenarios S1-S5, enhancing the quality of the generated pairs. For scenario S6, we use GPT-3.5-Turbo to reduce costs, as S6 involves iterative sampling of chunks and repeated application of the LLM.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Usage Example</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">RAGProbe is designed to be used by developers in three ways: 1) to generate question-answer pairs, 2) to enable evaluation of a RAG pipeline, and 3) add new evaluation scenarios. For generating question-answer pairs, developers provide a corpus of documents (e.g. a set of PDFs). Then, RAGProbe generates a set of questions and answers. The end-to-end evaluation requires developers to provide access to a RAG pipeline (i.e. access to the database and search interface) and to a set of generated questions. Adding new scenarios requires filling out the evaluation schema: selecting sampling strategies, chunking strategy, and creating the prompts for use in a prompting strategy.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S3.F1" title="Figure 1 ‣ 3.2. A set of Evaluation Scenarios ‣ 3. Evaluation Scenarios ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>, Q&amp;A Generator of RAGProbe will use the corpus of documents, provided by developers, to generate question-answer pairs. RAG Evaluation Runner takes the RAG pipeline, given by developers, adapts to its implementation (e.g. authentication, mapping to API) to generate answers for generated questions by Q&amp;A Generator. Semantic Answer Evaluator compares the answers generated by Q&amp;A Generator against the answers generated by the given RAG pipeline, and provides a report on passing and failing questions. Semantic Answer Evaluator allows developers to substitute other evaluation metrics.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">By using RAGProbe (as shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S3.F1" title="Figure 1 ‣ 3.2. A set of Evaluation Scenarios ‣ 3. Evaluation Scenarios ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>), Jack is able to: 1) generate questions and answers relevant to his domain, 2) have a schema of evaluation scenarios, 3) have a set of evaluation scenarios to evaluate a RAG pipeline, and 4) have an automated way to evaluate RAG during development or as part of a CI/CD pipeline.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Evaluation of RAGProbe </h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We evaluated RAGProbe to assess: 1) the effectiveness of evaluation scenarios, 2) how RAGProbe compares to state-of-the-art approaches, and 3) the impact of datasets from different domains. We present the following research questions for our evaluation:
</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">RQ1: How effective are the evaluation scenarios in exposing failure rates in open-source RAG pipelines?</span>
<br class="ltx_break"/>Unlike state-of-the-art techniques that randomly generate question-answer pairs, RAGProbe generates questions for specific scenarios relevant to RAG pipelines. We first evaluate these scenarios by analysing how effective they are at exposing limitations in 5 open-source RAG pipelines.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">RQ2: How does our approach compare to existing state-of-the-art approaches in terms of failure rate and validity?</span>
<br class="ltx_break"/>We compare RAGProbe with the state-of-the-art approach (RAGAS <cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib11" title="">2023</a>)</cite>) found in the literature in terms of how effective the approaches are for finding defects (failure rate) and the quality of the generated questions in terms of validity to the scenario. We did not find other tools that provide end-to-end evaluation of RAG and question and answer generation from a corpus.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1"><span class="ltx_text ltx_font_bold" id="S5.p4.1.1">RQ3: What is the impact of different domains on the effectiveness of automatically generated question-answer pairs?</span>
<br class="ltx_break"/>Our hypothesis is that different domains use different terms which will impact the quality of the evaluation scenarios (i.e. domains will make it harder to find defects). This question examines the influence of domain-specific concepts (found in different datasets) for the different scenarios.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Datasets</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">For our evaluation, we needed two different datasets, a) a set of corpus of documents, and b) a set of RAG implementations.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1. </span>Corpus Datasets</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1">For a set of corpus, we use i) Qasper <cite class="ltx_cite ltx_citemacro_citep">(Dasigi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib10" title="">2021</a>)</cite>, ii) Google Natural Questions (NQ) <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib18" title="">2019</a>)</cite>, and iii) MS Marco <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib23" title="">2016</a>)</cite>. Qasper is a dataset of 1,585 scientific research papers. Google NQ corpus is a question answering dataset with 307,373 training examples. These documents are from Wikipedia pages. MS Marco is a dataset of real web documents using the Bing search engine. MS Marco dataset contains 3.2 million documents and 8.8 million passages. From all three datasets, we used documents that could be downloaded and indexed by our selected RAG pipelines (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S5.SS1.SSS2" title="5.1.2. Open-source RAG Repositories ‣ 5.1. Datasets ‣ 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">subsubsection 5.1.2</span></a>). We noticed that some URLs in MS Marco dataset were obsolete and no longer valid, so we excluded those URLs when downloading documents. Qasper dataset contains of PDF files, Google NQ dataset contains of HTML files, and MS Marco dataset contains a combination of PDF and HTML files. With respect to domains, Qasper dataset belongs to academic or scientific research domain which has scientific research papers. Both Google NQ and MS Marco datasets are open-domain, meaning they do not pertain to any specific domain. All documents were converted to plain text before ingestion.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2. </span>Open-source RAG Repositories</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">We selected 5 open-source RAG repositories from Github (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S5.T4" title="Table 4 ‣ 5.1.2. Open-source RAG Repositories ‣ 5.1. Datasets ‣ 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Table 4</span></a>). Our selection criteria for RAG repositories was 1) contains keywords ‘retrieval augmented generation’ or ‘rag’, 2) timeline (the repository must be created in the last 5 years), 3) popularity (the repository must have at least 1000 stars), 4) activeness (the repository must have at least a commit in the last 2 years), 5) programming language (the repository doesn’t necessarily have to be Python-specific), 6) functionality (the repository should support uploading documents, the repository should not test RAG pipelines, and the repository does not require code changes to upload documents), 7) purpose (the repository should not be libraries or library wrappers or API wrappers), and 8) language (the repository description should be written in English).</p>
</div>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.1.1">
<span class="ltx_p" id="S5.T4.1.1.1.1.1.1" style="width:43.4pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1.1.1">Repo Name</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.2.1">
<span class="ltx_p" id="S5.T4.1.1.1.2.1.1" style="width:21.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.2.1.1.1">Stars</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.3.1">
<span class="ltx_p" id="S5.T4.1.1.1.3.1.1" style="width:86.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.3.1.1.1">Programming Language</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.4.1">
<span class="ltx_p" id="S5.T4.1.1.1.4.1.1" style="width:108.4pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.4.1.1.1">Large Language Model</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.1.1.5.1">
<span class="ltx_p" id="S5.T4.1.1.1.5.1.1" style="width:86.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.5.1.1.1">Embedding</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.2.1.1.1">
<span class="ltx_p" id="S5.T4.1.2.1.1.1.1" style="width:43.4pt;">Quivr</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T4.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.2.1.2.1">
<span class="ltx_p" id="S5.T4.1.2.1.2.1.1" style="width:21.7pt;">31,800</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T4.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.2.1.3.1">
<span class="ltx_p" id="S5.T4.1.2.1.3.1.1" style="width:86.7pt;">Python</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T4.1.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.2.1.4.1">
<span class="ltx_p" id="S5.T4.1.2.1.4.1.1" style="width:108.4pt;">gpt-3.5-turbo</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T4.1.2.1.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.2.1.5.1">
<span class="ltx_p" id="S5.T4.1.2.1.5.1.1" style="width:86.7pt;">text-embedding-ada-002</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S5.T4.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.3.2.1.1">
<span class="ltx_p" id="S5.T4.1.3.2.1.1.1" style="width:43.4pt;">Danswer</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T4.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.3.2.2.1">
<span class="ltx_p" id="S5.T4.1.3.2.2.1.1" style="width:21.7pt;">9,000</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T4.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.3.2.3.1">
<span class="ltx_p" id="S5.T4.1.3.2.3.1.1" style="width:86.7pt;">Python</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T4.1.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.3.2.4.1">
<span class="ltx_p" id="S5.T4.1.3.2.4.1.1" style="width:108.4pt;">gpt-3.5-turbo-16k-0613</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T4.1.3.2.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.3.2.5.1">
<span class="ltx_p" id="S5.T4.1.3.2.5.1.1" style="width:86.7pt;">intfloat/e5-base-v2</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S5.T4.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.4.3.1.1">
<span class="ltx_p" id="S5.T4.1.4.3.1.1.1" style="width:43.4pt;">Ragflow</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T4.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.4.3.2.1">
<span class="ltx_p" id="S5.T4.1.4.3.2.1.1" style="width:21.7pt;">4,400</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T4.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.4.3.3.1">
<span class="ltx_p" id="S5.T4.1.4.3.3.1.1" style="width:86.7pt;">Python</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T4.1.4.3.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.4.3.4.1">
<span class="ltx_p" id="S5.T4.1.4.3.4.1.1" style="width:108.4pt;">gpt-3.5-turbo</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T4.1.4.3.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.4.3.5.1">
<span class="ltx_p" id="S5.T4.1.4.3.5.1.1" style="width:86.7pt;">text-embedding-ada-002</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S5.T4.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.5.4.1.1">
<span class="ltx_p" id="S5.T4.1.5.4.1.1.1" style="width:43.4pt;">Verba</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T4.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.5.4.2.1">
<span class="ltx_p" id="S5.T4.1.5.4.2.1.1" style="width:21.7pt;">2,100</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T4.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.5.4.3.1">
<span class="ltx_p" id="S5.T4.1.5.4.3.1.1" style="width:86.7pt;">Python</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T4.1.5.4.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.5.4.4.1">
<span class="ltx_p" id="S5.T4.1.5.4.4.1.1" style="width:108.4pt;">gpt-4-1106-preview</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T4.1.5.4.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.5.4.5.1">
<span class="ltx_p" id="S5.T4.1.5.4.5.1.1" style="width:86.7pt;">text-embedding-ada-002</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r" id="S5.T4.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.6.5.1.1">
<span class="ltx_p" id="S5.T4.1.6.5.1.1.1" style="width:43.4pt;">Rag-stack</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S5.T4.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.6.5.2.1">
<span class="ltx_p" id="S5.T4.1.6.5.2.1.1" style="width:21.7pt;">1,400</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S5.T4.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.6.5.3.1">
<span class="ltx_p" id="S5.T4.1.6.5.3.1.1" style="width:86.7pt;">Python</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S5.T4.1.6.5.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.6.5.4.1">
<span class="ltx_p" id="S5.T4.1.6.5.4.1.1" style="width:108.4pt;">ggml-gpt4All-J-v1.3-groovy</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S5.T4.1.6.5.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T4.1.6.5.5.1">
<span class="ltx_p" id="S5.T4.1.6.5.5.1.1" style="width:86.7pt;">all-MiniLM-L6-v2</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4. </span>Overview of the open-source RAG pipelines with default settings. Note: our approach is a black box testing approach.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.SSS2.p2">
<p class="ltx_p" id="S5.SS1.SSS2.p2.1">The search term “retrieval augmented generation” yielded 1.2k projects, while the term “RAG” returned 36.4k projects. Out of these GitHub projects, repositories with more than 1000 stars and created after 2019, resulted in 53 repositories. Out of these 53 repositories, project description was written in English for 50 repositories. We manually inspected each of these repositories to check whether 1) the repository supports uploading documents, 2) the repository does not require changing code to upload documents, 3) the repository does not test RAG pipelines, and 4) the repository is not a library or a library wrapper or an API wrapper (For example, Haystack<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>https://github.com/deepset-ai/haystack</span></span></span>, RAGatouille<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>https://github.com/bclavie/RAGatouille</span></span></span> were excluded because they were libraries). Our selection criteria resulted in 6 repositories, and we manually set up all these 6 repositories for evaluation. We failed to set up one repository, superagent<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>https://github.com/superagent-ai/superagent</span></span></span> due to complexity with indexing documents. This resulted in 5 RAG repositories, which are 1) Quivr, 2) Danswer, 3) Ragflow, 4) Verba, and 5) Rag-stack. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S5.T4" title="Table 4 ‣ 5.1.2. Open-source RAG Repositories ‣ 5.1. Datasets ‣ 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Table 4</span></a> shows the summary of the selected RAG repositories for evaluation. We used default settings of these repositories to execute the generated questions.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Methodology</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">To answer RQ1, we random sampled 10 documents from each dataset of Qasper, Google NQ, and MS Marco for generation of question-answer pairs. Then, we converted the selected documents into a text file (both PDF and HTML files). We automatically generated 30 questions for each evaluation scenario. This resulted in 180 questions to be executed across one RAG pipeline. Then, we captured the generated responses and evaluated against the expected responses as per the evaluation metrics. Finally, we calculated the failure rate per evaluation scenario. Further, we break down failure rate per evaluation scenario per RAG pipeline. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S5.T5" title="Table 5 ‣ 5.2. Methodology ‣ 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Table 5</span></a> shows an example of failure with a question and answer.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S5.T5.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.1.1.1.1.1">
<span class="ltx_p" id="S5.T5.1.1.1.1.1.1" style="width:99.7pt;">Question</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T5.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.1.1.1.2.1">
<span class="ltx_p" id="S5.T5.1.1.1.2.1.1" style="width:303.5pt;">What is the ISBN number of the book ‘The Unity of Science and Economics: A New Foundation of Economic Theory’?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S5.T5.1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.1.2.2.1.1">
<span class="ltx_p" id="S5.T5.1.2.2.1.1.1" style="width:99.7pt;"><span class="ltx_ERROR undefined" id="S5.T5.1.2.2.1.1.1.1">\hdashline</span>Expected Response</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T5.1.2.2.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.1.2.2.2.1">
<span class="ltx_p" id="S5.T5.1.2.2.2.1.1" style="width:303.5pt;">978-1-4939-3464-5</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S5.T5.1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.1.3.3.1.1">
<span class="ltx_p" id="S5.T5.1.3.3.1.1.1" style="width:99.7pt;"><span class="ltx_ERROR undefined" id="S5.T5.1.3.3.1.1.1.1">\hdashline</span>RAG Response</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T5.1.3.3.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.1.3.3.2.1">
<span class="ltx_p" id="S5.T5.1.3.3.2.1.1" style="width:303.5pt;">The provided context does not contain any information about the ISBN number of the book “The Unity of Science and Economics: A New Foundation of Economic Theory.” Thus, I am unable to locate the specific ISBN you are looking for. Please refer to a library database, a book retailer, or the publisher’s information for the ISBN number of the book.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r" id="S5.T5.1.4.4.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.1.4.4.1.1">
<span class="ltx_p" id="S5.T5.1.4.4.1.1.1" style="width:99.7pt;"><span class="ltx_ERROR undefined" id="S5.T5.1.4.4.1.1.1.1">\hdashline</span>Failed Evaluation Metric</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S5.T5.1.4.4.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.1.4.4.2.1">
<span class="ltx_p" id="S5.T5.1.4.4.2.1.1" style="width:303.5pt;">Correctness</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5. </span>An example of failure with a question and answer </figcaption>
</figure>
<figure class="ltx_float ltx_float_tcbfloat" id="S5.SS2.1">
<p class="ltx_p" id="S5.SS2.1.1">[htb] <svg class="ltx_picture" height="71.52" id="S5.SS2.1.1.pic1" overflow="visible" version="1.1" width="330"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,71.52) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 0 L 0 71.52 L 330 71.52 L 330 0 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 0.55 0.55 L 0.55 70.97 L 329.45 70.97 L 329.45 0.55 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 4.7 4.7)"><foreignobject color="#000000" height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="320.59">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S5.SS2.1.1.pic1.1.1.1.1.1" style="width:231.7pt;">
<span class="ltx_p" id="S5.SS2.1.1.pic1.1.1.1.1.1.1">Failure is any answer from the RAG pipeline that does not pass one or more evaluation metrics. The LLM is used to assess pass/fail for each criteria given the ground-truth and the generated-answer.</span>
</span></foreignobject></g></g></svg></p>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">To answer RQ2, we compared our approach to a state-of-the-art approach, RAGAS <cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib11" title="">2023</a>)</cite>. We initially searched the literature for tools used to evaluate RAGs, focusing specifically on those that generate questions from documents. Then, we generated questions using the selected tools and executed against the selected 5 open-source RAG pipelines. We compared failure rate against our approach versus the state-of-the-art approach. From the generated questions, we performed a statistical test (the Wilcoxon Signed-Rank Test <cite class="ltx_cite ltx_citemacro_citep">(Woolson, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib35" title="">2005</a>)</cite>) to determine if there is a significant difference between the failure rates between the two approaches.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">We evaluated the validity of the generated questions by comparing chunks, a method which had previously been used in the literature <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib8" title="">2024a</a>)</cite>. We used similar chunking strategies from our approach and from the state-of-the art approaches to store in a vector store for the selected documents. Then, we used generated questions to retrieve relevant chunks using a retriever from the vector store. We excluded questions generated from S6 (a question for which answer is not in the document corpus) as they didn’t have pre-assigned chunks. Then, we compared whether the retrieved chunks and the pre-assigned chunks for generated questions were similar. We then calculated the percentage of similarity between the retrieved chunks and the pre-assigned chunks for generated questions from both our approach and the state-of-the-art approach.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">To answer RQ3, we group generated questions per dataset. There were 10 questions per dataset for a given scenario. In total, there were 300 questions (10 questions per dataset * 6 scenarios * 5 RAG pipelines). Then, we calculated the total number of failing questions per dataset. Further, we calculated the total number of RAG pipelines, that failed to answer all 10 questions from a dataset of a given scenario. All data and results are available online<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>https://figshare.com/s/e0d74c0d346fd2e05d59</span></span></span>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Results</h3>
<section class="ltx_subsubsection" id="S5.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1. </span>How effective are the evaluation scenarios in exposing failure rates in open-source RAG pipelines?</h4>
<div class="ltx_para" id="S5.SS3.SSS1.p1">
<p class="ltx_p" id="S5.SS3.SSS1.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S5.F2" title="Figure 2 ‣ 5.3.1. How effective are the evaluation scenarios in exposing failure rates in open-source RAG pipelines? ‣ 5.3. Results ‣ 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> shows the summary of automatic execution of automatically generated questions. In total, there were 150 generated questions per scenario, that were executed across 5 RAG pipelines. Scenario S5 has the highest failure rate at 91%, followed by S4 at 78% and S6 at 65%, indicating these scenarios are the most problematic. Scenarios S1 and S2 show moderate failure rates at 45% and 40% respectively. Scenario S3 has the lowest failure rate at 29%, suggesting it encounters the fewest number of failing questions across 5 RAG pipelines.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS1.p2">
<p class="ltx_p" id="S5.SS3.SSS1.p2.1">As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S5.T6" title="Table 6 ‣ 5.3.1. How effective are the evaluation scenarios in exposing failure rates in open-source RAG pipelines? ‣ 5.3. Results ‣ 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Table 6</span></a>, Quivr and Rag-stack exhibited the highest failure rates (100%) in scenarios S4 and S5. When asking questions for which the answer is not in the corpus, Danswer did not generate any correct responses, but performed well in multiple-choice questions. During the manual inspection, we found that Danswer generated responses by referring to the large language model’s trained knowledge, instead of answering “The system doesn’t know the answer”. Ragflow had the lowest failure rate (27%) with multiple-choice questions compared to all other scenarios for this RAG pipeline. Verba had the lowest failure rate (17%) when handling multiple-choice questions compared to all other scenarios for this RAG pipeline. Compared to 5 RAG pipelines, Rag-stack had the highest failure rates in multiple scenarios (S1, S2, S3, S4, S5), indicating significant challenges in handling complex questions and specific information retrieval.</p>
</div>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="514" id="S5.F2.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Total failure rate combining all 5 RAG pipelines. The failure rate is calculated as the number of failures divided by the total number of questions.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T6.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.1.1.1.1">
<span class="ltx_p" id="S5.T6.1.1.1.1.1.1" style="width:13.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.1.1.1.1.1">ID</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.1.1.2.1">
<span class="ltx_p" id="S5.T6.1.1.1.2.1.1" style="width:43.4pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.1.2.1.1.1">Quivr</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.1.1.3.1">
<span class="ltx_p" id="S5.T6.1.1.1.3.1.1" style="width:65.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.1.3.1.1.1">Danswer</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.1.1.4.1">
<span class="ltx_p" id="S5.T6.1.1.1.4.1.1" style="width:65.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.1.4.1.1.1">Ragflow</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.1.1.5.1">
<span class="ltx_p" id="S5.T6.1.1.1.5.1.1" style="width:43.4pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.1.5.1.1.1">Verba</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.1.1.1.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.1.1.6.1">
<span class="ltx_p" id="S5.T6.1.1.1.6.1.1" style="width:86.7pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.1.6.1.1.1">Rag-stack</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S5.T6.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.2.1.1.1">
<span class="ltx_p" id="S5.T6.1.2.1.1.1.1" style="width:13.0pt;">S1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T6.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.2.1.2.1">
<span class="ltx_p" id="S5.T6.1.2.1.2.1.1" style="width:43.4pt;">17%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T6.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.2.1.3.1">
<span class="ltx_p" id="S5.T6.1.2.1.3.1.1" style="width:65.0pt;">43%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T6.1.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.2.1.4.1">
<span class="ltx_p" id="S5.T6.1.2.1.4.1.1" style="width:65.0pt;">50%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T6.1.2.1.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.2.1.5.1">
<span class="ltx_p" id="S5.T6.1.2.1.5.1.1" style="width:43.4pt;">43%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S5.T6.1.2.1.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.2.1.6.1">
<span class="ltx_p" id="S5.T6.1.2.1.6.1.1" style="width:86.7pt;">73%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S5.T6.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.3.2.1.1">
<span class="ltx_p" id="S5.T6.1.3.2.1.1.1" style="width:13.0pt;">S2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.3.2.2.1">
<span class="ltx_p" id="S5.T6.1.3.2.2.1.1" style="width:43.4pt;">20%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.3.2.3.1">
<span class="ltx_p" id="S5.T6.1.3.2.3.1.1" style="width:65.0pt;">30%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.3.2.4.1">
<span class="ltx_p" id="S5.T6.1.3.2.4.1.1" style="width:65.0pt;">43%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.3.2.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.3.2.5.1">
<span class="ltx_p" id="S5.T6.1.3.2.5.1.1" style="width:43.4pt;">30%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.3.2.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.3.2.6.1">
<span class="ltx_p" id="S5.T6.1.3.2.6.1.1" style="width:86.7pt;">77%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S5.T6.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.4.3.1.1">
<span class="ltx_p" id="S5.T6.1.4.3.1.1.1" style="width:13.0pt;">S3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.4.3.2.1">
<span class="ltx_p" id="S5.T6.1.4.3.2.1.1" style="width:43.4pt;">17%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.4.3.3.1">
<span class="ltx_p" id="S5.T6.1.4.3.3.1.1" style="width:65.0pt;">17%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.4.3.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.4.3.4.1">
<span class="ltx_p" id="S5.T6.1.4.3.4.1.1" style="width:65.0pt;">27%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.4.3.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.4.3.5.1">
<span class="ltx_p" id="S5.T6.1.4.3.5.1.1" style="width:43.4pt;">17%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.4.3.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.4.3.6.1">
<span class="ltx_p" id="S5.T6.1.4.3.6.1.1" style="width:86.7pt;">70%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S5.T6.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.5.4.1.1">
<span class="ltx_p" id="S5.T6.1.5.4.1.1.1" style="width:13.0pt;">S4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.5.4.2.1">
<span class="ltx_p" id="S5.T6.1.5.4.2.1.1" style="width:43.4pt;">100%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.5.4.3.1">
<span class="ltx_p" id="S5.T6.1.5.4.3.1.1" style="width:65.0pt;">70%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.5.4.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.5.4.4.1">
<span class="ltx_p" id="S5.T6.1.5.4.4.1.1" style="width:65.0pt;">80%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.5.4.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.5.4.5.1">
<span class="ltx_p" id="S5.T6.1.5.4.5.1.1" style="width:43.4pt;">40%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.5.4.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.5.4.6.1">
<span class="ltx_p" id="S5.T6.1.5.4.6.1.1" style="width:86.7pt;">100%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r" id="S5.T6.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.6.5.1.1">
<span class="ltx_p" id="S5.T6.1.6.5.1.1.1" style="width:13.0pt;">S5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.6.5.2.1">
<span class="ltx_p" id="S5.T6.1.6.5.2.1.1" style="width:43.4pt;">100%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.6.5.3.1">
<span class="ltx_p" id="S5.T6.1.6.5.3.1.1" style="width:65.0pt;">70%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.6.5.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.6.5.4.1">
<span class="ltx_p" id="S5.T6.1.6.5.4.1.1" style="width:65.0pt;">97%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.6.5.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.6.5.5.1">
<span class="ltx_p" id="S5.T6.1.6.5.5.1.1" style="width:43.4pt;">90%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S5.T6.1.6.5.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.6.5.6.1">
<span class="ltx_p" id="S5.T6.1.6.5.6.1.1" style="width:86.7pt;">100%</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r" id="S5.T6.1.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.7.6.1.1">
<span class="ltx_p" id="S5.T6.1.7.6.1.1.1" style="width:13.0pt;">S6</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S5.T6.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.7.6.2.1">
<span class="ltx_p" id="S5.T6.1.7.6.2.1.1" style="width:43.4pt;">17%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S5.T6.1.7.6.3">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.7.6.3.1">
<span class="ltx_p" id="S5.T6.1.7.6.3.1.1" style="width:65.0pt;">100%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S5.T6.1.7.6.4">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.7.6.4.1">
<span class="ltx_p" id="S5.T6.1.7.6.4.1.1" style="width:65.0pt;">80%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S5.T6.1.7.6.5">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.7.6.5.1">
<span class="ltx_p" id="S5.T6.1.7.6.5.1.1" style="width:43.4pt;">50%</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S5.T6.1.7.6.6">
<span class="ltx_inline-block ltx_align_top" id="S5.T6.1.7.6.6.1">
<span class="ltx_p" id="S5.T6.1.7.6.6.1.1" style="width:86.7pt;">80%</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6. </span>Failure rates of RAG pipelines across evaluation scenarios</figcaption>
</figure>
<figure class="ltx_float ltx_float_tcbfloat" id="S5.SS3.SSS1.1">
<p class="ltx_p" id="S5.SS3.SSS1.1.1">[htb] <svg class="ltx_picture" height="104.73" id="S5.SS3.SSS1.1.1.pic1" overflow="visible" version="1.1" width="330"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,104.73) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 0 L 0 104.73 L 330 104.73 L 330 0 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 0.55 0.55 L 0.55 104.18 L 329.45 104.18 L 329.45 0.55 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 4.7 4.7)"><foreignobject color="#000000" height="95.32" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="320.59">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S5.SS3.SSS1.1.1.pic1.1.1.1.1.1" style="width:231.7pt;">
<span class="ltx_p" id="S5.SS3.SSS1.1.1.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.SSS1.1.1.pic1.1.1.1.1.1.1.1">Answer to RQ1:</span> All evaluation scenarios had a total failure rate between 29% and 91% for all 30 questions across 5 RAG pipelines. Asking multiple-choice questions exhibited the lowest failure rate, whilst asking questions by combining multiple questions from a set of documents had the highest.</span>
</span></foreignobject></g></g></svg></p>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2. </span>How does our approach compare to existing state-of-the-art approaches in terms of failure rate and validity?</h4>
<div class="ltx_para" id="S5.SS3.SSS2.p1">
<p class="ltx_p" id="S5.SS3.SSS2.p1.1">By reviewing the literature, we identified two tools capable of generating question-answer pairs from documents: RAGAS <cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib11" title="">2023</a>)</cite> and ARES <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib28" title="">2023</a>)</cite>. When setting up these tools, ARES <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib28" title="">2023</a>)</cite> required an initial set of question-answer pairs. Therefore, we used RAGAS <cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib11" title="">2023</a>)</cite> to generate question-answer pairs for comparison with our approach. Out of the 180 questions generated by RAGAS, 24 had answers listed as “nan” (i.e., without valid answers). This indicates that RAGAS has limitations in accurately generating valid answers, affecting the utility of the generated data for downstream tasks.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS2.p2">
<p class="ltx_p" id="S5.SS3.SSS2.p2.1">Our approach, compared to the state-of-the-art approach, exposed increased failure rates across each dataset and across each RAG pipeline. As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S5.F3" title="Figure 3 ‣ 5.3.2. How does our approach compare to existing state-of-the-art approaches in terms of failure rate and validity? ‣ 5.3. Results ‣ 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>, the state-of-the-art approach exposed 37%, 37%, and 42% failure rates, whereas our approach exposed 60%, 53%, and 62% failure rates for Qasper, Google NQ, and MS Marco datasets respectively. The state-of-the-art approach revealed 37%, 21%, 45%, 19%, and 71% failure rates whereas our approach revealed 45%, 55%, 63%, 45%, and 83% failure rates across Quivr, Danswer, Ragflow, Verba, and Rag-stack respectively, as can be seen in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S5.F4" title="Figure 4 ‣ 5.3.2. How does our approach compare to existing state-of-the-art approaches in terms of failure rate and validity? ‣ 5.3. Results ‣ 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>. When performed the Wilcoxon signed rank test on failure rate pairs, it resulted in p-values of 0.25, and 0.0625 per dataset and per RAG pipeline respectively. Both p-values being higher than the common significance level of 0.05, suggesting that there is no significant difference in the failure rates across different datasets and across different RAG pipelines. This implies that the observed differences in failure rates might be due to random chance rather than inherent differences in the datasets or RAG pipelines being evaluated. Consequently, our evaluation shows that the performance of the RAG pipelines is relatively consistent across different datasets and pipeline implementations.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="364" id="S5.F3.g1" src="extracted/5877563/Figures/Failure_Rate_Dataset_RAGAS_RAGProbe.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Comparison of failure rates across datasets. A higher failure rate is better as it reveals more limitations of the RAG pipelines.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="375" id="S5.F4.g1" src="extracted/5877563/Figures/Failure_Rate_RAG_Pipeline_RAGAS_RAGProbe.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Comparison of failure rates across RAG pipelines. A higher failure rate is better as it reveals more limitations of the RAG pipelines.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.SSS2.p3">
<p class="ltx_p" id="S5.SS3.SSS2.p3.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S5.T7" title="Table 7 ‣ 5.3.2. How does our approach compare to existing state-of-the-art approaches in terms of failure rate and validity? ‣ 5.3. Results ‣ 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Table 7</span></a> compares the validity of questions generated by RAGAS and our approach by evaluating the similarity between pre-assigned chunks for generated questions and retrieved chunks. For the Qasper dataset, RAGAS achieved an 87% validity rate, while our approach achieved 90%. For the Google NQ dataset, RAGAS had a 93% validity rate, compared to 98% with our approach. For the MS Marco dataset, RAGAS achieved an 85% validity rate, while our approach reached 92%. This demonstrates that our approach consistently outperforms RAGAS in generating valid questions across all datasets.</p>
</div>
<figure class="ltx_table" id="S5.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T7.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T7.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.2.1">RAGAS <cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib11" title="">2023</a>)</cite></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T7.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.3.1">RAGProbe</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T7.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T7.1.2.1.1">Qasper</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T7.1.2.1.2">87%</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T7.1.2.1.3">90%</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T7.1.3.2.1">Google NQ</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.1.3.2.2">93%</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T7.1.3.2.3">98%</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S5.T7.1.4.3.1">MS Marco</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T7.1.4.3.2">85%</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T7.1.4.3.3">92%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7. </span>Comparison of validity of questions by comparing similarity between pre-assigned chunks for generated questions versus retrieved chunks.</figcaption>
</figure>
<figure class="ltx_float ltx_float_tcbfloat" id="S5.SS3.SSS2.1">
<p class="ltx_p" id="S5.SS3.SSS2.1.1">[htb] <svg class="ltx_picture" height="85.44" id="S5.SS3.SSS2.1.1.pic1" overflow="visible" version="1.1" width="330"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,85.44) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 0 L 0 85.44 L 330 85.44 L 330 0 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 0.55 0.55 L 0.55 84.88 L 329.45 84.88 L 329.45 0.55 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 4.7 4.7)"><foreignobject color="#000000" height="76.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="320.59">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S5.SS3.SSS2.1.1.pic1.1.1.1.1.1" style="width:231.7pt;">
<span class="ltx_p" id="S5.SS3.SSS2.1.1.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.SSS2.1.1.pic1.1.1.1.1.1.1.1">Answer to RQ2:</span> RAGProbe found more failures (between 45-83% per RAG pipeline, 180 questions) as compared to state-of-the-art. RAGProbe had a higher percentage (90-98% per dataset) of valid questions than state-of-the-art.</span>
</span></foreignobject></g></g></svg></p>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.3. </span>What is the impact of different domains on the effectiveness of automatically generated question-answer pairs?</h4>
<div class="ltx_para" id="S5.SS3.SSS3.p1">
<p class="ltx_p" id="S5.SS3.SSS3.p1.1">As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S5.F5" title="Figure 5 ‣ 5.3.3. What is the impact of different domains on the effectiveness of automatically generated question-answer pairs? ‣ 5.3. Results ‣ 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>, MS Marco had the highest number of failing questions, whilst Google NQ had the lowest number of failing questions. 60%, 53%, and 62% failure rates were observed for Qasper, Google NQ, and MS Marco datasets respectively. The high failure rates across all datasets indicate that RAG pipelines struggle consistently with the complexity and variability of questions in both academic and open-domain datasets.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS3.p2">
<p class="ltx_p" id="S5.SS3.SSS3.p2.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S5.T8" title="Table 8 ‣ 5.3.3. What is the impact of different domains on the effectiveness of automatically generated question-answer pairs? ‣ 5.3. Results ‣ 5. Evaluation of RAGProbe ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Table 8</span></a> shows the number of RAG pipelines which have failed to produce correct responses across evaluation scenarios for each dataset. In summary, Google NQ and MS Marco have the highest number of failing scenarios, where all RAG pipelines failed in 5 scenarios. Qasper shows more variability, with one RAG pipeline passing in S2 and S6 scenarios.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="351" id="S5.F5.g1" src="extracted/5877563/Figures/Evaluation_Dataset.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Automated evaluation results per dataset.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T8">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T8.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T8.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T8.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T8.1.1.1.1.1">ID</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T8.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T8.1.1.1.2.1">Qasper</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T8.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T8.1.1.1.3.1">Google NQ</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T8.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T8.1.1.1.4.1">MS Marco</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T8.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T8.1.2.1.1">S1</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.1.2.1.2">5/5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.1.2.1.3">4/5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T8.1.2.1.4">5/5</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T8.1.3.2.1">S2</th>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T8.1.3.2.2">4/5</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T8.1.3.2.3">5/5</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T8.1.3.2.4">5/5</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T8.1.4.3.1">S3</th>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T8.1.4.3.2">5/5</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T8.1.4.3.3">5/5</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T8.1.4.3.4">3/5</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T8.1.5.4.1">S4</th>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T8.1.5.4.2">5/5</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T8.1.5.4.3">5/5</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T8.1.5.4.4">5/5</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T8.1.6.5.1">S5</th>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T8.1.6.5.2">5/5</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T8.1.6.5.3">5/5</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T8.1.6.5.4">5/5</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S5.T8.1.7.6.1">S6</th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T8.1.7.6.2">4/5</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T8.1.7.6.3">5/5</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S5.T8.1.7.6.4">5/5</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8. </span>Number of RAG pipelines failed across evaluation scenarios and datasets</figcaption>
</figure>
<figure class="ltx_float ltx_float_tcbfloat" id="S5.SS3.SSS3.1">
<p class="ltx_p" id="S5.SS3.SSS3.1.1">[htb] <svg class="ltx_picture" height="88.13" id="S5.SS3.SSS3.1.1.pic1" overflow="visible" version="1.1" width="330"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,88.13) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 0 L 0 88.13 L 330 88.13 L 330 0 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 0.55 0.55 L 0.55 87.57 L 329.45 87.57 L 329.45 0.55 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 4.7 4.7)"><foreignobject color="#000000" height="78.72" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="320.59">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S5.SS3.SSS3.1.1.pic1.1.1.1.1.1" style="width:231.7pt;">
<span class="ltx_p" id="S5.SS3.SSS3.1.1.pic1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.SSS3.1.1.pic1.1.1.1.1.1.1.1">Answer to RQ3:</span> Overall, RAG pipelines have difficulty across both academic and open-domain datasets, with a slightly better performance on open-domain questions compared to academic questions, but still exhibit high failure rates across all datasets.</span>
</span></foreignobject></g></g></svg></p>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Discussion</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Evaluating RAG Pipelines</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">The software development of RAG pipelines in practice involves making many decisions that ultimately contribute towards its quality, i.e., about text splitter, chunk size, overlap size, embedding model, large language model, vector store, distance metric for semantic similarity, top-k to rerank, reranker model, top-k to context, and prompt engineering. In real-world settings, these decisions are not grounded in methodologically sound evaluation practices but rather ad-hoc and driven by developers and product owners, constrained by project deadlines. Therefore, rigorous and robust evaluation of RAG pipelines plays an important role by having variations of questions and by automating the entire process. Comparisons between RAG pipelines are unwarranted because they involve differences in their configurations, such as using different LLMs and embedding techniques. These variations mean that differences cannot be directly attributed to the pipelines themselves, but rather to the distinct underlying technologies used.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">Results from RQ1 reveal that more than 50% of questions fail in the following scenarios: 1) a question combining multiple questions for which answers are in a single document, 2) a question combining multiple questions for which answers are in a set of documents, and 3) a question for which answer is not in the document corpus. This highlights the necessity for thorough evaluation when deploying RAG pipelines into production. Results from RQ2 show that our approach has increased failure rates, compared to the state-of-the-art approach. Results from RQ3 show high failure rates across both academic (Qasper) and open-domain (Google NQ and MS Marco) datasets, and suggests that these pipelines struggle to accurately process and respond to a wide range of questions. Notably, while there is a marginally better performance on open-domain questions compared to academic questions, the overall high failure rates indicate that improvements are needed in the robustness of RAG pipelines.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Threats to Validity</h3>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1. </span>Internal Validity</h4>
<div class="ltx_para" id="S6.SS2.SSS1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.p1.1">When evaluating the RAG generated responses against the expected responses, we considered different evaluation metrics such as correctness, relevance, completeness, consistency, explicitness, contradiction, and no-question related information. However, other evaluation metrics such as faithfulness and bias were not included. To address this, the existing Semantic Answer Evaluator (as shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S3.F1" title="Figure 1 ‣ 3.2. A set of Evaluation Scenarios ‣ 3. Evaluation Scenarios ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>) can be extended by defining additional metrics to evaluate the responses.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2. </span>External Validity</h4>
<div class="ltx_para" id="S6.SS2.SSS2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.p1.1">In the current study, we have considered only one LLM (i.e., GPT) from OpenAI. There are other LLMs such as LLaMA, Alpaca, Vicuna, and Falcon. This limits the generalizability of our results. Investigating additional LLMs would provide a more comprehensive understanding and improve the external validity of our findings.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.3. </span>Construct Validity</h4>
<div class="ltx_para" id="S6.SS2.SSS3.p1">
<p class="ltx_p" id="S6.SS2.SSS3.p1.1">The RAG pipelines we investigated employ various large language models, embedding models, and chunking strategies. Comparing results across these differing default settings ensures that the variability in these configurations does not bias our findings. However, having a consistent setting with the same large language model, embedding model, and chunking strategy might have yielded different results. To generate question-answer pairs, we have not considered other prompting techniques such as chain-of-thought, least-to-most, tree-of-thought, and graph-of-thought. These techniques could have produced different patterns in question-answer pairs. Exploring different prompting techniques is left for future work.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Implications</h3>
<section class="ltx_subsubsection" id="S6.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.1. </span>Researchers</h4>
<div class="ltx_para" id="S6.SS3.SSS1.p1">
<p class="ltx_p" id="S6.SS3.SSS1.p1.1">Based on identified evaluation scenarios, researchers can explore additional evaluation scenarios to create more variations in question-answer pairs. Further, researchers can include additional evaluation metrics such as faithfulness and context relevance <cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib11" title="">2023</a>; Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib28" title="">2023</a>)</cite> for evaluating answers. Commercially available large language models such as GPT-3.5 and GPT-4 were selected for the purpose of this study based on their popularity (i.e. under extensive investigation in recent research) <cite class="ltx_cite ltx_citemacro_citep">(White et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib34" title="">2023</a>; Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib33" title="">2022</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib15" title="">2023</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib40" title="">2023</a>)</cite>. Open-source LLMs were excluded from the study as they (i) underperform <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib39" title="">2023</a>)</cite>, (ii) require extensive infrastructure to operate, and (iii) have been trained on smaller datasets <cite class="ltx_cite ltx_citemacro_citep">(Bommasani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib6" title="">2021</a>)</cite>. However, researchers can investigate how open-source LLMs could perform when they are utilised to generate question-answer pairs.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.2. </span>Developers</h4>
<div class="ltx_para" id="S6.SS3.SSS2.p1">
<p class="ltx_p" id="S6.SS3.SSS2.p1.1">As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S3.F1" title="Figure 1 ‣ 3.2. A set of Evaluation Scenarios ‣ 3. Evaluation Scenarios ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>, developers can consider the proposed approach to evaluate RAG pipelines that they are developing, and pinpointing the failure points. As can be seen in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.19019v1#S6.T9" title="Table 9 ‣ 6.3.2. Developers ‣ 6.3. Implications ‣ 6. Discussion ‣ RAGProbe: An Automated Approach for Evaluating RAG Applications"><span class="ltx_text ltx_ref_tag">Table 9</span></a>, developers can apply suggested fixes based on failing question-answer pairs. Automated evaluation for RAG pipelines offers benefits, including increased efficiency by reducing manual effort, ensuring consistent and reliable evaluation by minimizing human errors, and enabling integration into CI/CD pipelines. Understanding and fixing these failure points is essential for the development of effective and robust RAG pipelines. Our approach provides a guidance for developers for evaluating RAG pipelines in information retrieval and generation contexts.</p>
</div>
<figure class="ltx_table" id="S6.T9">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T9.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T9.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T9.1.1.1.1.1">ID</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T9.1.1.1.2.1">Recommendation of Fixes</span></td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.1.2.2.1">S1</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.1.2.2.2">Upgrading embedding model <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib14" title="">2023</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib21" title="">2023</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S6.T9.1.3.3.1">S2</th>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T9.1.3.3.2">Upgrading embedding model <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib14" title="">2023</a>; Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib21" title="">2023</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S6.T9.1.4.4.1">S3</th>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T9.1.4.4.2">Upgrading LLM <cite class="ltx_cite ltx_citemacro_citep">(Balaguer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib4" title="">2024</a>)</cite>, Fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Siriwardhana et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib30" title="">2021</a>; Balaguer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib4" title="">2024</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib14" title="">2023</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S6.T9.1.5.5.1">S4</th>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T9.1.5.5.2">Query rewriting <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib21" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib19" title="">2024</a>; Tan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib31" title="">2024</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S6.T9.1.6.6.1">S5</th>
<td class="ltx_td ltx_align_left ltx_border_r" id="S6.T9.1.6.6.2">Query rewriting <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib21" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib19" title="">2024</a>; Tan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib31" title="">2024</a>)</cite>
</td>
</tr>
<tr class="ltx_tr" id="S6.T9.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S6.T9.1.7.7.1">S6</th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S6.T9.1.7.7.2">Prompt engineering during generation <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib19" title="">2024</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib14" title="">2023</a>)</cite>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9. </span>Overview of evaluation scenarios with the recommended fixes based on the literature</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Related Work</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Prior research proposed reference-free evaluation, such as RAGAS <cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib11" title="">2023</a>)</cite> and ARES <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib28" title="">2023</a>)</cite>. Similar to our study, these approaches also use LLM-generated data to evaluate RAG pipelines. Previous studies <cite class="ltx_cite ltx_citemacro_citep">(Es et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib11" title="">2023</a>; Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib28" title="">2023</a>)</cite> evaluate RAG pipelines with different evaluation metrics such as contextual relevance, faithfulness, and answer relevance, whereas in our study, we assess RAG responses based on evaluation metrics such as correctness, relevance, completeness, consistency, explicit, contradiction, and no-question related information. ARES <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib28" title="">2023</a>)</cite> utilizes a lightweight language model trained on synthetic data to assess RAG components and incorporates human-annotated data for prediction-powered inference (PPI) to mitigate errors. InspectorRAGet <cite class="ltx_cite ltx_citemacro_citep">(Fadnis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib12" title="">2024</a>)</cite> is a RAG evaluation lifecycle, which considers RAG pipelines as one of the inputs, similar to our study. However, the authors <cite class="ltx_cite ltx_citemacro_citep">(Fadnis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib12" title="">2024</a>)</cite> have not considered generating questions. Previous studies <cite class="ltx_cite ltx_citemacro_citep">(Fadnis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib12" title="">2024</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib7" title="">2024b</a>)</cite> have considered evaluation metrics which are generic to evaluating LLM responses and not specific to evaluating RAG responses. Codium-AI <cite class="ltx_cite ltx_citemacro_citep">(Alshahwan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib2" title="">2024</a>)</cite> generates unit tests using LLMs and requires an initial set of unit tests from developers. In contrast, our approach generates question-answer pairs from scratch using a corpus of documents. Since Codium-AI generates unit tests based on an initial set, there is a risk of propagating errors if the initial unit tests are flawed. Also, Codium-AI offers four prompt templates for generating tests, whereas our approach provides six evaluation scenarios for developers to choose from.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">The effectiveness of RAG pipelines is assessed through various methods, including evaluating individual components and their integration, using benchmarks, and applying different evaluation frameworks. For instance, PipeRAG <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib17" title="">2024</a>)</cite> system integrates pipeline parallelism, flexible retrieval intervals, and a performance model to reduce generation latency and enhance quality. The eRAG method <cite class="ltx_cite ltx_citemacro_citep">(Salemi and Zamani, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib29" title="">2024</a>)</cite> evaluates document relevance by utilizing each document individually and comparing the output against downstream task ground truth labels. ClapNQ <cite class="ltx_cite ltx_citemacro_citep">(Rosenthal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib27" title="">2024</a>)</cite> provides a benchmark dataset focusing on long-form question answering to highlight areas for improvement in RAG pipelines. Comprehensive benchmarks evaluating components in RAG pipelines in various scenarios are crucial for a thorough assessment, addressing limitations like outdated information. Another research <cite class="ltx_cite ltx_citemacro_citep">(Cuconasu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib9" title="">2024</a>)</cite> evaluates information retrieval component of RAG pipelines and found that having irrelevant documents in the corpus can unexpectedly improve performance of a RAG pipeline. Tang et al. <cite class="ltx_cite ltx_citemacro_citep">(Tang and Yang, <a class="ltx_ref" href="https://arxiv.org/html/2409.19019v1#bib.bib32" title="">2024</a>)</cite> developed a dataset with multi-hop queries to evaluate RAG pipelines, while our approach focuses on evaluating RAG pipelines with a diverse range of scenarios.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this paper, we present a novel approach for evaluating RAG pipelines through evaluation scenarios. We implemented our approach in RAGProbe and evaluated against 5 open-source RAG implementations. Our approach utilises six different evaluation scenarios to expose limitations in RAG pipelines. Additionally, our approach outperformed the existing state-of-the-art methods by demonstrating higher failure rates across each RAG pipeline and dataset. This addresses a critical aspect of robust evaluation by introducing challenging instances that mirror real-world complexities.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">Future work involves integrating more evaluation scenarios, assessing flakiness by executing generated questions multiple times, and providing suggested fixes (mined from the literature and empirically validated). Further, conducting ablation studies to investigate the impact of chosen configurations (e.g. large language model, embedding model, context size, chunk size, chunking strategy) for RAG pipelines is left for our future work. We also need to generalise the RAG Evaluation Runner to be independent of the RAG implementation. The internals of a RAG pipeline are not known to the developer during evaluation. Therefore, irrespective of internal configurations, identifying evaluation scenarios and generating question-answer pairs for each scenario plays a vital role in evaluating RAG pipelines.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alshahwan et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Nadia Alshahwan, Jubin Chheda, Anastasia Finegenova, Beliz Gokkaya, Mark Harman, Inna Harper, Alexandru Marginean, Shubho Sengupta, and Eddy Wang. 2024.

</span>
<span class="ltx_bibblock">Automated unit test improvement using large language models at meta.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">arXiv preprint arXiv:2402.09171</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock">Self-rag: Learning to retrieve, generate, and critique through self-reflection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">arXiv preprint arXiv:2310.11511</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balaguer et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Angels Balaguer, Vinamra Benara, Renato Luiz de Freitas Cunha, Roberto de M Estevão Filho, Todd Hendry, Daniel Holstein, Jennifer Marsman, Nick Mecklenburg, Sara Malvar, Leonardo O Nunes, et al<span class="ltx_text" id="bib.bib4.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.4.1">arXiv e-prints</em> (2024), arXiv–2401.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barnett et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, and Mohamed Abdelrazek. 2024.

</span>
<span class="ltx_bibblock">Seven Failure Points When Engineering a Retrieval Augmented Generation System.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">arXiv preprint arXiv:2401.05856</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bommasani et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al<span class="ltx_text" id="bib.bib6.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">On the opportunities and risks of foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.4.1">arXiv preprint arXiv:2108.07258</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024b.

</span>
<span class="ltx_bibblock">Benchmarking large language models in retrieval-augmented generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 38. 17754–17762.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Mingda Chen, Xilun Chen, and Wen-tau Yih. 2024a.

</span>
<span class="ltx_bibblock">Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 190–208.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cuconasu et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024.

</span>
<span class="ltx_bibblock">The Power of Noise: Redefining Retrieval for RAG Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">arXiv preprint arXiv:2401.14887</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dasigi et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. 2021.

</span>
<span class="ltx_bibblock">A dataset of information-seeking questions and answers anchored in research papers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">arXiv preprint arXiv:2105.03011</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Es et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2023.

</span>
<span class="ltx_bibblock">Ragas: Automated evaluation of retrieval augmented generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">arXiv preprint arXiv:2309.15217</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fadnis et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Kshitij Fadnis, Siva Sankalp Patel, Odellia Boni, Yannis Katsis, Sara Rosenthal, Benjamin Sznajder, and Marina Danilevsky. 2024.

</span>
<span class="ltx_bibblock">InspectorRAGet: An Introspection Platform for RAG Evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">arXiv preprint arXiv:2404.17347</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2024.

</span>
<span class="ltx_bibblock">Retrieval-generation synergy augmented large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE, 11661–11665.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">arXiv preprint arXiv:2312.10997</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. 2023.

</span>
<span class="ltx_bibblock">Large language models for software engineering: A systematic literature review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">arXiv preprint arXiv:2308.10620</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jeong et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C Park. 2024.

</span>
<span class="ltx_bibblock">Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">arXiv preprint arXiv:2403.14403</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Wenqi Jiang, Shuai Zhang, Boran Han, Jie Wang, Bernie Wang, and Tim Kraska. 2024.

</span>
<span class="ltx_bibblock">Piperag: Fast retrieval-augmented generation via algorithm-system co-design.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">arXiv preprint arXiv:2403.05676</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al<span class="ltx_text" id="bib.bib18.3.1">.</span> 2019.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.4.1">Transactions of the Association for Computational Linguistics</em> 7 (2019), 453–466.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and Zhicheng Dou. 2024.

</span>
<span class="ltx_bibblock">From Matching to Generation: A Survey on Generative Information Retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">arXiv preprint arXiv:2404.14851</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023.

</span>
<span class="ltx_bibblock">Gpteval: Nlg evaluation using gpt-4 with better human alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">arXiv preprint arXiv:2303.16634</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023.

</span>
<span class="ltx_bibblock">Query rewriting for retrieval-augmented large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">arXiv preprint arXiv:2305.14283</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2020.

</span>
<span class="ltx_bibblock">Generation-augmented retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">arXiv preprint arXiv:2009.08553</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016.

</span>
<span class="ltx_bibblock">Ms marco: A human-generated machine reading comprehension dataset.

</span>
<span class="ltx_bibblock">(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.08774 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasool et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zafaryab Rasool, Scott Barnett, Stefanus Kurniawan, Sherwin Balugo, Rajesh Vasa, Courtney Chesser, and Alex Bahar-Fuchs. 2023.

</span>
<span class="ltx_bibblock">Evaluating LLMs on Document-Based QA: Exact Answer Selection and Numerical Extraction using Cogtale dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">arXiv preprint arXiv:2311.07878</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasool et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zafaryab Rasool, Scott Barnett, David Willie, Stefanus Kurniawan, Sherwin Balugo, Srikanth Thudumu, and Mohamed Abdelrazek. 2024.

</span>
<span class="ltx_bibblock">LLMs for Test Input Generation for Semantic Caches.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">arXiv preprint arXiv:2401.08138</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosenthal et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Sara Rosenthal, Avirup Sil, Radu Florian, and Salim Roukos. 2024.

</span>
<span class="ltx_bibblock">CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">arXiv preprint arXiv:2404.02103</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saad-Falcon et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2023.

</span>
<span class="ltx_bibblock">Ares: An automated evaluation framework for retrieval-augmented generation systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">arXiv preprint arXiv:2311.09476</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salemi and Zamani (2024)</span>
<span class="ltx_bibblock">
Alireza Salemi and Hamed Zamani. 2024.

</span>
<span class="ltx_bibblock">Evaluating Retrieval Quality in Retrieval-Augmented Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2404.13781</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siriwardhana et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, and Suranga Nanayakkara. 2021.

</span>
<span class="ltx_bibblock">Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">arXiv preprint arXiv:2106.11517</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, and Ji-Rong Wen. 2024.

</span>
<span class="ltx_bibblock">Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">arXiv preprint arXiv:2402.12052</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang and Yang (2024)</span>
<span class="ltx_bibblock">
Yixuan Tang and Yi Yang. 2024.

</span>
<span class="ltx_bibblock">MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2401.15391</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al<span class="ltx_text" id="bib.bib33.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.4.1">Advances in Neural Information Processing Systems</em> 35 (2022), 24824–24837.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">White et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jules White, Sam Hays, Quchen Fu, Jesse Spencer-Smith, and Douglas C Schmidt. 2023.

</span>
<span class="ltx_bibblock">Chatgpt prompt patterns for improving code quality, refactoring, requirements elicitation, and software design.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">arXiv preprint arXiv:2303.07839</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Woolson (2005)</span>
<span class="ltx_bibblock">
Robert F Woolson. 2005.

</span>
<span class="ltx_bibblock">Wilcoxon signed-rank test.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Encyclopedia of Biostatistics</em> 8 (2005).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Kevin Wu, Eric Wu, and James Zou. 2024.

</span>
<span class="ltx_bibblock">How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">arXiv preprint arXiv:2404.10198</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024.

</span>
<span class="ltx_bibblock">Corrective Retrieval Augmented Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">arXiv preprint arXiv:2401.15884</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E Gonzalez. 2024.

</span>
<span class="ltx_bibblock">Raft: Adapting language model to domain specific rag.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">arXiv preprint arXiv:2403.10131</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al<span class="ltx_text" id="bib.bib39.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">A survey of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.4.1">arXiv preprint arXiv:2303.18223</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. 2023.

</span>
<span class="ltx_bibblock">Large language models can learn rules.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">arXiv preprint arXiv:2310.07064</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 24 23:28:21 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
