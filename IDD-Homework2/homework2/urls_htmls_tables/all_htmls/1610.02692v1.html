<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1610.02692] Open-Ended Visual Question-Answering</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Open-Ended Visual Question-Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Open-Ended Visual Question-Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1610.02692">

<!--Generated on Sun Mar  3 18:45:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<div id="id2" class="ltx_titlepage">
<div id="id2.2" class="ltx_logical-block">
<figure id="id1.1.1" class="ltx_figure ltx_align_center"><img src="/html/1610.02692/assets/figures/logo_upc.png" id="id1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="125" alt="[Uncaptioned image]">
</figure>
<div id="id2.2.p1" class="ltx_para">
<p id="id2.2.p1.1" class="ltx_p ltx_align_center"><span id="id2.2.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:298%;">Open-Ended Visual Question-Answering
<br class="ltx_break"></span>

<span id="id2.2.p1.1.2" class="ltx_text" style="font-size:144%;">A Degree Thesis 
<br class="ltx_break">Submitted to the Faculty of the 
<br class="ltx_break">Escola Tècnica Superior d’Enginyeria de Telecomunicació de Barcelona
<br class="ltx_break"></span>
<span id="id2.2.p1.1.3" class="ltx_text" style="font-size:144%;">In partial fulfilment 
<br class="ltx_break">of the requirements for the degree in 
<br class="ltx_break">SCIENCE AND TELECOMMUNICATION TECHNOLOGIES ENGINEERING
<br class="ltx_break"></span>

<span id="id2.2.p1.1.4" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="id2.2.p1.1.4.1.1" class="ltx_tr">
<span id="id2.2.p1.1.4.1.1.1" class="ltx_td ltx_align_left"><span id="id2.2.p1.1.4.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">Author:</span></span>
<span id="id2.2.p1.1.4.1.1.2" class="ltx_td ltx_align_left"><span id="id2.2.p1.1.4.1.1.2.1" class="ltx_text" style="font-size:120%;">Issey Masuda Mora</span></span></span>
<span id="id2.2.p1.1.4.2.2" class="ltx_tr">
<span id="id2.2.p1.1.4.2.2.1" class="ltx_td ltx_align_left"><span id="id2.2.p1.1.4.2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">Advisors:</span></span>
<span id="id2.2.p1.1.4.2.2.2" class="ltx_td ltx_align_left"><span id="id2.2.p1.1.4.2.2.2.1" class="ltx_text" style="font-size:120%;">Xavier Giró i Nieto, Santiago Pascual de la Puente</span></span></span>
</span>
</span></p>
<p id="id2.2.p1.2" class="ltx_p ltx_align_center"><span id="id2.2.p1.2.1" class="ltx_text ltx_font_bold" style="font-size:120%;">Universitat Politècnica de Catalunya (UPC)
<br class="ltx_break">June 2016
<br class="ltx_break"></span></p>
</div>
</div>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text"></span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Chx1" class="ltx_chapter ltx_indentfirst">
<h2 class="ltx_title ltx_title_chapter"></h2>

<div id="Chx1.p1" class="ltx_para">
<p id="Chx1.p1.1" class="ltx_p ltx_align_right"><em id="Chx1.p1.1.1" class="ltx_emph ltx_font_italic" style="font-size:144%;">To my grandfather</em><span id="Chx1.p1.1.2" class="ltx_text" style="font-size:144%;"></span></p>
</div>
</section>
<section id="Chx2" class="ltx_chapter ltx_indentfirst">
<h2 class="ltx_title ltx_title_chapter">Abstract</h2>

<div id="Chx2.p1" class="ltx_para">
<p id="Chx2.p1.1" class="ltx_p">This thesis studies methods to solve Visual Question-Answering (VQA) tasks with a Deep Learning framework.</p>
</div>
<div id="Chx2.p2" class="ltx_para">
<p id="Chx2.p2.1" class="ltx_p">As a preliminary step, we explore Long Short-Term Memory (LSTM) networks used in Natural Language Processing (NLP) to tackle Question-Answering (text based).</p>
</div>
<div id="Chx2.p3" class="ltx_para">
<p id="Chx2.p3.1" class="ltx_p">We then modify the previous model to accept an image as an input in addition to the question. For this purpose, we explore the VGG-16 and K-CNN convolutional neural networks to extract visual features from the image. These are merged with the word embedding or with a sentence embedding of the question to predict the answer.</p>
</div>
<div id="Chx2.p4" class="ltx_para">
<p id="Chx2.p4.1" class="ltx_p">This work was successfully submitted to the Visual Question Answering Challenge 2016, where it achieved a 53,62% of accuracy in the test dataset.</p>
</div>
<div id="Chx2.p5" class="ltx_para">
<p id="Chx2.p5.1" class="ltx_p">The developed software has followed the best programming practices and Python code style, providing a consistent baseline in Keras for different configurations. The source code and models are publicly available at <a target="_blank" href="https://github.com/imatge-upc/vqa-2016-cvprw" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/imatge-upc/vqa-2016-cvprw</a>.</p>
</div>
</section>
<section id="Chx3" class="ltx_chapter ltx_indentfirst">
<h2 class="ltx_title ltx_title_chapter">Resum</h2>

<div id="Chx3.p1" class="ltx_para">
<p id="Chx3.p1.1" class="ltx_p">Aquesta tesis estudia mètodes per resoldre tasques de <em id="Chx3.p1.1.1" class="ltx_emph ltx_font_italic">Visual Question-Answering</em> emprant tècniques de <em id="Chx3.p1.1.2" class="ltx_emph ltx_font_italic">Deep Learning</em>.</p>
</div>
<div id="Chx3.p2" class="ltx_para">
<p id="Chx3.p2.1" class="ltx_p">Com a pas preliminar, explorem les xarxes <em id="Chx3.p2.1.1" class="ltx_emph ltx_font_italic">Long Short-Term Memory</em> (LSTM) que s’utilitzen en el Processat del Llenguatge Natural (NLP) per atacar tasques de <em id="Chx3.p2.1.2" class="ltx_emph ltx_font_italic">Question-Answering</em> basades únicament en text.</p>
</div>
<div id="Chx3.p3" class="ltx_para">
<p id="Chx3.p3.1" class="ltx_p">A continuació modifiquem el model anterior per acceptar una imatge com a entrada juntament amb la pregunta. Per aquest propòsit, estudiem l’ús de les xarxes convolucionals VGG-16 i K-CNN per tal d’extreure els descriptors visuals de la imatge. Aquests descriptors són fusionats amb el <em id="Chx3.p3.1.1" class="ltx_emph ltx_font_italic">word embedding</em> o <em id="Chx3.p3.1.2" class="ltx_emph ltx_font_italic">sentence embedding</em> de la pregunta per poder predir la resposta.</p>
</div>
<div id="Chx3.p4" class="ltx_para">
<p id="Chx3.p4.1" class="ltx_p">Aquest treball ha estat presentat al <em id="Chx3.p4.1.1" class="ltx_emph ltx_font_italic">Visual Question Answering Challenge 2016</em>, on ha obtingut una precisió del 53,62% en les dades de test.</p>
</div>
<div id="Chx3.p5" class="ltx_para">
<p id="Chx3.p5.1" class="ltx_p">El software desenvolupat ha emprat bones pràctiques en programació i ha seguit les directrius d’estil de Python, proveïnt un projecte base en Keras consistent a diferents configuracions. El codi font i els models són públics a <a target="_blank" href="https://github.com/imatge-upc/vqa-2016-cvprw" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/imatge-upc/vqa-2016-cvprw</a>.</p>
</div>
</section>
<section id="Chx4" class="ltx_chapter ltx_indentfirst">
<h2 class="ltx_title ltx_title_chapter">Resumen</h2>

<div id="Chx4.p1" class="ltx_para">
<p id="Chx4.p1.1" class="ltx_p">Esta tesis estudia métodos para resolver tareas de <em id="Chx4.p1.1.1" class="ltx_emph ltx_font_italic">Visual Question-Answering</em> usando técnicas de <em id="Chx4.p1.1.2" class="ltx_emph ltx_font_italic">Deep Learning</em>.</p>
</div>
<div id="Chx4.p2" class="ltx_para">
<p id="Chx4.p2.1" class="ltx_p">Como primer paso, exploramos las redes <em id="Chx4.p2.1.1" class="ltx_emph ltx_font_italic">Long Short-Term Memory</em> (LST) que se usan en el Procesado del Lenguaje Natural (NLP) para atacar tareas de <em id="Chx4.p2.1.2" class="ltx_emph ltx_font_italic">Question-Answering</em> basadas únicamente en texto.</p>
</div>
<div id="Chx4.p3" class="ltx_para">
<p id="Chx4.p3.1" class="ltx_p">A continuación modificamos el modelo anterior para aceptar una imagen como entrada junto con la pregunta. Para este propósito, estudiamos el uso de las redes convolucionales VGG-16 y K-CNN para extraer los descriptores visuales de la imagen. Estos descriptores son fusionados con el <em id="Chx4.p3.1.1" class="ltx_emph ltx_font_italic">word embedding</em> o <em id="Chx4.p3.1.2" class="ltx_emph ltx_font_italic">sentence embedding</em> de la pregunta para poder predecir la respuesta.</p>
</div>
<div id="Chx4.p4" class="ltx_para">
<p id="Chx4.p4.1" class="ltx_p">Este trabajo se ha presentado al <em id="Chx4.p4.1.1" class="ltx_emph ltx_font_italic">Visual Question Answering Challenge 2016</em>, donde ha obtenido una precisión del 53,62% en los datos de test.</p>
</div>
<div id="Chx4.p5" class="ltx_para">
<p id="Chx4.p5.1" class="ltx_p">El software desarrollado ha usado buenas prácticas de programación y ha seguido las directrices de estilo de Python, proveyendo un proyecto base en Keras consistente a distintas configuraciones. El código fuente y los modelos son públicos en <a target="_blank" href="https://github.com/imatge-upc/vqa-2016-cvprw" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/imatge-upc/vqa-2016-cvprw</a>.</p>
</div>
</section>
<section id="Chx5" class="ltx_chapter ltx_indentfirst">
<h2 class="ltx_title ltx_title_chapter">Acknowledgements</h2>

<div id="Chx5.p1" class="ltx_para">
<p id="Chx5.p1.1" class="ltx_p">I would like to thank to my tutor Xavier Giró i Nieto for his help during the whole project, for letting me join his research group and initiating me in this amazing field and for his patience in my moments of stubbornness.</p>
</div>
<div id="Chx5.p2" class="ltx_para">
<p id="Chx5.p2.1" class="ltx_p">I also want to thank Santiago Pascual de la Puente for the countless times he helped me throughout the project with his wise advises and knowledge in Deep Learning.</p>
</div>
<div id="Chx5.p3" class="ltx_para">
<p id="Chx5.p3.1" class="ltx_p">My partners in the X-theses group deserve also a mention here as talking with them week after week about the project and listening what they have been researching has enriched this project. Together with them I would also want to thank Albert Gil for his help and support regarding the GPI cluster usage.</p>
</div>
<div id="Chx5.p4" class="ltx_para">
<p id="Chx5.p4.1" class="ltx_p">I would also like to thank to Marc Bolaños, Petia Radeva and the rest of the Computer Vision group in Universitat de Barcelona for their advice and for providing us with very useful data for our experiments.</p>
</div>
<div id="Chx5.p5" class="ltx_para">
<p id="Chx5.p5.1" class="ltx_p">Last but not least, I want to thank to my family and friends for being there when needed.</p>
</div>
</section>
<section id="Chx6" class="ltx_chapter ltx_indentfirst">
<h2 class="ltx_title ltx_title_chapter">Revision history and approval record</h2>

<figure id="Chx6.tab1" class="ltx_table">
<table id="Chx6.tab1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Chx6.tab1.1.1.1" class="ltx_tr">
<th id="Chx6.tab1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.1.1.1.1.1" class="ltx_p"><span id="Chx6.tab1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Revision</span></span>
</span>
</th>
<th id="Chx6.tab1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:100.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.1.1.2.1.1" class="ltx_p"><span id="Chx6.tab1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Date</span></span>
</span>
</th>
<th id="Chx6.tab1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:150.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.1.1.3.1.1" class="ltx_p"><span id="Chx6.tab1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Purpose</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Chx6.tab1.1.2.1" class="ltx_tr">
<td id="Chx6.tab1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.2.1.1.1.1" class="ltx_p">0</span>
</span>
</td>
<td id="Chx6.tab1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:100.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.2.1.2.1.1" class="ltx_p">11/06/2016</span>
</span>
</td>
<td id="Chx6.tab1.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:150.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.2.1.3.1.1" class="ltx_p">Document creation</span>
</span>
</td>
</tr>
<tr id="Chx6.tab1.1.3.2" class="ltx_tr">
<td id="Chx6.tab1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.3.2.1.1.1" class="ltx_p">1</span>
</span>
</td>
<td id="Chx6.tab1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:100.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.3.2.2.1.1" class="ltx_p">20/06/2016</span>
</span>
</td>
<td id="Chx6.tab1.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:150.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.3.2.3.1.1" class="ltx_p">Document finalization</span>
</span>
</td>
</tr>
<tr id="Chx6.tab1.1.4.3" class="ltx_tr">
<td id="Chx6.tab1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.4.3.1.1.1" class="ltx_p">2</span>
</span>
</td>
<td id="Chx6.tab1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:100.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.4.3.2.1.1" class="ltx_p">24/06/2015</span>
</span>
</td>
<td id="Chx6.tab1.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:150.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.4.3.3.1.1" class="ltx_p">Document revision &amp; corrections</span>
</span>
</td>
</tr>
<tr id="Chx6.tab1.1.5.4" class="ltx_tr">
<td id="Chx6.tab1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.5.4.1.1.1" class="ltx_p">3</span>
</span>
</td>
<td id="Chx6.tab1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:100.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.5.4.2.1.1" class="ltx_p">25/06/2015</span>
</span>
</td>
<td id="Chx6.tab1.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:150.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.5.4.3.1.1" class="ltx_p">Document revision &amp; corrections</span>
</span>
</td>
</tr>
<tr id="Chx6.tab1.1.6.5" class="ltx_tr">
<td id="Chx6.tab1.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.6.5.1.1.1" class="ltx_p">4</span>
</span>
</td>
<td id="Chx6.tab1.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:100.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.6.5.2.1.1" class="ltx_p">26/06/2015</span>
</span>
</td>
<td id="Chx6.tab1.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:150.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.6.5.3.1.1" class="ltx_p">Document revision &amp; corrections</span>
</span>
</td>
</tr>
<tr id="Chx6.tab1.1.7.6" class="ltx_tr">
<td id="Chx6.tab1.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.7.6.1.1.1" class="ltx_p">5</span>
</span>
</td>
<td id="Chx6.tab1.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:100.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.7.6.2.1.1" class="ltx_p">27/06/2015</span>
</span>
</td>
<td id="Chx6.tab1.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:150.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab1.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab1.1.7.6.3.1.1" class="ltx_p">Document revision &amp; corrections</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="Chx6.p1" class="ltx_para">
<p id="Chx6.p1.1" class="ltx_p">DOCUMENT DISTRIBUTION LIST</p>
</div>
<figure id="Chx6.tab2" class="ltx_table">
<table id="Chx6.tab2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Chx6.tab2.1.1.1" class="ltx_tr">
<th id="Chx6.tab2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="width:150.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab2.1.1.1.1.1.1" class="ltx_p"><span id="Chx6.tab2.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Name</span></span>
</span>
</th>
<th id="Chx6.tab2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:150.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab2.1.1.1.2.1.1" class="ltx_p"><span id="Chx6.tab2.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">e-mail</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Chx6.tab2.1.2.1" class="ltx_tr">
<td id="Chx6.tab2.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:150.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab2.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab2.1.2.1.1.1.1" class="ltx_p">Issey Masuda Mora</span>
</span>
</td>
<td id="Chx6.tab2.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:150.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab2.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab2.1.2.1.2.1.1" class="ltx_p">issey.masuda@alu-etsetb.upc.edu</span>
</span>
</td>
</tr>
<tr id="Chx6.tab2.1.3.2" class="ltx_tr">
<td id="Chx6.tab2.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:150.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab2.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab2.1.3.2.1.1.1" class="ltx_p">Xavier Giró i Nieto</span>
</span>
</td>
<td id="Chx6.tab2.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:150.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab2.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab2.1.3.2.2.1.1" class="ltx_p">xavier.giro@upc.edu</span>
</span>
</td>
</tr>
<tr id="Chx6.tab2.1.4.3" class="ltx_tr">
<td id="Chx6.tab2.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="width:150.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab2.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab2.1.4.3.1.1.1" class="ltx_p">Santiago Pascual de la Puente</span>
</span>
</td>
<td id="Chx6.tab2.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:150.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab2.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab2.1.4.3.2.1.1" class="ltx_p">santiago.pascual@tsc.upc.edu</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="Chx6.tab3" class="ltx_table">
<table id="Chx6.tab3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Chx6.tab3.1.1.1" class="ltx_tr">
<th id="Chx6.tab3.1.1.1.1" class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;" colspan="2"><span id="Chx6.tab3.1.1.1.1.1" class="ltx_text ltx_font_bold">Written by:</span></th>
<th id="Chx6.tab3.1.1.1.2" class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;" colspan="2"><span id="Chx6.tab3.1.1.1.2.1" class="ltx_text ltx_font_bold">Reviewed and approved by:</span></th>
<th id="Chx6.tab3.1.1.1.3" class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;" colspan="2"><span id="Chx6.tab3.1.1.1.3.1" class="ltx_text ltx_font_bold">Reviewed and approved by:</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Chx6.tab3.1.2.1" class="ltx_tr">
<td id="Chx6.tab3.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:40.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.2.1.1.1.1" class="ltx_p"><span id="Chx6.tab3.1.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Date</span></span>
</span>
</td>
<td id="Chx6.tab3.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:70.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.2.1.2.1.1" class="ltx_p">20/06/2016</span>
</span>
</td>
<td id="Chx6.tab3.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:40.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.2.1.3.1.1" class="ltx_p"><span id="Chx6.tab3.1.2.1.3.1.1.1" class="ltx_text ltx_font_bold">Date</span></span>
</span>
</td>
<td id="Chx6.tab3.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:70.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.2.1.4.1.1" class="ltx_p">27/06/2016</span>
</span>
</td>
<td id="Chx6.tab3.1.2.1.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:40.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.2.1.5.1.1" class="ltx_p"><span id="Chx6.tab3.1.2.1.5.1.1.1" class="ltx_text ltx_font_bold">Date</span></span>
</span>
</td>
<td id="Chx6.tab3.1.2.1.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:70.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.2.1.6.1.1" class="ltx_p">27/06/2016</span>
</span>
</td>
</tr>
<tr id="Chx6.tab3.1.3.2" class="ltx_tr">
<td id="Chx6.tab3.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:40.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.3.2.1.1.1" class="ltx_p"><span id="Chx6.tab3.1.3.2.1.1.1.1" class="ltx_text ltx_font_bold">Name</span></span>
</span>
</td>
<td id="Chx6.tab3.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:70.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.3.2.2.1.1" class="ltx_p">Issey Masuda Mora</span>
</span>
</td>
<td id="Chx6.tab3.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:40.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.3.2.3.1.1" class="ltx_p"><span id="Chx6.tab3.1.3.2.3.1.1.1" class="ltx_text ltx_font_bold">Name</span></span>
</span>
</td>
<td id="Chx6.tab3.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:70.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.3.2.4.1.1" class="ltx_p">Xavier Giró i Nieto</span>
</span>
</td>
<td id="Chx6.tab3.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:40.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.3.2.5.1.1" class="ltx_p"><span id="Chx6.tab3.1.3.2.5.1.1.1" class="ltx_text ltx_font_bold">Name</span></span>
</span>
</td>
<td id="Chx6.tab3.1.3.2.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:70.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.3.2.6.1.1" class="ltx_p">Santiago Pascual de la Puente</span>
</span>
</td>
</tr>
<tr id="Chx6.tab3.1.4.3" class="ltx_tr">
<td id="Chx6.tab3.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="width:40.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.4.3.1.1.1" class="ltx_p"><span id="Chx6.tab3.1.4.3.1.1.1.1" class="ltx_text ltx_font_bold">Position</span></span>
</span>
</td>
<td id="Chx6.tab3.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:70.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.4.3.2.1.1" class="ltx_p">Project Author</span>
</span>
</td>
<td id="Chx6.tab3.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:40.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.4.3.3.1.1" class="ltx_p"><span id="Chx6.tab3.1.4.3.3.1.1.1" class="ltx_text ltx_font_bold">Position</span></span>
</span>
</td>
<td id="Chx6.tab3.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:70.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.4.3.4.1.1" class="ltx_p">Project Supervisor</span>
</span>
</td>
<td id="Chx6.tab3.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:40.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.4.3.5.1.1" class="ltx_p"><span id="Chx6.tab3.1.4.3.5.1.1.1" class="ltx_text ltx_font_bold">Position</span></span>
</span>
</td>
<td id="Chx6.tab3.1.4.3.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:70.0pt;padding-bottom:2.15277pt;">
<span id="Chx6.tab3.1.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="Chx6.tab3.1.4.3.6.1.1" class="ltx_p">Project Co-supervisor</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<nav class="ltx_TOC ltx_list_toc ltx_toc_toc"><h6 class="ltx_title ltx_title_contents">Contents</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_chapter"><a href="#Chx2" title="In Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title">Abstract</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a href="#Chx3" title="In Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title">Resum</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a href="#Chx4" title="In Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title">Resumen</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a href="#Chx5" title="In Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title">Acknowledgements</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a href="#Chx6" title="In Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title">Revision history and approval record</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a href="#Ch1" title="In Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a href="#Ch1.S1" title="In Chapter 1 Introduction ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Statement of purpose</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#Ch1.S2" title="In Chapter 1 Introduction ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Requirements and specifications</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#Ch1.S3" title="In Chapter 1 Introduction ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>Methods and procedures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#Ch1.S4" title="In Chapter 1 Introduction ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4 </span>Work Plan</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch1.S4.SS1" title="In 1.4 Work Plan ‣ Chapter 1 Introduction ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.1 </span>Work Packages</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch1.S4.SS2" title="In 1.4 Work Plan ‣ Chapter 1 Introduction ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.2 </span>Gantt Diagram</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#Ch1.S5" title="In Chapter 1 Introduction ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.5 </span>Incidents and Modifications</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a href="#Ch2" title="In Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>State of the art</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a href="#Ch2.S1" title="In Chapter 2 State of the art ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Image processing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#Ch2.S2" title="In Chapter 2 State of the art ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Text processing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#Ch2.S3" title="In Chapter 2 State of the art ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Visual Question Answering</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a href="#Ch3" title="In Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#Ch3.S1" title="In Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>A programmer’s word</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch3.S1.SS1" title="In 3.1 A programmer’s word ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Choosing the best language</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch3.S1.SS2" title="In 3.1 A programmer’s word ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>A Pythonic project</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch3.S1.SS3" title="In 3.1 A programmer’s word ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>An eight-leg cat called octocat</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#Ch3.S2" title="In Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#Ch3.S3" title="In Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Text-based QA toy example</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch3.S3.SS1" title="In 3.3 Text-based QA toy example ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Tokenization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch3.S3.SS2" title="In 3.3 Text-based QA toy example ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Model architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch3.S3.SS3" title="In 3.3 Text-based QA toy example ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Model up and running</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#Ch3.S4" title="In Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Extending the text-based QA model for visual QA</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch3.S4.SS1" title="In 3.4 Extending the text-based QA model for visual QA ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Image processing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch3.S4.SS2" title="In 3.4 Extending the text-based QA model for visual QA ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Modifications on the question branch</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch3.S4.SS3" title="In 3.4 Extending the text-based QA model for visual QA ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>Model parameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch3.S4.SS4" title="In 3.4 Extending the text-based QA model for visual QA ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.4 </span>Implementation and training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#Ch3.S5" title="In Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Model improvement: towards the final model</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch3.S5.SS1" title="In 3.5 Model improvement: towards the final model ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.1 </span>Implementation and training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch3.S5.SS2" title="In 3.5 Model improvement: towards the final model ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.2 </span>Batch normalization and reducing the learning rate</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#Ch3.S6" title="In Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>The final model: sentence embedding</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch3.S6.SS1" title="In 3.6 The final model: sentence embedding ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.1 </span>Question embedding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch3.S6.SS2" title="In 3.6 The final model: sentence embedding ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.2 </span>Image semantic projection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch3.S6.SS3" title="In 3.6 The final model: sentence embedding ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.3 </span>Merging and predicting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch3.S6.SS4" title="In 3.6 The final model: sentence embedding ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6.4 </span>Other modifications</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a href="#Ch4" title="In Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a href="#Ch4.S1" title="In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Evaluation metric</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#Ch4.S2" title="In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#Ch4.S3" title="In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Models architectures and setups</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#Ch4.S4" title="In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Training and validation losses</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#Ch4.S5" title="In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Quantitative results in the VQA Challenge 2016</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch4.S5.SS1" title="In 4.5 Quantitative results in the VQA Challenge 2016 ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.1 </span>A general overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch4.S5.SS2" title="In 4.5 Quantitative results in the VQA Challenge 2016 ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.2 </span>Our results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#Ch4.S5.SS3" title="In 4.5 Quantitative results in the VQA Challenge 2016 ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.3 </span>Per answer type results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#Ch4.S6" title="In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Qualitative results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#Ch4.S7" title="In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>Some words at the VQA Challenge</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a href="#Ch5" title="In Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Budget</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a href="#Ch6" title="In Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a href="#Ch7" title="In Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Appendices</span></a></li>
</ol></nav>
<div class="ltx_pagination ltx_role_newpage"></div>
<nav class="ltx_TOC ltx_list_lof ltx_toc_lof"><h6 class="ltx_title ltx_title_contents">List of Figures</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch1.F1" title="Figure 1.1In Chapter 1 Introduction ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Real example of the Visual Question-Answering dataset. The complexity of the task and the required abilities are appreciable in this example where, in order to succeed, the model needs to solve an object retrieval-like task but with the addition of having to understand the scene and the mentions of the question, <em class="ltx_emph ltx_font_italic">e.g.</em> the relationship between the word ’flying’ and the object position</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch1.F2" title="Figure 1.2In 1.4 Work Plan ‣ Chapter 1 Introduction ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Gantt Diagram of the project</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch2.F1" title="Figure 2.1In Chapter 2 State of the art ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>LeNet, an example of Convolutional Neutal Network</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch2.F2" title="Figure 2.2In Chapter 2 State of the art ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>DPPnet, the question is used to learn how to predict parameters for a dynamic parameter layer in the classification network.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch3.F1" title="Figure 3.1In 3.3 Text-based QA toy example ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Schema of the model used for text-based QA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch3.F2" title="Figure 3.2In 3.4 Extending the text-based QA model for visual QA ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>First visual QA model’s schema</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch3.F3" title="Figure 3.3In Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Replacement of the VGG-16 by the KCNN model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch3.F4" title="Figure 3.4In 3.5 Model improvement: towards the final model ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Batch normalization added to help the training process</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch3.F5" title="Figure 3.5In 3.6 The final model: sentence embedding ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Final model. A sentence embedding is used for the question and the visual features are projected into the same semantic space than the question</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch4.F2" title="Figure 4.2In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Training and validation losses</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch4.F3" title="Figure 4.3In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Training losses (blue) and validation losses (green) for model 3</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch4.F5" title="Figure 4.5In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Training and validation losses</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch4.F6" title="Figure 4.6In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Result examples with accuracy 0</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch4.F7" title="Figure 4.7In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>Result example with accuracy 30</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch4.F8" title="Figure 4.8In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.8 </span>Result example with accuracy 60</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch4.F9" title="Figure 4.9In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.9 </span>Result example with accuracy 90</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#Ch4.F10" title="Figure 4.10In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.10 </span>Result example with accuracy 100</span></a></li>
</ol></nav>
<div class="ltx_pagination ltx_role_newpage"></div>
<nav class="ltx_TOC ltx_list_lot ltx_toc_lot"><h6 class="ltx_title ltx_title_contents">List of Tables</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_table"><a href="#Ch4.T1" title="Table 4.1In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Models identifiers and descriptions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a href="#Ch4.T2" title="Table 4.2In Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results of the four baselines provided by the VQA challenge organizer, the state-of-the-art (UC Berkeley &amp; Sony) and our five models. Model 4 was the one submitted to the challenge leaderboard as the results of our team</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a href="#Ch5.T1" title="Table 5.1In Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Budget of the project</span></a></li>
</ol></nav>
</section>
<section id="Ch1" class="ltx_chapter ltx_indent_first">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 1 </span>Introduction</h2>

<section id="Ch1.S1" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.1 </span>Statement of purpose</h3>

<div id="Ch1.S1.p1" class="ltx_para">
<p id="Ch1.S1.p1.1" class="ltx_p">In the last few years the number of published papers and job offers related to Deep Learning have exploded. Both the academic world and the industry are pushing forward to speed up the developments and the research in this area. The reason is that Deep Learning has shown a great performance solving a lot of problems that were previously tackled by more classic Machine Learning algorithms and it has also opened the door to more complex tasks that we could not solve before.</p>
</div>
<div id="Ch1.S1.p2" class="ltx_para">
<p id="Ch1.S1.p2.1" class="ltx_p">We humans are constantly asking questions and answering them. This is the way we learn, we transfer knowledge and lastly we communicate with each other. This basic framework of communication has inspired other ways of communications such as the HTTP protocol which is basically a combination of a request (question) and a response (answer). Frequently Asked Questions (FAQ) also uses this format.</p>
</div>
<div id="Ch1.S1.p3" class="ltx_para">
<p id="Ch1.S1.p3.1" class="ltx_p">But what about machines? Artificial Intelligence is a huge science-fiction topic and it is recurrently all over the news and media, but the reality is not that far from there. Deep Neural Networks are nowadays used in our everyday life when we surf the net, when we use recommendation systems or automatic translation systems. This has also been extended to tackle Question-Answering tasks from the Natural Language Processing perspective (<em id="Ch1.S1.p3.1.1" class="ltx_emph ltx_font_italic">e.g.</em> Facebook AI Research presented a set of tasks, called bAbI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, to evaluate AI models’ text understanding and reasoning).</p>
</div>
<div id="Ch1.S1.p4" class="ltx_para">
<p id="Ch1.S1.p4.1" class="ltx_p">Visual Question-Answering has emerged as an evolution of these text-based QA systems. These models aim to be able to answer a given natural question related to a given image. One of the interests in such models is that in order to succeed in these visual QA tasks (or even just text-based QA), they need to have a much deeper level of <em id="Ch1.S1.p4.1.1" class="ltx_emph ltx_font_italic">reasoning</em> and understanding than other similar models, for example image captioning models. An example of a VQA task is shown in Figure <a href="#Ch1.F1" title="Figure 1.1 ‣ 1.1 Statement of purpose ‣ Chapter 1 Introduction ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1.1</span></a></p>
</div>
<figure id="Ch1.F1" class="ltx_figure"><img src="/html/1610.02692/assets/figures/vqa_goal.png" id="Ch1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="292" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Ch1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1.1</span>: </span><span id="Ch1.F1.4.2" class="ltx_text" style="font-size:90%;">Real example of the Visual Question-Answering dataset. The complexity of the task and the required abilities are appreciable in this example where, in order to succeed, the model needs to solve an object retrieval-like task but with the addition of having to understand the scene and the mentions of the question, <em id="Ch1.F1.4.2.1" class="ltx_emph ltx_font_italic">e.g.</em> the relationship between the word ’flying’ and the object position</span></figcaption>
</figure>
<div id="Ch1.S1.p5" class="ltx_para">
<p id="Ch1.S1.p5.1" class="ltx_p">This thesis studies new models to tackle VQA problems. The common point of all these models is that they use Convolutional Neural Networks (CNN) to process the image and extract visual features, which are a summarized representation of the image, and Long Short-Term Memory networks (LSTM), a flavor of Recurrent Neural Network (RNN), to process the question sequence.</p>
</div>
<div id="Ch1.S1.p6" class="ltx_para">
<p id="Ch1.S1.p6.1" class="ltx_p">Based on the given context, the main objectives of this project are:</p>
<ul id="Ch1.S1.I1" class="ltx_itemize">
<li id="Ch1.S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S1.I1.i1.p1" class="ltx_para">
<p id="Ch1.S1.I1.i1.p1.1" class="ltx_p">Explore the techniques used for text-based Question-Answering</p>
</div>
</li>
<li id="Ch1.S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S1.I1.i2.p1" class="ltx_para">
<p id="Ch1.S1.I1.i2.p1.1" class="ltx_p">Build a model able to perform visual question-answering</p>
</div>
</li>
<li id="Ch1.S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S1.I1.i3.p1" class="ltx_para">
<p id="Ch1.S1.I1.i3.p1.1" class="ltx_p">Compare which approach is better to process the question: word embedding or sentence embedding. These are two different techniques to project a text into a space with semantic relations, meaning that you can perform some arithmetic operations with those embedding and the result will have semantic sense</p>
</div>
</li>
<li id="Ch1.S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S1.I1.i4.p1" class="ltx_para">
<p id="Ch1.S1.I1.i4.p1.1" class="ltx_p">Try different architectures and parameters to increase model’s accuracy</p>
</div>
</li>
<li id="Ch1.S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S1.I1.i5.p1" class="ltx_para">
<p id="Ch1.S1.I1.i5.p1.1" class="ltx_p">Develop a reusable software project using programming good practices</p>
</div>
</li>
<li id="Ch1.S1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S1.I1.i6.p1" class="ltx_para">
<p id="Ch1.S1.I1.i6.p1.1" class="ltx_p">Participate in Visual Question Answering Challenge<span id="Ch1.footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="http://www.visualqa.org/challenge.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.visualqa.org/challenge.html</a></span></span></span>, hosted as workshop in the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2016.</p>
</div>
</li>
</ul>
</div>
<div id="Ch1.S1.p7" class="ltx_para">
<p id="Ch1.S1.p7.1" class="ltx_p">Regarding the last item, we presented our results to the challenge with an accuracy of 53,62% (details on the results chapter <a href="#Ch4" title="Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and the model employed will be discussed further on the methodology chapter <a href="#Ch3" title="Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div id="Ch1.S1.p8" class="ltx_para">
<p id="Ch1.S1.p8.1" class="ltx_p">We also achieved an additional goal, which was not planned at the beginning of the project. We submitted an extended abstract (you can find it in the appendices <a href="#Ch7" title="Chapter 7 Appendices ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>) to the CVPR16 VQA workshop<span id="Ch1.footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="http://www.visualqa.org/workshop.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.visualqa.org/workshop.html</a></span></span></span> and it was accepted by the organizers. Due to this fact, we were invited to present our extended abstract and a poster in the workshop.</p>
</div>
</section>
<section id="Ch1.S2" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.2 </span>Requirements and specifications</h3>

<div id="Ch1.S2.p1" class="ltx_para">
<p id="Ch1.S2.p1.1" class="ltx_p">One of the main blocks of this project is the software developed to participate in the challenge and to be able to create and test different models.</p>
</div>
<div id="Ch1.S2.p2" class="ltx_para">
<p id="Ch1.S2.p2.1" class="ltx_p">Regarding with this, the requirements of this project are the following:</p>
</div>
<div id="Ch1.S2.p3" class="ltx_para">
<ul id="Ch1.S2.I1" class="ltx_itemize">
<li id="Ch1.S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S2.I1.i1.p1" class="ltx_para">
<p id="Ch1.S2.I1.i1.p1.1" class="ltx_p">Develop a software that can be used in the future to keep doing research in this field, having a skeleton/base project to start with</p>
</div>
</li>
<li id="Ch1.S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S2.I1.i2.p1" class="ltx_para">
<p id="Ch1.S2.I1.i2.p1.1" class="ltx_p">Build a deep neural network model that uses NLP and CV techniques to process the question and the image respectively</p>
</div>
</li>
<li id="Ch1.S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S2.I1.i3.p1" class="ltx_para">
<p id="Ch1.S2.I1.i3.p1.1" class="ltx_p">Try different model configurations to increase the accuracy of the original model</p>
</div>
</li>
<li id="Ch1.S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S2.I1.i4.p1" class="ltx_para">
<p id="Ch1.S2.I1.i4.p1.1" class="ltx_p">Submit results to the CVPR16 VQA Challenge</p>
</div>
</li>
</ul>
</div>
<div id="Ch1.S2.p4" class="ltx_para">
<p id="Ch1.S2.p4.1" class="ltx_p">The specifications are the following:</p>
</div>
<div id="Ch1.S2.p5" class="ltx_para">
<ul id="Ch1.S2.I2" class="ltx_itemize">
<li id="Ch1.S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S2.I2.i1.p1" class="ltx_para">
<p id="Ch1.S2.I2.i1.p1.1" class="ltx_p">Use Python as a programming language</p>
</div>
</li>
<li id="Ch1.S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S2.I2.i2.p1" class="ltx_para">
<p id="Ch1.S2.I2.i2.p1.1" class="ltx_p">Build the project using a deep learning framework. Keras<span id="Ch1.footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="http://keras.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://keras.io/</a></span></span></span> has been chosen as the framework and it can run upon Theano<span id="Ch1.footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="http://deeplearning.net/software/theano/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://deeplearning.net/software/theano/</a></span></span></span> or TensorFlow<span id="Ch1.footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://www.tensorflow.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/</a></span></span></span> backends.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="Ch1.S3" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.3 </span>Methods and procedures</h3>

<div id="Ch1.S3.p1" class="ltx_para">
<p id="Ch1.S3.p1.1" class="ltx_p">This thesis represents the first attempt at solving the Visual Question Answering problem by GPI and TALP research groups at the Universitat Politècnica de Catalunya. We started to develop the project from scratch (in terms of the project itself) but using a deep learning framework called Keras.
Keras is a neural network library for Python that has been build to be easy to use and allow fast prototyping. It accomplished this by building a wrapper around another deep learning python library that is in charge of managing tensors and the low level computations. This second library, that works as a <em id="Ch1.S3.p1.1.1" class="ltx_emph ltx_font_italic">backend</em>, can be either Theano or TensorFlow. We have run our experiments using Keras over Theano.</p>
</div>
<div id="Ch1.S3.p2" class="ltx_para">
<p id="Ch1.S3.p2.1" class="ltx_p">Apart from these libraries, the only resource developed by other authors are the visual features of our last model. The Computer Vision group at Universitat de Barcelona provided us with the precomputed visual features of the images from the VQA dataset. They extracted these features using a special kind of CNNs called Kernelized CNN (KCNN) as proposed by Liu <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
The KCNN method aims to provide a better vectorized representation of images than vanilla CNNs as they have a lack when the image has complex content. This model uses CNNs to extract features and then aggreagate them into a vectorial representation using the Fisher vector model.</p>
</div>
</section>
<section id="Ch1.S4" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.4 </span>Work Plan</h3>

<div id="Ch1.S4.p1" class="ltx_para">
<p id="Ch1.S4.p1.1" class="ltx_p">The project was developed as a collaboration between the GPI and TALP research groups of the Universitat Politècnica de Catalunya. Discussions and decisions about the project were held in a regular weekly meeting, which was complemented with a second research seminar of two hours per week with other students developing their bachelor, master or Phd thesis at GPI.</p>
</div>
<div id="Ch1.S4.p2" class="ltx_para">
<p id="Ch1.S4.p2.1" class="ltx_p">The following is the workplan of this project and its deviations from the original plan. These deviations are explained in detail in the Incidents and Modifications subsection <a href="#Ch1.S5" title="1.5 Incidents and Modifications ‣ Chapter 1 Introduction ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1.5</span></a>.</p>
</div>
<section id="Ch1.S4.SS1" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4.1 </span>Work Packages</h4>

<div id="Ch1.S4.SS1.p1" class="ltx_para">
<ul id="Ch1.S4.I1" class="ltx_itemize">
<li id="Ch1.S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S4.I1.i1.p1" class="ltx_para">
<p id="Ch1.S4.I1.i1.p1.1" class="ltx_p">WP 1: Project proposal and work plan.</p>
</div>
</li>
<li id="Ch1.S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S4.I1.i2.p1" class="ltx_para">
<p id="Ch1.S4.I1.i2.p1.1" class="ltx_p">WP 2: Introduction to deep learning and Python</p>
</div>
</li>
<li id="Ch1.S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S4.I1.i3.p1" class="ltx_para">
<p id="Ch1.S4.I1.i3.p1.1" class="ltx_p">WP 3: Introduction to (visual) question-answering tasks and Keras framework</p>
</div>
</li>
<li id="Ch1.S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S4.I1.i4.p1" class="ltx_para">
<p id="Ch1.S4.I1.i4.p1.1" class="ltx_p">WP 4: First VQA model</p>
</div>
</li>
<li id="Ch1.S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S4.I1.i5.p1" class="ltx_para">
<p id="Ch1.S4.I1.i5.p1.1" class="ltx_p">WP 5: Critical Review of the project</p>
</div>
</li>
<li id="Ch1.S4.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S4.I1.i6.p1" class="ltx_para">
<p id="Ch1.S4.I1.i6.p1.1" class="ltx_p">WP 6: Participate in the CVPR16 VQA Challenge</p>
</div>
</li>
<li id="Ch1.S4.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S4.I1.i7.p1" class="ltx_para">
<p id="Ch1.S4.I1.i7.p1.1" class="ltx_p">WP 7: Final report of the project</p>
</div>
</li>
<li id="Ch1.S4.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch1.S4.I1.i8.p1" class="ltx_para">
<p id="Ch1.S4.I1.i8.p1.1" class="ltx_p">WP 8: Presentation and oral defense</p>
</div>
</li>
</ul>
</div>
</section>
<section id="Ch1.S4.SS2" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4.2 </span>Gantt Diagram</h4>

<figure id="Ch1.F2" class="ltx_figure"><img src="/html/1610.02692/assets/figures/Gantt.jpg" id="Ch1.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="619" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Ch1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1.2</span>: </span><span id="Ch1.F2.3.2" class="ltx_text" style="font-size:90%;">Gantt Diagram of the project</span></figcaption>
</figure>
</section>
</section>
<section id="Ch1.S5" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1.5 </span>Incidents and Modifications</h3>

<div id="Ch1.S5.p1" class="ltx_para">
<p id="Ch1.S5.p1.1" class="ltx_p">During the project we needed to modify some work packages definition and tasks as we wanted to focus more on the Visual Question-Answering Challenge.</p>
</div>
<div id="Ch1.S5.p2" class="ltx_para">
<p id="Ch1.S5.p2.1" class="ltx_p">Initially, the goal of the project was developing a system capable of generating both questions and answers from an image. This would have medical application in patients with mild cognitive impairment (early stages of Alzheimer), who may receive an automatized reminiscence therapy based on the images captured by egocentric cameras. However, solving the VQA challenge was more feasible in terms of annotated datasets, metrics and potential impact, so it was decided to address this task first. The described medical applications are planned to be explored by other students during Fall 2016.</p>
</div>
<div id="Ch1.S5.p3" class="ltx_para">
<p id="Ch1.S5.p3.1" class="ltx_p">We also included the new task of writing an extended abstract for the VQA workshop. We decided to write and submit the extended abstract as this would gave me some expertise on paper composing and this way we could share with the community some of our ideas.</p>
</div>
</section>
</section>
<section id="Ch2" class="ltx_chapter ltx_indent_first">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 2 </span>State of the art</h2>

<div id="Ch2.p1" class="ltx_para">
<p id="Ch2.p1.1" class="ltx_p">In the past years, multidisciplinary problems of vision, language and reasoning have emerged as a trend in Artificial Intelligence (AI) research. This tasks join Computer Vision (CV), Natural Language Processing (NLP) and Knowledge Representation and Reasoning (KR) to be able to build models that can interact with both image and language input/output. However, this models still fail achieving accuracies close to human level.</p>
</div>
<div id="Ch2.p2" class="ltx_para">
<p id="Ch2.p2.1" class="ltx_p">Visual Question-Answering has appeared as a problem where models need to be able to perform different sub-problems of the above three fields in order to succeed. To solve this problems the models need a much deeper understanding and comprehension of the scene in the image, what the question is referring to and how the items are related.</p>
</div>
<div id="Ch2.p3" class="ltx_para">
<p id="Ch2.p3.1" class="ltx_p">We will revise some of the literature involved in the process of building a VQA model, from image and text processing, to the state-of-the-art approaches for VQA tasks.</p>
</div>
<section id="Ch2.S1" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.1 </span>Image processing</h3>

<div id="Ch2.S1.p1" class="ltx_para">
<p id="Ch2.S1.p1.1" class="ltx_p">Deep Convolutional Neural Networks (CNN) have been proved to achieve state-of-the-art results in typical Computer Vision tasks such as image retrieval, object detection and object recognition.</p>
</div>
<div id="Ch2.S1.p2" class="ltx_para">
<p id="Ch2.S1.p2.1" class="ltx_p">A common approach when dealing with images is to use an off-the-shelf model (VGG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, GoogLeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, etc.) pre-trained to do such tasks with some large image dataset such as ImageNet<span id="Ch2.footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="http://www.image-net.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.image-net.org/</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and use some of the inner-layer’s outputs as a representation of the visual features of the image.</p>
</div>
<figure id="Ch2.F1" class="ltx_figure"><img src="/html/1610.02692/assets/figures/lenet5.png" id="Ch2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Ch2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2.1</span>: </span><span id="Ch2.F1.3.2" class="ltx_text" style="font-size:90%;">LeNet, an example of Convolutional Neutal Network</span></figcaption>
</figure>
<div id="Ch2.S1.p3" class="ltx_para">
<p id="Ch2.S1.p3.1" class="ltx_p">Typically these models have different types of layers, amongst the most common convolutional layers (that give the name to the CNNs) and fully-connected layers. 
<br class="ltx_break">The convolutional layers used in image processing perform 2D convolutions of the previous layer output (which can be an image) where the weights specify the convolution filter. 
<br class="ltx_break">In contrast, fully-connected layers take each output from the previous layer and connect them to all of its neurons, losing the spatial information so they can be seen as one dimensional. One of the most common fully-connected layers is the so called softmax layer, which is a regular fully-connected with the softmax as activation function. Its output follows a distribution-like shape, taking values from 0 to 1 and being the addition of all of them equal to 1.</p>
</div>
</section>
<section id="Ch2.S2" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.2 </span>Text processing</h3>

<div id="Ch2.S2.p1" class="ltx_para">
<p id="Ch2.S2.p1.1" class="ltx_p">In order to process sequences of text, different approaches are used. For the sake of the simplicity, we will only review two of them that are important for this work.</p>
</div>
<div id="Ch2.S2.p2" class="ltx_para">
<p id="Ch2.S2.p2.1" class="ltx_p">The first one is the word embedding representation using Skip-gram technique presented by Mikolov <em id="Ch2.S2.p2.1.1" class="ltx_emph ltx_font_italic">et. al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. This method is used to learn high-quality word vector representations. The input is usually the index of the word in a dictionary (<em id="Ch2.S2.p2.1.2" class="ltx_emph ltx_font_italic">i.e.</em> its one-hot code), a vector as large as the size of the vocabulary which is zero-valued except at the index position corresponding to the word. These vectors are high-dimensional (as the dictionary size can have thousands or hundred of thousands of words) and sparse due to the nature of the one-hot representation. The word embedding projects this vector into a semantic space where each word is represented by a dense vector with less dimensions.
This technique captures semantic and syntactic relationships between words and also encodes many linguistic patterns based on the context where the words appear. These patterns can be expressed as algebraic operations, <em id="Ch2.S2.p2.1.3" class="ltx_emph ltx_font_italic">e.g.</em> embed(”King”) - embed(”Man”) + embed(”Woman”) has as the closest vector the embedding of ”Queen”.</p>
</div>
<div id="Ch2.S2.p3" class="ltx_para">
<p id="Ch2.S2.p3.1" class="ltx_p">The logical evolution of this representation is what is called sentence embedding. Word embedding fails at capturing the long-term dependencies between words that appear together in a sentence. To solve this problem, sentence embedding uses Recurrent Neural Networks (RNN) with Long Short-Term Memory cells (LSTM) to increasingly accumulate richer information about the sentence. 
<br class="ltx_break">Note that RNN are deep neural networks with a memory state that allows them to retain temporal context information, so they take care of the dependence of the current prediction based on the current input and also the past one. LSTM where proposed by Hochreiter <em id="Ch2.S2.p3.1.1" class="ltx_emph ltx_font_italic">et. al.</em> to improve the quality of the long-term memory that these models have by means of gating mechanisms that control the flow of information getting in and out of the network. For further details address <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. 
<br class="ltx_break">The RNN sentence embedding method presented by Palangi <em id="Ch2.S2.p3.1.2" class="ltx_emph ltx_font_italic">et. al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> takes the one-hot representation for each of the words in the text sequences, obtains its word embedding and then feeds the LSTM with them, one at each timestep, keeping the same order as presented in the sequence. The LSTM will update its state based on this embedding and therefore will be accumulating the information of each word and its own context. At the end of this process the LSTM state will have a condensed representation of the whole sentence.</p>
</div>
<div id="Ch2.S2.p4" class="ltx_para">
<p id="Ch2.S2.p4.1" class="ltx_p">Such dense representations of sequences of text have also been addressed by Cho <em id="Ch2.S2.p4.1.1" class="ltx_emph ltx_font_italic">et. al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> for statistical machine translation with GRU cells, which are a similar approach to that of LSTM. They proposed a RNN architecture called Encoder-Decoder. The first stage encodes a sequence into a fixed-length vector representation and the other decodes the vector into another sequence of arbitrary length. The resulting vector after the encoder stage can be used to represent the sentence.</p>
</div>
</section>
<section id="Ch2.S3" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2.3 </span>Visual Question Answering</h3>

<div id="Ch2.S3.p1" class="ltx_para">
<p id="Ch2.S3.p1.1" class="ltx_p">Visual Question Answering is a novel problem for the computer vision and natural language communities, but is has received a lot of attention thanks to the dataset and metrics released with the VQA challenge, together with the large investments of pioneering tech companies such as Microsoft, Facebook or Sony.</p>
</div>
<div id="Ch2.S3.p2" class="ltx_para">
<p id="Ch2.S3.p2.1" class="ltx_p">The most common approach is to extract visual features of the image using a pretrained off-the-shelf network and process the question using word embeddings or sentence embedding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<div id="Ch2.S3.p3" class="ltx_para">
<p id="Ch2.S3.p3.1" class="ltx_p">Antol <em id="Ch2.S3.p3.1.1" class="ltx_emph ltx_font_italic">et. al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, the organizers of the VQA challenge and the creators of the VQA dataset, propose as their baseline a model that uses VGG-16 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> to extract the visual features of the image. They use as the representation the output of the last hidden layer of this model. This features are then <math id="Ch2.S3.p3.1.m1.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="Ch2.S3.p3.1.m1.1a"><msub id="Ch2.S3.p3.1.m1.1.1" xref="Ch2.S3.p3.1.m1.1.1.cmml"><mi id="Ch2.S3.p3.1.m1.1.1.2" xref="Ch2.S3.p3.1.m1.1.1.2.cmml">l</mi><mn id="Ch2.S3.p3.1.m1.1.1.3" xref="Ch2.S3.p3.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="Ch2.S3.p3.1.m1.1b"><apply id="Ch2.S3.p3.1.m1.1.1.cmml" xref="Ch2.S3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="Ch2.S3.p3.1.m1.1.1.1.cmml" xref="Ch2.S3.p3.1.m1.1.1">subscript</csymbol><ci id="Ch2.S3.p3.1.m1.1.1.2.cmml" xref="Ch2.S3.p3.1.m1.1.1.2">𝑙</ci><cn type="integer" id="Ch2.S3.p3.1.m1.1.1.3.cmml" xref="Ch2.S3.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch2.S3.p3.1.m1.1c">l_{2}</annotation></semantics></math> normalized and given to a fully-connected layer to transform the incoming vector into a common space with the question representation. For the question, they use a 2-layer LSTM that takes as the input the word embedding of each question token, timestep by timestep, and when the whole question has been introduced into the LSTM, it outputs its last state as the question embedding. This vector (dimension 2048) is also given to a fully-connected (similarly with what they do with the image) to project it to the same space. Both features are combined using an element-wise multiplication for later use by a fully-connected layer and a softmax that will predict the class answer. Here the 1000 most common answers in the training set have been selected as the classes to predict.</p>
</div>
<div id="Ch2.S3.p4" class="ltx_para">
<p id="Ch2.S3.p4.1" class="ltx_p">A simple bag-of-words and word embedding model that uses GoogLeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> for the image processing and a concatenation of both visual and textual features is what Zhou <em id="Ch2.S3.p4.1.1" class="ltx_emph ltx_font_italic">et. al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> present in their paper as a basic approximation to VQA.</p>
</div>
<div id="Ch2.S3.p5" class="ltx_para">
<p id="Ch2.S3.p5.1" class="ltx_p">A quite different method from the ones presented above is what Noh <em id="Ch2.S3.p5.1.1" class="ltx_emph ltx_font_italic">et. al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> propose, called Dynamic Parameter Prediction Network (DPPnet). They state that in order to solve VQA tasks, different <em id="Ch2.S3.p5.1.2" class="ltx_emph ltx_font_italic">networks</em> need to be used as the model need to perform different tasks depending on the question. To accomplish this, they use the question to predict the weights of one of the layers in the networks, thus changing it at test time for each sample. They take the VGG-16 pretrained with ImageNet model as their starting point. Then, they remove the softmax layer and add three fully-connected layers (the last one being a softmax). They have named ’classification network’ to this modified VGGnet. The interesting point is that the second one of those new fully-connected layers is a dynamic parameter layer. This means that at test time, the weights of this layer will be changing from sample to sample. These weights are predicted by a network (parameter prediction network) composed by a Gated Recurrent Unit (GRU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> connected to a fully-connected layer. GRU is another kind of RNN similar to LSTM. This layer takes the embedding of each word in the question as its input and when the whole question has passed through the network, its last state is given to a fully-connected layer which predicts the weight candidates for the dynamic parameter layer in the classification network (the VGG-16 based network). To reduce the number of parameters to predict, a hashing function is used to map from the predicted weights of the prediction network to the actual weights of the fully-connected. Figure <a href="#Ch2.F2" title="Figure 2.2 ‣ 2.3 Visual Question Answering ‣ Chapter 2 State of the art ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a> depicts the whole architecture of their model.</p>
</div>
<figure id="Ch2.F2" class="ltx_figure"><img src="/html/1610.02692/assets/figures/dppnet.jpg" id="Ch2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Ch2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2.2</span>: </span><span id="Ch2.F2.3.2" class="ltx_text" style="font-size:90%;">DPPnet, the question is used to learn how to predict parameters for a dynamic parameter layer in the classification network.</span></figcaption>
</figure>
<div id="Ch2.S3.p6" class="ltx_para">
<p id="Ch2.S3.p6.1" class="ltx_p">Other authors propose attention models to improve the performance of the whole model, stating that most of the questions refer to specific image locations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> the visual features (the output of fc7 of VGG-16) are treated as if they were the first word in the question, that is fed into a LSTM word embedding by word embedding. The attention model depends on the LSTM state and is used to weight convolutional features of the image (output of the last conv layer of the VGG-16), that are again introduced in the LSTM merged (using addition) with a word embedding. Ren <em id="Ch2.S3.p6.1.1" class="ltx_emph ltx_font_italic">et. al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> present a similar but simplified method that also treats the image as the first word of the question but that does not have an attention model. Xiong <em id="Ch2.S3.p6.1.2" class="ltx_emph ltx_font_italic">et. al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> present a model based on Dynamic Memory Networks (DNM), that is a modular architecture with attention models. They created a new input module to be able to perform VQA tasks apart from text-based QA and improve the memory module. They use bidirectional GRU so that each feature (textual or visual) has a full context representation thus representing local and global information.</p>
</div>
<div id="Ch2.S3.p7" class="ltx_para">
<p id="Ch2.S3.p7.1" class="ltx_p">All these methods present visual attention models but as proposed by Lu <em id="Ch2.S3.p7.1.1" class="ltx_emph ltx_font_italic">et. al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, attention in the question can also be applied to increase the model performance. This method, called co-attention together with a hierarchical representation of the question helped them to achieve state-of-the-art accuracy with the VQA dataset (and using their evaluation metric). Recently another method has outperformed their results.</p>
</div>
<div id="Ch2.S3.p8" class="ltx_para">
<p id="Ch2.S3.p8.1" class="ltx_p">More sophisticated approaches have also been presented, such as Multimodal Residual Learning applied to VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> that uses Deep Residual Learning to build complex and very deep networks. Other works propose learning methods for specific sub-problems of VQA such as human action prediction and then apply those trained models for VQA tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="Ch2.S3.p9" class="ltx_para">
<p id="Ch2.S3.p9.1" class="ltx_p">At writing time, the model that achieves state-of-the-art accuracy is the one proposed by Fukui <em id="Ch2.S3.p9.1.1" class="ltx_emph ltx_font_italic">et. al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> which uses Multimodal Compact Bilinear pooling (MCB) to merge the visual features and the information from the question. They hypothesize that the typical merge actions (addition, element-wise product, concat…) do not express correctly all the information. Using MCB to merge those features they achieve an accuracy of 66,2% on the Real Open-ended test-dev dataset. A MCB is also used to create two different attention maps that are concatenated before feeding the main MCB.</p>
</div>
</section>
</section>
<section id="Ch3" class="ltx_chapter ltx_indent_first">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 3 </span>Methodology</h2>

<div id="Ch3.p1" class="ltx_para">
<p id="Ch3.p1.1" class="ltx_p">This chapter presents the methodology used to develop this project and the process followed to achieve our final results.
The baseline for the results that has been taken into account is the one provided by the CVPR16 VQA Challenge.</p>
</div>
<section id="Ch3.S1" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.1 </span>A programmer’s word</h3>

<div id="Ch3.S1.p1" class="ltx_para">
<p id="Ch3.S1.p1.1" class="ltx_p">During the first stages of this thesis, when we were looking for some baseline code to perform VQA tasks which we could start with, we found out that the open-sourced projects for VQA were not reusable at all. As a matter of fact, it seems that the vast majority of research code out there has not been developed with programming best practices or with reusability in mind.</p>
</div>
<div id="Ch3.S1.p2" class="ltx_para">
<p id="Ch3.S1.p2.1" class="ltx_p">That is why we decided to develop our code having in mind some important things: modularity, abstraction, reusability. We intended to apply as many good practices as possible given that we had a restriction in time as we wanted to present our results in the VQA Challenge. As it always happen with software projects, the time variable was crucial in terms of <em id="Ch3.S1.p2.1.1" class="ltx_emph ltx_font_italic">how much</em> modular, abstract or reusable was our code at the end.</p>
</div>
<div id="Ch3.S1.p3" class="ltx_para">
<p id="Ch3.S1.p3.1" class="ltx_p">Nevertheless, we think that the final work is going to be very useful as a starting point for future projects related with VQA and also as a good end-to-end Keras’ example. With that we mean that sometimes there is a lack of examples with some degree of complexity that cover the whole process of building a model, training, validating and testing.</p>
</div>
<section id="Ch3.S1.SS1" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1.1 </span>Choosing the best language</h4>

<div id="Ch3.S1.SS1.p1" class="ltx_para">
<p id="Ch3.S1.SS1.p1.1" class="ltx_p">In terms of actually coding the project we decided to use Python as a programming language. We considered this was the best language to approach this project in terms of prototyping speed and tools available. 
<br class="ltx_break">C++, Lua and Python were the finalists of this search. C++ was discarded as sometimes is a little bit cumbersome to prototype things fast, for the syntax itself and for the fact that it is a compiled language. Lua and Python have a quite similar syntax, both being a high-level and scripting programming language with a fast learning curve and fast for prototyping. At the beginning, all the open-sourced projects that we found that had something to do with VQA where written in Lua using a deep learning framework called Torch<span id="Ch3.footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="http://torch.ch/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://torch.ch/</a></span></span></span>. This seemed a good reason to choose Lua over Python but then, having a look into where the community was going to, we found out that frameworks like Theano or TensorFlow were having great success and the developers and research community was moving towards them. Both frameworks are for Python, which made us choose Python as a programming language for the project. Then, we were recommended to use Keras, a library able to work upon Theano or TensorFlow to expedite the prototyping process.</p>
</div>
</section>
<section id="Ch3.S1.SS2" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1.2 </span>A Pythonic project</h4>

<div id="Ch3.S1.SS2.p1" class="ltx_para">
<p id="Ch3.S1.SS2.p1.1" class="ltx_p">Having chosen Python as the programming language, one of the things we wanted to do to increase the readability and reusability of this project was to follow a code style guide.</p>
</div>
<div id="Ch3.S1.SS2.p2" class="ltx_para">
<p id="Ch3.S1.SS2.p2.1" class="ltx_p">In the programming world there are many languages and for everyone of them there are tons and tons of styles that the developers tend to program with, and we programmers are picky. That is why the most popular languages usually have a code style guide that define <em id="Ch3.S1.SS2.p2.1.1" class="ltx_emph ltx_font_italic">how the code should look</em> and <em id="Ch3.S1.SS2.p2.1.2" class="ltx_emph ltx_font_italic">what is a good practice</em> in that language. Using these code style guidelines increases the readability of the code and helps you to develop a better code that will be more easily extended or used.</p>
</div>
<div id="Ch3.S1.SS2.p3" class="ltx_para">
<p id="Ch3.S1.SS2.p3.1" class="ltx_p">For Python, this code style guideline is called PEP8<span id="Ch3.footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://www.python.org/dev/peps/pep-0008/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.python.org/dev/peps/pep-0008/</a></span></span></span><span id="Ch3.footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>There are different PEP guidelines, each one of them dictating best practices for different tasks</span></span></span>. The code presented with this project follows the PEP8 guideline.</p>
</div>
</section>
<section id="Ch3.S1.SS3" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1.3 </span>An eight-leg cat called octocat</h4>

<div id="Ch3.S1.SS3.p1" class="ltx_para">
<p id="Ch3.S1.SS3.p1.1" class="ltx_p">To develop the project as professional as possible and to keep track of the changes we made we have used Git<span id="Ch3.footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://git-scm.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://git-scm.com/</a></span></span></span> as a Version Control System (VCS). Git give us the ability to work in parallel when needed and to prototype things without the fear of not being able to restore our previous work or having to do all those annoying manual backups. Using Git we have created an historic of our project development process.</p>
</div>
<div id="Ch3.S1.SS3.p2" class="ltx_para">
<p id="Ch3.S1.SS3.p2.1" class="ltx_p">To store our Git repository we have used GitHub as it allows us to open-source the project once finished and to enable community contributions. After the VQA challenge deadline we published our GitHub repository<span id="Ch3.footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/imatge-upc/vqa-2016-cvprw" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/imatge-upc/vqa-2016-cvprw</a></span></span></span> as public so everyone can use the code.</p>
</div>
</section>
</section>
<section id="Ch3.S2" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.2 </span>Dataset</h3>

<div id="Ch3.S2.p1" class="ltx_para">
<p id="Ch3.S2.p1.1" class="ltx_p">In order to train a model (in supervised learning) we need a very large amount of data. This data are example of input-output pairs. In our case, the input are both the image and the question and the output is the answer.</p>
</div>
<div id="Ch3.S2.p2" class="ltx_para">
<p id="Ch3.S2.p2.1" class="ltx_p">To train our models we have used the real image VQA dataset<span id="Ch3.footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="http://www.visualqa.org/download.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.visualqa.org/download.html</a></span></span></span>, which is one of the largest visual question-answering datasets. This dataset is provided by the organizers of the VQA Challenge and is splitted in the typical three subsets: train, validation and test. The train subset is composed by 82.783 images, 248.349 questions and 2.483.490 answers; the validation by 40.504 images, 121.512 questions and 1.215.120 answers; and finally the test set is composed of 81.434 images and 244.302 questions. The whole explanation on how the organizers created this dataset can be found in their paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="Ch3.S2.p3" class="ltx_para">
<p id="Ch3.S2.p3.1" class="ltx_p">All the images are part of the Microsoft Common Objects in Context (MS COCO) image dataset<span id="Ch3.footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="http://mscoco.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://mscoco.org/</a></span></span></span> and the questions and answers have been generated by different workers. MS COCO was chosen as the image are very different (size, color and black&amp;white, quality…) and they are rich in contextual information. The questions are of different types, being the most common the ’what…?’, ’is…?’, ’how…?’ and ’are…?’.</p>
</div>
<div id="Ch3.S2.p4" class="ltx_para">
<p id="Ch3.S2.p4.1" class="ltx_p">As it is usual, the train subset has been used to learn the model parameters, at the same time that the validation set was used to check on the model’s generalization to unseen data. By using this information, we could improve some parameters and present the ones that achieved higher accuracy on the validation set. The test set does not have answers as it defines the problem to be solved during the challenge. The answers predicted for the test set were the ones that we submitted to the VQA 2016 challenge.</p>
</div>
</section>
<section id="Ch3.S3" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.3 </span>Text-based QA toy example</h3>

<div id="Ch3.S3.p1" class="ltx_para">
<p id="Ch3.S3.p1.1" class="ltx_p">As we already stated in our work plan <a href="#Ch1.S4" title="1.4 Work Plan ‣ Chapter 1 Introduction ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1.4</span></a>, we started familiarizing with VQA tasks and how Keras library works through a text-based QA model.</p>
</div>
<div id="Ch3.S3.p2" class="ltx_para">
<p id="Ch3.S3.p2.1" class="ltx_p">The kind of text-based QA problem that we addressed was a toy example were a short story and a question related to that story is given to the model so it can predict a single word answer.</p>
</div>
<section id="Ch3.S3.SS1" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3.1 </span>Tokenization</h4>

<div id="Ch3.S3.SS1.p1" class="ltx_para">
<p id="Ch3.S3.SS1.p1.1" class="ltx_p">The first step to take is transforming the words (from the story and the question) into numbers that can be feed into the model. We did this preprocessing with a tokenizer provided by Keras that is in charge of tokenizing the text sequences. By tokenizing here we mean splitting the whole string into words, remove the unnecessary ones (punctuation for example) and transform each word into a number. This number is the index of that word in a dictionary that we created previously. The dictionary or vocabulary of our tokenizer can be a predefined one or not. We did not use a predefined dictionary but created our own one using the training data. 
<br class="ltx_break">To create such a dictionary, its size is important, the number of unique words that it can include. A special word will also be included, which is the one representing ’unknown’ words, <em id="Ch3.S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">i.e.</em> words that are not in the previous list. 
<br class="ltx_break">From this point on, a word is no longer a string such as ’garden’ but a number representing its position in the dictionary.</p>
</div>
</section>
<section id="Ch3.S3.SS2" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3.2 </span>Model architecture</h4>

<div id="Ch3.S3.SS2.p1" class="ltx_para">
<p id="Ch3.S3.SS2.p1.1" class="ltx_p">Once we have tokenized the story and the question, their representation is a list of numbers. These numbers are the input of our model. The architecture of this model is represented in Figure <a href="#Ch3.F1" title="Figure 3.1 ‣ 3.3.2 Model architecture ‣ 3.3 Text-based QA toy example ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. The model has two branches, one for the story and the other one for the question, that are merged together to produce the output.</p>
</div>
<figure id="Ch3.F1" class="ltx_figure"><img src="/html/1610.02692/assets/figures/text-based_qa.png" id="Ch3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="219" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Ch3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3.1</span>: </span><span id="Ch3.F1.3.2" class="ltx_text" style="font-size:90%;">Schema of the model used for text-based QA</span></figcaption>
</figure>
<div id="Ch3.S3.SS2.p2" class="ltx_para">
<p id="Ch3.S3.SS2.p2.1" class="ltx_p">This model has an encoder structure (see Cho <em id="Ch3.S3.SS2.p2.1.1" class="ltx_emph ltx_font_italic">et. al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> for a complete encoder-decoder architecture for automatic machine translation). We take the input, which is high dimensional (the size of the vocabulary, a typical value is 20.000) and we encode it into a much smaller representation, a vector in a continuous space of a dimension that we have to specify beforehand (it is fixed and it is an hyper-parameter that we have to adjust manually), for example 100. This single vector holding the whole information of the question and the story is our encoding. This encoded vector is given to the fully-connected layer, a softmax, that will predict the one-hot representation of the answer. A one-hot representation is simply a vector with all zeros and just a one in a specific location. In our case, this vector has dimension equal to the vocabulary size and the one is placed in the location equal to the word’s index in the dictionary. The softmax will not predict 0 and 1 but a distribution between 0-1. We took the maximum value as our output.</p>
</div>
<div id="Ch3.S3.SS2.p3" class="ltx_para">
<p id="Ch3.S3.SS2.p3.1" class="ltx_p">Lets dig a little bit deeper into the encoder. The story branch has only a word embedding block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. The word embedding learns how to do a dense representation of the input word as a vector in a continuous space of the specified dimension. This reduces the dimensionality of our input as it is projected into a space with less dimensions. Such space has a very interesting property which is one of the reasons that we use these embeddings. The vectors in that space are not only dense but they are also a semantic representation of the word. One possible example of this is that the embeddings of words with similar meaning are close to each other (the distance between the vectors is small).
After the word embedding we will have a sequence of vectors representing the story, a vector for each word.</p>
</div>
<div id="Ch3.S3.SS2.p4" class="ltx_para">
<p id="Ch3.S3.SS2.p4.1" class="ltx_p">The question branch is a little bit more complex. We start with the same word embedding than the story. The output of that block is then given to a LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, which is a Recurrent Neural Network (RNN). RNNs have the advantage of having memory over <em id="Ch3.S3.SS2.p4.1.1" class="ltx_emph ltx_font_italic">time</em>, <em id="Ch3.S3.SS2.p4.1.2" class="ltx_emph ltx_font_italic">i.e.</em> they have a state that is kept in memory and it is updated in each iteration and their output is somehow dependent of this state. LSTMs are widely used to process sequences for these reasons. Specifically, we are using a non-stateful LSTM that means that this state is not preserved from batch to batch, it is resetted. We have also configured the LSTM so it only outputs its last state. We set the maximum sequence length to the LSTM so it knows when the question has finished and it can output a value.</p>
</div>
<div id="Ch3.S3.SS2.p5" class="ltx_para">
<p id="Ch3.S3.SS2.p5.1" class="ltx_p">The output of the LSTM is a representation in a single vector of the whole question. This vector is then given to a block that repeats the vector as many times as specified, in our case, the maximum story length (in tokens). This combination of the word embedding and a LSTM that sees all the question words and then outputs its memory state is known as a sentence embedding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
<div id="Ch3.S3.SS2.p6" class="ltx_para">
<p id="Ch3.S3.SS2.p6.1" class="ltx_p">This way, at the merge stage there is a sequence of word embeddings from the story branch, and a sequence of the question embedding repeated. Each iteration in the sequence is what we call a <em id="Ch3.S3.SS2.p6.1.1" class="ltx_emph ltx_font_italic">timestep</em>. That being said, at each timestep we are summing up the embedding of a story word and the embedding of the whole question. To be able to do so, both vectors need to have the same dimension and that forces that both word embeddings (story and question) must have the same dimension as hidden units in the LSTM that encodes the question (which determines the output dimension of it).</p>
</div>
<div id="Ch3.S3.SS2.p7" class="ltx_para">
<p id="Ch3.S3.SS2.p7.1" class="ltx_p">The result of adding both embeddings is given to another LSTM which is in charge of the last encoding. This LSTM is also non-stateful and it will accumulate all the merged features until it has seen the whole story and then it will output its state. This last vector is our encoding of the whole story and question merged and it is what we use to predict the answer, as we have explained before.</p>
</div>
<div id="Ch3.S3.SS2.p8" class="ltx_para">
<p id="Ch3.S3.SS2.p8.1" class="ltx_p">As an addition, this model also uses drop outs with a 0,3 value to prevent overfitting.</p>
</div>
</section>
<section id="Ch3.S3.SS3" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3.3 </span>Model up and running</h4>

<div id="Ch3.S3.SS3.p1" class="ltx_para">
<p id="Ch3.S3.SS3.p1.1" class="ltx_p">Once we had this model built, we trained it with 10.000 sample for the QA1 task defined by Weston <em id="Ch3.S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">et. al</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and we did some small tests. We did not run extensive experimentation at this stage as this was not our objective and because the parameters of the model and the dataset we used were designed more as a toy example than a real-world solution. This stage allowed us to train the model as fast as possible and check that the whole process was working.</p>
</div>
</section>
</section>
<section id="Ch3.S4" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.4 </span>Extending the text-based QA model for visual QA</h3>

<div id="Ch3.S4.p1" class="ltx_para">
<p id="Ch3.S4.p1.1" class="ltx_p">Taking as a starting point the previous text-based QA model, we modified it so it could be used for visual QA. Notice that the architecture shown in Figure <a href="#Ch3.F1" title="Figure 3.1 ‣ 3.3.2 Model architecture ‣ 3.3 Text-based QA toy example ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> has been built around the idea that we have a story that gives some information to the model and then we ask a question about that story. The model uses the information retrieved from the story to be able to answer the question. In visual QA our story is the image, is what give us the information needed to answer the question.</p>
</div>
<section id="Ch3.S4.SS1" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4.1 </span>Image processing</h4>

<div id="Ch3.S4.SS1.p1" class="ltx_para">
<p id="Ch3.S4.SS1.p1.1" class="ltx_p">With the idea of our image being the ”story” from which we have to extract information, we changed the story branch for an image branch. In such a branch we use the VGG-16 convolutional neural network proposed by Simonyan <em id="Ch3.S4.SS1.p1.1.1" class="ltx_emph ltx_font_italic">et. al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, an off-the-shelf model, to extract the visual features, as you can see in Figure <a href="#Ch3.F2" title="Figure 3.2 ‣ 3.4.2 Modifications on the question branch ‣ 3.4 Extending the text-based QA model for visual QA ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. We did not use the output of the whole model but we truncated it until the last convolutional layer, before the fully-connected fc-4096. Using the output of the conv layers instead of the fully-connected ones is a common practice to extract visual features maps.</p>
</div>
<div id="Ch3.S4.SS1.p2" class="ltx_para">
<p id="Ch3.S4.SS1.p2.1" class="ltx_p">In order to be able to combine this visual information with the one obtained from the question, we need to turn this 2D map into a vector. We used a Flatten layer to do so and then we give this vector to the repeat block. Notice that now we are repeating the image (our visual story) instead of the question. We are doing this as the question is our only sequence in this model and this way the model will <em id="Ch3.S4.SS1.p2.1.1" class="ltx_emph ltx_font_italic">see</em> the whole image for each question word.</p>
</div>
</section>
<section id="Ch3.S4.SS2" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4.2 </span>Modifications on the question branch</h4>

<div id="Ch3.S4.SS2.p1" class="ltx_para">
<p id="Ch3.S4.SS2.p1.1" class="ltx_p">As shown in Figure <a href="#Ch3.F2" title="Figure 3.2 ‣ 3.4.2 Modifications on the question branch ‣ 3.4 Extending the text-based QA model for visual QA ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, the question branch has only a word embedding now. This means that in each timestep a question word will be feed into the model and, because the visual features are repeated, each one will be merged with the information of the whole image.
The dimension of the word embedding and the visual features is different so our merge process now is not a summation but a concatenation of both vectors.</p>
</div>
<figure id="Ch3.F2" class="ltx_figure"><img src="/html/1610.02692/assets/figures/visual_qa_1.png" id="Ch3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Ch3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3.2</span>: </span><span id="Ch3.F2.3.2" class="ltx_text" style="font-size:90%;">First visual QA model’s schema</span></figcaption>
</figure>
</section>
<section id="Ch3.S4.SS3" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4.3 </span>Model parameters</h4>

<div id="Ch3.S4.SS3.p1" class="ltx_para">
<p id="Ch3.S4.SS3.p1.1" class="ltx_p">The important parameters of this model are: vocabulary size, LSTM hidden units, embedding size, question maximum length, learning rate and batch size. It is also important which optimizer to use.</p>
</div>
<div id="Ch3.S4.SS3.p2" class="ltx_para">
<p id="Ch3.S4.SS3.p2.1" class="ltx_p">We set the batch size to be the maximum that we could save in the GPU RAM, having a value of just 32 samples. We need to consider that we also need to fit the compiled model (its weights) in the GPU RAM and this is very expensive as some of our layers, and thus its weights, are huge as we will see now.</p>
</div>
<div id="Ch3.S4.SS3.p3" class="ltx_para">
<p id="Ch3.S4.SS3.p3.1" class="ltx_p">The learning rate for the network’s parameter was governed by the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> which modifies the learning rate through the training. We only need to specify the starting learning rate, and we chose the default one proposed by Kingma <em id="Ch3.S4.SS3.p3.1.1" class="ltx_emph ltx_font_italic">et. al.</em> in the original paper which is 0,001.</p>
</div>
<div id="Ch3.S4.SS3.p4" class="ltx_para">
<p id="Ch3.S4.SS3.p4.1" class="ltx_p">For the question maximum length we have taken the length of the largest question in the training subset. This parameter is used in the last LSTM so it knows when it has seen the whole question and can output its state. We found that for the training set, the maximum question length is 22. The questions that have a smaller length have been left-padded with 0, so the input is ’inactive’ and then it is activated with the question tokens. The network has been configured to ignore these padding zeros.</p>
</div>
<div id="Ch3.S4.SS3.p5" class="ltx_para">
<p id="Ch3.S4.SS3.p5.1" class="ltx_p">The vocabulary size is crucial for the softmax layer as this will set the number of neurons of this layer. A value of 20.000 was chosen as it is quite common and respects the tradeoff between number of words (which give more flexibility to the model) and number of weights to train (time consuming, training problems, memory constraints).</p>
</div>
<div id="Ch3.S4.SS3.p6" class="ltx_para">
<p id="Ch3.S4.SS3.p6.1" class="ltx_p">For this model we chose the number of LSTM hidden units and the embedding size to be the same, with a value of 100. We used this value for simplicity and due to some experience of a team member regarding these parameters.</p>
</div>
<div id="Ch3.S4.SS3.p7" class="ltx_para">
<p id="Ch3.S4.SS3.p7.1" class="ltx_p">We have also changed the dropout rate from 0,3 to 0,5.</p>
</div>
</section>
<section id="Ch3.S4.SS4" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4.4 </span>Implementation and training</h4>

<div id="Ch3.S4.SS4.p1" class="ltx_para">
<p id="Ch3.S4.SS4.p1.1" class="ltx_p">We built the whole model including the truncated VGG-16 and we used pretrained weights for this module, that we froze at training time. This weights were the result of training the whole VGG-16 on ImageNet<span id="Ch3.footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="http://www.image-net.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.image-net.org/</a></span></span></span> one of the biggest image datasets existing nowadays. As we freeze the VGG-16 weights we did not make a fine-tunning of it, we only trained our own layers.</p>
</div>
<div id="Ch3.S4.SS4.p2" class="ltx_para">
<p id="Ch3.S4.SS4.p2.1" class="ltx_p">To train this model we started making use of the computational service of the Image Processing Group (GPI) at the Universitat Politecnica de Catalunya. Some memory constraints were faced as the size of the vocabulary imposed the need to create huge vectors representing the answers and the size of the compiled model reflected this too.
As we could only fit 32 samples per batch, the training process was at a rate of 17-24 hours per epoch using NVidia Titan X GPUs, equipped with 12 GB of RAM. This forced an evolution to the next model as having this model train for a reasonable number of epochs (40) was not a valid option. Notice than an epoch is defined as a single pass of all the examples in the training set through the model under training.</p>
</div>
<div id="Ch3.S4.SS4.p3" class="ltx_para">
<p id="Ch3.S4.SS4.p3.1" class="ltx_p">In terms of software, we created an Image, Question, Answer, VQASample and VQADataset Python classes to hold the information of these entities and to allow single responsability and modularity of the code. Using these classes we encapsulate the logic in modules that we can easily control, instead of working with plain data such as Python dictionaries, lists, numpy arrays, etc.</p>
</div>
</section>
</section>
<section id="Ch3.S5" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.5 </span>Model improvement: towards the final model</h3>

<div id="Ch3.S5.p1" class="ltx_para">
<p id="Ch3.S5.p1.1" class="ltx_p">The prohibitive duration of the training process made us opt for precomputing the visual features of the image. This approach made sense as we were not modifying the values of the VGG-16 convolutional network that was in charge of extracting these features.</p>
</div>
<div id="Ch3.S5.p2" class="ltx_para">
<p id="Ch3.S5.p2.1" class="ltx_p">Instead of precomputing ourselves the visual features using an isolated VGG-16, our partners from the Computer Vision group at the Universitat de Barcelona (UB) provided us with these features extracted with a new kind of CNN called Kernelized CNN (Liu <em id="Ch3.S5.p2.1.1" class="ltx_emph ltx_font_italic">et. al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>). You can find a short description in the Methods and procedures section <a href="#Ch1.S3" title="1.3 Methods and procedures ‣ Chapter 1 Introduction ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1.3</span></a>. The dimension of the output vector from the KCNN module is 1024. The rest of the parameters and functionality remains the same as in the architecture described in Section <a href="#Ch3.S4" title="3.4 Extending the text-based QA model for visual QA ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>.</p>
</div>
<figure id="Ch3.F3" class="ltx_figure"><img src="/html/1610.02692/assets/figures/visual_qa_2.png" id="Ch3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="193" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Ch3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3.3</span>: </span><span id="Ch3.F3.3.2" class="ltx_text" style="font-size:90%;">Replacement of the VGG-16 by the KCNN model</span></figcaption>
</figure>
<section id="Ch3.S5.SS1" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5.1 </span>Implementation and training</h4>

<div id="Ch3.S5.SS1.p1" class="ltx_para">
<p id="Ch3.S5.SS1.p1.1" class="ltx_p">We also changed the way we programmed the model. In the previous case, we were using an abstract model implementation of Keras called Sequential<span id="Ch3.footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a target="_blank" href="http://keras.io/getting-started/sequential-model-guide/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://keras.io/getting-started/sequential-model-guide/</a></span></span></span> which is basically a stack of layers. This model also allows the possibility of merging two sequential models into one, that is what we used to create the two input branches. For this modified model we changed to the more flexible Functional API<span id="Ch3.footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a target="_blank" href="http://keras.io/getting-started/functional-api-guide/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://keras.io/getting-started/functional-api-guide/</a></span></span></span> which is thought to build more powerful models in a graph approximation. This new interface let us work with the tensors themselves so it is now easier to modify and make the model more complex.</p>
</div>
<div id="Ch3.S5.SS1.p2" class="ltx_para">
<p id="Ch3.S5.SS1.p2.1" class="ltx_p">By using the precomputed visual features and this new implementation, we reduced the training time of an epoch to less than an hour (40 minutes approximately).</p>
</div>
</section>
<section id="Ch3.S5.SS2" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5.2 </span>Batch normalization and reducing the learning rate</h4>

<div id="Ch3.S5.SS2.p1" class="ltx_para">
<p id="Ch3.S5.SS2.p1.1" class="ltx_p">One of the first modifications we tried was adding a batch normalization layer (Figure <a href="#Ch3.F4" title="Figure 3.4 ‣ 3.5.2 Batch normalization and reducing the learning rate ‣ 3.5 Model improvement: towards the final model ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>) after the merging process, as this helps the training process and usually increases the accuracy. Ioffe and Szegedy propose to introduce the normalization of the layers’ input distribution inside the model architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. They introduce this normalization using their novel layer (batch normalization) which reduces the internal covariate shift.</p>
</div>
<div id="Ch3.S5.SS2.p2" class="ltx_para">
<p id="Ch3.S5.SS2.p2.1" class="ltx_p">We also reduced the initial learning rate sequentially from 0,001 to 0,0003 and to 0,0001 and we found that the last one was giving the best accuracy as we will explain later in the results chapter <a href="#Ch4" title="Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="Ch3.F4" class="ltx_figure"><img src="/html/1610.02692/assets/figures/visual_qa_3.png" id="Ch3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="183" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Ch3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3.4</span>: </span><span id="Ch3.F4.3.2" class="ltx_text" style="font-size:90%;">Batch normalization added to help the training process</span></figcaption>
</figure>
</section>
</section>
<section id="Ch3.S6" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3.6 </span>The final model: sentence embedding</h3>

<div id="Ch3.S6.p1" class="ltx_para">
<p id="Ch3.S6.p1.1" class="ltx_p">Our last model was the one that predicted the answers with higher accuracy and presented to the VQA challenge. Several changes were introduced with respect to the preliminary prototypes so lets have a look into the different blocks, depicted in Figure <a href="#Ch3.F5" title="Figure 3.5 ‣ 3.6.4 Other modifications ‣ 3.6 The final model: sentence embedding ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a>.</p>
</div>
<section id="Ch3.S6.SS1" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6.1 </span>Question embedding</h4>

<div id="Ch3.S6.SS1.p1" class="ltx_para">
<p id="Ch3.S6.SS1.p1.1" class="ltx_p">The question branch was modified by adding a LSTM at the end of the word embedding, thus creating a sentence embedding, in our case the question embedding. The resulting vector of the sentence embedding module is a dense and semantic representation of the whole question as it was in our text-based QA model <a href="#Ch3.F1" title="Figure 3.1 ‣ 3.3.2 Model architecture ‣ 3.3 Text-based QA toy example ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. The difference here is that we did not choose the same value for the word embedding dimension and the number of LSTM hidden units. We set 100 as the word embedding dimension and 256 as the number of LSTM hidden units, which is a common value. We increased the number of hidden units as this can help increasing the accuracy in the condensed representation of the questions but we did not change the embedding dimension as this could decrease the density of the word embedding representation.</p>
</div>
</section>
<section id="Ch3.S6.SS2" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6.2 </span>Image semantic projection</h4>

<div id="Ch3.S6.SS2.p1" class="ltx_para">
<p id="Ch3.S6.SS2.p1.1" class="ltx_p">We decided to add a fully-connected layer after the KCNN module to be able to project the visual features into a space of the same dimension as the question embedding. The fully-connected layer can be seen as a matrix operation which projects the features’ 1024-vector into a 256-vector in the semantic space. We have chose ReLU as the activation function for this layer.</p>
</div>
</section>
<section id="Ch3.S6.SS3" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6.3 </span>Merging and predicting</h4>

<div id="Ch3.S6.SS3.p1" class="ltx_para">
<p id="Ch3.S6.SS3.p1.1" class="ltx_p">As both textual and visual features were projected into a 256-dimensional space, we can sum up them together and merge these features.
Now that both the question and image are represented by a single vector and not by a sequence of vectors, there is no need to add a LSTM after the merge and we can feed the resulting merged vector to the softmax so it can predict the answer.</p>
</div>
</section>
<section id="Ch3.S6.SS4" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6.4 </span>Other modifications</h4>

<div id="Ch3.S6.SS4.p1" class="ltx_para">
<p id="Ch3.S6.SS4.p1.1" class="ltx_p">The learning rate of this model was initialized to 0,0001 against the 0,001 of the first KCNN model.
We also tried to reduce the learning rate to 0,00001 and to add a batch normalization stage after the merging process but as we will see in the following chapter, neither of those increased the accuracy of the original final model. 
<br class="ltx_break">Before submitting to the VQA challenge over the test set, we also tried to train the model with the whole training subset and the 70% of the validation subset but this did not help either.</p>
</div>
<figure id="Ch3.F5" class="ltx_figure"><img src="/html/1610.02692/assets/figures/visual_qa_4.png" id="Ch3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="342" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Ch3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3.5</span>: </span><span id="Ch3.F5.3.2" class="ltx_text" style="font-size:90%;">Final model. A sentence embedding is used for the question and the visual features are projected into the same semantic space than the question</span></figcaption>
</figure>
</section>
</section>
</section>
<section id="Ch4" class="ltx_chapter ltx_indent_first">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 4 </span>Results</h2>

<div id="Ch4.p1" class="ltx_para">
<p id="Ch4.p1.1" class="ltx_p">In this chapter, the results of the different models exposed in the Methodology chapter <a href="#Ch3" title="Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> will be presented.</p>
</div>
<section id="Ch4.S1" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.1 </span>Evaluation metric</h3>

<div id="Ch4.S1.p1" class="ltx_para">
<p id="Ch4.S1.p1.1" class="ltx_p">The models have been evaluated using the metric introduced by the VQA challenge organizers. As they state in their evaluation page<span id="Ch4.footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="http://www.visualqa.org/evaluation.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.visualqa.org/evaluation.html</a></span></span></span>, this new metric is robust to inter-human variability in phrasing the answers.</p>
</div>
<div id="Ch4.S1.p2" class="ltx_para">
<p id="Ch4.S1.p2.1" class="ltx_p">The new accuracy formula per answer is the following:</p>
<table id="Ch4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Ch4.E1.m1.3" class="ltx_Math" alttext="Acc(ans)=min\left(\frac{\mbox{\#humans that said ans}}{3},1\right)" display="block"><semantics id="Ch4.E1.m1.3a"><mrow id="Ch4.E1.m1.3.3" xref="Ch4.E1.m1.3.3.cmml"><mrow id="Ch4.E1.m1.3.3.1" xref="Ch4.E1.m1.3.3.1.cmml"><mi id="Ch4.E1.m1.3.3.1.3" xref="Ch4.E1.m1.3.3.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="Ch4.E1.m1.3.3.1.2" xref="Ch4.E1.m1.3.3.1.2.cmml">​</mo><mi id="Ch4.E1.m1.3.3.1.4" xref="Ch4.E1.m1.3.3.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="Ch4.E1.m1.3.3.1.2a" xref="Ch4.E1.m1.3.3.1.2.cmml">​</mo><mi id="Ch4.E1.m1.3.3.1.5" xref="Ch4.E1.m1.3.3.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="Ch4.E1.m1.3.3.1.2b" xref="Ch4.E1.m1.3.3.1.2.cmml">​</mo><mrow id="Ch4.E1.m1.3.3.1.1.1" xref="Ch4.E1.m1.3.3.1.1.1.1.cmml"><mo stretchy="false" id="Ch4.E1.m1.3.3.1.1.1.2" xref="Ch4.E1.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="Ch4.E1.m1.3.3.1.1.1.1" xref="Ch4.E1.m1.3.3.1.1.1.1.cmml"><mi id="Ch4.E1.m1.3.3.1.1.1.1.2" xref="Ch4.E1.m1.3.3.1.1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="Ch4.E1.m1.3.3.1.1.1.1.1" xref="Ch4.E1.m1.3.3.1.1.1.1.1.cmml">​</mo><mi id="Ch4.E1.m1.3.3.1.1.1.1.3" xref="Ch4.E1.m1.3.3.1.1.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="Ch4.E1.m1.3.3.1.1.1.1.1a" xref="Ch4.E1.m1.3.3.1.1.1.1.1.cmml">​</mo><mi id="Ch4.E1.m1.3.3.1.1.1.1.4" xref="Ch4.E1.m1.3.3.1.1.1.1.4.cmml">s</mi></mrow><mo stretchy="false" id="Ch4.E1.m1.3.3.1.1.1.3" xref="Ch4.E1.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="Ch4.E1.m1.3.3.2" xref="Ch4.E1.m1.3.3.2.cmml">=</mo><mrow id="Ch4.E1.m1.3.3.3" xref="Ch4.E1.m1.3.3.3.cmml"><mi id="Ch4.E1.m1.3.3.3.2" xref="Ch4.E1.m1.3.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="Ch4.E1.m1.3.3.3.1" xref="Ch4.E1.m1.3.3.3.1.cmml">​</mo><mi id="Ch4.E1.m1.3.3.3.3" xref="Ch4.E1.m1.3.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="Ch4.E1.m1.3.3.3.1a" xref="Ch4.E1.m1.3.3.3.1.cmml">​</mo><mi id="Ch4.E1.m1.3.3.3.4" xref="Ch4.E1.m1.3.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="Ch4.E1.m1.3.3.3.1b" xref="Ch4.E1.m1.3.3.3.1.cmml">​</mo><mrow id="Ch4.E1.m1.3.3.3.5.2" xref="Ch4.E1.m1.3.3.3.5.1.cmml"><mo id="Ch4.E1.m1.3.3.3.5.2.1" xref="Ch4.E1.m1.3.3.3.5.1.cmml">(</mo><mfrac id="Ch4.E1.m1.1.1" xref="Ch4.E1.m1.1.1.cmml"><mtext id="Ch4.E1.m1.1.1.2" xref="Ch4.E1.m1.1.1.2a.cmml">#humans that said ans</mtext><mn id="Ch4.E1.m1.1.1.3" xref="Ch4.E1.m1.1.1.3.cmml">3</mn></mfrac><mo id="Ch4.E1.m1.3.3.3.5.2.2" xref="Ch4.E1.m1.3.3.3.5.1.cmml">,</mo><mn id="Ch4.E1.m1.2.2" xref="Ch4.E1.m1.2.2.cmml">1</mn><mo id="Ch4.E1.m1.3.3.3.5.2.3" xref="Ch4.E1.m1.3.3.3.5.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Ch4.E1.m1.3b"><apply id="Ch4.E1.m1.3.3.cmml" xref="Ch4.E1.m1.3.3"><eq id="Ch4.E1.m1.3.3.2.cmml" xref="Ch4.E1.m1.3.3.2"></eq><apply id="Ch4.E1.m1.3.3.1.cmml" xref="Ch4.E1.m1.3.3.1"><times id="Ch4.E1.m1.3.3.1.2.cmml" xref="Ch4.E1.m1.3.3.1.2"></times><ci id="Ch4.E1.m1.3.3.1.3.cmml" xref="Ch4.E1.m1.3.3.1.3">𝐴</ci><ci id="Ch4.E1.m1.3.3.1.4.cmml" xref="Ch4.E1.m1.3.3.1.4">𝑐</ci><ci id="Ch4.E1.m1.3.3.1.5.cmml" xref="Ch4.E1.m1.3.3.1.5">𝑐</ci><apply id="Ch4.E1.m1.3.3.1.1.1.1.cmml" xref="Ch4.E1.m1.3.3.1.1.1"><times id="Ch4.E1.m1.3.3.1.1.1.1.1.cmml" xref="Ch4.E1.m1.3.3.1.1.1.1.1"></times><ci id="Ch4.E1.m1.3.3.1.1.1.1.2.cmml" xref="Ch4.E1.m1.3.3.1.1.1.1.2">𝑎</ci><ci id="Ch4.E1.m1.3.3.1.1.1.1.3.cmml" xref="Ch4.E1.m1.3.3.1.1.1.1.3">𝑛</ci><ci id="Ch4.E1.m1.3.3.1.1.1.1.4.cmml" xref="Ch4.E1.m1.3.3.1.1.1.1.4">𝑠</ci></apply></apply><apply id="Ch4.E1.m1.3.3.3.cmml" xref="Ch4.E1.m1.3.3.3"><times id="Ch4.E1.m1.3.3.3.1.cmml" xref="Ch4.E1.m1.3.3.3.1"></times><ci id="Ch4.E1.m1.3.3.3.2.cmml" xref="Ch4.E1.m1.3.3.3.2">𝑚</ci><ci id="Ch4.E1.m1.3.3.3.3.cmml" xref="Ch4.E1.m1.3.3.3.3">𝑖</ci><ci id="Ch4.E1.m1.3.3.3.4.cmml" xref="Ch4.E1.m1.3.3.3.4">𝑛</ci><interval closure="open" id="Ch4.E1.m1.3.3.3.5.1.cmml" xref="Ch4.E1.m1.3.3.3.5.2"><apply id="Ch4.E1.m1.1.1.cmml" xref="Ch4.E1.m1.1.1"><divide id="Ch4.E1.m1.1.1.1.cmml" xref="Ch4.E1.m1.1.1"></divide><ci id="Ch4.E1.m1.1.1.2a.cmml" xref="Ch4.E1.m1.1.1.2"><mtext id="Ch4.E1.m1.1.1.2.cmml" xref="Ch4.E1.m1.1.1.2">#humans that said ans</mtext></ci><cn type="integer" id="Ch4.E1.m1.1.1.3.cmml" xref="Ch4.E1.m1.1.1.3">3</cn></apply><cn type="integer" id="Ch4.E1.m1.2.2.cmml" xref="Ch4.E1.m1.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch4.E1.m1.3c">Acc(ans)=min\left(\frac{\mbox{\#humans that said ans}}{3},1\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4.1)</span></td>
</tr></tbody>
</table>
</div>
<div id="Ch4.S1.p3" class="ltx_para">
<p id="Ch4.S1.p3.1" class="ltx_p">The accuracy over the whole dataset is an average of the accuracy per answer for all the samples.</p>
</div>
<div id="Ch4.S1.p4" class="ltx_para">
<p id="Ch4.S1.p4.1" class="ltx_p">The interpretation of equation <a href="#Ch4.E1" title="In 4.1 Evaluation metric ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> is as follows: an answer is given as correct (accuracy equals 1) if the same exact answer was given by at least three human annotators. Zero matches equals zero accuracy and from there each match gives 0,33 points to the accuracy with a maximum of 1.</p>
</div>
</section>
<section id="Ch4.S2" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.2 </span>Dataset</h3>

<div id="Ch4.S2.p1" class="ltx_para">
<p id="Ch4.S2.p1.1" class="ltx_p">At this point is worth summarizing the dataset characteristics mentioned in <a href="#Ch3.S2" title="3.2 Dataset ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>:</p>
<ul id="Ch4.S2.I1" class="ltx_itemize">
<li id="Ch4.S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch4.S2.I1.i1.p1" class="ltx_para">
<p id="Ch4.S2.I1.i1.p1.1" class="ltx_p"><span id="Ch4.S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Training dataset</span>: 82.783 images, 248.349 questions, 2.483.490 answers</p>
</div>
</li>
<li id="Ch4.S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch4.S2.I1.i2.p1" class="ltx_para">
<p id="Ch4.S2.I1.i2.p1.1" class="ltx_p"><span id="Ch4.S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Validation dataset</span>: 40.504 images, 121.512 questions, 1.215.120 answers</p>
</div>
</li>
<li id="Ch4.S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch4.S2.I1.i3.p1" class="ltx_para">
<p id="Ch4.S2.I1.i3.p1.1" class="ltx_p"><span id="Ch4.S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Test dataset</span>: 81.434 images, 244.302 questions</p>
</div>
</li>
</ul>
</div>
<div id="Ch4.S2.p2" class="ltx_para">
<p id="Ch4.S2.p2.1" class="ltx_p">Notice that for each image there are three questions and for each question there are ten answers. These ten answers were provided by human annotators and the most frequent ones were selected. Most of the answers are the same but rephrased.</p>
</div>
<div id="Ch4.S2.p3" class="ltx_para">
<p id="Ch4.S2.p3.1" class="ltx_p">The organizers also provide a Python script to evaluate the results<span id="Ch4.footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/VT-vision-lab/VQA/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/VT-vision-lab/VQA/</a></span></span></span> the same way that they do when you submit the test results. This script preprocess the answers to avoid format-like mismatches. As an example, they make all characters lowercase, remove articles, convert number words to digits…</p>
</div>
<div id="Ch4.S2.p4" class="ltx_para">
<p id="Ch4.S2.p4.1" class="ltx_p">This script needs a specific JSON file with the ground truth answers and another one with the machine generated answers (what the model has predicted) in a predefined format.
We used this script to perform an evaluation of our model over the validation set (because we do have the answers for this subset).</p>
</div>
</section>
<section id="Ch4.S3" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.3 </span>Models architectures and setups</h3>

<div id="Ch4.S3.p1" class="ltx_para">
<p id="Ch4.S3.p1.1" class="ltx_p">In the following section we will refer to the models by a number in order to be more clear and concise. These identifiers are defined here with a description of the model/configuration:</p>
</div>
<figure id="Ch4.T1" class="ltx_table">
<table id="Ch4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Ch4.T1.2.1.1" class="ltx_tr">
<th id="Ch4.T1.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T1.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T1.2.1.1.1.1.1" class="ltx_p" style="width:42.7pt;"><span id="Ch4.T1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Identifier</span></span>
</span>
</th>
<th id="Ch4.T1.2.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="Ch4.T1.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T1.2.1.1.2.1.1" class="ltx_p" style="width:313.0pt;"><span id="Ch4.T1.2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Description</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Ch4.T1.2.2.1" class="ltx_tr">
<td id="Ch4.T1.2.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T1.2.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T1.2.2.1.1.1.1" class="ltx_p" style="width:42.7pt;">0</span>
</span>
</td>
<td id="Ch4.T1.2.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Ch4.T1.2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T1.2.2.1.2.1.1" class="ltx_p" style="width:313.0pt;">First VQA model. Uses VGG-16 to extract visual features. Described in <a href="#Ch3.S4" title="3.4 Extending the text-based QA model for visual QA ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a> and schema in Figure <a href="#Ch3.F2" title="Figure 3.2 ‣ 3.4.2 Modifications on the question branch ‣ 3.4 Extending the text-based QA model for visual QA ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a></span>
</span>
</td>
</tr>
<tr id="Ch4.T1.2.3.2" class="ltx_tr">
<td id="Ch4.T1.2.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T1.2.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T1.2.3.2.1.1.1" class="ltx_p" style="width:42.7pt;">1</span>
</span>
</td>
<td id="Ch4.T1.2.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Ch4.T1.2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T1.2.3.2.2.1.1" class="ltx_p" style="width:313.0pt;">Improvement of model 0 using KCNN. Described in <a href="#Ch3.S5" title="3.5 Model improvement: towards the final model ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a> and schema in Figure <a href="#Ch3.F3" title="Figure 3.3 ‣ 3.5 Model improvement: towards the final model ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a></span>
</span>
</td>
</tr>
<tr id="Ch4.T1.2.4.3" class="ltx_tr">
<td id="Ch4.T1.2.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T1.2.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T1.2.4.3.1.1.1" class="ltx_p" style="width:42.7pt;">2</span>
</span>
</td>
<td id="Ch4.T1.2.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Ch4.T1.2.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T1.2.4.3.2.1.1" class="ltx_p" style="width:313.0pt;">Model 1 using with batch normalization. Schema in Figure <a href="#Ch3.F4" title="Figure 3.4 ‣ 3.5.2 Batch normalization and reducing the learning rate ‣ 3.5 Model improvement: towards the final model ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a></span>
</span>
</td>
</tr>
<tr id="Ch4.T1.2.5.4" class="ltx_tr">
<td id="Ch4.T1.2.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T1.2.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T1.2.5.4.1.1.1" class="ltx_p" style="width:42.7pt;">3</span>
</span>
</td>
<td id="Ch4.T1.2.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Ch4.T1.2.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T1.2.5.4.2.1.1" class="ltx_p" style="width:313.0pt;">Model 1 with a learning rate of 1/10 the original (0,0001)</span>
</span>
</td>
</tr>
<tr id="Ch4.T1.2.6.5" class="ltx_tr">
<td id="Ch4.T1.2.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T1.2.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T1.2.6.5.1.1.1" class="ltx_p" style="width:42.7pt;">4</span>
</span>
</td>
<td id="Ch4.T1.2.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="Ch4.T1.2.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T1.2.6.5.2.1.1" class="ltx_p" style="width:313.0pt;">Final model using sentence embedding for the question and a projection to the semantic space for the image. Described in section <a href="#Ch3.S6" title="3.6 The final model: sentence embedding ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.6</span></a> and schema in Figure <a href="#Ch3.F5" title="Figure 3.5 ‣ 3.6.4 Other modifications ‣ 3.6 The final model: sentence embedding ‣ Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a></span>
</span>
</td>
</tr>
<tr id="Ch4.T1.2.7.6" class="ltx_tr">
<td id="Ch4.T1.2.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T1.2.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T1.2.7.6.1.1.1" class="ltx_p" style="width:42.7pt;">5</span>
</span>
</td>
<td id="Ch4.T1.2.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="Ch4.T1.2.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T1.2.7.6.2.1.1" class="ltx_p" style="width:313.0pt;">Model 4 using batch normalization</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Ch4.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 4.1</span>: </span><span id="Ch4.T1.4.2" class="ltx_text" style="font-size:90%;">Models identifiers and descriptions</span></figcaption>
</figure>
<div id="Ch4.S3.p2" class="ltx_para">
<p id="Ch4.S3.p2.1" class="ltx_p">Results for model 0 will not be presented as we only completed the building stage but we did not finish the training process for the problems already explained in Chapter <a href="#Ch3" title="Chapter 3 Methodology ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We only include it here to state that this was our base VQA model.</p>
</div>
</section>
<section id="Ch4.S4" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.4 </span>Training and validation losses</h3>

<div id="Ch4.S4.p1" class="ltx_para">
<p id="Ch4.S4.p1.1" class="ltx_p">One of the earlier results that helped us to improve our models was the training, and most important, the validation loss. In the following figures you can see the evolution of the training and validation loss per epoch</p>
</div>
<figure id="Ch4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Ch4.F2.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:216.8pt;"><img src="/html/1610.02692/assets/figures/loss_curves_epochs_1.png" id="Ch4.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Ch4.F2.1.1.1.1" class="ltx_text" style="font-size:90%;">Figure 4.1</span>: </span><span id="Ch4.F2.1.2.2" class="ltx_text" style="font-size:90%;">Training losses (blue) and validation losses (green) for model 1</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Ch4.F2.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:216.8pt;"><img src="/html/1610.02692/assets/figures/loss_curves_epochs_2.png" id="Ch4.F2.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Ch4.F2.2.1.1.1" class="ltx_text" style="font-size:90%;">Figure 4.2</span>: </span><span id="Ch4.F2.2.2.2" class="ltx_text" style="font-size:90%;">Training losses (blue) and validation losses (green) for model 2</span></figcaption>
</figure>
</div>
</div>
</figure>
<div id="Ch4.S4.p2" class="ltx_para">
<p id="Ch4.S4.p2.1" class="ltx_p">For model 1 and 2 we have that the validation loss increases from epoch 3 until the end. We can also appreciate that after the second epoch the model is not learning anymore, the training loss gets stucked around a fixed value with some ”noise” and also that in the first epoch the model experiments a huge decrement in the training loss. Both factors are an indicator that the models are slowly diverging and thus the learning rate is too high.</p>
</div>
<figure id="Ch4.F3" class="ltx_figure"><img src="/html/1610.02692/assets/figures/loss_curves_epochs_3.png" id="Ch4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="359" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Ch4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4.3</span>: </span><span id="Ch4.F3.3.2" class="ltx_text" style="font-size:90%;">Training losses (blue) and validation losses (green) for model 3</span></figcaption>
</figure>
<div id="Ch4.S4.p3" class="ltx_para">
<p id="Ch4.S4.p3.1" class="ltx_p">In the model 3 we decrease the learning rate to 1/10 of the original one, having a value of 0,0001. As we can easily see in the plot in Figure <a href="#Ch4.F3" title="Figure 4.3 ‣ 4.4 Training and validation losses ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a> we experiment a slower decrease in the training loss and it does not stop learning after the first epochs. Even if the average training loss is higher than in the previous models, the validation loss (which is the one that helps us measure how good our model generalizes to unseen data), is lower in this model and does not increase over the iterations.</p>
</div>
<div id="Ch4.S4.p4" class="ltx_para">
<p id="Ch4.S4.p4.1" class="ltx_p">Changing to a sentence embedding and projecting the visual features to the same space than the question reduced the validation loss. Having a look at Figure <a href="#Ch4.F5" title="Figure 4.5 ‣ 4.4 Training and validation losses ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5</span></a> we can see that the validation loss slowly decreases epoch after epoch and it reaches the lowest value of the past models.</p>
</div>
<div id="Ch4.S4.p5" class="ltx_para">
<p id="Ch4.S4.p5.1" class="ltx_p">Adding a batch normalization layer did not help us in order to obtain better results with the model 5 (Figure <a href="#Ch4.F5" title="Figure 4.5 ‣ 4.4 Training and validation losses ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5</span></a>).</p>
</div>
<figure id="Ch4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Ch4.F5.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:216.8pt;"><img src="/html/1610.02692/assets/figures/loss_curves_epochs_5.png" id="Ch4.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Ch4.F5.1.1.1.1" class="ltx_text" style="font-size:90%;">Figure 4.4</span>: </span><span id="Ch4.F5.1.2.2" class="ltx_text" style="font-size:90%;">Training losses (blue) and validation losses (green) for model 4</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Ch4.F5.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:216.8pt;"><img src="/html/1610.02692/assets/figures/loss_curves_epochs_6.png" id="Ch4.F5.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Ch4.F5.2.1.1.1" class="ltx_text" style="font-size:90%;">Figure 4.5</span>: </span><span id="Ch4.F5.2.2.2" class="ltx_text" style="font-size:90%;">Training losses (blue) and validation losses (green) for model 5</span></figcaption>
</figure>
</div>
</div>
</figure>
</section>
<section id="Ch4.S5" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.5 </span>Quantitative results in the VQA Challenge 2016</h3>

<div id="Ch4.S5.p1" class="ltx_para">
<p id="Ch4.S5.p1.1" class="ltx_p">The model we presented to the CVPR16 VQA Challenge was the model number 4. We get an accuracy of <span id="Ch4.S5.p1.1.1" class="ltx_text ltx_font_bold">53,62%</span> over the test dataset. In table <a href="#Ch4.T2" title="Table 4.2 ‣ 4.5 Quantitative results in the VQA Challenge 2016 ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> we present a comparison between our accuracy and the accuracy of the baseline model and the top one.</p>
</div>
<div id="Ch4.S5.p2" class="ltx_para">
<p id="Ch4.S5.p2.1" class="ltx_p">As we did not submit all the results from the different models, we do not have test accuracies for some of them (model 2 and 3).</p>
</div>
<figure id="Ch4.T2" class="ltx_table">
<p id="Ch4.T2.2" class="ltx_p"><span id="Ch4.T2.2.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">

<span id="Ch4.T2.2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="Ch4.T2.2.1.1.1.1" class="ltx_tr">
<span id="Ch4.T2.2.1.1.1.1.1" class="ltx_td ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></span>
<span id="Ch4.T2.2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4"><span id="Ch4.T2.2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Validation set</span></span>
<span id="Ch4.T2.2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t ltx_colspan ltx_colspan_4"><span id="Ch4.T2.2.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Test set</span></span></span>
</span>
<span class="ltx_tbody">
<span id="Ch4.T2.2.1.1.2.1" class="ltx_tr">
<span id="Ch4.T2.2.1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T2.2.1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T2.2.1.1.2.1.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="Ch4.T2.2.1.1.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></span>
</span></span>
<span id="Ch4.T2.2.1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="Ch4.T2.2.1.1.2.1.2.1" class="ltx_text ltx_font_bold">Yes/No</span></span>
<span id="Ch4.T2.2.1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="Ch4.T2.2.1.1.2.1.3.1" class="ltx_text ltx_font_bold">Number</span></span>
<span id="Ch4.T2.2.1.1.2.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="Ch4.T2.2.1.1.2.1.4.1" class="ltx_text ltx_font_bold">Other</span></span>
<span id="Ch4.T2.2.1.1.2.1.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="Ch4.T2.2.1.1.2.1.5.1" class="ltx_text ltx_font_bold">Overall</span></span>
<span id="Ch4.T2.2.1.1.2.1.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="Ch4.T2.2.1.1.2.1.6.1" class="ltx_text ltx_font_bold">Yes/No</span></span>
<span id="Ch4.T2.2.1.1.2.1.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="Ch4.T2.2.1.1.2.1.7.1" class="ltx_text ltx_font_bold">Number</span></span>
<span id="Ch4.T2.2.1.1.2.1.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="Ch4.T2.2.1.1.2.1.8.1" class="ltx_text ltx_font_bold">Other</span></span>
<span id="Ch4.T2.2.1.1.2.1.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="Ch4.T2.2.1.1.2.1.9.1" class="ltx_text ltx_font_bold">Overall</span></span></span>
<span id="Ch4.T2.2.1.1.3.2" class="ltx_tr">
<span id="Ch4.T2.2.1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T2.2.1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T2.2.1.1.3.2.1.1.1" class="ltx_p" style="width:85.4pt;">Baseline All yes</span>
</span></span>
<span id="Ch4.T2.2.1.1.3.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.3.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.3.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.3.2.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.3.2.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">70,97</span>
<span id="Ch4.T2.2.1.1.3.2.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0,36</span>
<span id="Ch4.T2.2.1.1.3.2.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">1,21</span>
<span id="Ch4.T2.2.1.1.3.2.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">29,88</span></span>
<span id="Ch4.T2.2.1.1.4.3" class="ltx_tr">
<span id="Ch4.T2.2.1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T2.2.1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T2.2.1.1.4.3.1.1.1" class="ltx_p" style="width:85.4pt;">Baseline Prior per question type</span>
</span></span>
<span id="Ch4.T2.2.1.1.4.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.4.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.4.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.4.3.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.4.3.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">71,40</span>
<span id="Ch4.T2.2.1.1.4.3.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">34,90</span>
<span id="Ch4.T2.2.1.1.4.3.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">8,96</span>
<span id="Ch4.T2.2.1.1.4.3.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">37,47</span></span>
<span id="Ch4.T2.2.1.1.5.4" class="ltx_tr">
<span id="Ch4.T2.2.1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T2.2.1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T2.2.1.1.5.4.1.1.1" class="ltx_p" style="width:85.4pt;">Baseline Nearest neighbor</span>
</span></span>
<span id="Ch4.T2.2.1.1.5.4.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.5.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.5.4.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.5.4.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.5.4.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">71,89</span>
<span id="Ch4.T2.2.1.1.5.4.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">24,23</span>
<span id="Ch4.T2.2.1.1.5.4.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">22,10</span>
<span id="Ch4.T2.2.1.1.5.4.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">42,85</span></span>
<span id="Ch4.T2.2.1.1.6.5" class="ltx_tr">
<span id="Ch4.T2.2.1.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T2.2.1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T2.2.1.1.6.5.1.1.1" class="ltx_p" style="width:85.4pt;">Baseline LSTM&amp;CNN</span>
</span></span>
<span id="Ch4.T2.2.1.1.6.5.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.6.5.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.6.5.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.6.5.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.6.5.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">79,01</span>
<span id="Ch4.T2.2.1.1.6.5.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">35,55</span>
<span id="Ch4.T2.2.1.1.6.5.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">36,80</span>
<span id="Ch4.T2.2.1.1.6.5.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">54,06</span></span>
<span id="Ch4.T2.2.1.1.7.6" class="ltx_tr">
<span id="Ch4.T2.2.1.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T2.2.1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T2.2.1.1.7.6.1.1.1" class="ltx_p" style="width:85.4pt;">UC Berkeley &amp; Sony</span>
</span></span>
<span id="Ch4.T2.2.1.1.7.6.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.7.6.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.7.6.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.7.6.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.7.6.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">83,24</span>
<span id="Ch4.T2.2.1.1.7.6.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">39,47</span>
<span id="Ch4.T2.2.1.1.7.6.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">58,00</span>
<span id="Ch4.T2.2.1.1.7.6.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">66,47</span></span>
<span id="Ch4.T2.2.1.1.8.7" class="ltx_tr">
<span id="Ch4.T2.2.1.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T2.2.1.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T2.2.1.1.8.7.1.1.1" class="ltx_p" style="width:85.4pt;">Humans</span>
</span></span>
<span id="Ch4.T2.2.1.1.8.7.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.8.7.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.8.7.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.8.7.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.8.7.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">95,77</span>
<span id="Ch4.T2.2.1.1.8.7.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">83,39</span>
<span id="Ch4.T2.2.1.1.8.7.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">72,67</span>
<span id="Ch4.T2.2.1.1.8.7.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">83,30</span></span>
<span id="Ch4.T2.2.1.1.9.8" class="ltx_tr">
<span id="Ch4.T2.2.1.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">
<span id="Ch4.T2.2.1.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T2.2.1.1.9.8.1.1.1" class="ltx_p" style="width:85.4pt;">Model 1</span>
</span></span>
<span id="Ch4.T2.2.1.1.9.8.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">71,82</span>
<span id="Ch4.T2.2.1.1.9.8.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">23,79</span>
<span id="Ch4.T2.2.1.1.9.8.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">27,99</span>
<span id="Ch4.T2.2.1.1.9.8.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">43,87</span>
<span id="Ch4.T2.2.1.1.9.8.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">71,62</span>
<span id="Ch4.T2.2.1.1.9.8.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">28,76</span>
<span id="Ch4.T2.2.1.1.9.8.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">29,32</span>
<span id="Ch4.T2.2.1.1.9.8.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">46,70</span></span>
<span id="Ch4.T2.2.1.1.10.9" class="ltx_tr">
<span id="Ch4.T2.2.1.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T2.2.1.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T2.2.1.1.10.9.1.1.1" class="ltx_p" style="width:85.4pt;">Model 3</span>
</span></span>
<span id="Ch4.T2.2.1.1.10.9.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">75,02</span>
<span id="Ch4.T2.2.1.1.10.9.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">28,60</span>
<span id="Ch4.T2.2.1.1.10.9.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">29,30</span>
<span id="Ch4.T2.2.1.1.10.9.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">46,32</span>
<span id="Ch4.T2.2.1.1.10.9.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.10.9.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.10.9.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.10.9.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span></span>
<span id="Ch4.T2.2.1.1.11.10" class="ltx_tr">
<span id="Ch4.T2.2.1.1.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T2.2.1.1.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T2.2.1.1.11.10.1.1.1" class="ltx_p" style="width:85.4pt;">Model 2</span>
</span></span>
<span id="Ch4.T2.2.1.1.11.10.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">75,62</span>
<span id="Ch4.T2.2.1.1.11.10.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">31,81</span>
<span id="Ch4.T2.2.1.1.11.10.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">28,11</span>
<span id="Ch4.T2.2.1.1.11.10.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">46,36</span>
<span id="Ch4.T2.2.1.1.11.10.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.11.10.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.11.10.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span>
<span id="Ch4.T2.2.1.1.11.10.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">-</span></span>
<span id="Ch4.T2.2.1.1.12.11" class="ltx_tr">
<span id="Ch4.T2.2.1.1.12.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T2.2.1.1.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T2.2.1.1.12.11.1.1.1" class="ltx_p" style="width:85.4pt;">Model 5</span>
</span></span>
<span id="Ch4.T2.2.1.1.12.11.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">78,15</span>
<span id="Ch4.T2.2.1.1.12.11.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">32,79</span>
<span id="Ch4.T2.2.1.1.12.11.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">33,91</span>
<span id="Ch4.T2.2.1.1.12.11.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">50,32</span>
<span id="Ch4.T2.2.1.1.12.11.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">78,15</span>
<span id="Ch4.T2.2.1.1.12.11.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">36,20</span>
<span id="Ch4.T2.2.1.1.12.11.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">35,26</span>
<span id="Ch4.T2.2.1.1.12.11.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">53,03</span></span>
<span id="Ch4.T2.2.1.1.13.12" class="ltx_tr">
<span id="Ch4.T2.2.1.1.13.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="Ch4.T2.2.1.1.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch4.T2.2.1.1.13.12.1.1.1" class="ltx_p" style="width:85.4pt;">Model 4</span>
</span></span>
<span id="Ch4.T2.2.1.1.13.12.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="Ch4.T2.2.1.1.13.12.2.1" class="ltx_text ltx_font_bold">78,73</span></span>
<span id="Ch4.T2.2.1.1.13.12.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="Ch4.T2.2.1.1.13.12.3.1" class="ltx_text ltx_font_bold">32,82</span></span>
<span id="Ch4.T2.2.1.1.13.12.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="Ch4.T2.2.1.1.13.12.4.1" class="ltx_text ltx_font_bold">35,5</span></span>
<span id="Ch4.T2.2.1.1.13.12.5" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="Ch4.T2.2.1.1.13.12.5.1" class="ltx_text ltx_font_bold">51,34</span></span>
<span id="Ch4.T2.2.1.1.13.12.6" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="Ch4.T2.2.1.1.13.12.6.1" class="ltx_text ltx_font_bold">78,02</span></span>
<span id="Ch4.T2.2.1.1.13.12.7" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="Ch4.T2.2.1.1.13.12.7.1" class="ltx_text ltx_font_bold">35,68</span></span>
<span id="Ch4.T2.2.1.1.13.12.8" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="Ch4.T2.2.1.1.13.12.8.1" class="ltx_text ltx_font_bold">36,54</span></span>
<span id="Ch4.T2.2.1.1.13.12.9" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span id="Ch4.T2.2.1.1.13.12.9.1" class="ltx_text ltx_font_bold">53,62</span></span></span>
</span>
</span>
</span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="Ch4.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 4.2</span>: </span><span id="Ch4.T2.4.2" class="ltx_text" style="font-size:90%;">Results of the four baselines provided by the VQA challenge organizer, the state-of-the-art (UC Berkeley &amp; Sony) and our five models. Model 4 was the one submitted to the challenge leaderboard as the results of our team</span></figcaption>
</figure>
<section id="Ch4.S5.SS1" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5.1 </span>A general overview</h4>

<div id="Ch4.S5.SS1.p1" class="ltx_para">
<p id="Ch4.S5.SS1.p1.1" class="ltx_p">The first interpretation of these results is that the gap between the accuracy of the baseline model and the best one (UC Berkeley &amp; Sony) is quite small, only a 12,41%. What this means is that it is very hard to create models good at solving visual QA as the model needs to have a deep understanding of the scene and the question and also quite good reasoning abilities.</p>
</div>
<div id="Ch4.S5.SS1.p2" class="ltx_para">
<p id="Ch4.S5.SS1.p2.1" class="ltx_p">Another fact to notice is that there is a performance difference between humans and models performing such tasks, and that means that there is still space for further research in this area.
Related with this, it is worth mentioning that human accuracy using this metric is quite low, comparing with what one would expect it to be (close to 1). This may imply that the metric used to evaluate this tasks may not be the best one to use as it does not reflect correctly the performance in such tasks. This could also be a problem on how the dataset is built. If we check the human accuracy using the metric and dataset presented by Zhu <em id="Ch4.S5.SS1.p2.1.1" class="ltx_emph ltx_font_italic">et. al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> we can see that it is 96%, much more logical a priori.</p>
</div>
</section>
<section id="Ch4.S5.SS2" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5.2 </span>Our results</h4>

<div id="Ch4.S5.SS2.p1" class="ltx_para">
<p id="Ch4.S5.SS2.p1.1" class="ltx_p">Now, if we evaluate our results we can see in table <a href="#Ch4.T2" title="Table 4.2 ‣ 4.5 Quantitative results in the VQA Challenge 2016 ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> that our model performs slightly worse than the baseline provided by the VQA challenge organizers. This has a reason underneath it.</p>
</div>
<div id="Ch4.S5.SS2.p2" class="ltx_para">
<p id="Ch4.S5.SS2.p2.1" class="ltx_p">The first reason is that our model predicts only single word answers. This means that we will not have a 100% accuracy in multi word answers as we will never have a complete match. It is true that the VQA dataset’s answers are mostly single word but it is also true that we already start with fewer accuracy due to this fact. The VQA dataset answer average length is 1,1 word with a deviation of 0,4.</p>
</div>
<div id="Ch4.S5.SS2.p3" class="ltx_para">
<p id="Ch4.S5.SS2.p3.1" class="ltx_p">The second and most important reason is that the baseline model and many of other models presented in the challenge (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>), including the top ones, are a classifier built upon the N (in their case 1000) most frequent answers. This means that they were taking the 1000 answers from the training set that appear more frequently and assign a class label to each one of them. Then, they train their model to learn how to predict which one of these classes is the answer. At test time they predict a class label that it is matched with a predefined answer. Notice that their classes are the whole answer, not words. What this implies is that the model can only predict some of the answers that it has already seen in the training subset but it can not generate new ones, thus being very limited to this specific dataset.</p>
</div>
<div id="Ch4.S5.SS2.p4" class="ltx_para">
<p id="Ch4.S5.SS2.p4.1" class="ltx_p">In contrast, our model was built with the idea of being able to generate any word of the vocabulary as an answer, even if during training time that word was never used as an answer. We accomplished that by having the model output a word instead of a predefined answer. As our model has an encoder structure, we could also attach at the end a decoder stage to predict a multi word answer with a generative language model (future work <a href="#Ch6" title="Chapter 6 Conclusions ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<div id="Ch4.S5.SS2.p5" class="ltx_para">
<p id="Ch4.S5.SS2.p5.1" class="ltx_p">We decided to use this kind of model knowing that our accuracy was going to be lower as we thought that our model was more innovative and more capable of being applied in <em id="Ch4.S5.SS2.p5.1.1" class="ltx_emph ltx_font_italic">real life</em>, meaning that we did not improved our model towards the VQA challenge or VQA datasets but to Visual Question-Answering tasks in general and to our ambitious goal (which is out of the scope of this thesis but is our future objective) of generating question-answer pairs from images. To do so, we certainly need a model able to answer with unseen answers from the training subset, to generate them. We believe that our model outperforms other participants of the challenge in flexibility and in interest from the research point of view.</p>
</div>
</section>
<section id="Ch4.S5.SS3" class="ltx_subsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5.3 </span>Per answer type results</h4>

<div id="Ch4.S5.SS3.p1" class="ltx_para">
<p id="Ch4.S5.SS3.p1.1" class="ltx_p">The VQA dataset annotations (answers) are classified in three different types: yes/no, number or other. Each question has assigned one of these three answer types, that allows us to better understand how our model acts given different types of questions and how good is it answering them.</p>
</div>
<div id="Ch4.S5.SS3.p2" class="ltx_para">
<p id="Ch4.S5.SS3.p2.1" class="ltx_p">Analyzing the results per answer type shown in Table <a href="#Ch4.T2" title="Table 4.2 ‣ 4.5 Quantitative results in the VQA Challenge 2016 ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> we can see a huge difference when it comes to accuracy between the yes/no answers and the number or other answer types. The latest usually need a higher comprehension of the image and the question to be able to answer them due to the type of questions (why…?, what is…?) as opposed to the more common question type <em id="Ch4.S5.SS3.p2.1.1" class="ltx_emph ltx_font_italic">is this…?</em> for the yes/no answer type. These difference can be better understand with the qualitative results in the following section.</p>
</div>
</section>
</section>
<section id="Ch4.S6" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.6 </span>Qualitative results</h3>

<div id="Ch4.S6.p1" class="ltx_para">
<p id="Ch4.S6.p1.1" class="ltx_p">In this section we will present some examples of the results obtained for our best model. These results are from the validation subset, as we do not have the answers for the test subset. The following examples are grouped by accuracy, having three examples of each accuracy, one per question type (yes/no, number and other). The VQA evaluation script punctuate with 5 different accuracies (0, 30, 60, 90, 100) following equation<span id="Ch4.footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Even if the challenge provide the mentioned formula to compute the accuracy, it seems that the script is rounding the result of <math id="Ch4.footnote3.m1.1" class="ltx_Math" alttext="(\#humansthatsaidans)/3" display="inline"><semantics id="Ch4.footnote3.m1.1b"><mrow id="Ch4.footnote3.m1.1.1" xref="Ch4.footnote3.m1.1.1.cmml"><mrow id="Ch4.footnote3.m1.1.1.1.1" xref="Ch4.footnote3.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="Ch4.footnote3.m1.1.1.1.1.2" xref="Ch4.footnote3.m1.1.1.1.1.1.cmml">(</mo><mrow id="Ch4.footnote3.m1.1.1.1.1.1" xref="Ch4.footnote3.m1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="Ch4.footnote3.m1.1.1.1.1.1.2" xref="Ch4.footnote3.m1.1.1.1.1.1.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.3" xref="Ch4.footnote3.m1.1.1.1.1.1.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1b" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.4" xref="Ch4.footnote3.m1.1.1.1.1.1.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1c" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.5" xref="Ch4.footnote3.m1.1.1.1.1.1.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1d" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.6" xref="Ch4.footnote3.m1.1.1.1.1.1.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1e" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.7" xref="Ch4.footnote3.m1.1.1.1.1.1.7.cmml">n</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1f" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.8" xref="Ch4.footnote3.m1.1.1.1.1.1.8.cmml">s</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1g" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.9" xref="Ch4.footnote3.m1.1.1.1.1.1.9.cmml">t</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1h" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.10" xref="Ch4.footnote3.m1.1.1.1.1.1.10.cmml">h</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1i" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.11" xref="Ch4.footnote3.m1.1.1.1.1.1.11.cmml">a</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1j" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.12" xref="Ch4.footnote3.m1.1.1.1.1.1.12.cmml">t</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1k" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.13" xref="Ch4.footnote3.m1.1.1.1.1.1.13.cmml">s</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1l" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.14" xref="Ch4.footnote3.m1.1.1.1.1.1.14.cmml">a</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1m" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.15" xref="Ch4.footnote3.m1.1.1.1.1.1.15.cmml">i</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1n" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.16" xref="Ch4.footnote3.m1.1.1.1.1.1.16.cmml">d</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1o" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.17" xref="Ch4.footnote3.m1.1.1.1.1.1.17.cmml">a</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1p" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.18" xref="Ch4.footnote3.m1.1.1.1.1.1.18.cmml">n</mi><mo lspace="0em" rspace="0em" id="Ch4.footnote3.m1.1.1.1.1.1.1q" xref="Ch4.footnote3.m1.1.1.1.1.1.1.cmml">​</mo><mi id="Ch4.footnote3.m1.1.1.1.1.1.19" xref="Ch4.footnote3.m1.1.1.1.1.1.19.cmml">s</mi></mrow><mo stretchy="false" id="Ch4.footnote3.m1.1.1.1.1.3" xref="Ch4.footnote3.m1.1.1.1.1.1.cmml">)</mo></mrow><mo id="Ch4.footnote3.m1.1.1.2" xref="Ch4.footnote3.m1.1.1.2.cmml">/</mo><mn id="Ch4.footnote3.m1.1.1.3" xref="Ch4.footnote3.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="Ch4.footnote3.m1.1c"><apply id="Ch4.footnote3.m1.1.1.cmml" xref="Ch4.footnote3.m1.1.1"><divide id="Ch4.footnote3.m1.1.1.2.cmml" xref="Ch4.footnote3.m1.1.1.2"></divide><apply id="Ch4.footnote3.m1.1.1.1.1.1.cmml" xref="Ch4.footnote3.m1.1.1.1.1"><times id="Ch4.footnote3.m1.1.1.1.1.1.1.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.1"></times><ci id="Ch4.footnote3.m1.1.1.1.1.1.2.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.2">#</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.3.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.3">ℎ</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.4.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.4">𝑢</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.5.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.5">𝑚</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.6.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.6">𝑎</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.7.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.7">𝑛</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.8.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.8">𝑠</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.9.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.9">𝑡</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.10.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.10">ℎ</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.11.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.11">𝑎</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.12.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.12">𝑡</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.13.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.13">𝑠</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.14.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.14">𝑎</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.15.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.15">𝑖</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.16.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.16">𝑑</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.17.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.17">𝑎</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.18.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.18">𝑛</ci><ci id="Ch4.footnote3.m1.1.1.1.1.1.19.cmml" xref="Ch4.footnote3.m1.1.1.1.1.1.19">𝑠</ci></apply><cn type="integer" id="Ch4.footnote3.m1.1.1.3.cmml" xref="Ch4.footnote3.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Ch4.footnote3.m1.1d">(\#humansthatsaidans)/3</annotation></semantics></math> to the closest lower integer</span></span></span> <a href="#Ch4.E1" title="In 4.1 Evaluation metric ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.</p>
</div>
<div id="Ch4.S6.p2" class="ltx_para">
<p id="Ch4.S6.p2.1" class="ltx_p">These examples have been chosen randomly from the results in order to obtain a sample as representative as possible of the whole dataset.</p>
</div>
<figure id="Ch4.F6" class="ltx_figure">
<p id="Ch4.F6.1" class="ltx_p ltx_align_center"><span id="Ch4.F6.1.1" class="ltx_text">
<img src="/html/1610.02692/assets/figures/0_accuracy.png" id="Ch4.F6.1.1.g1" class="ltx_graphics ltx_img_landscape" width="718" height="292" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Ch4.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4.6</span>: </span><span id="Ch4.F6.4.2" class="ltx_text" style="font-size:90%;">Result examples with accuracy 0</span></figcaption>
</figure>
<figure id="Ch4.F7" class="ltx_figure">
<p id="Ch4.F7.1" class="ltx_p ltx_align_center"><span id="Ch4.F7.1.1" class="ltx_text">
<img src="/html/1610.02692/assets/figures/30_accuracy.png" id="Ch4.F7.1.1.g1" class="ltx_graphics ltx_img_landscape" width="718" height="328" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Ch4.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4.7</span>: </span><span id="Ch4.F7.4.2" class="ltx_text" style="font-size:90%;">Result example with accuracy 30</span></figcaption>
</figure>
<figure id="Ch4.F8" class="ltx_figure">
<p id="Ch4.F8.1" class="ltx_p ltx_align_center"><span id="Ch4.F8.1.1" class="ltx_text">
<img src="/html/1610.02692/assets/figures/60_accuracy.png" id="Ch4.F8.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="248" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Ch4.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4.8</span>: </span><span id="Ch4.F8.4.2" class="ltx_text" style="font-size:90%;">Result example with accuracy 60</span></figcaption>
</figure>
<figure id="Ch4.F9" class="ltx_figure">
<p id="Ch4.F9.1" class="ltx_p ltx_align_center"><span id="Ch4.F9.1.1" class="ltx_text">
<img src="/html/1610.02692/assets/figures/90_accuracy.png" id="Ch4.F9.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="244" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Ch4.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4.9</span>: </span><span id="Ch4.F9.4.2" class="ltx_text" style="font-size:90%;">Result example with accuracy 90</span></figcaption>
</figure>
<figure id="Ch4.F10" class="ltx_figure">
<p id="Ch4.F10.1" class="ltx_p ltx_align_center"><span id="Ch4.F10.1.1" class="ltx_text">
<img src="/html/1610.02692/assets/figures/100_accuracy.png" id="Ch4.F10.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="272" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Ch4.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4.10</span>: </span><span id="Ch4.F10.4.2" class="ltx_text" style="font-size:90%;">Result example with accuracy 100</span></figcaption>
</figure>
<div id="Ch4.S6.p3" class="ltx_para">
<p id="Ch4.S6.p3.1" class="ltx_p">From these examples we can see that the images in the VQA dataset (that are MS COCO) are rich in information and very variate throughout the dataset. The orientation, ratio, resolution and number of channels vary from example to example, as well as the kind of content appearing.</p>
</div>
<div id="Ch4.S6.p4" class="ltx_para">
<p id="Ch4.S6.p4.1" class="ltx_p">The questions are very different in terms of what task does the model need to do in order to answer the question. These questions show perfectly the deep understanding of both the image and the question and how they are related needed to answer them. Different tasks need to be performed in order to succeed, such as sentiment analysis (Figure <a href="#Ch4.F10" title="Figure 4.10 ‣ 4.6 Qualitative results ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.10</span></a>, first example), object recognition (Figure <a href="#Ch4.F8" title="Figure 4.8 ‣ 4.6 Qualitative results ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.8</span></a>, second example and Figure <a href="#Ch4.F7" title="Figure 4.7 ‣ 4.6 Qualitative results ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.7</span></a>, second example), Optical Character Recognition (OCR) (Figure <a href="#Ch4.F6" title="Figure 4.6 ‣ 4.6 Qualitative results ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.6</span></a>, second example), activity recognition (Figure <a href="#Ch4.F9" title="Figure 4.9 ‣ 4.6 Qualitative results ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.9</span></a>, first example) and so on.</p>
</div>
<div id="Ch4.S6.p5" class="ltx_para">
<p id="Ch4.S6.p5.1" class="ltx_p">As for the answers, we can appreciate why the metric provided by the challenge maybe it is not the best one to use in this task. The second example of Figure <a href="#Ch4.F7" title="Figure 4.7 ‣ 4.6 Qualitative results ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.7</span></a> and the last one in Figure <a href="#Ch4.F8" title="Figure 4.8 ‣ 4.6 Qualitative results ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.8</span></a> show that the predicted answer was indeed correct, but due to rephrasing and some details, only 30 and 60 of accuracy was given to them. The annotation errors also distorts the results, as in Figure <a href="#Ch4.F8" title="Figure 4.8 ‣ 4.6 Qualitative results ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.8</span></a> second example, where the correct answer is 3 and, even if we predicted 2, the script evaluated our answer with 60.</p>
</div>
</section>
<section id="Ch4.S7" class="ltx_section ltx_indent_first">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4.7 </span>Some words at the VQA Challenge</h3>

<div id="Ch4.S7.p1" class="ltx_para">
<p id="Ch4.S7.p1.1" class="ltx_p">At date of 26 of June, 2016, some of our team’s members assist to the Computer Vision and Pattern Recognition 2016 at Las Vegas, USA, to present our extended abstract ”Towards Automatic Generation of Question Answer Pairs from Images” in the Visual Question-Answering workshop. In this workshop several speakers explained their research in the VQA field, including the winner of the VQA challenge.</p>
</div>
<div id="Ch4.S7.p2" class="ltx_para">
<p id="Ch4.S7.p2.1" class="ltx_p">We want to highlight some remarkable comments from the last session. 
<br class="ltx_break">Margaret Mitchell, from Microsoft Research, mentioned the interest of generating questions and answers from image as an extension to VQA. Mostafazadeh <em id="Ch4.S7.p2.1.1" class="ltx_emph ltx_font_italic">et. al.</em> (including Mitchell) have recently published a paper where they propose a model to generate natural questions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> (we presented our extended abstract before this paper was published). 
<br class="ltx_break">Another interesting comment, coming from Trevor Darrell (UC Berkeley), was his concern about solving VQA with a closed set of answers. This is building a classifier upon the most common seen answers in the training set (which is what a lot of the participants did) as opposite of our model which generates new answers even if the model have not seen them before in training time.</p>
</div>
<div id="Ch4.S7.p3" class="ltx_para">
<p id="Ch4.S7.p3.1" class="ltx_p">These comments supports the route we have taken to accomplish this thesis.</p>
</div>
</section>
</section>
<section id="Ch5" class="ltx_chapter ltx_indent_first">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 5 </span>Budget</h2>

<div id="Ch5.p1" class="ltx_para">
<p id="Ch5.p1.1" class="ltx_p">This project is a research study and it has not been developed with a product or service in mind that could be sold in the marketplace. We have used the computational resources provided by the <em id="Ch5.p1.1.1" class="ltx_emph ltx_font_italic">Grup de Processat d’Imatge</em> of UPC, so there has not been any additional cost in terms of hardware.</p>
</div>
<div id="Ch5.p2" class="ltx_para">
<p id="Ch5.p2.1" class="ltx_p">The hardware resources needed for this project were a CPU and a GPU with at least 12GB of GPU RAM and over 50GB of regular RAM. To be able to estimate the cost of the hardware in this project we will use the Amazon Web Services (AWS) Elastic Compute Cloud (EC2) service as they offer cloud computing resources per hour and they are a common solution for this needs. The EC2 instance more similar to our specifications is the g2.8xlarge which provides 60GB of RAM and 4 GPUs with 4GB of RAM each one. The cost of this service is $2,808 per hour which is $67,40 per day. We spend 60 days approximately using the computing resources, thus giving an approximate cost of $4.043,52 for the hardware needs.</p>
</div>
<div id="Ch5.p3" class="ltx_para">
<p id="Ch5.p3.1" class="ltx_p">Regarding software, everything we have used is open-source and thus this does not add any cost.</p>
</div>
<div id="Ch5.p4" class="ltx_para">
<p id="Ch5.p4.1" class="ltx_p">Being said that, the only <em id="Ch5.p4.1.1" class="ltx_emph ltx_font_italic">real</em> cost we can deduce from this project could be the salary of the team involved in developing it.
Basically three members have formed this team: a senior engineer as the advisor, a junior engineer as the co-advisor and myself as a junior engineer.</p>
</div>
<div id="Ch5.p5" class="ltx_para">
<p id="Ch5.p5.1" class="ltx_p">As presented in the workplan’s Gantt <a href="#Ch1.F2" title="Figure 1.2 ‣ 1.4.2 Gantt Diagram ‣ 1.4 Work Plan ‣ Chapter 1 Introduction ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1.2</span></a> the total duration of the project has been 24 weeks but the first weeks of work were only personal research. The other difference in the number of weeks is due to the fact that the co-advisor joined the project after few weeks.</p>
</div>
<figure id="Ch5.T1" class="ltx_table">
<table id="Ch5.T1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Ch5.T1.2.1.1" class="ltx_tr">
<td id="Ch5.T1.2.1.1.1" class="ltx_td ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:100.0pt;padding-bottom:2.15277pt;"></td>
<td id="Ch5.T1.2.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:40.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.1.1.2.1.1" class="ltx_p"><span id="Ch5.T1.2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Weeks</span></span>
</span>
</td>
<td id="Ch5.T1.2.1.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.1.1.3.1.1" class="ltx_p"><span id="Ch5.T1.2.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Wage/hour</span></span>
</span>
</td>
<td id="Ch5.T1.2.1.1.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.1.1.4.1.1" class="ltx_p"><span id="Ch5.T1.2.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Dedication</span></span>
</span>
</td>
<td id="Ch5.T1.2.1.1.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.1.1.5.1.1" class="ltx_p"><span id="Ch5.T1.2.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Total</span></span>
</span>
</td>
</tr>
<tr id="Ch5.T1.2.2.2" class="ltx_tr">
<td id="Ch5.T1.2.2.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:100.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.2.2.1.1.1" class="ltx_p">Junior engineer</span>
</span>
</td>
<td id="Ch5.T1.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:40.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.2.2.2.1.1" class="ltx_p">24</span>
</span>
</td>
<td id="Ch5.T1.2.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.2.2.3.1.1" class="ltx_p">10,00 €/h</span>
</span>
</td>
<td id="Ch5.T1.2.2.2.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.2.2.4.1.1" class="ltx_p">25 h/week</span>
</span>
</td>
<td id="Ch5.T1.2.2.2.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.2.2.5.1.1" class="ltx_p">6,000 €</span>
</span>
</td>
</tr>
<tr id="Ch5.T1.2.3.3" class="ltx_tr">
<td id="Ch5.T1.2.3.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:100.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.3.3.1.1.1" class="ltx_p">Junior engineer</span>
</span>
</td>
<td id="Ch5.T1.2.3.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:40.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.3.3.2.1.1" class="ltx_p">16</span>
</span>
</td>
<td id="Ch5.T1.2.3.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.3.3.3.1.1" class="ltx_p">10,00 €/h</span>
</span>
</td>
<td id="Ch5.T1.2.3.3.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.3.3.4.1.1" class="ltx_p">4 h/week</span>
</span>
</td>
<td id="Ch5.T1.2.3.3.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.3.3.5.1.1" class="ltx_p">640 €</span>
</span>
</td>
</tr>
<tr id="Ch5.T1.2.4.4" class="ltx_tr">
<td id="Ch5.T1.2.4.4.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:100.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.4.4.1.1.1" class="ltx_p">Senior engineer</span>
</span>
</td>
<td id="Ch5.T1.2.4.4.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:40.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.4.4.2.1.1" class="ltx_p">20</span>
</span>
</td>
<td id="Ch5.T1.2.4.4.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.4.4.3.1.1" class="ltx_p">20,00 €/h</span>
</span>
</td>
<td id="Ch5.T1.2.4.4.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.4.4.4.1.1" class="ltx_p">4 h/week</span>
</span>
</td>
<td id="Ch5.T1.2.4.4.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.4.4.5.1.1" class="ltx_p">1.600 €</span>
</span>
</td>
</tr>
<tr id="Ch5.T1.2.5.5" class="ltx_tr">
<td id="Ch5.T1.2.5.5.1" class="ltx_td ltx_align_right ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-bottom:2.15277pt;" colspan="4"><span id="Ch5.T1.2.5.5.1.1" class="ltx_text ltx_font_bold">Total</span></td>
<td id="Ch5.T1.2.5.5.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:50.0pt;padding-bottom:2.15277pt;">
<span id="Ch5.T1.2.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="Ch5.T1.2.5.5.2.1.1" class="ltx_p">8.240 €</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Ch5.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 5.1</span>: </span><span id="Ch5.T1.4.2" class="ltx_text" style="font-size:90%;">Budget of the project</span></figcaption>
</figure>
</section>
<section id="Ch6" class="ltx_chapter ltx_indent_first">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 6 </span>Conclusions</h2>

<div id="Ch6.p1" class="ltx_para">
<p id="Ch6.p1.1" class="ltx_p">When we started this thesis we had three main goals in mind. The first one was to be able to build a model for VQA and present our results to the CVPR16 VQA Challenge. The second was to, through the process of building the VQA model, have a better understanding of the techniques used to process text in the deep learning framework. We also wanted to explore how to combine text and visual features together. Our last goal was to build the software around this project as modular, reusable and following best practices as possible.</p>
</div>
<div id="Ch6.p2" class="ltx_para">
<p id="Ch6.p2.1" class="ltx_p">Looking back to our results and all the work presented here, we believe that we have accomplished all three goals successfully. This has not been an easy journey and we are not saying that there is no space for improvements. As we have seen in table <a href="#Ch4.T2" title="Table 4.2 ‣ 4.5 Quantitative results in the VQA Challenge 2016 ‣ Chapter 4 Results ‣ Open-Ended Visual Question-Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> there is still a big gap to fill in terms of accuracy for VQA models.</p>
</div>
<div id="Ch6.p3" class="ltx_para">
<p id="Ch6.p3.1" class="ltx_p">At the beginning we started the project with little knowledge of how Recurrent Neural Networks (RNN) worked and how to apply them to process sequences of text. Building our first QA model only for text gave us the needed expertise to move forward to the more complex systems that we wanted to face, VQA. A remarkable point here was the incorporation of Santiago Pascual to the team, which helped us a lot in the process of gaining this understanding of the RNN and the NLP world.</p>
</div>
<div id="Ch6.p4" class="ltx_para">
<p id="Ch6.p4.1" class="ltx_p">Having this QA model as a starting point, we started developing new VQA models that could merge the text and visual information, but not without reaching dead ends such as the use of VGG-16 (which, even if it is possible to use it, we could not due to timing constraints). We tried different model’s configuration parameters and architectures and through this iterative process of modifying the model and checking its performance we gain this notion of how the model is affected by those parameters and also we noticed that the complexity of the task does not give much space in terms of adjusting the parameters. With that we mean that the models worked at some specific range of values which was not very large.</p>
</div>
<div id="Ch6.p5" class="ltx_para">
<p id="Ch6.p5.1" class="ltx_p">Finally we could train and publish a model with a similar accuracy of the baseline one defined by the VQA organizers but more prone to extending it and improving it.</p>
</div>
<div id="Ch6.p6" class="ltx_para">
<p id="Ch6.p6.1" class="ltx_p">We would like to highlight that during the course of this project we presented an extended abstract to the CVPR16 VQA workshop and it was accepted. This extended abstract with its own poster was presented in the VQA workshop at 26th June 2016. The extended abstract exposed one of our ideas of future work.</p>
</div>
<div id="Ch6.p7" class="ltx_para">
<p id="Ch6.p7.1" class="ltx_p">Having this in mind, as a future work we are planing to take this last model and attach a generative language model at the end so it can predict multiple word answers. We believe that making this improvement we will be able to outperform the baseline. 
<br class="ltx_break">Another improvement that we are thinking about is to change the sentence embedding of the question to a character embedding, which exploits even more information of the words such as the relation between prefix and sufixes. 
<br class="ltx_break">As we have already mention, we also want to actually implement the ideas in our extended abstract to create a model that is able to generate Question-Answer Pairs (QAP) from an image.</p>
</div>
</section>
<section id="Ch7" class="ltx_chapter ltx_indent_first">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter 7 </span>Appendices</h2>

<div id="Ch7.p1" class="ltx_para">
<p id="Ch7.p1.1" class="ltx_p">In this appendices you will find our accepted extended abstract presented in the CVPR16 VQA workshop and the poster we presented in Las Vegas, Nevada (USA) on June 26, 2016.</p>
</div>
<div id="Ch7.p2" class="ltx_para">
<p id="Ch7.p2.1" class="ltx_p">See pages - of <a href="appendices/automatic-question-answer-generation.pdf" title="" class="ltx_ref">appendices/automatic-question-answer-generation.pdf</a></p>
</div>
<div id="Ch7.p3" class="ltx_para">
<p id="Ch7.p3.1" class="ltx_p">See pages - of <a href="appendices/poster_cvpr_vqa_workshop.pdf" title="" class="ltx_ref">appendices/poster_cvpr_vqa_workshop.pdf</a></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">Bibliography</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</span>, pages 2425–2433, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">On the properties of neural machine translation: Encoder-decoder
approaches.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1409.1259</span>, 2014.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Learning phrase representations using rnn encoder-decoder for
statistical machine translation.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1406.1078</span>, 2014.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on</span>, pages 248–255. IEEE, 2009.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for visual question answering and
visual grounding.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.01847</span>, 2016.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Sepp Hochreiter and Jürgen Schmidhuber.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Neural computation</span>, 9(8):1735–1780, 1997.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Sergey Ioffe and Christian Szegedy.

</span>
<span class="ltx_bibblock">Batch normalization: Accelerating deep network training by reducing
internal covariate shift.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1502.03167</span>, 2015.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Jin-Hwa Kim, Sang-Woo Lee, Dong-Hyun Kwak, Min-Oh Heo, Jeonghee Kim, Jung-Woo
Ha, and Byoung-Tak Zhang.

</span>
<span class="ltx_bibblock">Multimodal residual learning for visual qa.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.01455</span>, 2016.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Diederik Kingma and Jimmy Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.6980</span>, 2014.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
1097–1105, 2012.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Zhen Liu.

</span>
<span class="ltx_bibblock">Kernelized deep convolutional neural network for describing complex
images.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1509.04581</span>, 2015.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question
answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.00061</span>, 2016.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Lin Ma, Zhengdong Lu, and Hang Li.

</span>
<span class="ltx_bibblock">Learning to answer questions from image using convolutional neural
network.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1506.00333</span>, 2015.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Arun Mallya and Svetlana Lazebnik.

</span>
<span class="ltx_bibblock">Learning models for actions and person-object interactions with
transfer to question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1604.04808</span>, 2016.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.

</span>
<span class="ltx_bibblock">Efficient estimation of word representations in vector space.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1301.3781</span>, 2013.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean.

</span>
<span class="ltx_bibblock">Distributed representations of words and phrases and their
compositionality.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
3111–3119, 2013.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell, Xiaodong He,
and Lucy Vanderwende.

</span>
<span class="ltx_bibblock">Generating natural questions about an image.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1603.06059</span>, 2016.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han.

</span>
<span class="ltx_bibblock">Image question answering using convolutional neural network with
dynamic parameter prediction.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.05756</span>, 2015.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen,
Xinying Song, and Rabab Ward.

</span>
<span class="ltx_bibblock">Deep sentence embedding using long short-term memory networks:
Analysis and application to information retrieval.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language
Processing</span>, 24(4):694–707, 2016.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Mengye Ren, Ryan Kiros, and Richard Zemel.

</span>
<span class="ltx_bibblock">Exploring models and data for image question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pages
2953–2961, 2015.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2015.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.

</span>
<span class="ltx_bibblock">Going deeper with convolutions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 1–9, 2015.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov.

</span>
<span class="ltx_bibblock">Towards ai-complete question answering: A set of prerequisite toy
tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1502.05698</span>, 2015.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Caiming Xiong, Stephen Merity, and Richard Socher.

</span>
<span class="ltx_bibblock">Dynamic memory networks for visual and textual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1603.01417</span>, 2016.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus.

</span>
<span class="ltx_bibblock">Simple baseline for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1512.02167</span>, 2015.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Visual7w: Grounded question answering in images.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.03416</span>, 2015.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1610.02691" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1610.02692" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1610.02692">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1610.02692" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1610.02693" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar  3 18:45:59 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
