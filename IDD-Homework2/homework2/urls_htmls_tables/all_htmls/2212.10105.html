<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2212.10105] On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics</title><meta property="og:description" content="This contribution demonstrates the feasibility of applying Generative Adversarial Networks (GANs) on images of EPAL pallet blocks for dataset enhancement in the context of re-identification.
For many industrial applica…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2212.10105">

<!--Generated on Fri Mar  1 10:25:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">On the Applicability of Synthetic Data for Re-Identification
in Warehousing Logistics
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Written by AAAI Press Staff<sup id="id1.1.id1" class="ltx_sup">1</sup>
<br class="ltx_break">AAAI Style Contributions by Pater Patel Schneider,
Sunil Issar,
<br class="ltx_break">J. Scott Penberthy,
George Ferguson,
Hans Guesgen,
Francisco Cruz<span id="id2.2.id2" class="ltx_ERROR undefined">\equalcontrib</span>,
Marc Pujol-Gonzalez<span id="id3.3.id3" class="ltx_ERROR undefined">\equalcontrib</span>
</span><span class="ltx_author_notes">With help from the AAAI Publications Committee.</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jérôme Rutinowski<sup id="id4.1.id1" class="ltx_sup">1</sup>,
Bhargav Vankayalapati<sup id="id5.2.id2" class="ltx_sup">1</sup>,
Nils Schwenzfeier<sup id="id6.3.id3" class="ltx_sup">2</sup>,
<br class="ltx_break">Maribel Acosta<sup id="id7.4.id4" class="ltx_sup">3</sup>,
Christopher Reining<sup id="id8.5.id5" class="ltx_sup">1</sup>
</span></span>
</div>

<h1 class="ltx_title ltx_title_document">On the Applicability of Synthetic Data for Re-Identification
in Warehousing Logistics</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Written by AAAI Press Staff<sup id="id1.1.id1" class="ltx_sup">1</sup>
<br class="ltx_break">AAAI Style Contributions by Pater Patel Schneider,
Sunil Issar,
<br class="ltx_break">J. Scott Penberthy,
George Ferguson,
Hans Guesgen,
Francisco Cruz<span id="id2.2.id2" class="ltx_ERROR undefined">\equalcontrib</span>,
Marc Pujol-Gonzalez<span id="id3.3.id3" class="ltx_ERROR undefined">\equalcontrib</span>
</span><span class="ltx_author_notes">With help from the AAAI Publications Committee.</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jérôme Rutinowski<sup id="id4.1.id1" class="ltx_sup">1</sup>,
Bhargav Vankayalapati<sup id="id5.2.id2" class="ltx_sup">1</sup>,
Nils Schwenzfeier<sup id="id6.3.id3" class="ltx_sup">2</sup>,
<br class="ltx_break">Maribel Acosta<sup id="id7.4.id4" class="ltx_sup">3</sup>,
Christopher Reining<sup id="id8.5.id5" class="ltx_sup">1</sup>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">This contribution demonstrates the feasibility of applying Generative Adversarial Networks (GANs) on images of EPAL pallet blocks for dataset enhancement in the context of re-identification.
For many industrial applications of re-identification methods, datasets of sufficient volume would otherwise be unattainable in non-laboratory settings.
Using a state-of-the-art GAN architecture, namely CycleGAN, images of pallet blocks rotated to their left-hand side were generated from images of visually centered pallet blocks, based on images of rotated pallet blocks that were recorded as part of a previously recorded and published dataset.
In this process, the unique chipwood pattern of the pallet block surface structure was retained, only changing the orientation of the pallet block itself.
By doing so, synthetic data for re-identification testing and training purposes was generated, in a manner that is distinct from ordinary data augmentation.
In total, 1,004 new images of pallet blocks were generated.
The quality of the generated images was gauged using a perspective classifier that was trained on the original images and then applied to the synthetic ones, comparing the accuracy between the two sets of images.
The classification accuracy was 98% for the original images and 92% for the synthetic images.
In addition, the generated images were also used in a re-identification task, in order to re-identify original images based on synthetic ones.
The accuracy in this scenario was up to 88% for synthetic images, compared to 96% for original images.
Through this evaluation, it is established, whether or not a generated pallet block image closely resembles its original counterpart.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Generative Adversarial Networks (GANs) have shown to be an efficient way to generate synthetic data (herein defined as data generated by GANs or similar generative methods, which typically are supposed to resemble original data in at least one metric), producing better results than other generative methods <cite class="ltx_cite ltx_citemacro_citep">(Cai et al. <a href="#bib.bib3" title="" class="ltx_ref">2021</a>; Goodfellow et al. <a href="#bib.bib5" title="" class="ltx_ref">2014</a>; Wang, She, and Ward <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>.
The generation of such data can be helpful for data-driven approaches and in situations in which the acquisition of sufficient original data is not feasible.
This, for example, is the case when trying to examine the scalability of a solution which retrieves its input from a high volume database, for which the gathering of enough original data would not be possible in an economically feasible and timely manner.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">An example of one such case is the development of a re-identification solution for warehousing entities, as presented in <cite class="ltx_cite ltx_citemacro_citep">(Rutinowski et al. <a href="#bib.bib14" title="" class="ltx_ref">2021b</a>)</cite>.
In this work, the authors focused on applying the concept of pedestrian re-identification to chipwood pallet blocks.
Re-identification is commonly defined as the identification of a previously recorded subject over a network of cameras <cite class="ltx_cite ltx_citemacro_citep">(Ye et al. <a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite> and assigning a unique descriptor to it instead of assigning it to a class holding multiple entities of the kind.
The motivation of this contribution lies in the absence of an inbuilt identification feature on Euro-pallets, which would be of great value for the automation of warehousing processes.
Thus far, the standardized Euro-pallet, whose specifications are defined in a European Norm <cite class="ltx_cite ltx_citemacro_citep">(Packaging <a href="#bib.bib10" title="" class="ltx_ref">2004</a>)</cite>, guaranteeing production standards and interoperability, rely on methods that use artificial features (e.g., barcodes, QR-codes or RFID).
However, equipping pallets with these artificial features implies further expenses, labor, and the risk of illegibility after an elongated period of use, due to material deterioration.
Even though it has been demonstrated that the re-identification of pallet blocks can be achieved in a laboratory environment <cite class="ltx_cite ltx_citemacro_citep">(Rutinowski et al. <a href="#bib.bib14" title="" class="ltx_ref">2021b</a>)</cite>, it is not evident that the system that has been developed would yield similar results (i.e., a similar re-identification accuracy) if the dataset were of much greater extent.
It is not evident either, whether real-time performance (i.e., registering a pallet and receiving a re-identification result in a matter of a few seconds) could still be achieved.
These two aspects however do matter, since hundreds of millions of Euro-pallets are in constant circulation in the industry <cite class="ltx_cite ltx_citemacro_citep">(Deviatkin and Horttanainen <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>.
Even so, the acquisition of millions of images of pallet blocks is not feasible in a timely manner, which is why the use of synthetic data is of such great benefit in this very case.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Therefore, this contribution presents a GAN based approach for the generation of synthetic pallet block images.
The result of this approach is that the centered input images of pallet blocks from the dataset <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">pallet-block-502</em> <cite class="ltx_cite ltx_citemacro_citep">(Rutinowski et al. <a href="#bib.bib13" title="" class="ltx_ref">2021a</a>)</cite> are rotated to their left-hand side, while maintaining the unique chipwood pattern of their surface structure.
By doing so, a new image of the same pallet block from another perspective is generated.
This dataset enhancement approach is distinct from ordinary data augmentation, in which an image would be flipped, rotated, etc.
Since to the best of our knowledge, the synthetic generation of pallet block images is a novel application of GANs, no standard practices for performance or result quantification have been encountered.
As to assess the quality of the synthetic images, a classifier is therefore trained on original centered and rotated images and applied to a hold-out set of original and synthetic images.
The results of applying this classifier on the two sets of images are subsequently compared, determining whether the synthetic images are classified accordingly.
In addition, the synthetic images are used in the re-identification pipeline presented in <cite class="ltx_cite ltx_citemacro_citep">(Rutinowski et al. <a href="#bib.bib14" title="" class="ltx_ref">2021b</a>)</cite>, as to determine whether the retention of the chipwood pattern during the rotation of the pallet blocks was successful, i.e., whether the same pallet block, only rotated instead of centered, would still get assigned the same ID and thus be recognized accordingly in the re-identification scenario.
Therefore, while the main contribution of this work is the generation of synthetic data, the classification and re-identification tasks are used as benchmarks to validate the applicability of said data.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The remainder of this work is structured as follows:
Section <a href="#S2" title="2 Related Work ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> discusses the relevant research related to the subject of GANs and their applications as well as the concept of deep learning based re-identification.
Section <a href="#S3" title="3 Methodological Approach ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> lays out the approach and methodology proposed in this work.
Section <a href="#S4" title="4 Experimental Evaluation ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the evaluation of the results that have been obtained.
Finally, Section <a href="#S5" title="5 Conclusion &amp; Future Work ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> discusses these results and provide some suggestions on the research that could further be conducted based on this contribution.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The relevant state of the art for this contribution comprises GANs for dataset enhancement and deep learning based re-identification.
The two will be discussed briefly in this Section.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Generative Adversarial Networks</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Generative Adversarial Networks can be understood as a two-player zero-sum game, in which (at least) two neural networks compete against each other in the form of a minimax optimization problem <cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a href="#bib.bib16" title="" class="ltx_ref">2017</a>)</cite>.
On the most rudimentary level, one of these networks, called the generator, tries to generate new, synthetic data from a set of original data or from noise that it is given as an input <cite class="ltx_cite ltx_citemacro_citep">(Wang, She, and Ward <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>.
The discriminator then acts as the generator’s adversary, often in the form of a binary classifier, trying to distinguish between original and synthetic data <cite class="ltx_cite ltx_citemacro_citep">(Wang, She, and Ward <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>.
Starting from this basic premise <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow et al. <a href="#bib.bib5" title="" class="ltx_ref">2014</a>)</cite>, different kinds of GAN architectures and approaches have since been developed.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.2" class="ltx_p">One such development is pix2pix GAN <cite class="ltx_cite ltx_citemacro_citep">(Isola et al. <a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>, which is a conditional GAN, meaning that a conditional variable (i.e., a label) is added to both the generator and the discriminator model during training <cite class="ltx_cite ltx_citemacro_citep">(Pan et al. <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>.
By adding this additional information during training, the data generation process is affected and can therefore effectively be influenced by altering the conditional variable <cite class="ltx_cite ltx_citemacro_citep">(Langr and Bok <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>.
Isola et al. argue that many image processing tasks revolve around a translation from input to output, which can be abstracted as mapping pixels to pixels <cite class="ltx_cite ltx_citemacro_citep">(Isola et al. <a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>.
The authors claim that this challenge can therefore be solved by conditional GANs that only need the respective training data but keep their same architecture and objective (i.e., loss function) for each task.
To prove this claim, Isola et al. used pix2pix GAN for various image generation and semantic segmentation tasks, such as generating photo-realistic images from sketches or converting daytime images to nighttime images.
Pix2pix GAN was later proceeded by pix2pixHD <cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a href="#bib.bib17" title="" class="ltx_ref">2018</a>)</cite>, which provides photorealistic image-to-image translation at resolutions of up to <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mn id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><cn type="integer" id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">2048</annotation></semantics></math> × <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="1024" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mn id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><cn type="integer" id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">1024</annotation></semantics></math> px.
Like its predecessor, pix2pixHD translates semantic label maps into images.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<p id="S2.F1.1" class="ltx_p ltx_align_center"><span id="S2.F1.1.1" class="ltx_text"><img src="/html/2212.10105/assets/x1.png" id="S2.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="112" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A schematic depiction of the concept of cycle consistency, as described in <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al. <a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite>.</figcaption>
</figure>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.8" class="ltx_p">Another GAN architecture that is of interest is CycleGAN <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al. <a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite>.
This GAN has a deep convolutional architecture that learns a mapping between one image domain and another, using unsupervised learning to train two generators and discriminators.
While the previously mentioned pix2pix GAN uses an explicit pairing between labeled inputs and outputs, such paired data can be hard to come by in real life applications.
In such cases, an unsupervised approach is advantageous, in the specific case of CycleGAN the use of cycle consistency.
Cycle consistency means that one generator <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="G_{1}" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><msub id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml"><mi id="S2.SS1.p3.1.m1.1.1.2" xref="S2.SS1.p3.1.m1.1.1.2.cmml">G</mi><mn id="S2.SS1.p3.1.m1.1.1.3" xref="S2.SS1.p3.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><apply id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.1.m1.1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p3.1.m1.1.1.2.cmml" xref="S2.SS1.p3.1.m1.1.1.2">𝐺</ci><cn type="integer" id="S2.SS1.p3.1.m1.1.1.3.cmml" xref="S2.SS1.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">G_{1}</annotation></semantics></math> translates input <math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="I_{1}" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><msub id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml"><mi id="S2.SS1.p3.2.m2.1.1.2" xref="S2.SS1.p3.2.m2.1.1.2.cmml">I</mi><mn id="S2.SS1.p3.2.m2.1.1.3" xref="S2.SS1.p3.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><apply id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.1.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p3.2.m2.1.1.2.cmml" xref="S2.SS1.p3.2.m2.1.1.2">𝐼</ci><cn type="integer" id="S2.SS1.p3.2.m2.1.1.3.cmml" xref="S2.SS1.p3.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">I_{1}</annotation></semantics></math> from the source domain to the target domain, where a discriminator <math id="S2.SS1.p3.3.m3.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S2.SS1.p3.3.m3.1a"><msub id="S2.SS1.p3.3.m3.1.1" xref="S2.SS1.p3.3.m3.1.1.cmml"><mi id="S2.SS1.p3.3.m3.1.1.2" xref="S2.SS1.p3.3.m3.1.1.2.cmml">D</mi><mn id="S2.SS1.p3.3.m3.1.1.3" xref="S2.SS1.p3.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.1b"><apply id="S2.SS1.p3.3.m3.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.3.m3.1.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p3.3.m3.1.1.2.cmml" xref="S2.SS1.p3.3.m3.1.1.2">𝐷</ci><cn type="integer" id="S2.SS1.p3.3.m3.1.1.3.cmml" xref="S2.SS1.p3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.1c">D_{1}</annotation></semantics></math> tests whether the translated input <math id="S2.SS1.p3.4.m4.1" class="ltx_Math" alttext="T_{1}" display="inline"><semantics id="S2.SS1.p3.4.m4.1a"><msub id="S2.SS1.p3.4.m4.1.1" xref="S2.SS1.p3.4.m4.1.1.cmml"><mi id="S2.SS1.p3.4.m4.1.1.2" xref="S2.SS1.p3.4.m4.1.1.2.cmml">T</mi><mn id="S2.SS1.p3.4.m4.1.1.3" xref="S2.SS1.p3.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.4.m4.1b"><apply id="S2.SS1.p3.4.m4.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.4.m4.1.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p3.4.m4.1.1.2.cmml" xref="S2.SS1.p3.4.m4.1.1.2">𝑇</ci><cn type="integer" id="S2.SS1.p3.4.m4.1.1.3.cmml" xref="S2.SS1.p3.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.4.m4.1c">T_{1}</annotation></semantics></math> is distinguishable from target domain samples <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al. <a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite>.
Simultaneously, another generator <math id="S2.SS1.p3.5.m5.1" class="ltx_Math" alttext="G_{2}" display="inline"><semantics id="S2.SS1.p3.5.m5.1a"><msub id="S2.SS1.p3.5.m5.1.1" xref="S2.SS1.p3.5.m5.1.1.cmml"><mi id="S2.SS1.p3.5.m5.1.1.2" xref="S2.SS1.p3.5.m5.1.1.2.cmml">G</mi><mn id="S2.SS1.p3.5.m5.1.1.3" xref="S2.SS1.p3.5.m5.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.5.m5.1b"><apply id="S2.SS1.p3.5.m5.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.5.m5.1.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p3.5.m5.1.1.2.cmml" xref="S2.SS1.p3.5.m5.1.1.2">𝐺</ci><cn type="integer" id="S2.SS1.p3.5.m5.1.1.3.cmml" xref="S2.SS1.p3.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.5.m5.1c">G_{2}</annotation></semantics></math> translates <math id="S2.SS1.p3.6.m6.1" class="ltx_Math" alttext="I_{2}" display="inline"><semantics id="S2.SS1.p3.6.m6.1a"><msub id="S2.SS1.p3.6.m6.1.1" xref="S2.SS1.p3.6.m6.1.1.cmml"><mi id="S2.SS1.p3.6.m6.1.1.2" xref="S2.SS1.p3.6.m6.1.1.2.cmml">I</mi><mn id="S2.SS1.p3.6.m6.1.1.3" xref="S2.SS1.p3.6.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.6.m6.1b"><apply id="S2.SS1.p3.6.m6.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.p3.6.m6.1.1.2.cmml" xref="S2.SS1.p3.6.m6.1.1.2">𝐼</ci><cn type="integer" id="S2.SS1.p3.6.m6.1.1.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.6.m6.1c">I_{2}</annotation></semantics></math> from the target domain to the source domain, where another discriminator <math id="S2.SS1.p3.7.m7.1" class="ltx_Math" alttext="D_{2}" display="inline"><semantics id="S2.SS1.p3.7.m7.1a"><msub id="S2.SS1.p3.7.m7.1.1" xref="S2.SS1.p3.7.m7.1.1.cmml"><mi id="S2.SS1.p3.7.m7.1.1.2" xref="S2.SS1.p3.7.m7.1.1.2.cmml">D</mi><mn id="S2.SS1.p3.7.m7.1.1.3" xref="S2.SS1.p3.7.m7.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.7.m7.1b"><apply id="S2.SS1.p3.7.m7.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.1.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1">subscript</csymbol><ci id="S2.SS1.p3.7.m7.1.1.2.cmml" xref="S2.SS1.p3.7.m7.1.1.2">𝐷</ci><cn type="integer" id="S2.SS1.p3.7.m7.1.1.3.cmml" xref="S2.SS1.p3.7.m7.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.7.m7.1c">D_{2}</annotation></semantics></math> again tests whether the translated input <math id="S2.SS1.p3.8.m8.1" class="ltx_Math" alttext="T_{2}" display="inline"><semantics id="S2.SS1.p3.8.m8.1a"><msub id="S2.SS1.p3.8.m8.1.1" xref="S2.SS1.p3.8.m8.1.1.cmml"><mi id="S2.SS1.p3.8.m8.1.1.2" xref="S2.SS1.p3.8.m8.1.1.2.cmml">T</mi><mn id="S2.SS1.p3.8.m8.1.1.3" xref="S2.SS1.p3.8.m8.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.8.m8.1b"><apply id="S2.SS1.p3.8.m8.1.1.cmml" xref="S2.SS1.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.8.m8.1.1.1.cmml" xref="S2.SS1.p3.8.m8.1.1">subscript</csymbol><ci id="S2.SS1.p3.8.m8.1.1.2.cmml" xref="S2.SS1.p3.8.m8.1.1.2">𝑇</ci><cn type="integer" id="S2.SS1.p3.8.m8.1.1.3.cmml" xref="S2.SS1.p3.8.m8.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.8.m8.1c">T_{2}</annotation></semantics></math> is distinguishable from source domain samples (see Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Generative Adversarial Networks ‣ 2 Related Work ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
Due to this functionality, even if no precisely matched training image pairings are available, CycleGAN can generate synthetic images from the target domain based on images from the source domain.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Deep Learning based Re-Identification</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">As has been hinted in the introduction of this contribution, for the purpose of re-identification, the subject of interest is assigned a descriptor when first recorded by one of the cameras in the camera network.
This descriptor assignment, which is similar to a class assignment, as would be common in classification tasks, is what the re-identification of a previously recorded subject at a later point in time is based on.
The difference, compared to an ordinary classification task, lies in the uniqueness of the descriptor, meaning that classes containing only a single instance are created (when re-identification is treated as a classification task and not a metric learning task, which would be a valid alternative <cite class="ltx_cite ltx_citemacro_citep">(Yi et al. <a href="#bib.bib21" title="" class="ltx_ref">2014</a>; Yu, Wu, and Zheng <a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite>).</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.3" class="ltx_p">Re-identification is commonly used in the context of pedestrian surveillance and in this context commonly realized by use of deep learning based methods <cite class="ltx_cite ltx_citemacro_citep">(Ye et al. <a href="#bib.bib20" title="" class="ltx_ref">2021</a>; Yi et al. <a href="#bib.bib21" title="" class="ltx_ref">2014</a>; Zheng, Yang, and Hauptmann <a href="#bib.bib23" title="" class="ltx_ref">2016</a>)</cite>.
Apart from pedestrians, other entities, such as vehicles, have been the subject of re-identification <cite class="ltx_cite ltx_citemacro_citep">(Rong et al. <a href="#bib.bib12" title="" class="ltx_ref">2021</a>; Wei et al. <a href="#bib.bib19" title="" class="ltx_ref">2018</a>)</cite>, but the focus remains strongly on humans.
In a previous publication <cite class="ltx_cite ltx_citemacro_citep">(Rutinowski et al. <a href="#bib.bib14" title="" class="ltx_ref">2021b</a>)</cite>, it was demonstrated that re-identification methods can also be of value in the context of warehousing logistics.
For this purpose, a state-of-the-art pedestrian re-identification framework, Torchreid <cite class="ltx_cite ltx_citemacro_citep">(Zhou and Xiang <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite>, was used in conjunction with PCB_P4 <cite class="ltx_cite ltx_citemacro_citep">(Sun et al. <a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite> on a self developed dataset called pallet-block-502 <cite class="ltx_cite ltx_citemacro_citep">(Rutinowski et al. <a href="#bib.bib13" title="" class="ltx_ref">2021a</a>)</cite>.
This dataset contains <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mn id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><cn type="integer" id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">10</annotation></semantics></math> images each of <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="502" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mn id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">502</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><cn type="integer" id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">502</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">502</annotation></semantics></math> pallet blocks of Euro-pallets made out of chipwood.
The <math id="S2.SS2.p2.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S2.SS2.p2.3.m3.1a"><mn id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><cn type="integer" id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">10</annotation></semantics></math> images are made up of five different perspectives (central perspective, left/right-hand side rotation, left/right-hand side shift) and two lighting modes (natural and artificial lighting).
With this setup, it was possible to re-identify the respective pallet blocks based on their individual, unique chipwood pattern with an accuracy of at least 96%, depending on the matching scenario.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodological Approach</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this work, we use a subset of the dataset pallet-block-502 <cite class="ltx_cite ltx_citemacro_citep">(Rutinowski et al. <a href="#bib.bib13" title="" class="ltx_ref">2021a</a>)</cite>, presented in Section <a href="#S2.SS2" title="2.2 Deep Learning based Re-Identification ‣ 2 Related Work ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
It is a suitable dataset for the task at hand, since it contains multiple labeled images of pallet blocks, taken from different perspectives, with different lighting conditions.
We focus on images belonging to two distinct perspectives, which will be our source and target domains, namely the central (C) and left-hand side rotation (RL) perspectives.
Formally, the input data can be defined as follows:</p>
</div>
<div id="Thmdefinition1" class="ltx_theorem ltx_theorem_definition">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span id="Thmdefinition1.1.1.1" class="ltx_text ltx_font_bold">Definition 1</span></span></h6>
<div id="Thmdefinition1.p1" class="ltx_para">
<p id="Thmdefinition1.p1.10" class="ltx_p"><span id="Thmdefinition1.p1.10.10" class="ltx_text ltx_font_italic">Let <math id="Thmdefinition1.p1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="Thmdefinition1.p1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.1.1.m1.1.1" xref="Thmdefinition1.p1.1.1.m1.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.1.1.m1.1b"><ci id="Thmdefinition1.p1.1.1.m1.1.1.cmml" xref="Thmdefinition1.p1.1.1.m1.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.1.1.m1.1c">\mathcal{D}</annotation></semantics></math> be a dataset composed of images of <math id="Thmdefinition1.p1.2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="Thmdefinition1.p1.2.2.m2.1a"><mi id="Thmdefinition1.p1.2.2.m2.1.1" xref="Thmdefinition1.p1.2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.2.2.m2.1b"><ci id="Thmdefinition1.p1.2.2.m2.1.1.cmml" xref="Thmdefinition1.p1.2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.2.2.m2.1c">N</annotation></semantics></math> pallet blocks from a central <math id="Thmdefinition1.p1.3.3.m3.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="Thmdefinition1.p1.3.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.3.3.m3.1.1" xref="Thmdefinition1.p1.3.3.m3.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.3.3.m3.1b"><ci id="Thmdefinition1.p1.3.3.m3.1.1.cmml" xref="Thmdefinition1.p1.3.3.m3.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.3.3.m3.1c">\mathcal{C}</annotation></semantics></math> and left-hand side rotation <math id="Thmdefinition1.p1.4.4.m4.1" class="ltx_Math" alttext="\mathcal{RL}" display="inline"><semantics id="Thmdefinition1.p1.4.4.m4.1a"><mrow id="Thmdefinition1.p1.4.4.m4.1.1" xref="Thmdefinition1.p1.4.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.4.4.m4.1.1.2" xref="Thmdefinition1.p1.4.4.m4.1.1.2.cmml">ℛ</mi><mo lspace="0em" rspace="0em" id="Thmdefinition1.p1.4.4.m4.1.1.1" xref="Thmdefinition1.p1.4.4.m4.1.1.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.4.4.m4.1.1.3" xref="Thmdefinition1.p1.4.4.m4.1.1.3.cmml">ℒ</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.4.4.m4.1b"><apply id="Thmdefinition1.p1.4.4.m4.1.1.cmml" xref="Thmdefinition1.p1.4.4.m4.1.1"><times id="Thmdefinition1.p1.4.4.m4.1.1.1.cmml" xref="Thmdefinition1.p1.4.4.m4.1.1.1"></times><ci id="Thmdefinition1.p1.4.4.m4.1.1.2.cmml" xref="Thmdefinition1.p1.4.4.m4.1.1.2">ℛ</ci><ci id="Thmdefinition1.p1.4.4.m4.1.1.3.cmml" xref="Thmdefinition1.p1.4.4.m4.1.1.3">ℒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.4.4.m4.1c">\mathcal{RL}</annotation></semantics></math> perspective, s.t. <math id="Thmdefinition1.p1.5.5.m5.1" class="ltx_Math" alttext="\mathcal{C}\cap\mathcal{RL}=\emptyset" display="inline"><semantics id="Thmdefinition1.p1.5.5.m5.1a"><mrow id="Thmdefinition1.p1.5.5.m5.1.1" xref="Thmdefinition1.p1.5.5.m5.1.1.cmml"><mrow id="Thmdefinition1.p1.5.5.m5.1.1.2" xref="Thmdefinition1.p1.5.5.m5.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.5.5.m5.1.1.2.2" xref="Thmdefinition1.p1.5.5.m5.1.1.2.2.cmml">𝒞</mi><mo id="Thmdefinition1.p1.5.5.m5.1.1.2.1" xref="Thmdefinition1.p1.5.5.m5.1.1.2.1.cmml">∩</mo><mrow id="Thmdefinition1.p1.5.5.m5.1.1.2.3" xref="Thmdefinition1.p1.5.5.m5.1.1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.5.5.m5.1.1.2.3.2" xref="Thmdefinition1.p1.5.5.m5.1.1.2.3.2.cmml">ℛ</mi><mo lspace="0em" rspace="0em" id="Thmdefinition1.p1.5.5.m5.1.1.2.3.1" xref="Thmdefinition1.p1.5.5.m5.1.1.2.3.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.5.5.m5.1.1.2.3.3" xref="Thmdefinition1.p1.5.5.m5.1.1.2.3.3.cmml">ℒ</mi></mrow></mrow><mo id="Thmdefinition1.p1.5.5.m5.1.1.1" xref="Thmdefinition1.p1.5.5.m5.1.1.1.cmml">=</mo><mi mathvariant="normal" id="Thmdefinition1.p1.5.5.m5.1.1.3" xref="Thmdefinition1.p1.5.5.m5.1.1.3.cmml">∅</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.5.5.m5.1b"><apply id="Thmdefinition1.p1.5.5.m5.1.1.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1"><eq id="Thmdefinition1.p1.5.5.m5.1.1.1.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1.1"></eq><apply id="Thmdefinition1.p1.5.5.m5.1.1.2.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1.2"><intersect id="Thmdefinition1.p1.5.5.m5.1.1.2.1.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1.2.1"></intersect><ci id="Thmdefinition1.p1.5.5.m5.1.1.2.2.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1.2.2">𝒞</ci><apply id="Thmdefinition1.p1.5.5.m5.1.1.2.3.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1.2.3"><times id="Thmdefinition1.p1.5.5.m5.1.1.2.3.1.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1.2.3.1"></times><ci id="Thmdefinition1.p1.5.5.m5.1.1.2.3.2.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1.2.3.2">ℛ</ci><ci id="Thmdefinition1.p1.5.5.m5.1.1.2.3.3.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1.2.3.3">ℒ</ci></apply></apply><emptyset id="Thmdefinition1.p1.5.5.m5.1.1.3.cmml" xref="Thmdefinition1.p1.5.5.m5.1.1.3"></emptyset></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.5.5.m5.1c">\mathcal{C}\cap\mathcal{RL}=\emptyset</annotation></semantics></math>.
In <math id="Thmdefinition1.p1.6.6.m6.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="Thmdefinition1.p1.6.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.6.6.m6.1.1" xref="Thmdefinition1.p1.6.6.m6.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.6.6.m6.1b"><ci id="Thmdefinition1.p1.6.6.m6.1.1.cmml" xref="Thmdefinition1.p1.6.6.m6.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.6.6.m6.1c">\mathcal{D}</annotation></semantics></math>, there are two distinct images for each perspective (<math id="Thmdefinition1.p1.7.7.m7.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="Thmdefinition1.p1.7.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.7.7.m7.1.1" xref="Thmdefinition1.p1.7.7.m7.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.7.7.m7.1b"><ci id="Thmdefinition1.p1.7.7.m7.1.1.cmml" xref="Thmdefinition1.p1.7.7.m7.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.7.7.m7.1c">\mathcal{C}</annotation></semantics></math> and <math id="Thmdefinition1.p1.8.8.m8.1" class="ltx_Math" alttext="\mathcal{RL}" display="inline"><semantics id="Thmdefinition1.p1.8.8.m8.1a"><mrow id="Thmdefinition1.p1.8.8.m8.1.1" xref="Thmdefinition1.p1.8.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.8.8.m8.1.1.2" xref="Thmdefinition1.p1.8.8.m8.1.1.2.cmml">ℛ</mi><mo lspace="0em" rspace="0em" id="Thmdefinition1.p1.8.8.m8.1.1.1" xref="Thmdefinition1.p1.8.8.m8.1.1.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.8.8.m8.1.1.3" xref="Thmdefinition1.p1.8.8.m8.1.1.3.cmml">ℒ</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.8.8.m8.1b"><apply id="Thmdefinition1.p1.8.8.m8.1.1.cmml" xref="Thmdefinition1.p1.8.8.m8.1.1"><times id="Thmdefinition1.p1.8.8.m8.1.1.1.cmml" xref="Thmdefinition1.p1.8.8.m8.1.1.1"></times><ci id="Thmdefinition1.p1.8.8.m8.1.1.2.cmml" xref="Thmdefinition1.p1.8.8.m8.1.1.2">ℛ</ci><ci id="Thmdefinition1.p1.8.8.m8.1.1.3.cmml" xref="Thmdefinition1.p1.8.8.m8.1.1.3">ℒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.8.8.m8.1c">\mathcal{RL}</annotation></semantics></math>) of a single pallet block <math id="Thmdefinition1.p1.9.9.m9.1" class="ltx_Math" alttext="i" display="inline"><semantics id="Thmdefinition1.p1.9.9.m9.1a"><mi id="Thmdefinition1.p1.9.9.m9.1.1" xref="Thmdefinition1.p1.9.9.m9.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.9.9.m9.1b"><ci id="Thmdefinition1.p1.9.9.m9.1.1.cmml" xref="Thmdefinition1.p1.9.9.m9.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.9.9.m9.1c">i</annotation></semantics></math> (<math id="Thmdefinition1.p1.10.10.m10.1" class="ltx_Math" alttext="1\leq i\leq N" display="inline"><semantics id="Thmdefinition1.p1.10.10.m10.1a"><mrow id="Thmdefinition1.p1.10.10.m10.1.1" xref="Thmdefinition1.p1.10.10.m10.1.1.cmml"><mn id="Thmdefinition1.p1.10.10.m10.1.1.2" xref="Thmdefinition1.p1.10.10.m10.1.1.2.cmml">1</mn><mo id="Thmdefinition1.p1.10.10.m10.1.1.3" xref="Thmdefinition1.p1.10.10.m10.1.1.3.cmml">≤</mo><mi id="Thmdefinition1.p1.10.10.m10.1.1.4" xref="Thmdefinition1.p1.10.10.m10.1.1.4.cmml">i</mi><mo id="Thmdefinition1.p1.10.10.m10.1.1.5" xref="Thmdefinition1.p1.10.10.m10.1.1.5.cmml">≤</mo><mi id="Thmdefinition1.p1.10.10.m10.1.1.6" xref="Thmdefinition1.p1.10.10.m10.1.1.6.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.10.10.m10.1b"><apply id="Thmdefinition1.p1.10.10.m10.1.1.cmml" xref="Thmdefinition1.p1.10.10.m10.1.1"><and id="Thmdefinition1.p1.10.10.m10.1.1a.cmml" xref="Thmdefinition1.p1.10.10.m10.1.1"></and><apply id="Thmdefinition1.p1.10.10.m10.1.1b.cmml" xref="Thmdefinition1.p1.10.10.m10.1.1"><leq id="Thmdefinition1.p1.10.10.m10.1.1.3.cmml" xref="Thmdefinition1.p1.10.10.m10.1.1.3"></leq><cn type="integer" id="Thmdefinition1.p1.10.10.m10.1.1.2.cmml" xref="Thmdefinition1.p1.10.10.m10.1.1.2">1</cn><ci id="Thmdefinition1.p1.10.10.m10.1.1.4.cmml" xref="Thmdefinition1.p1.10.10.m10.1.1.4">𝑖</ci></apply><apply id="Thmdefinition1.p1.10.10.m10.1.1c.cmml" xref="Thmdefinition1.p1.10.10.m10.1.1"><leq id="Thmdefinition1.p1.10.10.m10.1.1.5.cmml" xref="Thmdefinition1.p1.10.10.m10.1.1.5"></leq><share href="#Thmdefinition1.p1.10.10.m10.1.1.4.cmml" id="Thmdefinition1.p1.10.10.m10.1.1d.cmml" xref="Thmdefinition1.p1.10.10.m10.1.1"></share><ci id="Thmdefinition1.p1.10.10.m10.1.1.6.cmml" xref="Thmdefinition1.p1.10.10.m10.1.1.6">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.10.10.m10.1c">1\leq i\leq N</annotation></semantics></math>) with varying lighting conditions.</span></p>
</div>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.4" class="ltx_p">In this work, the problem is defined as given an image <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="c_{i}\in\mathcal{C}" display="inline"><semantics id="S3.p2.1.m1.1a"><mrow id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><msub id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml"><mi id="S3.p2.1.m1.1.1.2.2" xref="S3.p2.1.m1.1.1.2.2.cmml">c</mi><mi id="S3.p2.1.m1.1.1.2.3" xref="S3.p2.1.m1.1.1.2.3.cmml">i</mi></msub><mo id="S3.p2.1.m1.1.1.1" xref="S3.p2.1.m1.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml">𝒞</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><in id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1.1"></in><apply id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.2.1.cmml" xref="S3.p2.1.m1.1.1.2">subscript</csymbol><ci id="S3.p2.1.m1.1.1.2.2.cmml" xref="S3.p2.1.m1.1.1.2.2">𝑐</ci><ci id="S3.p2.1.m1.1.1.2.3.cmml" xref="S3.p2.1.m1.1.1.2.3">𝑖</ci></apply><ci id="S3.p2.1.m1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.3">𝒞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">c_{i}\in\mathcal{C}</annotation></semantics></math> of a pallet block <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">i</annotation></semantics></math>, generate a synthetic image for the RL perspective.
We denote these synthetic images as <math id="S3.p2.3.m3.1" class="ltx_Math" alttext="\hat{r}_{i}" display="inline"><semantics id="S3.p2.3.m3.1a"><msub id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml"><mover accent="true" id="S3.p2.3.m3.1.1.2" xref="S3.p2.3.m3.1.1.2.cmml"><mi id="S3.p2.3.m3.1.1.2.2" xref="S3.p2.3.m3.1.1.2.2.cmml">r</mi><mo id="S3.p2.3.m3.1.1.2.1" xref="S3.p2.3.m3.1.1.2.1.cmml">^</mo></mover><mi id="S3.p2.3.m3.1.1.3" xref="S3.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><apply id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.1.cmml" xref="S3.p2.3.m3.1.1">subscript</csymbol><apply id="S3.p2.3.m3.1.1.2.cmml" xref="S3.p2.3.m3.1.1.2"><ci id="S3.p2.3.m3.1.1.2.1.cmml" xref="S3.p2.3.m3.1.1.2.1">^</ci><ci id="S3.p2.3.m3.1.1.2.2.cmml" xref="S3.p2.3.m3.1.1.2.2">𝑟</ci></apply><ci id="S3.p2.3.m3.1.1.3.cmml" xref="S3.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">\hat{r}_{i}</annotation></semantics></math> and their set as <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="\widehat{\mathcal{RL}}" display="inline"><semantics id="S3.p2.4.m4.1a"><mover accent="true" id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mrow id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p2.4.m4.1.1.2.2" xref="S3.p2.4.m4.1.1.2.2.cmml">ℛ</mi><mo lspace="0em" rspace="0em" id="S3.p2.4.m4.1.1.2.1" xref="S3.p2.4.m4.1.1.2.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="S3.p2.4.m4.1.1.2.3" xref="S3.p2.4.m4.1.1.2.3.cmml">ℒ</mi></mrow><mo id="S3.p2.4.m4.1.1.1" xref="S3.p2.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><ci id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1.1">^</ci><apply id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2"><times id="S3.p2.4.m4.1.1.2.1.cmml" xref="S3.p2.4.m4.1.1.2.1"></times><ci id="S3.p2.4.m4.1.1.2.2.cmml" xref="S3.p2.4.m4.1.1.2.2">ℛ</ci><ci id="S3.p2.4.m4.1.1.2.3.cmml" xref="S3.p2.4.m4.1.1.2.3">ℒ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">\widehat{\mathcal{RL}}</annotation></semantics></math>.
To address this problem, in Section <a href="#S3.SS1" title="3.1 Learning Process: GAN Selection &amp; Training ‣ 3 Methodological Approach ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> we first present the GAN architecture that was chosen for these experiments and discuss the way in which it was applied to our dataset.
Then, in Section <a href="#S3.SS2" title="3.2 Formulating the Evaluation Tasks ‣ 3 Methodological Approach ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we present the downstream tasks that are used to assess the quality of the synthetic images that were generated by the GAN.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Learning Process: GAN Selection &amp; Training</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.3" class="ltx_p">Based on the C and RL perspective subset, consisting of two times <math id="S3.SS1.p1.1.m1.2" class="ltx_Math" alttext="1,004" display="inline"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.3.2" xref="S3.SS1.p1.1.m1.2.3.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">1</mn><mo id="S3.SS1.p1.1.m1.2.3.2.1" xref="S3.SS1.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml">004</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><list id="S3.SS1.p1.1.m1.2.3.1.cmml" xref="S3.SS1.p1.1.m1.2.3.2"><cn type="integer" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">1</cn><cn type="integer" id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2">004</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">1,004</annotation></semantics></math> images of the dataset, we aim to again generate <math id="S3.SS1.p1.2.m2.2" class="ltx_Math" alttext="1,004" display="inline"><semantics id="S3.SS1.p1.2.m2.2a"><mrow id="S3.SS1.p1.2.m2.2.3.2" xref="S3.SS1.p1.2.m2.2.3.1.cmml"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">1</mn><mo id="S3.SS1.p1.2.m2.2.3.2.1" xref="S3.SS1.p1.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS1.p1.2.m2.2.2" xref="S3.SS1.p1.2.m2.2.2.cmml">004</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.2b"><list id="S3.SS1.p1.2.m2.2.3.1.cmml" xref="S3.SS1.p1.2.m2.2.3.2"><cn type="integer" id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">1</cn><cn type="integer" id="S3.SS1.p1.2.m2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2">004</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.2c">1,004</annotation></semantics></math> images from the RL perspective, by providing <math id="S3.SS1.p1.3.m3.2" class="ltx_Math" alttext="1,004" display="inline"><semantics id="S3.SS1.p1.3.m3.2a"><mrow id="S3.SS1.p1.3.m3.2.3.2" xref="S3.SS1.p1.3.m3.2.3.1.cmml"><mn id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">1</mn><mo id="S3.SS1.p1.3.m3.2.3.2.1" xref="S3.SS1.p1.3.m3.2.3.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.2.2" xref="S3.SS1.p1.3.m3.2.2.cmml">004</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.2b"><list id="S3.SS1.p1.3.m3.2.3.1.cmml" xref="S3.SS1.p1.3.m3.2.3.2"><cn type="integer" id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">1</cn><cn type="integer" id="S3.SS1.p1.3.m3.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2">004</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.2c">1,004</annotation></semantics></math> corresponding images from the C perspective, while preserving the chipwood surface structure of the respective pallet block.
Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Learning Process: GAN Selection &amp; Training ‣ 3 Methodological Approach ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows an example of a single pallet block from these two rotational domains, embedded in the conceptualized representation of an adversarial network with cycle consistency.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">With this aim in mind, image-to-image translation can be used to create RL perspective images from C perspective images.
While pix2pix GAN could be used to create a supervised one-to-one mapping between the two domains, it requires a paired set of C and RL images.
This, however, cannot be guaranteed for future use cases, reducing the appeal of pix2pix GAN for our intents and purposes.
CycleGAN, on the other hand, uses unsupervised learning to train two generators and two discriminators.
By doing so, even without precisely matched training image pairings, it can generate synthetic images from the RL domain based on input images from the C domain, which is the perspective shift we will focus on.
For this reason, we will use CycleGAN for our experiments.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<p id="S3.F2.1" class="ltx_p ltx_align_center"><span id="S3.F2.1.1" class="ltx_text"><img src="/html/2212.10105/assets/x2.png" id="S3.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="102" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of centered (source domain) and rotated (target domain) images used for CycleGAN training (ID <math id="S3.F2.3.m1.1" class="ltx_Math" alttext="187" display="inline"><semantics id="S3.F2.3.m1.1b"><mn id="S3.F2.3.m1.1.1" xref="S3.F2.3.m1.1.1.cmml">187</mn><annotation-xml encoding="MathML-Content" id="S3.F2.3.m1.1c"><cn type="integer" id="S3.F2.3.m1.1.1.cmml" xref="S3.F2.3.m1.1.1">187</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.3.m1.1d">187</annotation></semantics></math>).</figcaption>
</figure>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Therefore, the CycleGAN generator in this scenario consists of an encoder and a decoder, with the encoder being a ResNet <cite class="ltx_cite ltx_citemacro_citep">(He et al. <a href="#bib.bib6" title="" class="ltx_ref">2016</a>)</cite> with convolutional layers that map the input (i.e., a C perspective image) to a small feature representation and the decoder being a ResNet with transpose convolutional layers, transforming the feature representation it obtains into a transformed output (i.e., an RL perspective image).
In this unsupervised setting, a single generator may map multiple input images to a single output, leading to what is called mode collapse <cite class="ltx_cite ltx_citemacro_citep">(Lala et al. <a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite>, which means that outputs with little to no variance would be generated.
To avoid this pitfall, CycleGAN uses cycle consistency, in this case therefore inversely performing RL to C transformation while simultaneously training another generator to perform C to RL transformation.
Hence, cycle consistency allows CycleGAN to correctly map C perspective images to RL perspective images, while simultaneously preserving the features of the input image in the generated image.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">CycleGAN’s two discriminator networks work analogously to its generators.
The discriminator network is a convolutional network that distinguishes between original and synthetic images, classifying them accordingly.
During each iteration the discriminator is applied to the current batch of synthetic images generated by the respective generator as well as <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mn id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><cn type="integer" id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">50</annotation></semantics></math> images generated during previous iterations.
This enables the discriminator to generalize well in the target domain, i.e., RL perspective images.
CycleGAN optimizes the adversarial loss and the cycle consistency loss, which quantifies the difference between the original C perspective input image, an image translated into the RL perspective and back again, to produce what it considers to be appropriate output images.
Additionally, an identity loss is taken into account as well, in order to retain the image’s color space.
As a result, even when using an unpaired dataset, CycleGAN can generate RL perspective images from a C perspective input image.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Formulating the Evaluation Tasks</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Different GAN evaluation settings do exist <cite class="ltx_cite ltx_citemacro_citep">(Borji <a href="#bib.bib1" title="" class="ltx_ref">2019</a>, <a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite> and can in many ways be valuable tools that enable researchers to assess the performance of their GAN.
However, these evaluations primarily focus on GAN training, leaving room for ambiguity when it comes to the application of a trained generator on new input data.
This however, is a key factor in many use cases, such as our own, which determines the usefulness of a trained generator.
Given this background, we developed our own evaluation method, tailored to our use case.
Therefore, as to evaluate the performance of the images generated by the trained CycleGAN model, the two following evaluation tasks are performed:</p>
</div>
<section id="S3.SS2.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Image perspective classification</h4>

<div id="S3.SS2.SSSx1.p1" class="ltx_para">
<p id="S3.SS2.SSSx1.p1.1" class="ltx_p">In this task, the class is defined as the perspective of the image. This is formally defined as follows:</p>
</div>
<div id="Thmdefinition2" class="ltx_theorem ltx_theorem_definition">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span id="Thmdefinition2.1.1.1" class="ltx_text ltx_font_bold">Definition 2</span></span></h6>
<div id="Thmdefinition2.p1" class="ltx_para">
<p id="Thmdefinition2.p1.3" class="ltx_p"><span id="Thmdefinition2.p1.3.3" class="ltx_text ltx_font_italic">Given an image <math id="Thmdefinition2.p1.1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="Thmdefinition2.p1.1.1.m1.1a"><mi id="Thmdefinition2.p1.1.1.m1.1.1" xref="Thmdefinition2.p1.1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition2.p1.1.1.m1.1b"><ci id="Thmdefinition2.p1.1.1.m1.1.1.cmml" xref="Thmdefinition2.p1.1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition2.p1.1.1.m1.1c">x</annotation></semantics></math>, the image perspective classification task is defined as the problem of determining if <math id="Thmdefinition2.p1.2.2.m2.1" class="ltx_Math" alttext="x\in\mathcal{C}" display="inline"><semantics id="Thmdefinition2.p1.2.2.m2.1a"><mrow id="Thmdefinition2.p1.2.2.m2.1.1" xref="Thmdefinition2.p1.2.2.m2.1.1.cmml"><mi id="Thmdefinition2.p1.2.2.m2.1.1.2" xref="Thmdefinition2.p1.2.2.m2.1.1.2.cmml">x</mi><mo id="Thmdefinition2.p1.2.2.m2.1.1.1" xref="Thmdefinition2.p1.2.2.m2.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="Thmdefinition2.p1.2.2.m2.1.1.3" xref="Thmdefinition2.p1.2.2.m2.1.1.3.cmml">𝒞</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition2.p1.2.2.m2.1b"><apply id="Thmdefinition2.p1.2.2.m2.1.1.cmml" xref="Thmdefinition2.p1.2.2.m2.1.1"><in id="Thmdefinition2.p1.2.2.m2.1.1.1.cmml" xref="Thmdefinition2.p1.2.2.m2.1.1.1"></in><ci id="Thmdefinition2.p1.2.2.m2.1.1.2.cmml" xref="Thmdefinition2.p1.2.2.m2.1.1.2">𝑥</ci><ci id="Thmdefinition2.p1.2.2.m2.1.1.3.cmml" xref="Thmdefinition2.p1.2.2.m2.1.1.3">𝒞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition2.p1.2.2.m2.1c">x\in\mathcal{C}</annotation></semantics></math> or <math id="Thmdefinition2.p1.3.3.m3.1" class="ltx_Math" alttext="x\in\mathcal{RL}" display="inline"><semantics id="Thmdefinition2.p1.3.3.m3.1a"><mrow id="Thmdefinition2.p1.3.3.m3.1.1" xref="Thmdefinition2.p1.3.3.m3.1.1.cmml"><mi id="Thmdefinition2.p1.3.3.m3.1.1.2" xref="Thmdefinition2.p1.3.3.m3.1.1.2.cmml">x</mi><mo id="Thmdefinition2.p1.3.3.m3.1.1.1" xref="Thmdefinition2.p1.3.3.m3.1.1.1.cmml">∈</mo><mrow id="Thmdefinition2.p1.3.3.m3.1.1.3" xref="Thmdefinition2.p1.3.3.m3.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition2.p1.3.3.m3.1.1.3.2" xref="Thmdefinition2.p1.3.3.m3.1.1.3.2.cmml">ℛ</mi><mo lspace="0em" rspace="0em" id="Thmdefinition2.p1.3.3.m3.1.1.3.1" xref="Thmdefinition2.p1.3.3.m3.1.1.3.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="Thmdefinition2.p1.3.3.m3.1.1.3.3" xref="Thmdefinition2.p1.3.3.m3.1.1.3.3.cmml">ℒ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition2.p1.3.3.m3.1b"><apply id="Thmdefinition2.p1.3.3.m3.1.1.cmml" xref="Thmdefinition2.p1.3.3.m3.1.1"><in id="Thmdefinition2.p1.3.3.m3.1.1.1.cmml" xref="Thmdefinition2.p1.3.3.m3.1.1.1"></in><ci id="Thmdefinition2.p1.3.3.m3.1.1.2.cmml" xref="Thmdefinition2.p1.3.3.m3.1.1.2">𝑥</ci><apply id="Thmdefinition2.p1.3.3.m3.1.1.3.cmml" xref="Thmdefinition2.p1.3.3.m3.1.1.3"><times id="Thmdefinition2.p1.3.3.m3.1.1.3.1.cmml" xref="Thmdefinition2.p1.3.3.m3.1.1.3.1"></times><ci id="Thmdefinition2.p1.3.3.m3.1.1.3.2.cmml" xref="Thmdefinition2.p1.3.3.m3.1.1.3.2">ℛ</ci><ci id="Thmdefinition2.p1.3.3.m3.1.1.3.3.cmml" xref="Thmdefinition2.p1.3.3.m3.1.1.3.3">ℒ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition2.p1.3.3.m3.1c">x\in\mathcal{RL}</annotation></semantics></math>.</span></p>
</div>
</div>
<div id="S3.SS2.SSSx1.p2" class="ltx_para">
<p id="S3.SS2.SSSx1.p2.6" class="ltx_p">In this work, a classifier is trained on <math id="S3.SS2.SSSx1.p2.1.m1.1" class="ltx_Math" alttext="80\%" display="inline"><semantics id="S3.SS2.SSSx1.p2.1.m1.1a"><mrow id="S3.SS2.SSSx1.p2.1.m1.1.1" xref="S3.SS2.SSSx1.p2.1.m1.1.1.cmml"><mn id="S3.SS2.SSSx1.p2.1.m1.1.1.2" xref="S3.SS2.SSSx1.p2.1.m1.1.1.2.cmml">80</mn><mo id="S3.SS2.SSSx1.p2.1.m1.1.1.1" xref="S3.SS2.SSSx1.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p2.1.m1.1b"><apply id="S3.SS2.SSSx1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSSx1.p2.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.SSSx1.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSSx1.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.SSSx1.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSSx1.p2.1.m1.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p2.1.m1.1c">80\%</annotation></semantics></math> of the C and RL pallet block images of the dataset pallet-block-502, with the aim of distinguishing between these two types of perspectives (<math id="S3.SS2.SSSx1.p2.2.m2.2" class="ltx_Math" alttext="1,608" display="inline"><semantics id="S3.SS2.SSSx1.p2.2.m2.2a"><mrow id="S3.SS2.SSSx1.p2.2.m2.2.3.2" xref="S3.SS2.SSSx1.p2.2.m2.2.3.1.cmml"><mn id="S3.SS2.SSSx1.p2.2.m2.1.1" xref="S3.SS2.SSSx1.p2.2.m2.1.1.cmml">1</mn><mo id="S3.SS2.SSSx1.p2.2.m2.2.3.2.1" xref="S3.SS2.SSSx1.p2.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS2.SSSx1.p2.2.m2.2.2" xref="S3.SS2.SSSx1.p2.2.m2.2.2.cmml">608</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p2.2.m2.2b"><list id="S3.SS2.SSSx1.p2.2.m2.2.3.1.cmml" xref="S3.SS2.SSSx1.p2.2.m2.2.3.2"><cn type="integer" id="S3.SS2.SSSx1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSSx1.p2.2.m2.1.1">1</cn><cn type="integer" id="S3.SS2.SSSx1.p2.2.m2.2.2.cmml" xref="S3.SS2.SSSx1.p2.2.m2.2.2">608</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p2.2.m2.2c">1,608</annotation></semantics></math> images in total, <math id="S3.SS2.SSSx1.p2.3.m3.1" class="ltx_Math" alttext="804" display="inline"><semantics id="S3.SS2.SSSx1.p2.3.m3.1a"><mn id="S3.SS2.SSSx1.p2.3.m3.1.1" xref="S3.SS2.SSSx1.p2.3.m3.1.1.cmml">804</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p2.3.m3.1b"><cn type="integer" id="S3.SS2.SSSx1.p2.3.m3.1.1.cmml" xref="S3.SS2.SSSx1.p2.3.m3.1.1">804</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p2.3.m3.1c">804</annotation></semantics></math> per class).
The trained model is then applied to the hold-out dataset consisting of the remaining <math id="S3.SS2.SSSx1.p2.4.m4.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S3.SS2.SSSx1.p2.4.m4.1a"><mrow id="S3.SS2.SSSx1.p2.4.m4.1.1" xref="S3.SS2.SSSx1.p2.4.m4.1.1.cmml"><mn id="S3.SS2.SSSx1.p2.4.m4.1.1.2" xref="S3.SS2.SSSx1.p2.4.m4.1.1.2.cmml">20</mn><mo id="S3.SS2.SSSx1.p2.4.m4.1.1.1" xref="S3.SS2.SSSx1.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p2.4.m4.1b"><apply id="S3.SS2.SSSx1.p2.4.m4.1.1.cmml" xref="S3.SS2.SSSx1.p2.4.m4.1.1"><csymbol cd="latexml" id="S3.SS2.SSSx1.p2.4.m4.1.1.1.cmml" xref="S3.SS2.SSSx1.p2.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.SSSx1.p2.4.m4.1.1.2.cmml" xref="S3.SS2.SSSx1.p2.4.m4.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p2.4.m4.1c">20\%</annotation></semantics></math> of the respective dataset images (<math id="S3.SS2.SSSx1.p2.5.m5.1" class="ltx_Math" alttext="400" display="inline"><semantics id="S3.SS2.SSSx1.p2.5.m5.1a"><mn id="S3.SS2.SSSx1.p2.5.m5.1.1" xref="S3.SS2.SSSx1.p2.5.m5.1.1.cmml">400</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p2.5.m5.1b"><cn type="integer" id="S3.SS2.SSSx1.p2.5.m5.1.1.cmml" xref="S3.SS2.SSSx1.p2.5.m5.1.1">400</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p2.5.m5.1c">400</annotation></semantics></math> images in total, <math id="S3.SS2.SSSx1.p2.6.m6.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S3.SS2.SSSx1.p2.6.m6.1a"><mn id="S3.SS2.SSSx1.p2.6.m6.1.1" xref="S3.SS2.SSSx1.p2.6.m6.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx1.p2.6.m6.1b"><cn type="integer" id="S3.SS2.SSSx1.p2.6.m6.1.1.cmml" xref="S3.SS2.SSSx1.p2.6.m6.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx1.p2.6.m6.1c">200</annotation></semantics></math> images per class), thereby establishing a benchmark for how accurately an original yet unknown image can be classified by its perspective.
Finally, the model is applied to a randomly chosen subset (of the same size as the hold-out dataset) of the synthetic images and the classification accuracy on both sets of images compared to one another.</p>
</div>
</section>
<section id="S3.SS2.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Re-identification of pallet blocks based on synthetic images</h4>

<div id="S3.SS2.SSSx2.p1" class="ltx_para">
<p id="S3.SS2.SSSx2.p1.1" class="ltx_p">Based on our definition of the input data, the task of re-identification is defined as:</p>
</div>
<div id="Thmdefinition3" class="ltx_theorem ltx_theorem_definition">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span id="Thmdefinition3.1.1.1" class="ltx_text ltx_font_bold">Definition 3</span></span></h6>
<div id="Thmdefinition3.p1" class="ltx_para">
<p id="Thmdefinition3.p1.8" class="ltx_p"><span id="Thmdefinition3.p1.8.8" class="ltx_text ltx_font_italic">Given a query set <math id="Thmdefinition3.p1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{Q}\subset\mathcal{D}" display="inline"><semantics id="Thmdefinition3.p1.1.1.m1.1a"><mrow id="Thmdefinition3.p1.1.1.m1.1.1" xref="Thmdefinition3.p1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition3.p1.1.1.m1.1.1.2" xref="Thmdefinition3.p1.1.1.m1.1.1.2.cmml">𝒬</mi><mo id="Thmdefinition3.p1.1.1.m1.1.1.1" xref="Thmdefinition3.p1.1.1.m1.1.1.1.cmml">⊂</mo><mi class="ltx_font_mathcaligraphic" id="Thmdefinition3.p1.1.1.m1.1.1.3" xref="Thmdefinition3.p1.1.1.m1.1.1.3.cmml">𝒟</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition3.p1.1.1.m1.1b"><apply id="Thmdefinition3.p1.1.1.m1.1.1.cmml" xref="Thmdefinition3.p1.1.1.m1.1.1"><subset id="Thmdefinition3.p1.1.1.m1.1.1.1.cmml" xref="Thmdefinition3.p1.1.1.m1.1.1.1"></subset><ci id="Thmdefinition3.p1.1.1.m1.1.1.2.cmml" xref="Thmdefinition3.p1.1.1.m1.1.1.2">𝒬</ci><ci id="Thmdefinition3.p1.1.1.m1.1.1.3.cmml" xref="Thmdefinition3.p1.1.1.m1.1.1.3">𝒟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition3.p1.1.1.m1.1c">\mathcal{Q}\subset\mathcal{D}</annotation></semantics></math> and a gallery set <math id="Thmdefinition3.p1.2.2.m2.1" class="ltx_Math" alttext="\mathcal{G}\subset\mathcal{D}" display="inline"><semantics id="Thmdefinition3.p1.2.2.m2.1a"><mrow id="Thmdefinition3.p1.2.2.m2.1.1" xref="Thmdefinition3.p1.2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition3.p1.2.2.m2.1.1.2" xref="Thmdefinition3.p1.2.2.m2.1.1.2.cmml">𝒢</mi><mo id="Thmdefinition3.p1.2.2.m2.1.1.1" xref="Thmdefinition3.p1.2.2.m2.1.1.1.cmml">⊂</mo><mi class="ltx_font_mathcaligraphic" id="Thmdefinition3.p1.2.2.m2.1.1.3" xref="Thmdefinition3.p1.2.2.m2.1.1.3.cmml">𝒟</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition3.p1.2.2.m2.1b"><apply id="Thmdefinition3.p1.2.2.m2.1.1.cmml" xref="Thmdefinition3.p1.2.2.m2.1.1"><subset id="Thmdefinition3.p1.2.2.m2.1.1.1.cmml" xref="Thmdefinition3.p1.2.2.m2.1.1.1"></subset><ci id="Thmdefinition3.p1.2.2.m2.1.1.2.cmml" xref="Thmdefinition3.p1.2.2.m2.1.1.2">𝒢</ci><ci id="Thmdefinition3.p1.2.2.m2.1.1.3.cmml" xref="Thmdefinition3.p1.2.2.m2.1.1.3">𝒟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition3.p1.2.2.m2.1c">\mathcal{G}\subset\mathcal{D}</annotation></semantics></math> with <math id="Thmdefinition3.p1.3.3.m3.1" class="ltx_Math" alttext="\mathcal{Q}~{}\cap~{}\mathcal{G}=\emptyset" display="inline"><semantics id="Thmdefinition3.p1.3.3.m3.1a"><mrow id="Thmdefinition3.p1.3.3.m3.1.1" xref="Thmdefinition3.p1.3.3.m3.1.1.cmml"><mrow id="Thmdefinition3.p1.3.3.m3.1.1.2" xref="Thmdefinition3.p1.3.3.m3.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition3.p1.3.3.m3.1.1.2.2" xref="Thmdefinition3.p1.3.3.m3.1.1.2.2.cmml">𝒬</mi><mo lspace="0.552em" rspace="0.552em" id="Thmdefinition3.p1.3.3.m3.1.1.2.1" xref="Thmdefinition3.p1.3.3.m3.1.1.2.1.cmml">∩</mo><mi class="ltx_font_mathcaligraphic" id="Thmdefinition3.p1.3.3.m3.1.1.2.3" xref="Thmdefinition3.p1.3.3.m3.1.1.2.3.cmml">𝒢</mi></mrow><mo id="Thmdefinition3.p1.3.3.m3.1.1.1" xref="Thmdefinition3.p1.3.3.m3.1.1.1.cmml">=</mo><mi mathvariant="normal" id="Thmdefinition3.p1.3.3.m3.1.1.3" xref="Thmdefinition3.p1.3.3.m3.1.1.3.cmml">∅</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition3.p1.3.3.m3.1b"><apply id="Thmdefinition3.p1.3.3.m3.1.1.cmml" xref="Thmdefinition3.p1.3.3.m3.1.1"><eq id="Thmdefinition3.p1.3.3.m3.1.1.1.cmml" xref="Thmdefinition3.p1.3.3.m3.1.1.1"></eq><apply id="Thmdefinition3.p1.3.3.m3.1.1.2.cmml" xref="Thmdefinition3.p1.3.3.m3.1.1.2"><intersect id="Thmdefinition3.p1.3.3.m3.1.1.2.1.cmml" xref="Thmdefinition3.p1.3.3.m3.1.1.2.1"></intersect><ci id="Thmdefinition3.p1.3.3.m3.1.1.2.2.cmml" xref="Thmdefinition3.p1.3.3.m3.1.1.2.2">𝒬</ci><ci id="Thmdefinition3.p1.3.3.m3.1.1.2.3.cmml" xref="Thmdefinition3.p1.3.3.m3.1.1.2.3">𝒢</ci></apply><emptyset id="Thmdefinition3.p1.3.3.m3.1.1.3.cmml" xref="Thmdefinition3.p1.3.3.m3.1.1.3"></emptyset></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition3.p1.3.3.m3.1c">\mathcal{Q}~{}\cap~{}\mathcal{G}=\emptyset</annotation></semantics></math>, the problem of re-identification of an image <math id="Thmdefinition3.p1.4.4.m4.1" class="ltx_Math" alttext="x_{i}\in\mathcal{Q}" display="inline"><semantics id="Thmdefinition3.p1.4.4.m4.1a"><mrow id="Thmdefinition3.p1.4.4.m4.1.1" xref="Thmdefinition3.p1.4.4.m4.1.1.cmml"><msub id="Thmdefinition3.p1.4.4.m4.1.1.2" xref="Thmdefinition3.p1.4.4.m4.1.1.2.cmml"><mi id="Thmdefinition3.p1.4.4.m4.1.1.2.2" xref="Thmdefinition3.p1.4.4.m4.1.1.2.2.cmml">x</mi><mi id="Thmdefinition3.p1.4.4.m4.1.1.2.3" xref="Thmdefinition3.p1.4.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="Thmdefinition3.p1.4.4.m4.1.1.1" xref="Thmdefinition3.p1.4.4.m4.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="Thmdefinition3.p1.4.4.m4.1.1.3" xref="Thmdefinition3.p1.4.4.m4.1.1.3.cmml">𝒬</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition3.p1.4.4.m4.1b"><apply id="Thmdefinition3.p1.4.4.m4.1.1.cmml" xref="Thmdefinition3.p1.4.4.m4.1.1"><in id="Thmdefinition3.p1.4.4.m4.1.1.1.cmml" xref="Thmdefinition3.p1.4.4.m4.1.1.1"></in><apply id="Thmdefinition3.p1.4.4.m4.1.1.2.cmml" xref="Thmdefinition3.p1.4.4.m4.1.1.2"><csymbol cd="ambiguous" id="Thmdefinition3.p1.4.4.m4.1.1.2.1.cmml" xref="Thmdefinition3.p1.4.4.m4.1.1.2">subscript</csymbol><ci id="Thmdefinition3.p1.4.4.m4.1.1.2.2.cmml" xref="Thmdefinition3.p1.4.4.m4.1.1.2.2">𝑥</ci><ci id="Thmdefinition3.p1.4.4.m4.1.1.2.3.cmml" xref="Thmdefinition3.p1.4.4.m4.1.1.2.3">𝑖</ci></apply><ci id="Thmdefinition3.p1.4.4.m4.1.1.3.cmml" xref="Thmdefinition3.p1.4.4.m4.1.1.3">𝒬</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition3.p1.4.4.m4.1c">x_{i}\in\mathcal{Q}</annotation></semantics></math> of pallet block <math id="Thmdefinition3.p1.5.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="Thmdefinition3.p1.5.5.m5.1a"><mi id="Thmdefinition3.p1.5.5.m5.1.1" xref="Thmdefinition3.p1.5.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition3.p1.5.5.m5.1b"><ci id="Thmdefinition3.p1.5.5.m5.1.1.cmml" xref="Thmdefinition3.p1.5.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition3.p1.5.5.m5.1c">i</annotation></semantics></math> is to find an image <math id="Thmdefinition3.p1.6.6.m6.1" class="ltx_Math" alttext="y_{j}\in\mathcal{G}" display="inline"><semantics id="Thmdefinition3.p1.6.6.m6.1a"><mrow id="Thmdefinition3.p1.6.6.m6.1.1" xref="Thmdefinition3.p1.6.6.m6.1.1.cmml"><msub id="Thmdefinition3.p1.6.6.m6.1.1.2" xref="Thmdefinition3.p1.6.6.m6.1.1.2.cmml"><mi id="Thmdefinition3.p1.6.6.m6.1.1.2.2" xref="Thmdefinition3.p1.6.6.m6.1.1.2.2.cmml">y</mi><mi id="Thmdefinition3.p1.6.6.m6.1.1.2.3" xref="Thmdefinition3.p1.6.6.m6.1.1.2.3.cmml">j</mi></msub><mo id="Thmdefinition3.p1.6.6.m6.1.1.1" xref="Thmdefinition3.p1.6.6.m6.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="Thmdefinition3.p1.6.6.m6.1.1.3" xref="Thmdefinition3.p1.6.6.m6.1.1.3.cmml">𝒢</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition3.p1.6.6.m6.1b"><apply id="Thmdefinition3.p1.6.6.m6.1.1.cmml" xref="Thmdefinition3.p1.6.6.m6.1.1"><in id="Thmdefinition3.p1.6.6.m6.1.1.1.cmml" xref="Thmdefinition3.p1.6.6.m6.1.1.1"></in><apply id="Thmdefinition3.p1.6.6.m6.1.1.2.cmml" xref="Thmdefinition3.p1.6.6.m6.1.1.2"><csymbol cd="ambiguous" id="Thmdefinition3.p1.6.6.m6.1.1.2.1.cmml" xref="Thmdefinition3.p1.6.6.m6.1.1.2">subscript</csymbol><ci id="Thmdefinition3.p1.6.6.m6.1.1.2.2.cmml" xref="Thmdefinition3.p1.6.6.m6.1.1.2.2">𝑦</ci><ci id="Thmdefinition3.p1.6.6.m6.1.1.2.3.cmml" xref="Thmdefinition3.p1.6.6.m6.1.1.2.3">𝑗</ci></apply><ci id="Thmdefinition3.p1.6.6.m6.1.1.3.cmml" xref="Thmdefinition3.p1.6.6.m6.1.1.3">𝒢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition3.p1.6.6.m6.1c">y_{j}\in\mathcal{G}</annotation></semantics></math> of pallet block <math id="Thmdefinition3.p1.7.7.m7.1" class="ltx_Math" alttext="j" display="inline"><semantics id="Thmdefinition3.p1.7.7.m7.1a"><mi id="Thmdefinition3.p1.7.7.m7.1.1" xref="Thmdefinition3.p1.7.7.m7.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition3.p1.7.7.m7.1b"><ci id="Thmdefinition3.p1.7.7.m7.1.1.cmml" xref="Thmdefinition3.p1.7.7.m7.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition3.p1.7.7.m7.1c">j</annotation></semantics></math> such that <math id="Thmdefinition3.p1.8.8.m8.1" class="ltx_Math" alttext="i=j" display="inline"><semantics id="Thmdefinition3.p1.8.8.m8.1a"><mrow id="Thmdefinition3.p1.8.8.m8.1.1" xref="Thmdefinition3.p1.8.8.m8.1.1.cmml"><mi id="Thmdefinition3.p1.8.8.m8.1.1.2" xref="Thmdefinition3.p1.8.8.m8.1.1.2.cmml">i</mi><mo id="Thmdefinition3.p1.8.8.m8.1.1.1" xref="Thmdefinition3.p1.8.8.m8.1.1.1.cmml">=</mo><mi id="Thmdefinition3.p1.8.8.m8.1.1.3" xref="Thmdefinition3.p1.8.8.m8.1.1.3.cmml">j</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition3.p1.8.8.m8.1b"><apply id="Thmdefinition3.p1.8.8.m8.1.1.cmml" xref="Thmdefinition3.p1.8.8.m8.1.1"><eq id="Thmdefinition3.p1.8.8.m8.1.1.1.cmml" xref="Thmdefinition3.p1.8.8.m8.1.1.1"></eq><ci id="Thmdefinition3.p1.8.8.m8.1.1.2.cmml" xref="Thmdefinition3.p1.8.8.m8.1.1.2">𝑖</ci><ci id="Thmdefinition3.p1.8.8.m8.1.1.3.cmml" xref="Thmdefinition3.p1.8.8.m8.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition3.p1.8.8.m8.1c">i=j</annotation></semantics></math>.</span></p>
</div>
</div>
<div id="S3.SS2.SSSx2.p2" class="ltx_para">
<p id="S3.SS2.SSSx2.p2.1" class="ltx_p">The re-identification pipeline from <cite class="ltx_cite ltx_citemacro_citep">(Rutinowski et al. <a href="#bib.bib14" title="" class="ltx_ref">2021b</a>)</cite> is used to match an original image of a pallet block to a synthetic image of the same pallet block (original C to synthetic RL perspectives and original RL to synthetic RL perspectives).
The accuracy, with which the re-identification could be performed is then compared to the results for the same scenario, that are obtained using only original images, matching the C and RL perspectives.</p>
</div>
<div id="S3.SS2.SSSx2.p3" class="ltx_para">
<p id="S3.SS2.SSSx2.p3.6" class="ltx_p">The dimensions of the generator’s input and output images are set to <math id="S3.SS2.SSSx2.p3.1.m1.1" class="ltx_Math" alttext="1280" display="inline"><semantics id="S3.SS2.SSSx2.p3.1.m1.1a"><mn id="S3.SS2.SSSx2.p3.1.m1.1.1" xref="S3.SS2.SSSx2.p3.1.m1.1.1.cmml">1280</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx2.p3.1.m1.1b"><cn type="integer" id="S3.SS2.SSSx2.p3.1.m1.1.1.cmml" xref="S3.SS2.SSSx2.p3.1.m1.1.1">1280</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx2.p3.1.m1.1c">1280</annotation></semantics></math> × <math id="S3.SS2.SSSx2.p3.2.m2.1" class="ltx_Math" alttext="640" display="inline"><semantics id="S3.SS2.SSSx2.p3.2.m2.1a"><mn id="S3.SS2.SSSx2.p3.2.m2.1.1" xref="S3.SS2.SSSx2.p3.2.m2.1.1.cmml">640</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx2.p3.2.m2.1b"><cn type="integer" id="S3.SS2.SSSx2.p3.2.m2.1.1.cmml" xref="S3.SS2.SSSx2.p3.2.m2.1.1">640</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx2.p3.2.m2.1c">640</annotation></semantics></math> px and therefore a <math id="S3.SS2.SSSx2.p3.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS2.SSSx2.p3.3.m3.1a"><mn id="S3.SS2.SSSx2.p3.3.m3.1.1" xref="S3.SS2.SSSx2.p3.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx2.p3.3.m3.1b"><cn type="integer" id="S3.SS2.SSSx2.p3.3.m3.1.1.cmml" xref="S3.SS2.SSSx2.p3.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx2.p3.3.m3.1c">2</annotation></semantics></math>:<math id="S3.SS2.SSSx2.p3.4.m4.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.SSSx2.p3.4.m4.1a"><mn id="S3.SS2.SSSx2.p3.4.m4.1.1" xref="S3.SS2.SSSx2.p3.4.m4.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx2.p3.4.m4.1b"><cn type="integer" id="S3.SS2.SSSx2.p3.4.m4.1.1.cmml" xref="S3.SS2.SSSx2.p3.4.m4.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx2.p3.4.m4.1c">1</annotation></semantics></math> aspect ratio, while the images from pallet-block-502 have an aspect ratio of approximately <math id="S3.SS2.SSSx2.p3.5.m5.1" class="ltx_Math" alttext="1.7" display="inline"><semantics id="S3.SS2.SSSx2.p3.5.m5.1a"><mn id="S3.SS2.SSSx2.p3.5.m5.1.1" xref="S3.SS2.SSSx2.p3.5.m5.1.1.cmml">1.7</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx2.p3.5.m5.1b"><cn type="float" id="S3.SS2.SSSx2.p3.5.m5.1.1.cmml" xref="S3.SS2.SSSx2.p3.5.m5.1.1">1.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx2.p3.5.m5.1c">1.7</annotation></semantics></math>:<math id="S3.SS2.SSSx2.p3.6.m6.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.SSSx2.p3.6.m6.1a"><mn id="S3.SS2.SSSx2.p3.6.m6.1.1" xref="S3.SS2.SSSx2.p3.6.m6.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSSx2.p3.6.m6.1b"><cn type="integer" id="S3.SS2.SSSx2.p3.6.m6.1.1.cmml" xref="S3.SS2.SSSx2.p3.6.m6.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSSx2.p3.6.m6.1c">1</annotation></semantics></math>, due to the images being cropped automatically by YOLO.
Taking this aspect ratio discrepancy into account, two modes of re-identification result evaluation will be carried out.
The first one will apply the exact approach used in <cite class="ltx_cite ltx_citemacro_citep">(Rutinowski et al. <a href="#bib.bib14" title="" class="ltx_ref">2021b</a>)</cite>.
The second one (subsequently called modified re-identification) will apply projective transformations and Gaussian blur to the training images and will center-crop the generated images to an aspect ratio of 1.7, as to match the aspect ratio of pallet-block-502.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Evaluation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Before displaying and analyzing the results of this contribution, the experimental configuration, in terms of software and hardware usage, will be laid out.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Configuration</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.5" class="ltx_p">For the experiments in this contribution, three different networks were used.
First of all, we used a Tensorflow implementation of CycleGAN<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/LynnHo/CycleGAN-Tensorflow-2</span></span></span>.
The generators and discriminators of this network were trained for <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn type="integer" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">200</annotation></semantics></math> epochs, with nine residual blocks for the generator architectures, using the Adam optimizer with a learning rate of <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="0.0002" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mn id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">0.0002</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><cn type="float" id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">0.0002</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">0.0002</annotation></semantics></math> and the corresponding <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="\beta_{1}" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><msub id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mi id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml">β</mi><mn id="S4.SS1.p1.3.m3.1.1.3" xref="S4.SS1.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2">𝛽</ci><cn type="integer" id="S4.SS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">\beta_{1}</annotation></semantics></math> argument set to 0.5.
These and all unmentioned parameters are the recommended default settings provided by the authors of <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al. <a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite>, taking the selected resolution into account.
Next, we trained a perspective classifier, as described in Section <a href="#S3.SS2" title="3.2 Formulating the Evaluation Tasks ‣ 3 Methodological Approach ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, again using the Tensorflow library.
It employs Adam as an optimizer (with the default learning rate of <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mn id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><cn type="float" id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">0.001</annotation></semantics></math>), sparse categorical cross entropy as a loss function and was trained for <math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><mn id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><cn type="integer" id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">20</annotation></semantics></math> epochs.
The architecture and further hyperparameters of the classifier can be seen in Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Experimental Configuration ‣ 4 Experimental Evaluation ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:258.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(51.0pt,-30.4pt) scale(1.30750402347402,1.30750402347402) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding:1pt 2.5pt;">Layer</th>
<th id="S4.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1pt 2.5pt;">Shape</th>
<th id="S4.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding:1pt 2.5pt;">Hyperparameters</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1pt 2.5pt;">Input</td>
<td id="S4.T1.1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding:1pt 2.5pt;">640 x 1280 x 3</td>
<td id="S4.T1.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding:1pt 2.5pt;">-</td>
</tr>
<tr id="S4.T1.1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.1.3.2.1" class="ltx_td ltx_align_left" style="padding:1pt 2.5pt;">Conv2D</td>
<td id="S4.T1.1.1.3.2.2" class="ltx_td ltx_align_right" style="padding:1pt 2.5pt;">640 x 1280 x 16</td>
<td id="S4.T1.1.1.3.2.3" class="ltx_td ltx_align_left" style="padding:1pt 2.5pt;">Filter size: 16; Stride: 3; Activation: ReLU</td>
</tr>
<tr id="S4.T1.1.1.4.3" class="ltx_tr">
<td id="S4.T1.1.1.4.3.1" class="ltx_td ltx_align_left" style="padding:1pt 2.5pt;">MaxPool2D</td>
<td id="S4.T1.1.1.4.3.2" class="ltx_td ltx_align_right" style="padding:1pt 2.5pt;">320 x 640 x 16</td>
<td id="S4.T1.1.1.4.3.3" class="ltx_td ltx_align_left" style="padding:1pt 2.5pt;">Stride: 2</td>
</tr>
<tr id="S4.T1.1.1.5.4" class="ltx_tr">
<td id="S4.T1.1.1.5.4.1" class="ltx_td ltx_align_left" style="padding:1pt 2.5pt;">Conv2D</td>
<td id="S4.T1.1.1.5.4.2" class="ltx_td ltx_align_right" style="padding:1pt 2.5pt;">320 x 640 x 32</td>
<td id="S4.T1.1.1.5.4.3" class="ltx_td ltx_align_left" style="padding:1pt 2.5pt;">Filter size: 32; Stride: 3; Activation: ReLU</td>
</tr>
<tr id="S4.T1.1.1.6.5" class="ltx_tr">
<td id="S4.T1.1.1.6.5.1" class="ltx_td ltx_align_left" style="padding:1pt 2.5pt;">MaxPool2D</td>
<td id="S4.T1.1.1.6.5.2" class="ltx_td ltx_align_right" style="padding:1pt 2.5pt;">160 x 320 x 32</td>
<td id="S4.T1.1.1.6.5.3" class="ltx_td ltx_align_left" style="padding:1pt 2.5pt;">Stride: 2</td>
</tr>
<tr id="S4.T1.1.1.7.6" class="ltx_tr">
<td id="S4.T1.1.1.7.6.1" class="ltx_td ltx_align_left" style="padding:1pt 2.5pt;">Conv2D</td>
<td id="S4.T1.1.1.7.6.2" class="ltx_td ltx_align_right" style="padding:1pt 2.5pt;">160 x 320 x 64</td>
<td id="S4.T1.1.1.7.6.3" class="ltx_td ltx_align_left" style="padding:1pt 2.5pt;">Filter size: 64; Stride: 3; Activation: ReLU</td>
</tr>
<tr id="S4.T1.1.1.8.7" class="ltx_tr">
<td id="S4.T1.1.1.8.7.1" class="ltx_td ltx_align_left" style="padding:1pt 2.5pt;">MaxPool2D</td>
<td id="S4.T1.1.1.8.7.2" class="ltx_td ltx_align_right" style="padding:1pt 2.5pt;">80 x 160 x 64</td>
<td id="S4.T1.1.1.8.7.3" class="ltx_td ltx_align_left" style="padding:1pt 2.5pt;">Stride: 2</td>
</tr>
<tr id="S4.T1.1.1.9.8" class="ltx_tr">
<td id="S4.T1.1.1.9.8.1" class="ltx_td ltx_align_left" style="padding:1pt 2.5pt;">Flatten</td>
<td id="S4.T1.1.1.9.8.2" class="ltx_td ltx_align_right" style="padding:1pt 2.5pt;">819200</td>
<td id="S4.T1.1.1.9.8.3" class="ltx_td ltx_align_left" style="padding:1pt 2.5pt;">-</td>
</tr>
<tr id="S4.T1.1.1.10.9" class="ltx_tr">
<td id="S4.T1.1.1.10.9.1" class="ltx_td ltx_align_left" style="padding:1pt 2.5pt;">Dense</td>
<td id="S4.T1.1.1.10.9.2" class="ltx_td ltx_align_right" style="padding:1pt 2.5pt;">128</td>
<td id="S4.T1.1.1.10.9.3" class="ltx_td ltx_align_left" style="padding:1pt 2.5pt;">Activation: ReLU</td>
</tr>
<tr id="S4.T1.1.1.11.10" class="ltx_tr">
<td id="S4.T1.1.1.11.10.1" class="ltx_td ltx_align_left ltx_border_b" style="padding:1pt 2.5pt;">Dense</td>
<td id="S4.T1.1.1.11.10.2" class="ltx_td ltx_align_right ltx_border_b" style="padding:1pt 2.5pt;">2</td>
<td id="S4.T1.1.1.11.10.3" class="ltx_td ltx_align_left ltx_border_b" style="padding:1pt 2.5pt;">Activation: Linear</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Architecture of the pallet block perspective classifier.</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Finally our re-identification method, which is described in detail in <cite class="ltx_cite ltx_citemacro_citep">(Rutinowski et al. <a href="#bib.bib14" title="" class="ltx_ref">2021b</a>)</cite>, was applied to the data in its modified and unmodified manner (see Section <a href="#S3.SS2" title="3.2 Formulating the Evaluation Tasks ‣ 3 Methodological Approach ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
For the image perspective classification task, accuracy (defined as the amount of correctly classified images divided by the total number of all images) was the evaluation metric of choice.
Additionally, ranked accuracy (from rank 1 to rank 5) was used for the re-identification task, since unlike the herein described perspective classification, re-identification is not a binary classification task.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">The experiments and evaluations described throughout this work were run on an NVIDIA DGX-2 (equipped with <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mn id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><cn type="integer" id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">16</annotation></semantics></math> NVIDIA Tesla V100 GPUs and a 24-core Intel Xeon Platinum 8168 CPU).
To support the reproducibility of our results,
the code used for this work and the resulting images can be found online<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/jerome-rutinowski/gan_data_for_re-id</span></span></span>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Synthetic Data Generation Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">By using the methods described in the previous Section, <math id="S4.SS2.p1.1.m1.2" class="ltx_Math" alttext="1,004" display="inline"><semantics id="S4.SS2.p1.1.m1.2a"><mrow id="S4.SS2.p1.1.m1.2.3.2" xref="S4.SS2.p1.1.m1.2.3.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">1</mn><mo id="S4.SS2.p1.1.m1.2.3.2.1" xref="S4.SS2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS2.p1.1.m1.2.2" xref="S4.SS2.p1.1.m1.2.2.cmml">004</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.2b"><list id="S4.SS2.p1.1.m1.2.3.1.cmml" xref="S4.SS2.p1.1.m1.2.3.2"><cn type="integer" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">1</cn><cn type="integer" id="S4.SS2.p1.1.m1.2.2.cmml" xref="S4.SS2.p1.1.m1.2.2">004</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.2c">1,004</annotation></semantics></math> images of RL perspective pallet blocks were generated from their C perspective counterpart that they are based on, retaining their chipwood surface structure while only changing the perspective.
Some examples of generated images and their original counterparts can be seen in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Synthetic Data Generation Results ‣ 4 Experimental Evaluation ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<p id="S4.F3.1" class="ltx_p ltx_align_center"><span id="S4.F3.1.1" class="ltx_text"><img src="/html/2212.10105/assets/x3.png" id="S4.F3.1.1.g1" class="ltx_graphics ltx_img_square" width="415" height="352" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Original and corresponding synthetic images generated by applying a left-hand side rotation to a centered input image.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">For original images that had a low degree of luminosity, as can be seen with ID 34 in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Synthetic Data Generation Results ‣ 4 Experimental Evaluation ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the generator was consistently unable to change their perspective, generating only a (visually) very similar copy of the original image.
This was the case for <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="102" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mn id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">102</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><cn type="integer" id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">102</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">102</annotation></semantics></math> of the <math id="S4.SS2.p2.2.m2.2" class="ltx_Math" alttext="1,004" display="inline"><semantics id="S4.SS2.p2.2.m2.2a"><mrow id="S4.SS2.p2.2.m2.2.3.2" xref="S4.SS2.p2.2.m2.2.3.1.cmml"><mn id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">1</mn><mo id="S4.SS2.p2.2.m2.2.3.2.1" xref="S4.SS2.p2.2.m2.2.3.1.cmml">,</mo><mn id="S4.SS2.p2.2.m2.2.2" xref="S4.SS2.p2.2.m2.2.2.cmml">004</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.2b"><list id="S4.SS2.p2.2.m2.2.3.1.cmml" xref="S4.SS2.p2.2.m2.2.3.2"><cn type="integer" id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">1</cn><cn type="integer" id="S4.SS2.p2.2.m2.2.2.cmml" xref="S4.SS2.p2.2.m2.2.2">004</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.2c">1,004</annotation></semantics></math> images that were generated.
The generator likely experienced mode collapse in these instances and the affected images were therefore excluded from further evaluation.
For the remaining majority of the data however, no mode collapse could be observed and the resulting images look visually promising.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results of the Classification Task</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.4" class="ltx_p">Following the data generation process, a classifier was trained to discriminate between RL and C perspective images.
The classifier was then applied to the hold-out dataset of original images as well as a set of generated images of the same size (<math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><cn type="integer" id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">200</annotation></semantics></math> images each).
Even though the resulting images (see Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Synthetic Data Generation Results ‣ 4 Experimental Evaluation ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) seem very promising visually, a discrepancy can still be measured in terms of the perspective classification accuracy.
While the original images are being classified with an accuracy of <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="98\%" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mn id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">98</mn><mo id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">98</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">98\%</annotation></semantics></math>, their synthetic counterparts score an accuracy of <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="92\%" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mrow id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mn id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml">92</mn><mo id="S4.SS3.p1.3.m3.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2">92</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">92\%</annotation></semantics></math>, meaning that the evaluation displayed a <math id="S4.SS3.p1.4.m4.1" class="ltx_Math" alttext="6\%" display="inline"><semantics id="S4.SS3.p1.4.m4.1a"><mrow id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml"><mn id="S4.SS3.p1.4.m4.1.1.2" xref="S4.SS3.p1.4.m4.1.1.2.cmml">6</mn><mo id="S4.SS3.p1.4.m4.1.1.1" xref="S4.SS3.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><apply id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1"><csymbol cd="latexml" id="S4.SS3.p1.4.m4.1.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.4.m4.1.1.2.cmml" xref="S4.SS3.p1.4.m4.1.1.2">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">6\%</annotation></semantics></math> difference in classification accuracy.
This difference could be due to artifacts that might have been generated along with the new perspective of the image.
Additionally, the edges of the pallet blocks in the generated images, in some instances, are sharper and less natural looking than their original counterparts.
For the task of classification, a factor of uncertainty to also take into consideration is the vague definition of the terms “centered” and “rotated” during the creation of the original dataset.
During this process, no clear definition of these terms was given (i.e., in terms of an angle at which a pallet block ought to be facing the camera).
Since only a broad visual notion of the terms “centered” and “rotated” was used, the accuracy with which the dataset setup could be reproduced is reduced (i.e., it would be difficult to replicate the images, without knowing from what specific angle they should be taken) and categorical ambiguity is persistent.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Results of the Re-Identification Task</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.2" class="ltx_p">In addition to the classification task described in the Subsection above, the re-identification method from <cite class="ltx_cite ltx_citemacro_citep">(Rutinowski et al. <a href="#bib.bib14" title="" class="ltx_ref">2021b</a>)</cite> was applied to the original images, as described in Section <a href="#S3.SS2" title="3.2 Formulating the Evaluation Tasks ‣ 3 Methodological Approach ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
As can bee seen in Table <a href="#S4.T2" title="Table 2 ‣ 4.4 Results of the Re-Identification Task ‣ 4 Experimental Evaluation ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, for the re-identification results, a distinct difference is to be noted between the results obtained using the exact methodology of <cite class="ltx_cite ltx_citemacro_citep">(Rutinowski et al. <a href="#bib.bib14" title="" class="ltx_ref">2021b</a>)</cite> and the modified version described in Section <a href="#S3.SS2" title="3.2 Formulating the Evaluation Tasks ‣ 3 Methodological Approach ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> (discrepancies of <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="23" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mn id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">23</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><cn type="integer" id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">23</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">23</annotation></semantics></math>% to <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="58" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><mn id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">58</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><cn type="integer" id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">58</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">58</annotation></semantics></math>%).</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:134.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(71.4pt,-22.1pt) scale(1.49063911585105,1.49063911585105) ;">
<table id="S4.T2.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.3.3.4.1" class="ltx_tr">
<th id="S4.T2.3.3.4.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding:1pt 3.0pt;"></th>
<td id="S4.T2.3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 3.0pt;" colspan="2">Evaluation type</td>
</tr>
<tr id="S4.T2.3.3.5.2" class="ltx_tr">
<th id="S4.T2.3.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1pt 3.0pt;">Metric</th>
<td id="S4.T2.3.3.5.2.2" class="ltx_td ltx_align_right ltx_border_t" style="padding:1pt 3.0pt;">Re-identification [%]</td>
<td id="S4.T2.3.3.5.2.3" class="ltx_td ltx_align_right ltx_border_t" style="padding:1pt 3.0pt;">Re-identification (modified) [%]</td>
</tr>
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:1pt 3.0pt;"><math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="Acc_{C\rightarrow RL}" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.1.cmml">​</mo><mi id="S4.T2.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.1a" xref="S4.T2.1.1.1.1.m1.1.1.1.cmml">​</mo><msub id="S4.T2.1.1.1.1.m1.1.1.4" xref="S4.T2.1.1.1.1.m1.1.1.4.cmml"><mi id="S4.T2.1.1.1.1.m1.1.1.4.2" xref="S4.T2.1.1.1.1.m1.1.1.4.2.cmml">c</mi><mrow id="S4.T2.1.1.1.1.m1.1.1.4.3" xref="S4.T2.1.1.1.1.m1.1.1.4.3.cmml"><mi id="S4.T2.1.1.1.1.m1.1.1.4.3.2" xref="S4.T2.1.1.1.1.m1.1.1.4.3.2.cmml">C</mi><mo stretchy="false" id="S4.T2.1.1.1.1.m1.1.1.4.3.1" xref="S4.T2.1.1.1.1.m1.1.1.4.3.1.cmml">→</mo><mrow id="S4.T2.1.1.1.1.m1.1.1.4.3.3" xref="S4.T2.1.1.1.1.m1.1.1.4.3.3.cmml"><mi id="S4.T2.1.1.1.1.m1.1.1.4.3.3.2" xref="S4.T2.1.1.1.1.m1.1.1.4.3.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.4.3.3.1" xref="S4.T2.1.1.1.1.m1.1.1.4.3.3.1.cmml">​</mo><mi id="S4.T2.1.1.1.1.m1.1.1.4.3.3.3" xref="S4.T2.1.1.1.1.m1.1.1.4.3.3.3.cmml">L</mi></mrow></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"><times id="S4.T2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.1"></times><ci id="S4.T2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2">𝐴</ci><ci id="S4.T2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3">𝑐</ci><apply id="S4.T2.1.1.1.1.m1.1.1.4.cmml" xref="S4.T2.1.1.1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S4.T2.1.1.1.1.m1.1.1.4.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.4">subscript</csymbol><ci id="S4.T2.1.1.1.1.m1.1.1.4.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.4.2">𝑐</ci><apply id="S4.T2.1.1.1.1.m1.1.1.4.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.4.3"><ci id="S4.T2.1.1.1.1.m1.1.1.4.3.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.4.3.1">→</ci><ci id="S4.T2.1.1.1.1.m1.1.1.4.3.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.4.3.2">𝐶</ci><apply id="S4.T2.1.1.1.1.m1.1.1.4.3.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.4.3.3"><times id="S4.T2.1.1.1.1.m1.1.1.4.3.3.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.4.3.3.1"></times><ci id="S4.T2.1.1.1.1.m1.1.1.4.3.3.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.4.3.3.2">𝑅</ci><ci id="S4.T2.1.1.1.1.m1.1.1.4.3.3.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.4.3.3.3">𝐿</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">Acc_{C\rightarrow RL}</annotation></semantics></math></th>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding:1pt 3.0pt;">73</td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding:1pt 3.0pt;">96</td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:1pt 3.0pt;"><math id="S4.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="Acc_{C\rightarrow\widehat{RL}}" display="inline"><semantics id="S4.T2.2.2.2.1.m1.1a"><mrow id="S4.T2.2.2.2.1.m1.1.1" xref="S4.T2.2.2.2.1.m1.1.1.cmml"><mi id="S4.T2.2.2.2.1.m1.1.1.2" xref="S4.T2.2.2.2.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.2.1.m1.1.1.1" xref="S4.T2.2.2.2.1.m1.1.1.1.cmml">​</mo><mi id="S4.T2.2.2.2.1.m1.1.1.3" xref="S4.T2.2.2.2.1.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.2.1.m1.1.1.1a" xref="S4.T2.2.2.2.1.m1.1.1.1.cmml">​</mo><msub id="S4.T2.2.2.2.1.m1.1.1.4" xref="S4.T2.2.2.2.1.m1.1.1.4.cmml"><mi id="S4.T2.2.2.2.1.m1.1.1.4.2" xref="S4.T2.2.2.2.1.m1.1.1.4.2.cmml">c</mi><mrow id="S4.T2.2.2.2.1.m1.1.1.4.3" xref="S4.T2.2.2.2.1.m1.1.1.4.3.cmml"><mi id="S4.T2.2.2.2.1.m1.1.1.4.3.2" xref="S4.T2.2.2.2.1.m1.1.1.4.3.2.cmml">C</mi><mo stretchy="false" id="S4.T2.2.2.2.1.m1.1.1.4.3.1" xref="S4.T2.2.2.2.1.m1.1.1.4.3.1.cmml">→</mo><mover accent="true" id="S4.T2.2.2.2.1.m1.1.1.4.3.3" xref="S4.T2.2.2.2.1.m1.1.1.4.3.3.cmml"><mrow id="S4.T2.2.2.2.1.m1.1.1.4.3.3.2" xref="S4.T2.2.2.2.1.m1.1.1.4.3.3.2.cmml"><mi id="S4.T2.2.2.2.1.m1.1.1.4.3.3.2.2" xref="S4.T2.2.2.2.1.m1.1.1.4.3.3.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.2.1.m1.1.1.4.3.3.2.1" xref="S4.T2.2.2.2.1.m1.1.1.4.3.3.2.1.cmml">​</mo><mi id="S4.T2.2.2.2.1.m1.1.1.4.3.3.2.3" xref="S4.T2.2.2.2.1.m1.1.1.4.3.3.2.3.cmml">L</mi></mrow><mo id="S4.T2.2.2.2.1.m1.1.1.4.3.3.1" xref="S4.T2.2.2.2.1.m1.1.1.4.3.3.1.cmml">^</mo></mover></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.m1.1b"><apply id="S4.T2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1"><times id="S4.T2.2.2.2.1.m1.1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1.1"></times><ci id="S4.T2.2.2.2.1.m1.1.1.2.cmml" xref="S4.T2.2.2.2.1.m1.1.1.2">𝐴</ci><ci id="S4.T2.2.2.2.1.m1.1.1.3.cmml" xref="S4.T2.2.2.2.1.m1.1.1.3">𝑐</ci><apply id="S4.T2.2.2.2.1.m1.1.1.4.cmml" xref="S4.T2.2.2.2.1.m1.1.1.4"><csymbol cd="ambiguous" id="S4.T2.2.2.2.1.m1.1.1.4.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1.4">subscript</csymbol><ci id="S4.T2.2.2.2.1.m1.1.1.4.2.cmml" xref="S4.T2.2.2.2.1.m1.1.1.4.2">𝑐</ci><apply id="S4.T2.2.2.2.1.m1.1.1.4.3.cmml" xref="S4.T2.2.2.2.1.m1.1.1.4.3"><ci id="S4.T2.2.2.2.1.m1.1.1.4.3.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1.4.3.1">→</ci><ci id="S4.T2.2.2.2.1.m1.1.1.4.3.2.cmml" xref="S4.T2.2.2.2.1.m1.1.1.4.3.2">𝐶</ci><apply id="S4.T2.2.2.2.1.m1.1.1.4.3.3.cmml" xref="S4.T2.2.2.2.1.m1.1.1.4.3.3"><ci id="S4.T2.2.2.2.1.m1.1.1.4.3.3.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1.4.3.3.1">^</ci><apply id="S4.T2.2.2.2.1.m1.1.1.4.3.3.2.cmml" xref="S4.T2.2.2.2.1.m1.1.1.4.3.3.2"><times id="S4.T2.2.2.2.1.m1.1.1.4.3.3.2.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1.4.3.3.2.1"></times><ci id="S4.T2.2.2.2.1.m1.1.1.4.3.3.2.2.cmml" xref="S4.T2.2.2.2.1.m1.1.1.4.3.3.2.2">𝑅</ci><ci id="S4.T2.2.2.2.1.m1.1.1.4.3.3.2.3.cmml" xref="S4.T2.2.2.2.1.m1.1.1.4.3.3.2.3">𝐿</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.m1.1c">Acc_{C\rightarrow\widehat{RL}}</annotation></semantics></math></th>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_right" style="padding:1pt 3.0pt;">33</td>
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_right" style="padding:1pt 3.0pt;">88</td>
</tr>
<tr id="S4.T2.3.3.3" class="ltx_tr">
<th id="S4.T2.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-bottom:2.15277pt;padding:1pt 3.0pt;"><math id="S4.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="Acc_{RL\rightarrow\widehat{RL}}" display="inline"><semantics id="S4.T2.3.3.3.1.m1.1a"><mrow id="S4.T2.3.3.3.1.m1.1.1" xref="S4.T2.3.3.3.1.m1.1.1.cmml"><mi id="S4.T2.3.3.3.1.m1.1.1.2" xref="S4.T2.3.3.3.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.T2.3.3.3.1.m1.1.1.1" xref="S4.T2.3.3.3.1.m1.1.1.1.cmml">​</mo><mi id="S4.T2.3.3.3.1.m1.1.1.3" xref="S4.T2.3.3.3.1.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.T2.3.3.3.1.m1.1.1.1a" xref="S4.T2.3.3.3.1.m1.1.1.1.cmml">​</mo><msub id="S4.T2.3.3.3.1.m1.1.1.4" xref="S4.T2.3.3.3.1.m1.1.1.4.cmml"><mi id="S4.T2.3.3.3.1.m1.1.1.4.2" xref="S4.T2.3.3.3.1.m1.1.1.4.2.cmml">c</mi><mrow id="S4.T2.3.3.3.1.m1.1.1.4.3" xref="S4.T2.3.3.3.1.m1.1.1.4.3.cmml"><mrow id="S4.T2.3.3.3.1.m1.1.1.4.3.2" xref="S4.T2.3.3.3.1.m1.1.1.4.3.2.cmml"><mi id="S4.T2.3.3.3.1.m1.1.1.4.3.2.2" xref="S4.T2.3.3.3.1.m1.1.1.4.3.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.T2.3.3.3.1.m1.1.1.4.3.2.1" xref="S4.T2.3.3.3.1.m1.1.1.4.3.2.1.cmml">​</mo><mi id="S4.T2.3.3.3.1.m1.1.1.4.3.2.3" xref="S4.T2.3.3.3.1.m1.1.1.4.3.2.3.cmml">L</mi></mrow><mo stretchy="false" id="S4.T2.3.3.3.1.m1.1.1.4.3.1" xref="S4.T2.3.3.3.1.m1.1.1.4.3.1.cmml">→</mo><mover accent="true" id="S4.T2.3.3.3.1.m1.1.1.4.3.3" xref="S4.T2.3.3.3.1.m1.1.1.4.3.3.cmml"><mrow id="S4.T2.3.3.3.1.m1.1.1.4.3.3.2" xref="S4.T2.3.3.3.1.m1.1.1.4.3.3.2.cmml"><mi id="S4.T2.3.3.3.1.m1.1.1.4.3.3.2.2" xref="S4.T2.3.3.3.1.m1.1.1.4.3.3.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.T2.3.3.3.1.m1.1.1.4.3.3.2.1" xref="S4.T2.3.3.3.1.m1.1.1.4.3.3.2.1.cmml">​</mo><mi id="S4.T2.3.3.3.1.m1.1.1.4.3.3.2.3" xref="S4.T2.3.3.3.1.m1.1.1.4.3.3.2.3.cmml">L</mi></mrow><mo id="S4.T2.3.3.3.1.m1.1.1.4.3.3.1" xref="S4.T2.3.3.3.1.m1.1.1.4.3.3.1.cmml">^</mo></mover></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.1.m1.1b"><apply id="S4.T2.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1"><times id="S4.T2.3.3.3.1.m1.1.1.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1.1"></times><ci id="S4.T2.3.3.3.1.m1.1.1.2.cmml" xref="S4.T2.3.3.3.1.m1.1.1.2">𝐴</ci><ci id="S4.T2.3.3.3.1.m1.1.1.3.cmml" xref="S4.T2.3.3.3.1.m1.1.1.3">𝑐</ci><apply id="S4.T2.3.3.3.1.m1.1.1.4.cmml" xref="S4.T2.3.3.3.1.m1.1.1.4"><csymbol cd="ambiguous" id="S4.T2.3.3.3.1.m1.1.1.4.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1.4">subscript</csymbol><ci id="S4.T2.3.3.3.1.m1.1.1.4.2.cmml" xref="S4.T2.3.3.3.1.m1.1.1.4.2">𝑐</ci><apply id="S4.T2.3.3.3.1.m1.1.1.4.3.cmml" xref="S4.T2.3.3.3.1.m1.1.1.4.3"><ci id="S4.T2.3.3.3.1.m1.1.1.4.3.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1.4.3.1">→</ci><apply id="S4.T2.3.3.3.1.m1.1.1.4.3.2.cmml" xref="S4.T2.3.3.3.1.m1.1.1.4.3.2"><times id="S4.T2.3.3.3.1.m1.1.1.4.3.2.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1.4.3.2.1"></times><ci id="S4.T2.3.3.3.1.m1.1.1.4.3.2.2.cmml" xref="S4.T2.3.3.3.1.m1.1.1.4.3.2.2">𝑅</ci><ci id="S4.T2.3.3.3.1.m1.1.1.4.3.2.3.cmml" xref="S4.T2.3.3.3.1.m1.1.1.4.3.2.3">𝐿</ci></apply><apply id="S4.T2.3.3.3.1.m1.1.1.4.3.3.cmml" xref="S4.T2.3.3.3.1.m1.1.1.4.3.3"><ci id="S4.T2.3.3.3.1.m1.1.1.4.3.3.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1.4.3.3.1">^</ci><apply id="S4.T2.3.3.3.1.m1.1.1.4.3.3.2.cmml" xref="S4.T2.3.3.3.1.m1.1.1.4.3.3.2"><times id="S4.T2.3.3.3.1.m1.1.1.4.3.3.2.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1.4.3.3.2.1"></times><ci id="S4.T2.3.3.3.1.m1.1.1.4.3.3.2.2.cmml" xref="S4.T2.3.3.3.1.m1.1.1.4.3.3.2.2">𝑅</ci><ci id="S4.T2.3.3.3.1.m1.1.1.4.3.3.2.3.cmml" xref="S4.T2.3.3.3.1.m1.1.1.4.3.3.2.3">𝐿</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.1.m1.1c">Acc_{RL\rightarrow\widehat{RL}}</annotation></semantics></math></th>
<td id="S4.T2.3.3.3.2" class="ltx_td ltx_align_right ltx_border_b" style="padding-bottom:2.15277pt;padding:1pt 3.0pt;">20</td>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_right ltx_border_b" style="padding-bottom:2.15277pt;padding:1pt 3.0pt;">78</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Re-identification rank-1-accuracy (<math id="S4.T2.5.m1.1" class="ltx_Math" alttext="Acc" display="inline"><semantics id="S4.T2.5.m1.1b"><mrow id="S4.T2.5.m1.1.1" xref="S4.T2.5.m1.1.1.cmml"><mi id="S4.T2.5.m1.1.1.2" xref="S4.T2.5.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.T2.5.m1.1.1.1" xref="S4.T2.5.m1.1.1.1.cmml">​</mo><mi id="S4.T2.5.m1.1.1.3" xref="S4.T2.5.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.T2.5.m1.1.1.1b" xref="S4.T2.5.m1.1.1.1.cmml">​</mo><mi id="S4.T2.5.m1.1.1.4" xref="S4.T2.5.m1.1.1.4.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.5.m1.1c"><apply id="S4.T2.5.m1.1.1.cmml" xref="S4.T2.5.m1.1.1"><times id="S4.T2.5.m1.1.1.1.cmml" xref="S4.T2.5.m1.1.1.1"></times><ci id="S4.T2.5.m1.1.1.2.cmml" xref="S4.T2.5.m1.1.1.2">𝐴</ci><ci id="S4.T2.5.m1.1.1.3.cmml" xref="S4.T2.5.m1.1.1.3">𝑐</ci><ci id="S4.T2.5.m1.1.1.4.cmml" xref="S4.T2.5.m1.1.1.4">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.m1.1d">Acc</annotation></semantics></math>) on synthetic and original images.</figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.4" class="ltx_p">In this case, the results suggest that using similar aspect ratios is paramount to a successful re-identification.
In addition, we assume that the use of Gaussian blur is advantageous in this case as well, blurring pixel level patterns, that would seem irrelevant to humans but might be mistakenly treated as a relevant feature by the re-identification algorithm.
However, when observing the results that were obtained using these modifications of compatibility, the discrepancy between the re-identification accuracy obtained using original images and synthetic images ranged between <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><mn id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><cn type="integer" id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">8</annotation></semantics></math>% and <math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="18" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><mn id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><cn type="integer" id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">18</annotation></semantics></math>%.
In the former case, this represents a result that is very similar to the classification discrepancy that was described in Section <a href="#S4.SS3" title="4.3 Results of the Classification Task ‣ 4 Experimental Evaluation ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.
Of particular interest however, is the lower accuracy for <math id="S4.SS4.p2.3.m3.1" class="ltx_Math" alttext="Acc_{RL\rightarrow\widehat{RL}}" display="inline"><semantics id="S4.SS4.p2.3.m3.1a"><mrow id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml"><mi id="S4.SS4.p2.3.m3.1.1.2" xref="S4.SS4.p2.3.m3.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p2.3.m3.1.1.1" xref="S4.SS4.p2.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS4.p2.3.m3.1.1.3" xref="S4.SS4.p2.3.m3.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p2.3.m3.1.1.1a" xref="S4.SS4.p2.3.m3.1.1.1.cmml">​</mo><msub id="S4.SS4.p2.3.m3.1.1.4" xref="S4.SS4.p2.3.m3.1.1.4.cmml"><mi id="S4.SS4.p2.3.m3.1.1.4.2" xref="S4.SS4.p2.3.m3.1.1.4.2.cmml">c</mi><mrow id="S4.SS4.p2.3.m3.1.1.4.3" xref="S4.SS4.p2.3.m3.1.1.4.3.cmml"><mrow id="S4.SS4.p2.3.m3.1.1.4.3.2" xref="S4.SS4.p2.3.m3.1.1.4.3.2.cmml"><mi id="S4.SS4.p2.3.m3.1.1.4.3.2.2" xref="S4.SS4.p2.3.m3.1.1.4.3.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p2.3.m3.1.1.4.3.2.1" xref="S4.SS4.p2.3.m3.1.1.4.3.2.1.cmml">​</mo><mi id="S4.SS4.p2.3.m3.1.1.4.3.2.3" xref="S4.SS4.p2.3.m3.1.1.4.3.2.3.cmml">L</mi></mrow><mo stretchy="false" id="S4.SS4.p2.3.m3.1.1.4.3.1" xref="S4.SS4.p2.3.m3.1.1.4.3.1.cmml">→</mo><mover accent="true" id="S4.SS4.p2.3.m3.1.1.4.3.3" xref="S4.SS4.p2.3.m3.1.1.4.3.3.cmml"><mrow id="S4.SS4.p2.3.m3.1.1.4.3.3.2" xref="S4.SS4.p2.3.m3.1.1.4.3.3.2.cmml"><mi id="S4.SS4.p2.3.m3.1.1.4.3.3.2.2" xref="S4.SS4.p2.3.m3.1.1.4.3.3.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p2.3.m3.1.1.4.3.3.2.1" xref="S4.SS4.p2.3.m3.1.1.4.3.3.2.1.cmml">​</mo><mi id="S4.SS4.p2.3.m3.1.1.4.3.3.2.3" xref="S4.SS4.p2.3.m3.1.1.4.3.3.2.3.cmml">L</mi></mrow><mo id="S4.SS4.p2.3.m3.1.1.4.3.3.1" xref="S4.SS4.p2.3.m3.1.1.4.3.3.1.cmml">^</mo></mover></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><apply id="S4.SS4.p2.3.m3.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1"><times id="S4.SS4.p2.3.m3.1.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1.1"></times><ci id="S4.SS4.p2.3.m3.1.1.2.cmml" xref="S4.SS4.p2.3.m3.1.1.2">𝐴</ci><ci id="S4.SS4.p2.3.m3.1.1.3.cmml" xref="S4.SS4.p2.3.m3.1.1.3">𝑐</ci><apply id="S4.SS4.p2.3.m3.1.1.4.cmml" xref="S4.SS4.p2.3.m3.1.1.4"><csymbol cd="ambiguous" id="S4.SS4.p2.3.m3.1.1.4.1.cmml" xref="S4.SS4.p2.3.m3.1.1.4">subscript</csymbol><ci id="S4.SS4.p2.3.m3.1.1.4.2.cmml" xref="S4.SS4.p2.3.m3.1.1.4.2">𝑐</ci><apply id="S4.SS4.p2.3.m3.1.1.4.3.cmml" xref="S4.SS4.p2.3.m3.1.1.4.3"><ci id="S4.SS4.p2.3.m3.1.1.4.3.1.cmml" xref="S4.SS4.p2.3.m3.1.1.4.3.1">→</ci><apply id="S4.SS4.p2.3.m3.1.1.4.3.2.cmml" xref="S4.SS4.p2.3.m3.1.1.4.3.2"><times id="S4.SS4.p2.3.m3.1.1.4.3.2.1.cmml" xref="S4.SS4.p2.3.m3.1.1.4.3.2.1"></times><ci id="S4.SS4.p2.3.m3.1.1.4.3.2.2.cmml" xref="S4.SS4.p2.3.m3.1.1.4.3.2.2">𝑅</ci><ci id="S4.SS4.p2.3.m3.1.1.4.3.2.3.cmml" xref="S4.SS4.p2.3.m3.1.1.4.3.2.3">𝐿</ci></apply><apply id="S4.SS4.p2.3.m3.1.1.4.3.3.cmml" xref="S4.SS4.p2.3.m3.1.1.4.3.3"><ci id="S4.SS4.p2.3.m3.1.1.4.3.3.1.cmml" xref="S4.SS4.p2.3.m3.1.1.4.3.3.1">^</ci><apply id="S4.SS4.p2.3.m3.1.1.4.3.3.2.cmml" xref="S4.SS4.p2.3.m3.1.1.4.3.3.2"><times id="S4.SS4.p2.3.m3.1.1.4.3.3.2.1.cmml" xref="S4.SS4.p2.3.m3.1.1.4.3.3.2.1"></times><ci id="S4.SS4.p2.3.m3.1.1.4.3.3.2.2.cmml" xref="S4.SS4.p2.3.m3.1.1.4.3.3.2.2">𝑅</ci><ci id="S4.SS4.p2.3.m3.1.1.4.3.3.2.3.cmml" xref="S4.SS4.p2.3.m3.1.1.4.3.3.2.3">𝐿</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">Acc_{RL\rightarrow\widehat{RL}}</annotation></semantics></math> compared to <math id="S4.SS4.p2.4.m4.1" class="ltx_Math" alttext="Acc_{C\rightarrow\widehat{RL}}" display="inline"><semantics id="S4.SS4.p2.4.m4.1a"><mrow id="S4.SS4.p2.4.m4.1.1" xref="S4.SS4.p2.4.m4.1.1.cmml"><mi id="S4.SS4.p2.4.m4.1.1.2" xref="S4.SS4.p2.4.m4.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p2.4.m4.1.1.1" xref="S4.SS4.p2.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS4.p2.4.m4.1.1.3" xref="S4.SS4.p2.4.m4.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p2.4.m4.1.1.1a" xref="S4.SS4.p2.4.m4.1.1.1.cmml">​</mo><msub id="S4.SS4.p2.4.m4.1.1.4" xref="S4.SS4.p2.4.m4.1.1.4.cmml"><mi id="S4.SS4.p2.4.m4.1.1.4.2" xref="S4.SS4.p2.4.m4.1.1.4.2.cmml">c</mi><mrow id="S4.SS4.p2.4.m4.1.1.4.3" xref="S4.SS4.p2.4.m4.1.1.4.3.cmml"><mi id="S4.SS4.p2.4.m4.1.1.4.3.2" xref="S4.SS4.p2.4.m4.1.1.4.3.2.cmml">C</mi><mo stretchy="false" id="S4.SS4.p2.4.m4.1.1.4.3.1" xref="S4.SS4.p2.4.m4.1.1.4.3.1.cmml">→</mo><mover accent="true" id="S4.SS4.p2.4.m4.1.1.4.3.3" xref="S4.SS4.p2.4.m4.1.1.4.3.3.cmml"><mrow id="S4.SS4.p2.4.m4.1.1.4.3.3.2" xref="S4.SS4.p2.4.m4.1.1.4.3.3.2.cmml"><mi id="S4.SS4.p2.4.m4.1.1.4.3.3.2.2" xref="S4.SS4.p2.4.m4.1.1.4.3.3.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p2.4.m4.1.1.4.3.3.2.1" xref="S4.SS4.p2.4.m4.1.1.4.3.3.2.1.cmml">​</mo><mi id="S4.SS4.p2.4.m4.1.1.4.3.3.2.3" xref="S4.SS4.p2.4.m4.1.1.4.3.3.2.3.cmml">L</mi></mrow><mo id="S4.SS4.p2.4.m4.1.1.4.3.3.1" xref="S4.SS4.p2.4.m4.1.1.4.3.3.1.cmml">^</mo></mover></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.4.m4.1b"><apply id="S4.SS4.p2.4.m4.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1"><times id="S4.SS4.p2.4.m4.1.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1.1"></times><ci id="S4.SS4.p2.4.m4.1.1.2.cmml" xref="S4.SS4.p2.4.m4.1.1.2">𝐴</ci><ci id="S4.SS4.p2.4.m4.1.1.3.cmml" xref="S4.SS4.p2.4.m4.1.1.3">𝑐</ci><apply id="S4.SS4.p2.4.m4.1.1.4.cmml" xref="S4.SS4.p2.4.m4.1.1.4"><csymbol cd="ambiguous" id="S4.SS4.p2.4.m4.1.1.4.1.cmml" xref="S4.SS4.p2.4.m4.1.1.4">subscript</csymbol><ci id="S4.SS4.p2.4.m4.1.1.4.2.cmml" xref="S4.SS4.p2.4.m4.1.1.4.2">𝑐</ci><apply id="S4.SS4.p2.4.m4.1.1.4.3.cmml" xref="S4.SS4.p2.4.m4.1.1.4.3"><ci id="S4.SS4.p2.4.m4.1.1.4.3.1.cmml" xref="S4.SS4.p2.4.m4.1.1.4.3.1">→</ci><ci id="S4.SS4.p2.4.m4.1.1.4.3.2.cmml" xref="S4.SS4.p2.4.m4.1.1.4.3.2">𝐶</ci><apply id="S4.SS4.p2.4.m4.1.1.4.3.3.cmml" xref="S4.SS4.p2.4.m4.1.1.4.3.3"><ci id="S4.SS4.p2.4.m4.1.1.4.3.3.1.cmml" xref="S4.SS4.p2.4.m4.1.1.4.3.3.1">^</ci><apply id="S4.SS4.p2.4.m4.1.1.4.3.3.2.cmml" xref="S4.SS4.p2.4.m4.1.1.4.3.3.2"><times id="S4.SS4.p2.4.m4.1.1.4.3.3.2.1.cmml" xref="S4.SS4.p2.4.m4.1.1.4.3.3.2.1"></times><ci id="S4.SS4.p2.4.m4.1.1.4.3.3.2.2.cmml" xref="S4.SS4.p2.4.m4.1.1.4.3.3.2.2">𝑅</ci><ci id="S4.SS4.p2.4.m4.1.1.4.3.3.2.3.cmml" xref="S4.SS4.p2.4.m4.1.1.4.3.3.2.3">𝐿</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.4.m4.1c">Acc_{C\rightarrow\widehat{RL}}</annotation></semantics></math>.
This could potentially mean that the synthetically generated RL perspective images still have a greater degree of similarity to the C perspective images that they are based on, than the RL perspective images that they are trying to replicate.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<p id="S4.F4.1" class="ltx_p ltx_align_center"><span id="S4.F4.1.1" class="ltx_text"><img src="/html/2212.10105/assets/x4.png" id="S4.F4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="353" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Ranked accuracy of the re-identification evaluation plotted as a CMC curve.</figcaption>
</figure>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.3" class="ltx_p">Finally, the re-identification results are shown in greater detail in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.4 Results of the Re-Identification Task ‣ 4 Experimental Evaluation ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, in the form of a CMC (Cumulative Match Characteristic) curve.
Taking more than the first rank accuracy into account, it can be seen that for all evaluation tasks using the modified re-identification method, results upwards of <math id="S4.SS4.p3.1.m1.1" class="ltx_Math" alttext="90\%" display="inline"><semantics id="S4.SS4.p3.1.m1.1a"><mrow id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml"><mn id="S4.SS4.p3.1.m1.1.1.2" xref="S4.SS4.p3.1.m1.1.1.2.cmml">90</mn><mo id="S4.SS4.p3.1.m1.1.1.1" xref="S4.SS4.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><apply id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS4.p3.1.m1.1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS4.p3.1.m1.1.1.2.cmml" xref="S4.SS4.p3.1.m1.1.1.2">90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">90\%</annotation></semantics></math> are consistently achieved.
For the un-modified re-identification method, an improvement in accuracy can be observed as well, as to be expected, with an increasing number of ranks.
However, until rank 5, a considerable difference in accuracy remains between the modified and the un-modified re-identification method.
This difference is pronounced enough, that <math id="S4.SS4.p3.2.m2.1" class="ltx_Math" alttext="Acc_{C\rightarrow RL}" display="inline"><semantics id="S4.SS4.p3.2.m2.1a"><mrow id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml"><mi id="S4.SS4.p3.2.m2.1.1.2" xref="S4.SS4.p3.2.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p3.2.m2.1.1.1" xref="S4.SS4.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS4.p3.2.m2.1.1.3" xref="S4.SS4.p3.2.m2.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p3.2.m2.1.1.1a" xref="S4.SS4.p3.2.m2.1.1.1.cmml">​</mo><msub id="S4.SS4.p3.2.m2.1.1.4" xref="S4.SS4.p3.2.m2.1.1.4.cmml"><mi id="S4.SS4.p3.2.m2.1.1.4.2" xref="S4.SS4.p3.2.m2.1.1.4.2.cmml">c</mi><mrow id="S4.SS4.p3.2.m2.1.1.4.3" xref="S4.SS4.p3.2.m2.1.1.4.3.cmml"><mi id="S4.SS4.p3.2.m2.1.1.4.3.2" xref="S4.SS4.p3.2.m2.1.1.4.3.2.cmml">C</mi><mo stretchy="false" id="S4.SS4.p3.2.m2.1.1.4.3.1" xref="S4.SS4.p3.2.m2.1.1.4.3.1.cmml">→</mo><mrow id="S4.SS4.p3.2.m2.1.1.4.3.3" xref="S4.SS4.p3.2.m2.1.1.4.3.3.cmml"><mi id="S4.SS4.p3.2.m2.1.1.4.3.3.2" xref="S4.SS4.p3.2.m2.1.1.4.3.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p3.2.m2.1.1.4.3.3.1" xref="S4.SS4.p3.2.m2.1.1.4.3.3.1.cmml">​</mo><mi id="S4.SS4.p3.2.m2.1.1.4.3.3.3" xref="S4.SS4.p3.2.m2.1.1.4.3.3.3.cmml">L</mi></mrow></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><apply id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"><times id="S4.SS4.p3.2.m2.1.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1.1"></times><ci id="S4.SS4.p3.2.m2.1.1.2.cmml" xref="S4.SS4.p3.2.m2.1.1.2">𝐴</ci><ci id="S4.SS4.p3.2.m2.1.1.3.cmml" xref="S4.SS4.p3.2.m2.1.1.3">𝑐</ci><apply id="S4.SS4.p3.2.m2.1.1.4.cmml" xref="S4.SS4.p3.2.m2.1.1.4"><csymbol cd="ambiguous" id="S4.SS4.p3.2.m2.1.1.4.1.cmml" xref="S4.SS4.p3.2.m2.1.1.4">subscript</csymbol><ci id="S4.SS4.p3.2.m2.1.1.4.2.cmml" xref="S4.SS4.p3.2.m2.1.1.4.2">𝑐</ci><apply id="S4.SS4.p3.2.m2.1.1.4.3.cmml" xref="S4.SS4.p3.2.m2.1.1.4.3"><ci id="S4.SS4.p3.2.m2.1.1.4.3.1.cmml" xref="S4.SS4.p3.2.m2.1.1.4.3.1">→</ci><ci id="S4.SS4.p3.2.m2.1.1.4.3.2.cmml" xref="S4.SS4.p3.2.m2.1.1.4.3.2">𝐶</ci><apply id="S4.SS4.p3.2.m2.1.1.4.3.3.cmml" xref="S4.SS4.p3.2.m2.1.1.4.3.3"><times id="S4.SS4.p3.2.m2.1.1.4.3.3.1.cmml" xref="S4.SS4.p3.2.m2.1.1.4.3.3.1"></times><ci id="S4.SS4.p3.2.m2.1.1.4.3.3.2.cmml" xref="S4.SS4.p3.2.m2.1.1.4.3.3.2">𝑅</ci><ci id="S4.SS4.p3.2.m2.1.1.4.3.3.3.cmml" xref="S4.SS4.p3.2.m2.1.1.4.3.3.3">𝐿</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">Acc_{C\rightarrow RL}</annotation></semantics></math>, which is performed only on original images, is consistently lower than <math id="S4.SS4.p3.3.m3.1" class="ltx_Math" alttext="Acc_{C\rightarrow\widehat{RL}}" display="inline"><semantics id="S4.SS4.p3.3.m3.1a"><mrow id="S4.SS4.p3.3.m3.1.1" xref="S4.SS4.p3.3.m3.1.1.cmml"><mi id="S4.SS4.p3.3.m3.1.1.2" xref="S4.SS4.p3.3.m3.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p3.3.m3.1.1.1" xref="S4.SS4.p3.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS4.p3.3.m3.1.1.3" xref="S4.SS4.p3.3.m3.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p3.3.m3.1.1.1a" xref="S4.SS4.p3.3.m3.1.1.1.cmml">​</mo><msub id="S4.SS4.p3.3.m3.1.1.4" xref="S4.SS4.p3.3.m3.1.1.4.cmml"><mi id="S4.SS4.p3.3.m3.1.1.4.2" xref="S4.SS4.p3.3.m3.1.1.4.2.cmml">c</mi><mrow id="S4.SS4.p3.3.m3.1.1.4.3" xref="S4.SS4.p3.3.m3.1.1.4.3.cmml"><mi id="S4.SS4.p3.3.m3.1.1.4.3.2" xref="S4.SS4.p3.3.m3.1.1.4.3.2.cmml">C</mi><mo stretchy="false" id="S4.SS4.p3.3.m3.1.1.4.3.1" xref="S4.SS4.p3.3.m3.1.1.4.3.1.cmml">→</mo><mover accent="true" id="S4.SS4.p3.3.m3.1.1.4.3.3" xref="S4.SS4.p3.3.m3.1.1.4.3.3.cmml"><mrow id="S4.SS4.p3.3.m3.1.1.4.3.3.2" xref="S4.SS4.p3.3.m3.1.1.4.3.3.2.cmml"><mi id="S4.SS4.p3.3.m3.1.1.4.3.3.2.2" xref="S4.SS4.p3.3.m3.1.1.4.3.3.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p3.3.m3.1.1.4.3.3.2.1" xref="S4.SS4.p3.3.m3.1.1.4.3.3.2.1.cmml">​</mo><mi id="S4.SS4.p3.3.m3.1.1.4.3.3.2.3" xref="S4.SS4.p3.3.m3.1.1.4.3.3.2.3.cmml">L</mi></mrow><mo id="S4.SS4.p3.3.m3.1.1.4.3.3.1" xref="S4.SS4.p3.3.m3.1.1.4.3.3.1.cmml">^</mo></mover></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.m3.1b"><apply id="S4.SS4.p3.3.m3.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1"><times id="S4.SS4.p3.3.m3.1.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1.1"></times><ci id="S4.SS4.p3.3.m3.1.1.2.cmml" xref="S4.SS4.p3.3.m3.1.1.2">𝐴</ci><ci id="S4.SS4.p3.3.m3.1.1.3.cmml" xref="S4.SS4.p3.3.m3.1.1.3">𝑐</ci><apply id="S4.SS4.p3.3.m3.1.1.4.cmml" xref="S4.SS4.p3.3.m3.1.1.4"><csymbol cd="ambiguous" id="S4.SS4.p3.3.m3.1.1.4.1.cmml" xref="S4.SS4.p3.3.m3.1.1.4">subscript</csymbol><ci id="S4.SS4.p3.3.m3.1.1.4.2.cmml" xref="S4.SS4.p3.3.m3.1.1.4.2">𝑐</ci><apply id="S4.SS4.p3.3.m3.1.1.4.3.cmml" xref="S4.SS4.p3.3.m3.1.1.4.3"><ci id="S4.SS4.p3.3.m3.1.1.4.3.1.cmml" xref="S4.SS4.p3.3.m3.1.1.4.3.1">→</ci><ci id="S4.SS4.p3.3.m3.1.1.4.3.2.cmml" xref="S4.SS4.p3.3.m3.1.1.4.3.2">𝐶</ci><apply id="S4.SS4.p3.3.m3.1.1.4.3.3.cmml" xref="S4.SS4.p3.3.m3.1.1.4.3.3"><ci id="S4.SS4.p3.3.m3.1.1.4.3.3.1.cmml" xref="S4.SS4.p3.3.m3.1.1.4.3.3.1">^</ci><apply id="S4.SS4.p3.3.m3.1.1.4.3.3.2.cmml" xref="S4.SS4.p3.3.m3.1.1.4.3.3.2"><times id="S4.SS4.p3.3.m3.1.1.4.3.3.2.1.cmml" xref="S4.SS4.p3.3.m3.1.1.4.3.3.2.1"></times><ci id="S4.SS4.p3.3.m3.1.1.4.3.3.2.2.cmml" xref="S4.SS4.p3.3.m3.1.1.4.3.3.2.2">𝑅</ci><ci id="S4.SS4.p3.3.m3.1.1.4.3.3.2.3.cmml" xref="S4.SS4.p3.3.m3.1.1.4.3.3.2.3">𝐿</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.m3.1c">Acc_{C\rightarrow\widehat{RL}}</annotation></semantics></math> (modified).</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion &amp; Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.3" class="ltx_p">The results obtained in this contribution demonstrate that, in general, the generation of synthetic images of pallet blocks for the purpose of dataset enhancement is feasible.
Using the dataset pallet-block-502 and the state-of-the-art GAN architecture CycleGAN, <math id="S5.p1.1.m1.2" class="ltx_Math" alttext="1,004" display="inline"><semantics id="S5.p1.1.m1.2a"><mrow id="S5.p1.1.m1.2.3.2" xref="S5.p1.1.m1.2.3.1.cmml"><mn id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">1</mn><mo id="S5.p1.1.m1.2.3.2.1" xref="S5.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.p1.1.m1.2.2" xref="S5.p1.1.m1.2.2.cmml">004</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.2b"><list id="S5.p1.1.m1.2.3.1.cmml" xref="S5.p1.1.m1.2.3.2"><cn type="integer" id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">1</cn><cn type="integer" id="S5.p1.1.m1.2.2.cmml" xref="S5.p1.1.m1.2.2">004</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.2c">1,004</annotation></semantics></math> images of rotated pallet blocks were generated from images of the same pallet blocks in a centered perspective.
For <math id="S5.p1.2.m2.1" class="ltx_Math" alttext="102" display="inline"><semantics id="S5.p1.2.m2.1a"><mn id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml">102</mn><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><cn type="integer" id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">102</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">102</annotation></semantics></math> of these images, which were taken under comparatively low lighting conditions, no visual rotation could be perceived.
We assume this to be the case due to mode collapse, during the generator’s training.
The remaining <math id="S5.p1.3.m3.1" class="ltx_Math" alttext="902" display="inline"><semantics id="S5.p1.3.m3.1a"><mn id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml">902</mn><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><cn type="integer" id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1">902</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">902</annotation></semantics></math> synthetic images closely resemble the original images in terms of their chipwood surface structure, but differ in the way the pallet blocks in the images are oriented towards the camera (i.e., they are now rotated to the left-hand side, instead of being centered).
Therefore, from a visual perspective, the aim of the herein described procedure was accomplished and new data could reliably be generated, enhancing the existing dataset.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.6" class="ltx_p">Beyond visuals, the synthetic images were evaluated by using a classifier, trained on original images, that discriminates between rotated and centered perspectives.
Both original and synthetic images were run through the classifier and the classification accuracy was compared.
While the classification accuracy for original images was <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="98\%" display="inline"><semantics id="S5.p2.1.m1.1a"><mrow id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml"><mn id="S5.p2.1.m1.1.1.2" xref="S5.p2.1.m1.1.1.2.cmml">98</mn><mo id="S5.p2.1.m1.1.1.1" xref="S5.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><apply id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.p2.1.m1.1.1.1.cmml" xref="S5.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.p2.1.m1.1.1.2.cmml" xref="S5.p2.1.m1.1.1.2">98</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">98\%</annotation></semantics></math>, the classification accuracy for synthetic images was <math id="S5.p2.2.m2.1" class="ltx_Math" alttext="92\%" display="inline"><semantics id="S5.p2.2.m2.1a"><mrow id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml"><mn id="S5.p2.2.m2.1.1.2" xref="S5.p2.2.m2.1.1.2.cmml">92</mn><mo id="S5.p2.2.m2.1.1.1" xref="S5.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><apply id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1"><csymbol cd="latexml" id="S5.p2.2.m2.1.1.1.cmml" xref="S5.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.p2.2.m2.1.1.2.cmml" xref="S5.p2.2.m2.1.1.2">92</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">92\%</annotation></semantics></math>, meaning that there remains a <math id="S5.p2.3.m3.1" class="ltx_Math" alttext="6\%" display="inline"><semantics id="S5.p2.3.m3.1a"><mrow id="S5.p2.3.m3.1.1" xref="S5.p2.3.m3.1.1.cmml"><mn id="S5.p2.3.m3.1.1.2" xref="S5.p2.3.m3.1.1.2.cmml">6</mn><mo id="S5.p2.3.m3.1.1.1" xref="S5.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.3.m3.1b"><apply id="S5.p2.3.m3.1.1.cmml" xref="S5.p2.3.m3.1.1"><csymbol cd="latexml" id="S5.p2.3.m3.1.1.1.cmml" xref="S5.p2.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S5.p2.3.m3.1.1.2.cmml" xref="S5.p2.3.m3.1.1.2">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.3.m3.1c">6\%</annotation></semantics></math> discrepancy.
This discrepancy implies that there is still a measurable difference between original and synthetic images, in terms of their perspective.
Additionally, the (modified) re-identification method presented in <cite class="ltx_cite ltx_citemacro_citep">(Rutinowski et al. <a href="#bib.bib14" title="" class="ltx_ref">2021b</a>)</cite> was applied to the synthetic images and the resulting re-identification accuracy of <math id="S5.p2.4.m4.1" class="ltx_Math" alttext="88\%" display="inline"><semantics id="S5.p2.4.m4.1a"><mrow id="S5.p2.4.m4.1.1" xref="S5.p2.4.m4.1.1.cmml"><mn id="S5.p2.4.m4.1.1.2" xref="S5.p2.4.m4.1.1.2.cmml">88</mn><mo id="S5.p2.4.m4.1.1.1" xref="S5.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.4.m4.1b"><apply id="S5.p2.4.m4.1.1.cmml" xref="S5.p2.4.m4.1.1"><csymbol cd="latexml" id="S5.p2.4.m4.1.1.1.cmml" xref="S5.p2.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S5.p2.4.m4.1.1.2.cmml" xref="S5.p2.4.m4.1.1.2">88</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.4.m4.1c">88\%</annotation></semantics></math> was compared to the accuracy of <math id="S5.p2.5.m5.1" class="ltx_Math" alttext="96\%" display="inline"><semantics id="S5.p2.5.m5.1a"><mrow id="S5.p2.5.m5.1.1" xref="S5.p2.5.m5.1.1.cmml"><mn id="S5.p2.5.m5.1.1.2" xref="S5.p2.5.m5.1.1.2.cmml">96</mn><mo id="S5.p2.5.m5.1.1.1" xref="S5.p2.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.5.m5.1b"><apply id="S5.p2.5.m5.1.1.cmml" xref="S5.p2.5.m5.1.1"><csymbol cd="latexml" id="S5.p2.5.m5.1.1.1.cmml" xref="S5.p2.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S5.p2.5.m5.1.1.2.cmml" xref="S5.p2.5.m5.1.1.2">96</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.5.m5.1c">96\%</annotation></semantics></math> resulting from the use of original images only.
Again, the <math id="S5.p2.6.m6.1" class="ltx_Math" alttext="8\%" display="inline"><semantics id="S5.p2.6.m6.1a"><mrow id="S5.p2.6.m6.1.1" xref="S5.p2.6.m6.1.1.cmml"><mn id="S5.p2.6.m6.1.1.2" xref="S5.p2.6.m6.1.1.2.cmml">8</mn><mo id="S5.p2.6.m6.1.1.1" xref="S5.p2.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.6.m6.1b"><apply id="S5.p2.6.m6.1.1.cmml" xref="S5.p2.6.m6.1.1"><csymbol cd="latexml" id="S5.p2.6.m6.1.1.1.cmml" xref="S5.p2.6.m6.1.1.1">percent</csymbol><cn type="integer" id="S5.p2.6.m6.1.1.2.cmml" xref="S5.p2.6.m6.1.1.2">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.6.m6.1c">8\%</annotation></semantics></math> discrepancy shows that there remains a measurable difference between original and synthetic images, even in terms of their surface structure.
Therefore, while discrepancies still can be made out, we perceive the results obtained in this contribution as valuable and promising, further confirming the visually satisfactory results.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Finally, the work presented in this contribution can be improved, for instance, by using more input images for the GAN or by using a different GAN architecture to begin with.
Additionally, reducing the dependency on lighting conditions could improve the results obtained when applying the GAN to images using low lighting, as shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Synthetic Data Generation Results ‣ 4 Experimental Evaluation ‣ On the Applicability of Synthetic Data for Re-Identification in Warehousing Logistics" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
More perspectives could be generated from the same pallet block, or conversely new pallet blocks (i.e., a new chipwood surface structure) could be generated while retaining the same perspective.
Lastly, further experiments using other GAN architectures should be conducted, as a methodological comparison was beyond the scope of this work.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgments</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This work is part of the project “Silicon Economy Logistics Ecosystem” which is funded by the German Federal Ministry of Transport and Digital Infrastructure.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borji (2019)</span>
<span class="ltx_bibblock">
Borji, A. 2019.

</span>
<span class="ltx_bibblock">Pros and Cons of GAN Evaluation Measures.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Image Understanding</em>, 179: 41–65.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borji (2022)</span>
<span class="ltx_bibblock">
Borji, A. 2022.

</span>
<span class="ltx_bibblock">Pros and Cons of GAN Evaluation Measures: New
Developments.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Image Understanding</em>, 215.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. (2021)</span>
<span class="ltx_bibblock">
Cai, Z.; Xiong, Z.; Xu, H.; Wang, P.; Li, W.; and Pan, Y. 2021.

</span>
<span class="ltx_bibblock">Generative Adversarial Networks: A Survey Toward Private
and Secure Applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em>, 54(6): 1–38.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deviatkin and Horttanainen (2020)</span>
<span class="ltx_bibblock">
Deviatkin, I.; and Horttanainen, M. 2020.

</span>
<span class="ltx_bibblock">Carbon Footprint of an EUR-sized Wooden and a Plastic
Pallet.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">E3S Web of Conferences</em>, volume 158. EDP Sciences.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et al. (2014)</span>
<span class="ltx_bibblock">
Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair,
S.; Courville, A.; and Bengio, Y. 2014.

</span>
<span class="ltx_bibblock">Generative Adversarial Networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 27 (NIPS)</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016.

</span>
<span class="ltx_bibblock">Deep Residual Learning for Image Recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, 770–778.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Isola et al. (2017)</span>
<span class="ltx_bibblock">
Isola, P.; Zhu, J.-Y.; Zhou, T.; and Efros, A. A. 2017.

</span>
<span class="ltx_bibblock">Image-to-image Translation with Conditional Adversarial
Networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, 1125–1134.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lala et al. (2018)</span>
<span class="ltx_bibblock">
Lala, S.; Shady, M.; Belyaeva, A.; and Liu, M. 2018.

</span>
<span class="ltx_bibblock">Evaluation of Mode Collapse in Generative Adversarial
Networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">High Performance Extreme Computing</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Langr and Bok (2019)</span>
<span class="ltx_bibblock">
Langr, J.; and Bok, V. 2019.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">GANs in Action: Deep Learning with Generative
Adversarial Networks</em>.

</span>
<span class="ltx_bibblock">Manning.

</span>
<span class="ltx_bibblock">ISBN 9781617295560.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Packaging (2004)</span>
<span class="ltx_bibblock">
Packaging, D. S. C. 2004.

</span>
<span class="ltx_bibblock">Pallet Production Specification - Part 1: Construction
Specification for 800 mm × 1200 mm Flat Wooden Pallets; German version
EN 13698-1:2003.

</span>
<span class="ltx_bibblock">Technical report, Beuth Verlag GmbH.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. (2019)</span>
<span class="ltx_bibblock">
Pan, Z.; Yu, W.; Yi, X.; Khan, A.; Yuan, F.; and Zheng, Y. 2019.

</span>
<span class="ltx_bibblock">Recent Progress on Generative Adversarial Networks (GANs):
A Survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 7: 36322–36333.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rong et al. (2021)</span>
<span class="ltx_bibblock">
Rong, L.; Xu, Y.; Zhou, X.; Han, L.; Li, L.; and Pan, X. 2021.

</span>
<span class="ltx_bibblock">A Vehicle Re-Identification Framework based on the Improved
Multi-branch Feature Fusion Network.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Scientific Reports</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rutinowski et al. (2021a)</span>
<span class="ltx_bibblock">
Rutinowski, J.; Chilla, T.; Pionzewski, C.; Reining, C.; and ten Hompel, M.
2021a.

</span>
<span class="ltx_bibblock">Pallet-Block-502 – A chipwood Re-Identification Dataset.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rutinowski et al. (2021b)</span>
<span class="ltx_bibblock">
Rutinowski, J.; Chilla, T.; Pionzewski, C.; Reining, C.; and ten Hompel, M.
2021b.

</span>
<span class="ltx_bibblock">Towards Re-Identification for Warehousing Entities – A
Work-in-Progress Study.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Emerging Technologies
in Factory Automation (ETFA)</em>, 501–504.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2018)</span>
<span class="ltx_bibblock">
Sun, Y.; Zheng, L.; Yang, Y.; Tian, Q.; and Wang, S. 2018.

</span>
<span class="ltx_bibblock">Beyond Part Models: Person Retrieval with Refined Part
Pooling (and a Strong Convolutional Baseline).

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</em>, 480–496.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2017)</span>
<span class="ltx_bibblock">
Wang, K.; Gou, C.; Duan, Y.; Lin, Y.; Zheng, X.; and Wang, F.-Y. 2017.

</span>
<span class="ltx_bibblock">Generative Adversarial Networks: Introduction and Outlook.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE/CAA Journal of Automatica Sinica</em>, 4(4): 588–598.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2018)</span>
<span class="ltx_bibblock">
Wang, T.-C.; Liu, M.-Y.; Zhu, J.-Y.; Tao, A.; Kautz, J.; and Catanzaro, B.
2018.

</span>
<span class="ltx_bibblock">High-resolution Image Synthesis and Semantic Manipulation
with Conditional GANs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, 8798–8807.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang, She, and Ward (2021)</span>
<span class="ltx_bibblock">
Wang, Z.; She, Q.; and Ward, T. E. 2021.

</span>
<span class="ltx_bibblock">Generative Adversarial Networks in Computer Vision: A
Survey and Taxonomy.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em>, 54(2): 1–38.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2018)</span>
<span class="ltx_bibblock">
Wei, L.; Liu, X.; Li, J.; and Zhang, S. 2018.

</span>
<span class="ltx_bibblock">VP-ReID: Vehicle and Person Re-Identification System.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on International Conference on
Multimedia Retrieval</em>, 501–504.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2021)</span>
<span class="ltx_bibblock">
Ye, M.; Shen, J.; Lin, G.; Xiang, T.; Shao, L.; and Hoi, S. C. 2021.

</span>
<span class="ltx_bibblock">Deep Learning for Person Re-Identification: A Survey and
Outlook.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence
(TPAMI)</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yi et al. (2014)</span>
<span class="ltx_bibblock">
Yi, D.; Lei, Z.; Liao, S.; and Li, S. Z. 2014.

</span>
<span class="ltx_bibblock">Deep Metric Learning for Person Re-Identification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Pattern
Recognition</em>, 34–39.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu, Wu, and Zheng (2017)</span>
<span class="ltx_bibblock">
Yu, H.-X.; Wu, A.; and Zheng, W.-S. 2017.

</span>
<span class="ltx_bibblock">Cross-View Asymmetric Metric Learning for Unsupervised
Person Re-Identification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision (ICCV)</em>, 994–1002.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng, Yang, and Hauptmann (2016)</span>
<span class="ltx_bibblock">
Zheng, L.; Yang, Y.; and Hauptmann, A. G. 2016.

</span>
<span class="ltx_bibblock">Person Re-Identification: Past, Present and Future.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint: 1610.02984</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou and Xiang (2019)</span>
<span class="ltx_bibblock">
Zhou, K.; and Xiang, T. 2019.

</span>
<span class="ltx_bibblock">Torchreid: A Library for Deep Learning Person
Re-Identification in Pytorch.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint: 1910.10093</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2017)</span>
<span class="ltx_bibblock">
Zhu, J.-Y.; Park, T.; Isola, P.; and Efros, A. A. 2017.

</span>
<span class="ltx_bibblock">Unpaired Image-to-Image Translation using Cycle-consistent
Adversarial Networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision (ICCV)</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2212.10104" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2212.10105" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2212.10105">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2212.10105" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2212.10106" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 10:25:25 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
