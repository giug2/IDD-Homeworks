<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2007.01780] Visual Question Answering as a Multi-task Problem</title><meta property="og:description" content="Visual Question Answering(VQA)[2] is a highly complex problem set, relying on many sub-problems to produce reasonable answers. In this paper, we present the hypothesis that Visual Question Answering should be viewed as…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Visual Question Answering as a Multi-task Problem">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Visual Question Answering as a Multi-task Problem">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2007.01780">

<!--Generated on Mon Mar 18 15:25:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Visual Question Answering as a Multi-task Problem</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amelia E. Pollard
<br class="ltx_break">Department of Computer Science
<br class="ltx_break">University of Manchester
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">amelia.pollard@postgrad.manchester.ac.uk</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jonathan L. Shapiro
<br class="ltx_break">Department of Computer Science
<br class="ltx_break">University of Manchester
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter">jonathan.shapiro@manchester.ac.uk</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Visual Question Answering(VQA)<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> is a highly complex problem set, relying on many sub-problems to produce reasonable answers. In this paper, we present the hypothesis that Visual Question Answering should be viewed as a multi-task problem<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, and provide evidence to support this hypothesis. We demonstrate this by reformatting two commonly used Visual Question Answering datasets, COCO-QA<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and DAQUAR<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, into a multi-task format and train these reformatted datasets on two baseline networks, with one designed specifically to eliminate other possible causes for performance changes as a result of the reformatting. Though the networks demonstrated in this paper do not achieve strongly competitive results, we find that the multi-task approach to Visual Question Answering results in increases in performance of 5-9% against the single-task formatting, and that the networks reach convergence much faster than in the single-task case. Finally we discuss possible reasons for the observed difference in performance, and perform additional experiments which ruled out causes not associated with the learning of the dataset as a multi-task problem.</p>
</div>
<section id="Ch0.S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">0.1 </span>Introduction</h2>

<div id="Ch0.S1.p1" class="ltx_para ltx_noindent">
<p id="Ch0.S1.p1.1" class="ltx_p">Visual Question Answering (VQA)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> is the act of showing a machine learning architecture an image of a natural scene and a natural language question about that scene with the expectation of a reasonable or appropriate answer. VQA is a relatively new problem in machine learning research; the first VQA architectures were developed in 2015. Despite being a new research area, it has seen use in numerous applications, including assisting the blind<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite><cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, validating human generated captions on images<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, and automatic querying of surveillance video<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. VQA is a highly complex task relying on so many aspects of general intelligence that it has even been proposed as a metric for measuring the general intelligence of Artificial Intelligence systems<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
<div id="Ch0.S1.p2" class="ltx_para ltx_noindent">
<p id="Ch0.S1.p2.1" class="ltx_p">There are several methods to generate answers for the questions presented by Visual Question Answering, from the simplest yes/no questions<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> to multiple choice<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> to fill-in-the-blank style questions<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. The datasets used in this paper are open-ended, the network may select as its answer any possible answer from the dataset.</p>
</div>
<div id="Ch0.S1.p3" class="ltx_para ltx_noindent">
<p id="Ch0.S1.p3.1" class="ltx_p">VQA is also quite different to other machine learning problems and presents its own unique set of challenges. In other machine learning problems, the network need only answer a single fixed question, whereas in VQA, the network must learn to answer multiple unrelated questions, without having previously seen any of the specific questions until runtime. Whilst most machine learning problems will have numerous examples of input and output data for their single fixed question, Visual Question Answering must respond to the more general problem of understanding a scene and the natural language that describes it in order to correctly answer a never before seen question. Thus, it is clear both that VQA is markedly different from other types of machine learning problems and that it involves a great deal of complexity as a research area.</p>
</div>
<div id="Ch0.S1.p4" class="ltx_para ltx_noindent">
<p id="Ch0.S1.p4.1" class="ltx_p">To fully understand the aforementioned complexities of VQA and how these can be and are addressed, we must look at each problem and the challenges presented by it. There are three main problems that must be tackled by VQA networks, one of which is especially complex. The first of these problems is teaching the network <span id="Ch0.S1.p4.1.1" class="ltx_text ltx_font_italic">how to see</span>. This is handled using computer vision; in this case, we are using a pre-trained deep CNN, VGG19. The second, <span id="Ch0.S1.p4.1.2" class="ltx_text ltx_font_italic">how to read</span>, is tackled by natural language processing, which is handled by a CNN in our research as in <cite class="ltx_cite ltx_citemacro_citet">Yang et al. [<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. These two areas are entire research fields in themselves and present their own unique challenges which are outside the scope of this paper.</p>
</div>
<div id="Ch0.S1.p5" class="ltx_para ltx_noindent">
<p id="Ch0.S1.p5.1" class="ltx_p">The third and by far the most difficult problem set is <span id="Ch0.S1.p5.1.1" class="ltx_text ltx_font_italic">how to understand</span>, and it is this problem that our research focuses on. To solve this problem set, we must enable the network to understand the question in the context of the scene and to understand the objects and their relationships in the image. In order to correctly respond to the question and produce a reasonable answer, the network must therefore be trained to understand these problems. This general understanding of natural language and how it applies to an observed scene is hoped to eventually lead to interactive systems of many kinds, from industrial robots capable of being programmed with natural language instructions to manipulate their environment, to smart-home systems capable of interacting with their user in a fluid and intuitive way.</p>
</div>
<div id="Ch0.S1.p6" class="ltx_para ltx_noindent">
<p id="Ch0.S1.p6.1" class="ltx_p">Due to the relationship between these three problems, VQA finds itself on the intersection between several research topics that have seen significant focus in recent years: computer vision, reasoning, and natural language processing. It has therefore received much attention over the last decade. Current state-of-the-art methods rely primarily on deep neural networks, which when designed well, excel at capturing the underlying complexity of input datasets. Many variations on the standard deep neural network have been proposed for VQA problems with varying degrees of success, for example, using Convolutional Neural Networks (CNN) to extract image properties, using Long Short Term Memory (LSTM) or CNNs to process natural language questions, or using attention techniques to reduce dimensionality. Fundamentally, all of these approaches have relied on improving the components of the network for each sub-problem (NLP, image processing, object recognition), and thereby finding their own ways to address one of the three problems presented above.
These approaches to Visual Question Answering have so far fallen into four main categories:</p>
<ul id="Ch0.S1.I1" class="ltx_itemize">
<li id="Ch0.S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch0.S1.I1.i1.p1" class="ltx_para">
<p id="Ch0.S1.I1.i1.p1.1" class="ltx_p">joint embedding: extracting and then combining features from the image and question.</p>
</div>
</li>
<li id="Ch0.S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch0.S1.I1.i2.p1" class="ltx_para">
<p id="Ch0.S1.I1.i2.p1.1" class="ltx_p">attentional models: using features from the question to generate an attentional mask over the image.</p>
</div>
</li>
<li id="Ch0.S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch0.S1.I1.i3.p1" class="ltx_para">
<p id="Ch0.S1.I1.i3.p1.1" class="ltx_p">compositional models: in which features from the inputs are used to select pre-trained modules to answer the question.</p>
</div>
</li>
<li id="Ch0.S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch0.S1.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="Ch0.S1.I1.i4.p1.1" class="ltx_p">and knowledge enhanced models: using an existing knowledge base which can be queried to help answer the question.</p>
</div>
</li>
</ul>
</div>
<div id="Ch0.S1.p7" class="ltx_para ltx_noindent">
<p id="Ch0.S1.p7.1" class="ltx_p">Much work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> has focused on the joint embedding approach. This approach involves combining extracted features of the natural language question with the features of the associated image in a higher dimensional space, before running a classifier on this new high dimensional data. Modifications to this method such as integrating attention methods<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> have also been shown to be incredibly effective, resulting in a large quantity of research and current state-of-the-art VQA networks being attention based.
<br class="ltx_break"></p>
</div>
<div id="Ch0.S1.p8" class="ltx_para ltx_noindent">
<p id="Ch0.S1.p8.1" class="ltx_p">However, as VQA is a relatively young research area having been coined in 2015, there remain many research topics which have yet to be explored, especially in the area of reasoning about the inputs, where simple deep neural networks have made up the bulk of this research. Datasets for Visual Question Answering which tackle issues such as bias in the answer sets and provide counter examples for each question have been proposed, though capturing the totality of the domain is a nigh impossible task, and difficulties with truly understanding the datasets still exist. In particular, the complex multi-task nature of the Visual Question Answering datasets has so far been ignored. This is the topic that we tackle in this research. While several VQA dataset authors have taken the time to label the individual task types, no research has been performed to analyse the interaction between those tasks which, while they share the same domain of semantic understanding of visual scenes, focus on very different aspects of that domain. Multiple task types in a shared domain require careful handling in order to prevent destructive interference between those tasks, bringing us to the field of multi-task networks.
<br class="ltx_break">Multi-task networks are typically designed to solve one primary target task, with one or more additional related tasks also learned during training. These secondary tasks are generally ignored at runtime. Multi-task networks have shown performance increases where the tasks share a domain <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. This shared domain biases the hypothesis space into a more generalisable form, which results in increased performance over single task approaches. We hypothesise that the more complex nature of VQA datasets makes them particularly suited for this shared domain utilisation, as each question relies upon a series of subtasks which may be shared between question types.
<br class="ltx_break"></p>
</div>
<div id="Ch0.S1.p9" class="ltx_para ltx_noindent">
<p id="Ch0.S1.p9.1" class="ltx_p">Visual Question Answering datasets like COCO-QA<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and DAQUAR<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> contain multiple categories of questions that are easily definable. In COCO-QA, those question categories are labelled by the authors. We propose that the categories of questions (object, number, colour, location) can be thought of as separate tasks in a multi-task environment and further, that the overlap between the tasks (for example, spatial reasoning is required by all the question categories) can be considered as separate tasks also.
<br class="ltx_break"></p>
</div>
<div id="Ch0.S1.p10" class="ltx_para ltx_noindent">
<p id="Ch0.S1.p10.1" class="ltx_p">We propose that VQA datasets would be better viewed as a multi-task problem, given that the datasets are composed of several different question types and their corresponding subtasks. With this in mind, measures should be also be taken to avoid the interference problems that occur when training a neural network over multiple separate tasks.</p>
</div>
<div id="Ch0.S1.p11" class="ltx_para ltx_noindent">
<p id="Ch0.S1.p11.1" class="ltx_p">By taking this collection of separate tasks into account when building and training the network, we can avoid common problems inherent in networks where multiple tasks are trained together such as inter-task interference and catastrophic forgetting<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="Ch0.S1.p12" class="ltx_para ltx_noindent">
<p id="Ch0.S1.p12.1" class="ltx_p">In the first section of this paper, we perform a simple experiment to demonstrate that VQA datasets can be learned by a multi-task network with a performance increase over a single task network in the simplest case. We then present arguments for alternative explanations for this performance increase. We then demonstrate that that performance increase can be extended to more complex networks, before drawing conclusions and proposing further work.</p>
</div>
</section>
<section id="Ch0.S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">0.2 </span>VQA as a multi-task Problem</h2>

<div id="Ch0.S2.p1" class="ltx_para ltx_noindent">
<p id="Ch0.S2.p1.1" class="ltx_p">As Caruana<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> showed in his seminal paper on multi-task learning, a shared domain over multiple tasks results in better generalisation of the internal representation over training just the primary task alone. Such increased generalisation over the domain results in increased performance over single tasks. It is our intention to show that such a generalisation can be attained by training a network by treating the question types in VQA datasets as different tasks.</p>
</div>
<div id="Ch0.S2.p2" class="ltx_para ltx_noindent">
<p id="Ch0.S2.p2.1" class="ltx_p">In this paper, we therefore present the hypothesis that Visual Question Answering datasets can be treated as multi-task datasets, where each question type is a single task within the shared domain of natural scenes. This hypothesis is tested by first testing the simplest possible multi-task formation of the VQA dataset and network, before comparing results against those of a simple single-task version of the network.</p>
</div>
</section>
<section id="Ch0.S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">0.3 </span>Experiments</h2>

<div id="Ch0.S3.p1" class="ltx_para ltx_noindent">
<p id="Ch0.S3.p1.1" class="ltx_p">In order to evidence our hypothesis, we must first show that a performance increase can be obtained in the simplest case by changing from a single task to a multi-task paradigm. For that purpose, we constructed two networks. The first network is a multi-task learning network, where each question type is treated as an individual task in the shared domain. We then created a second network with single-task learning paradigm, as is normally applied in Visual Question Answering networks. These networks were then trained over the same datasets, and performance is compared. We then considered potential sources for the observed performance differences and performed additional tests to rule out those potential sources.
Finally, we reconstructed a network from the visualqa.org leaderboard and constructed an additional variation of this network with modifications to train it in a multi-task manner, to demonstrate that this approach is not an artefact of network architecture.</p>
</div>
<section id="Ch0.S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.3.1 </span>Multi-Task Learning Network</h3>

<div id="Ch0.S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS1.p1.1" class="ltx_p">To allow for the training of multiple question types simultaneously, we constructed a simple multi-task style network with multiple question inputs (one for each question type), a single image input, and multiple answer outputs (corresponding to the question inputs). We utilised a pre-trained ImageNet CNN (VGG19<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>) for image feature extraction and a separate CNN for question feature extraction which was trained by back propagation. A single shared hidden layer then receives all image and question features, before multiple fully connected softmax layers output the predicted answers. All networks are trained until convergence with Nadam<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, before being fine-tuned by SGD with momentum, as described in <cite class="ltx_cite ltx_citemacro_citet">Keskar and Socher [<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<figure id="Ch0.F1" class="ltx_figure">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>MTL Network Structure. Note that rounded orange rectangles represent sections of the network that are learned during training, while non-rounded blue rectangles indicate fixed weights.</figcaption><img src="/html/2007.01780/assets/MTLNN.png" id="Ch0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="269" alt="Refer to caption">
</figure>
<div id="Ch0.S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS1.p2.1" class="ltx_p">The VGG19 CNN is composed of multiple layers of neurons which each complete the desired calculations, before passing onto the next layer of neurons. There are 19 convolutional layers in total, ending with a final classification layer. In this process, we began by extracting the image features from this final classification layer, resulting in a 512 by 14 by 14 matrix of image features. We then compressed these features to a common size by passing them through a linear hidden layer. This resulted in a vector of image features of a much more manageable size.</p>
</div>
<div id="Ch0.S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS1.p3.1" class="ltx_p">To generate the question features we used for this experiment, we embedded the questions using GloVe<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> pre-trained embedding, which is then fine-tuned by then end-to-end training of the network in order to increase the training speed of the network. The GloVe database is a publicly available database that was created using a global log bilinear regression model to train a word-word co-occurrence matrix which produces a vector-space with meaningful sub-structure. Essentially, this process results in groups of words with a similar meaning, as high dimensional vectors. The process of embedding converts the natural language words from the questions into a sequence of these high dimensional vectors. Words not present in the GloVe database are initialised with random vectors.
We then passed these embedded questions to the question feature extraction CNN created for this experiment. This question feature extraction CNN is constructed in a convolutional sentence model as described in <cite class="ltx_cite ltx_citemacro_citet">Hu et al. [<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. This model uses filters of size 1, 2 and 3, which in this case, would be a single word, a pair of words, and a triplet of words respectively, and applies convolutions to each of these sections to compress the information and extract meaning from the sentences. This results in a feature vector which describes the meaning of each question, both semantically, and syntactically.
Once these two processes are complete, the compressed image features are concatenated with the feature vectors from the question feature extraction CNN, before being fed into the hidden classification layer.</p>
</div>
<div id="Ch0.S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS1.p4.1" class="ltx_p">In order to train this network on standard VQA datasets, we must restructure them significantly. Firstly, we require question type labels in order to better utilise the multiple input format. The COCO-QA dataset has question types as part of the label data falling into 4 classes: object recognition questions, numeric questions, colour questions, and spatial questions. The DAQUAR dataset does not have these labels, and so we generated them automatically with keyword searches. Keywords were chosen by the author manually detecting words that were present in only the relevant question types and checked for accuracy by selecting two hundred random samples of questions and the generated type label and verifying them manually; no errors were detected. Some questions defied classification, and were eliminated from the datasets. The majority of these classification failures were due to misspellings in the dataset, though some were due to grammatical errors (for example, ”Which object is more?”).
The algorithm we designed for automatically classifying question types in the DAQUAR dataset is a simple keyword search. The questions were searched for words associated only with colour questions (’color’, ’colour’, ’red’, ’orange’, etc.), positional questions (’on’, ’in’, ’between’, ’left’, etc.), numerical questions (’count’, ’many’, ’number’, ’frequent’, etc.) and size questions (’largest’, ’smallest’, ’large’, ’big’, etc.). It is important to note that the order of the keyword searches is important as some keywords take priority for classifying the question type. For example, ”How many orange balls are on the table?” should be classified as a count question as the keyword ”How many” takes priority over the keyword ”orange”. Keywords were therefore searched in the order size, numeric, positional, then colour.</p>
</div>
<div id="Ch0.S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS1.p5.1" class="ltx_p">We then collected all images which have multiple question types associated with them and generated training examples by selecting all unique combinations of those questions for each image, padding with zero-vector questions and answers for examples where a question type was not available. The zero-vector placeholder questions were not used for back-propagation or accuracy measurements. For DAQUAR this resulted in a total of 92,288 training examples, while COCO-QA had 240,080 training examples.
<br class="ltx_break"></p>
</div>
<div id="Ch0.S3.SS1.p6" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS1.p6.1" class="ltx_p">Finally, we train the network using a sum of the softmax cross entropy across all answer outputs and back-propagation using the entire training set, and test using the entire test set.</p>
</div>
</section>
<section id="Ch0.S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.3.2 </span>Single-Task Learning Network</h3>

<div id="Ch0.S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS2.p1.1" class="ltx_p">To act as a baseline for comparison, we took the reduced datasets from the multi-task pre-processing of the original datasets and reformatted them into the original single task form, with one image and one question associated with one answer. This removed question-answer pairs that were not present in the multi-task training and so prevented the single task network from having the advantage of additional data.
<br class="ltx_break"></p>
</div>
<div id="Ch0.S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS2.p2.1" class="ltx_p">To reduce the effect of other possible influences, we preserved as much of the multi-task network’s structure as possible by simply removing the additional input and output connections while keeping the hidden layers the same. The final network architecture remained the same as in Figure 1, with input questions 2 to 4 and the corresponding answers removed. As a result, the question feature CNN also has a reduced output size.
<br class="ltx_break"></p>
</div>
</section>
<section id="Ch0.S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.3.3 </span>Results</h3>

<figure id="Ch0.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Table of results showing the final accuracy of both the single-task network (STL) and the multi-task-network (MTL) on each dataset, broken down by question type. Performance figures are rounded to one decimal place in accordance with observed variance between runs. The difference between the STL and MTL figures is in accordance with our predictions, with the MTL network consistently outperforming the STL network across all question types.</figcaption>
<table id="Ch0.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Ch0.T1.1.1.1" class="ltx_tr">
<th id="Ch0.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" colspan="6">Simple VQA Network Comparison Results</th>
</tr>
<tr id="Ch0.T1.1.2.2" class="ltx_tr">
<th id="Ch0.T1.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="Ch0.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="Ch0.T1.1.2.2.2.1" class="ltx_text ltx_font_bold">Colour</span></th>
<th id="Ch0.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="Ch0.T1.1.2.2.3.1" class="ltx_text ltx_font_bold">Count</span></th>
<th id="Ch0.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="Ch0.T1.1.2.2.4.1" class="ltx_text ltx_font_bold">Position</span></th>
<th id="Ch0.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Ch0.T1.1.2.2.5.1" class="ltx_text ltx_font_bold">Size</span></th>
<th id="Ch0.T1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="Ch0.T1.1.2.2.6.1" class="ltx_text ltx_font_bold">Total</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Ch0.T1.1.3.1" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="Ch0.T1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="Ch0.T1.1.3.1.1.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">MTL COCO-QA</span></th>
<td id="Ch0.T1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T1.1.3.1.2.1" class="ltx_text" style="background-color:#E6E6E6;">25.5%</span></td>
<td id="Ch0.T1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T1.1.3.1.3.1" class="ltx_text" style="background-color:#E6E6E6;">35.8%</span></td>
<td id="Ch0.T1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T1.1.3.1.4.1" class="ltx_text" style="background-color:#E6E6E6;">32.5%</span></td>
<td id="Ch0.T1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Ch0.T1.1.3.1.5.1" class="ltx_text" style="background-color:#E6E6E6;">28.3%</span></td>
<td id="Ch0.T1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T1.1.3.1.6.1" class="ltx_text" style="background-color:#E6E6E6;">27.0%</span></td>
</tr>
<tr id="Ch0.T1.1.4.2" class="ltx_tr">
<th id="Ch0.T1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="Ch0.T1.1.4.2.1.1" class="ltx_text ltx_font_bold">STL COCO-QA</span></th>
<td id="Ch0.T1.1.4.2.2" class="ltx_td ltx_align_center">21.2%</td>
<td id="Ch0.T1.1.4.2.3" class="ltx_td ltx_align_center">28.4%</td>
<td id="Ch0.T1.1.4.2.4" class="ltx_td ltx_align_center">27.1%</td>
<td id="Ch0.T1.1.4.2.5" class="ltx_td ltx_align_center ltx_border_r">21.9%</td>
<td id="Ch0.T1.1.4.2.6" class="ltx_td ltx_align_center">22.9%</td>
</tr>
<tr id="Ch0.T1.1.5.3" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="Ch0.T1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="Ch0.T1.1.5.3.1.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">Difference</span></th>
<td id="Ch0.T1.1.5.3.2" class="ltx_td ltx_align_center"><span id="Ch0.T1.1.5.3.2.1" class="ltx_text" style="background-color:#E6E6E6;">+4.3</span></td>
<td id="Ch0.T1.1.5.3.3" class="ltx_td ltx_align_center"><span id="Ch0.T1.1.5.3.3.1" class="ltx_text" style="background-color:#E6E6E6;">+7.4</span></td>
<td id="Ch0.T1.1.5.3.4" class="ltx_td ltx_align_center"><span id="Ch0.T1.1.5.3.4.1" class="ltx_text" style="background-color:#E6E6E6;">+5.4</span></td>
<td id="Ch0.T1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_r"><span id="Ch0.T1.1.5.3.5.1" class="ltx_text" style="background-color:#E6E6E6;">+6.4</span></td>
<td id="Ch0.T1.1.5.3.6" class="ltx_td ltx_align_center"><span id="Ch0.T1.1.5.3.6.1" class="ltx_text" style="background-color:#E6E6E6;">+4.1</span></td>
</tr>
<tr id="Ch0.T1.1.6.4" class="ltx_tr">
<th id="Ch0.T1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="Ch0.T1.1.6.4.1.1" class="ltx_text ltx_font_bold">MTL DAQUAR</span></th>
<td id="Ch0.T1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_t">32.0%</td>
<td id="Ch0.T1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_t">23.1%</td>
<td id="Ch0.T1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_t">22.2%</td>
<td id="Ch0.T1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.4%</td>
<td id="Ch0.T1.1.6.4.6" class="ltx_td ltx_align_center ltx_border_t">17.0%</td>
</tr>
<tr id="Ch0.T1.1.7.5" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="Ch0.T1.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="Ch0.T1.1.7.5.1.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">STL DAQUAR</span></th>
<td id="Ch0.T1.1.7.5.2" class="ltx_td ltx_align_center"><span id="Ch0.T1.1.7.5.2.1" class="ltx_text" style="background-color:#E6E6E6;">12.1%</span></td>
<td id="Ch0.T1.1.7.5.3" class="ltx_td ltx_align_center"><span id="Ch0.T1.1.7.5.3.1" class="ltx_text" style="background-color:#E6E6E6;">17.4%</span></td>
<td id="Ch0.T1.1.7.5.4" class="ltx_td ltx_align_center"><span id="Ch0.T1.1.7.5.4.1" class="ltx_text" style="background-color:#E6E6E6;">0%<span id="Ch0.footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span></span></span></span></span></td>
<td id="Ch0.T1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_r"><span id="Ch0.T1.1.7.5.5.1" class="ltx_text" style="background-color:#E6E6E6;">2.6%</span></td>
<td id="Ch0.T1.1.7.5.6" class="ltx_td ltx_align_center"><span id="Ch0.T1.1.7.5.6.1" class="ltx_text" style="background-color:#E6E6E6;">8.5%</span></td>
</tr>
<tr id="Ch0.T1.1.8.6" class="ltx_tr">
<th id="Ch0.T1.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="Ch0.T1.1.8.6.1.1" class="ltx_text ltx_font_bold">Difference</span></th>
<td id="Ch0.T1.1.8.6.2" class="ltx_td ltx_align_center">+8.9</td>
<td id="Ch0.T1.1.8.6.3" class="ltx_td ltx_align_center">+5.7</td>
<td id="Ch0.T1.1.8.6.4" class="ltx_td ltx_align_center">+22.2</td>
<td id="Ch0.T1.1.8.6.5" class="ltx_td ltx_align_center ltx_border_r">+0.8</td>
<td id="Ch0.T1.1.8.6.6" class="ltx_td ltx_align_center">+8.5</td>
</tr>
</tbody>
</table>
</figure><span id="Ch0.footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotetext: </span>Due to the unlikely nature of this result we trained the network under a number of different initial conditions, however the result remained that zero of this question type were answered correctly. We are confident in the validity of the result, though we are continuing to investigate.</span></span></span>
<div id="Ch0.S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS3.p1.1" class="ltx_p">The simple nature of the tested network precluded the possibility of state-of-the-art results, however a significant performance increase over the simple STL network can clearly be seen in each individual category and the total result.</p>
</div>
<div id="Ch0.S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS3.p2.1" class="ltx_p">The results for the DAQUAR dataset were substantially lower than those of the COCO-QA dataset. We believe this was due to the more complex questions in the DAQUAR dataset, in combination with the much smaller size of the dataset itself which contains only 6794 training examples in contrast with COCO-QA’s 78736. The position category questions in particular are often more complex, requiring 2nd order reasoning. For example, ”what are on the wall on the left side of the green curtain but not behind the garbage bin”.</p>
</div>
</section>
<section id="Ch0.S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.3.4 </span>Alternate Reasons for Performance Changes</h3>

<div id="Ch0.S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS4.p1.1" class="ltx_p">To positively assert that the primary cause of performance changes is the effect of multi-task interference, we must first eliminate other potential explanations.
Notable possible contributions to performance changes are:</p>
<ul id="Ch0.S3.I1" class="ltx_itemize">
<li id="Ch0.S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch0.S3.I1.i1.p1" class="ltx_para">
<p id="Ch0.S3.I1.i1.p1.1" class="ltx_p">Architecture</p>
</div>
</li>
<li id="Ch0.S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch0.S3.I1.i2.p1" class="ltx_para">
<p id="Ch0.S3.I1.i2.p1.1" class="ltx_p">Reduced problem set</p>
</div>
</li>
<li id="Ch0.S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Ch0.S3.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="Ch0.S3.I1.i3.p1.1" class="ltx_p">Shared information</p>
</div>
</li>
</ul>
</div>
</section>
<section id="Ch0.S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.3.5 </span>Architecture</h3>

<div id="Ch0.S3.SS5.p1" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS5.p1.1" class="ltx_p">It is possible that the wider input layer allows for more information to be encoded in the connections between the input and hidden layer. With a reduced need to compress the relevant information for all questions types in the hidden layer, it is possible that the multi-task network would have an advantage over the single task network. We test this by training the multi-task network with the single task data, feeding only one question type at a time with blank zero-vectors for the other question type inputs while preserving all examples from the multi-task reformatting. We find that while this does result in a slight performance increase over the single-task network, it does not completely account for the observed increase in performance achieved by the multi-task network trained on the multi-task data.</p>
</div>
<figure id="Ch0.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Table of results showing the difference in results obtained by testing single task data on the multi-task network in comparison to the results obtained from testing single-task data on the single-task network.</figcaption>
<table id="Ch0.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Ch0.T2.1.1.1" class="ltx_tr">
<th id="Ch0.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="6">Single-Task Data in Both Single and Multi-task Architectures Results</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Ch0.T2.1.2.1" class="ltx_tr">
<th id="Ch0.T2.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="Ch0.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T2.1.2.1.2.1" class="ltx_text ltx_font_bold">Colour</span></td>
<td id="Ch0.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T2.1.2.1.3.1" class="ltx_text ltx_font_bold">Count</span></td>
<td id="Ch0.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T2.1.2.1.4.1" class="ltx_text ltx_font_bold">Position</span></td>
<td id="Ch0.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Ch0.T2.1.2.1.5.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="Ch0.T2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T2.1.2.1.6.1" class="ltx_text ltx_font_bold">Total</span></td>
</tr>
<tr id="Ch0.T2.1.3.2" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="Ch0.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="Ch0.T2.1.3.2.1.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">STL Data MTL COCO-QA</span></th>
<td id="Ch0.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T2.1.3.2.2.1" class="ltx_text" style="background-color:#E6E6E6;">21.3%</span></td>
<td id="Ch0.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T2.1.3.2.3.1" class="ltx_text" style="background-color:#E6E6E6;">28.6%</span></td>
<td id="Ch0.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T2.1.3.2.4.1" class="ltx_text" style="background-color:#E6E6E6;">20.3%</span></td>
<td id="Ch0.T2.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Ch0.T2.1.3.2.5.1" class="ltx_text" style="background-color:#E6E6E6;">19.3%</span></td>
<td id="Ch0.T2.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T2.1.3.2.6.1" class="ltx_text" style="background-color:#E6E6E6;">22.0%</span></td>
</tr>
<tr id="Ch0.T2.1.4.3" class="ltx_tr">
<th id="Ch0.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="Ch0.T2.1.4.3.1.1" class="ltx_text ltx_font_bold">STL Data STL COCO-QA</span></th>
<td id="Ch0.T2.1.4.3.2" class="ltx_td ltx_align_center">21.2%</td>
<td id="Ch0.T2.1.4.3.3" class="ltx_td ltx_align_center">28.4%</td>
<td id="Ch0.T2.1.4.3.4" class="ltx_td ltx_align_center">27.1%</td>
<td id="Ch0.T2.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">21.9%</td>
<td id="Ch0.T2.1.4.3.6" class="ltx_td ltx_align_center">22.9%</td>
</tr>
<tr id="Ch0.T2.1.5.4" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="Ch0.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="Ch0.T2.1.5.4.1.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">Difference</span></th>
<td id="Ch0.T2.1.5.4.2" class="ltx_td ltx_align_center"><span id="Ch0.T2.1.5.4.2.1" class="ltx_text" style="background-color:#E6E6E6;">+0.1</span></td>
<td id="Ch0.T2.1.5.4.3" class="ltx_td ltx_align_center"><span id="Ch0.T2.1.5.4.3.1" class="ltx_text" style="background-color:#E6E6E6;">+0.2</span></td>
<td id="Ch0.T2.1.5.4.4" class="ltx_td ltx_align_center"><span id="Ch0.T2.1.5.4.4.1" class="ltx_text" style="background-color:#E6E6E6;">-6.8</span></td>
<td id="Ch0.T2.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r"><span id="Ch0.T2.1.5.4.5.1" class="ltx_text" style="background-color:#E6E6E6;">-2.6</span></td>
<td id="Ch0.T2.1.5.4.6" class="ltx_td ltx_align_center"><span id="Ch0.T2.1.5.4.6.1" class="ltx_text" style="background-color:#E6E6E6;">+2.9</span></td>
</tr>
<tr id="Ch0.T2.1.6.5" class="ltx_tr">
<th id="Ch0.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="Ch0.T2.1.6.5.1.1" class="ltx_text ltx_font_bold">STL Data MTL DAQUAR</span></th>
<td id="Ch0.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t">18.2%</td>
<td id="Ch0.T2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">30.9%</td>
<td id="Ch0.T2.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t">12.3%</td>
<td id="Ch0.T2.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">38.6%</td>
<td id="Ch0.T2.1.6.5.6" class="ltx_td ltx_align_center ltx_border_t">9.7%</td>
</tr>
<tr id="Ch0.T2.1.7.6" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="Ch0.T2.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="Ch0.T2.1.7.6.1.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">STL Data STL DAQUAR</span></th>
<td id="Ch0.T2.1.7.6.2" class="ltx_td ltx_align_center"><span id="Ch0.T2.1.7.6.2.1" class="ltx_text" style="background-color:#E6E6E6;">12.1%</span></td>
<td id="Ch0.T2.1.7.6.3" class="ltx_td ltx_align_center"><span id="Ch0.T2.1.7.6.3.1" class="ltx_text" style="background-color:#E6E6E6;">17.4%</span></td>
<td id="Ch0.T2.1.7.6.4" class="ltx_td ltx_align_center"><span id="Ch0.T2.1.7.6.4.1" class="ltx_text" style="background-color:#E6E6E6;">0%</span></td>
<td id="Ch0.T2.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r"><span id="Ch0.T2.1.7.6.5.1" class="ltx_text" style="background-color:#E6E6E6;">2.6%</span></td>
<td id="Ch0.T2.1.7.6.6" class="ltx_td ltx_align_center"><span id="Ch0.T2.1.7.6.6.1" class="ltx_text" style="background-color:#E6E6E6;">8.5%</span></td>
</tr>
<tr id="Ch0.T2.1.8.7" class="ltx_tr">
<th id="Ch0.T2.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="Ch0.T2.1.8.7.1.1" class="ltx_text ltx_font_bold">Difference</span></th>
<td id="Ch0.T2.1.8.7.2" class="ltx_td ltx_align_center">+5.9</td>
<td id="Ch0.T2.1.8.7.3" class="ltx_td ltx_align_center">+13.5</td>
<td id="Ch0.T2.1.8.7.4" class="ltx_td ltx_align_center">+12.3</td>
<td id="Ch0.T2.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r">+36.0</td>
<td id="Ch0.T2.1.8.7.6" class="ltx_td ltx_align_center">+1.2</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="Ch0.S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.3.6 </span>Shared Information</h3>

<div id="Ch0.S3.SS6.p1" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS6.p1.1" class="ltx_p">It is possible that shared information between questions could result in higher performance of the network. For example, the questions ”How many oranges are on the table?” and ”What fruit is on the table?” share information about the nature of the objects on the table. This could affect the performance in a way that indicates that the tasks are benefiting from a shared domain where in fact they are benefiting from simply regarding the same object. In order to test the contribution of this performance increase we needed to train the network as a multi-task network but test it as a single task network. This demonstrates that the shared representation learned by the network is functioning to represent the domain well, without questions imparting information to each other. Following the training procedure outlined in the section multi-task Network, we then broke from the testing procedure and split the combined questions back into single examples. We tested those questions individually and compared the results to those attained by testing multiple question types simultaneously.
<br class="ltx_break"></p>
</div>
<div id="Ch0.S3.SS6.p2" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS6.p2.1" class="ltx_p">We found that the difference in performance was minuscule, on average resulting in a 0.01% variation, well within the variation observed between runs of the network. This result also enables us to test the network with the original dataset without reductions and thus we are able to present results which are comparable with the results obtained by other researchers.</p>
</div>
</section>
<section id="Ch0.S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.3.7 </span>Reduced problem set</h3>

<div id="Ch0.S3.SS7.p1" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS7.p1.1" class="ltx_p">The reduced test dataset could contain less complex examples than the original dataset, leading to an inaccurate perception of increased performance. Given that the result from the investigation into the shared information problem above, this allowed us to test over the complete dataset without reduction, we can rule out an artificially boosted performance score as a potential contribution to our results. However, the reduction in the quantity of training data is likely to reduce overall performance, which does go towards explaining the observed difference between results obtained by our experimentation and those reported by the authors of the network we test in the section ”Applicability to other networks”.</p>
</div>
</section>
<section id="Ch0.S3.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">0.3.8 </span>Applicability to other networks</h3>

<div id="Ch0.S3.SS8.p1" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS8.p1.1" class="ltx_p">As we have shown the hypothesised improvement in performance of the multi-task network over the single task network in the simplest case, we then applied the restructured multi-task dataset to a previous competitor for the state of the art. The network chosen was the highest scoring network from VisualQA.org’s 2018 challenge leaderboard<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> which could reasonably be adapted to a multi-task format. Several networks were unsuitable for this task due to the nature of their approach. Specifically, attention based models are particularly problematic for adaptation to a multi-task network format, as this we would require a multi-headed attention implementation which would signifantly increase the computational cost of the network. Due to the current surge in attention based research, this eliminated many potential candidates. We selected ”vqateam_deeper_LSTM_Q_norm_I”<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> with an overall accuracy score of 54.08% as it was the highest scoring network architecture which met the requirement of being sufficiently adaptable. This chosen network performs significantly less well than other networks on the VisualQA leaderboard, however the lack of an attention model makes adapting it to a multi-task architecture much more viable without modifying the network beyond recognition.
<br class="ltx_break">The vqateam network is composed of the VGG19 CNN for image feature extraction in much the same way as the MTL and STL networks described in <a href="#Ch0.S3" title="0.3 Experiments ‣ Visual Question Answering as a Multi-task Problem" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">section 0.3</span></a> (this being a common approach in VQA networks), with natural language processing carried out using a deep LSTM. Both outputs from the CNN and LSTM are then compressed to a common size using a tanh linear layer, before being elementwise multiplied together to combine the features in a manner the authors describe as multimodal. These combined features are then passed through a deep linear network to return a final answer.
<br class="ltx_break">To modify this network for a multi-task format, we passed all four question types into the LSTM separately, then elementwise multiplied the resulting vectors for each question type with the feature vector extracted from the image. We then concatenated all four vectors and fed the total vector into the deep linear classifier as shown in figure 2.</p>
</div>
<figure id="Ch0.F2" class="ltx_figure">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>MTL VQATeam Network Structure. Note that rounded orange rectangles represent sections of the network that are learned during training, while non-rounded blue rectangles indicate fixed weights.</figcaption><img src="/html/2007.01780/assets/vqateamMTL.png" id="Ch0.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="269" alt="Refer to caption">
</figure>
<div id="Ch0.S3.SS8.p2" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS8.p2.1" class="ltx_p">As in the experiments performed above, the hyperparameters were found by Bayesian optimisation over a subset of the training dataset and a validation set which was also selected from the training set.
<br class="ltx_break">The results were as follows:</p>
</div>
<figure id="Ch0.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Table of results showing the accuracy achieved on the modified VisualQA leaderboard network in both single and multi-task mode, and the corresponding difference between single and multi task performance. </figcaption>
<table id="Ch0.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Ch0.T3.1.1.1" class="ltx_tr">
<th id="Ch0.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="6">Visual QA Leaderboard Network Results</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Ch0.T3.1.2.1" class="ltx_tr">
<th id="Ch0.T3.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="Ch0.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T3.1.2.1.2.1" class="ltx_text ltx_font_bold">Colour</span></td>
<td id="Ch0.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T3.1.2.1.3.1" class="ltx_text ltx_font_bold">Count</span></td>
<td id="Ch0.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T3.1.2.1.4.1" class="ltx_text ltx_font_bold">Position</span></td>
<td id="Ch0.T3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Ch0.T3.1.2.1.5.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="Ch0.T3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T3.1.2.1.6.1" class="ltx_text ltx_font_bold">Total</span></td>
</tr>
<tr id="Ch0.T3.1.3.2" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="Ch0.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="Ch0.T3.1.3.2.1.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">MTL COCO-QA</span></th>
<td id="Ch0.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T3.1.3.2.2.1" class="ltx_text" style="background-color:#E6E6E6;">2.1%</span></td>
<td id="Ch0.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T3.1.3.2.3.1" class="ltx_text" style="background-color:#E6E6E6;">39.9%</span></td>
<td id="Ch0.T3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T3.1.3.2.4.1" class="ltx_text" style="background-color:#E6E6E6;">12.0%</span></td>
<td id="Ch0.T3.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Ch0.T3.1.3.2.5.1" class="ltx_text" style="background-color:#E6E6E6;">0.1%</span></td>
<td id="Ch0.T3.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="Ch0.T3.1.3.2.6.1" class="ltx_text" style="background-color:#E6E6E6;">7.1%</span></td>
</tr>
<tr id="Ch0.T3.1.4.3" class="ltx_tr">
<th id="Ch0.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="Ch0.T3.1.4.3.1.1" class="ltx_text ltx_font_bold">STL COCO-QA</span></th>
<td id="Ch0.T3.1.4.3.2" class="ltx_td ltx_align_center">0.1%</td>
<td id="Ch0.T3.1.4.3.3" class="ltx_td ltx_align_center">1.5%</td>
<td id="Ch0.T3.1.4.3.4" class="ltx_td ltx_align_center">12.2%</td>
<td id="Ch0.T3.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">0.0%</td>
<td id="Ch0.T3.1.4.3.6" class="ltx_td ltx_align_center">2.5%</td>
</tr>
<tr id="Ch0.T3.1.5.4" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="Ch0.T3.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="Ch0.T3.1.5.4.1.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">Difference</span></th>
<td id="Ch0.T3.1.5.4.2" class="ltx_td ltx_align_center"><span id="Ch0.T3.1.5.4.2.1" class="ltx_text" style="background-color:#E6E6E6;">+2.0%</span></td>
<td id="Ch0.T3.1.5.4.3" class="ltx_td ltx_align_center"><span id="Ch0.T3.1.5.4.3.1" class="ltx_text" style="background-color:#E6E6E6;">+38.4%</span></td>
<td id="Ch0.T3.1.5.4.4" class="ltx_td ltx_align_center"><span id="Ch0.T3.1.5.4.4.1" class="ltx_text" style="background-color:#E6E6E6;">+12.0%</span></td>
<td id="Ch0.T3.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r"><span id="Ch0.T3.1.5.4.5.1" class="ltx_text" style="background-color:#E6E6E6;">+0.1%</span></td>
<td id="Ch0.T3.1.5.4.6" class="ltx_td ltx_align_center"><span id="Ch0.T3.1.5.4.6.1" class="ltx_text" style="background-color:#E6E6E6;">+4.6%</span></td>
</tr>
<tr id="Ch0.T3.1.6.5" class="ltx_tr">
<th id="Ch0.T3.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="Ch0.T3.1.6.5.1.1" class="ltx_text ltx_font_bold">MTL DAQUAR</span></th>
<td id="Ch0.T3.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t">18.2%</td>
<td id="Ch0.T3.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">30.8%</td>
<td id="Ch0.T3.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t">12.3%</td>
<td id="Ch0.T3.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">38.6%</td>
<td id="Ch0.T3.1.6.5.6" class="ltx_td ltx_align_center ltx_border_t">16.3%</td>
</tr>
<tr id="Ch0.T3.1.7.6" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="Ch0.T3.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="Ch0.T3.1.7.6.1.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">STL DAQUAR</span></th>
<td id="Ch0.T3.1.7.6.2" class="ltx_td ltx_align_center"><span id="Ch0.T3.1.7.6.2.1" class="ltx_text" style="background-color:#E6E6E6;">1.6%</span></td>
<td id="Ch0.T3.1.7.6.3" class="ltx_td ltx_align_center"><span id="Ch0.T3.1.7.6.3.1" class="ltx_text" style="background-color:#E6E6E6;">34.8%</span></td>
<td id="Ch0.T3.1.7.6.4" class="ltx_td ltx_align_center"><span id="Ch0.T3.1.7.6.4.1" class="ltx_text" style="background-color:#E6E6E6;">0.3%</span></td>
<td id="Ch0.T3.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r"><span id="Ch0.T3.1.7.6.5.1" class="ltx_text" style="background-color:#E6E6E6;">1.3%</span></td>
<td id="Ch0.T3.1.7.6.6" class="ltx_td ltx_align_center"><span id="Ch0.T3.1.7.6.6.1" class="ltx_text" style="background-color:#E6E6E6;">10.8%</span></td>
</tr>
<tr id="Ch0.T3.1.8.7" class="ltx_tr">
<th id="Ch0.T3.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="Ch0.T3.1.8.7.1.1" class="ltx_text ltx_font_bold">Difference</span></th>
<td id="Ch0.T3.1.8.7.2" class="ltx_td ltx_align_center">+17.5%</td>
<td id="Ch0.T3.1.8.7.3" class="ltx_td ltx_align_center">-4.0%</td>
<td id="Ch0.T3.1.8.7.4" class="ltx_td ltx_align_center">+12.0%</td>
<td id="Ch0.T3.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r">+37.3%</td>
<td id="Ch0.T3.1.8.7.6" class="ltx_td ltx_align_center">+6.5%</td>
</tr>
</tbody>
</table>
</figure>
<div id="Ch0.S3.SS8.p3" class="ltx_para ltx_noindent">
<p id="Ch0.S3.SS8.p3.1" class="ltx_p">In DAQUAR at least, we observe a significant performance increase over the single task networks, further demonstrating the advantage of treating Visual Question Answering as a multi-task problem, though significantly lower overall accuracy than that reported by the authors of the VisualQA leaderboard network. Performance on COCO-QA is very poor in both cases. Although the multi-task network outperforming the single task network still, neither performed well. We hypothesise that these poor results are the result of the reformatting of the dataset, as the examples that were removed from the datasets are those that do not have multiple questions about the image, and this may be because the image (and thus associated questions) are simpler. Images that contain many objects with many relationships are more likely to have multiple questions about them, while simpler images have fewer questions and so are eliminated from the training set. For reasons which are not clear, this has a significantly greater impact on the VQA Leaderboard network than on the simple network described in <a href="#Ch0.S3.SS5" title="0.3.5 Architecture ‣ 0.3 Experiments ‣ Visual Question Answering as a Multi-task Problem" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection 0.3.5</span></a>. The primary difference between the two networks is the choice of joint-embedding, with the element-wise multiplication of features in the VQA Leaderboard performing much worse in this case than the concatenation of features in the simple baseline network.</p>
</div>
</section>
</section>
<section id="Ch0.S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">0.4 </span>Discussion</h2>

<div id="Ch0.S4.p1" class="ltx_para ltx_noindent">
<p id="Ch0.S4.p1.1" class="ltx_p">Although the observed performance increase between the single and multi-task formulations of the network was small, it is significant. The elimination of the alternate explanations and the performance increase combined lend considerable evidence to the hypothesis that the contribution of a multi-task learning paradigm to the Visual Question Answering problem results in an improved generalisation over the domain that is not obtained by treating each task in the problem set as a monolithic task. Most notably, the experiment detailed in <a href="#Ch0.S3.SS5" title="0.3.5 Architecture ‣ 0.3 Experiments ‣ Visual Question Answering as a Multi-task Problem" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection 0.3.5</span></a> demonstrates quite soundly that this performance increase is not an artefact of the network architecture and is, in fact, a direct result of training over multiple question types simultaneously.
<br class="ltx_break">As in other multi-task research, we found that the networks trained in the multi-task format reached convergence much faster than the networks trained in a single-task format, with both multi-task networks achieving their respective final results in less than 50 epochs, while the single-task networks converge only after 200 epochs.
While neither of the networks presented herein achieved state-of-the-art results, the results we attained clearly show the effect of inter-task interference. As a result, we feel confident in stating that a multi-task approach to Visual Question Answering datasets should be considered, and that future work should take into account the distinct nature of each question type as they relate to one another.</p>
</div>
</section>
<section id="Ch0.S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">0.5 </span>Conclusions</h2>

<div id="Ch0.S5.p1" class="ltx_para ltx_noindent">
<p id="Ch0.S5.p1.1" class="ltx_p">In this paper, we have shown that in the simplest case, Visual Question Answering benefits from being treated as a multi-task problem on both DAQUAR and COCO-QA datasets and show through a process of elimination that this result is in fact due to the increased generalisability of the shared domain. We also demonstrated that this effect is not limited to the simplest case, and while some network architectures are not easily modified into a multi-task architecture, joint-embedding approaches can almost certainly benefit from being so modified. Overall we conclude that the effect of intertask interference should be accounted for in future designs for Visual Question Answering architectures, though ideally in more balanced datasets with many examples per image, as reducing the dataset for such a complex problem has a significant performance impact.
<br class="ltx_break"></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://visualqa.org/roe_2018.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://visualqa.org/roe_2018.html</a>.

</span>
<span class="ltx_bibblock">[Accessed 24 Oct. 2019].

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. [2015]</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 2425–2433, 2015.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caruana [1997]</span>
<span class="ltx_bibblock">
Rich Caruana.

</span>
<span class="ltx_bibblock">Multitask learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Machine learning</em>, 28(1):41–75, 1997.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dozat [2016]</span>
<span class="ltx_bibblock">
Timothy Dozat.

</span>
<span class="ltx_bibblock">Incorporating nesterov momentum into adam.

</span>
<span class="ltx_bibblock">2016.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">French [1999]</span>
<span class="ltx_bibblock">
Robert M French.

</span>
<span class="ltx_bibblock">Catastrophic forgetting in connectionist networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Trends in cognitive sciences</em>, 3(4):128–135, 1999.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurari et al. [2018]</span>
<span class="ltx_bibblock">
Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman,
Jiebo Luo, and Jeffrey P Bigham.

</span>
<span class="ltx_bibblock">Vizwiz grand challenge: Answering visual questions from blind people.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 3608–3617, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2014]</span>
<span class="ltx_bibblock">
Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen.

</span>
<span class="ltx_bibblock">Convolutional neural network architectures for matching natural
language sentences.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, pages
2042–2050, 2014.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Keskar and Socher [2017]</span>
<span class="ltx_bibblock">
Nitish Shirish Keskar and Richard Socher.

</span>
<span class="ltx_bibblock">Improving generalization performance by switching from adam to sgd.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.07628</em>, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kolluru et al. [2017]</span>
<span class="ltx_bibblock">
S. Kolluru, Shreyans Shrimal, and Sudharsan Krishnaswamy.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">CognitiveCam: A Visual Question Answering Application</em>, pages
85–90.

</span>
<span class="ltx_bibblock">11 2017.

</span>
<span class="ltx_bibblock">ISBN 978-981-10-6417-3.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/978-981-10-6418-0˙11</span>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Parikh [2016]</span>
<span class="ltx_bibblock">
Xiao Lin and Devi Parikh.

</span>
<span class="ltx_bibblock">Leveraging visual question answering for image-caption ranking.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 261–277.
Springer, 2016.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2015]</span>
<span class="ltx_bibblock">
Jiasen Lu, Xiao Lin, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Deeper lstm and normalized cnn visual question answering model, 2015.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://github.com/VT-vision-lab/VQA_LSTM_CNN" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/VT-vision-lab/VQA_LSTM_CNN</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2016]</span>
<span class="ltx_bibblock">
Lin Ma, Zhengdong Lu, and Hang Li.

</span>
<span class="ltx_bibblock">Learning to answer questions from image using convolutional neural
network.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Thirtieth AAAI Conference on Artificial Intelligence</em>, 2016.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski and Fritz [2014]</span>
<span class="ltx_bibblock">
Mateusz Malinowski and Mario Fritz.

</span>
<span class="ltx_bibblock">A multi-world approach to question answering about real-world scenes
based on uncertain input.

</span>
<span class="ltx_bibblock">In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q.
Weinberger, editors, <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems
27</em>, pages 1682–1690. Curran Associates, Inc., 2014.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="http://papers.nips.cc/paper/5411-a-multi-world-approach-to-question-answering-about-real-world-scenes-based-on-uncertain-input.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://papers.nips.cc/paper/5411-a-multi-world-approach-to-question-answering-about-real-world-scenes-based-on-uncertain-input.pdf</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski et al. [2015]</span>
<span class="ltx_bibblock">
Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz.

</span>
<span class="ltx_bibblock">Ask your neurons: A neural-based approach to answering questions
about images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 1–9, 2015.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington et al. [2014]</span>
<span class="ltx_bibblock">
Jeffrey Pennington, Richard Socher, and Christopher D Manning.

</span>
<span class="ltx_bibblock">Glove: Global vectors for word representation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 conference on empirical methods in
natural language processing (EMNLP)</em>, pages 1532–1543, 2014.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. [2015]</span>
<span class="ltx_bibblock">
Mengye Ren, Ryan Kiros, and Richard Zemel.

</span>
<span class="ltx_bibblock">Exploring models and data for image question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, pages
2953–2961, 2015.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman [2014]</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.1556</em>, 2014.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tu et al. [2014]</span>
<span class="ltx_bibblock">
Kewei Tu, Meng Meng, Mun Wai Lee, Tae Eun Choe, and Song-Chun Zhu.

</span>
<span class="ltx_bibblock">Joint video and text parsing for understanding events and answering
queries.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE MultiMedia</em>, 21(2):42–70, 2014.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu and Saenko [2016]</span>
<span class="ltx_bibblock">
Huijuan Xu and Kate Saenko.

</span>
<span class="ltx_bibblock">Ask, attend and answer: Exploring question-guided spatial attention
for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 451–466.
Springer, 2016.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2016]</span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola.

</span>
<span class="ltx_bibblock">Stacked attention networks for image question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 21–29, 2016.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2015]</span>
<span class="ltx_bibblock">
Licheng Yu, Eunbyung Park, Alexander C Berg, and Tamara L Berg.

</span>
<span class="ltx_bibblock">Visual madlibs: Fill in the blank image generation and question
answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1506.00278</em>, 2015.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2016]</span>
<span class="ltx_bibblock">
Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Yin and yang: Balancing and answering binary visual questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, June 2016.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2016]</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Visual7w: Grounded question answering in images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, June 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zitnick et al. [2016]</span>
<span class="ltx_bibblock">
C Lawrence Zitnick, Aishwarya Agrawal, Stanislaw Antol, Margaret Mitchell,
Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Measuring machine intelligence through visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">AI Magazine</em>, 37(1):63–72, 2016.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2007.01779" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2007.01780" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2007.01780">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2007.01780" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2007.01781" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 18 15:25:44 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
