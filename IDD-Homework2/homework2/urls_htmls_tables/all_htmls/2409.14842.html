<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks</title>
<!--Generated on Tue Oct  8 09:33:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="CCMT 2024 NMT Transformer LLM APE." lang="en" name="keywords"/>
<base href="/html/2409.14842v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S1" title="In HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S2" title="In HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S2.SS1" title="In 2 Dataset ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data Size</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S2.SS2" title="In 2 Dataset ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Data Pre-processing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S3" title="In HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>NMT System</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S3.SS1" title="In 3 NMT System ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>System Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S3.SS2" title="In 3 NMT System ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Regularized Dropout</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S3.SS3" title="In 3 NMT System ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Bidirectional Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S3.SS4" title="In 3 NMT System ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Data Diversification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S3.SS5" title="In 3 NMT System ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Forward Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S3.SS6" title="In 3 NMT System ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Back-Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S3.SS7" title="In 3 NMT System ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.7 </span>Alternated Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S3.SS8" title="In 3 NMT System ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.8 </span>Curriculum Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S3.SS9" title="In 3 NMT System ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.9 </span>Transductive Ensemble Learning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S4" title="In HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>APE System</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S4.SS1" title="In 4 APE System ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>System Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S4.SS2" title="In 4 APE System ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Efficient LLM Finetuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S4.SS3" title="In 4 APE System ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>HypoTranslate Dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S5" title="In HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S5.SS1" title="In 5 Experiments ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S5.SS2" title="In 5 Experiments ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Bilingual MT Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S5.SS2.SSS1" title="In 5.2 Bilingual MT Results ‣ 5 Experiments ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>en<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>zh &amp; zh<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>en</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S5.SS2.SSS2" title="In 5.2 Bilingual MT Results ‣ 5 Experiments ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>mn<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>zh, ti<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>zh &amp; uy<math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math>zh</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S5.SS3" title="In 5 Experiments ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Multi-domain MT Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S6" title="In HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Huawei Translation Service Center, Beijing, China
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id1.1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{wuzhanglin2,luoyuanchang1,weidaimeng,zhengjiawei15,
<br class="ltx_break"/>weibin29,lizongyao,shanghengchao,guojiaxin1,lishaojun18,
<br class="ltx_break"/>zhangweidong17,nicolas.xie,yanghao30}@huawei.com</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhanglin Wu
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-2920-0773" title="ORCID identifier">0000-0002-2920-0773</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuanchang Luo
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Daimeng Wei
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Jiawei Zheng
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Bin Wei
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Zongyao Li
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Hengchao Shang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Jiaxin Guo
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Shaojun Li
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Weidong Zhang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Ning Xie
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Hao Yang
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">This paper presents the submission of Huawei Translation Services Center (HW-TSC) to machine translation tasks of the 20th China Conference on Machine Translation (CCMT 2024). We participate in the bilingual machine translation task and multi-domain machine translation task. For these two translation tasks, we use training strategies such as regularized dropout, bidirectional training, data diversification, forward translation, back translation, alternated training, curriculum learning, and transductive ensemble learning to train neural machine translation (NMT) models based on the deep Transformer-big architecture. Furthermore, to explore whether large language model (LLM) can effectively improve the translation quality of NMT models, we use supervised fine-tuning (SFT) to train llama2-13b as an Automatic post-editing (APE) model to improve the translation results of the NMT model on the multi-domain machine translation task. By using these plyometric strategies, our submission achieves a competitive result in the final evaluation.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>CCMT 2024 NMT Transformer LLM APE.
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Neural machine translation (NMT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib3" title="">3</a>]</cite> allows translation systems to be trained end-to-end without dealing with issues like word alignment, translation rules, and complex decoding algorithms that characterize statistical machine translation (SMT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib4" title="">4</a>]</cite> systems. Although NMT develops rapidly in recent years, it relies heavily on big data - large-scale, high-quality bilingual corpora. Due to the cost and scarcity of real corpora, synthetic data plays an important role in improving translation quality. Existing methods for synthesizing data in NMT focus on leveraging monolingual data during training. Among them, data diversification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib5" title="">5</a>]</cite>, forward translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib6" title="">6</a>]</cite>, and back translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib7" title="">7</a>]</cite> are widely used to generate synthetic bilingual corpora. Such synthetic data can be used to improve the performance of NMT models. Although synthetic data is efficient, it inevitably contains noise and erroneous translations. Alternated training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib8" title="">8</a>]</cite> introduces real data as a guide and alternately uses synthetic data and real data during the training process, which can prevent the training of the NMT model from being interfered with by noisy synthetic data. Another direction to improve the performance of NMT models is to use more efficient training strategies. Methods such as regularized dropout <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib9" title="">9</a>]</cite>, bidirectional training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib10" title="">10</a>]</cite>, and curriculum learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib11" title="">11</a>]</cite> allow the NMT model to more effectively utilize limited data during the training process, while transductive ensemble learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib12" title="">12</a>]</cite> can aggregate the translation capabilities of multiple models into one model.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The powerful capabilities of LLM in logical reasoning and language generation promote the further development of machine translation. Despite the superior performance of translation models, existing models usually use beam search decoding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib13" title="">13</a>]</cite> and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in various N-best hypotheses, making them suboptimal for translation tasks that require a single high-quality output sequence. GenTranslate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib14" title="">14</a>]</cite> utilizes the rich language knowledge and powerful reasoning capabilities of large language model (LLM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib18" title="">18</a>]</cite> to integrate the rich information in the N-best list from the basic model, generating higher-quality translation results.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To promote academic exchanges and connections between domestic and foreign research units and relevant industry partners, and to jointly advance the development of machine translation research and technology, we participate in the bilingual machine translation tasks and multi-domain machine translation tasks organized by CCMT2024. For these two translation tasks, we use training strategies such as regularized dropout <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib9" title="">9</a>]</cite>, bidirectional training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib10" title="">10</a>]</cite>, data diversification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib5" title="">5</a>]</cite>, forward translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib6" title="">6</a>]</cite>, back translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib7" title="">7</a>]</cite>, alternated training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib8" title="">8</a>]</cite>, curriculum learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib11" title="">11</a>]</cite>, and transductive ensemble learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib12" title="">12</a>]</cite> to train neural machine translation (NMT) models based on the deep Transformer-big architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib22" title="">22</a>]</cite>. Additionally, drawing inspiration from the GenTranslate method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib14" title="">14</a>]</cite>, we utilize supervised fine-tuning (SFT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib23" title="">23</a>]</cite> to train llama2-13b<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref" href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf" title="">https://huggingface.co/meta-llama/Llama-2-13b-chat-hf</a></span></span></span> as an automatic post-editing (APE) model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib24" title="">24</a>]</cite>, aimed at enhancing the translation outputs of NMT models in the multi-domain machine translation task.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">This paper expands on the details of our translation system in different translation tasks. The structure of the remaining sections is as follows: Section 2 describes the data size and data pre-processing; Section 3 provides an overview of our NMT system; Section 4 explains our APE system; Section 5 presents the parameter settings and experimental results; Section 6 summarizes our systems.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Dataset</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Size</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.12">According to the requirements of the CCMT 2024 outline, we train the NMT system from scratch on the bilingual translation task using the officially provided data. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S2.T1" title="Table 1 ‣ 2.1 Data Size ‣ 2 Dataset ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_tag">1</span></a> shows the training data size for each language pair of the bilingual MT task after data pre-processing. These language pairs include English<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mo id="S2.SS1.p1.1.m1.1.1" stretchy="false" xref="S2.SS1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">→</annotation></semantics></math>Chinese (en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><mo id="S2.SS1.p1.2.m2.1.1" stretchy="false" xref="S2.SS1.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">→</annotation></semantics></math>zh), Chinese<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.1"><semantics id="S2.SS1.p1.3.m3.1a"><mo id="S2.SS1.p1.3.m3.1.1" stretchy="false" xref="S2.SS1.p1.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">→</annotation></semantics></math>English (zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS1.p1.4.m4.1"><semantics id="S2.SS1.p1.4.m4.1a"><mo id="S2.SS1.p1.4.m4.1.1" stretchy="false" xref="S2.SS1.p1.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.1d">→</annotation></semantics></math>en), Mongolian<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS1.p1.5.m5.1"><semantics id="S2.SS1.p1.5.m5.1a"><mo id="S2.SS1.p1.5.m5.1.1" stretchy="false" xref="S2.SS1.p1.5.m5.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.5.m5.1d">→</annotation></semantics></math>Chinese ( mn<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS1.p1.6.m6.1"><semantics id="S2.SS1.p1.6.m6.1a"><mo id="S2.SS1.p1.6.m6.1.1" stretchy="false" xref="S2.SS1.p1.6.m6.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><ci id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.6.m6.1d">→</annotation></semantics></math>zh), Tibetan<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS1.p1.7.m7.1"><semantics id="S2.SS1.p1.7.m7.1a"><mo id="S2.SS1.p1.7.m7.1.1" stretchy="false" xref="S2.SS1.p1.7.m7.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><ci id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.7.m7.1d">→</annotation></semantics></math>Chinese (ti<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS1.p1.8.m8.1"><semantics id="S2.SS1.p1.8.m8.1a"><mo id="S2.SS1.p1.8.m8.1.1" stretchy="false" xref="S2.SS1.p1.8.m8.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><ci id="S2.SS1.p1.8.m8.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.8.m8.1d">→</annotation></semantics></math>zh), and Uyghur<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS1.p1.9.m9.1"><semantics id="S2.SS1.p1.9.m9.1a"><mo id="S2.SS1.p1.9.m9.1.1" stretchy="false" xref="S2.SS1.p1.9.m9.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.9.m9.1b"><ci id="S2.SS1.p1.9.m9.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.9.m9.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.9.m9.1d">→</annotation></semantics></math>Chinese (uy<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS1.p1.10.m10.1"><semantics id="S2.SS1.p1.10.m10.1a"><mo id="S2.SS1.p1.10.m10.1.1" stretchy="false" xref="S2.SS1.p1.10.m10.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.10.m10.1b"><ci id="S2.SS1.p1.10.m10.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.10.m10.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.10.m10.1d">→</annotation></semantics></math>zh). It should be noted that in the en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS1.p1.11.m11.1"><semantics id="S2.SS1.p1.11.m11.1a"><mo id="S2.SS1.p1.11.m11.1.1" stretchy="false" xref="S2.SS1.p1.11.m11.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.11.m11.1b"><ci id="S2.SS1.p1.11.m11.1.1.cmml" xref="S2.SS1.p1.11.m11.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.11.m11.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.11.m11.1d">→</annotation></semantics></math>zh and zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS1.p1.12.m12.1"><semantics id="S2.SS1.p1.12.m12.1a"><mo id="S2.SS1.p1.12.m12.1.1" stretchy="false" xref="S2.SS1.p1.12.m12.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.12.m12.1b"><ci id="S2.SS1.p1.12.m12.1.1.cmml" xref="S2.SS1.p1.12.m12.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.12.m12.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.12.m12.1d">→</annotation></semantics></math>en translation tasks, since the training data of WMT 2024 is shared with CCMT 2024, we additionally use the training data provided by the WMT 2024 general MT task to scale up the training data.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Data size for each bilingual MT task after data pre-processing</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.5.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.5.5.5">
<th class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_t" id="S2.T1.5.5.5.6"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S2.T1.5.5.5.7"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S2.T1.5.5.5.8"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.1.1.1">en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S2.T1.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.m1.1d">→</annotation></semantics></math>zh</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.2.2.2.2">zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T1.2.2.2.2.m1.1"><semantics id="S2.T1.2.2.2.2.m1.1a"><mo id="S2.T1.2.2.2.2.m1.1.1" stretchy="false" xref="S2.T1.2.2.2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.2.m1.1b"><ci id="S2.T1.2.2.2.2.m1.1.1.cmml" xref="S2.T1.2.2.2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.2.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.2.2.m1.1d">→</annotation></semantics></math>en</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.3.3.3.3">mn<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T1.3.3.3.3.m1.1"><semantics id="S2.T1.3.3.3.3.m1.1a"><mo id="S2.T1.3.3.3.3.m1.1.1" stretchy="false" xref="S2.T1.3.3.3.3.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.3.m1.1b"><ci id="S2.T1.3.3.3.3.m1.1.1.cmml" xref="S2.T1.3.3.3.3.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.3.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.3.3.3.3.m1.1d">→</annotation></semantics></math>zh</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.4.4.4.4">ti<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T1.4.4.4.4.m1.1"><semantics id="S2.T1.4.4.4.4.m1.1a"><mo id="S2.T1.4.4.4.4.m1.1.1" stretchy="false" xref="S2.T1.4.4.4.4.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.4.4.m1.1b"><ci id="S2.T1.4.4.4.4.m1.1.1.cmml" xref="S2.T1.4.4.4.4.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.4.4.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.4.4.4.4.m1.1d">→</annotation></semantics></math>zh</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.5.5.5.5">uy<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T1.5.5.5.5.m1.1"><semantics id="S2.T1.5.5.5.5.m1.1a"><mo id="S2.T1.5.5.5.5.m1.1.1" stretchy="false" xref="S2.T1.5.5.5.5.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.5.5.m1.1b"><ci id="S2.T1.5.5.5.5.m1.1.1.cmml" xref="S2.T1.5.5.5.5.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.5.5.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.5.5.5.5.m1.1d">→</annotation></semantics></math>zh</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.5.5.6.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_t" id="S2.T1.5.5.6.1.1">Bilingual</td>
<td class="ltx_td ltx_border_t" id="S2.T1.5.5.6.1.2"></td>
<td class="ltx_td ltx_border_t" id="S2.T1.5.5.6.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.5.5.6.1.4">25.12M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.5.5.6.1.5">25.12M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.5.5.6.1.6">1.24M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.5.5.6.1.7">0.97M</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.5.5.6.1.8">0.16M</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.5.7.2">
<td class="ltx_td ltx_align_left ltx_border_l" id="S2.T1.5.5.7.2.1">Source Monolingual</td>
<td class="ltx_td" id="S2.T1.5.5.7.2.2"></td>
<td class="ltx_td" id="S2.T1.5.5.7.2.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.5.5.7.2.4">50M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.5.5.7.2.5">50M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.5.5.7.2.6">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.5.5.7.2.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.5.5.7.2.8">-</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.5.8.3">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l" id="S2.T1.5.5.8.3.1">Target Monolingual</td>
<td class="ltx_td ltx_border_b" id="S2.T1.5.5.8.3.2"></td>
<td class="ltx_td ltx_border_b" id="S2.T1.5.5.8.3.3"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S2.T1.5.5.8.3.4">50M</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S2.T1.5.5.8.3.5">50M</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S2.T1.5.5.8.3.6">4.89M</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S2.T1.5.5.8.3.7">4.89M</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S2.T1.5.5.8.3.8">4.89M</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S2.T2" title="Table 2 ‣ 2.1 Data Size ‣ 2 Dataset ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_tag">2</span></a> shows the training data size for the multi-domain machine translation task after data preprocessing. For this zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS1.p2.1.m1.1"><semantics id="S2.SS1.p2.1.m1.1a"><mo id="S2.SS1.p2.1.m1.1.1" stretchy="false" xref="S2.SS1.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">→</annotation></semantics></math>en task, the official does not provide any training data, but allows participating units to construct their own training data. Therefore, we use the training data from the bilingual translation task and also collect 33.36 million high-quality domain-related bilingual data.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Data size for multi-domain MT task after data pre-processing</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.1.1.2.1">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.1.2.1.1"></td>
<td class="ltx_td ltx_border_t" id="S2.T2.1.1.2.1.2"></td>
<td class="ltx_td ltx_border_t" id="S2.T2.1.1.2.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T2.1.1.2.1.4">Bilingual</td>
<td class="ltx_td ltx_border_t" id="S2.T2.1.1.2.1.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T2.1.1.2.1.6">Source Monolingual</td>
<td class="ltx_td ltx_border_t" id="S2.T2.1.1.2.1.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T2.1.1.2.1.8">Target Monolingual</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S2.T2.1.1.1.1">zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.T2.1.1.1.1.m1.1"><semantics id="S2.T2.1.1.1.1.m1.1a"><mo id="S2.T2.1.1.1.1.m1.1.1" stretchy="false" xref="S2.T2.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.1.m1.1b"><ci id="S2.T2.1.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T2.1.1.1.1.m1.1d">→</annotation></semantics></math>en</td>
<td class="ltx_td ltx_border_b ltx_border_t" id="S2.T2.1.1.1.2"></td>
<td class="ltx_td ltx_border_b ltx_border_t" id="S2.T2.1.1.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.1.1.1.4">58.48M</td>
<td class="ltx_td ltx_border_b ltx_border_t" id="S2.T2.1.1.1.5"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.1.1.1.6">50M</td>
<td class="ltx_td ltx_border_b ltx_border_t" id="S2.T2.1.1.1.7"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.1.1.1.8">50M</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data Pre-processing</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The data pre-processing process is as follows:</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">Remove duplicate sentences or sentence pairs.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">Remove invisible characters and xml escape characters.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">Convert full-width symbols to half-width symbols.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1">Use jieba<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref" href="https://github.com/fxsjy/jieba" title="">https://github.com/fxsjy/jieba</a></span></span></span> to pre-segment Chinese sentences.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i5.p1">
<p class="ltx_p" id="S2.I1.i5.p1.1">Use mosesdecoder<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref" href="https://github.com/moses-smt/mosesdecoder" title="">https://github.com/moses-smt/mosesdecoder</a></span></span></span> to normalize English punctuation.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i6.p1">
<p class="ltx_p" id="S2.I1.i6.p1.1">Use opencc<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref" href="https://github.com/BYVoid/OpenCC" title="">https://github.com/BYVoid/OpenCC</a></span></span></span> to convert traditional Chinese to simplified Chinese.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i7.p1">
<p class="ltx_p" id="S2.I1.i7.p1.1">Use fasttext<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref" href="https://github.com/facebookresearch/fastText" title="">https://github.com/facebookresearch/fastText</a></span></span></span> to filter other language sentences.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i8.p1">
<p class="ltx_p" id="S2.I1.i8.p1.1">Use fast_align<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref" href="https://github.com/clab/fast_align" title="">https://github.com/clab/fast_align</a></span></span></span> to filter poorly aligned sentence pairs.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i9.p1">
<p class="ltx_p" id="S2.I1.i9.p1.1">Filter out sentences with more than 150 tokens in bilingual data.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i10.p1">
<p class="ltx_p" id="S2.I1.i10.p1.1">Split long sentences in monolingual data into multiple short sentences.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i11" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i11.p1">
<p class="ltx_p" id="S2.I1.i11.p1.1">Filter out sentence pairs with token ratio greater than 4 or less than 0.25.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i12" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i12.p1">
<p class="ltx_p" id="S2.I1.i12.p1.5">When performing subword segmentation, joint Byte Pair Encoding<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref" href="https://github.com/soaxelbrooke/python-bpe" title="">https://github.com/soaxelbrooke/python-bpe</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib25" title="">25</a>]</cite> is used for mn<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.I1.i12.p1.1.m1.1"><semantics id="S2.I1.i12.p1.1.m1.1a"><mo id="S2.I1.i12.p1.1.m1.1.1" stretchy="false" xref="S2.I1.i12.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.I1.i12.p1.1.m1.1b"><ci id="S2.I1.i12.p1.1.m1.1.1.cmml" xref="S2.I1.i12.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i12.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i12.p1.1.m1.1d">→</annotation></semantics></math>zh, ti<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.I1.i12.p1.2.m2.1"><semantics id="S2.I1.i12.p1.2.m2.1a"><mo id="S2.I1.i12.p1.2.m2.1.1" stretchy="false" xref="S2.I1.i12.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.I1.i12.p1.2.m2.1b"><ci id="S2.I1.i12.p1.2.m2.1.1.cmml" xref="S2.I1.i12.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i12.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i12.p1.2.m2.1d">→</annotation></semantics></math>zh and uy<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.I1.i12.p1.3.m3.1"><semantics id="S2.I1.i12.p1.3.m3.1a"><mo id="S2.I1.i12.p1.3.m3.1.1" stretchy="false" xref="S2.I1.i12.p1.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.I1.i12.p1.3.m3.1b"><ci id="S2.I1.i12.p1.3.m3.1.1.cmml" xref="S2.I1.i12.p1.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i12.p1.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i12.p1.3.m3.1d">→</annotation></semantics></math>zh translation tasks, and joint sentencepiece<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref" href="https://github.com/google/sentencepiece" title="">https://github.com/google/sentencepiece</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib26" title="">26</a>]</cite> is used for zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.I1.i12.p1.4.m4.1"><semantics id="S2.I1.i12.p1.4.m4.1a"><mo id="S2.I1.i12.p1.4.m4.1.1" stretchy="false" xref="S2.I1.i12.p1.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.I1.i12.p1.4.m4.1b"><ci id="S2.I1.i12.p1.4.m4.1.1.cmml" xref="S2.I1.i12.p1.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i12.p1.4.m4.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i12.p1.4.m4.1d">→</annotation></semantics></math>en and en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.I1.i12.p1.5.m5.1"><semantics id="S2.I1.i12.p1.5.m5.1a"><mo id="S2.I1.i12.p1.5.m5.1.1" stretchy="false" xref="S2.I1.i12.p1.5.m5.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.I1.i12.p1.5.m5.1b"><ci id="S2.I1.i12.p1.5.m5.1.1.cmml" xref="S2.I1.i12.p1.5.m5.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i12.p1.5.m5.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i12.p1.5.m5.1d">→</annotation></semantics></math>zh translation tasks.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>NMT System</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>System Overview</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Transformer is the state-of-the-art model structure in recent MT evaluations. There are two parts of research to improve this kind: the first part uses wide networks (eg: Transformer-Big <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib27" title="">27</a>]</cite>), and the other part uses deeper language representations (eg: Deep Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib28" title="">28</a>]</cite>). For all MT tasks, we combine these two improvements, adopting the Deep Transformer-Big <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib22" title="">22</a>]</cite> model structure to train the NMT system. Deep Transformer-Big uses pre-layer normalization, features 25-layer encoder, 6-layer decoder, 16-heads self-attention, 1024-dimensional word embedding and 4096-dimensional ffn embedding.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="281" id="S3.F1.g1" src="x1.png" width="436"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The overall training flow chart of our NMT system.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.5">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S3.F1" title="Figure 1 ‣ 3.1 System Overview ‣ 3 NMT System ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_tag">1</span></a> shows the overall training flow chart of our NMT system on the bilingual machine translation task and multi-domain machine translation task. We use training strategies such as regularized dropout <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib9" title="">9</a>]</cite>, bidirectional training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib10" title="">10</a>]</cite>, data diversification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib5" title="">5</a>]</cite>, forward translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib6" title="">6</a>]</cite>, back translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib7" title="">7</a>]</cite>, alternated training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib8" title="">8</a>]</cite>, curriculum learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib11" title="">11</a>]</cite>, and transductive ensemble learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib12" title="">12</a>]</cite> to train NMT models based on the deep Transformer-big architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib21" title="">21</a>]</cite>. Since forward translation relies on source monolingual and mn<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mo id="S3.SS1.p2.1.m1.1.1" stretchy="false" xref="S3.SS1.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">→</annotation></semantics></math>zh, ti<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mo id="S3.SS1.p2.2.m2.1.1" stretchy="false" xref="S3.SS1.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">→</annotation></semantics></math>zh and uy<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mo id="S3.SS1.p2.3.m3.1.1" stretchy="false" xref="S3.SS1.p2.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">→</annotation></semantics></math>zh translation tasks do not provide source monolingual, we do not use forward translation on these three tasks. Furthermore, our choice of back translation methods varies across different tasks. For en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mo id="S3.SS1.p2.4.m4.1.1" stretchy="false" xref="S3.SS1.p2.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">→</annotation></semantics></math>zh and zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><mo id="S3.SS1.p2.5.m5.1.1" stretchy="false" xref="S3.SS1.p2.5.m5.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">→</annotation></semantics></math>en tasks where forward translation is available, we use sampling back translation (ST) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib29" title="">29</a>]</cite>, and for other tasks we use tagged back translation (Tagged BT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib30" title="">30</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Regularized Dropout</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Dropout <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib31" title="">31</a>]</cite> is a widely used technique for regularizing deep neural network training, which is crucial to prevent over-fitting and improve the generalization ability of deep models. Dropout performs implicit ensemble by simply dropping a certain proportion of hidden units from the neural network during training, which may cause an unnegligible inconsistency between training and inference. Regularized Dropout<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref" href="https://github.com/dropreg/R-Drop" title="">https://github.com/dropreg/R-Drop</a></span></span></span> (R-Drop) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib9" title="">9</a>]</cite> is a simple yet more effective alternative to regularize the training inconsistency induced by dropout. Concretely, in each mini-batch training, each data sample goes through the forward pass twice, and each pass is processed by a different sub model by randomly dropping out some hidden units. R-Drop forces the two distributions for the same data sample outputted by the two sub models to be consistent with each other, through minimizing the bidirectional Kullback-Leibler (KL) divergence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib32" title="">32</a>]</cite> between the two distributions. In this way, the inconsistency between the training and inference stage can be alleviated.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Bidirectional Training</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.2">Many studies have shown that pre-training can transfer the knowledge and data distribution, hence improving the generalization. Bidirectional training (BiT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib10" title="">10</a>]</cite> happens to be a simple and effective pre-training method to improve the translation quality of NMT models. Bidirectional training is divided into two stages, the early stage bidirectionally updates model parameters, and then tune the model normally. To achieve bidirectional updating, we only need to reconstruct the training samples from "src<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mo id="S3.SS3.p1.1.m1.1.1" stretchy="false" xref="S3.SS3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">→</annotation></semantics></math>tgt" to "src+tgt<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mo id="S3.SS3.p1.2.m2.1.1" stretchy="false" xref="S3.SS3.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">→</annotation></semantics></math>tgt+src" without any complicated model modifications. Notably, BiT does not increase any parameters or training steps, requiring the parallel data merely.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Data Diversification</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Data Diversification (DD) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib5" title="">5</a>]</cite> is a data augmentation method to boost NMT performance. It diversifies the training data by using the predictions of multiple forward and backward models and then merging them with the original dataset on which the final NMT model is trained. DD is applicable to all NMT models. It does not require extra monolingual data, nor does it add more computations and parameters. To conserve training resources, we only use one forward model and one backward model when using DD.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Forward Translation</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Forward translation (FT), also known as self-training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib6" title="">6</a>]</cite>, is one of the most commonly used data augmentation methods. FT has proven effective for improving NMT performance by augmenting model training with synthetic and authentic parallel data. Generally, FT is performed in three steps: (1) randomly sample a subset from the large-scale source monolingual data; (2) use a “teacher” NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a “student” NMT model.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Back-Translation</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">An effective method to improve NMT with target monolingual data is to augment the parallel training data with back translation (BT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib40" title="">40</a>]</cite>. There are many works broaden the understanding of BT and investigates a number of methods to generate synthetic source sentences. Edunov et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib29" title="">29</a>]</cite> find that back translations obtained via sampling or noised beam outputs are more effective than back translations generated by beam or greedy search in most scenarios. Caswell et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib30" title="">30</a>]</cite> show that the main role of such noised beam outputs is not to diversify the source side, but simply to indicate to the model that the given source is synthetic. Therefore, they propose a simpler technique, Tagged BT. This method uses an extra token to mark back translated source sentences, which is generally outperform than noised BT.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>Alternated Training</h3>
<div class="ltx_para" id="S3.SS7.p1">
<p class="ltx_p" id="S3.SS7.p1.1">While synthetic bilingual data have demonstrated their effectiveness in NMT, adding more synthetic data often deteriorates translation performance since the synthetic data inevitably contains noise and erroneous translations. Alternated training (AT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib8" title="">8</a>]</cite> introduce authentic data as guidance to prevent the training of NMT models from being disturbed by noisy synthetic data. AT describes the synthetic and authentic data as two types of different approximations for the distribution of infinite authentic data, and its basic idea is to alternate synthetic and authentic data iteratively during training until the model converges.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.8 </span>Curriculum Learning</h3>
<div class="ltx_para" id="S3.SS8.p1">
<p class="ltx_p" id="S3.SS8.p1.2">A practical curriculum learning (CL) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib33" title="">33</a>]</cite> method should address two main questions: how to rank the training examples, and how to modify the sampling procedure based on this ranking. For ranking, we choose to estimate the difficulty of training samples according to their domain feature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib11" title="">11</a>]</cite>. The calculation formula of domain feature is as follows, where <math alttext="\theta_{in}" class="ltx_Math" display="inline" id="S3.SS8.p1.1.m1.1"><semantics id="S3.SS8.p1.1.m1.1a"><msub id="S3.SS8.p1.1.m1.1.1" xref="S3.SS8.p1.1.m1.1.1.cmml"><mi id="S3.SS8.p1.1.m1.1.1.2" xref="S3.SS8.p1.1.m1.1.1.2.cmml">θ</mi><mrow id="S3.SS8.p1.1.m1.1.1.3" xref="S3.SS8.p1.1.m1.1.1.3.cmml"><mi id="S3.SS8.p1.1.m1.1.1.3.2" xref="S3.SS8.p1.1.m1.1.1.3.2.cmml">i</mi><mo id="S3.SS8.p1.1.m1.1.1.3.1" xref="S3.SS8.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS8.p1.1.m1.1.1.3.3" xref="S3.SS8.p1.1.m1.1.1.3.3.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS8.p1.1.m1.1b"><apply id="S3.SS8.p1.1.m1.1.1.cmml" xref="S3.SS8.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS8.p1.1.m1.1.1.1.cmml" xref="S3.SS8.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS8.p1.1.m1.1.1.2.cmml" xref="S3.SS8.p1.1.m1.1.1.2">𝜃</ci><apply id="S3.SS8.p1.1.m1.1.1.3.cmml" xref="S3.SS8.p1.1.m1.1.1.3"><times id="S3.SS8.p1.1.m1.1.1.3.1.cmml" xref="S3.SS8.p1.1.m1.1.1.3.1"></times><ci id="S3.SS8.p1.1.m1.1.1.3.2.cmml" xref="S3.SS8.p1.1.m1.1.1.3.2">𝑖</ci><ci id="S3.SS8.p1.1.m1.1.1.3.3.cmml" xref="S3.SS8.p1.1.m1.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p1.1.m1.1c">\theta_{in}</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.p1.1.m1.1d">italic_θ start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT</annotation></semantics></math> represents an in-domain NMT model, and <math alttext="\theta_{out}" class="ltx_Math" display="inline" id="S3.SS8.p1.2.m2.1"><semantics id="S3.SS8.p1.2.m2.1a"><msub id="S3.SS8.p1.2.m2.1.1" xref="S3.SS8.p1.2.m2.1.1.cmml"><mi id="S3.SS8.p1.2.m2.1.1.2" xref="S3.SS8.p1.2.m2.1.1.2.cmml">θ</mi><mrow id="S3.SS8.p1.2.m2.1.1.3" xref="S3.SS8.p1.2.m2.1.1.3.cmml"><mi id="S3.SS8.p1.2.m2.1.1.3.2" xref="S3.SS8.p1.2.m2.1.1.3.2.cmml">o</mi><mo id="S3.SS8.p1.2.m2.1.1.3.1" xref="S3.SS8.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS8.p1.2.m2.1.1.3.3" xref="S3.SS8.p1.2.m2.1.1.3.3.cmml">u</mi><mo id="S3.SS8.p1.2.m2.1.1.3.1a" xref="S3.SS8.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS8.p1.2.m2.1.1.3.4" xref="S3.SS8.p1.2.m2.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS8.p1.2.m2.1b"><apply id="S3.SS8.p1.2.m2.1.1.cmml" xref="S3.SS8.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS8.p1.2.m2.1.1.1.cmml" xref="S3.SS8.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS8.p1.2.m2.1.1.2.cmml" xref="S3.SS8.p1.2.m2.1.1.2">𝜃</ci><apply id="S3.SS8.p1.2.m2.1.1.3.cmml" xref="S3.SS8.p1.2.m2.1.1.3"><times id="S3.SS8.p1.2.m2.1.1.3.1.cmml" xref="S3.SS8.p1.2.m2.1.1.3.1"></times><ci id="S3.SS8.p1.2.m2.1.1.3.2.cmml" xref="S3.SS8.p1.2.m2.1.1.3.2">𝑜</ci><ci id="S3.SS8.p1.2.m2.1.1.3.3.cmml" xref="S3.SS8.p1.2.m2.1.1.3.3">𝑢</ci><ci id="S3.SS8.p1.2.m2.1.1.3.4.cmml" xref="S3.SS8.p1.2.m2.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p1.2.m2.1c">\theta_{out}</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.p1.2.m2.1d">italic_θ start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT</annotation></semantics></math> represents a out-of-domain NMT model.</p>
</div>
<div class="ltx_para" id="S3.SS8.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="q(x,y)=\frac{\log{P(y|x;\theta_{in})}-\log{P(y|x;\theta_{out})}}{|y|}" class="ltx_Math" display="block" id="S3.E1.m1.7"><semantics id="S3.E1.m1.7a"><mrow id="S3.E1.m1.7.8" xref="S3.E1.m1.7.8.cmml"><mrow id="S3.E1.m1.7.8.2" xref="S3.E1.m1.7.8.2.cmml"><mi id="S3.E1.m1.7.8.2.2" xref="S3.E1.m1.7.8.2.2.cmml">q</mi><mo id="S3.E1.m1.7.8.2.1" xref="S3.E1.m1.7.8.2.1.cmml">⁢</mo><mrow id="S3.E1.m1.7.8.2.3.2" xref="S3.E1.m1.7.8.2.3.1.cmml"><mo id="S3.E1.m1.7.8.2.3.2.1" stretchy="false" xref="S3.E1.m1.7.8.2.3.1.cmml">(</mo><mi id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml">x</mi><mo id="S3.E1.m1.7.8.2.3.2.2" xref="S3.E1.m1.7.8.2.3.1.cmml">,</mo><mi id="S3.E1.m1.7.7" xref="S3.E1.m1.7.7.cmml">y</mi><mo id="S3.E1.m1.7.8.2.3.2.3" stretchy="false" xref="S3.E1.m1.7.8.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.7.8.1" xref="S3.E1.m1.7.8.1.cmml">=</mo><mfrac id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml"><mrow id="S3.E1.m1.4.4.4" xref="S3.E1.m1.4.4.4.cmml"><mrow id="S3.E1.m1.3.3.3.3" xref="S3.E1.m1.3.3.3.3.cmml"><mrow id="S3.E1.m1.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.cmml"><mi id="S3.E1.m1.3.3.3.3.3.1" xref="S3.E1.m1.3.3.3.3.3.1.cmml">log</mi><mo id="S3.E1.m1.3.3.3.3.3a" lspace="0.167em" xref="S3.E1.m1.3.3.3.3.3.cmml">⁡</mo><mi id="S3.E1.m1.3.3.3.3.3.2" xref="S3.E1.m1.3.3.3.3.3.2.cmml">P</mi></mrow><mo id="S3.E1.m1.3.3.3.3.2" xref="S3.E1.m1.3.3.3.3.2.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.3.3.1.1" xref="S3.E1.m1.3.3.3.3.1.1.1.cmml"><mo id="S3.E1.m1.3.3.3.3.1.1.2" stretchy="false" xref="S3.E1.m1.3.3.3.3.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.3.3.1.1.1" xref="S3.E1.m1.3.3.3.3.1.1.1.cmml"><mi id="S3.E1.m1.3.3.3.3.1.1.1.3" xref="S3.E1.m1.3.3.3.3.1.1.1.3.cmml">y</mi><mo fence="false" id="S3.E1.m1.3.3.3.3.1.1.1.2" xref="S3.E1.m1.3.3.3.3.1.1.1.2.cmml">|</mo><mrow id="S3.E1.m1.3.3.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.3.3.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">x</mi><mo id="S3.E1.m1.3.3.3.3.1.1.1.1.1.2" xref="S3.E1.m1.3.3.3.3.1.1.1.1.2.cmml">;</mo><msub id="S3.E1.m1.3.3.3.3.1.1.1.1.1.1" xref="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.2.cmml">θ</mi><mrow id="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.3.1" xref="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.3.3.cmml">n</mi></mrow></msub></mrow></mrow><mo id="S3.E1.m1.3.3.3.3.1.1.3" stretchy="false" xref="S3.E1.m1.3.3.3.3.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.4.5" xref="S3.E1.m1.4.4.4.5.cmml">−</mo><mrow id="S3.E1.m1.4.4.4.4" xref="S3.E1.m1.4.4.4.4.cmml"><mrow id="S3.E1.m1.4.4.4.4.3" xref="S3.E1.m1.4.4.4.4.3.cmml"><mi id="S3.E1.m1.4.4.4.4.3.1" xref="S3.E1.m1.4.4.4.4.3.1.cmml">log</mi><mo id="S3.E1.m1.4.4.4.4.3a" lspace="0.167em" xref="S3.E1.m1.4.4.4.4.3.cmml">⁡</mo><mi id="S3.E1.m1.4.4.4.4.3.2" xref="S3.E1.m1.4.4.4.4.3.2.cmml">P</mi></mrow><mo id="S3.E1.m1.4.4.4.4.2" xref="S3.E1.m1.4.4.4.4.2.cmml">⁢</mo><mrow id="S3.E1.m1.4.4.4.4.1.1" xref="S3.E1.m1.4.4.4.4.1.1.1.cmml"><mo id="S3.E1.m1.4.4.4.4.1.1.2" stretchy="false" xref="S3.E1.m1.4.4.4.4.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.4.4.4.4.1.1.1" xref="S3.E1.m1.4.4.4.4.1.1.1.cmml"><mi id="S3.E1.m1.4.4.4.4.1.1.1.3" xref="S3.E1.m1.4.4.4.4.1.1.1.3.cmml">y</mi><mo fence="false" id="S3.E1.m1.4.4.4.4.1.1.1.2" xref="S3.E1.m1.4.4.4.4.1.1.1.2.cmml">|</mo><mrow id="S3.E1.m1.4.4.4.4.1.1.1.1.1" xref="S3.E1.m1.4.4.4.4.1.1.1.1.2.cmml"><mi id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">x</mi><mo id="S3.E1.m1.4.4.4.4.1.1.1.1.1.2" xref="S3.E1.m1.4.4.4.4.1.1.1.1.2.cmml">;</mo><msub id="S3.E1.m1.4.4.4.4.1.1.1.1.1.1" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.2" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.2.cmml">θ</mi><mrow id="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.2" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.2.cmml">o</mi><mo id="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.1" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.3" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.3.cmml">u</mi><mo id="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.1a" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.4" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.4.cmml">t</mi></mrow></msub></mrow></mrow><mo id="S3.E1.m1.4.4.4.4.1.1.3" stretchy="false" xref="S3.E1.m1.4.4.4.4.1.1.1.cmml">)</mo></mrow></mrow></mrow><mrow id="S3.E1.m1.5.5.5.3" xref="S3.E1.m1.5.5.5.2.cmml"><mo id="S3.E1.m1.5.5.5.3.1" stretchy="false" xref="S3.E1.m1.5.5.5.2.1.cmml">|</mo><mi id="S3.E1.m1.5.5.5.1" xref="S3.E1.m1.5.5.5.1.cmml">y</mi><mo id="S3.E1.m1.5.5.5.3.2" stretchy="false" xref="S3.E1.m1.5.5.5.2.1.cmml">|</mo></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.7b"><apply id="S3.E1.m1.7.8.cmml" xref="S3.E1.m1.7.8"><eq id="S3.E1.m1.7.8.1.cmml" xref="S3.E1.m1.7.8.1"></eq><apply id="S3.E1.m1.7.8.2.cmml" xref="S3.E1.m1.7.8.2"><times id="S3.E1.m1.7.8.2.1.cmml" xref="S3.E1.m1.7.8.2.1"></times><ci id="S3.E1.m1.7.8.2.2.cmml" xref="S3.E1.m1.7.8.2.2">𝑞</ci><interval closure="open" id="S3.E1.m1.7.8.2.3.1.cmml" xref="S3.E1.m1.7.8.2.3.2"><ci id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6">𝑥</ci><ci id="S3.E1.m1.7.7.cmml" xref="S3.E1.m1.7.7">𝑦</ci></interval></apply><apply id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5"><divide id="S3.E1.m1.5.5.6.cmml" xref="S3.E1.m1.5.5"></divide><apply id="S3.E1.m1.4.4.4.cmml" xref="S3.E1.m1.4.4.4"><minus id="S3.E1.m1.4.4.4.5.cmml" xref="S3.E1.m1.4.4.4.5"></minus><apply id="S3.E1.m1.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3"><times id="S3.E1.m1.3.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.3.2"></times><apply id="S3.E1.m1.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3"><log id="S3.E1.m1.3.3.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.3.3.1"></log><ci id="S3.E1.m1.3.3.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.3.3.2">𝑃</ci></apply><apply id="S3.E1.m1.3.3.3.3.1.1.1.cmml" xref="S3.E1.m1.3.3.3.3.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.3.3.1.1.1.2.cmml" xref="S3.E1.m1.3.3.3.3.1.1.1.2">conditional</csymbol><ci id="S3.E1.m1.3.3.3.3.1.1.1.3.cmml" xref="S3.E1.m1.3.3.3.3.1.1.1.3">𝑦</ci><list id="S3.E1.m1.3.3.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.3.3.1.1.1.1.1"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝑥</ci><apply id="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.2">𝜃</ci><apply id="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.3"><times id="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.3.1"></times><ci id="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.3.2">𝑖</ci><ci id="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.3.3.1.1.1.1.1.1.3.3">𝑛</ci></apply></apply></list></apply></apply><apply id="S3.E1.m1.4.4.4.4.cmml" xref="S3.E1.m1.4.4.4.4"><times id="S3.E1.m1.4.4.4.4.2.cmml" xref="S3.E1.m1.4.4.4.4.2"></times><apply id="S3.E1.m1.4.4.4.4.3.cmml" xref="S3.E1.m1.4.4.4.4.3"><log id="S3.E1.m1.4.4.4.4.3.1.cmml" xref="S3.E1.m1.4.4.4.4.3.1"></log><ci id="S3.E1.m1.4.4.4.4.3.2.cmml" xref="S3.E1.m1.4.4.4.4.3.2">𝑃</ci></apply><apply id="S3.E1.m1.4.4.4.4.1.1.1.cmml" xref="S3.E1.m1.4.4.4.4.1.1"><csymbol cd="latexml" id="S3.E1.m1.4.4.4.4.1.1.1.2.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.2">conditional</csymbol><ci id="S3.E1.m1.4.4.4.4.1.1.1.3.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.3">𝑦</ci><list id="S3.E1.m1.4.4.4.4.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1"><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">𝑥</ci><apply id="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.2">𝜃</ci><apply id="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3"><times id="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.1"></times><ci id="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.2">𝑜</ci><ci id="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.3">𝑢</ci><ci id="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.4.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.1.1.1.3.4">𝑡</ci></apply></apply></list></apply></apply></apply><apply id="S3.E1.m1.5.5.5.2.cmml" xref="S3.E1.m1.5.5.5.3"><abs id="S3.E1.m1.5.5.5.2.1.cmml" xref="S3.E1.m1.5.5.5.3.1"></abs><ci id="S3.E1.m1.5.5.5.1.cmml" xref="S3.E1.m1.5.5.5.1">𝑦</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.7c">q(x,y)=\frac{\log{P(y|x;\theta_{in})}-\log{P(y|x;\theta_{out})}}{|y|}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.7d">italic_q ( italic_x , italic_y ) = divide start_ARG roman_log italic_P ( italic_y | italic_x ; italic_θ start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT ) - roman_log italic_P ( italic_y | italic_x ; italic_θ start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT ) end_ARG start_ARG | italic_y | end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS8.p3">
<p class="ltx_p" id="S3.SS8.p3.1">For the sampling procedure, we adopt a probabilistic CL strategy<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref" href="https://github.com/kevinduh/sockeye-recipes/tree/master/egs/curriculum" title="">https://github.com/kevinduh/sockeye-recipes/tree/master/egs/curriculum</a></span></span></span> that takes advantage of the spirit of CL in a nondeterministic fashion without discarding the good practice of original standard training policy.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS9">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.9 </span>Transductive Ensemble Learning</h3>
<div class="ltx_para" id="S3.SS9.p1">
<p class="ltx_p" id="S3.SS9.p1.1">Ensemble learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib34" title="">34</a>]</cite>, which aggregates multiple diverse models for inference, is a common practice to improve the accuracy of machine learning tasks. However, it has been observed that the conventional ensemble methods only bring marginal improvement for NMT when individual models are strong or there are a large number of individual models. Transductive Ensemble Learning (TEL) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib12" title="">12</a>]</cite> study how to effectively aggregate multiple NMT models under the transductive setting where the source sentences of the test set are known. TEL uses all individual models to translate the source test set into the target language space and then finetune a strong model on the translated synthetic data, which boosts strong individual models with significant improvement and benefits a lot from more individual models.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>APE System</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>System Overview</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">There is recently a surge in research interests in Transformer-based LLMs, such as ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib16" title="">16</a>]</cite> and LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib39" title="">39</a>]</cite>. Benefiting from the giant model size and oceans of training data, LLMs can understand better the language structures and semantic meanings behind raw text, thereby showing excellent performance in a wide range of natural language processing (NLP) tasks. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S4.F2" title="Figure 2 ‣ 4.1 System Overview ‣ 4 APE System ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_tag">2</span></a>, we use supervised fine-tuning to train LLM as an APE model to improve the translation quality of our NMT model on the zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mo id="S4.SS1.p1.1.m1.1.1" stretchy="false" xref="S4.SS1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">→</annotation></semantics></math>en multi-domain machine translation task. Our APE system is inspired by the GenTranslate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib14" title="">14</a>]</cite>, but the difference is that we use source language text as part of the input information of LLM because we believe that adding source language text helps ensure the fidelity of the target language translation.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="341" id="S4.F2.g1" src="x2.png" width="545"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>APE System for the zh<math alttext="\rightarrow" class="ltx_centering" display="inline" id="S4.F2.2.m1.1"><semantics id="S4.F2.2.m1.1b"><mo id="S4.F2.2.m1.1.1" stretchy="false" xref="S4.F2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.2.m1.1c"><ci id="S4.F2.2.m1.1.1.cmml" xref="S4.F2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.2.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.2.m1.1e">→</annotation></semantics></math>en multi-domain machine translation task.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Efficient LLM Finetuning</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We choose Llama2-13b as the base LLM for our APE system. When performing supervised fine-tuning on this base LLM, full fine-tuning by retraining all model parameters is usually expensive and requires a long training period. Therefore, we adopt the popular LoRA-based efficient parameter fine-tuning method<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref" href="https://github.com/microsoft/LoRA" title="">https://github.com/microsoft/LoRA</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib35" title="">35</a>]</cite>. This method freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. LoRA can lower the hardware threshold by up to 3x when using an adaptive optimizer because we do not need to compute gradients or maintain optimizer state for most parameters.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>HypoTranslate Dataset</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">To build the APE training data for Efficient LLM fine-tuning, we first use the cometkiwi model<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a class="ltx_ref" href="https://huggingface.co/Unbabel/wmt22-cometkiwi-da" title="">https://huggingface.co/Unbabel/wmt22-cometkiwi-da</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib36" title="">36</a>]</cite> to select high-quality bilingual data. Specifically, we select bilingual data with a cometkiwi score greater than 0.8 on the zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mo id="S4.SS3.p1.1.m1.1.1" stretchy="false" xref="S4.SS3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">→</annotation></semantics></math>en language pair. Then we use our trained NMT model as a base translation model to decode the N-best hypotheses from the source language text via a beam search algorithm, where the beam size N is set to 10.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Setup</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.2">We use Pytorch-based Fairseq <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib37" title="">37</a>]</cite> open-source framework to train the NMT model, and use Adam optimizer with <math alttext="\beta 1" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">β</mi><mo id="S5.SS1.p1.1.m1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.cmml">⁢</mo><mn id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><times id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1"></times><ci id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">𝛽</ci><cn id="S5.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.SS1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\beta 1</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">italic_β 1</annotation></semantics></math>=0.9 and <math alttext="\beta 2" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2.1"><semantics id="S5.SS1.p1.2.m2.1a"><mrow id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mi id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">β</mi><mo id="S5.SS1.p1.2.m2.1.1.1" xref="S5.SS1.p1.2.m2.1.1.1.cmml">⁢</mo><mn id="S5.SS1.p1.2.m2.1.1.3" xref="S5.SS1.p1.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><times id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1.1"></times><ci id="S5.SS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2">𝛽</ci><cn id="S5.SS1.p1.2.m2.1.1.3.cmml" type="integer" xref="S5.SS1.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">\beta 2</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.1d">italic_β 2</annotation></semantics></math>=0.98 to guide the parameter optimization. During the training phase, each model uses 8 GPUs for training, batch size is 2048, update frequency is 4, learning rate is 5e-4, label smoothing rate is 0.1 and warm-up steps is 4000. We set dropout to 0.1 for high-resource translation tasks and 0.3 for low-resource translation tasks respectively. In addition, when applying R-Drop for training, we follow the setting of L et al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib9" title="">9</a>]</cite>, using <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.2.1">reg_label_smoothed_cross_entropy</span> as the loss function, and set reg-alpha to 5. Then, we use SacreBLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib38" title="">38</a>]</cite> and COMET<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref" href="https://huggingface.co/Unbabel/wmt20-comet-da" title="">https://huggingface.co/Unbabel/wmt20-comet-da</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib35" title="">35</a>]</cite> to evaluate the overall translation quality of each NMT model.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">To adapt LoRA-based efficient LLM fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib35" title="">35</a>]</cite>, we use llama-recipes<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><a class="ltx_ref" href="https://github.com/meta-llama/llama-recipes" title="">https://github.com/meta-llama/llama-recipes</a></span></span></span> open source framework to train the APE model. The following is the configuration of LoRA: lora_rank is 32, lora_alpha is 64, lora_dropout is 0.05, lora_modules are <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.1">"q", "k", "v", "o", "gate", "down", "up"</span>. We train 2 epochs with AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#bib.bib36" title="">36</a>]</cite>, with learning rate initialized to 1e-4 and then linearly decrease to 1e-5 during training. The batch size is set to 6, with accumulation iterations set to 8 (i.e., real batch size is 48), the context length is 4096, and the batch strategy is packing.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Bilingual MT Results</h3>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS1.1.m1.1"><semantics id="S5.SS2.SSS1.1.m1.1b"><mo id="S5.SS2.SSS1.1.m1.1.1" stretchy="false" xref="S5.SS2.SSS1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.1.m1.1c"><ci id="S5.SS2.SSS1.1.m1.1.1.cmml" xref="S5.SS2.SSS1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.1.m1.1e">→</annotation></semantics></math>zh &amp; zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS1.2.m2.1"><semantics id="S5.SS2.SSS1.2.m2.1b"><mo id="S5.SS2.SSS1.2.m2.1.1" stretchy="false" xref="S5.SS2.SSS1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.2.m2.1c"><ci id="S5.SS2.SSS1.2.m2.1.1.cmml" xref="S5.SS2.SSS1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.2.m2.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.2.m2.1e">→</annotation></semantics></math>en</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.2">On en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p1.1.m1.1"><semantics id="S5.SS2.SSS1.p1.1.m1.1a"><mo id="S5.SS2.SSS1.p1.1.m1.1.1" stretchy="false" xref="S5.SS2.SSS1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p1.1.m1.1b"><ci id="S5.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p1.1.m1.1d">→</annotation></semantics></math>zh and zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p1.2.m2.1"><semantics id="S5.SS2.SSS1.p1.2.m2.1a"><mo id="S5.SS2.SSS1.p1.2.m2.1.1" stretchy="false" xref="S5.SS2.SSS1.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p1.2.m2.1b"><ci id="S5.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S5.SS2.SSS1.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p1.2.m2.1d">→</annotation></semantics></math>en translation tasks, we use BiT and R-Drop to build a strong baseline system. Subsequently, we adopt the data augmentation methods of DD, FT and ST to improve the translation quality of baseline System. Next, we use AT guide model training with authentic bilingual data. Then, we use CL for domain adaptation. Finally, we train multiple NMT systems and integrate them using TEL as the final translation system.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p2">
<p class="ltx_p" id="S5.SS2.SSS1.p2.4">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S5.T3" title="Table 3 ‣ 5.2.1 en→zh &amp; zh→en ‣ 5.2 Bilingual MT Results ‣ 5 Experiments ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_tag">3</span></a> shows the evaluation results of en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p2.1.m1.1"><semantics id="S5.SS2.SSS1.p2.1.m1.1a"><mo id="S5.SS2.SSS1.p2.1.m1.1.1" stretchy="false" xref="S5.SS2.SSS1.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.1.m1.1b"><ci id="S5.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p2.1.m1.1d">→</annotation></semantics></math>zh and zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p2.2.m2.1"><semantics id="S5.SS2.SSS1.p2.2.m2.1a"><mo id="S5.SS2.SSS1.p2.2.m2.1.1" stretchy="false" xref="S5.SS2.SSS1.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.2.m2.1b"><ci id="S5.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S5.SS2.SSS1.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p2.2.m2.1d">→</annotation></semantics></math>en translation systems. Compared with the baseline system, the final en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p2.3.m3.1"><semantics id="S5.SS2.SSS1.p2.3.m3.1a"><mo id="S5.SS2.SSS1.p2.3.m3.1.1" stretchy="false" xref="S5.SS2.SSS1.p2.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.3.m3.1b"><ci id="S5.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S5.SS2.SSS1.p2.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p2.3.m3.1d">→</annotation></semantics></math>zh and zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p2.4.m4.1"><semantics id="S5.SS2.SSS1.p2.4.m4.1a"><mo id="S5.SS2.SSS1.p2.4.m4.1.1" stretchy="false" xref="S5.SS2.SSS1.p2.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.4.m4.1b"><ci id="S5.SS2.SSS1.p2.4.m4.1.1.cmml" xref="S5.SS2.SSS1.p2.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.4.m4.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p2.4.m4.1d">→</annotation></semantics></math>en translation systems improves significantly on CCMT 2022 test sets.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>BLEU and COMET scores of en<math alttext="\rightarrow" class="ltx_centering" display="inline" id="S5.T3.3.3.m1.1"><semantics id="S5.T3.3.3.m1.1b"><mo id="S5.T3.3.3.m1.1.1" stretchy="false" xref="S5.T3.3.3.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.m1.1c"><ci id="S5.T3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.3.m1.1e">→</annotation></semantics></math>zh &amp; zh<math alttext="\rightarrow" class="ltx_centering" display="inline" id="S5.T3.4.4.m2.1"><semantics id="S5.T3.4.4.m2.1b"><mo id="S5.T3.4.4.m2.1.1" stretchy="false" xref="S5.T3.4.4.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.m2.1c"><ci id="S5.T3.4.4.m2.1.1.cmml" xref="S5.T3.4.4.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.m2.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.4.4.m2.1e">→</annotation></semantics></math>en NMT system</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.6.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.6.6.2">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.6.6.2.3"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T3.6.6.2.4"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T3.6.6.2.5"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T3.6.6.2.6"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S5.T3.5.5.1.1">en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.T3.5.5.1.1.m1.1"><semantics id="S5.T3.5.5.1.1.m1.1a"><mo id="S5.T3.5.5.1.1.m1.1.1" stretchy="false" xref="S5.T3.5.5.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.1.1.m1.1b"><ci id="S5.T3.5.5.1.1.m1.1.1.cmml" xref="S5.T3.5.5.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.5.5.1.1.m1.1d">→</annotation></semantics></math>zh</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S5.T3.6.6.2.2">zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.T3.6.6.2.2.m1.1"><semantics id="S5.T3.6.6.2.2.m1.1a"><mo id="S5.T3.6.6.2.2.m1.1.1" stretchy="false" xref="S5.T3.6.6.2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.2.2.m1.1b"><ci id="S5.T3.6.6.2.2.m1.1.1.cmml" xref="S5.T3.6.6.2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.2.2.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.6.6.2.2.m1.1d">→</annotation></semantics></math>en</th>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.6.6.3.1.1">CCMT 2022 test set</th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T3.6.6.3.1.2"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T3.6.6.3.1.3"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T3.6.6.3.1.4"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.6.6.3.1.5">BLEU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.6.6.3.1.6">COMET</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.6.6.3.1.7">BLEU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.6.6.3.1.8">COMET</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.6.6.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.6.6.4.1.1">BiT R-Drop baseline</th>
<td class="ltx_td ltx_border_t" id="S5.T3.6.6.4.1.2"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.6.6.4.1.3"></td>
<td class="ltx_td ltx_border_t" id="S5.T3.6.6.4.1.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.6.6.4.1.5">54.37</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.6.6.4.1.6">0.6953</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.6.6.4.1.7">43.46</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.6.6.4.1.8">0.6754</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T3.6.6.5.2.1">+ DD, FT &amp; ST</th>
<td class="ltx_td" id="S5.T3.6.6.5.2.2"></td>
<td class="ltx_td" id="S5.T3.6.6.5.2.3"></td>
<td class="ltx_td" id="S5.T3.6.6.5.2.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.6.5.2.5">56.71</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.6.5.2.6">0.7203</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.6.5.2.7">44.57</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.6.5.2.8">0.6895</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T3.6.6.6.3.1">+ AT</th>
<td class="ltx_td" id="S5.T3.6.6.6.3.2"></td>
<td class="ltx_td" id="S5.T3.6.6.6.3.3"></td>
<td class="ltx_td" id="S5.T3.6.6.6.3.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.6.6.3.5">57.03</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.6.6.3.6">0.7358</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.6.6.3.7">44.74</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.6.6.3.8">0.6915</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T3.6.6.7.4.1">+ CL</th>
<td class="ltx_td" id="S5.T3.6.6.7.4.2"></td>
<td class="ltx_td" id="S5.T3.6.6.7.4.3"></td>
<td class="ltx_td" id="S5.T3.6.6.7.4.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.6.7.4.5">57.49</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.6.7.4.6">0.7527</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.6.7.4.7">46.83</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.6.6.7.4.8">0.7060</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S5.T3.6.6.8.5.1">+ TEL</th>
<td class="ltx_td ltx_border_b" id="S5.T3.6.6.8.5.2"></td>
<td class="ltx_td ltx_border_b" id="S5.T3.6.6.8.5.3"></td>
<td class="ltx_td ltx_border_b" id="S5.T3.6.6.8.5.4"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T3.6.6.8.5.5"><span class="ltx_text ltx_font_bold" id="S5.T3.6.6.8.5.5.1">57.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T3.6.6.8.5.6"><span class="ltx_text ltx_font_bold" id="S5.T3.6.6.8.5.6.1">0.7610</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T3.6.6.8.5.7"><span class="ltx_text ltx_font_bold" id="S5.T3.6.6.8.5.7.1">47.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T3.6.6.8.5.8"><span class="ltx_text ltx_font_bold" id="S5.T3.6.6.8.5.8.1">0.7264</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>mn<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS2.1.m1.1"><semantics id="S5.SS2.SSS2.1.m1.1b"><mo id="S5.SS2.SSS2.1.m1.1.1" stretchy="false" xref="S5.SS2.SSS2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.1.m1.1c"><ci id="S5.SS2.SSS2.1.m1.1.1.cmml" xref="S5.SS2.SSS2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.1.m1.1e">→</annotation></semantics></math>zh, ti<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS2.2.m2.1"><semantics id="S5.SS2.SSS2.2.m2.1b"><mo id="S5.SS2.SSS2.2.m2.1.1" stretchy="false" xref="S5.SS2.SSS2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.2.m2.1c"><ci id="S5.SS2.SSS2.2.m2.1.1.cmml" xref="S5.SS2.SSS2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.2.m2.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.2.m2.1e">→</annotation></semantics></math>zh &amp; uy<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS2.3.m3.1"><semantics id="S5.SS2.SSS2.3.m3.1b"><mo id="S5.SS2.SSS2.3.m3.1.1" stretchy="false" xref="S5.SS2.SSS2.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.3.m3.1c"><ci id="S5.SS2.SSS2.3.m3.1.1.cmml" xref="S5.SS2.SSS2.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.3.m3.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.3.m3.1e">→</annotation></semantics></math>zh</h4>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.8">On mn<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.1.m1.1"><semantics id="S5.SS2.SSS2.p1.1.m1.1a"><mo id="S5.SS2.SSS2.p1.1.m1.1.1" stretchy="false" xref="S5.SS2.SSS2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.1.m1.1b"><ci id="S5.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.1.m1.1d">→</annotation></semantics></math>zh, ti<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.2.m2.1"><semantics id="S5.SS2.SSS2.p1.2.m2.1a"><mo id="S5.SS2.SSS2.p1.2.m2.1.1" stretchy="false" xref="S5.SS2.SSS2.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.2.m2.1b"><ci id="S5.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.2.m2.1d">→</annotation></semantics></math>zh and uy<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.3.m3.1"><semantics id="S5.SS2.SSS2.p1.3.m3.1a"><mo id="S5.SS2.SSS2.p1.3.m3.1.1" stretchy="false" xref="S5.SS2.SSS2.p1.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.3.m3.1b"><ci id="S5.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S5.SS2.SSS2.p1.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.3.m3.1d">→</annotation></semantics></math>zh translation tasks, we also use BiT and R-Drop to build a strong baseline system. The subsequent training method is similar to en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.4.m4.1"><semantics id="S5.SS2.SSS2.p1.4.m4.1a"><mo id="S5.SS2.SSS2.p1.4.m4.1.1" stretchy="false" xref="S5.SS2.SSS2.p1.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.4.m4.1b"><ci id="S5.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S5.SS2.SSS2.p1.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.4.m4.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.4.m4.1d">→</annotation></semantics></math>zh and zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.5.m5.1"><semantics id="S5.SS2.SSS2.p1.5.m5.1a"><mo id="S5.SS2.SSS2.p1.5.m5.1.1" stretchy="false" xref="S5.SS2.SSS2.p1.5.m5.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.5.m5.1b"><ci id="S5.SS2.SSS2.p1.5.m5.1.1.cmml" xref="S5.SS2.SSS2.p1.5.m5.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.5.m5.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.5.m5.1d">→</annotation></semantics></math>en translation tasks. The only difference is that we adopt DD and Tagged BT in the data augmentation stage, which is due to the lack of source language monolingual for these three tasks. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S5.T4" title="Table 4 ‣ 5.2.2 mn→zh, ti→zh &amp; uy→zh ‣ 5.2 Bilingual MT Results ‣ 5 Experiments ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_tag">4</span></a> is the evaluation results of mn<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.6.m6.1"><semantics id="S5.SS2.SSS2.p1.6.m6.1a"><mo id="S5.SS2.SSS2.p1.6.m6.1.1" stretchy="false" xref="S5.SS2.SSS2.p1.6.m6.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.6.m6.1b"><ci id="S5.SS2.SSS2.p1.6.m6.1.1.cmml" xref="S5.SS2.SSS2.p1.6.m6.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.6.m6.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.6.m6.1d">→</annotation></semantics></math>zh, ti<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.7.m7.1"><semantics id="S5.SS2.SSS2.p1.7.m7.1a"><mo id="S5.SS2.SSS2.p1.7.m7.1.1" stretchy="false" xref="S5.SS2.SSS2.p1.7.m7.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.7.m7.1b"><ci id="S5.SS2.SSS2.p1.7.m7.1.1.cmml" xref="S5.SS2.SSS2.p1.7.m7.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.7.m7.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.7.m7.1d">→</annotation></semantics></math>zh and uy<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.8.m8.1"><semantics id="S5.SS2.SSS2.p1.8.m8.1a"><mo id="S5.SS2.SSS2.p1.8.m8.1.1" stretchy="false" xref="S5.SS2.SSS2.p1.8.m8.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.8.m8.1b"><ci id="S5.SS2.SSS2.p1.8.m8.1.1.cmml" xref="S5.SS2.SSS2.p1.8.m8.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.8.m8.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.8.m8.1d">→</annotation></semantics></math>zh translation systems on CCMT 2022 test set or CCMT 2023 test set. Overall, the final translation systems for all three tasks improves significantly compared to the baseline.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>BLEU scores of mn<math alttext="\rightarrow" class="ltx_centering" display="inline" id="S5.T4.4.4.m1.1"><semantics id="S5.T4.4.4.m1.1b"><mo id="S5.T4.4.4.m1.1.1" stretchy="false" xref="S5.T4.4.4.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.m1.1c"><ci id="S5.T4.4.4.m1.1.1.cmml" xref="S5.T4.4.4.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.4.4.m1.1e">→</annotation></semantics></math>zh, ti<math alttext="\rightarrow" class="ltx_centering" display="inline" id="S5.T4.5.5.m2.1"><semantics id="S5.T4.5.5.m2.1b"><mo id="S5.T4.5.5.m2.1.1" stretchy="false" xref="S5.T4.5.5.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.5.5.m2.1c"><ci id="S5.T4.5.5.m2.1.1.cmml" xref="S5.T4.5.5.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.5.m2.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.5.5.m2.1e">→</annotation></semantics></math>zh and uy<math alttext="\rightarrow" class="ltx_centering" display="inline" id="S5.T4.6.6.m3.1"><semantics id="S5.T4.6.6.m3.1b"><mo id="S5.T4.6.6.m3.1.1" stretchy="false" xref="S5.T4.6.6.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.6.6.m3.1c"><ci id="S5.T4.6.6.m3.1.1.cmml" xref="S5.T4.6.6.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.6.m3.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.6.6.m3.1e">→</annotation></semantics></math>zh translation system</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.9.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.9.9.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.9.9.3.4">CCMT test set</th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T4.9.9.3.5"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T4.9.9.3.6"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T4.9.9.3.7"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T4.9.9.3.8"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.7.7.1.1">mn<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.T4.7.7.1.1.m1.1"><semantics id="S5.T4.7.7.1.1.m1.1a"><mo id="S5.T4.7.7.1.1.m1.1.1" stretchy="false" xref="S5.T4.7.7.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.7.7.1.1.m1.1b"><ci id="S5.T4.7.7.1.1.m1.1.1.cmml" xref="S5.T4.7.7.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.7.7.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.7.7.1.1.m1.1d">→</annotation></semantics></math>zh (2023)</th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T4.9.9.3.9"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T4.9.9.3.10"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.8.8.2.2">ti<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.T4.8.8.2.2.m1.1"><semantics id="S5.T4.8.8.2.2.m1.1a"><mo id="S5.T4.8.8.2.2.m1.1.1" stretchy="false" xref="S5.T4.8.8.2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.8.8.2.2.m1.1b"><ci id="S5.T4.8.8.2.2.m1.1.1.cmml" xref="S5.T4.8.8.2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.8.8.2.2.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.8.8.2.2.m1.1d">→</annotation></semantics></math>zh (2022)</th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T4.9.9.3.11"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_t" id="S5.T4.9.9.3.12"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.9.9.3.3">uy<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.T4.9.9.3.3.m1.1"><semantics id="S5.T4.9.9.3.3.m1.1a"><mo id="S5.T4.9.9.3.3.m1.1.1" stretchy="false" xref="S5.T4.9.9.3.3.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T4.9.9.3.3.m1.1b"><ci id="S5.T4.9.9.3.3.m1.1.1.cmml" xref="S5.T4.9.9.3.3.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.9.9.3.3.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.9.9.3.3.m1.1d">→</annotation></semantics></math>zh (2023)</th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.9.9.3.13"></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.9.9.4.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.9.9.4.1.1">BiT R-Drop baseline</td>
<td class="ltx_td ltx_border_t" id="S5.T4.9.9.4.1.2"></td>
<td class="ltx_td ltx_border_t" id="S5.T4.9.9.4.1.3"></td>
<td class="ltx_td ltx_border_t" id="S5.T4.9.9.4.1.4"></td>
<td class="ltx_td ltx_border_t" id="S5.T4.9.9.4.1.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.9.9.4.1.6">55.87</td>
<td class="ltx_td ltx_border_t" id="S5.T4.9.9.4.1.7"></td>
<td class="ltx_td ltx_border_t" id="S5.T4.9.9.4.1.8"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.9.9.4.1.9">32.69</td>
<td class="ltx_td ltx_border_t" id="S5.T4.9.9.4.1.10"></td>
<td class="ltx_td ltx_border_t" id="S5.T4.9.9.4.1.11"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.9.9.4.1.12">42.58</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T4.9.9.4.1.13"></td>
</tr>
<tr class="ltx_tr" id="S5.T4.9.9.5.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T4.9.9.5.2.1">+ DD &amp; Tagged BT</td>
<td class="ltx_td" id="S5.T4.9.9.5.2.2"></td>
<td class="ltx_td" id="S5.T4.9.9.5.2.3"></td>
<td class="ltx_td" id="S5.T4.9.9.5.2.4"></td>
<td class="ltx_td" id="S5.T4.9.9.5.2.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.9.9.5.2.6">59.73</td>
<td class="ltx_td" id="S5.T4.9.9.5.2.7"></td>
<td class="ltx_td" id="S5.T4.9.9.5.2.8"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.9.9.5.2.9">36.52</td>
<td class="ltx_td" id="S5.T4.9.9.5.2.10"></td>
<td class="ltx_td" id="S5.T4.9.9.5.2.11"></td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.9.5.2.12">49.31</td>
<td class="ltx_td ltx_border_r" id="S5.T4.9.9.5.2.13"></td>
</tr>
<tr class="ltx_tr" id="S5.T4.9.9.6.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T4.9.9.6.3.1">+ AT</td>
<td class="ltx_td" id="S5.T4.9.9.6.3.2"></td>
<td class="ltx_td" id="S5.T4.9.9.6.3.3"></td>
<td class="ltx_td" id="S5.T4.9.9.6.3.4"></td>
<td class="ltx_td" id="S5.T4.9.9.6.3.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.9.9.6.3.6">62.01</td>
<td class="ltx_td" id="S5.T4.9.9.6.3.7"></td>
<td class="ltx_td" id="S5.T4.9.9.6.3.8"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.9.9.6.3.9">37.45</td>
<td class="ltx_td" id="S5.T4.9.9.6.3.10"></td>
<td class="ltx_td" id="S5.T4.9.9.6.3.11"></td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.9.6.3.12">49.91</td>
<td class="ltx_td ltx_border_r" id="S5.T4.9.9.6.3.13"></td>
</tr>
<tr class="ltx_tr" id="S5.T4.9.9.7.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S5.T4.9.9.7.4.1">+ CL</td>
<td class="ltx_td" id="S5.T4.9.9.7.4.2"></td>
<td class="ltx_td" id="S5.T4.9.9.7.4.3"></td>
<td class="ltx_td" id="S5.T4.9.9.7.4.4"></td>
<td class="ltx_td" id="S5.T4.9.9.7.4.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.9.9.7.4.6">62.96</td>
<td class="ltx_td" id="S5.T4.9.9.7.4.7"></td>
<td class="ltx_td" id="S5.T4.9.9.7.4.8"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.9.9.7.4.9">38.40</td>
<td class="ltx_td" id="S5.T4.9.9.7.4.10"></td>
<td class="ltx_td" id="S5.T4.9.9.7.4.11"></td>
<td class="ltx_td ltx_align_center" id="S5.T4.9.9.7.4.12">50.54</td>
<td class="ltx_td ltx_border_r" id="S5.T4.9.9.7.4.13"></td>
</tr>
<tr class="ltx_tr" id="S5.T4.9.9.8.5">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S5.T4.9.9.8.5.1">+ TEL</td>
<td class="ltx_td ltx_border_b" id="S5.T4.9.9.8.5.2"></td>
<td class="ltx_td ltx_border_b" id="S5.T4.9.9.8.5.3"></td>
<td class="ltx_td ltx_border_b" id="S5.T4.9.9.8.5.4"></td>
<td class="ltx_td ltx_border_b" id="S5.T4.9.9.8.5.5"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T4.9.9.8.5.6"><span class="ltx_text ltx_font_bold" id="S5.T4.9.9.8.5.6.1">63.59</span></td>
<td class="ltx_td ltx_border_b" id="S5.T4.9.9.8.5.7"></td>
<td class="ltx_td ltx_border_b" id="S5.T4.9.9.8.5.8"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T4.9.9.8.5.9"><span class="ltx_text ltx_font_bold" id="S5.T4.9.9.8.5.9.1">40.90</span></td>
<td class="ltx_td ltx_border_b" id="S5.T4.9.9.8.5.10"></td>
<td class="ltx_td ltx_border_b" id="S5.T4.9.9.8.5.11"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.9.9.8.5.12"><span class="ltx_text ltx_font_bold" id="S5.T4.9.9.8.5.12.1">51.27</span></td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S5.T4.9.9.8.5.13"></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Multi-domain MT Results</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.2">On the zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><mo id="S5.SS3.p1.1.m1.1.1" stretchy="false" xref="S5.SS3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><ci id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">→</annotation></semantics></math>en multi-domain machine translation task, we first select the best model (TEL) in the zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS3.p1.2.m2.1"><semantics id="S5.SS3.p1.2.m2.1a"><mo id="S5.SS3.p1.2.m2.1.1" stretchy="false" xref="S5.SS3.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><ci id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.2.m2.1d">→</annotation></semantics></math>en bilingual machine translation task as the baseline model. Then, we use the collected high-quality domain-related bilingual data to fine-tune for domain adaptation. Finally, we use the APE model based on llama-13b to improve the translation results generated by the NMT model, and use the post-editing result as the final translation result.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S5.T5" title="Table 5 ‣ 5.3 Multi-domain MT Results ‣ 5 Experiments ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_tag">5</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14842v3#S5.T6" title="Table 6 ‣ 5.3 Multi-domain MT Results ‣ 5 Experiments ‣ HW-TSC’s Submission to the CCMT 2024 Machine Translation Tasks"><span class="ltx_text ltx_ref_tag">6</span></a> shows the evaluation results of zh<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS3.p2.1.m1.1"><semantics id="S5.SS3.p2.1.m1.1a"><mo id="S5.SS3.p2.1.m1.1.1" stretchy="false" xref="S5.SS3.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><ci id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.1.m1.1d">→</annotation></semantics></math>en multi-domain translation system on the dev set. It shows that domain adaptation is critical to multi-domain translation, and the powerful generation capability of LLM can further help improve the translation quality of the NMT model in the multi-domain translation task.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>BLEU scores of zh<math alttext="\rightarrow" class="ltx_centering" display="inline" id="S5.T5.2.2.m1.1"><semantics id="S5.T5.2.2.m1.1b"><mo id="S5.T5.2.2.m1.1.1" stretchy="false" xref="S5.T5.2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.m1.1c"><ci id="S5.T5.2.2.m1.1.1.cmml" xref="S5.T5.2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.2.2.m1.1e">→</annotation></semantics></math>en multi-domain translation system</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T5.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T5.3.1.1.1">dev set</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.1.1.2">IT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.1.1.3">car</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.1.1.4">electronic</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.1.1.5">energy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.1.1.6">finance</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.1.1.7">literatrue</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.1.1.8">machine</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.1.1.9">medical</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.1.1.10">average</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.3.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T5.3.2.1.1">TEL</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.2.1.2">30.72</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.2.1.3">30.08</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.2.1.4">42.57</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.2.1.5">32.94</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.2.1.6">33.35</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.2.1.7">36.78</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.2.1.8">37.38</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.2.1.9">49.51</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.2.1.10">36.67</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T5.3.3.2.1">+ fine-tuning</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.2.2">39.08</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.2.3">36.50</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.2.4">50.12</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.2.5">39.42</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.2.6">44.18</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.2.7">43.71</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.2.8">45.11</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.2.9">57.26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.2.10">44.42</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S5.T5.3.4.3.1">+ APE</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T5.3.4.3.2">43.3</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T5.3.4.3.3">39.59</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T5.3.4.3.4">50.76</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T5.3.4.3.5">39.59</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T5.3.4.3.6">47.75</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T5.3.4.3.7">47.71</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T5.3.4.3.8">50.09</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T5.3.4.3.9">59.82</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T5.3.4.3.10">47.33</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>COMET scores of zh<math alttext="\rightarrow" class="ltx_centering" display="inline" id="S5.T6.2.2.m1.1"><semantics id="S5.T6.2.2.m1.1b"><mo id="S5.T6.2.2.m1.1.1" stretchy="false" xref="S5.T6.2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T6.2.2.m1.1c"><ci id="S5.T6.2.2.m1.1.1.cmml" xref="S5.T6.2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.2.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.2.m1.1e">→</annotation></semantics></math>en multi-domain translation system</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T6.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T6.3.1.1.1">dev set</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.3.1.1.2">IT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.3.1.1.3">car</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.3.1.1.4">electronic</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.3.1.1.5">energy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.3.1.1.6">finance</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.3.1.1.7">literatrue</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.3.1.1.8">machine</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.3.1.1.9">medical</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T6.3.1.1.10">average</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.3.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T6.3.2.1.1">TEL</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.3.2.1.2">0.4218</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.3.2.1.3">0.5406</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.3.2.1.4">0.6828</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.3.2.1.5">0.4785</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.3.2.1.6">0.5919</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.3.2.1.7">0.5128</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.3.2.1.8">0.5732</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.3.2.1.9">0.7747</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.3.2.1.10">0.5720</td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T6.3.3.2.1">+ fine-tuning</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.3.3.2.2">0.5299</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.3.3.2.3">0.5788</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.3.3.2.4">0.7417</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.3.3.2.5">0.5850</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.3.3.2.6">0.6432</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.3.3.2.7">0.6689</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.3.3.2.8">0.6935</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.3.3.2.9">0.8227</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.3.3.2.10">0.6580</td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S5.T6.3.4.3.1">+ APE</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T6.3.4.3.2">0.5649</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T6.3.4.3.3">0.5868</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T6.3.4.3.4">0.7547</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T6.3.4.3.5">0.5857</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T6.3.4.3.6">0.6788</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T6.3.4.3.7">0.7012</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T6.3.4.3.8">0.7608</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T6.3.4.3.9">0.8221</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T6.3.4.3.10">0.6819</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This paper presents HW-TSC’s submission to the bilingual machine translation task and multi-domain machine translation task of CCMT 2024. For both translation tasks, we use a series of training strategies to train NMT models based on the deep Transformer-big architecture. Additionally, for the multi-domain machine translation task, we use the powerful generation capabilities of LLM to post-edit the translation results of the NMT model to obtain better translations. Relevant experimental results also show the effectiveness of our adopted strategies. By using these enhancement strategies, our submission achieves a competitive result in the final evaluation.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">Sutskever I, Vinyals O, Le Q V. Sequence to sequence learning with neural networks[J]. Advances in neural information processing systems, 2014, 27.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Bahdanau D, Cho K H, Bengio Y. Neural machine translation by jointly learning to align and translate[C]//3rd International Conference on Learning Representations, ICLR 2015. 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Gehring J, Auli M, Grangier D, et al. Convolutional sequence to sequence learning[C]//Proceedings of the 34th International Conference on Machine Learning-Volume 70. 2017: 1243-1252.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Koehn P, Hoang H, Birch A, et al. Moses: Open source toolkit for statistical machine translation[C]//Proceedings of the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions. Association for Computational Linguistics, 2007: 177-180.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">Nguyen, Xuan-Phi, et al. "Data diversification: A simple strategy for neural machine translation." Advances in Neural Information Processing Systems 33 (2020): 10018-10029.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Abdulmumin I, Galadanci B S, Isa A. Enhanced back-translation for low resource neural machine translation using self-training[C]//International Conference on Information and Communication Technology and Applications. Springer, Cham, 2020: 355-371.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Sennrich, Rico, Barry Haddow, and Alexandra Birch. "Improving Neural Machine Translation Models with Monolingual Data." Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">Jiao R, Yang Z, Sun M, et al. Alternated Training with Synthetic and Authentic Data for Neural Machine Translation[C]//Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021: 1828-1834.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">Wu L, Li J, Wang Y, et al. R-Drop: Regularized Dropout for Neural Networks[C]//Advances in Neural Information Processing Systems. 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">Ding, Liang, Di Wu, and Dacheng Tao. "Improving Neural Machine Translation by Bidirectional Training." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Wang, Wei, et al. "Learning a Multi-Domain Curriculum for Neural Machine Translation." Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Wang, Yiren, et al. "Transductive ensemble learning for neural machine translation." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Freitag M, Al-Onaizan Y. Beam Search Strategies for Neural Machine Translation[J]. ACL 2017, 2017: 56.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Hu Y, Chen C, Yang C H H, et al. GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators[J]. arXiv preprint arXiv:2402.06894, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Wu T, He S, Liu J, et al. A brief overview of ChatGPT: The history, status quo and potential future development[J]. IEEE/CAA Journal of Automatica Sinica, 2023, 10(5): 1122-1136.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">Achiam J, Adler S, Agarwal S, et al. Gpt-4 technical report[J]. arXiv preprint arXiv:2303.08774, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Touvron H, Lavril T, Izacard G, et al. Llama: Open and efficient foundation language models[J]. arXiv preprint arXiv:2302.13971, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Touvron H, Martin L, Stone K, et al. Llama 2: Open foundation and fine-tuned chat models[J]. arXiv preprint arXiv:2307.09288, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">Wu, Zhanglin, Daimeng Wei, Xiaoyu Chen, Ming Zhu, Zongyao Li, Hengchao Shang, Jinlong Yang et al. "Multi-strategy enhanced neural machine translation for chinese minority languages." In China Conference on Machine Translation, pp. 37-44. Singapore: Springer Nature Singapore, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">Wu, Zhanglin, Zhengzhe Yu, Zongyao Li, Daimeng Wei, Yuhao Xie, Xiaoyu Chen, Hengchao Shang et al. "HW-TSC’s Neural Machine Translation System for CCMT 2023." In China Conference on Machine Translation, pp. 13-27. Singapore: Springer Nature Singapore, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Wu, Zhanglin, Daimeng Wei, Zongyao Li, Zhengzhe Yu, Shaojun Li, Xiaoyu Chen, Hengchao Shang et al. "Treating General MT Shared Task as a Multi-Domain Adaptation Problem: HW-TSC’s Submission to the WMT23 General MT Shared Task." In Proceedings of the Eighth Conference on Machine Translation, pp. 170-174. 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">Wei, Daimeng, Zhiqiang Rao, Zhanglin Wu, Shaojun Li, Yuanchang Luo, Yuhao Xie, Xiaoyu Chen et al. "Hw-tsc’s submissions to the wmt 2022 general machine translation shared task." In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 403-410. 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">Dong G, Yuan H, Lu K, et al. How abilities in large language models are affected by supervised fine-tuning data composition[J]. arXiv preprint arXiv:2310.05492, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">Raunak V, Sharaf A, Wang Y, et al. Leveraging GPT-4 for Automatic Translation Post-Editing[C]//Findings of the Association for Computational Linguistics: EMNLP 2023. 2023: 12009-12024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">Sennrich R, Haddow B, Birch A. Neural Machine Translation of Rare Words with Subword Units[C]//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2016: 1715-1725.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">Kudo, Taku, and John Richardson. "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing." Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">Wang Q, Li B, Xiao T, et al. Learning Deep Transformer Models for Machine Translation[C]//Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019: 1810-1822.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">Edunov S, Ott M, Auli M, et al. Understanding Back-Translation at Scale[C]//Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018: 489-500.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">Caswell I, Chelba C, Grangier D. Tagged Back-Translation[C]//Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers). 2019: 53-63.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: a simple way to prevent neural networks from overfitting[J]. The journal of machine learning research, 2014, 15(1): 1929-1958.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">Van Erven, Tim, and Peter Harremos. "Rényi divergence and Kullback-Leibler divergence." IEEE Transactions on Information Theory 60.7 (2014): 3797-3820.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">Zhang, Xuan, et al. "Curriculum Learning for Domain Adaptation in Neural Machine Translation." Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">Garmash E, Monz C. Ensemble learning for multi-source neural machine translation[C]//Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. 2016: 1409-1418.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">Hu E J, Shen Y, Wallis P, et al. Lora: Low-rank adaptation of large language models[J]. arXiv preprint arXiv:2106.09685, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">Rei R, Stewart C, Farinha A C, et al. COMET: A Neural Framework for MT Evaluation[C]//Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020: 2685-2702.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">Ott M, Edunov S, Baevski A, et al. fairseq: A Fast, Extensible Toolkit for Sequence Modeling[C]//Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations). 2019: 48-53.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">Post M. A Call for Clarity in Reporting BLEU Scores[C]//Proceedings of the Third Conference on Machine Translation: Research Papers. 2018: 186-191.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">Guo, Jiaxin, Hao Yang, Zongyao Li, Daimeng Wei, Hengchao Shang, and Xiaoyu Chen. "A Novel Paradigm Boosting Translation Capabilities of Large Language Models." In Findings of the Association for Computational Linguistics: NAACL 2024, pp. 639-649. 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">Wei, Daimeng, Zhanglin Wu, Hengchao Shang, Zongyao Li, Minghan Wang, Jiaxin Guo, Xiaoyu Chen, Zhengzhe Yu, and Hao Yang. "Text Style Transfer Back-Translation." In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 7944-7959. 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct  8 09:33:11 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
