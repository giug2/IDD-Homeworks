<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.04255] Second Place Solution of WSDM2023 Toloka Visual Question Answering Challenge</title><meta property="og:description" content="In this paper, we present our solution for the WSDM2023 Toloka Visual Question Answering Challenge. Inspired by the application of multimodal pre-trained models to various downstream tasks(e.g., visual question answeri…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Second Place Solution of WSDM2023 Toloka Visual Question Answering Challenge">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Second Place Solution of WSDM2023 Toloka Visual Question Answering Challenge">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.04255">

<!--Generated on Mon Aug  5 18:53:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Second Place Solution of WSDM2023 Toloka 
<br class="ltx_break">Visual Question Answering Challenge</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Xiangyu Wu<sup id="id6.2.id1" class="ltx_sup"><span id="id6.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhouyang Chi<sup id="id7.2.id1" class="ltx_sup"><span id="id7.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yang Yang<sup id="id8.2.id1" class="ltx_sup"><span id="id8.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jianfeng Lu<sup id="id9.2.id1" class="ltx_sup"><span id="id9.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><sup id="id10.2.id1" class="ltx_sup">1</sup> Nanjing University of Science and Technology 
<br class="ltx_break">{wxy_yyjhl,1527612210,yyang,lujf}@njust.edu.cn
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p">In this paper, we present our solution for the WSDM2023 Toloka Visual Question Answering Challenge. Inspired by the application of multimodal pre-trained models to various downstream tasks(e.g., visual question answering, visual grounding, and cross-modal retrieval), we approached this competition as a visual grounding task, where the input is an image and a question, guiding the model to answer the question and display the answer as a bounding box on the image. We designed a three-stage solution for this task. Specifically, we used the visual-language pre-trained model OFA as the foundation. In the first stage, we constructed a large-scale synthetic dataset similar to the competition dataset and coarse-tuned the model to learn generalized semantic information. In the second stage, we treated the competition task as a visual grounding task, loaded the weights from the previous stage, and continued to fine-tune the model on the competition dataset, transferring the semantic information learned in the first stage to the competition task. Finally, we designed a bounding box matching and replacing post-processing strategy to correct the model’s prediction results. Our team achieved a score of 76.342 on the final leaderboard, ranking second.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual-language pretraining (VLP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> models have seen rapid development and significant advancements in recent years. These models aim to bridge the gap between visual and linguistic information, enabling machines to understand and generate contextually rich content combining images and text. Early work, such as OFA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> and BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, laid the groundwork for integrating visual and textual information. OFA aimed to unify vision-language tasks using large-scale pretraining, while BLIP improved data efficiency and model performance through self-supervised learning. Recent advancements, including ChatGPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>, have led to more powerful models capable of tasks like image generation from text, multimodal conversation, and advanced visual reasoning.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">One key application is Visual Question Answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>, where the model answers questions based on an image. This task requires understanding both the image and the natural language question, combining image recognition, language processing, and reasoning. Visual grounding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> is a specific task within the broader domain of VQA. In VQA, the primary objective is to answer questions about an image using natural language understanding. Visual grounding within VQA focuses on precisely locating and identifying objects or regions in the image that correspond to specific elements mentioned in the textual query. This task requires the model to effectively link the textual description with relevant visual features in the image, enabling accurate and contextually relevant answers to questions posed about visual content.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">VLP models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> achieve visual grounding tasks by first pretraining on large-scale datasets containing paired image and text data. These models extract feature embeddings from images using CNNs and process textual descriptions using transformer architectures like BERT. Through cross-modal attention mechanisms, they align visual and textual features, enabling precise linking of natural language queries to specific visual elements in images. Fine-tuning on task-specific datasets further refines their ability to understand and respond to queries that require nuanced visual comprehension within applications such as VQA and multimodal interaction systems.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">With the rapid development of multimodal pre-trained models, they have shown strong generalization capabilities in various downstream tasks, such as visual grounding, visual question answering, cross-modal retrieval, image captioning, and more. We found that the visual grounding task refers to predicting the bounding box for a given text in an image, which inspired us to transform the competition task, namely visual question grounding, into a visual grounding task, effectively leveraging the power of multimodal pre-trained models.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In the first stage, we collected and constructed a large amount of external data for coarse tuning of the model, with image distributions, question distributions, and label distributions similar to the competition data. Secondly, we used the OFA model as the foundation and coarse-tuned a model with strong generalization and rich semantics. In the second stage, we loaded the coarse-tuned weights from the previous stage, treated the competition task as a visual grounding task, constructed different question templates, and continued fine-tuning the OFA model on the competition dataset. Finally, we designed a bounding box matching and replacing the post-processing strategy. The bounding box position predicted by visual grounding is roughly correct, but the accuracy of coordinates is not higher than that of an object detector. Therefore, by calculating the IoU, we replaced the predicted bounding box with the object detector’s bounding box to correct the model’s prediction results. Our team secured the runner-up position on the final leaderboard with a score of 76.342.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Visual-Language Pretraining (VLP)</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Recent developments in VLP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> have been propelled by pioneering models such as OFA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> and BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>. These models have innovatively utilized large-scale datasets to pretrain transformer-based architectures on paired image-text data. OFA aims to unify various vision-language tasks by leveraging comprehensive pretraining, thereby enhancing performance in applications such as VQA and image-text retrieval. Meanwhile, BLIP focuses on optimizing data efficiency and model performance through advanced self-supervised learning techniques, contributing significantly to the robustness of multimodal understanding. In this evolving landscape, ChatGPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> has emerged as a notable advancement, showcasing remarkable capabilities in generating nuanced natural language responses and pushing the boundaries of language modeling tasks.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Visual Question Answering (VQA)</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> involves answering questions about images using natural language understanding, which has been a pivotal area of research in multimodal AI. These tasks require models to comprehend both the visual content of images and the semantic context of textual queries. Early approaches focused on integrating vision and language through methods like joint embeddings and attention mechanisms. More recent advancements, leveraging models such as OFA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> and BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, have significantly improved the accuracy and efficiency of VQA systems by employing large-scale pretraining on diverse datasets. These models aim to enhance the model’s ability to reason about complex scenes and provide accurate responses based on multimodal inputs. VQA continues to evolve with the introduction of transformer-based architectures like ChatGPT-4, which further pushes the boundaries of multimodal understanding and response generation in AI systems.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Visual Grounding (VG)</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Visual grounding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> tasks involve the localization and identification of specific objects or regions within an image based on textual descriptions. This task is crucial in multimodal AI applications, where models need to link natural language queries to corresponding visual elements effectively. By leveraging deep learning techniques, such as convolutional neural networks (CNNs) for image feature extraction and transformer architectures for textual understanding, models can align visual and textual modalities. This alignment enables precise object localization, scene understanding, and context-based reasoning, enhancing applications like VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, image retrieval, and interactive systems that require accurate interpretation and response generation based on visual content.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The solution of this competition is based on the OFA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> visual language pre-training model, so we introduce the OFA model first. OFA is a large multi-modal pre-training model that transforms all pre-training tasks into text generation tasks so that all pre-training tasks can be performed by one decoder module. For example, VG, visual grounding pre-training task construct the template which region does the text ”Man in white shirt” describe? bounding box coordinates are generated by the decoder module; ITM, image text match pre-training task, construct the template, Does the image describe ”Two boys playing frisbee on the grass”? yes or no is generated by the same decoder module. It can be seen, that the visual grounding task is very similar to the competition, which are both output bounding box coordinates. Therefore, we transformed the competition task into visual Grounding task and used the large pre-training model for fine-tuning.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Our methods mainly include three parts, respectively Coarse tuning, Fine tuning, and Postprocessing. In the first part, Coarse tuning aims to build a synthetic dataset similar to the competition dataset and train a weak semantics but strong generalization model on the synthetic dataset. The weak semantics is because the synthetic dataset contains dirty data and labels so the semantic learning is not accurate enough. Strong generalization is due to the large scale of the synthetic dataset, so the generalization ability is relatively strong. In the second part, the Fine-tuning stage loads the coarse-tuning part weight and continues to train the competition dataset. The third part, post-processing, through bounding box matching, replacing, and model ensemble, further improves the accuracy of model prediction.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2407.04255/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="291" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">Coarse Tuning Stage.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Coarse Tuning Stage</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The first part is the coarse tuning stage. We construct the coarse-tuning dataset through coco images. There are several features of constructing synthetic datasets. First, the image distribution, question distribution, and bounding box distribution of the coarse tuning dataset should be as close as possible to the competition dataset, so that the model can avoid learning the wrong data distribution; Secondly, any sampling operation must be Random Sampling, which can ensure that the model will not learn any data bias. Finally, do not sample any data in the test public set, which can avoid overfitting the model to the test public set. The coarse-tuning paradigm should be the same as the competition task, similar to the pre-training, save the coarse-tuned model weight for the next fine-tuning part. similar to the pre-training, the more coarse tuning epochs, the stronger generalization, and the better performance.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">This figure illustrates our process of constructing the synthetic dataset. For each sample in the training set, we directly input images and questions into the multi-modal pre-training model to obtain textual answers, which we define as pseudo-answers. Now, each sample consists of an image, a question, and a pseudo-answer; therefore, we can build a mapping table of pseudo-answers and questions, where one pseudo-answer corresponds to multiple questions, for example, the pseudo-answers clock and vase. Then, randomly selected an image from the coco and detected several objects by the object detector. Each detected object includes confidence, class counts, class name, and bounding box, where class number is defined as the number of the object in the image, such as the number of clocks is one and the number of roll paper is two. Next, we sorted the detected objects according to confidence, selected an object class with the largest confidence, the class count is one, and belonging to the pseudo answer, then, randomly selected a question corresponding to the pseudo answer from the mapping table. Now, a new sample is obtained, the question of the sample is randomly selected question, the pseudo-answer is the selected object class, and the target bounding box is obtained by the object detector. After the synthetic dataset is constructed, a model with weak semantics but strong generalization can be coarse tuning in the form of visual grounding.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2407.04255/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="295" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Fine Tuning Stage.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Fine-tuning Stage</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The second part is the Fine-tuning stage. In the form of a visual grounding task, load the trained weight obtained by the last Coarse tuning part and continue to train the model on the competition dataset. Each sample consists of an image, a question, and a pseudo-answer. Question and pseudo answer are combined into the template, which is used as text input of the model. For the four templates we constructed, we finally adopted the second template, and the first word of the question, such as what, where, and which, was replaced with which region. After the template construction, the template and image are input into the OFA model, through text encoder, image encoder, and cross-modal fusion, the coordinates of the bounding box are finally output by text decoder.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2407.04255/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="305" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Postprocessing Stage.</span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Postprocessing Stage</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The third part is the post-processing stage, the bounding box position predicted by OFA is roughly correct, but the accuracy of coordinates is not higher than object detector. Therefore, by calculating the iou, we replace the predicted bounding box with the object detector’s bounding box. For each image, all bounding boxes in the image are detected by object detectors yolor and vitDet, and sorted according to the confidence, which are called candidate Bounding boxes. Then, for the predicted bounding box, the first candidate bounding box with iou higher than zero point six is selected to replace the predicted bounding box. Finally, the model ensemble includes three ways, the first is five folds with standard Corase tuning, the second is ten folds with standard Corase tuning, and the third is five folds with back translation Corase tuning. The back translation was applied to the coarse tuning stage, which expanded the diversity of question data.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset.</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The Toloka dataset comprises images paired with textual questions, where each entry includes a question-image pair annotated with ground truth bounding box coordinates pinpointing the visual answer. In total, the dataset consists of 45,199 instances distributed across three subsets: 38,990 instances in the training set, 1,705 instances in the public test set, and 4,504 instances in the private test set.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The dataset is structured with several key columns: ”image” contains URLs linking to images hosted on a public content delivery network; ”question” provides English-language queries associated with each image. Additional metadata includes ”width” and ”height” integers representing the dimensions of each image. For bounding box annotation, the dataset includes ”left,” ”top,” ”right,” and ”bottom” integers detailing the coordinates that define the spatial extent of the object or region in the image that corresponds to the answer to the question.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Leadboards.</h3>

<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.1.1" class="ltx_tr">
<th id="S4.T1.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding:1.5pt 10.0pt;">Method</th>
<th id="S4.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:1.5pt 10.0pt;">Score</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2.1" class="ltx_tr">
<td id="S4.T1.2.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 10.0pt;">Baseline</td>
<td id="S4.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 10.0pt;">71.0</td>
</tr>
<tr id="S4.T1.2.3.2" class="ltx_tr">
<td id="S4.T1.2.3.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 10.0pt;">Pseudo Answer</td>
<td id="S4.T1.2.3.2.2" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">73.5</td>
</tr>
<tr id="S4.T1.2.4.3" class="ltx_tr">
<td id="S4.T1.2.4.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 10.0pt;">Template</td>
<td id="S4.T1.2.4.3.2" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">74.2</td>
</tr>
<tr id="S4.T1.2.5.4" class="ltx_tr">
<td id="S4.T1.2.5.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 10.0pt;">Coarse Tuning</td>
<td id="S4.T1.2.5.4.2" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">75.1</td>
</tr>
<tr id="S4.T1.2.6.5" class="ltx_tr">
<td id="S4.T1.2.6.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 10.0pt;">Postprocessing</td>
<td id="S4.T1.2.6.5.2" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">75.8</td>
</tr>
<tr id="S4.T1.2.7.6" class="ltx_tr">
<td id="S4.T1.2.7.6.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 10.0pt;">Test Public</td>
<td id="S4.T1.2.7.6.2" class="ltx_td ltx_align_center" style="padding:1.5pt 10.0pt;">76.5</td>
</tr>
<tr id="S4.T1.2.8.7" class="ltx_tr">
<td id="S4.T1.2.8.7.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:1.5pt 10.0pt;">Test Private</td>
<td id="S4.T1.2.8.7.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 10.0pt;">76.342</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">Results of each component.</span></figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ 4.2 Leadboards. ‣ 4 Experiments ‣ Second Place Solution of WSDM2023 Toloka Visual Question Answering Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the improvement in model performance by each of our components. The baseline is defined as the OFA model which directly inputs the competition dataset and reasoning. The performance of coarse tuning and pseudo answer increased the most because the pseudo answer showed the category of objects corresponding to the bounding box. On the test public set, our method obtained a seventy-six point five score, and on the test private set, our method obtained a seventy-six point three score, which shows that our method has strong generalization.


<span id="S4.SS2.p1.1.1" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Chongyan Chen, Samreen Anjum, and Danna Gurari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Vqa therapy: Exploring answer differences by visually grounding answers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib1.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICCV</em><span id="bib.bib1.11.3" class="ltx_text" style="font-size:90%;">, pages 15269–15279. IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Ding et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Yang Ding, Jing Yu, Bang Liu, Yue Hu, Mingxin Cui, and Qi Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Mukea: Multimodal knowledge extraction and accumulation for knowledge-based visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib2.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib2.11.3" class="ltx_text" style="font-size:90%;">, pages 5079–5088. IEEE, 2022.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Fu et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Zhongtian Fu, Kefei Song, Luping Zhou, and Yang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Noise-aware image captioning with progressively exploring mismatched words.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib3.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">AAAI</em><span id="bib.bib3.11.3" class="ltx_text" style="font-size:90%;">, pages 12091–12099, 2024.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Gupta et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Vipul Gupta, Zhuowan Li, Adam Kortylewski, Chenyu Zhang, Yingwei Li, and Alan L. Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Swapmix: Diagnosing and regularizing the over-reliance on visual context in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib4.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib4.11.3" class="ltx_text" style="font-size:90%;">, pages 5068–5078. IEEE, 2022.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Ke et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Junjie Ke, Keren Ye, Jiahui Yu, Yonghui Wu, Peyman Milanfar, and Feng Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Vila: Learning image aesthetics from user comments with vision-language pretraining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib5.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib5.11.3" class="ltx_text" style="font-size:90%;">, pages 10041–10051, 2023.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib6.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib6.11.3" class="ltx_text" style="font-size:90%;">, pages 12888–12900, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Mingxiao Li, Zehao Wang, Tinne Tuytelaars, and Marie-Francine Moens.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Layout-aware dreamer for embodied visual referring expression grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib7.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">AAAI</em><span id="bib.bib7.11.3" class="ltx_text" style="font-size:90%;">, pages 1386–1395, 2023.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Xin-Chun Li, Shaoming Song, Yinchuan Li, Bingshuai Li, Yunfeng Shao, Yang Yang, and De-Chuan Zhan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">MAP: model aggregation and personalization in federated learning with incomplete classes.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib8.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib8.10.2" class="ltx_text" style="font-size:90%;">, abs/2404.09232, 2024.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Yang Liu, Jiahua Zhang, Qingchao Chen, and Yuxin Peng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Confidence-aware pseudo-label learning for weakly supervised visual grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib9.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICCV</em><span id="bib.bib9.11.3" class="ltx_text" style="font-size:90%;">, pages 2816–2826, 2023.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Meng et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Lingwu Meng, Jing Wang, Ran Meng, Yang Yang, and Liang Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">A multiscale grouping transformer with CLIP latents for remote sensing image captioning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib10.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Geosci. Remote. Sens.</em><span id="bib.bib10.10.2" class="ltx_text" style="font-size:90%;">, 62:1–15, 2024.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.4.4.1" class="ltx_text" style="font-size:90%;">OpenAI [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.6.1" class="ltx_text" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">Gpt-4 technical report.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib11.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib11.9.2" class="ltx_text" style="font-size:90%;">, abs/2303.08774, 2023.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Rigoni et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Davide Rigoni, Luca Parolari, Luciano Serafini, Alessandro Sperduti, and Lamberto Ballan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Weakly-supervised visual-textual grounding with semantic prior refinement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib12.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">BMCV</em><span id="bib.bib12.11.3" class="ltx_text" style="font-size:90%;">, page 229, 2023.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Tian et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Weidong Tian, Haodong Li, and Zhong-Qiu Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Dual capsule attention mask network with mutual learning for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib13.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">COLING</em><span id="bib.bib13.11.3" class="ltx_text" style="font-size:90%;">, pages 5678–5688, 2022.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib14.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib14.11.3" class="ltx_text" style="font-size:90%;">, pages 23318–23340, 2022.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">Image as a foreign language: BEIT pretraining for vision and vision-language tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib15.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib15.11.3" class="ltx_text" style="font-size:90%;">, pages 19175–19186, 2023.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Wu et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Xiangyu Wu, Qing-Yuan Jiang, Yang Yang, Yi-Feng Wu, Qingguo Chen, and Jianfeng Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Tai++: Text as image for multi-label image classification by co-learning transferable prompt.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib16.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib16.10.2" class="ltx_text" style="font-size:90%;">, abs/2405.06926, 2024.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Xi et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Wenjuan Xi, Xin Song, Weili Guo, and Yang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Robust semi-supervised learning for self-learning open-world classes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib17.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICDM</em><span id="bib.bib17.11.3" class="ltx_text" style="font-size:90%;">, pages 658–667, 2023.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Yang Yang, Jingshuai Zhang, Fan Gao, Xiaoru Gao, and Hengshu Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Domfn: A divergence-orientated multi-modal fusion network for resume assessment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib18.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM MM</em><span id="bib.bib18.11.3" class="ltx_text" style="font-size:90%;">, pages 1612–1620, 2022.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Yang Yang, Yurui Huang, Weili Guo, Baohua Xu, and Dingyin Xia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Towards global video scene segmentation with context-aware transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib19.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">AAAI</em><span id="bib.bib19.11.3" class="ltx_text" style="font-size:90%;">, pages 3206–3213, 2023a.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Yang Yang, Yuxuan Zhang, Xin Song, and Yi Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Not all out-of-distribution data are harmful to open-set active learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib20.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NeurIPS</em><span id="bib.bib20.11.3" class="ltx_text" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Yang Yang, Nan Jiang, Yi Xu, and De-Chuan Zhan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Robust semi-supervised learning by wisely leveraging open-set data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib21.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib21.10.2" class="ltx_text" style="font-size:90%;">, abs/2405.06979, 2024.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Zhan et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Huayi Zhan, Peixi Xiong, Xin Wang, Xin Wang, and Lan Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Visual question answering by pattern matching and reasoning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib22.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Neurocomputing</em><span id="bib.bib22.10.2" class="ltx_text" style="font-size:90%;">, 467:323–336, 2022.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Da-Wei Zhou, Yang Yang, and De-Chuan Zhan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Learning to classify with incremental new class.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib23.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Neural Networks Learn. Syst.</em><span id="bib.bib23.10.2" class="ltx_text" style="font-size:90%;">, 33(6):2429–2443, 2022.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Li Zhou, Zikun Zhou, Kaige Mao, and Zhenyu He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Joint visual grounding and tracking with natural language specification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib24.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib24.11.3" class="ltx_text" style="font-size:90%;">, pages 23151–23160, 2023.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Zhu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Hongguang Zhu, Yunchao Wei, Xiaodan Liang, Chunjie Zhang, and Yao Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Ctp: Towards vision-language continual pretraining via compatible momentum contrast and topology preservation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib25.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICCV</em><span id="bib.bib25.11.3" class="ltx_text" style="font-size:90%;">, pages 22200–22210, 2023.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Zou et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Bo Zou, Chao Yang, Chengbin Quan, and Youjian Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Spaceclip: A vision-language pretraining framework with spatial reconstruction on text.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib26.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM MM</em><span id="bib.bib26.11.3" class="ltx_text" style="font-size:90%;">, pages 519–528, 2023.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.04254" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.04255" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.04255">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.04255" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.04257" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 18:53:12 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
