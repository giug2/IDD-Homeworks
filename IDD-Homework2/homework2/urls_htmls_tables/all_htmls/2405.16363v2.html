<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>LLMs for User Interest Exploration in Large-scale Recommendation Systems</title>
<!--Generated on Fri Jun  7 18:05:02 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Large Language Models,  Recommendation System,  User Interest Exploration" lang="en" name="keywords"/>
<base href="/html/2405.16363v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S1" title="In LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S2" title="In LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S3" title="In LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S3.SS1" title="In 3. Method ‣ LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Preliminaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S3.SS2" title="In 3. Method ‣ LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Hybrid Hierarchical Planning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S3.SS3" title="In 3. Method ‣ LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Fine-Tuning for User Behavior Alignment</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S4" title="In LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Live Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S4.SS1" title="In 4. Live Experiments ‣ LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S4.SS2" title="In 4. Live Experiments ‣ LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results and Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S5" title="In LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">LLMs for User Interest Exploration in Large-scale Recommendation Systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jianling Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Google DeepMind</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jianlingw@google.com">jianlingw@google.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haokai Lu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id2.1.id1">Google DeepMind</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:haokai@google.com">haokai@google.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yifan Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id3.1.id1">Google</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yifanliu@google.com">yifanliu@google.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">He Ma
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Google</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:htm@google.com">htm@google.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yueqi Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">Google</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yueqiw@google.com">yueqiw@google.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yang Gu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id6.1.id1">Google</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:greeness@google.com">greeness@google.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuzhou Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Google</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:shuzhouz@google.com">shuzhouz@google.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ningren Han
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id8.1.id1">Google</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:peterhan@google.com">peterhan@google.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuchao Bi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">Google</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:shuchaobi@google.com">shuchaobi@google.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lexi Baugher
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">Google</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:baugher@google.com">baugher@google.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ed H. Chi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id11.1.id1">Google DeepMind</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:edchi@google.com">edchi@google.com</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Minmin Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id12.1.id1">Google DeepMind</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:minminc@google.com">minminc@google.com</a>
</span></span></span>
</div>
<div class="ltx_dates">(2018)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id13.id1">Traditional recommendation systems are subject to a strong feedback loop by learning from and reinforcing past user-item interactions, which in turn limits the discovery of novel user interests. To address this, we introduce a hybrid hierarchical framework combining Large Language Models (LLMs) and classic recommendation models for user interest exploration. The framework controls the interfacing between the LLMs and the classic recommendation models through “interest clusters”, the granularity of which can be explicitly determined by algorithm designers. It recommends the next novel interests by first representing “interest clusters” using language, and employs a fine-tuned LLM to generate novel interest descriptions that are strictly within these predefined clusters. At the low level, it grounds these generated interests to an item-level policy by restricting classic recommendation models, in this case a transformer-based sequence recommender to return items that fall within the novel clusters generated at the high level. We showcase the efficacy of this approach on an industrial-scale commercial platform serving billions of users. Live experiments show a significant increase in both exploration of novel interests and overall user enjoyment of the platform.</p>
</div>
<div class="ltx_keywords">Large Language Models, Recommendation System, User Interest Exploration
</div>
<div class="ltx_acknowledgements">* indicates Equal Contribution
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2018</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/1122445.1122456</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Woodstock ’18: ACM Symposium on Neural
Gaze Detection; June 03–05, 2018; Woodstock, NY</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Woodstock ’18: ACM Symposium on Neural Gaze Detection,
June 03–05, 2018, Woodstock, NY</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_price" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Information retrieval</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recommendation systems are indispensable in helping users navigate the vast and ever-growing content on the web nowadays. These systems however are often subject to a strong feedback loop <cite class="ltx_cite ltx_citemacro_citep">(Chaney et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib6" title="">2018</a>; Mansoury et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib27" title="">2020</a>)</cite>, recommending items similar to a user’s past behavior. Classic recommendation systems infer a user’s next interest based on their historical interactions. While this can be effective for short-term engagement, it limits users from discovering novel interests, leading to content fatigue. Recent research highlights the importance of <span class="ltx_text ltx_font_bold" id="S1.p1.1.1">user interest exploration</span> <cite class="ltx_cite ltx_citemacro_citep">(Su et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib30" title="">2024</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib10" title="">2021</a>; Chen, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib8" title="">2021</a>; Mahajan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib26" title="">2023</a>; Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib29" title="">2022</a>)</cite>, aiming to introduce diverse content that goes beyond a user’s historical preferences. Effectively introducing novel interests to users are however challenging due to the vast interest space and the high uncertainty of a user’s affinity to previously unseen interests <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib10" title="">2021</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib33" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Recent breakthroughs in Large Language Models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(Anil et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib2" title="">2023</a>; Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib5" title="">2020</a>; Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib31" title="">2023</a>)</cite> and other foundation models offer exciting opportunities to revolutionize recommendation systems <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib3" title="">2023</a>; Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib12" title="">2023</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib23" title="">2024</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib32" title="">2024</a>; Christakopoulou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib11" title="">2023</a>)</cite>. The pre-trained world knowledge in these models holds the potential to break recommendation feedback loops by introducing diverse and serendipitous recommendations, addressing the challenge of user interest exploration. While prior work <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib21" title="">2023b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib22" title="">c</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib18" title="">2023b</a>; Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib14" title="">2022</a>)</cite> has demonstrated the potential of using LLMs for recommendation by translating recommendation problems into natural language processing tasks, deploying these approaches in <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">real-world</span> industrial recommendation systems remain extremely challenging as: (1) unlike domain-specific recommendation models, LLMs lack deep knowledge of the massive, and rapidly evolving item corpus on industrial-scale online platforms (e.g., more than 500 hours of content are uploaded to YouTube every minute <cite class="ltx_cite ltx_citemacro_citep">(Blog, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib4" title="">2023</a>; Hale, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib16" title="">2019</a>)</cite>, a new track is uploaded to Spotify every second <cite class="ltx_cite ltx_citemacro_citep">(Ingham, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib19" title="">2023</a>)</cite>); (2) off-the-shelf LLMs are unaware of the collaborative signals from users, failing to capture domain-specific user behaviors; and (3) the latency and cost of serving LLMs per user request are prohibitively large, cannot meet the O(100ms) response time expected and production Query-Per-Second (QPS) required on industrial recommendation platforms.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To overcome the above challenges, we introduce a <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">hybrid hierarchical planning</span> paradigm combining LLMs and classic recommendation models (as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_tag">1</span></a>) for user interest exploration in large-scale recommendation systems. At the high level, considering the massive number of incoming items in the system, instead of directly predicting the next item, we use LLMs to infer the next novel interest. At the low level, to leverage classic recommendation models with strong personalization, we ground these novel interests to item recommendations by ”restricting” conventional transformer-based sequence models <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib9" title="">2019</a>; Shaw et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib28" title="">2018</a>)</cite> to items within the ”clusters” defined by those novel interests.
By combining the best of both worlds, the hybrid approach leverages LLMs’ reasoning and generalization capability in exploring user’s novel interests effectively, at the same time bridges the knowledge gap by relying on domain-specific models for actual item recommendation.
We further perform supervised fine-tuning (SFT) with real-world novel consumption behaviors for in-domain user alignment, and also enable LLMs to perform <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">controlled generation</span>, producing novel interest descriptions that directly match one of the pre-defined clusters. The controlled generation allow algorithm designers to easily define the granularity of the interests generated by LLMs for different applications, which is critical for effectively exploring the interest space. We find diversification and label balance treatment while curating the SFT data significantly mitigate the long-tailed distribution of LLM generation, and thus improves interest exploration efficiency. To address the LLM inference challenge, we propose to use topically coherent interest clusters with cluster-level descriptions to represent recommendation objects, i.e, both the historical user interests and the recommended next novel interests. By using a small number of historical consumed clusters as high-level user interest, we pre-compute the novel interest transitions offline with LLM bulk inference, which can be then be served online with simple table lookup operations. In summary, we make the following contributions:</p>
</div>
<div class="ltx_para" id="S1.p4">
<ul class="ltx_itemize" id="S1.p4.1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i1.1.1.m1.1"><semantics id="S1.I1.i1.1.1.m1.1b"><mo id="S1.I1.i1.1.1.m1.1.1" xref="S1.I1.i1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i1.1.1.m1.1c"><ci id="S1.I1.i1.1.1.m1.1.1.cmml" xref="S1.I1.i1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i1.1.1.m1.1e">∙</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a hybrid hierarchical framework that combines LLM’s reasoning and generalization capabilities with classic recommendation models with strong personalization and grounded item corpus knowledge for effective user interest exploration.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i2.1.1.m1.1"><semantics id="S1.I1.i2.1.1.m1.1b"><mo id="S1.I1.i2.1.1.m1.1.1" xref="S1.I1.i2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i2.1.1.m1.1c"><ci id="S1.I1.i2.1.1.m1.1.1.cmml" xref="S1.I1.i2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i2.1.1.m1.1e">∙</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We fine-tune LLMs using a diverse and balanced set of novel interest transitions curated from real-world user interactions for controlled generation and user behavior alignment, to ensure LLMs generate novel interests that match one of the predefined interest ”clusters” and align with actual user behaviors.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i3.1.1.m1.1"><semantics id="S1.I1.i3.1.1.m1.1b"><mo id="S1.I1.i3.1.1.m1.1.1" xref="S1.I1.i3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i3.1.1.m1.1c"><ci id="S1.I1.i3.1.1.m1.1.1.cmml" xref="S1.I1.i3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i3.1.1.m1.1e">∙</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We propose to adopt topical clusters instead of items to represent user’s high-level interests. The coarser representation allows us to limit the length of historical cluster sequence used to represent dynamic user interests and move the expensive LLM inference to offline stage, making it feasible to serve LLM generated novel interest transitions online.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i4.1.1.m1.1"><semantics id="S1.I1.i4.1.1.m1.1b"><mo id="S1.I1.i4.1.1.m1.1.1" xref="S1.I1.i4.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i4.1.1.m1.1c"><ci id="S1.I1.i4.1.1.m1.1.1.cmml" xref="S1.I1.i4.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i4.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i4.1.1.m1.1e">∙</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We validate our method through live experiments on a large commercial recommendation platform with billions of users. The results clearly show our approach successfully expands user interests while boosting user enjoyment of the platform demonstrated through more active users with longer dwell time.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="121" id="S1.F1.g1" src="x1.png" width="291"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>LLM-powered hybrid hierarchical planning diagram for user interest exploration.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">LLMs for Recommendation Systems</span>.
Application of LLMs to recommendation systems is a rapidly growing research area. Some studies explore using LLMs directly for generating recommendations <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib3" title="">2023</a>; Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib12" title="">2023</a>; Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib15" title="">2023</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib18" title="">2023b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib22" title="">2023c</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib24" title="">2023b</a>)</cite>, while others focus on augmenting traditional recommendation models with LLM-powered feature engineering <cite class="ltx_cite ltx_citemacro_citep">(Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib17" title="">2023a</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib36" title="">2022</a>)</cite> or enriched user/item representations <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib20" title="">2023a</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib25" title="">2023a</a>; Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib34" title="">2023</a>)</cite>. The computational cost of LLMs however presents a critical challenge. Directly using them for large-scale retrieval is expensive and hinders adoption. Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib32" title="">2024</a>)</cite> use LLMs as data augmenters for conventional recommendation systems during training, to improve model performance without additional serving cost.
Different from prior work, we focus on directly incorporating LLM-generated content to break the feedback loop, aiming for more diverse and serendipitous recommendations while maintaining efficiency.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">User Interest Exploration</span>.
Prior research has established the benefits of User Interest Exploration in recommendation systems, demonstrating its ability to expand user preferences and enhance long-term engagement <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib10" title="">2021</a>; Chen, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib8" title="">2021</a>; Su et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib30" title="">2024</a>)</cite>. However, a key challenge lies in the inherent closed-loop nature of existing systems <cite class="ltx_cite ltx_citemacro_citep">(Chaney et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib6" title="">2018</a>; Mansoury et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib27" title="">2020</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib10" title="">2021</a>)</cite>. Training data is primarily derived from past user-item interactions, limiting the system’s ability to explore truly novel interests. While methods like PIE <cite class="ltx_cite ltx_citemacro_citep">(Mahajan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib26" title="">2023</a>)</cite> address this to certain extent through user-creator affinity and online bandit formulations, they remain confined by the system’s internal knowledge <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib10" title="">2021</a>)</cite>. Our work introduces a novel approach that integrates world knowledge from LLMs to overcome these limitations.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we introduce the hybrid hierarchical planning paradigm and the LLM fine-tuning process designed to enable controlled generation and user behavior alignment, to apply LLMs in real-world large-scale recommenders.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Preliminaries</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.5">The sheer number of items and the constant influx of new items on online platforms make LLM planning at the individual item-level infeasible. Instead, we leverage the planning capabilities of LLMs at the item interest-level to reduce the planning space. A prerequisite for efficient hierarchical planning is a set of high-quality <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.5.1">item interest clusters</span>, where items within each cluster are topically coherent. Following the same procedure as in <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib7" title="">2024</a>)</cite>, we group items into <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_N</annotation></semantics></math> traffic-weighted equal sized <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.5.2">clusters</span> based on their topical coherence, a method proven to scale well to the magnitude of our problem. To create these clusters, we first represent each item as a 256-dimensional embedding based on its metadata (title, hashtags, etc.) and content (frames and audio). Then, we connect items in a graph based on their similarity and cluster it into traffic-balanced clusters. This clustering process is repeated multiple times to create a 4-level tree structure, with each item associated with different tree levels. Higher-level clusters represent broader topics, while lower-level clusters represent more specific ones. These clusters in each level, denoted by <math alttext="\mathcal{C}^{l}=\{c_{1}^{l},c_{2}^{l},...,c_{M_{l}}^{l}\},l=1,2,3,4" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.7"><semantics id="S3.SS1.p1.2.m2.7a"><mrow id="S3.SS1.p1.2.m2.7.7.2" xref="S3.SS1.p1.2.m2.7.7.3.cmml"><mrow id="S3.SS1.p1.2.m2.6.6.1.1" xref="S3.SS1.p1.2.m2.6.6.1.1.cmml"><msup id="S3.SS1.p1.2.m2.6.6.1.1.5" xref="S3.SS1.p1.2.m2.6.6.1.1.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.2.m2.6.6.1.1.5.2" xref="S3.SS1.p1.2.m2.6.6.1.1.5.2.cmml">𝒞</mi><mi id="S3.SS1.p1.2.m2.6.6.1.1.5.3" xref="S3.SS1.p1.2.m2.6.6.1.1.5.3.cmml">l</mi></msup><mo id="S3.SS1.p1.2.m2.6.6.1.1.4" xref="S3.SS1.p1.2.m2.6.6.1.1.4.cmml">=</mo><mrow id="S3.SS1.p1.2.m2.6.6.1.1.3.3" xref="S3.SS1.p1.2.m2.6.6.1.1.3.4.cmml"><mo id="S3.SS1.p1.2.m2.6.6.1.1.3.3.4" stretchy="false" xref="S3.SS1.p1.2.m2.6.6.1.1.3.4.cmml">{</mo><msubsup id="S3.SS1.p1.2.m2.6.6.1.1.1.1.1" xref="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.2.2" xref="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.2.2.cmml">c</mi><mn id="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.2.3" xref="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.2.3.cmml">1</mn><mi id="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.3" xref="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.3.cmml">l</mi></msubsup><mo id="S3.SS1.p1.2.m2.6.6.1.1.3.3.5" xref="S3.SS1.p1.2.m2.6.6.1.1.3.4.cmml">,</mo><msubsup id="S3.SS1.p1.2.m2.6.6.1.1.2.2.2" xref="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.cmml"><mi id="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.2.2" xref="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.2.2.cmml">c</mi><mn id="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.2.3" xref="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.2.3.cmml">2</mn><mi id="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.3" xref="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.3.cmml">l</mi></msubsup><mo id="S3.SS1.p1.2.m2.6.6.1.1.3.3.6" xref="S3.SS1.p1.2.m2.6.6.1.1.3.4.cmml">,</mo><mi id="S3.SS1.p1.2.m2.1.1" mathvariant="normal" xref="S3.SS1.p1.2.m2.1.1.cmml">…</mi><mo id="S3.SS1.p1.2.m2.6.6.1.1.3.3.7" xref="S3.SS1.p1.2.m2.6.6.1.1.3.4.cmml">,</mo><msubsup id="S3.SS1.p1.2.m2.6.6.1.1.3.3.3" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.cmml"><mi id="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.2" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.2.cmml">c</mi><msub id="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.3" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.3.cmml"><mi id="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.3.2" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.3.2.cmml">M</mi><mi id="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.3.3" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.3.3.cmml">l</mi></msub><mi id="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.3" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.3.cmml">l</mi></msubsup><mo id="S3.SS1.p1.2.m2.6.6.1.1.3.3.8" stretchy="false" xref="S3.SS1.p1.2.m2.6.6.1.1.3.4.cmml">}</mo></mrow></mrow><mo id="S3.SS1.p1.2.m2.7.7.2.3" xref="S3.SS1.p1.2.m2.7.7.3a.cmml">,</mo><mrow id="S3.SS1.p1.2.m2.7.7.2.2" xref="S3.SS1.p1.2.m2.7.7.2.2.cmml"><mi id="S3.SS1.p1.2.m2.7.7.2.2.2" xref="S3.SS1.p1.2.m2.7.7.2.2.2.cmml">l</mi><mo id="S3.SS1.p1.2.m2.7.7.2.2.1" xref="S3.SS1.p1.2.m2.7.7.2.2.1.cmml">=</mo><mrow id="S3.SS1.p1.2.m2.7.7.2.2.3.2" xref="S3.SS1.p1.2.m2.7.7.2.2.3.1.cmml"><mn id="S3.SS1.p1.2.m2.2.2" xref="S3.SS1.p1.2.m2.2.2.cmml">1</mn><mo id="S3.SS1.p1.2.m2.7.7.2.2.3.2.1" xref="S3.SS1.p1.2.m2.7.7.2.2.3.1.cmml">,</mo><mn id="S3.SS1.p1.2.m2.3.3" xref="S3.SS1.p1.2.m2.3.3.cmml">2</mn><mo id="S3.SS1.p1.2.m2.7.7.2.2.3.2.2" xref="S3.SS1.p1.2.m2.7.7.2.2.3.1.cmml">,</mo><mn id="S3.SS1.p1.2.m2.4.4" xref="S3.SS1.p1.2.m2.4.4.cmml">3</mn><mo id="S3.SS1.p1.2.m2.7.7.2.2.3.2.3" xref="S3.SS1.p1.2.m2.7.7.2.2.3.1.cmml">,</mo><mn id="S3.SS1.p1.2.m2.5.5" xref="S3.SS1.p1.2.m2.5.5.cmml">4</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.7b"><apply id="S3.SS1.p1.2.m2.7.7.3.cmml" xref="S3.SS1.p1.2.m2.7.7.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.7.7.3a.cmml" xref="S3.SS1.p1.2.m2.7.7.2.3">formulae-sequence</csymbol><apply id="S3.SS1.p1.2.m2.6.6.1.1.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1"><eq id="S3.SS1.p1.2.m2.6.6.1.1.4.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.4"></eq><apply id="S3.SS1.p1.2.m2.6.6.1.1.5.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.5"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.6.6.1.1.5.1.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.5">superscript</csymbol><ci id="S3.SS1.p1.2.m2.6.6.1.1.5.2.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.5.2">𝒞</ci><ci id="S3.SS1.p1.2.m2.6.6.1.1.5.3.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.5.3">𝑙</ci></apply><set id="S3.SS1.p1.2.m2.6.6.1.1.3.4.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3"><apply id="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.1.1.1">superscript</csymbol><apply id="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.2.2">𝑐</ci><cn id="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.2.3.cmml" type="integer" xref="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.2.3">1</cn></apply><ci id="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.1.1.1.3">𝑙</ci></apply><apply id="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.1.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.2.2.2">superscript</csymbol><apply id="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.2.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.2.1.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.2.2.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.2.2">𝑐</ci><cn id="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.2.3.cmml" type="integer" xref="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.2.3">2</cn></apply><ci id="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.3.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.2.2.2.3">𝑙</ci></apply><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">…</ci><apply id="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.1.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3.3">superscript</csymbol><apply id="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.1.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.2.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.2">𝑐</ci><apply id="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.3.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.3.1.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.3.2.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.3.2">𝑀</ci><ci id="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.3.3.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.2.3.3">𝑙</ci></apply></apply><ci id="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.3.cmml" xref="S3.SS1.p1.2.m2.6.6.1.1.3.3.3.3">𝑙</ci></apply></set></apply><apply id="S3.SS1.p1.2.m2.7.7.2.2.cmml" xref="S3.SS1.p1.2.m2.7.7.2.2"><eq id="S3.SS1.p1.2.m2.7.7.2.2.1.cmml" xref="S3.SS1.p1.2.m2.7.7.2.2.1"></eq><ci id="S3.SS1.p1.2.m2.7.7.2.2.2.cmml" xref="S3.SS1.p1.2.m2.7.7.2.2.2">𝑙</ci><list id="S3.SS1.p1.2.m2.7.7.2.2.3.1.cmml" xref="S3.SS1.p1.2.m2.7.7.2.2.3.2"><cn id="S3.SS1.p1.2.m2.2.2.cmml" type="integer" xref="S3.SS1.p1.2.m2.2.2">1</cn><cn id="S3.SS1.p1.2.m2.3.3.cmml" type="integer" xref="S3.SS1.p1.2.m2.3.3">2</cn><cn id="S3.SS1.p1.2.m2.4.4.cmml" type="integer" xref="S3.SS1.p1.2.m2.4.4">3</cn><cn id="S3.SS1.p1.2.m2.5.5.cmml" type="integer" xref="S3.SS1.p1.2.m2.5.5">4</cn></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.7c">\mathcal{C}^{l}=\{c_{1}^{l},c_{2}^{l},...,c_{M_{l}}^{l}\},l=1,2,3,4</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.7d">caligraphic_C start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = { italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , … , italic_c start_POSTSUBSCRIPT italic_M start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT } , italic_l = 1 , 2 , 3 , 4</annotation></semantics></math>, represent different user interests, with each cluster linked to a <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.5.3">set of keywords</span> describing its theme. Here <math alttext="M_{l}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">M</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝑀</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">M_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_M start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> denotes the number of clusters within level <math alttext="l" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_l</annotation></semantics></math>, with <math alttext="M_{4}&gt;M_{3}&gt;M_{2}&gt;M_{1}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mrow id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><msub id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2.2" xref="S3.SS1.p1.5.m5.1.1.2.2.cmml">M</mi><mn id="S3.SS1.p1.5.m5.1.1.2.3" xref="S3.SS1.p1.5.m5.1.1.2.3.cmml">4</mn></msub><mo id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">&gt;</mo><msub id="S3.SS1.p1.5.m5.1.1.4" xref="S3.SS1.p1.5.m5.1.1.4.cmml"><mi id="S3.SS1.p1.5.m5.1.1.4.2" xref="S3.SS1.p1.5.m5.1.1.4.2.cmml">M</mi><mn id="S3.SS1.p1.5.m5.1.1.4.3" xref="S3.SS1.p1.5.m5.1.1.4.3.cmml">3</mn></msub><mo id="S3.SS1.p1.5.m5.1.1.5" xref="S3.SS1.p1.5.m5.1.1.5.cmml">&gt;</mo><msub id="S3.SS1.p1.5.m5.1.1.6" xref="S3.SS1.p1.5.m5.1.1.6.cmml"><mi id="S3.SS1.p1.5.m5.1.1.6.2" xref="S3.SS1.p1.5.m5.1.1.6.2.cmml">M</mi><mn id="S3.SS1.p1.5.m5.1.1.6.3" xref="S3.SS1.p1.5.m5.1.1.6.3.cmml">2</mn></msub><mo id="S3.SS1.p1.5.m5.1.1.7" xref="S3.SS1.p1.5.m5.1.1.7.cmml">&gt;</mo><msub id="S3.SS1.p1.5.m5.1.1.8" xref="S3.SS1.p1.5.m5.1.1.8.cmml"><mi id="S3.SS1.p1.5.m5.1.1.8.2" xref="S3.SS1.p1.5.m5.1.1.8.2.cmml">M</mi><mn id="S3.SS1.p1.5.m5.1.1.8.3" xref="S3.SS1.p1.5.m5.1.1.8.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><and id="S3.SS1.p1.5.m5.1.1a.cmml" xref="S3.SS1.p1.5.m5.1.1"></and><apply id="S3.SS1.p1.5.m5.1.1b.cmml" xref="S3.SS1.p1.5.m5.1.1"><gt id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3"></gt><apply id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.2.1.cmml" xref="S3.SS1.p1.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2.2">𝑀</ci><cn id="S3.SS1.p1.5.m5.1.1.2.3.cmml" type="integer" xref="S3.SS1.p1.5.m5.1.1.2.3">4</cn></apply><apply id="S3.SS1.p1.5.m5.1.1.4.cmml" xref="S3.SS1.p1.5.m5.1.1.4"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.4.1.cmml" xref="S3.SS1.p1.5.m5.1.1.4">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.4.2.cmml" xref="S3.SS1.p1.5.m5.1.1.4.2">𝑀</ci><cn id="S3.SS1.p1.5.m5.1.1.4.3.cmml" type="integer" xref="S3.SS1.p1.5.m5.1.1.4.3">3</cn></apply></apply><apply id="S3.SS1.p1.5.m5.1.1c.cmml" xref="S3.SS1.p1.5.m5.1.1"><gt id="S3.SS1.p1.5.m5.1.1.5.cmml" xref="S3.SS1.p1.5.m5.1.1.5"></gt><share href="https://arxiv.org/html/2405.16363v2#S3.SS1.p1.5.m5.1.1.4.cmml" id="S3.SS1.p1.5.m5.1.1d.cmml" xref="S3.SS1.p1.5.m5.1.1"></share><apply id="S3.SS1.p1.5.m5.1.1.6.cmml" xref="S3.SS1.p1.5.m5.1.1.6"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.6.1.cmml" xref="S3.SS1.p1.5.m5.1.1.6">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.6.2.cmml" xref="S3.SS1.p1.5.m5.1.1.6.2">𝑀</ci><cn id="S3.SS1.p1.5.m5.1.1.6.3.cmml" type="integer" xref="S3.SS1.p1.5.m5.1.1.6.3">2</cn></apply></apply><apply id="S3.SS1.p1.5.m5.1.1e.cmml" xref="S3.SS1.p1.5.m5.1.1"><gt id="S3.SS1.p1.5.m5.1.1.7.cmml" xref="S3.SS1.p1.5.m5.1.1.7"></gt><share href="https://arxiv.org/html/2405.16363v2#S3.SS1.p1.5.m5.1.1.6.cmml" id="S3.SS1.p1.5.m5.1.1f.cmml" xref="S3.SS1.p1.5.m5.1.1"></share><apply id="S3.SS1.p1.5.m5.1.1.8.cmml" xref="S3.SS1.p1.5.m5.1.1.8"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.8.1.cmml" xref="S3.SS1.p1.5.m5.1.1.8">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.8.2.cmml" xref="S3.SS1.p1.5.m5.1.1.8.2">𝑀</ci><cn id="S3.SS1.p1.5.m5.1.1.8.3.cmml" type="integer" xref="S3.SS1.p1.5.m5.1.1.8.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">M_{4}&gt;M_{3}&gt;M_{2}&gt;M_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_M start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT &gt; italic_M start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT &gt; italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT &gt; italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>. Each item belongs to a single interest cluster in each level. As discussed in <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib7" title="">2024</a>)</cite>, we focus on level-2 clusters to balance granularity and feasible planning space<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>To reduce notation complexity, we drop level <math alttext="l" class="ltx_Math" display="inline" id="footnote1.m1.1"><semantics id="footnote1.m1.1b"><mi id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><ci id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">l</annotation><annotation encoding="application/x-llamapun" id="footnote1.m1.1e">italic_l</annotation></semantics></math> in cluster moving forward.</span></span></span>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Hybrid Hierarchical Planning</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The hybrid approach combines a LLM to produce a language policy that generates novel interests on the high-level, and a classic recommendation models to produce an item policy that grounds these language-based interests to the low-level item space.
Such a hybrid approach combines the strength of LLMs in reasoning and generalization, and domain-specific recommendation models in handling item dynamics and enhanced personalization.
</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">High-level language policy.</span>
Given the historical user interest representation by language, we first use LLM to learn a high-level language policy that generates novel user interests. Instead of using item descriptions to represent users, we propose to adopt cluster descriptions (i.e., a set of keywords) to represent user’s consumption history, i.e., a user’s historical interest is represented as a sequence of her <math alttext="K" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_K</annotation></semantics></math> most recent interacted unique clusters, with each cluster represented by its description. Specifically, with a user’s previously consumed unique clusters, we can ask LLM to generate the next novel interest with the prompt illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S3.F2" title="Figure 2 ‣ 3.2. Hybrid Hierarchical Planning ‣ 3. Method ‣ LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="163" id="S3.F2.g1" src="x2.png" width="373"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Prompt for Novel Interest Prediction when <math alttext="K=2" class="ltx_Math" display="inline" id="S3.F2.2.m1.1"><semantics id="S3.F2.2.m1.1b"><mrow id="S3.F2.2.m1.1.1" xref="S3.F2.2.m1.1.1.cmml"><mi id="S3.F2.2.m1.1.1.2" xref="S3.F2.2.m1.1.1.2.cmml">K</mi><mo id="S3.F2.2.m1.1.1.1" xref="S3.F2.2.m1.1.1.1.cmml">=</mo><mn id="S3.F2.2.m1.1.1.3" xref="S3.F2.2.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.2.m1.1c"><apply id="S3.F2.2.m1.1.1.cmml" xref="S3.F2.2.m1.1.1"><eq id="S3.F2.2.m1.1.1.1.cmml" xref="S3.F2.2.m1.1.1.1"></eq><ci id="S3.F2.2.m1.1.1.2.cmml" xref="S3.F2.2.m1.1.1.2">𝐾</ci><cn id="S3.F2.2.m1.1.1.3.cmml" type="integer" xref="S3.F2.2.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.m1.1d">K=2</annotation><annotation encoding="application/x-llamapun" id="S3.F2.2.m1.1e">italic_K = 2</annotation></semantics></math>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.3"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.3.1">Practical implication.</span> One major challenge of deploying LLM to industrial-scale recommendation system lies in its prohibitively high inference cost failing to meet latency and QPS requirements. Empirically, we find that relying on a small number of historical clusters to represent each user (e.g., <math alttext="K=2" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">K</mi><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><eq id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"></eq><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝐾</ci><cn id="S3.SS2.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">K=2</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">italic_K = 2</annotation></semantics></math>) can effectively balance representation granularity and computation efficiency. In our experiment, the level-2 clustering produced 761 clusters. We can therefore enumerate all <math alttext="761*761=579,121" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.2"><semantics id="S3.SS2.p3.2.m2.2a"><mrow id="S3.SS2.p3.2.m2.2.3" xref="S3.SS2.p3.2.m2.2.3.cmml"><mrow id="S3.SS2.p3.2.m2.2.3.2" xref="S3.SS2.p3.2.m2.2.3.2.cmml"><mn id="S3.SS2.p3.2.m2.2.3.2.2" xref="S3.SS2.p3.2.m2.2.3.2.2.cmml">761</mn><mo id="S3.SS2.p3.2.m2.2.3.2.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p3.2.m2.2.3.2.1.cmml">∗</mo><mn id="S3.SS2.p3.2.m2.2.3.2.3" xref="S3.SS2.p3.2.m2.2.3.2.3.cmml">761</mn></mrow><mo id="S3.SS2.p3.2.m2.2.3.1" xref="S3.SS2.p3.2.m2.2.3.1.cmml">=</mo><mrow id="S3.SS2.p3.2.m2.2.3.3.2" xref="S3.SS2.p3.2.m2.2.3.3.1.cmml"><mn id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">579</mn><mo id="S3.SS2.p3.2.m2.2.3.3.2.1" xref="S3.SS2.p3.2.m2.2.3.3.1.cmml">,</mo><mn id="S3.SS2.p3.2.m2.2.2" xref="S3.SS2.p3.2.m2.2.2.cmml">121</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.2b"><apply id="S3.SS2.p3.2.m2.2.3.cmml" xref="S3.SS2.p3.2.m2.2.3"><eq id="S3.SS2.p3.2.m2.2.3.1.cmml" xref="S3.SS2.p3.2.m2.2.3.1"></eq><apply id="S3.SS2.p3.2.m2.2.3.2.cmml" xref="S3.SS2.p3.2.m2.2.3.2"><times id="S3.SS2.p3.2.m2.2.3.2.1.cmml" xref="S3.SS2.p3.2.m2.2.3.2.1"></times><cn id="S3.SS2.p3.2.m2.2.3.2.2.cmml" type="integer" xref="S3.SS2.p3.2.m2.2.3.2.2">761</cn><cn id="S3.SS2.p3.2.m2.2.3.2.3.cmml" type="integer" xref="S3.SS2.p3.2.m2.2.3.2.3">761</cn></apply><list id="S3.SS2.p3.2.m2.2.3.3.1.cmml" xref="S3.SS2.p3.2.m2.2.3.3.2"><cn id="S3.SS2.p3.2.m2.1.1.cmml" type="integer" xref="S3.SS2.p3.2.m2.1.1">579</cn><cn id="S3.SS2.p3.2.m2.2.2.cmml" type="integer" xref="S3.SS2.p3.2.m2.2.2">121</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.2c">761*761=579,121</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.2d">761 ∗ 761 = 579 , 121</annotation></semantics></math> cluster pairs and perform a batch inference with LLM to obtain novel interests for each cluster pair under just a few hours. These novel interests, together with the input cluster pairs, could be stored in a table. During <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.3.2">online serving</span> as a new user request comes in, we first represent the user by sampling <math alttext="K=2" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><mrow id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">K</mi><mo id="S3.SS2.p3.3.m3.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><eq id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1"></eq><ci id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2">𝐾</ci><cn id="S3.SS2.p3.3.m3.1.1.3.cmml" type="integer" xref="S3.SS2.p3.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">K=2</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">italic_K = 2</annotation></semantics></math> items from their watch history<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>30-day user history are used in sampling, and items with high-quality interactions are more likely to be sampled.</span></span></span>, and convert them into the cluster pair for lookup to determine the recommended novel interest cluster.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">Low-level item policy.</span> Once the language based novel user interest is obtained, the next step is to convert it to item-level recommendation policy. A straight-forward approach is to rely on search engine <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib22" title="">2023c</a>)</cite> to retrieve the most relevant items according to the keywords of the novel interest. The search results however often lack personalization since these language based novel interests can still be broad and lack specificity. To enhance personalization, we propose to reuse domain specific recommendation models, specifically <span class="ltx_text ltx_font_italic" id="S3.SS2.p4.1.2">transformer-based sequential recommender</span> model <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib9" title="">2019</a>; Shaw et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib28" title="">2018</a>)</cite>, but restrict the items to clusters prescribed by the language based novel interests. Specifically, we follow the two steps: (i) map the generated novel interests to cluster ID space, and (ii) restrict the original item level softmax policy on these cluster IDs, to retrieve items <span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.3">only</span> from these clusters.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="145" id="S3.F3.g1" src="x3.png" width="232"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Label (i.e., generated by fine-tuned LLM) Distribution: X-axis represents label frequency; Y-axis represents the percentage of labels within each frequency range.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p5.1.1">Controlled generation.</span> The remaining challenges are how to specify the granularity of the LLM generated interests and map the generated novel interests to cluster IDs. Free-form responses from LLMs can be arbitrary, which are unlikely to be directly matched to the predefined cluster descriptions. We control the granularity of the generation through the hierarchical clustering and picking the cluster level explained in Section  <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S3.SS1" title="3.1. Preliminaries ‣ 3. Method ‣ LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_tag">3.1</span></a>. Furthermore, appropriate finetuning as detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S3.SS3" title="3.3. Fine-Tuning for User Behavior Alignment ‣ 3. Method ‣ LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_tag">3.3</span></a> enables LLMs to speak the language of interest clusters, producing cluster descriptions that exactly match one of the predefined clusters.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" id="S3.F4.1" style="width:374.9pt;height:114.6pt;vertical-align:-114.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.6pt,0.0pt) scale(0.88,0.88) ;">
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S3.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="179" id="S3.F4.sf1.g1" src="x4.png" width="277"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf1.2.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S3.F4.sf1.3.2" style="font-size:80%;">Control Generation Capability &amp; Alignment Learning</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S3.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="178" id="S3.F4.sf2.g1" src="x5.png" width="236"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf2.2.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S3.F4.sf2.3.2" style="font-size:80%;">Novelty (x-axis) and Quality (y-axis) Comparison</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S3.F4.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="179" id="S3.F4.sf3.g1" src="x6.png" width="243"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf3.2.1.1" style="font-size:80%;">(c)</span> </span><span class="ltx_text" id="S3.F4.sf3.3.2" style="font-size:80%;">Improvement of UCI@N</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>(a) Model Finetuning Process. (b) and (c) Comparison between different recommenders in live experiments.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Fine-Tuning for User Behavior Alignment</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">LLMs trained on massive publicly available data on the Internet contains rich global knowledge, it however lacks capabilities to perform: 1) controlled generation (i.e., generate in the interest cluster space) and 2) domain-specific user behavior alignment. We propose to inject these domain specific knowledge through supervised fine-tuning using a dataset curated from real user watch histories on the commercial platform. The quality of data used for fine-tuning is thus crucial for its success.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.8"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.8.1">Diversified Data Curation.</span> Take K=2 as an example. Each fine-tuning data sample, denoted as <math alttext="[(C_{1},C_{2}),C_{L}]" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.2"><semantics id="S3.SS3.p2.1.m1.2a"><mrow id="S3.SS3.p2.1.m1.2.2.2" xref="S3.SS3.p2.1.m1.2.2.3.cmml"><mo id="S3.SS3.p2.1.m1.2.2.2.3" stretchy="false" xref="S3.SS3.p2.1.m1.2.2.3.cmml">[</mo><mrow id="S3.SS3.p2.1.m1.1.1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.1.1.3.cmml"><mo id="S3.SS3.p2.1.m1.1.1.1.1.2.3" stretchy="false" xref="S3.SS3.p2.1.m1.1.1.1.1.3.cmml">(</mo><msub id="S3.SS3.p2.1.m1.1.1.1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.1.1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.2.cmml">C</mi><mn id="S3.SS3.p2.1.m1.1.1.1.1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.p2.1.m1.1.1.1.1.2.4" xref="S3.SS3.p2.1.m1.1.1.1.1.3.cmml">,</mo><msub id="S3.SS3.p2.1.m1.1.1.1.1.2.2" xref="S3.SS3.p2.1.m1.1.1.1.1.2.2.cmml"><mi id="S3.SS3.p2.1.m1.1.1.1.1.2.2.2" xref="S3.SS3.p2.1.m1.1.1.1.1.2.2.2.cmml">C</mi><mn id="S3.SS3.p2.1.m1.1.1.1.1.2.2.3" xref="S3.SS3.p2.1.m1.1.1.1.1.2.2.3.cmml">2</mn></msub><mo id="S3.SS3.p2.1.m1.1.1.1.1.2.5" stretchy="false" xref="S3.SS3.p2.1.m1.1.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS3.p2.1.m1.2.2.2.4" xref="S3.SS3.p2.1.m1.2.2.3.cmml">,</mo><msub id="S3.SS3.p2.1.m1.2.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.2.cmml"><mi id="S3.SS3.p2.1.m1.2.2.2.2.2" xref="S3.SS3.p2.1.m1.2.2.2.2.2.cmml">C</mi><mi id="S3.SS3.p2.1.m1.2.2.2.2.3" xref="S3.SS3.p2.1.m1.2.2.2.2.3.cmml">L</mi></msub><mo id="S3.SS3.p2.1.m1.2.2.2.5" stretchy="false" xref="S3.SS3.p2.1.m1.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.2b"><interval closure="closed" id="S3.SS3.p2.1.m1.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.2"><interval closure="open" id="S3.SS3.p2.1.m1.1.1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.2"><apply id="S3.SS3.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.2">𝐶</ci><cn id="S3.SS3.p2.1.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS3.p2.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.1.2.2.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.1.1.2.2.2.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.2.2.2">𝐶</ci><cn id="S3.SS3.p2.1.m1.1.1.1.1.2.2.3.cmml" type="integer" xref="S3.SS3.p2.1.m1.1.1.1.1.2.2.3">2</cn></apply></interval><apply id="S3.SS3.p2.1.m1.2.2.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.2.2.2.2.1.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p2.1.m1.2.2.2.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.2">𝐶</ci><ci id="S3.SS3.p2.1.m1.2.2.2.2.3.cmml" xref="S3.SS3.p2.1.m1.2.2.2.2.3">𝐿</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.2c">[(C_{1},C_{2}),C_{L}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.2d">[ ( italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , italic_C start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ]</annotation></semantics></math>, consists of a cluster pair <math alttext="(C_{1},C_{2})" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.2"><semantics id="S3.SS3.p2.2.m2.2a"><mrow id="S3.SS3.p2.2.m2.2.2.2" xref="S3.SS3.p2.2.m2.2.2.3.cmml"><mo id="S3.SS3.p2.2.m2.2.2.2.3" stretchy="false" xref="S3.SS3.p2.2.m2.2.2.3.cmml">(</mo><msub id="S3.SS3.p2.2.m2.1.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.1.1.2" xref="S3.SS3.p2.2.m2.1.1.1.1.2.cmml">C</mi><mn id="S3.SS3.p2.2.m2.1.1.1.1.3" xref="S3.SS3.p2.2.m2.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.p2.2.m2.2.2.2.4" xref="S3.SS3.p2.2.m2.2.2.3.cmml">,</mo><msub id="S3.SS3.p2.2.m2.2.2.2.2" xref="S3.SS3.p2.2.m2.2.2.2.2.cmml"><mi id="S3.SS3.p2.2.m2.2.2.2.2.2" xref="S3.SS3.p2.2.m2.2.2.2.2.2.cmml">C</mi><mn id="S3.SS3.p2.2.m2.2.2.2.2.3" xref="S3.SS3.p2.2.m2.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS3.p2.2.m2.2.2.2.5" stretchy="false" xref="S3.SS3.p2.2.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.2b"><interval closure="open" id="S3.SS3.p2.2.m2.2.2.3.cmml" xref="S3.SS3.p2.2.m2.2.2.2"><apply id="S3.SS3.p2.2.m2.1.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.2">𝐶</ci><cn id="S3.SS3.p2.2.m2.1.1.1.1.3.cmml" type="integer" xref="S3.SS3.p2.2.m2.1.1.1.1.3">1</cn></apply><apply id="S3.SS3.p2.2.m2.2.2.2.2.cmml" xref="S3.SS3.p2.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.2.2.2.2.1.cmml" xref="S3.SS3.p2.2.m2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p2.2.m2.2.2.2.2.2.cmml" xref="S3.SS3.p2.2.m2.2.2.2.2.2">𝐶</ci><cn id="S3.SS3.p2.2.m2.2.2.2.2.3.cmml" type="integer" xref="S3.SS3.p2.2.m2.2.2.2.2.3">2</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.2c">(C_{1},C_{2})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.2d">( italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )</annotation></semantics></math> to form the prompt and the subsequent novel cluster <math alttext="C_{L}" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><msub id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">C</mi><mi id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">𝐶</ci><ci id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">C_{L}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.1d">italic_C start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT</annotation></semantics></math> as the finetune label. By definition of novelty, <math alttext="C_{L}" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.1"><semantics id="S3.SS3.p2.4.m4.1a"><msub id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mi id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml">C</mi><mi id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2">𝐶</ci><ci id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">C_{L}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.1d">italic_C start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT</annotation></semantics></math> must be different from <math alttext="C_{1}" class="ltx_Math" display="inline" id="S3.SS3.p2.5.m5.1"><semantics id="S3.SS3.p2.5.m5.1a"><msub id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml"><mi id="S3.SS3.p2.5.m5.1.1.2" xref="S3.SS3.p2.5.m5.1.1.2.cmml">C</mi><mn id="S3.SS3.p2.5.m5.1.1.3" xref="S3.SS3.p2.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><apply id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p2.5.m5.1.1.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2">𝐶</ci><cn id="S3.SS3.p2.5.m5.1.1.3.cmml" type="integer" xref="S3.SS3.p2.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">C_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.5.m5.1d">italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="C_{2}" class="ltx_Math" display="inline" id="S3.SS3.p2.6.m6.1"><semantics id="S3.SS3.p2.6.m6.1a"><msub id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><mi id="S3.SS3.p2.6.m6.1.1.2" xref="S3.SS3.p2.6.m6.1.1.2.cmml">C</mi><mn id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2">𝐶</ci><cn id="S3.SS3.p2.6.m6.1.1.3.cmml" type="integer" xref="S3.SS3.p2.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">C_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.6.m6.1d">italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>. Initially, we gathered approximately 250K <math alttext="[(C_{1},C_{2}),C_{L}]" class="ltx_Math" display="inline" id="S3.SS3.p2.7.m7.2"><semantics id="S3.SS3.p2.7.m7.2a"><mrow id="S3.SS3.p2.7.m7.2.2.2" xref="S3.SS3.p2.7.m7.2.2.3.cmml"><mo id="S3.SS3.p2.7.m7.2.2.2.3" stretchy="false" xref="S3.SS3.p2.7.m7.2.2.3.cmml">[</mo><mrow id="S3.SS3.p2.7.m7.1.1.1.1.2" xref="S3.SS3.p2.7.m7.1.1.1.1.3.cmml"><mo id="S3.SS3.p2.7.m7.1.1.1.1.2.3" stretchy="false" xref="S3.SS3.p2.7.m7.1.1.1.1.3.cmml">(</mo><msub id="S3.SS3.p2.7.m7.1.1.1.1.1.1" xref="S3.SS3.p2.7.m7.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p2.7.m7.1.1.1.1.1.1.2" xref="S3.SS3.p2.7.m7.1.1.1.1.1.1.2.cmml">C</mi><mn id="S3.SS3.p2.7.m7.1.1.1.1.1.1.3" xref="S3.SS3.p2.7.m7.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.p2.7.m7.1.1.1.1.2.4" xref="S3.SS3.p2.7.m7.1.1.1.1.3.cmml">,</mo><msub id="S3.SS3.p2.7.m7.1.1.1.1.2.2" xref="S3.SS3.p2.7.m7.1.1.1.1.2.2.cmml"><mi id="S3.SS3.p2.7.m7.1.1.1.1.2.2.2" xref="S3.SS3.p2.7.m7.1.1.1.1.2.2.2.cmml">C</mi><mn id="S3.SS3.p2.7.m7.1.1.1.1.2.2.3" xref="S3.SS3.p2.7.m7.1.1.1.1.2.2.3.cmml">2</mn></msub><mo id="S3.SS3.p2.7.m7.1.1.1.1.2.5" stretchy="false" xref="S3.SS3.p2.7.m7.1.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS3.p2.7.m7.2.2.2.4" xref="S3.SS3.p2.7.m7.2.2.3.cmml">,</mo><msub id="S3.SS3.p2.7.m7.2.2.2.2" xref="S3.SS3.p2.7.m7.2.2.2.2.cmml"><mi id="S3.SS3.p2.7.m7.2.2.2.2.2" xref="S3.SS3.p2.7.m7.2.2.2.2.2.cmml">C</mi><mi id="S3.SS3.p2.7.m7.2.2.2.2.3" xref="S3.SS3.p2.7.m7.2.2.2.2.3.cmml">L</mi></msub><mo id="S3.SS3.p2.7.m7.2.2.2.5" stretchy="false" xref="S3.SS3.p2.7.m7.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.2b"><interval closure="closed" id="S3.SS3.p2.7.m7.2.2.3.cmml" xref="S3.SS3.p2.7.m7.2.2.2"><interval closure="open" id="S3.SS3.p2.7.m7.1.1.1.1.3.cmml" xref="S3.SS3.p2.7.m7.1.1.1.1.2"><apply id="S3.SS3.p2.7.m7.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.7.m7.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.7.m7.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.7.m7.1.1.1.1.1.1.2">𝐶</ci><cn id="S3.SS3.p2.7.m7.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS3.p2.7.m7.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS3.p2.7.m7.1.1.1.1.2.2.cmml" xref="S3.SS3.p2.7.m7.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.7.m7.1.1.1.1.2.2.1.cmml" xref="S3.SS3.p2.7.m7.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS3.p2.7.m7.1.1.1.1.2.2.2.cmml" xref="S3.SS3.p2.7.m7.1.1.1.1.2.2.2">𝐶</ci><cn id="S3.SS3.p2.7.m7.1.1.1.1.2.2.3.cmml" type="integer" xref="S3.SS3.p2.7.m7.1.1.1.1.2.2.3">2</cn></apply></interval><apply id="S3.SS3.p2.7.m7.2.2.2.2.cmml" xref="S3.SS3.p2.7.m7.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.7.m7.2.2.2.2.1.cmml" xref="S3.SS3.p2.7.m7.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p2.7.m7.2.2.2.2.2.cmml" xref="S3.SS3.p2.7.m7.2.2.2.2.2">𝐶</ci><ci id="S3.SS3.p2.7.m7.2.2.2.2.3.cmml" xref="S3.SS3.p2.7.m7.2.2.2.2.3">𝐿</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.2c">[(C_{1},C_{2}),C_{L}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.7.m7.2d">[ ( italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , italic_C start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ]</annotation></semantics></math> data samples from our log, focusing solely on high quality interactions<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>In other words, these samples are novel interest transitions existed in the log where a user was successfully introduced a novel interest.</span></span></span>. These samples are then grouped by their labels, and the top-10 most frequently occurring cluster pairs are selected for each label, forming the final data samples which is diverse and cover all the labels. Following these steps, we obtain <math alttext="761*10=7,610" class="ltx_Math" display="inline" id="S3.SS3.p2.8.m8.2"><semantics id="S3.SS3.p2.8.m8.2a"><mrow id="S3.SS3.p2.8.m8.2.3" xref="S3.SS3.p2.8.m8.2.3.cmml"><mrow id="S3.SS3.p2.8.m8.2.3.2" xref="S3.SS3.p2.8.m8.2.3.2.cmml"><mn id="S3.SS3.p2.8.m8.2.3.2.2" xref="S3.SS3.p2.8.m8.2.3.2.2.cmml">761</mn><mo id="S3.SS3.p2.8.m8.2.3.2.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.8.m8.2.3.2.1.cmml">∗</mo><mn id="S3.SS3.p2.8.m8.2.3.2.3" xref="S3.SS3.p2.8.m8.2.3.2.3.cmml">10</mn></mrow><mo id="S3.SS3.p2.8.m8.2.3.1" xref="S3.SS3.p2.8.m8.2.3.1.cmml">=</mo><mrow id="S3.SS3.p2.8.m8.2.3.3.2" xref="S3.SS3.p2.8.m8.2.3.3.1.cmml"><mn id="S3.SS3.p2.8.m8.1.1" xref="S3.SS3.p2.8.m8.1.1.cmml">7</mn><mo id="S3.SS3.p2.8.m8.2.3.3.2.1" xref="S3.SS3.p2.8.m8.2.3.3.1.cmml">,</mo><mn id="S3.SS3.p2.8.m8.2.2" xref="S3.SS3.p2.8.m8.2.2.cmml">610</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m8.2b"><apply id="S3.SS3.p2.8.m8.2.3.cmml" xref="S3.SS3.p2.8.m8.2.3"><eq id="S3.SS3.p2.8.m8.2.3.1.cmml" xref="S3.SS3.p2.8.m8.2.3.1"></eq><apply id="S3.SS3.p2.8.m8.2.3.2.cmml" xref="S3.SS3.p2.8.m8.2.3.2"><times id="S3.SS3.p2.8.m8.2.3.2.1.cmml" xref="S3.SS3.p2.8.m8.2.3.2.1"></times><cn id="S3.SS3.p2.8.m8.2.3.2.2.cmml" type="integer" xref="S3.SS3.p2.8.m8.2.3.2.2">761</cn><cn id="S3.SS3.p2.8.m8.2.3.2.3.cmml" type="integer" xref="S3.SS3.p2.8.m8.2.3.2.3">10</cn></apply><list id="S3.SS3.p2.8.m8.2.3.3.1.cmml" xref="S3.SS3.p2.8.m8.2.3.3.2"><cn id="S3.SS3.p2.8.m8.1.1.cmml" type="integer" xref="S3.SS3.p2.8.m8.1.1">7</cn><cn id="S3.SS3.p2.8.m8.2.2.cmml" type="integer" xref="S3.SS3.p2.8.m8.2.2">610</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m8.2c">761*10=7,610</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.8.m8.2d">761 ∗ 10 = 7 , 610</annotation></semantics></math> data samples (10 per label cluster) and proceed to perform supervised fine-tuning for the LLM using these samples.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S3.F3" title="Figure 3 ‣ 3.2. Hybrid Hierarchical Planning ‣ 3. Method ‣ LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_tag">3</span></a>, we plot the distributions of interest clusters generated by fine-tuned LLMs on the 579,121 context cluster pairs.
When using finetuning data of low diversity, where we randomly select 7,610 transitions and their corresponding subsequent clusters from the initial <math alttext="250K" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mn id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">250</mn><mo id="S3.SS3.p3.1.m1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><times id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1"></times><cn id="S3.SS3.p3.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.p3.1.m1.1.1.2">250</cn><ci id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">250K</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">250 italic_K</annotation></semantics></math> to form the data, the fine-tuned LLM generates interests that are highly-skewed, where a few generated clusters have very high frequencies (depicted as Region B). When we increase the diversity of our fine-tuning data, these dominant labels disappear, and the number of generated clusters with very low frequency (depicted in Region A) also decreases. Ensuring the fine-tuning data covers all clusters evenly allows us to address the long-tailed distribution issue in the model’s generated clusters. This treatment not only mitigates the feedback loop effect in behavior data but also enhances overall user satisfaction as shown in Section <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S4" title="4. Live Experiments ‣ LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.1">Control Generation Capability &amp; User Behavior Alignment.</span>
The number of fine-tuning steps determines the balance between the LLM’s global and task-specific knowledge. Our fine-tuning process has two main objectives: (1) <span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.2">controlling</span> LLM generation to speak the language of interest clusters. We evaluate the <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.3">match rate</span> of the generation from the fine-tuned LLM to determine if the output matches exactly with one of the cluster descriptions; and (2) <span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.4">aligning</span> with real-world user transitions, measured by comparing the fine-tuned LLM’s output with the successful user interest transition in both fine-tuning and test set to compute <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.5">recall</span>. A higher recall indicates LLM learning the domain specific novel transitions from the fine-tuned data, and aligning with user behaviors.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S3.F4" title="Figure 4 ‣ 3.2. Hybrid Hierarchical Planning ‣ 3. Method ‣ LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_tag">4</span></a> (a), with batch size of 16, we illustrate the changes in match rate and recall as the fine-tuning steps progress. We note that formatting learning, i.e., learning the language of intrest clusters, kicks in first, peaking at around 2,000 steps. With a high match rate (over 99%), we can efficiently map the generation to cluster ID space and restrict the original item-level softmax policy on these clusters. Subsequently, the model begins to align with user behaviors, resulting in a significant increase in recall (on the fine-tuned set). Moreover, we find that the recall for a separate test set increases following transition alignment, reaching its peak at around 3,000 steps before gradually decreasing. Therefore, we select models fine-tuned with 3,000 steps. Note that the recall on the test set is much lower than that of the fine-tuning set, indicating LLM is still relying heavily on its global knowledge instead of memorizing interest transitions in the log while generating novel interests.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Live Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Experimental Setup</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We conduct a series of live experiments on a <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">commercial short-form video recommendation platform that serves billions of users</span>. Our experiments are conducted with <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.2">Gemini-Pro</span> <cite class="ltx_cite ltx_citemacro_citep">(Gemini Team Google, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib13" title="">2023</a>)</cite>, but the same fine-tuning process and pipeline can be readily adapted to other LLMs. We set the number of historical clusters for LLM inference <math alttext="K=2" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">K</mi><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><eq id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></eq><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">𝐾</ci><cn id="S4.SS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">K=2</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_K = 2</annotation></semantics></math>, it however can easily scale to accommodate larger numbers in future iterations with a sparse table.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Baseline.</span>
We compare the proposed method to existing production models: (1) <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.2">Exploration-oriented</span> models include: a <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.3">Novelty-enhanced sequence recommender</span>  <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib10" title="">2021</a>)</cite> trained with labels from both positive and novel items whose clusters have not appeared in user’s consumption history before; <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.4">Hierarchical contextual bandit</span> <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib29" title="">2022</a>)</cite> based on the hierarchical clusters introduced in <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S3.SS1" title="3.1. Preliminaries ‣ 3. Method ‣ LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_tag">3.1</span></a> to explore user’s interests through a tree-based LinUCB to obtain the next clusters, from which the sequential model is then used to restrict the retrieval to items.
Although these models are tailored to explore user interests, they are trained on interest transitions existing in the system and therefore are still subject to the feedback loop. (2) <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.5">Exploitation-oriented</span> models include a regular <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.6">two-tower</span> model <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib35" title="">2020</a>)</cite> and <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.7">transformer-based</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib9" title="">2019</a>; Shaw et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib28" title="">2018</a>)</cite> sequential model trained on all positive user feedback. Our live experimental results demonstrate our proposed method can lead to recommendation which are more novel and in better quality compared to these existing models.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Results and Analysis</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">Novelty and Quality.</span> In Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S3.F4" title="Figure 4 ‣ 3.2. Hybrid Hierarchical Planning ‣ 3. Method ‣ LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_tag">4</span></a> (b), we compare the proposed method with various baseline models currently in production. Using the performance of Hierarchical contextual bandit <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#bib.bib29" title="">2022</a>)</cite> as the reference, we measure the improvement of the other models. Specifically, we plot the increase in ratio of novel impressions (considering only impressions from interest clusters the user has never interacted with) to highlight recommendation novelty (x-axis), and the increase in positive feedback rate to demonstrate recommendation quality (y-axis). The proposed method recommends more novel items compared to all the baseline methods (to the right on x-axis). Additionally, it achieves much better quality than existing exploration-oriented methods, comparable to exploitation-oriented methods (high on x-axis). In other words, the proposed method presents an effective approach to introduce users to novel interests that are of interests to the user.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">User Interest Exploration.</span> To measure if the recommenders encourage users to explore new interests, we use a metric called <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.2">UCI@N</span>, which tracks the number of users who have consumed items from N <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S4.SS2.p2.1.3">u</span>nique <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S4.SS2.p2.1.4">c</span>lustered <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S4.SS2.p2.1.5">i</span>nterests within the past 7 days. Higher UCI@N indicates more users are consuming N interests. By monitoring UCI@N for different values of N (20 to 200), we can gauge the effectiveness of our system for user interest exploration. Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S3.F4" title="Figure 4 ‣ 3.2. Hybrid Hierarchical Planning ‣ 3. Method ‣ LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_tag">4</span></a>(c) summarizes the improvement of our method compared to Hierarchical Contextual Bandit, to evaluate its effectiveness in user interest exploration. Notably, our proposed method shows very significant improvement compared to the prominent exploratory model currently deployed in production for different values of <math alttext="N" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">italic_N</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">User Growth.</span> At the same time, we monitor increase in overall watch time and number of active users who had total watch time <math alttext="&gt;=" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mo id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">&gt;=</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><geq id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">&gt;=</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">&gt; =</annotation></semantics></math> 10 minute (in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.16363v2#S4.F5" title="Figure 5 ‣ 4.2. Results and Analysis ‣ 4. Live Experiments ‣ LLMs for User Interest Exploration in Large-scale Recommendation Systems"><span class="ltx_text ltx_ref_tag">5</span></a>), to measure user growth on the short-form video platform. The x-axis represents the experiment periods (the exact dates are redacted), and the y-axis shows the relative percentage difference between the experiment and control, which excludes the proposed system. Our method successfully broadens user interest by recommending diverse and novel content, with user growth. This underscores the quality and relevance of the recommended novel content.</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="163" id="S4.F5.sf1.g1" src="extracted/5652296/wt.png" width="258"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.sf1.2.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S4.F5.sf1.3.2" style="font-size:80%;">Overall Watch Time</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="161" id="S4.F5.sf2.g1" src="extracted/5652296/seu10.png" width="246"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.sf2.2.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S4.F5.sf2.3.2" style="font-size:80%;">Number of Users watch ¿= 10min</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>The proposed method drives user growth.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We present a hybrid approach to leverage LLMs for user interest exploration. It combines the strength of LLMs in reasoning and generalization, and the grounding of classic recommendation models.
We showcase a successful recipe to inject domain-specific recommendation knowledge to LLMs for controlled generation and user behavior alignment.
Extensive testing on a commercial platform with billions of users has yielded significant improvements in both exploration of novel interests and user growth.
The future work will focus on
taking long-term effects into account to further improve hierarchical planning with LLMs for recommendation systems.
</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil et al<span class="ltx_text" id="bib.bib2.3.3.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al<span class="ltx_text" id="bib.bib2.4.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Palm 2 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.5.1">arXiv preprint arXiv:2305.10403</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023.

</span>
<span class="ltx_bibblock">Tallrec: An effective and efficient tuning framework to align large language model with recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">arXiv preprint arXiv:2305.00447</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blog (2023)</span>
<span class="ltx_bibblock">
Youtube Official Blog. 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">YouTube by the Number</em>.

</span>
<span class="ltx_bibblock">
Retrieved January, 2023 from <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.youtube/press/" title="">https://blog.youtube/press/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span class="ltx_text" id="bib.bib5.3.3.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al<span class="ltx_text" id="bib.bib5.4.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.5.1">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chaney et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Allison JB Chaney, Brandon M Stewart, and Barbara E Engelhardt. 2018.

</span>
<span class="ltx_bibblock">How algorithmic confounding in recommendation systems increases homogeneity and decreases utility. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">RecSys</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al<span class="ltx_text" id="bib.bib7.3.3.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Bo Chang, Changping Meng, He Ma, Shuo Chang, Yang Gu, Yajun Peng, Jingchen Feng, Yaping Zhang, Shuchao Bi, Ed H Chi, et al<span class="ltx_text" id="bib.bib7.4.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Cluster Anchor Regularization to Alleviate Popularity Bias in Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.5.1">Companion Proceedings of the ACM on Web Conference 2024</em>. 151–160.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen (2021)</span>
<span class="ltx_bibblock">
Minmin Chen. 2021.

</span>
<span class="ltx_bibblock">Exploration in recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">RecSys</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed H Chi. 2019.

</span>
<span class="ltx_bibblock">Top-k off-policy correction for a REINFORCE recommender system. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">WSDM</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Minmin Chen, Yuyan Wang, Can Xu, Ya Le, Mohit Sharma, Lee Richardson, Su-Lin Wu, and Ed Chi. 2021.

</span>
<span class="ltx_bibblock">Values of user exploration in recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">RecSys</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christakopoulou et al<span class="ltx_text" id="bib.bib11.3.3.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Konstantina Christakopoulou, Alberto Lalama, Cj Adams, Iris Qu, Yifat Amir, Samer Chucri, Pierce Vollucci, Fabio Soldo, Dina Bseiso, Sarah Scodel, et al<span class="ltx_text" id="bib.bib11.4.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Large language models for user interest journeys.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.5.1">arXiv preprint arXiv:2305.15498</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023.

</span>
<span class="ltx_bibblock">Uncovering ChatGPT’s Capabilities in Recommender Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">arXiv preprint arXiv:2305.02182</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini Team Google (2023)</span>
<span class="ltx_bibblock">
Gemini Team Google. 2023.

</span>
<span class="ltx_bibblock">Gemini: A family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2312.11805</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.

</span>
<span class="ltx_bibblock">Recommendation as language processing (rlp): A unified pretrain, personalized prompt &amp; predict paradigm (p5). In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">ResSys</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, and Yongfeng Zhang. 2023.

</span>
<span class="ltx_bibblock">VIP5: Towards Multimodal Foundation Models for Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">arXiv preprint arXiv:2305.14302</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hale (2019)</span>
<span class="ltx_bibblock">
James Hale. 2019.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">More Than 500 Hours Of Content Are Now Being Uploaded To YouTube Every Minute</em>.

</span>
<span class="ltx_bibblock">
Retrieved January, 2023 from <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.tubefilter.com/2019/05/07/number-hours-video-uploaded-to-youtube-per-minute/" title="">https://www.tubefilter.com/2019/05/07/number-hours-video-uploaded-to-youtube-per-minute/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yupeng Hou, Zhankui He, Julian McAuley, and Wayne Xin Zhao. 2023a.

</span>
<span class="ltx_bibblock">Learning vector-quantized item representation for transferable sequential recommenders. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">TheWebConf</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023b.

</span>
<span class="ltx_bibblock">Large language models are zero-shot rankers for recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">ECIR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ingham (2023)</span>
<span class="ltx_bibblock">
Tim Ingham. 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Over 60,000 Tracks are Now Uploaded to Spotify Every Day. That’s Nearly One per Second.</em>
</span>
<span class="ltx_bibblock">
Retrieved January, 2023 from <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.musicbusinessworldwide.com/over-60000-tracks-are-now-uploaded-to-spotify-daily-thats-nearly-one-per-second/" title="">https://www.musicbusinessworldwide.com/over-60000-tracks-are-now-uploaded-to-spotify-daily-thats-nearly-one-per-second/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Chen Li, Yixiao Ge, Jiayong Mao, Dian Li, and Ying Shan. 2023a.

</span>
<span class="ltx_bibblock">TagGPT: Large Language Models are Zero-shot Multimodal Taggers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">arXiv preprint arXiv:2304.03022</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian McAuley. 2023b.

</span>
<span class="ltx_bibblock">Text Is All You Need: Learning Language Representations for Sequential Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">KDD</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Jinming Li, Wentao Zhang, Tian Wang, Guanglei Xiong, Alan Lu, and Gerard Medioni. 2023c.

</span>
<span class="ltx_bibblock">GPT4Rec: A generative framework for personalized recommendation and user interests interpretation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">arXiv preprint arXiv:2304.03879</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024.

</span>
<span class="ltx_bibblock">Rella: Retrieval-enhanced large language models for lifelong sequential behavior comprehension in recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">TheWebConf</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023b.

</span>
<span class="ltx_bibblock">Is chatgpt a good recommender? a preliminary study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">arXiv preprint arXiv:2304.10149</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2023a.

</span>
<span class="ltx_bibblock">A First Look at LLM-Powered Generative News Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">arXiv preprint arXiv:2305.06566</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahajan et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Khushhall Chandra Mahajan, Amey Porobo Dharwadker, Romil Shah, Simeng Qu, Gaurav Bang, and Brad Schumitsch. 2023.

</span>
<span class="ltx_bibblock">PIE: Personalized Interest Exploration for Large-Scale Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Companion Proceedings of the ACM Web Conference 2023</em>. 508–512.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mansoury et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad Mobasher, and Robin Burke. 2020.

</span>
<span class="ltx_bibblock">Feedback loop and bias amplification in recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">CIKM</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shaw et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.

</span>
<span class="ltx_bibblock">Self-attention with relative position representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">arXiv preprint arXiv:1803.02155</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yu Song, Shuai Sun, Jianxun Lian, Hong Huang, Yu Li, Hai Jin, and Xing Xie. 2022.

</span>
<span class="ltx_bibblock">Show me the whole world: Towards entire item space exploration for interactive personalized recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">WSDM</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al<span class="ltx_text" id="bib.bib30.3.3.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yi Su, Xiangyu Wang, Elaine Ya Le, Liang Liu, Yuening Li, Haokai Lu, Benjamin Lipshitz, Sriraj Badam, Lukasz Heldt, Shuchao Bi, et al<span class="ltx_text" id="bib.bib30.4.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Long-Term Value of Exploration: Measurements, Findings and Algorithms. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.5.1">WSDM</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span class="ltx_text" id="bib.bib31.3.3.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al<span class="ltx_text" id="bib.bib31.4.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.5.1">arXiv preprint arXiv:2307.09288</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jianling Wang, Haokai Lu, James Caverlee, Ed Chi, and Minmin Chen. 2024.

</span>
<span class="ltx_bibblock">Large Language Models as Data Augmenters for Cold-Start Item Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">arXiv preprint arXiv:2402.11724</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Haolun Wu, Yansen Zhang, Chen Ma, Fuyuan Lyu, Bowei He, Bhaskar Mitra, and Xue Liu. 2024.

</span>
<span class="ltx_bibblock">Result Diversification in Search and Recommendation: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">TKDE</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xi et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2023.

</span>
<span class="ltx_bibblock">Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">arXiv preprint arXiv:2306.10933</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ji Yang, Xinyang Yi, Derek Zhiyuan Cheng, Lichan Hong, Yang Li, Simon Xiaoming Wang, Taibai Xu, and Ed H Chi. 2020.

</span>
<span class="ltx_bibblock">Mixed negative sampling for learning two-tower neural networks in recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Companion Proceedings of the Web Conference 2020</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yang Yu, Fangzhao Wu, Chuhan Wu, Jingwei Yi, and Qi Liu. 2022.

</span>
<span class="ltx_bibblock">Tiny-newsrec: Effective and efficient plm-based news recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">EMNLP</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Jun  7 18:05:02 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
