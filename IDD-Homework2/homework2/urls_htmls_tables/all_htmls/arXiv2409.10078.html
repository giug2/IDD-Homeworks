<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis</title>
<!--Generated on Thu Sep 19 16:56:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.10078v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S1" title="In IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S2" title="In IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S3" title="In IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S3.SS1" title="In III Methodology ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">System Overview</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S3.SS2" title="In III Methodology ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Cross-modal Attention Computation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S3.SS3" title="In III Methodology ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Object Localization and Classification</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S3.SS3.SSS1" title="In III-C Object Localization and Classification ‣ III Methodology ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>1 </span>Localization Score</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S3.SS3.SSS2" title="In III-C Object Localization and Classification ‣ III Methodology ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>2 </span>Bounding Box Prediction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S3.SS3.SSS3" title="In III-C Object Localization and Classification ‣ III Methodology ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>3 </span>Object Classification</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S3.SS4" title="In III Methodology ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Decision Module and Point Cloud Retrieval</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S3.SS5" title="In III Methodology ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-E</span> </span><span class="ltx_text ltx_font_italic">Language-Guided 3D Affordance Segmentation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S4" title="In IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Dataset</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S4.SS1" title="In IV Dataset ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Dataset Structure and Annotation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S4.SS2" title="In IV Dataset ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Affordance Segmentation for Embodied Agents</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S5" title="In IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S5.SS1" title="In V Experiments ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Experimental Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S5.SS2" title="In V Experiments ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Performance Metrics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S5.SS3" title="In V Experiments ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Quantitative Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S5.SS3.SSS1" title="In V-C Quantitative Results ‣ V Experiments ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span>1 </span>Comparative Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S5.SS3.SSS2" title="In V-C Quantitative Results ‣ V Experiments ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span>2 </span>Performance Across Different environments</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S5.SS4" title="In V Experiments ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span> </span><span class="ltx_text ltx_font_italic">Qualitative Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S5.SS5" title="In V Experiments ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-E</span> </span><span class="ltx_text ltx_font_italic">Discussion</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S6" title="In IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">IRIS<span class="ltx_text ltx_font_medium" id="id1.1">
<span class="ltx_text" id="id1.1.1" style="position:relative; bottom:-5.2pt;">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="3" id="id1.1.1.g1" src="extracted/5866516/images/eye.png" width="3"/>
</span>
</span>: Interactive Responsive Intelligent Segmentation
<br class="ltx_break"/>for 3D Affordance Analysis
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Meng Chu<sup class="ltx_sup" id="id5.4.id1"><span class="ltx_text ltx_font_italic" id="id5.4.id1.1">1</span></sup>, Xuan Zhang<sup class="ltx_sup" id="id6.5.id2"><span class="ltx_text ltx_font_italic" id="id6.5.id2.1">1</span></sup>
</span><span class="ltx_author_notes"><sup class="ltx_sup" id="id7.6.id1"><span class="ltx_text ltx_font_italic" id="id7.6.id1.1">1</span></sup>All authors are with School of Computing,
National University of Singapore.
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id8.id1">Recent advancements in large language and vision-language models have significantly enhanced multimodal understanding, yet translating high-level linguistic instructions into precise robotic actions in 3D space remains challenging. This paper introduces IRIS (Interactive Responsive Intelligent Segmentation), a novel training-free multimodal system for 3D affordance segmentation, alongside a benchmark for evaluating interactive language-guided affordance in everyday environments. IRIS integrates a large multimodal model with a specialized 3D vision network, enabling seamless fusion of 2D and 3D visual understanding with language comprehension. To facilitate evaluation, we present a dataset of 10 typical indoor environments, each with 50 images annotated for object actions and 3D affordance segmentation. Extensive experiments demonstrate IRIS’s capability in handling interactive 3D affordance segmentation tasks across diverse settings, showcasing competitive performance across various metrics. Our results highlight IRIS’s potential for enhancing human-robot interaction based on affordance understanding in complex indoor environments, advancing the development of more intuitive and efficient robotic systems for real-world applications.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In the rapidly evolving field of robotics and computer vision, the ability to understand and interact with complex 3D environments remains a frontier ripe for exploration. Recent years have witnessed unprecedented advancements in artificial intelligence, particularly with the emergence of large language models (LLMs) and vision-language models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib3" title="">3</a>]</cite>. These breakthroughs have revolutionized numerous aspects of AI, from natural language processing to image recognition. However, a significant challenge persists: bridging the gap between high-level linguistic instructions and precise 3D robotic actions in real-world scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The integration of language understanding with spatial reasoning and manipulation skills is crucial for the next generation of intelligent systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib8" title="">8</a>]</cite>. While LLMs excel at processing and generating human-like text, and vision models can interpret complex visual scenes, translating this understanding into actionable 3D interactions remains an open problem. This challenge is particularly evident in embodied AI applications, where agents must navigate, manipulate, and interact with their physical surroundings based on natural language instructions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="170" id="S1.F1.g1" src="extracted/5866516/images/head.jpg" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S1.F1.2.1">Comparison of 2D Affordance Segmentation and interactive 3D Affordance Segmentation.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Traditional approaches in robotics and computer vision have often addressed 2D and 3D domains separately, lacking the holistic perspective necessary for effective embodied interaction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib12" title="">12</a>]</cite>. Two-dimensional visual understanding, while advanced, falls short in capturing the full complexity of real-world environments. Conversely, pure 3D approaches often struggle with semantic interpretation and language grounding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib14" title="">14</a>]</cite>. This dichotomy has limited the development of truly versatile and intuitive robotic systems capable of understanding and acting upon nuanced human instructions in diverse settings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib16" title="">16</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Recent research has begun to explore the potential of LLMs in embodied navigation and planning tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib18" title="">18</a>]</cite>. These studies have shown promising results in high-level decision-making and route planning. However, they frequently encounter limitations when it comes to fine-grained manipulation tasks that demand precise spatial understanding and object interaction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib19" title="">19</a>]</cite>. The ability to grasp the affordances of objects—their potential uses and interactions—in a 3D context while aligning with natural language instructions remains a significant hurdle <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib20" title="">20</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="141" id="S1.F2.g1" src="extracted/5866516/images/enironment.jpg" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S1.F2.2.1">Demonstration of IRIS Interactive Query-Answer Function and Possible Affordance Segmentation in a Typical Kitchen Environment.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address these challenges, we propose IRIS (Interactive Responsive Intelligent Segmentation), a novel training-free multimodal framework. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S1.F1" title="Figure 1 ‣ I Introduction ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_tag">1</span></a>, IRIS is designed to bridge the gap between linguistic comprehension and 3D spatial understanding, enabling more intuitive and effective human-robot interaction. The core motivation behind IRIS stems from the need to equip embodied agents with the capability to seamlessly integrate 2D and 3D visual understanding with language comprehension <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib21" title="">21</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our framework leverages the strengths of large multimodal models, combining them with specialized networks to process and reason about visual and linguistic inputs in tandem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib23" title="">23</a>]</cite>. This integration allows IRIS to perform sophisticated vision-language reasoning, translating high-level instructions into precise 3D affordance segmentation without additional training. By doing so, IRIS opens new possibilities for robots to understand and interact with their environment in ways that more closely align with human intentions and expectations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib4" title="">4</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">IRIS tackles these limitations by uniquely combining two effective components: a large multimodal model for vision-language understanding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib24" title="">24</a>]</cite> and a specialized network for language-guided 3D affordance segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib25" title="">25</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">This integration enables IRIS to process multimodal inputs, perform vision-language understanding, localize objects, retrieve and register 3D point clouds, and execute language-guided 3D affordance segmentation—all without requiring additional training.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">The key contributions of this work are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present a training-free multimodal system linking high-level instructions and precise robotic actions in 3D environments. Our IRIS framework introduces a novel pipeline that integrates 2D and 3D visual understanding with language comprehension for embodied agents, demonstrating state-of-the-art performance in 3D affordance analysis and segmentation across diverse indoor environments.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We provide a new dataset specifically designed for evaluating interactive language-guided affordance segmentation in everyday environments. This dataset enables more comprehensive testing and development of multimodal systems for complex spatial understanding tasks.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Large Models for Visual Understanding.</span>
Large models have significantly trumped visual understanding tasks with the supervision of language <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib27" title="">27</a>]</cite>. In 2D visual grounding, GPT4ROI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib28" title="">28</a>]</cite> encodes region features interleaved with language embeddings for fine-grained multimodal reasoning. Shikra <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib29" title="">29</a>]</cite> further improves visual grounding in the unified natural language form. 3D environments bring out greater complexity but provide more precise details compared to 2D images. For 3D understanding, Chen et al. established ScanRefer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib30" title="">30</a>]</cite> to learn the correlated representation between 3D object proposals and encoded description embeddings. Building on this, ScanQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib31" title="">31</a>]</cite> is formulated for 3D question answering. However, these methods only focus on either 2D or 3D domains separately, lacking the holistic perspective for embodied agents. Our IRIS framework bridges this gap by seamlessly combining 2D and 3D visual understanding with language comprehension.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Embodied Agents for Robotic Tasks.</span>
Embodied agents in robotics aim to unify visual perception and physical action in real-world environments. To enable and encourage the application of situated multimodal learning, vision-and-language navigation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib32" title="">32</a>]</cite> is first presented for embodied learning. Furthermore, Hong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib33" title="">33</a>]</cite> equips the BERT model recurrent functions to capture the cross-model time-aware information for agents. As the planning capability of Large Language Models (LLMs) for has revolutionized the vision-language problem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib36" title="">36</a>]</cite>, some researchers attempt to apply LLMs as an auxiliary module for embodied navigation. Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib37" title="">37</a>]</cite> extends the powerful reasoning ability of LLMs grounding on embodied context and language feedback, while Singh et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib38" title="">38</a>]</cite> structures program-like prompts to enable the universal plan generation across diverse situated tasks. Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib39" title="">39</a>]</cite> builds an online language-formed map to extend the agent action space from local to global. These works highlight the potential of language models in embodied robot planning, yet they often struggle with fine-grained manipulation that requires precise spatial understanding. Our paper addresses this limitation by integrating LLMs with 3D point cloud processing for highly accurate object interaction and manipulation.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="303" id="S2.F3.g1" src="extracted/5866516/images/main.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold" id="S2.F3.2.1">Sturcture and Working Flow of IRIS System.</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Affordance Learning in Robotics.</span>
Affordance learning is crucial for robotic manipulation tasks. Traditional approaches like 3D AffordanceNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib40" title="">40</a>]</cite> focused on learning affordances by detecting objects in the end-to-end architecture. Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib41" title="">41</a>]</cite> extends this by proposing a setting for learning 3D affordance parts guided by image demonstrations, but discarding the semantic information. More recently, Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib25" title="">25</a>]</cite> introduces PointRefer, a novel task for language-guided affordance segmentation on 3D objects. While these works have made significant strides in affordance detection, they often lack the flexibility to integrate with diverse, context-rich instructions under LLM generation. Our approach differs by directly learning from linguistic context, aligning more closely with the semantic richness of LLMs and their potential downstream applications in robotics.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S1.F2" title="Figure 2 ‣ I Introduction ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_tag">2</span></a>, humans typically perceive and communicate about their environment in 2D, while robots need to perform tasks in 3D spaces. IRIS bridges this gap by interpreting 2D visual information from humans and translating it into 3D actions for robots. This capability is crucial as robots become more integrated into our daily lives, from homes to factories.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methodology</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We present IRIS, a novel training-free multimodal system for advanced object understanding and interaction. Our approach integrates a large multimodal model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib24" title="">24</a>]</cite> for vision-language understanding and a specialized network <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib25" title="">25</a>]</cite> for language-guided 3D affordance segmentation.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">System Overview</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Figure  <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S2.F3" title="Figure 3 ‣ II Related Work ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates a comprehensive system for language-guided robotic interactions, comprising several key stages. The process begins with multimodal input processing of visual and textual information, followed by vision-language understanding and object localization to interpret the input and identify relevant objects. Next, 3D point cloud retrieval and registration aligns 2D visual data with 3D spatial information. The fourth stage involves language-guided 3D affordance segmentation, determining how objects can be interacted with based on given instructions. Finally, 2D and 3D information are integrated for the final output, bridging the gap between high-level commands and precise robotic actions. This approach allows IRIS to understand complex instructions and translate them into actionable insights for robotic systems, enabling precise, language-guided interactions in 3D environments.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The vision-language understanding process in NExT-Chat involves two primary components: cross-modal attention computation and object localization and classification. Each of these components is represented by specific equations that warrant detailed explanation.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Cross-modal Attention Computation</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The cross-modal attention mechanism, a key element in transformer-based models, is represented by the following equation:</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="A=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V" class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.2" xref="S3.E1.m1.1.2.cmml"><mi id="S3.E1.m1.1.2.2" xref="S3.E1.m1.1.2.2.cmml">A</mi><mo id="S3.E1.m1.1.2.1" xref="S3.E1.m1.1.2.1.cmml">=</mo><mrow id="S3.E1.m1.1.2.3" xref="S3.E1.m1.1.2.3.cmml"><mtext id="S3.E1.m1.1.2.3.2" xref="S3.E1.m1.1.2.3.2a.cmml">softmax</mtext><mo id="S3.E1.m1.1.2.3.1" xref="S3.E1.m1.1.2.3.1.cmml">⁢</mo><mrow id="S3.E1.m1.1.2.3.3.2" xref="S3.E1.m1.1.1.cmml"><mo id="S3.E1.m1.1.2.3.3.2.1" xref="S3.E1.m1.1.1.cmml">(</mo><mfrac id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mrow id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2.cmml">Q</mi><mo id="S3.E1.m1.1.1.2.1" xref="S3.E1.m1.1.1.2.1.cmml">⁢</mo><msup id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.2.3.2" xref="S3.E1.m1.1.1.2.3.2.cmml">K</mi><mi id="S3.E1.m1.1.1.2.3.3" xref="S3.E1.m1.1.1.2.3.3.cmml">T</mi></msup></mrow><msqrt id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><msub id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml">d</mi><mi id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml">k</mi></msub></msqrt></mfrac><mo id="S3.E1.m1.1.2.3.3.2.2" xref="S3.E1.m1.1.1.cmml">)</mo></mrow><mo id="S3.E1.m1.1.2.3.1a" xref="S3.E1.m1.1.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.2.3.4" xref="S3.E1.m1.1.2.3.4.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.2.cmml" xref="S3.E1.m1.1.2"><eq id="S3.E1.m1.1.2.1.cmml" xref="S3.E1.m1.1.2.1"></eq><ci id="S3.E1.m1.1.2.2.cmml" xref="S3.E1.m1.1.2.2">𝐴</ci><apply id="S3.E1.m1.1.2.3.cmml" xref="S3.E1.m1.1.2.3"><times id="S3.E1.m1.1.2.3.1.cmml" xref="S3.E1.m1.1.2.3.1"></times><ci id="S3.E1.m1.1.2.3.2a.cmml" xref="S3.E1.m1.1.2.3.2"><mtext id="S3.E1.m1.1.2.3.2.cmml" xref="S3.E1.m1.1.2.3.2">softmax</mtext></ci><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.2.3.3.2"><divide id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.2.3.3.2"></divide><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><times id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2.1"></times><ci id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2">𝑄</ci><apply id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.2.3">superscript</csymbol><ci id="S3.E1.m1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.2.3.2">𝐾</ci><ci id="S3.E1.m1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.2.3.3">𝑇</ci></apply></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><root id="S3.E1.m1.1.1.3a.cmml" xref="S3.E1.m1.1.1.3"></root><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2">𝑑</ci><ci id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3">𝑘</ci></apply></apply></apply><ci id="S3.E1.m1.1.2.3.4.cmml" xref="S3.E1.m1.1.2.3.4">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">A=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">italic_A = softmax ( divide start_ARG italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ) italic_V</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.6">where <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">italic_Q</annotation></semantics></math>, <math alttext="K" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">italic_K</annotation></semantics></math>, and <math alttext="V" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><mi id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">italic_V</annotation></semantics></math> denote Query, Key, and Value matrices, respectively. In cross-modal contexts, these may be derived from different modalities (e.g., <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS2.p3.4.m4.1"><semantics id="S3.SS2.p3.4.m4.1a"><mi id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><ci id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.4.m4.1d">italic_Q</annotation></semantics></math> from text, <math alttext="K" class="ltx_Math" display="inline" id="S3.SS2.p3.5.m5.1"><semantics id="S3.SS2.p3.5.m5.1a"><mi id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><ci id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.5.m5.1d">italic_K</annotation></semantics></math> and <math alttext="V" class="ltx_Math" display="inline" id="S3.SS2.p3.6.m6.1"><semantics id="S3.SS2.p3.6.m6.1a"><mi id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><ci id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.6.m6.1d">italic_V</annotation></semantics></math> from image features).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">Object Localization and Classification</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The object localization and classification process is represented by three distinct equations:</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS1.4.1.1">III-C</span>1 </span>Localization Score</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L=\sigma(W_{L}F+b_{L})" class="ltx_Math" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">L</mi><mo id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml">σ</mi><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.2.2.2.cmml">W</mi><mi id="S3.E2.m1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.1.1.2.2.3.cmml">L</mi></msub><mo id="S3.E2.m1.1.1.1.1.1.1.2.1" xref="S3.E2.m1.1.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.2.3.cmml">F</mi></mrow><mo id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.3.2.cmml">b</mi><mi id="S3.E2.m1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.3.3.cmml">L</mi></msub></mrow><mo id="S3.E2.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"></eq><ci id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">𝐿</ci><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><times id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3">𝜎</ci><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><plus id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"></plus><apply id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"><times id="S3.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.1"></times><apply id="S3.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2.2">𝑊</ci><ci id="S3.E2.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2.3">𝐿</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.3">𝐹</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.2">𝑏</ci><ci id="S3.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.3">𝐿</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">L=\sigma(W_{L}F+b_{L})</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_L = italic_σ ( italic_W start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT italic_F + italic_b start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.5">where <math alttext="F" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.1.m1.1"><semantics id="S3.SS3.SSS1.p2.1.m1.1a"><mi id="S3.SS3.SSS1.p2.1.m1.1.1" xref="S3.SS3.SSS1.p2.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.1.m1.1b"><ci id="S3.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.1.m1.1c">F</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.1.m1.1d">italic_F</annotation></semantics></math> represents a feature vector extracted from the image, <math alttext="W_{L}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.2.m2.1"><semantics id="S3.SS3.SSS1.p2.2.m2.1a"><msub id="S3.SS3.SSS1.p2.2.m2.1.1" xref="S3.SS3.SSS1.p2.2.m2.1.1.cmml"><mi id="S3.SS3.SSS1.p2.2.m2.1.1.2" xref="S3.SS3.SSS1.p2.2.m2.1.1.2.cmml">W</mi><mi id="S3.SS3.SSS1.p2.2.m2.1.1.3" xref="S3.SS3.SSS1.p2.2.m2.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.2.m2.1b"><apply id="S3.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.2.m2.1.1.1.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.2.m2.1.1.2.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1.2">𝑊</ci><ci id="S3.SS3.SSS1.p2.2.m2.1.1.3.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.2.m2.1c">W_{L}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.2.m2.1d">italic_W start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="b_{L}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.3.m3.1"><semantics id="S3.SS3.SSS1.p2.3.m3.1a"><msub id="S3.SS3.SSS1.p2.3.m3.1.1" xref="S3.SS3.SSS1.p2.3.m3.1.1.cmml"><mi id="S3.SS3.SSS1.p2.3.m3.1.1.2" xref="S3.SS3.SSS1.p2.3.m3.1.1.2.cmml">b</mi><mi id="S3.SS3.SSS1.p2.3.m3.1.1.3" xref="S3.SS3.SSS1.p2.3.m3.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.3.m3.1b"><apply id="S3.SS3.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p2.3.m3.1.1.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p2.3.m3.1.1.2.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1.2">𝑏</ci><ci id="S3.SS3.SSS1.p2.3.m3.1.1.3.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.3.m3.1c">b_{L}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.3.m3.1d">italic_b start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT</annotation></semantics></math> are learned weight matrix and bias for localization, <math alttext="\sigma(\cdot)" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.4.m4.1"><semantics id="S3.SS3.SSS1.p2.4.m4.1a"><mrow id="S3.SS3.SSS1.p2.4.m4.1.2" xref="S3.SS3.SSS1.p2.4.m4.1.2.cmml"><mi id="S3.SS3.SSS1.p2.4.m4.1.2.2" xref="S3.SS3.SSS1.p2.4.m4.1.2.2.cmml">σ</mi><mo id="S3.SS3.SSS1.p2.4.m4.1.2.1" xref="S3.SS3.SSS1.p2.4.m4.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.SSS1.p2.4.m4.1.2.3.2" xref="S3.SS3.SSS1.p2.4.m4.1.2.cmml"><mo id="S3.SS3.SSS1.p2.4.m4.1.2.3.2.1" stretchy="false" xref="S3.SS3.SSS1.p2.4.m4.1.2.cmml">(</mo><mo id="S3.SS3.SSS1.p2.4.m4.1.1" lspace="0em" rspace="0em" xref="S3.SS3.SSS1.p2.4.m4.1.1.cmml">⋅</mo><mo id="S3.SS3.SSS1.p2.4.m4.1.2.3.2.2" stretchy="false" xref="S3.SS3.SSS1.p2.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.4.m4.1b"><apply id="S3.SS3.SSS1.p2.4.m4.1.2.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.2"><times id="S3.SS3.SSS1.p2.4.m4.1.2.1.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.2.1"></times><ci id="S3.SS3.SSS1.p2.4.m4.1.2.2.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.2.2">𝜎</ci><ci id="S3.SS3.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.4.m4.1c">\sigma(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.4.m4.1d">italic_σ ( ⋅ )</annotation></semantics></math> denotes the sigmoid activation function, which constrains the output to a range between 0 and 1, <math alttext="L" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.5.m5.1"><semantics id="S3.SS3.SSS1.p2.5.m5.1a"><mi id="S3.SS3.SSS1.p2.5.m5.1.1" xref="S3.SS3.SSS1.p2.5.m5.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.5.m5.1b"><ci id="S3.SS3.SSS1.p2.5.m5.1.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.5.m5.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.5.m5.1d">italic_L</annotation></semantics></math> represents the localization score, indicating the likelihood of an object’s presence.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS2.4.1.1">III-C</span>2 </span>Bounding Box Prediction</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="B=W_{B}F+b_{B}" class="ltx_Math" display="block" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mi id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">B</mi><mo id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mrow id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml"><msub id="S3.E3.m1.1.1.3.2.2" xref="S3.E3.m1.1.1.3.2.2.cmml"><mi id="S3.E3.m1.1.1.3.2.2.2" xref="S3.E3.m1.1.1.3.2.2.2.cmml">W</mi><mi id="S3.E3.m1.1.1.3.2.2.3" xref="S3.E3.m1.1.1.3.2.2.3.cmml">B</mi></msub><mo id="S3.E3.m1.1.1.3.2.1" xref="S3.E3.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S3.E3.m1.1.1.3.2.3" xref="S3.E3.m1.1.1.3.2.3.cmml">F</mi></mrow><mo id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">+</mo><msub id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.3.3.2" xref="S3.E3.m1.1.1.3.3.2.cmml">b</mi><mi id="S3.E3.m1.1.1.3.3.3" xref="S3.E3.m1.1.1.3.3.3.cmml">B</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"></eq><ci id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2">𝐵</ci><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><plus id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></plus><apply id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2"><times id="S3.E3.m1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.3.2.1"></times><apply id="S3.E3.m1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.2.1.cmml" xref="S3.E3.m1.1.1.3.2.2">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2.2">𝑊</ci><ci id="S3.E3.m1.1.1.3.2.2.3.cmml" xref="S3.E3.m1.1.1.3.2.2.3">𝐵</ci></apply><ci id="S3.E3.m1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.3.2.3">𝐹</ci></apply><apply id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2">𝑏</ci><ci id="S3.E3.m1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3">𝐵</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">B=W_{B}F+b_{B}</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">italic_B = italic_W start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT italic_F + italic_b start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.3">This equation is similar to the localization score equation but lacks an activation function. <math alttext="W_{B}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p2.1.m1.1"><semantics id="S3.SS3.SSS2.p2.1.m1.1a"><msub id="S3.SS3.SSS2.p2.1.m1.1.1" xref="S3.SS3.SSS2.p2.1.m1.1.1.cmml"><mi id="S3.SS3.SSS2.p2.1.m1.1.1.2" xref="S3.SS3.SSS2.p2.1.m1.1.1.2.cmml">W</mi><mi id="S3.SS3.SSS2.p2.1.m1.1.1.3" xref="S3.SS3.SSS2.p2.1.m1.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.1.m1.1b"><apply id="S3.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p2.1.m1.1.1.1.cmml" xref="S3.SS3.SSS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p2.1.m1.1.1.2.cmml" xref="S3.SS3.SSS2.p2.1.m1.1.1.2">𝑊</ci><ci id="S3.SS3.SSS2.p2.1.m1.1.1.3.cmml" xref="S3.SS3.SSS2.p2.1.m1.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.1.m1.1c">W_{B}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p2.1.m1.1d">italic_W start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="b_{B}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p2.2.m2.1"><semantics id="S3.SS3.SSS2.p2.2.m2.1a"><msub id="S3.SS3.SSS2.p2.2.m2.1.1" xref="S3.SS3.SSS2.p2.2.m2.1.1.cmml"><mi id="S3.SS3.SSS2.p2.2.m2.1.1.2" xref="S3.SS3.SSS2.p2.2.m2.1.1.2.cmml">b</mi><mi id="S3.SS3.SSS2.p2.2.m2.1.1.3" xref="S3.SS3.SSS2.p2.2.m2.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.2.m2.1b"><apply id="S3.SS3.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p2.2.m2.1.1.1.cmml" xref="S3.SS3.SSS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p2.2.m2.1.1.2.cmml" xref="S3.SS3.SSS2.p2.2.m2.1.1.2">𝑏</ci><ci id="S3.SS3.SSS2.p2.2.m2.1.1.3.cmml" xref="S3.SS3.SSS2.p2.2.m2.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.2.m2.1c">b_{B}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p2.2.m2.1d">italic_b start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math> are learned parameters for bounding box prediction, and <math alttext="B" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p2.3.m3.1"><semantics id="S3.SS3.SSS2.p2.3.m3.1a"><mi id="S3.SS3.SSS2.p2.3.m3.1.1" xref="S3.SS3.SSS2.p2.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.3.m3.1b"><ci id="S3.SS3.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS2.p2.3.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.3.m3.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p2.3.m3.1d">italic_B</annotation></semantics></math> directly outputs the predicted bounding box coordinates.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS3.4.1.1">III-C</span>3 </span>Object Classification</h4>
<div class="ltx_para" id="S3.SS3.SSS3.p1">
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="O=\operatorname*{arg\,max}(\text{softmax}(W_{O}F+b_{O}))" class="ltx_Math" display="block" id="S3.E4.m1.2"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml"><mi id="S3.E4.m1.2.2.3" xref="S3.E4.m1.2.2.3.cmml">O</mi><mo id="S3.E4.m1.2.2.2" xref="S3.E4.m1.2.2.2.cmml">=</mo><mrow id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.2.cmml"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mi id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">arg</mi><mo id="S3.E4.m1.1.1.1" lspace="0.170em" xref="S3.E4.m1.1.1.1.cmml">⁢</mo><mi id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml">max</mi></mrow><mo id="S3.E4.m1.2.2.1.1a" xref="S3.E4.m1.2.2.1.2.cmml">⁡</mo><mrow id="S3.E4.m1.2.2.1.1.1" xref="S3.E4.m1.2.2.1.2.cmml"><mo id="S3.E4.m1.2.2.1.1.1.2" stretchy="false" xref="S3.E4.m1.2.2.1.2.cmml">(</mo><mrow id="S3.E4.m1.2.2.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.cmml"><mtext id="S3.E4.m1.2.2.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.3a.cmml">softmax</mtext><mo id="S3.E4.m1.2.2.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.2.2.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml"><mrow id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.cmml">W</mi><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.3.cmml">O</mi></msub><mo id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">F</mi></mrow><mo id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.2.cmml">b</mi><mi id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.cmml">O</mi></msub></mrow><mo id="S3.E4.m1.2.2.1.1.1.1.1.1.3" stretchy="false" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.2.2.1.1.1.3" stretchy="false" xref="S3.E4.m1.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2"><eq id="S3.E4.m1.2.2.2.cmml" xref="S3.E4.m1.2.2.2"></eq><ci id="S3.E4.m1.2.2.3.cmml" xref="S3.E4.m1.2.2.3">𝑂</ci><apply id="S3.E4.m1.2.2.1.2.cmml" xref="S3.E4.m1.2.2.1.1"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><times id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"></times><ci id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2">arg</ci><ci id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3">max</ci></apply><apply id="S3.E4.m1.2.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1"><times id="S3.E4.m1.2.2.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.2"></times><ci id="S3.E4.m1.2.2.1.1.1.1.3a.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3"><mtext id="S3.E4.m1.2.2.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3">softmax</mtext></ci><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1"><plus id="S3.E4.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.1"></plus><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2"><times id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.1"></times><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.2">𝑊</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.2.3">𝑂</ci></apply><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.2.3">𝐹</ci></apply><apply id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.2">𝑏</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.1.1.3.3">𝑂</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">O=\operatorname*{arg\,max}(\text{softmax}(W_{O}F+b_{O}))</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.2d">italic_O = start_OPERATOR roman_arg roman_max end_OPERATOR ( softmax ( italic_W start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT italic_F + italic_b start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.SSS3.p2">
<p class="ltx_p" id="S3.SS3.SSS3.p2.5">where: <math alttext="W_{O}" class="ltx_Math" display="inline" id="S3.SS3.SSS3.p2.1.m1.1"><semantics id="S3.SS3.SSS3.p2.1.m1.1a"><msub id="S3.SS3.SSS3.p2.1.m1.1.1" xref="S3.SS3.SSS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.SSS3.p2.1.m1.1.1.2" xref="S3.SS3.SSS3.p2.1.m1.1.1.2.cmml">W</mi><mi id="S3.SS3.SSS3.p2.1.m1.1.1.3" xref="S3.SS3.SSS3.p2.1.m1.1.1.3.cmml">O</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p2.1.m1.1b"><apply id="S3.SS3.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.SSS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.SSS3.p2.1.m1.1.1.2">𝑊</ci><ci id="S3.SS3.SSS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.SSS3.p2.1.m1.1.1.3">𝑂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p2.1.m1.1c">W_{O}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS3.p2.1.m1.1d">italic_W start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="b_{O}" class="ltx_Math" display="inline" id="S3.SS3.SSS3.p2.2.m2.1"><semantics id="S3.SS3.SSS3.p2.2.m2.1a"><msub id="S3.SS3.SSS3.p2.2.m2.1.1" xref="S3.SS3.SSS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.SSS3.p2.2.m2.1.1.2" xref="S3.SS3.SSS3.p2.2.m2.1.1.2.cmml">b</mi><mi id="S3.SS3.SSS3.p2.2.m2.1.1.3" xref="S3.SS3.SSS3.p2.2.m2.1.1.3.cmml">O</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p2.2.m2.1b"><apply id="S3.SS3.SSS3.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.SSS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.SSS3.p2.2.m2.1.1.2">𝑏</ci><ci id="S3.SS3.SSS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.SSS3.p2.2.m2.1.1.3">𝑂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p2.2.m2.1c">b_{O}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS3.p2.2.m2.1d">italic_b start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT</annotation></semantics></math> are learned parameters for object classification, <math alttext="\text{softmax}(\cdot)" class="ltx_Math" display="inline" id="S3.SS3.SSS3.p2.3.m3.1"><semantics id="S3.SS3.SSS3.p2.3.m3.1a"><mrow id="S3.SS3.SSS3.p2.3.m3.1.2" xref="S3.SS3.SSS3.p2.3.m3.1.2.cmml"><mtext id="S3.SS3.SSS3.p2.3.m3.1.2.2" xref="S3.SS3.SSS3.p2.3.m3.1.2.2a.cmml">softmax</mtext><mo id="S3.SS3.SSS3.p2.3.m3.1.2.1" xref="S3.SS3.SSS3.p2.3.m3.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.SSS3.p2.3.m3.1.2.3.2" xref="S3.SS3.SSS3.p2.3.m3.1.2.cmml"><mo id="S3.SS3.SSS3.p2.3.m3.1.2.3.2.1" stretchy="false" xref="S3.SS3.SSS3.p2.3.m3.1.2.cmml">(</mo><mo id="S3.SS3.SSS3.p2.3.m3.1.1" lspace="0em" rspace="0em" xref="S3.SS3.SSS3.p2.3.m3.1.1.cmml">⋅</mo><mo id="S3.SS3.SSS3.p2.3.m3.1.2.3.2.2" stretchy="false" xref="S3.SS3.SSS3.p2.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p2.3.m3.1b"><apply id="S3.SS3.SSS3.p2.3.m3.1.2.cmml" xref="S3.SS3.SSS3.p2.3.m3.1.2"><times id="S3.SS3.SSS3.p2.3.m3.1.2.1.cmml" xref="S3.SS3.SSS3.p2.3.m3.1.2.1"></times><ci id="S3.SS3.SSS3.p2.3.m3.1.2.2a.cmml" xref="S3.SS3.SSS3.p2.3.m3.1.2.2"><mtext id="S3.SS3.SSS3.p2.3.m3.1.2.2.cmml" xref="S3.SS3.SSS3.p2.3.m3.1.2.2">softmax</mtext></ci><ci id="S3.SS3.SSS3.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS3.p2.3.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p2.3.m3.1c">\text{softmax}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS3.p2.3.m3.1d">softmax ( ⋅ )</annotation></semantics></math> normalizes the output into a probability distribution over object classes, <math alttext="\operatorname*{arg\,max}(\cdot)" class="ltx_Math" display="inline" id="S3.SS3.SSS3.p2.4.m4.2"><semantics id="S3.SS3.SSS3.p2.4.m4.2a"><mrow id="S3.SS3.SSS3.p2.4.m4.2.3.2" xref="S3.SS3.SSS3.p2.4.m4.2.3.1.cmml"><mrow id="S3.SS3.SSS3.p2.4.m4.1.1" xref="S3.SS3.SSS3.p2.4.m4.1.1.cmml"><mi id="S3.SS3.SSS3.p2.4.m4.1.1.2" xref="S3.SS3.SSS3.p2.4.m4.1.1.2.cmml">arg</mi><mo id="S3.SS3.SSS3.p2.4.m4.1.1.1" lspace="0.170em" xref="S3.SS3.SSS3.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="S3.SS3.SSS3.p2.4.m4.1.1.3" xref="S3.SS3.SSS3.p2.4.m4.1.1.3.cmml">max</mi></mrow><mo id="S3.SS3.SSS3.p2.4.m4.2.3.2a" xref="S3.SS3.SSS3.p2.4.m4.2.3.1.cmml">⁡</mo><mrow id="S3.SS3.SSS3.p2.4.m4.2.3.2.1" xref="S3.SS3.SSS3.p2.4.m4.2.3.1.cmml"><mo id="S3.SS3.SSS3.p2.4.m4.2.3.2.1.1" stretchy="false" xref="S3.SS3.SSS3.p2.4.m4.2.3.1.cmml">(</mo><mo id="S3.SS3.SSS3.p2.4.m4.2.2" lspace="0em" rspace="0em" xref="S3.SS3.SSS3.p2.4.m4.2.2.cmml">⋅</mo><mo id="S3.SS3.SSS3.p2.4.m4.2.3.2.1.2" stretchy="false" xref="S3.SS3.SSS3.p2.4.m4.2.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p2.4.m4.2b"><apply id="S3.SS3.SSS3.p2.4.m4.2.3.1.cmml" xref="S3.SS3.SSS3.p2.4.m4.2.3.2"><apply id="S3.SS3.SSS3.p2.4.m4.1.1.cmml" xref="S3.SS3.SSS3.p2.4.m4.1.1"><times id="S3.SS3.SSS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.SSS3.p2.4.m4.1.1.1"></times><ci id="S3.SS3.SSS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.SSS3.p2.4.m4.1.1.2">arg</ci><ci id="S3.SS3.SSS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.SSS3.p2.4.m4.1.1.3">max</ci></apply><ci id="S3.SS3.SSS3.p2.4.m4.2.2.cmml" xref="S3.SS3.SSS3.p2.4.m4.2.2">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p2.4.m4.2c">\operatorname*{arg\,max}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS3.p2.4.m4.2d">start_OPERATOR roman_arg roman_max end_OPERATOR ( ⋅ )</annotation></semantics></math> selects the class with the highest probability, <math alttext="O" class="ltx_Math" display="inline" id="S3.SS3.SSS3.p2.5.m5.1"><semantics id="S3.SS3.SSS3.p2.5.m5.1a"><mi id="S3.SS3.SSS3.p2.5.m5.1.1" xref="S3.SS3.SSS3.p2.5.m5.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p2.5.m5.1b"><ci id="S3.SS3.SSS3.p2.5.m5.1.1.cmml" xref="S3.SS3.SSS3.p2.5.m5.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p2.5.m5.1c">O</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS3.p2.5.m5.1d">italic_O</annotation></semantics></math> represents the predicted object category.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.4.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.5.2">Decision Module and Point Cloud Retrieval</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Based on NExT-Chat’s output, the system performs the following steps:</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.3">1) Decision making:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="D=f(R_{VL},B,O)" class="ltx_Math" display="block" id="S3.E5.m1.3"><semantics id="S3.E5.m1.3a"><mrow id="S3.E5.m1.3.3" xref="S3.E5.m1.3.3.cmml"><mi id="S3.E5.m1.3.3.3" xref="S3.E5.m1.3.3.3.cmml">D</mi><mo id="S3.E5.m1.3.3.2" xref="S3.E5.m1.3.3.2.cmml">=</mo><mrow id="S3.E5.m1.3.3.1" xref="S3.E5.m1.3.3.1.cmml"><mi id="S3.E5.m1.3.3.1.3" xref="S3.E5.m1.3.3.1.3.cmml">f</mi><mo id="S3.E5.m1.3.3.1.2" xref="S3.E5.m1.3.3.1.2.cmml">⁢</mo><mrow id="S3.E5.m1.3.3.1.1.1" xref="S3.E5.m1.3.3.1.1.2.cmml"><mo id="S3.E5.m1.3.3.1.1.1.2" stretchy="false" xref="S3.E5.m1.3.3.1.1.2.cmml">(</mo><msub id="S3.E5.m1.3.3.1.1.1.1" xref="S3.E5.m1.3.3.1.1.1.1.cmml"><mi id="S3.E5.m1.3.3.1.1.1.1.2" xref="S3.E5.m1.3.3.1.1.1.1.2.cmml">R</mi><mrow id="S3.E5.m1.3.3.1.1.1.1.3" xref="S3.E5.m1.3.3.1.1.1.1.3.cmml"><mi id="S3.E5.m1.3.3.1.1.1.1.3.2" xref="S3.E5.m1.3.3.1.1.1.1.3.2.cmml">V</mi><mo id="S3.E5.m1.3.3.1.1.1.1.3.1" xref="S3.E5.m1.3.3.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E5.m1.3.3.1.1.1.1.3.3" xref="S3.E5.m1.3.3.1.1.1.1.3.3.cmml">L</mi></mrow></msub><mo id="S3.E5.m1.3.3.1.1.1.3" xref="S3.E5.m1.3.3.1.1.2.cmml">,</mo><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">B</mi><mo id="S3.E5.m1.3.3.1.1.1.4" xref="S3.E5.m1.3.3.1.1.2.cmml">,</mo><mi id="S3.E5.m1.2.2" xref="S3.E5.m1.2.2.cmml">O</mi><mo id="S3.E5.m1.3.3.1.1.1.5" stretchy="false" xref="S3.E5.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.3b"><apply id="S3.E5.m1.3.3.cmml" xref="S3.E5.m1.3.3"><eq id="S3.E5.m1.3.3.2.cmml" xref="S3.E5.m1.3.3.2"></eq><ci id="S3.E5.m1.3.3.3.cmml" xref="S3.E5.m1.3.3.3">𝐷</ci><apply id="S3.E5.m1.3.3.1.cmml" xref="S3.E5.m1.3.3.1"><times id="S3.E5.m1.3.3.1.2.cmml" xref="S3.E5.m1.3.3.1.2"></times><ci id="S3.E5.m1.3.3.1.3.cmml" xref="S3.E5.m1.3.3.1.3">𝑓</ci><vector id="S3.E5.m1.3.3.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1"><apply id="S3.E5.m1.3.3.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.3.3.1.1.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.2">𝑅</ci><apply id="S3.E5.m1.3.3.1.1.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.3"><times id="S3.E5.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.E5.m1.3.3.1.1.1.1.3.1"></times><ci id="S3.E5.m1.3.3.1.1.1.1.3.2.cmml" xref="S3.E5.m1.3.3.1.1.1.1.3.2">𝑉</ci><ci id="S3.E5.m1.3.3.1.1.1.1.3.3.cmml" xref="S3.E5.m1.3.3.1.1.1.1.3.3">𝐿</ci></apply></apply><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">𝐵</ci><ci id="S3.E5.m1.2.2.cmml" xref="S3.E5.m1.2.2">𝑂</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.3c">D=f(R_{VL},B,O)</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.3d">italic_D = italic_f ( italic_R start_POSTSUBSCRIPT italic_V italic_L end_POSTSUBSCRIPT , italic_B , italic_O )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.p2.2">where <math alttext="f" class="ltx_Math" display="inline" id="S3.SS4.p2.1.m1.1"><semantics id="S3.SS4.p2.1.m1.1a"><mi id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">f</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.1.m1.1d">italic_f</annotation></semantics></math> is a decision function, outputting <math alttext="D\in\{0,1\}" class="ltx_Math" display="inline" id="S3.SS4.p2.2.m2.2"><semantics id="S3.SS4.p2.2.m2.2a"><mrow id="S3.SS4.p2.2.m2.2.3" xref="S3.SS4.p2.2.m2.2.3.cmml"><mi id="S3.SS4.p2.2.m2.2.3.2" xref="S3.SS4.p2.2.m2.2.3.2.cmml">D</mi><mo id="S3.SS4.p2.2.m2.2.3.1" xref="S3.SS4.p2.2.m2.2.3.1.cmml">∈</mo><mrow id="S3.SS4.p2.2.m2.2.3.3.2" xref="S3.SS4.p2.2.m2.2.3.3.1.cmml"><mo id="S3.SS4.p2.2.m2.2.3.3.2.1" stretchy="false" xref="S3.SS4.p2.2.m2.2.3.3.1.cmml">{</mo><mn id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml">0</mn><mo id="S3.SS4.p2.2.m2.2.3.3.2.2" xref="S3.SS4.p2.2.m2.2.3.3.1.cmml">,</mo><mn id="S3.SS4.p2.2.m2.2.2" xref="S3.SS4.p2.2.m2.2.2.cmml">1</mn><mo id="S3.SS4.p2.2.m2.2.3.3.2.3" stretchy="false" xref="S3.SS4.p2.2.m2.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.2b"><apply id="S3.SS4.p2.2.m2.2.3.cmml" xref="S3.SS4.p2.2.m2.2.3"><in id="S3.SS4.p2.2.m2.2.3.1.cmml" xref="S3.SS4.p2.2.m2.2.3.1"></in><ci id="S3.SS4.p2.2.m2.2.3.2.cmml" xref="S3.SS4.p2.2.m2.2.3.2">𝐷</ci><set id="S3.SS4.p2.2.m2.2.3.3.1.cmml" xref="S3.SS4.p2.2.m2.2.3.3.2"><cn id="S3.SS4.p2.2.m2.1.1.cmml" type="integer" xref="S3.SS4.p2.2.m2.1.1">0</cn><cn id="S3.SS4.p2.2.m2.2.2.cmml" type="integer" xref="S3.SS4.p2.2.m2.2.2">1</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.2c">D\in\{0,1\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.2.m2.2d">italic_D ∈ { 0 , 1 }</annotation></semantics></math> indicating whether to proceed to the next step.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">2) If <math alttext="D=1" class="ltx_Math" display="inline" id="S3.SS4.p3.1.m1.1"><semantics id="S3.SS4.p3.1.m1.1a"><mrow id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">D</mi><mo id="S3.SS4.p3.1.m1.1.1.1" xref="S3.SS4.p3.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><eq id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1.1"></eq><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">𝐷</ci><cn id="S3.SS4.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.SS4.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">D=1</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.1.m1.1d">italic_D = 1</annotation></semantics></math>, the system retrieves the corresponding point cloud data from the database:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P=\text{Database}(O)" class="ltx_Math" display="block" id="S3.E6.m1.1"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.2" xref="S3.E6.m1.1.2.cmml"><mi id="S3.E6.m1.1.2.2" xref="S3.E6.m1.1.2.2.cmml">P</mi><mo id="S3.E6.m1.1.2.1" xref="S3.E6.m1.1.2.1.cmml">=</mo><mrow id="S3.E6.m1.1.2.3" xref="S3.E6.m1.1.2.3.cmml"><mtext id="S3.E6.m1.1.2.3.2" xref="S3.E6.m1.1.2.3.2a.cmml">Database</mtext><mo id="S3.E6.m1.1.2.3.1" xref="S3.E6.m1.1.2.3.1.cmml">⁢</mo><mrow id="S3.E6.m1.1.2.3.3.2" xref="S3.E6.m1.1.2.3.cmml"><mo id="S3.E6.m1.1.2.3.3.2.1" stretchy="false" xref="S3.E6.m1.1.2.3.cmml">(</mo><mi id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml">O</mi><mo id="S3.E6.m1.1.2.3.3.2.2" stretchy="false" xref="S3.E6.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.2.cmml" xref="S3.E6.m1.1.2"><eq id="S3.E6.m1.1.2.1.cmml" xref="S3.E6.m1.1.2.1"></eq><ci id="S3.E6.m1.1.2.2.cmml" xref="S3.E6.m1.1.2.2">𝑃</ci><apply id="S3.E6.m1.1.2.3.cmml" xref="S3.E6.m1.1.2.3"><times id="S3.E6.m1.1.2.3.1.cmml" xref="S3.E6.m1.1.2.3.1"></times><ci id="S3.E6.m1.1.2.3.2a.cmml" xref="S3.E6.m1.1.2.3.2"><mtext id="S3.E6.m1.1.2.3.2.cmml" xref="S3.E6.m1.1.2.3.2">Database</mtext></ci><ci id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1">𝑂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">P=\text{Database}(O)</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m1.1d">italic_P = Database ( italic_O )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.p3.2">where <math alttext="P" class="ltx_Math" display="inline" id="S3.SS4.p3.2.m1.1"><semantics id="S3.SS4.p3.2.m1.1a"><mi id="S3.SS4.p3.2.m1.1.1" xref="S3.SS4.p3.2.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m1.1b"><ci id="S3.SS4.p3.2.m1.1.1.cmml" xref="S3.SS4.p3.2.m1.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m1.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.2.m1.1d">italic_P</annotation></semantics></math> is the retrieved 3D point cloud data.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS5.4.1.1">III-E</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS5.5.2">Language-Guided 3D Affordance Segmentation</span>
</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">PointRefer receives the output from the decision module and the retrieved point cloud <math alttext="P" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1.1"><semantics id="S3.SS5.p1.1.m1.1a"><mi id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><ci id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.1.m1.1d">italic_P</annotation></semantics></math> to perform language-guided affordance segmentation:</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">1) Adaptive Fusion Module (AFM):</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="F_{fused}=\text{AFM}(R_{VL},P)" class="ltx_Math" display="block" id="S3.E7.m1.2"><semantics id="S3.E7.m1.2a"><mrow id="S3.E7.m1.2.2" xref="S3.E7.m1.2.2.cmml"><msub id="S3.E7.m1.2.2.3" xref="S3.E7.m1.2.2.3.cmml"><mi id="S3.E7.m1.2.2.3.2" xref="S3.E7.m1.2.2.3.2.cmml">F</mi><mrow id="S3.E7.m1.2.2.3.3" xref="S3.E7.m1.2.2.3.3.cmml"><mi id="S3.E7.m1.2.2.3.3.2" xref="S3.E7.m1.2.2.3.3.2.cmml">f</mi><mo id="S3.E7.m1.2.2.3.3.1" xref="S3.E7.m1.2.2.3.3.1.cmml">⁢</mo><mi id="S3.E7.m1.2.2.3.3.3" xref="S3.E7.m1.2.2.3.3.3.cmml">u</mi><mo id="S3.E7.m1.2.2.3.3.1a" xref="S3.E7.m1.2.2.3.3.1.cmml">⁢</mo><mi id="S3.E7.m1.2.2.3.3.4" xref="S3.E7.m1.2.2.3.3.4.cmml">s</mi><mo id="S3.E7.m1.2.2.3.3.1b" xref="S3.E7.m1.2.2.3.3.1.cmml">⁢</mo><mi id="S3.E7.m1.2.2.3.3.5" xref="S3.E7.m1.2.2.3.3.5.cmml">e</mi><mo id="S3.E7.m1.2.2.3.3.1c" xref="S3.E7.m1.2.2.3.3.1.cmml">⁢</mo><mi id="S3.E7.m1.2.2.3.3.6" xref="S3.E7.m1.2.2.3.3.6.cmml">d</mi></mrow></msub><mo id="S3.E7.m1.2.2.2" xref="S3.E7.m1.2.2.2.cmml">=</mo><mrow id="S3.E7.m1.2.2.1" xref="S3.E7.m1.2.2.1.cmml"><mtext id="S3.E7.m1.2.2.1.3" xref="S3.E7.m1.2.2.1.3a.cmml">AFM</mtext><mo id="S3.E7.m1.2.2.1.2" xref="S3.E7.m1.2.2.1.2.cmml">⁢</mo><mrow id="S3.E7.m1.2.2.1.1.1" xref="S3.E7.m1.2.2.1.1.2.cmml"><mo id="S3.E7.m1.2.2.1.1.1.2" stretchy="false" xref="S3.E7.m1.2.2.1.1.2.cmml">(</mo><msub id="S3.E7.m1.2.2.1.1.1.1" xref="S3.E7.m1.2.2.1.1.1.1.cmml"><mi id="S3.E7.m1.2.2.1.1.1.1.2" xref="S3.E7.m1.2.2.1.1.1.1.2.cmml">R</mi><mrow id="S3.E7.m1.2.2.1.1.1.1.3" xref="S3.E7.m1.2.2.1.1.1.1.3.cmml"><mi id="S3.E7.m1.2.2.1.1.1.1.3.2" xref="S3.E7.m1.2.2.1.1.1.1.3.2.cmml">V</mi><mo id="S3.E7.m1.2.2.1.1.1.1.3.1" xref="S3.E7.m1.2.2.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E7.m1.2.2.1.1.1.1.3.3" xref="S3.E7.m1.2.2.1.1.1.1.3.3.cmml">L</mi></mrow></msub><mo id="S3.E7.m1.2.2.1.1.1.3" xref="S3.E7.m1.2.2.1.1.2.cmml">,</mo><mi id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml">P</mi><mo id="S3.E7.m1.2.2.1.1.1.4" stretchy="false" xref="S3.E7.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.2b"><apply id="S3.E7.m1.2.2.cmml" xref="S3.E7.m1.2.2"><eq id="S3.E7.m1.2.2.2.cmml" xref="S3.E7.m1.2.2.2"></eq><apply id="S3.E7.m1.2.2.3.cmml" xref="S3.E7.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.3.1.cmml" xref="S3.E7.m1.2.2.3">subscript</csymbol><ci id="S3.E7.m1.2.2.3.2.cmml" xref="S3.E7.m1.2.2.3.2">𝐹</ci><apply id="S3.E7.m1.2.2.3.3.cmml" xref="S3.E7.m1.2.2.3.3"><times id="S3.E7.m1.2.2.3.3.1.cmml" xref="S3.E7.m1.2.2.3.3.1"></times><ci id="S3.E7.m1.2.2.3.3.2.cmml" xref="S3.E7.m1.2.2.3.3.2">𝑓</ci><ci id="S3.E7.m1.2.2.3.3.3.cmml" xref="S3.E7.m1.2.2.3.3.3">𝑢</ci><ci id="S3.E7.m1.2.2.3.3.4.cmml" xref="S3.E7.m1.2.2.3.3.4">𝑠</ci><ci id="S3.E7.m1.2.2.3.3.5.cmml" xref="S3.E7.m1.2.2.3.3.5">𝑒</ci><ci id="S3.E7.m1.2.2.3.3.6.cmml" xref="S3.E7.m1.2.2.3.3.6">𝑑</ci></apply></apply><apply id="S3.E7.m1.2.2.1.cmml" xref="S3.E7.m1.2.2.1"><times id="S3.E7.m1.2.2.1.2.cmml" xref="S3.E7.m1.2.2.1.2"></times><ci id="S3.E7.m1.2.2.1.3a.cmml" xref="S3.E7.m1.2.2.1.3"><mtext id="S3.E7.m1.2.2.1.3.cmml" xref="S3.E7.m1.2.2.1.3">AFM</mtext></ci><interval closure="open" id="S3.E7.m1.2.2.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.1"><apply id="S3.E7.m1.2.2.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.1.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S3.E7.m1.2.2.1.1.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.1.1.2">𝑅</ci><apply id="S3.E7.m1.2.2.1.1.1.1.3.cmml" xref="S3.E7.m1.2.2.1.1.1.1.3"><times id="S3.E7.m1.2.2.1.1.1.1.3.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1.3.1"></times><ci id="S3.E7.m1.2.2.1.1.1.1.3.2.cmml" xref="S3.E7.m1.2.2.1.1.1.1.3.2">𝑉</ci><ci id="S3.E7.m1.2.2.1.1.1.1.3.3.cmml" xref="S3.E7.m1.2.2.1.1.1.1.3.3">𝐿</ci></apply></apply><ci id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1">𝑃</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.2c">F_{fused}=\text{AFM}(R_{VL},P)</annotation><annotation encoding="application/x-llamapun" id="S3.E7.m1.2d">italic_F start_POSTSUBSCRIPT italic_f italic_u italic_s italic_e italic_d end_POSTSUBSCRIPT = AFM ( italic_R start_POSTSUBSCRIPT italic_V italic_L end_POSTSUBSCRIPT , italic_P )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.1">2) Referred Point Decoder (RPD) generates dynamic convolution kernels:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="K_{dynamic}=\text{RPD}(F_{fused})" class="ltx_Math" display="block" id="S3.E8.m1.1"><semantics id="S3.E8.m1.1a"><mrow id="S3.E8.m1.1.1" xref="S3.E8.m1.1.1.cmml"><msub id="S3.E8.m1.1.1.3" xref="S3.E8.m1.1.1.3.cmml"><mi id="S3.E8.m1.1.1.3.2" xref="S3.E8.m1.1.1.3.2.cmml">K</mi><mrow id="S3.E8.m1.1.1.3.3" xref="S3.E8.m1.1.1.3.3.cmml"><mi id="S3.E8.m1.1.1.3.3.2" xref="S3.E8.m1.1.1.3.3.2.cmml">d</mi><mo id="S3.E8.m1.1.1.3.3.1" xref="S3.E8.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E8.m1.1.1.3.3.3" xref="S3.E8.m1.1.1.3.3.3.cmml">y</mi><mo id="S3.E8.m1.1.1.3.3.1a" xref="S3.E8.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E8.m1.1.1.3.3.4" xref="S3.E8.m1.1.1.3.3.4.cmml">n</mi><mo id="S3.E8.m1.1.1.3.3.1b" xref="S3.E8.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E8.m1.1.1.3.3.5" xref="S3.E8.m1.1.1.3.3.5.cmml">a</mi><mo id="S3.E8.m1.1.1.3.3.1c" xref="S3.E8.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E8.m1.1.1.3.3.6" xref="S3.E8.m1.1.1.3.3.6.cmml">m</mi><mo id="S3.E8.m1.1.1.3.3.1d" xref="S3.E8.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E8.m1.1.1.3.3.7" xref="S3.E8.m1.1.1.3.3.7.cmml">i</mi><mo id="S3.E8.m1.1.1.3.3.1e" xref="S3.E8.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E8.m1.1.1.3.3.8" xref="S3.E8.m1.1.1.3.3.8.cmml">c</mi></mrow></msub><mo id="S3.E8.m1.1.1.2" xref="S3.E8.m1.1.1.2.cmml">=</mo><mrow id="S3.E8.m1.1.1.1" xref="S3.E8.m1.1.1.1.cmml"><mtext id="S3.E8.m1.1.1.1.3" xref="S3.E8.m1.1.1.1.3a.cmml">RPD</mtext><mo id="S3.E8.m1.1.1.1.2" xref="S3.E8.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E8.m1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.cmml"><mo id="S3.E8.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E8.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E8.m1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.2.cmml">F</mi><mrow id="S3.E8.m1.1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.3.2" xref="S3.E8.m1.1.1.1.1.1.1.3.2.cmml">f</mi><mo id="S3.E8.m1.1.1.1.1.1.1.3.1" xref="S3.E8.m1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E8.m1.1.1.1.1.1.1.3.3" xref="S3.E8.m1.1.1.1.1.1.1.3.3.cmml">u</mi><mo id="S3.E8.m1.1.1.1.1.1.1.3.1a" xref="S3.E8.m1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E8.m1.1.1.1.1.1.1.3.4" xref="S3.E8.m1.1.1.1.1.1.1.3.4.cmml">s</mi><mo id="S3.E8.m1.1.1.1.1.1.1.3.1b" xref="S3.E8.m1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E8.m1.1.1.1.1.1.1.3.5" xref="S3.E8.m1.1.1.1.1.1.1.3.5.cmml">e</mi><mo id="S3.E8.m1.1.1.1.1.1.1.3.1c" xref="S3.E8.m1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E8.m1.1.1.1.1.1.1.3.6" xref="S3.E8.m1.1.1.1.1.1.1.3.6.cmml">d</mi></mrow></msub><mo id="S3.E8.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E8.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.1b"><apply id="S3.E8.m1.1.1.cmml" xref="S3.E8.m1.1.1"><eq id="S3.E8.m1.1.1.2.cmml" xref="S3.E8.m1.1.1.2"></eq><apply id="S3.E8.m1.1.1.3.cmml" xref="S3.E8.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.3.1.cmml" xref="S3.E8.m1.1.1.3">subscript</csymbol><ci id="S3.E8.m1.1.1.3.2.cmml" xref="S3.E8.m1.1.1.3.2">𝐾</ci><apply id="S3.E8.m1.1.1.3.3.cmml" xref="S3.E8.m1.1.1.3.3"><times id="S3.E8.m1.1.1.3.3.1.cmml" xref="S3.E8.m1.1.1.3.3.1"></times><ci id="S3.E8.m1.1.1.3.3.2.cmml" xref="S3.E8.m1.1.1.3.3.2">𝑑</ci><ci id="S3.E8.m1.1.1.3.3.3.cmml" xref="S3.E8.m1.1.1.3.3.3">𝑦</ci><ci id="S3.E8.m1.1.1.3.3.4.cmml" xref="S3.E8.m1.1.1.3.3.4">𝑛</ci><ci id="S3.E8.m1.1.1.3.3.5.cmml" xref="S3.E8.m1.1.1.3.3.5">𝑎</ci><ci id="S3.E8.m1.1.1.3.3.6.cmml" xref="S3.E8.m1.1.1.3.3.6">𝑚</ci><ci id="S3.E8.m1.1.1.3.3.7.cmml" xref="S3.E8.m1.1.1.3.3.7">𝑖</ci><ci id="S3.E8.m1.1.1.3.3.8.cmml" xref="S3.E8.m1.1.1.3.3.8">𝑐</ci></apply></apply><apply id="S3.E8.m1.1.1.1.cmml" xref="S3.E8.m1.1.1.1"><times id="S3.E8.m1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.2"></times><ci id="S3.E8.m1.1.1.1.3a.cmml" xref="S3.E8.m1.1.1.1.3"><mtext id="S3.E8.m1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.3">RPD</mtext></ci><apply id="S3.E8.m1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E8.m1.1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.2">𝐹</ci><apply id="S3.E8.m1.1.1.1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3"><times id="S3.E8.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1"></times><ci id="S3.E8.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.2">𝑓</ci><ci id="S3.E8.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.3">𝑢</ci><ci id="S3.E8.m1.1.1.1.1.1.1.3.4.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.4">𝑠</ci><ci id="S3.E8.m1.1.1.1.1.1.1.3.5.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.5">𝑒</ci><ci id="S3.E8.m1.1.1.1.1.1.1.3.6.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.6">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.1c">K_{dynamic}=\text{RPD}(F_{fused})</annotation><annotation encoding="application/x-llamapun" id="S3.E8.m1.1d">italic_K start_POSTSUBSCRIPT italic_d italic_y italic_n italic_a italic_m italic_i italic_c end_POSTSUBSCRIPT = RPD ( italic_F start_POSTSUBSCRIPT italic_f italic_u italic_s italic_e italic_d end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS5.p4">
<p class="ltx_p" id="S3.SS5.p4.2">3) 3D affordance segmentation:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="M_{3D}=\sigma(K_{dynamic}*F_{fused})" class="ltx_Math" display="block" id="S3.E9.m1.1"><semantics id="S3.E9.m1.1a"><mrow id="S3.E9.m1.1.1" xref="S3.E9.m1.1.1.cmml"><msub id="S3.E9.m1.1.1.3" xref="S3.E9.m1.1.1.3.cmml"><mi id="S3.E9.m1.1.1.3.2" xref="S3.E9.m1.1.1.3.2.cmml">M</mi><mrow id="S3.E9.m1.1.1.3.3" xref="S3.E9.m1.1.1.3.3.cmml"><mn id="S3.E9.m1.1.1.3.3.2" xref="S3.E9.m1.1.1.3.3.2.cmml">3</mn><mo id="S3.E9.m1.1.1.3.3.1" xref="S3.E9.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E9.m1.1.1.3.3.3" xref="S3.E9.m1.1.1.3.3.3.cmml">D</mi></mrow></msub><mo id="S3.E9.m1.1.1.2" xref="S3.E9.m1.1.1.2.cmml">=</mo><mrow id="S3.E9.m1.1.1.1" xref="S3.E9.m1.1.1.1.cmml"><mi id="S3.E9.m1.1.1.1.3" xref="S3.E9.m1.1.1.1.3.cmml">σ</mi><mo id="S3.E9.m1.1.1.1.2" xref="S3.E9.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E9.m1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.cmml"><mo id="S3.E9.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E9.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E9.m1.1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.cmml"><msub id="S3.E9.m1.1.1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E9.m1.1.1.1.1.1.1.2.2" xref="S3.E9.m1.1.1.1.1.1.1.2.2.cmml">K</mi><mrow id="S3.E9.m1.1.1.1.1.1.1.2.3" xref="S3.E9.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E9.m1.1.1.1.1.1.1.2.3.2" xref="S3.E9.m1.1.1.1.1.1.1.2.3.2.cmml">d</mi><mo id="S3.E9.m1.1.1.1.1.1.1.2.3.1" xref="S3.E9.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E9.m1.1.1.1.1.1.1.2.3.3" xref="S3.E9.m1.1.1.1.1.1.1.2.3.3.cmml">y</mi><mo id="S3.E9.m1.1.1.1.1.1.1.2.3.1a" xref="S3.E9.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E9.m1.1.1.1.1.1.1.2.3.4" xref="S3.E9.m1.1.1.1.1.1.1.2.3.4.cmml">n</mi><mo id="S3.E9.m1.1.1.1.1.1.1.2.3.1b" xref="S3.E9.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E9.m1.1.1.1.1.1.1.2.3.5" xref="S3.E9.m1.1.1.1.1.1.1.2.3.5.cmml">a</mi><mo id="S3.E9.m1.1.1.1.1.1.1.2.3.1c" xref="S3.E9.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E9.m1.1.1.1.1.1.1.2.3.6" xref="S3.E9.m1.1.1.1.1.1.1.2.3.6.cmml">m</mi><mo id="S3.E9.m1.1.1.1.1.1.1.2.3.1d" xref="S3.E9.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E9.m1.1.1.1.1.1.1.2.3.7" xref="S3.E9.m1.1.1.1.1.1.1.2.3.7.cmml">i</mi><mo id="S3.E9.m1.1.1.1.1.1.1.2.3.1e" xref="S3.E9.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E9.m1.1.1.1.1.1.1.2.3.8" xref="S3.E9.m1.1.1.1.1.1.1.2.3.8.cmml">c</mi></mrow></msub><mo id="S3.E9.m1.1.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.E9.m1.1.1.1.1.1.1.1.cmml">∗</mo><msub id="S3.E9.m1.1.1.1.1.1.1.3" xref="S3.E9.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E9.m1.1.1.1.1.1.1.3.2" xref="S3.E9.m1.1.1.1.1.1.1.3.2.cmml">F</mi><mrow id="S3.E9.m1.1.1.1.1.1.1.3.3" xref="S3.E9.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E9.m1.1.1.1.1.1.1.3.3.2" xref="S3.E9.m1.1.1.1.1.1.1.3.3.2.cmml">f</mi><mo id="S3.E9.m1.1.1.1.1.1.1.3.3.1" xref="S3.E9.m1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E9.m1.1.1.1.1.1.1.3.3.3" xref="S3.E9.m1.1.1.1.1.1.1.3.3.3.cmml">u</mi><mo id="S3.E9.m1.1.1.1.1.1.1.3.3.1a" xref="S3.E9.m1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E9.m1.1.1.1.1.1.1.3.3.4" xref="S3.E9.m1.1.1.1.1.1.1.3.3.4.cmml">s</mi><mo id="S3.E9.m1.1.1.1.1.1.1.3.3.1b" xref="S3.E9.m1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E9.m1.1.1.1.1.1.1.3.3.5" xref="S3.E9.m1.1.1.1.1.1.1.3.3.5.cmml">e</mi><mo id="S3.E9.m1.1.1.1.1.1.1.3.3.1c" xref="S3.E9.m1.1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E9.m1.1.1.1.1.1.1.3.3.6" xref="S3.E9.m1.1.1.1.1.1.1.3.3.6.cmml">d</mi></mrow></msub></mrow><mo id="S3.E9.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E9.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m1.1b"><apply id="S3.E9.m1.1.1.cmml" xref="S3.E9.m1.1.1"><eq id="S3.E9.m1.1.1.2.cmml" xref="S3.E9.m1.1.1.2"></eq><apply id="S3.E9.m1.1.1.3.cmml" xref="S3.E9.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.3.1.cmml" xref="S3.E9.m1.1.1.3">subscript</csymbol><ci id="S3.E9.m1.1.1.3.2.cmml" xref="S3.E9.m1.1.1.3.2">𝑀</ci><apply id="S3.E9.m1.1.1.3.3.cmml" xref="S3.E9.m1.1.1.3.3"><times id="S3.E9.m1.1.1.3.3.1.cmml" xref="S3.E9.m1.1.1.3.3.1"></times><cn id="S3.E9.m1.1.1.3.3.2.cmml" type="integer" xref="S3.E9.m1.1.1.3.3.2">3</cn><ci id="S3.E9.m1.1.1.3.3.3.cmml" xref="S3.E9.m1.1.1.3.3.3">𝐷</ci></apply></apply><apply id="S3.E9.m1.1.1.1.cmml" xref="S3.E9.m1.1.1.1"><times id="S3.E9.m1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.2"></times><ci id="S3.E9.m1.1.1.1.3.cmml" xref="S3.E9.m1.1.1.1.3">𝜎</ci><apply id="S3.E9.m1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1"><times id="S3.E9.m1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1"></times><apply id="S3.E9.m1.1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E9.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.2">𝐾</ci><apply id="S3.E9.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.3"><times id="S3.E9.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.E9.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.3.2">𝑑</ci><ci id="S3.E9.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.3.3">𝑦</ci><ci id="S3.E9.m1.1.1.1.1.1.1.2.3.4.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.3.4">𝑛</ci><ci id="S3.E9.m1.1.1.1.1.1.1.2.3.5.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.3.5">𝑎</ci><ci id="S3.E9.m1.1.1.1.1.1.1.2.3.6.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.3.6">𝑚</ci><ci id="S3.E9.m1.1.1.1.1.1.1.2.3.7.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.3.7">𝑖</ci><ci id="S3.E9.m1.1.1.1.1.1.1.2.3.8.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.3.8">𝑐</ci></apply></apply><apply id="S3.E9.m1.1.1.1.1.1.1.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E9.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E9.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.3.2">𝐹</ci><apply id="S3.E9.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.3.3"><times id="S3.E9.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.3.3.1"></times><ci id="S3.E9.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.3.3.2">𝑓</ci><ci id="S3.E9.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.3.3.3">𝑢</ci><ci id="S3.E9.m1.1.1.1.1.1.1.3.3.4.cmml" xref="S3.E9.m1.1.1.1.1.1.1.3.3.4">𝑠</ci><ci id="S3.E9.m1.1.1.1.1.1.1.3.3.5.cmml" xref="S3.E9.m1.1.1.1.1.1.1.3.3.5">𝑒</ci><ci id="S3.E9.m1.1.1.1.1.1.1.3.3.6.cmml" xref="S3.E9.m1.1.1.1.1.1.1.3.3.6">𝑑</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.1c">M_{3D}=\sigma(K_{dynamic}*F_{fused})</annotation><annotation encoding="application/x-llamapun" id="S3.E9.m1.1d">italic_M start_POSTSUBSCRIPT 3 italic_D end_POSTSUBSCRIPT = italic_σ ( italic_K start_POSTSUBSCRIPT italic_d italic_y italic_n italic_a italic_m italic_i italic_c end_POSTSUBSCRIPT ∗ italic_F start_POSTSUBSCRIPT italic_f italic_u italic_s italic_e italic_d end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS5.p4.1">where <math alttext="M_{3D}" class="ltx_Math" display="inline" id="S3.SS5.p4.1.m1.1"><semantics id="S3.SS5.p4.1.m1.1a"><msub id="S3.SS5.p4.1.m1.1.1" xref="S3.SS5.p4.1.m1.1.1.cmml"><mi id="S3.SS5.p4.1.m1.1.1.2" xref="S3.SS5.p4.1.m1.1.1.2.cmml">M</mi><mrow id="S3.SS5.p4.1.m1.1.1.3" xref="S3.SS5.p4.1.m1.1.1.3.cmml"><mn id="S3.SS5.p4.1.m1.1.1.3.2" xref="S3.SS5.p4.1.m1.1.1.3.2.cmml">3</mn><mo id="S3.SS5.p4.1.m1.1.1.3.1" xref="S3.SS5.p4.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS5.p4.1.m1.1.1.3.3" xref="S3.SS5.p4.1.m1.1.1.3.3.cmml">D</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.1.m1.1b"><apply id="S3.SS5.p4.1.m1.1.1.cmml" xref="S3.SS5.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p4.1.m1.1.1.1.cmml" xref="S3.SS5.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p4.1.m1.1.1.2.cmml" xref="S3.SS5.p4.1.m1.1.1.2">𝑀</ci><apply id="S3.SS5.p4.1.m1.1.1.3.cmml" xref="S3.SS5.p4.1.m1.1.1.3"><times id="S3.SS5.p4.1.m1.1.1.3.1.cmml" xref="S3.SS5.p4.1.m1.1.1.3.1"></times><cn id="S3.SS5.p4.1.m1.1.1.3.2.cmml" type="integer" xref="S3.SS5.p4.1.m1.1.1.3.2">3</cn><ci id="S3.SS5.p4.1.m1.1.1.3.3.cmml" xref="S3.SS5.p4.1.m1.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.1.m1.1c">M_{3D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p4.1.m1.1d">italic_M start_POSTSUBSCRIPT 3 italic_D end_POSTSUBSCRIPT</annotation></semantics></math> is the final 3D affordance segmentation mask.</p>
</div>
<div class="ltx_para" id="S3.SS5.p5">
<p class="ltx_p" id="S3.SS5.p5.1">Through this comprehensive process, the IRIS system achieves end-to-end mapping from 2D images and text instructions to 3D affordance segmentation. The system’s innovation lies in combining powerful vision-language understanding capabilities, intelligent decision-making mechanisms, and precise 3D affordance segmentation techniques, enabling complex language instructions to be directly transformed into operational areas in 3D space.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="547" id="S3.F4.g1" src="extracted/5866516/images/show.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S3.F4.2.1"> Case-Study of IRIS’s Interactive 3D segmentation.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Dataset</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We present a comprehensive dataset designed for evaluating interactive language-guided affordance segmentation in everyday environments, building upon the work of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib42" title="">42</a>]</cite>. Our dataset encompasses 10 diverse indoor settings commonly encountered in daily life, with each environment represented by 50 high-quality annotated images. These annotations include object bounding boxes, labels, 3D point cloud data, and affordance segmentation information. The query question from humans is made from GPT4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib43" title="">43</a>]</cite>. By integrating 2D image data with 3D point cloud representations and affordance segmentation, this dataset bridges the gap between high-level linguistic instructions and precise 3D affordance understanding. The dataset’s focus on everyday environments enhances its real-world applicability, making it particularly suitable for tasks involving language-guided robotic interactions in common indoor settings.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Dataset Structure and Annotation</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Our dataset is structured around common household and workplace scenarios, each designed to test different aspects of embodied AI:</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Kitchen</span>: Includes objects like faucets, drawers, ovens, and refrigerators.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Living Room</span>: Features furniture, electronics, and decorative items.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">Bedroom</span>: Contains sleep-related furniture and personal items.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i4.p1.1.1">Office</span>: Focuses on work-related equipment and furniture.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i5.p1.1.1">Entrance/Hallway</span>: Highlights transitional spaces and storage.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S4.I1.i6.p1">
<p class="ltx_p" id="S4.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i6.p1.1.1">Study Area</span>: Emphasizes learning environments and materials.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="S4.I1.i7.p1">
<p class="ltx_p" id="S4.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i7.p1.1.1">Leisure Space</span>: Includes entertainment and relaxation items.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span>
<div class="ltx_para" id="S4.I1.i8.p1">
<p class="ltx_p" id="S4.I1.i8.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i8.p1.1.1">Cleaning Area</span>: Features cleaning tools and appliances.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span>
<div class="ltx_para" id="S4.I1.i9.p1">
<p class="ltx_p" id="S4.I1.i9.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i9.p1.1.1">Storage and Organization</span>: Focuses on storage solutions and personal belongings.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span>
<div class="ltx_para" id="S4.I1.i10.p1">
<p class="ltx_p" id="S4.I1.i10.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i10.p1.1.1">Dining Area</span>: Includes dining furniture and tableware.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Each image in the dataset is meticulously annotated with: precise bounding boxes and its label for all relevant objects; detailed affordance segmentation masks for manipulable objects; object-action correspondences listing possible actions for each object; and natural language descriptions of possible tasks and interactions.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Affordance Segmentation for Embodied Agents</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">A key feature of our dataset is the inclusion of detailed affordance segmentation annotations. These annotations provide pixel-level information about how objects can be interacted with, crucial for embodied agents to understand and manipulate their environment. For example: <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">Faucet</span> is annotated with actions such as “Open”, “Turn off”, and “Pull”; <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.2">Drawer</span> is labeled with “Open” and “Push” affordances; <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.3">Oven</span> is segmented with “Contain” and “Open” functionalities; and <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.4">Refrigerator</span> is marked with “Open” and “Push” interactions.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">This granular level of annotation allows for testing an agent’s ability to not just recognize objects, but to understand how to interact with them in context-appropriate ways.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Experiments</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">To evaluate the effectiveness of our proposed IRIS system for 3D affordance analysis, we conducted extensive experiments comparing it with state-of-the-art methods and analyzing its performance across various indoor environments. This section details our experimental setup, quantitative results, qualitative analysis, and discussion of findings.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.4.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">Experimental Setup</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We evaluated IRIS on a diverse dataset of indoor environments, encompassing ten different room types commonly found in residential settings on one A100 GPU. The dataset includes a wide range of objects with various affordances to test the system’s capability in 3D affordance analysis and segmentation. Our experiments were designed to assess both the quantitative performance metrics and qualitative aspects of the system’s understanding and interaction capabilities.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.4.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.5.2">Performance Metrics</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We used four standard metrics to evaluate quantitative performance:</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">mIoU (mean Intersection over Union)</span>: Measures 3D segmentation accuracy.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">AUC (Area Under the Curve)</span>: Evaluates overall performance across different affordance detection thresholds.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">SIM (Similarity)</span>: Assesses similarity between predicted and ground truth 3D affordance segmentations.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i4.p1.1.1">MAE (Mean Absolute Error)</span>: Measures average magnitude of affordance prediction errors in 3D space.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.4.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.5.2">Quantitative Results</span>
</h3>
<section class="ltx_subsubsection" id="S5.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS3.SSS1.4.1.1">V-C</span>1 </span>Comparative Analysis</h4>
<div class="ltx_para" id="S5.SS3.SSS1.p1">
<p class="ltx_p" id="S5.SS3.SSS1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S5.T1" title="TABLE I ‣ V-C1 Comparative Analysis ‣ V-C Quantitative Results ‣ V Experiments ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_tag">I</span></a> presents a comparison of IRIS with other state-of-the-art methods in 3D affordance analysis and segmentation.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Performance Comparison of Different Methods</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.4.4">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T1.4.4.5" style="padding-left:1.0pt;padding-right:1.0pt;">Method</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">mIoU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.m1.1.1" stretchy="false" xref="S5.T1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.2.2.2" style="padding-left:1.0pt;padding-right:1.0pt;">AUC<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.2.2.2.m1.1"><semantics id="S5.T1.2.2.2.m1.1a"><mo id="S5.T1.2.2.2.m1.1.1" stretchy="false" xref="S5.T1.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.m1.1b"><ci id="S5.T1.2.2.2.m1.1.1.cmml" xref="S5.T1.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.3.3.3" style="padding-left:1.0pt;padding-right:1.0pt;">SIM<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.3.3.3.m1.1"><semantics id="S5.T1.3.3.3.m1.1a"><mo id="S5.T1.3.3.3.m1.1.1" stretchy="false" xref="S5.T1.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.m1.1b"><ci id="S5.T1.3.3.3.m1.1.1.cmml" xref="S5.T1.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.4.4" style="padding-left:1.0pt;padding-right:1.0pt;">MAE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.4.4.4.m1.1"><semantics id="S5.T1.4.4.4.m1.1a"><mo id="S5.T1.4.4.4.m1.1.1" stretchy="false" xref="S5.T1.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.4.m1.1b"><ci id="S5.T1.4.4.4.m1.1.1.cmml" xref="S5.T1.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.4.4.m1.1d">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.4.5.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.4.5.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">GLIP<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib44" title="">44</a>]</cite> + ReferTrans <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib45" title="">45</a>]</cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.4.5.1.2" style="padding-left:1.0pt;padding-right:1.0pt;">11.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.4.5.1.3" style="padding-left:1.0pt;padding-right:1.0pt;">75.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.4.5.1.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.414</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.4.5.1.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.135</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.6.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T1.4.6.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">GDINO<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib46" title="">46</a>]</cite> + ReferTrans <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib45" title="">45</a>]</cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.6.2.2" style="padding-left:1.0pt;padding-right:1.0pt;">12.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.6.2.3" style="padding-left:1.0pt;padding-right:1.0pt;">77.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.6.2.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.425</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.6.2.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.131</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.7.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T1.4.7.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">Next-chat<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib24" title="">24</a>]</cite> + ReferTrans <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib45" title="">45</a>]</cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.7.3.2" style="padding-left:1.0pt;padding-right:1.0pt;">13.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.7.3.3" style="padding-left:1.0pt;padding-right:1.0pt;">78.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.7.3.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.457</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.7.3.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.128</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.8.4">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T1.4.8.4.1" style="padding-left:1.0pt;padding-right:1.0pt;">GLIP<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib44" title="">44</a>]</cite> + ReLa<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib47" title="">47</a>]</cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.8.4.2" style="padding-left:1.0pt;padding-right:1.0pt;">14.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.8.4.3" style="padding-left:1.0pt;padding-right:1.0pt;">73.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.8.4.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.502</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.8.4.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.123</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.9.5">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T1.4.9.5.1" style="padding-left:1.0pt;padding-right:1.0pt;">GDINO<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib46" title="">46</a>]</cite> + ReLa<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib47" title="">47</a>]</cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.9.5.2" style="padding-left:1.0pt;padding-right:1.0pt;">15.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.9.5.3" style="padding-left:1.0pt;padding-right:1.0pt;">75.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.9.5.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.515</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.9.5.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.121</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.10.6">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T1.4.10.6.1" style="padding-left:1.0pt;padding-right:1.0pt;">Next-chat<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib24" title="">24</a>]</cite> + ReLa<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib47" title="">47</a>]</cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.10.6.2" style="padding-left:1.0pt;padding-right:1.0pt;">15.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.10.6.3" style="padding-left:1.0pt;padding-right:1.0pt;">76.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.10.6.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.525</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.10.6.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.118</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.11.7">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T1.4.11.7.1" style="padding-left:1.0pt;padding-right:1.0pt;">GLIP<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib44" title="">44</a>]</cite> + IAGNet<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib41" title="">41</a>]</cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.11.7.2" style="padding-left:1.0pt;padding-right:1.0pt;">16.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.11.7.3" style="padding-left:1.0pt;padding-right:1.0pt;">77.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.11.7.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.533</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.11.7.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.116</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.12.8">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T1.4.12.8.1" style="padding-left:1.0pt;padding-right:1.0pt;">GDINO<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib46" title="">46</a>]</cite> + IAGNet<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib41" title="">41</a>]</cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.12.8.2" style="padding-left:1.0pt;padding-right:1.0pt;">16.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.12.8.3" style="padding-left:1.0pt;padding-right:1.0pt;">79.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.12.8.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.537</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.12.8.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.114</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.13.9">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T1.4.13.9.1" style="padding-left:1.0pt;padding-right:1.0pt;">Next-chat<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib24" title="">24</a>]</cite> + IAGNet<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib41" title="">41</a>]</cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.13.9.2" style="padding-left:1.0pt;padding-right:1.0pt;">17.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.13.9.3" style="padding-left:1.0pt;padding-right:1.0pt;">80.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.13.9.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.544</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.13.9.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.113</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.14.10">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T1.4.14.10.1" style="padding-left:1.0pt;padding-right:1.0pt;">GLIP<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib44" title="">44</a>]</cite> + PointRefer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib25" title="">25</a>]</cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.14.10.2" style="padding-left:1.0pt;padding-right:1.0pt;">17.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.14.10.3" style="padding-left:1.0pt;padding-right:1.0pt;">81.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.14.10.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.551</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.14.10.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.112</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.15.11">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T1.4.15.11.1" style="padding-left:1.0pt;padding-right:1.0pt;">GDINO<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib46" title="">46</a>]</cite> + PointRefer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#bib.bib25" title="">25</a>]</cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.15.11.2" style="padding-left:1.0pt;padding-right:1.0pt;">18.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.15.11.3" style="padding-left:1.0pt;padding-right:1.0pt;">82.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.15.11.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.575</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.4.15.11.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.105</td>
</tr>
<tr class="ltx_tr" id="S5.T1.4.16.12">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S5.T1.4.16.12.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.4.16.12.1.1">IRIS (Ours)</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t" id="S5.T1.4.16.12.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.4.16.12.2.1">19.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t" id="S5.T1.4.16.12.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.4.16.12.3.1">83.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t" id="S5.T1.4.16.12.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.4.16.12.4.1">0.603</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t" id="S5.T1.4.16.12.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.4.16.12.5.1">0.098</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS3.SSS1.p2">
<p class="ltx_p" id="S5.SS3.SSS1.p2.1">IRIS outperforms all baseline methods across all metrics, with notable improvements: 19.2% in mIoU over the next best method with 1% higher. Highest AUC score of 83.1, indicating superior overall performance. The best SIM score was 0.603, demonstrating accurate 3D affordance segmentations. The lowest MAE of 0.098, highlighting precision in affordance localization.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS3.SSS2.4.1.1">V-C</span>2 </span>Performance Across Different environments</h4>
<div class="ltx_para" id="S5.SS3.SSS2.p1">
<p class="ltx_p" id="S5.SS3.SSS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S5.T2" title="TABLE II ‣ V-D Qualitative Results ‣ V Experiments ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_tag">II</span></a> presents IRIS’s performance across ten different indoor environments.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS2.p2">
<p class="ltx_p" id="S5.SS3.SSS2.p2.1">Observations from the environment-specific performance data reveal consistent performance across all environments, with the mIoU range spanning from 18.3 to 19.9. IRIS demonstrates its best performance in structured environments, achieving an mIoU of 19.9 in kitchen environments and 19.7 in dining areas. Notably, the system maintains robust performance even in challenging environments, as evidenced by the mIoU of 18.3 in entrance/hallway environments, which often present complex spatial arrangements and diverse object types. Other metrics also show this trend.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS4.4.1.1">V-D</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS4.5.2">Qualitative Results</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">To complement our quantitative results, we conducted a qualitative analysis of IRIS’s performance across various scenarios, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.10078v3#S3.F4" title="Figure 4 ‣ III-E Language-Guided 3D Affordance Segmentation ‣ III Methodology ‣ IRIS : Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis"><span class="ltx_text ltx_ref_tag">4</span></a>. IRIS demonstrates strong performance in diverse environments, accurately identifying and segmenting objects such as a sofa in a living room environment, a blue bag among multiple bags, a vase in an entrance area, and a bed in a bedroom environment. The system shows a good grasp of object affordances, associating “sit on” with the sofa, recognizing the “grasp” affordance for the blue bag, understanding the “pour water” action for the vase, and correctly interpreting the “lay down” affordance for the bed. IRIS generates accurate 3D point cloud representations of segmented objects, capturing their shape and structure, which is crucial for potential applications in robotics and augmented reality. The qualitative results also reveal IRIS’s interactive function. It correctly identifies its inability to perform physical actions (e.g., giving an apple, taking a table). It demonstrates an understanding of its role as an analysis and segmentation system, not a physical actor.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>IRIS Performance Across Different environments</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.4.4">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T2.4.4.5" style="padding-left:1.0pt;padding-right:1.0pt;">Environment</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">mIoU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.m1.1.1" stretchy="false" xref="S5.T2.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.2.2.2" style="padding-left:1.0pt;padding-right:1.0pt;">AUC<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.2.2.2.m1.1"><semantics id="S5.T2.2.2.2.m1.1a"><mo id="S5.T2.2.2.2.m1.1.1" stretchy="false" xref="S5.T2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.m1.1b"><ci id="S5.T2.2.2.2.m1.1.1.cmml" xref="S5.T2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.3.3" style="padding-left:1.0pt;padding-right:1.0pt;">SIM<math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T2.3.3.3.m1.1"><semantics id="S5.T2.3.3.3.m1.1a"><mo id="S5.T2.3.3.3.m1.1.1" stretchy="false" xref="S5.T2.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.m1.1b"><ci id="S5.T2.3.3.3.m1.1.1.cmml" xref="S5.T2.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4" style="padding-left:1.0pt;padding-right:1.0pt;">MAE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T2.4.4.4.m1.1"><semantics id="S5.T2.4.4.4.m1.1a"><mo id="S5.T2.4.4.4.m1.1.1" stretchy="false" xref="S5.T2.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.m1.1b"><ci id="S5.T2.4.4.4.m1.1.1.cmml" xref="S5.T2.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.4.m1.1d">↓</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.4.5.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.4.5.1.1" style="padding-left:1.0pt;padding-right:1.0pt;">Kitchen</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T2.4.5.1.2" style="padding-left:1.0pt;padding-right:1.0pt;">19.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T2.4.5.1.3" style="padding-left:1.0pt;padding-right:1.0pt;">84.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T2.4.5.1.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.613</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T2.4.5.1.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.095</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.6.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.6.2.1" style="padding-left:1.0pt;padding-right:1.0pt;">Living Room</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.6.2.2" style="padding-left:1.0pt;padding-right:1.0pt;">19.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.6.2.3" style="padding-left:1.0pt;padding-right:1.0pt;">83.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.6.2.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.606</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.6.2.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.097</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.7.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.7.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">Bedroom</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.7.3.2" style="padding-left:1.0pt;padding-right:1.0pt;">18.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.7.3.3" style="padding-left:1.0pt;padding-right:1.0pt;">82.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.7.3.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.599</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.7.3.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.099</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.8.4">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.8.4.1" style="padding-left:1.0pt;padding-right:1.0pt;">Office</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.8.4.2" style="padding-left:1.0pt;padding-right:1.0pt;">19.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.8.4.3" style="padding-left:1.0pt;padding-right:1.0pt;">83.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.8.4.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.610</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.8.4.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.096</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.9.5">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.9.5.1" style="padding-left:1.0pt;padding-right:1.0pt;">Entrance/Hallway</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.9.5.2" style="padding-left:1.0pt;padding-right:1.0pt;">18.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.9.5.3" style="padding-left:1.0pt;padding-right:1.0pt;">82.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.9.5.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.592</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.9.5.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.102</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.10.6">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.10.6.1" style="padding-left:1.0pt;padding-right:1.0pt;">Study Area</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.10.6.2" style="padding-left:1.0pt;padding-right:1.0pt;">19.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.10.6.3" style="padding-left:1.0pt;padding-right:1.0pt;">83.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.10.6.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.604</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.10.6.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.098</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.11.7">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.11.7.1" style="padding-left:1.0pt;padding-right:1.0pt;">Leisure Space</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.11.7.2" style="padding-left:1.0pt;padding-right:1.0pt;">19.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.11.7.3" style="padding-left:1.0pt;padding-right:1.0pt;">82.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.11.7.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.601</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.11.7.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.100</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.12.8">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.12.8.1" style="padding-left:1.0pt;padding-right:1.0pt;">Cleaning Area</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.12.8.2" style="padding-left:1.0pt;padding-right:1.0pt;">18.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.12.8.3" style="padding-left:1.0pt;padding-right:1.0pt;">82.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.12.8.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.595</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.12.8.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.101</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.13.9">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.13.9.1" style="padding-left:1.0pt;padding-right:1.0pt;">Storage and Organization</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.13.9.2" style="padding-left:1.0pt;padding-right:1.0pt;">18.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.13.9.3" style="padding-left:1.0pt;padding-right:1.0pt;">82.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.13.9.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.597</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.13.9.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.100</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.14.10">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.14.10.1" style="padding-left:1.0pt;padding-right:1.0pt;">Dining Area</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.14.10.2" style="padding-left:1.0pt;padding-right:1.0pt;">19.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.14.10.3" style="padding-left:1.0pt;padding-right:1.0pt;">83.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.14.10.4" style="padding-left:1.0pt;padding-right:1.0pt;">0.612</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T2.4.14.10.5" style="padding-left:1.0pt;padding-right:1.0pt;">0.095</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.15.11">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S5.T2.4.15.11.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.4.15.11.1.1">Average</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t" id="S5.T2.4.15.11.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.4.15.11.2.1">19.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t" id="S5.T2.4.15.11.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.4.15.11.3.1">83.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t" id="S5.T2.4.15.11.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.4.15.11.4.1">0.603</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t" id="S5.T2.4.15.11.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.4.15.11.5.1">0.098</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS5.4.1.1">V-E</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS5.5.2">Discussion</span>
</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">The experimental results demonstrate IRIS’s superior performance in 3D affordance analysis and segmentation tasks. The system exhibits environment versatility, maintaining high performance across various indoor environments, from structured environments like kitchens to more challenging areas like hallways. Future work could explore IRIS’s performance in even more challenging scenarios, investigate potential enhancements for handling dynamic environments, and explore integration with robotic systems for physical interaction based on the affordance analysis provided by IRIS.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This paper presents two main contributions to 3D affordance analysis and segmentation. First, we introduce IRIS, a novel training-free multimodal system that bridges high-level instructions and precise robotic actions in 3D environments. IRIS integrates 2D and 3D visual understanding with language comprehension, demonstrating competitive performance across diverse indoor settings. Second, we provide a new dataset for evaluating interactive language-guided affordance segmentation in everyday environments, enabling comprehensive testing of multimodal systems for complex spatial understanding tasks. By combining sophisticated visual-language processing with precise 3D affordance segmentation, these contributions advance embodied AI applications and enhance human-robot interactions in real-world scenarios, paving the way for more capable and responsive robotic systems</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">et al.</em>, “Openvla: An open-source vision-language-action model,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.2.2">arXiv preprint arXiv:2406.09246</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
C. Wei and Z. Deng, “Incorporating scene graphs into pre-trained vision-language models for multimodal open-vocabulary action recognition,” in <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">2024 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2024, pp. 440–447.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Y. Hong, M. J. Kim, I. Lee, and S. B. Yoo, “Fluxformer: Flow-guided duplex attention transformer via spatio-temporal clustering for action recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE Robotics and Automation Letters</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Chen, R. G. Pinel, C. Schmid, and I. Laptev, “Polarnet: 3d point clouds for language-guided robotic manipulation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Conference on Robot Learning</em>.   PMLR, 2023, pp. 1761–1781.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
G. Tziafas, Y. Xu, A. Goel, M. Kasaei, Z. Li, and H. Kasaei, “Language-guided robot grasping: Clip-based referring grasp synthesis in clutter,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2311.05779</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">et al.</em>, “Do as i can, not as i say: Grounding language in robotic affordances,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.2.2">arXiv preprint arXiv:2204.01691</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
H. Ha, P. Florence, and S. Song, “Scaling up and distilling down: Language-guided robot skill acquisition,” in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Conference on Robot Learning</em>.   PMLR, 2023, pp. 3766–3777.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
R. Zhang, S. Lee, M. Hwang, A. Hiranaka, C. Wang, W. Ai, J. J. R. Tan, S. Gupta, Y. Hao, G. Levine, <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">et al.</em>, “Noir: Neural signal operated intelligent robots for everyday activities,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2">arXiv preprint arXiv:2311.01454</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
W. Xia, D. Wang, X. Pang, Z. Wang, B. Zhao, D. Hu, and X. Li, “Kinematic-aware prompting for generalizable articulated object manipulation with llms,” in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">2024 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2024, pp. 2073–2080.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Z. Zhou, J. Song, K. Yao, Z. Shu, and L. Ma, “Isr-llm: Iterative self-refined large language model for long-horizon sequential task planning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">2024 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2024, pp. 2081–2088.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
B. Xing, X. Ying, and R. Wang, “Masked local-global representation learning for 3d point cloud domain adaptation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">2024 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2024, pp. 418–424.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
K. Vidanapathirana, P. Moghadam, S. Sridharan, and C. Fookes, “Spectral geometric verification: Re-ranking point cloud retrieval for metric localization,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">IEEE Robotics and Automation Letters</em>, vol. 8, no. 5, pp. 2494–2501, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C. Liu, G. Chen, and R. Song, “Lps-net: Lightweight parameter-shared network for point cloud-based place recognition,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">2024 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2024, pp. 448–454.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
B. Wang, W. Li, B. Zhang, and Y. Liu, “Joint response and background learning for uav visual tracking,” in <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">2024 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2024, pp. 455–462.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
P. Ausserlechner, D. Haberger, S. Thalhammer, J.-B. Weibel, and M. Vincze, “Zs6d: Zero-shot 6d object pose estimation using vision transformers,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">2024 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2024, pp. 463–469.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
M. Chu, Z. Cui, A. Zhang, J. Yao, C. Tang, Z. Fu, A. Nathan, and S. Gao, “Multisensory fusion, haptic, and visual feedback teleoperation system under iot framework,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">IEEE Internet of Things Journal</em>, vol. 9, no. 20, pp. 19 717–19 727, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
D. Shah, B. Osiński, b. ichter, and S. Levine, “Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action,” in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of The 6th Conference on Robot Learning</em>, ser. Proceedings of Machine Learning Research, K. Liu, D. Kulic, and J. Ichnowski, Eds., vol. 205.   PMLR, 14–18 Dec 2023, pp. 492–504.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
D. Shah, M. R. Equi, B. Osiński, F. Xia, B. Ichter, and S. Levine, “Navigation with large language models: Semantic guesswork as a heuristic for planning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Conference on Robot Learning</em>.   PMLR, 2023, pp. 2683–2699.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
X. Long, H. Zhao, C. Chen, F. Gu, and Q. Gu, “A novel wide-area multiobject detection system with high-probability region searching,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2405.04589</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S. Lu, H. Chang, E. P. Jing, A. Boularias, and K. Bekris, “Ovir-3d: Open-vocabulary 3d instance retrieval without training on 3d data,” in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Conference on Robot Learning</em>.   PMLR, 2023, pp. 1610–1620.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid, <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">et al.</em>, “Rt-2: Vision-language-action models transfer web knowledge to robotic control,” in <em class="ltx_emph ltx_font_italic" id="bib.bib21.2.2">Conference on Robot Learning</em>.   PMLR, 2023, pp. 2165–2183.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
S. Saxena, M. Sharma, and O. Kroemer, “Multi-resolution sensing for real-time control with vision-language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Conference on Robot Learning</em>.   PMLR, 2023, pp. 2210–2228.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M. Chu, Z. Zheng, W. Ji, T. Wang, and T.-S. Chua, “Towards natural language-guided drones: Geotext-1652 benchmark with spatial relation matching,” in <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the European Conference on Computer Vision (ECCV)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. Zhang, Y. Yao, W. Ji, Z. Liu, and T.-S. Chua, “Next-chat: An lmm for chat, detection and segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Forty-first International Conference on Machine Learning</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y. Li, N. Zhao, J. Xiao, C. Feng, X. Wang, and T.-s. Chua, “Laso: Language-guided affordance segmentation on 3d object,” in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2024, pp. 14 251–14 260.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, B. Patra, <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">et al.</em>, “Language is not all you need: Aligning perception with language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.2.2">Advances in Neural Information Processing Systems</em>, vol. 36, pp. 72 096–72 109, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">International conference on machine learning</em>.   PMLR, 2023, pp. 19 730–19 742.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
S. Zhang, P. Sun, S. Chen, M. Xiao, W. Shao, W. Zhang, Y. Liu, K. Chen, and P. Luo, “Gpt4roi: Instruction tuning large language model on region-of-interest,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2307.03601</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao, “Shikra: Unleashing multimodal llm’s referential dialogue magic,” <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2306.15195</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
D. Z. Chen, A. X. Chang, and M. Nießner, “Scanrefer: 3d object localization in rgb-d scans using natural language,” in <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">European Conference on Computer Vision</em>.   Springer, 2020, pp. 202–221.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
D. Azuma, T. Miyanishi, S. Kurita, and M. Kawanabe, “Scanqa: 3d question answering for spatial scene understanding,” in <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2022, pp. 19 129–19 139.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. Van Den Hengel, “Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments,” in <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2018, pp. 3674–3683.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Y. Hong, Q. Wu, Y. Qi, C. Rodriguez-Opazo, and S. Gould, “A recurrent vision-and-language bert for navigation,” <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2011.13922</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
T. B. Brown, “Language models are few-shot learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2005.14165</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">et al.</em>, “Llama: Open and efficient foundation language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib35.2.2">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">et al.</em>, “Mistral 7b,” <em class="ltx_emph ltx_font_italic" id="bib.bib36.2.2">arXiv preprint arXiv:2310.06825</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">et al.</em>, “Inner monologue: Embodied reasoning through planning with language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib37.2.2">arXiv preprint arXiv:2207.05608</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg, “Progprompt: Generating situated robot task plans using large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">2023 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2023, pp. 11 523–11 530.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
J. Chen, B. Lin, R. Xu, Z. Chai, X. Liang, and K.-Y. K. Wong, “Mapgpt: Map-guided prompting for unified vision-and-language navigation,” <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2401.07314</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
T.-T. Do, A. Nguyen, and I. Reid, “Affordancenet: An end-to-end deep learning approach for object affordance detection,” in <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">2018 IEEE international conference on robotics and automation (ICRA)</em>.   IEEE, 2018, pp. 5882–5889.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Y. Yang, W. Zhai, H. Luo, Y. Cao, J. Luo, and Z.-J. Zha, “Grounding 3d object affordance from 2d interactions in images,” in <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2023, pp. 10 905–10 915.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
S. Deng, X. Xu, C. Wu, K. Chen, and K. Jia, “3d affordancenet: A benchmark for visual object affordance understanding,” in <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2021, pp. 1778–1787.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">et al.</em>, “Gpt-4 technical report,” <em class="ltx_emph ltx_font_italic" id="bib.bib43.2.2">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan, L. Zhang, J.-N. Hwang, K.-W. Chang, and J. Gao, “Grounded language-image pre-training,” in <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2022, pp. 10 965–10 975.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
M. Li and L. Sigal, “Referring transformer: A one-step approach to multi-task visual grounding,” <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Advances in neural information processing systems</em>, vol. 34, pp. 19 652–19 664, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su, J. Zhu, <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">et al.</em>, “Grounding dino: Marrying dino with grounded pre-training for open-set object detection,” <em class="ltx_emph ltx_font_italic" id="bib.bib46.2.2">arXiv preprint arXiv:2303.05499</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
C. Liu, H. Ding, and X. Jiang, “Gres: Generalized referring expression segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2023, pp. 23 592–23 601.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 19 16:56:50 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
