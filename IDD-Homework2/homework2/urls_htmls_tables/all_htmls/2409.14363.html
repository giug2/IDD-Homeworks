<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>MANTA - Model Adapter Native generations thatâ€™s Affordable</title>
<!--Generated on Sun Sep 22 08:36:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.14363v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1" title="In MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.SS1" title="In 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Background</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.SS2" title="In 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Core Research Problem</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.SS3" title="In 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>Past Work</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.SS3.SSS1" title="In 1.3 Past Work â€£ 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3.1 </span>Model Based Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.SS3.SSS2" title="In 1.3 Past Work â€£ 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3.2 </span>Retrieval Based Methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.SS4" title="In 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4 </span>Retrieval Methods Limitations and Opportunities</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.SS4.SSS1" title="In 1.4 Retrieval Methods Limitations and Opportunities â€£ 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.1 </span>Lack of Task Diversity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.SS4.SSS2" title="In 1.4 Retrieval Methods Limitations and Opportunities â€£ 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4.2 </span>Low Alignment</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.SS5" title="In 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.5 </span>Current Image Generation Workflow Challenges</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.SS5.SSS1" title="In 1.5 Current Image Generation Workflow Challenges â€£ 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.5.1 </span>Image Resolution</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.SS5.SSS2" title="In 1.5 Current Image Generation Workflow Challenges â€£ 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.5.2 </span>Alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.SS5.SSS3" title="In 1.5 Current Image Generation Workflow Challenges â€£ 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.5.3 </span>Image Diversity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.SS5.SSS4" title="In 1.5 Current Image Generation Workflow Challenges â€£ 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.5.4 </span>Consumer Friendliness</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S2" title="In MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S2.SS1" title="In 2 Related Works â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Adapters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S2.SS2" title="In 2 Related Works â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S2.SS3" title="In 2 Related Works â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>MANTA Overview</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S3" title="In MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Our Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S3.SS1" title="In 3 Our Method â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Structured Concept Development</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S3.SS2" title="In 3 Our Method â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Benefits of Structured Concept Development</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S3.SS3" title="In 3 Our Method â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Detail Enhancement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S3.SS4" title="In 3 Our Method â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Checkpoint / Adapter Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S3.SS4.SSS1" title="In 3.4 Checkpoint / Adapter Retrieval â€£ 3 Our Method â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Checkpoint Document Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S3.SS4.SSS2" title="In 3.4 Checkpoint / Adapter Retrieval â€£ 3 Our Method â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Checkpoint Document Retrieval</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4" title="In MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4.SS1" title="In 4 Results â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4.SS2" title="In 4 Results â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Experimental Procedures</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4.SS2.SSS1" title="In 4.2 Experimental Procedures â€£ 4 Results â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Automated Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4.SS2.SSS2" title="In 4.2 Experimental Procedures â€£ 4 Results â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Human Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4.SS2.SSS3" title="In 4.2 Experimental Procedures â€£ 4 Results â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Token Count Comparison</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4.SS3" title="In 4 Results â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Ablation Studies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4.SS3.SSS1" title="In 4.3 Ablation Studies â€£ 4 Results â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Concept Enhancement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4.SS3.SSS2" title="In 4.3 Ablation Studies â€£ 4 Results â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Base Checkpoint Variation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4.SS3.SSS3" title="In 4.3 Ablation Studies â€£ 4 Results â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Configuration Scale can systematically improves Diversity</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S5" title="In MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S6" title="In MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S6.SS1" title="In 6 Discussion â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Consumer Optimizations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S6.SS2" title="In 6 Discussion â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Next Steps</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S6.SS2.SSS1" title="In 6.2 Next Steps â€£ 6 Discussion â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span>Performance Improvements</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S6.SS2.SSS2" title="In 6.2 Next Steps â€£ 6 Discussion â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span>Future Development</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S6.SS3" title="In 6 Discussion â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Reproducibility</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S6.SS4" title="In 6 Discussion â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Code</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#A1" title="In MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#A1.SS1" title="In Appendix A Appendix â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Use cases</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#A1.SS2" title="In Appendix A Appendix â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Failure Modes</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#A1.SS2.SSS1" title="In A.2 Failure Modes â€£ Appendix A Appendix â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.1 </span>Concept Overload.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#A1.SS2.SSS2" title="In A.2 Failure Modes â€£ Appendix A Appendix â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.2 </span>Concept Relationship Misunderstanding.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#A1.SS2.SSS3" title="In A.2 Failure Modes â€£ Appendix A Appendix â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.3 </span>Adapter Gating.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#A1.SS2.SSS4" title="In A.2 Failure Modes â€£ Appendix A Appendix â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.4 </span>Rogue adapter / checkpoints.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#A1.SS3" title="In Appendix A Appendix â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Prompts</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#A1.SS3.SSS1" title="In A.3 Prompts â€£ Appendix A Appendix â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3.1 </span>Detail Enhancement</span></a></li>
</ol>
</li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">MANTA - Model Adapter Native generations thatâ€™s Affordable</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Ansh Chaurasia 
<br class="ltx_break"/>University of California, Berkeley 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="4.1.1">achaurasia@berkeley.edu</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.2">The presiding model generation algorithms rely on simple, inflexible adapter selection to provide personalized results. We propose the <span class="ltx_text ltx_font_italic" id="id2.2.1">model-adapter</span> composition problem as a generalized problem to past work factoring in practical hardware and affordability constraints, and introduce MANTA as a new approach to the problem. Experiments on COCO 2014 validation show MANTA to be superior in image task diversity and quality at the cost of a modest drop in alignment. Our system achieves a <math alttext="94\%" class="ltx_Math" display="inline" id="1.m1.1"><semantics id="1.m1.1a"><mrow id="1.m1.1.1" xref="1.m1.1.1.cmml"><mn id="1.m1.1.1.2" xref="1.m1.1.1.2.cmml">94</mn><mo id="1.m1.1.1.1" xref="1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="1.m1.1b"><apply id="1.m1.1.1.cmml" xref="1.m1.1.1"><csymbol cd="latexml" id="1.m1.1.1.1.cmml" xref="1.m1.1.1.1">percent</csymbol><cn id="1.m1.1.1.2.cmml" type="integer" xref="1.m1.1.1.2">94</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="1.m1.1c">94\%</annotation><annotation encoding="application/x-llamapun" id="1.m1.1d">94 %</annotation></semantics></math> win rate in task diversity and a <math alttext="80\%" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mrow id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mn id="id2.2.m2.1.1.2" xref="id2.2.m2.1.1.2.cmml">80</mn><mo id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><csymbol cd="latexml" id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1">percent</csymbol><cn id="id2.2.m2.1.1.2.cmml" type="integer" xref="id2.2.m2.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">80\%</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">80 %</annotation></semantics></math> task quality win rate versus the best known system, and demonstrates strong potential for direct use in synthetic data generation and the creative art domains .</p>
</div>
<figure class="ltx_figure" id="id3"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="240" id="id3.g1" src="extracted/5871043/cover.png" width="299"/>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Background</h3>
<div class="ltx_para ltx_noindent" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Since the recent popularization of diffusion models by Ho et al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib1" title="">1</a>]</cite>, significant work has been undertaken into creating AI models that can be easily harnessed by users in generating images for a specific, custom use case <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib2" title="">2</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib3" title="">3</a>]</cite>. Stable Diffusion provided the first contemporary example of a <span class="ltx_text ltx_font_italic" id="S1.SS1.p1.1.1">text-to-image</span> latent diffusion model, i.e, a diffusion model that could be conditioned using embeddings from a text encoder, operate with memory-efficiently using a compressed latent space, and provide extremely high resolution and quality images. With the popularization of Stable Diffusion, users gained the capability of being able to finetune a base checkpoint Stable Diffusion model for additional customizability.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">However, the pretrain-then-finetune approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib4" title="">4</a>]</cite> was still computationally expensive, and broadly infeasible to the general public. The creation of parameter efficient finetuning methods such as Low Rank Adapation (LoRA) and Parameter Efficient Finetuning (PEFT) attempted to tackle this shortcoming, popularizing the <span class="ltx_text ltx_font_italic" id="S1.SS1.p2.1.1">adapter</span> paradigm, where one could more easily create an adapter co-existing as an addendum to the model that would provide the necessary customization at a fraction of the computation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.p3">
<p class="ltx_p" id="S1.SS1.p3.1">The current state of the art comprises of pairing image generation model checkpoints with additional adapters such as LoRAs to construct an image generation workflow. However, finding an appropriate combination of models and checkpoints continues to remain an open challenge, particularly useful for synthetic data generation, where additional data can be used to augment the data distribution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib5" title="">5</a>]</cite>, or creative AI art <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.p4">
<p class="ltx_p" id="S1.SS1.p4.1">Checkpoint and adapter selection is predominantly done manually, leading to very little exploration in finding model-adapter combinations that would address a custom workflow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib7" title="">7</a>]</cite>. Oftentimes, users may attempt to pick from a series of existing popular models, experiment with the model to understand itâ€™s capability for an image concept, and then find adapters to enhance the quality of the generated images.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS1.p5">
<p class="ltx_p" id="S1.SS1.p5.1">With the advent of large-language models, the retrieval augmented generation (RAG) paradigm has become extremely common for systems attempting to find the most relevant content related to some given input. This typically consists of a large language model, some input source documents, and a query. Given the query, the retrieval augmented generation system searches for the most relevant documents from the sources, appends it to its response, and then attempts to answer the question.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Core Research Problem</h3>
<div class="ltx_para ltx_noindent" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.6">Previous retrieval based systems (Stylus) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib8" title="">8</a>]</cite> defined their system as solving the <span class="ltx_text ltx_font_italic" id="S1.SS2.p1.6.1">adapter composition problem</span>. Given a prompt <math alttext="P" class="ltx_Math" display="inline" id="S1.SS2.p1.1.m1.1"><semantics id="S1.SS2.p1.1.m1.1a"><mi id="S1.SS2.p1.1.m1.1.1" xref="S1.SS2.p1.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S1.SS2.p1.1.m1.1b"><ci id="S1.SS2.p1.1.m1.1.1.cmml" xref="S1.SS2.p1.1.m1.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS2.p1.1.m1.1c">P</annotation><annotation encoding="application/x-llamapun" id="S1.SS2.p1.1.m1.1d">italic_P</annotation></semantics></math> and a fixed model <math alttext="C" class="ltx_Math" display="inline" id="S1.SS2.p1.2.m2.1"><semantics id="S1.SS2.p1.2.m2.1a"><mi id="S1.SS2.p1.2.m2.1.1" xref="S1.SS2.p1.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S1.SS2.p1.2.m2.1b"><ci id="S1.SS2.p1.2.m2.1.1.cmml" xref="S1.SS2.p1.2.m2.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS2.p1.2.m2.1c">C</annotation><annotation encoding="application/x-llamapun" id="S1.SS2.p1.2.m2.1d">italic_C</annotation></semantics></math> and a set of adapter <math alttext="A=\{L_{1},L_{2},...L_{k}\}" class="ltx_Math" display="inline" id="S1.SS2.p1.3.m3.3"><semantics id="S1.SS2.p1.3.m3.3a"><mrow id="S1.SS2.p1.3.m3.3.3" xref="S1.SS2.p1.3.m3.3.3.cmml"><mi id="S1.SS2.p1.3.m3.3.3.5" xref="S1.SS2.p1.3.m3.3.3.5.cmml">A</mi><mo id="S1.SS2.p1.3.m3.3.3.4" xref="S1.SS2.p1.3.m3.3.3.4.cmml">=</mo><mrow id="S1.SS2.p1.3.m3.3.3.3.3" xref="S1.SS2.p1.3.m3.3.3.3.4.cmml"><mo id="S1.SS2.p1.3.m3.3.3.3.3.4" stretchy="false" xref="S1.SS2.p1.3.m3.3.3.3.4.cmml">{</mo><msub id="S1.SS2.p1.3.m3.1.1.1.1.1" xref="S1.SS2.p1.3.m3.1.1.1.1.1.cmml"><mi id="S1.SS2.p1.3.m3.1.1.1.1.1.2" xref="S1.SS2.p1.3.m3.1.1.1.1.1.2.cmml">L</mi><mn id="S1.SS2.p1.3.m3.1.1.1.1.1.3" xref="S1.SS2.p1.3.m3.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S1.SS2.p1.3.m3.3.3.3.3.5" xref="S1.SS2.p1.3.m3.3.3.3.4.cmml">,</mo><msub id="S1.SS2.p1.3.m3.2.2.2.2.2" xref="S1.SS2.p1.3.m3.2.2.2.2.2.cmml"><mi id="S1.SS2.p1.3.m3.2.2.2.2.2.2" xref="S1.SS2.p1.3.m3.2.2.2.2.2.2.cmml">L</mi><mn id="S1.SS2.p1.3.m3.2.2.2.2.2.3" xref="S1.SS2.p1.3.m3.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S1.SS2.p1.3.m3.3.3.3.3.6" xref="S1.SS2.p1.3.m3.3.3.3.4.cmml">,</mo><mrow id="S1.SS2.p1.3.m3.3.3.3.3.3" xref="S1.SS2.p1.3.m3.3.3.3.3.3.cmml"><mi id="S1.SS2.p1.3.m3.3.3.3.3.3.2" mathvariant="normal" xref="S1.SS2.p1.3.m3.3.3.3.3.3.2.cmml">â€¦</mi><mo id="S1.SS2.p1.3.m3.3.3.3.3.3.1" xref="S1.SS2.p1.3.m3.3.3.3.3.3.1.cmml">â¢</mo><msub id="S1.SS2.p1.3.m3.3.3.3.3.3.3" xref="S1.SS2.p1.3.m3.3.3.3.3.3.3.cmml"><mi id="S1.SS2.p1.3.m3.3.3.3.3.3.3.2" xref="S1.SS2.p1.3.m3.3.3.3.3.3.3.2.cmml">L</mi><mi id="S1.SS2.p1.3.m3.3.3.3.3.3.3.3" xref="S1.SS2.p1.3.m3.3.3.3.3.3.3.3.cmml">k</mi></msub></mrow><mo id="S1.SS2.p1.3.m3.3.3.3.3.7" stretchy="false" xref="S1.SS2.p1.3.m3.3.3.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.SS2.p1.3.m3.3b"><apply id="S1.SS2.p1.3.m3.3.3.cmml" xref="S1.SS2.p1.3.m3.3.3"><eq id="S1.SS2.p1.3.m3.3.3.4.cmml" xref="S1.SS2.p1.3.m3.3.3.4"></eq><ci id="S1.SS2.p1.3.m3.3.3.5.cmml" xref="S1.SS2.p1.3.m3.3.3.5">ğ´</ci><set id="S1.SS2.p1.3.m3.3.3.3.4.cmml" xref="S1.SS2.p1.3.m3.3.3.3.3"><apply id="S1.SS2.p1.3.m3.1.1.1.1.1.cmml" xref="S1.SS2.p1.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S1.SS2.p1.3.m3.1.1.1.1.1.1.cmml" xref="S1.SS2.p1.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S1.SS2.p1.3.m3.1.1.1.1.1.2.cmml" xref="S1.SS2.p1.3.m3.1.1.1.1.1.2">ğ¿</ci><cn id="S1.SS2.p1.3.m3.1.1.1.1.1.3.cmml" type="integer" xref="S1.SS2.p1.3.m3.1.1.1.1.1.3">1</cn></apply><apply id="S1.SS2.p1.3.m3.2.2.2.2.2.cmml" xref="S1.SS2.p1.3.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S1.SS2.p1.3.m3.2.2.2.2.2.1.cmml" xref="S1.SS2.p1.3.m3.2.2.2.2.2">subscript</csymbol><ci id="S1.SS2.p1.3.m3.2.2.2.2.2.2.cmml" xref="S1.SS2.p1.3.m3.2.2.2.2.2.2">ğ¿</ci><cn id="S1.SS2.p1.3.m3.2.2.2.2.2.3.cmml" type="integer" xref="S1.SS2.p1.3.m3.2.2.2.2.2.3">2</cn></apply><apply id="S1.SS2.p1.3.m3.3.3.3.3.3.cmml" xref="S1.SS2.p1.3.m3.3.3.3.3.3"><times id="S1.SS2.p1.3.m3.3.3.3.3.3.1.cmml" xref="S1.SS2.p1.3.m3.3.3.3.3.3.1"></times><ci id="S1.SS2.p1.3.m3.3.3.3.3.3.2.cmml" xref="S1.SS2.p1.3.m3.3.3.3.3.3.2">â€¦</ci><apply id="S1.SS2.p1.3.m3.3.3.3.3.3.3.cmml" xref="S1.SS2.p1.3.m3.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S1.SS2.p1.3.m3.3.3.3.3.3.3.1.cmml" xref="S1.SS2.p1.3.m3.3.3.3.3.3.3">subscript</csymbol><ci id="S1.SS2.p1.3.m3.3.3.3.3.3.3.2.cmml" xref="S1.SS2.p1.3.m3.3.3.3.3.3.3.2">ğ¿</ci><ci id="S1.SS2.p1.3.m3.3.3.3.3.3.3.3.cmml" xref="S1.SS2.p1.3.m3.3.3.3.3.3.3.3">ğ‘˜</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS2.p1.3.m3.3c">A=\{L_{1},L_{2},...L_{k}\}</annotation><annotation encoding="application/x-llamapun" id="S1.SS2.p1.3.m3.3d">italic_A = { italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ italic_L start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }</annotation></semantics></math> how can one find a set of adapters <math alttext="(\{L^{\prime}_{1},L^{\prime}_{2},...L^{\prime}_{n}\})" class="ltx_Math" display="inline" id="S1.SS2.p1.4.m4.1"><semantics id="S1.SS2.p1.4.m4.1a"><mrow id="S1.SS2.p1.4.m4.1.1.1"><mo id="S1.SS2.p1.4.m4.1.1.1.2" stretchy="false">(</mo><mrow id="S1.SS2.p1.4.m4.1.1.1.1.3" xref="S1.SS2.p1.4.m4.1.1.1.1.4.cmml"><mo id="S1.SS2.p1.4.m4.1.1.1.1.3.4" stretchy="false" xref="S1.SS2.p1.4.m4.1.1.1.1.4.cmml">{</mo><msubsup id="S1.SS2.p1.4.m4.1.1.1.1.1.1" xref="S1.SS2.p1.4.m4.1.1.1.1.1.1.cmml"><mi id="S1.SS2.p1.4.m4.1.1.1.1.1.1.2.2" xref="S1.SS2.p1.4.m4.1.1.1.1.1.1.2.2.cmml">L</mi><mn id="S1.SS2.p1.4.m4.1.1.1.1.1.1.3" xref="S1.SS2.p1.4.m4.1.1.1.1.1.1.3.cmml">1</mn><mo id="S1.SS2.p1.4.m4.1.1.1.1.1.1.2.3" xref="S1.SS2.p1.4.m4.1.1.1.1.1.1.2.3.cmml">â€²</mo></msubsup><mo id="S1.SS2.p1.4.m4.1.1.1.1.3.5" xref="S1.SS2.p1.4.m4.1.1.1.1.4.cmml">,</mo><msubsup id="S1.SS2.p1.4.m4.1.1.1.1.2.2" xref="S1.SS2.p1.4.m4.1.1.1.1.2.2.cmml"><mi id="S1.SS2.p1.4.m4.1.1.1.1.2.2.2.2" xref="S1.SS2.p1.4.m4.1.1.1.1.2.2.2.2.cmml">L</mi><mn id="S1.SS2.p1.4.m4.1.1.1.1.2.2.3" xref="S1.SS2.p1.4.m4.1.1.1.1.2.2.3.cmml">2</mn><mo id="S1.SS2.p1.4.m4.1.1.1.1.2.2.2.3" xref="S1.SS2.p1.4.m4.1.1.1.1.2.2.2.3.cmml">â€²</mo></msubsup><mo id="S1.SS2.p1.4.m4.1.1.1.1.3.6" xref="S1.SS2.p1.4.m4.1.1.1.1.4.cmml">,</mo><mrow id="S1.SS2.p1.4.m4.1.1.1.1.3.3" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3.cmml"><mi id="S1.SS2.p1.4.m4.1.1.1.1.3.3.2" mathvariant="normal" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3.2.cmml">â€¦</mi><mo id="S1.SS2.p1.4.m4.1.1.1.1.3.3.1" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3.1.cmml">â¢</mo><msubsup id="S1.SS2.p1.4.m4.1.1.1.1.3.3.3" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.cmml"><mi id="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.2.2" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.2.2.cmml">L</mi><mi id="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.3" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.3.cmml">n</mi><mo id="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.2.3" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.2.3.cmml">â€²</mo></msubsup></mrow><mo id="S1.SS2.p1.4.m4.1.1.1.1.3.7" stretchy="false" xref="S1.SS2.p1.4.m4.1.1.1.1.4.cmml">}</mo></mrow><mo id="S1.SS2.p1.4.m4.1.1.1.3" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.SS2.p1.4.m4.1b"><set id="S1.SS2.p1.4.m4.1.1.1.1.4.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.3"><apply id="S1.SS2.p1.4.m4.1.1.1.1.1.1.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S1.SS2.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.1.1">subscript</csymbol><apply id="S1.SS2.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S1.SS2.p1.4.m4.1.1.1.1.1.1.2.1.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.1.1">superscript</csymbol><ci id="S1.SS2.p1.4.m4.1.1.1.1.1.1.2.2.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.1.1.2.2">ğ¿</ci><ci id="S1.SS2.p1.4.m4.1.1.1.1.1.1.2.3.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.1.1.2.3">â€²</ci></apply><cn id="S1.SS2.p1.4.m4.1.1.1.1.1.1.3.cmml" type="integer" xref="S1.SS2.p1.4.m4.1.1.1.1.1.1.3">1</cn></apply><apply id="S1.SS2.p1.4.m4.1.1.1.1.2.2.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S1.SS2.p1.4.m4.1.1.1.1.2.2.1.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.2.2">subscript</csymbol><apply id="S1.SS2.p1.4.m4.1.1.1.1.2.2.2.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S1.SS2.p1.4.m4.1.1.1.1.2.2.2.1.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.2.2">superscript</csymbol><ci id="S1.SS2.p1.4.m4.1.1.1.1.2.2.2.2.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.2.2.2.2">ğ¿</ci><ci id="S1.SS2.p1.4.m4.1.1.1.1.2.2.2.3.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.2.2.2.3">â€²</ci></apply><cn id="S1.SS2.p1.4.m4.1.1.1.1.2.2.3.cmml" type="integer" xref="S1.SS2.p1.4.m4.1.1.1.1.2.2.3">2</cn></apply><apply id="S1.SS2.p1.4.m4.1.1.1.1.3.3.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3"><times id="S1.SS2.p1.4.m4.1.1.1.1.3.3.1.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3.1"></times><ci id="S1.SS2.p1.4.m4.1.1.1.1.3.3.2.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3.2">â€¦</ci><apply id="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.1.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3.3">subscript</csymbol><apply id="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.2.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.2.1.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3.3">superscript</csymbol><ci id="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.2.2.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.2.2">ğ¿</ci><ci id="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.2.3.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.2.3">â€²</ci></apply><ci id="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.3.cmml" xref="S1.SS2.p1.4.m4.1.1.1.1.3.3.3.3">ğ‘›</ci></apply></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S1.SS2.p1.4.m4.1c">(\{L^{\prime}_{1},L^{\prime}_{2},...L^{\prime}_{n}\})</annotation><annotation encoding="application/x-llamapun" id="S1.SS2.p1.4.m4.1d">( { italic_L start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_L start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ italic_L start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } )</annotation></semantics></math> that would systematically improve <span class="ltx_text ltx_font_italic" id="S1.SS2.p1.6.2">image diversity</span> while having model output generation <math alttext="O" class="ltx_Math" display="inline" id="S1.SS2.p1.5.m5.1"><semantics id="S1.SS2.p1.5.m5.1a"><mi id="S1.SS2.p1.5.m5.1.1" xref="S1.SS2.p1.5.m5.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S1.SS2.p1.5.m5.1b"><ci id="S1.SS2.p1.5.m5.1.1.cmml" xref="S1.SS2.p1.5.m5.1.1">ğ‘‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS2.p1.5.m5.1c">O</annotation><annotation encoding="application/x-llamapun" id="S1.SS2.p1.5.m5.1d">italic_O</annotation></semantics></math> retaining <span class="ltx_text ltx_font_italic" id="S1.SS2.p1.6.3">alignment</span> to <math alttext="P" class="ltx_Math" display="inline" id="S1.SS2.p1.6.m6.1"><semantics id="S1.SS2.p1.6.m6.1a"><mi id="S1.SS2.p1.6.m6.1.1" xref="S1.SS2.p1.6.m6.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S1.SS2.p1.6.m6.1b"><ci id="S1.SS2.p1.6.m6.1.1.cmml" xref="S1.SS2.p1.6.m6.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS2.p1.6.m6.1c">P</annotation><annotation encoding="application/x-llamapun" id="S1.SS2.p1.6.m6.1d">italic_P</annotation></semantics></math> .</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS2.p2">
<p class="ltx_p" id="S1.SS2.p2.9">This line of research attempts to generalize this to the broader <span class="ltx_text ltx_font_italic" id="S1.SS2.p2.9.1">model-adapter composition problem</span> by assuming there are additional choices to make for the model <math alttext="C" class="ltx_Math" display="inline" id="S1.SS2.p2.1.m1.1"><semantics id="S1.SS2.p2.1.m1.1a"><mi id="S1.SS2.p2.1.m1.1.1" xref="S1.SS2.p2.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S1.SS2.p2.1.m1.1b"><ci id="S1.SS2.p2.1.m1.1.1.cmml" xref="S1.SS2.p2.1.m1.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS2.p2.1.m1.1c">C</annotation><annotation encoding="application/x-llamapun" id="S1.SS2.p2.1.m1.1d">italic_C</annotation></semantics></math> as well. Given a prompt <math alttext="P" class="ltx_Math" display="inline" id="S1.SS2.p2.2.m2.1"><semantics id="S1.SS2.p2.2.m2.1a"><mi id="S1.SS2.p2.2.m2.1.1" xref="S1.SS2.p2.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S1.SS2.p2.2.m2.1b"><ci id="S1.SS2.p2.2.m2.1.1.cmml" xref="S1.SS2.p2.2.m2.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS2.p2.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="S1.SS2.p2.2.m2.1d">italic_P</annotation></semantics></math> , a set of models <math alttext="C=\{C_{1},C_{2},...C_{k}\}" class="ltx_Math" display="inline" id="S1.SS2.p2.3.m3.3"><semantics id="S1.SS2.p2.3.m3.3a"><mrow id="S1.SS2.p2.3.m3.3.3" xref="S1.SS2.p2.3.m3.3.3.cmml"><mi id="S1.SS2.p2.3.m3.3.3.5" xref="S1.SS2.p2.3.m3.3.3.5.cmml">C</mi><mo id="S1.SS2.p2.3.m3.3.3.4" xref="S1.SS2.p2.3.m3.3.3.4.cmml">=</mo><mrow id="S1.SS2.p2.3.m3.3.3.3.3" xref="S1.SS2.p2.3.m3.3.3.3.4.cmml"><mo id="S1.SS2.p2.3.m3.3.3.3.3.4" stretchy="false" xref="S1.SS2.p2.3.m3.3.3.3.4.cmml">{</mo><msub id="S1.SS2.p2.3.m3.1.1.1.1.1" xref="S1.SS2.p2.3.m3.1.1.1.1.1.cmml"><mi id="S1.SS2.p2.3.m3.1.1.1.1.1.2" xref="S1.SS2.p2.3.m3.1.1.1.1.1.2.cmml">C</mi><mn id="S1.SS2.p2.3.m3.1.1.1.1.1.3" xref="S1.SS2.p2.3.m3.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S1.SS2.p2.3.m3.3.3.3.3.5" xref="S1.SS2.p2.3.m3.3.3.3.4.cmml">,</mo><msub id="S1.SS2.p2.3.m3.2.2.2.2.2" xref="S1.SS2.p2.3.m3.2.2.2.2.2.cmml"><mi id="S1.SS2.p2.3.m3.2.2.2.2.2.2" xref="S1.SS2.p2.3.m3.2.2.2.2.2.2.cmml">C</mi><mn id="S1.SS2.p2.3.m3.2.2.2.2.2.3" xref="S1.SS2.p2.3.m3.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S1.SS2.p2.3.m3.3.3.3.3.6" xref="S1.SS2.p2.3.m3.3.3.3.4.cmml">,</mo><mrow id="S1.SS2.p2.3.m3.3.3.3.3.3" xref="S1.SS2.p2.3.m3.3.3.3.3.3.cmml"><mi id="S1.SS2.p2.3.m3.3.3.3.3.3.2" mathvariant="normal" xref="S1.SS2.p2.3.m3.3.3.3.3.3.2.cmml">â€¦</mi><mo id="S1.SS2.p2.3.m3.3.3.3.3.3.1" xref="S1.SS2.p2.3.m3.3.3.3.3.3.1.cmml">â¢</mo><msub id="S1.SS2.p2.3.m3.3.3.3.3.3.3" xref="S1.SS2.p2.3.m3.3.3.3.3.3.3.cmml"><mi id="S1.SS2.p2.3.m3.3.3.3.3.3.3.2" xref="S1.SS2.p2.3.m3.3.3.3.3.3.3.2.cmml">C</mi><mi id="S1.SS2.p2.3.m3.3.3.3.3.3.3.3" xref="S1.SS2.p2.3.m3.3.3.3.3.3.3.3.cmml">k</mi></msub></mrow><mo id="S1.SS2.p2.3.m3.3.3.3.3.7" stretchy="false" xref="S1.SS2.p2.3.m3.3.3.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.SS2.p2.3.m3.3b"><apply id="S1.SS2.p2.3.m3.3.3.cmml" xref="S1.SS2.p2.3.m3.3.3"><eq id="S1.SS2.p2.3.m3.3.3.4.cmml" xref="S1.SS2.p2.3.m3.3.3.4"></eq><ci id="S1.SS2.p2.3.m3.3.3.5.cmml" xref="S1.SS2.p2.3.m3.3.3.5">ğ¶</ci><set id="S1.SS2.p2.3.m3.3.3.3.4.cmml" xref="S1.SS2.p2.3.m3.3.3.3.3"><apply id="S1.SS2.p2.3.m3.1.1.1.1.1.cmml" xref="S1.SS2.p2.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S1.SS2.p2.3.m3.1.1.1.1.1.1.cmml" xref="S1.SS2.p2.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S1.SS2.p2.3.m3.1.1.1.1.1.2.cmml" xref="S1.SS2.p2.3.m3.1.1.1.1.1.2">ğ¶</ci><cn id="S1.SS2.p2.3.m3.1.1.1.1.1.3.cmml" type="integer" xref="S1.SS2.p2.3.m3.1.1.1.1.1.3">1</cn></apply><apply id="S1.SS2.p2.3.m3.2.2.2.2.2.cmml" xref="S1.SS2.p2.3.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S1.SS2.p2.3.m3.2.2.2.2.2.1.cmml" xref="S1.SS2.p2.3.m3.2.2.2.2.2">subscript</csymbol><ci id="S1.SS2.p2.3.m3.2.2.2.2.2.2.cmml" xref="S1.SS2.p2.3.m3.2.2.2.2.2.2">ğ¶</ci><cn id="S1.SS2.p2.3.m3.2.2.2.2.2.3.cmml" type="integer" xref="S1.SS2.p2.3.m3.2.2.2.2.2.3">2</cn></apply><apply id="S1.SS2.p2.3.m3.3.3.3.3.3.cmml" xref="S1.SS2.p2.3.m3.3.3.3.3.3"><times id="S1.SS2.p2.3.m3.3.3.3.3.3.1.cmml" xref="S1.SS2.p2.3.m3.3.3.3.3.3.1"></times><ci id="S1.SS2.p2.3.m3.3.3.3.3.3.2.cmml" xref="S1.SS2.p2.3.m3.3.3.3.3.3.2">â€¦</ci><apply id="S1.SS2.p2.3.m3.3.3.3.3.3.3.cmml" xref="S1.SS2.p2.3.m3.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S1.SS2.p2.3.m3.3.3.3.3.3.3.1.cmml" xref="S1.SS2.p2.3.m3.3.3.3.3.3.3">subscript</csymbol><ci id="S1.SS2.p2.3.m3.3.3.3.3.3.3.2.cmml" xref="S1.SS2.p2.3.m3.3.3.3.3.3.3.2">ğ¶</ci><ci id="S1.SS2.p2.3.m3.3.3.3.3.3.3.3.cmml" xref="S1.SS2.p2.3.m3.3.3.3.3.3.3.3">ğ‘˜</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS2.p2.3.m3.3c">C=\{C_{1},C_{2},...C_{k}\}</annotation><annotation encoding="application/x-llamapun" id="S1.SS2.p2.3.m3.3d">italic_C = { italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ italic_C start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }</annotation></semantics></math> and the adapter set <math alttext="A=\{L_{1},L_{2},...L_{k}\}" class="ltx_Math" display="inline" id="S1.SS2.p2.4.m4.3"><semantics id="S1.SS2.p2.4.m4.3a"><mrow id="S1.SS2.p2.4.m4.3.3" xref="S1.SS2.p2.4.m4.3.3.cmml"><mi id="S1.SS2.p2.4.m4.3.3.5" xref="S1.SS2.p2.4.m4.3.3.5.cmml">A</mi><mo id="S1.SS2.p2.4.m4.3.3.4" xref="S1.SS2.p2.4.m4.3.3.4.cmml">=</mo><mrow id="S1.SS2.p2.4.m4.3.3.3.3" xref="S1.SS2.p2.4.m4.3.3.3.4.cmml"><mo id="S1.SS2.p2.4.m4.3.3.3.3.4" stretchy="false" xref="S1.SS2.p2.4.m4.3.3.3.4.cmml">{</mo><msub id="S1.SS2.p2.4.m4.1.1.1.1.1" xref="S1.SS2.p2.4.m4.1.1.1.1.1.cmml"><mi id="S1.SS2.p2.4.m4.1.1.1.1.1.2" xref="S1.SS2.p2.4.m4.1.1.1.1.1.2.cmml">L</mi><mn id="S1.SS2.p2.4.m4.1.1.1.1.1.3" xref="S1.SS2.p2.4.m4.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S1.SS2.p2.4.m4.3.3.3.3.5" xref="S1.SS2.p2.4.m4.3.3.3.4.cmml">,</mo><msub id="S1.SS2.p2.4.m4.2.2.2.2.2" xref="S1.SS2.p2.4.m4.2.2.2.2.2.cmml"><mi id="S1.SS2.p2.4.m4.2.2.2.2.2.2" xref="S1.SS2.p2.4.m4.2.2.2.2.2.2.cmml">L</mi><mn id="S1.SS2.p2.4.m4.2.2.2.2.2.3" xref="S1.SS2.p2.4.m4.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S1.SS2.p2.4.m4.3.3.3.3.6" xref="S1.SS2.p2.4.m4.3.3.3.4.cmml">,</mo><mrow id="S1.SS2.p2.4.m4.3.3.3.3.3" xref="S1.SS2.p2.4.m4.3.3.3.3.3.cmml"><mi id="S1.SS2.p2.4.m4.3.3.3.3.3.2" mathvariant="normal" xref="S1.SS2.p2.4.m4.3.3.3.3.3.2.cmml">â€¦</mi><mo id="S1.SS2.p2.4.m4.3.3.3.3.3.1" xref="S1.SS2.p2.4.m4.3.3.3.3.3.1.cmml">â¢</mo><msub id="S1.SS2.p2.4.m4.3.3.3.3.3.3" xref="S1.SS2.p2.4.m4.3.3.3.3.3.3.cmml"><mi id="S1.SS2.p2.4.m4.3.3.3.3.3.3.2" xref="S1.SS2.p2.4.m4.3.3.3.3.3.3.2.cmml">L</mi><mi id="S1.SS2.p2.4.m4.3.3.3.3.3.3.3" xref="S1.SS2.p2.4.m4.3.3.3.3.3.3.3.cmml">k</mi></msub></mrow><mo id="S1.SS2.p2.4.m4.3.3.3.3.7" stretchy="false" xref="S1.SS2.p2.4.m4.3.3.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.SS2.p2.4.m4.3b"><apply id="S1.SS2.p2.4.m4.3.3.cmml" xref="S1.SS2.p2.4.m4.3.3"><eq id="S1.SS2.p2.4.m4.3.3.4.cmml" xref="S1.SS2.p2.4.m4.3.3.4"></eq><ci id="S1.SS2.p2.4.m4.3.3.5.cmml" xref="S1.SS2.p2.4.m4.3.3.5">ğ´</ci><set id="S1.SS2.p2.4.m4.3.3.3.4.cmml" xref="S1.SS2.p2.4.m4.3.3.3.3"><apply id="S1.SS2.p2.4.m4.1.1.1.1.1.cmml" xref="S1.SS2.p2.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="S1.SS2.p2.4.m4.1.1.1.1.1.1.cmml" xref="S1.SS2.p2.4.m4.1.1.1.1.1">subscript</csymbol><ci id="S1.SS2.p2.4.m4.1.1.1.1.1.2.cmml" xref="S1.SS2.p2.4.m4.1.1.1.1.1.2">ğ¿</ci><cn id="S1.SS2.p2.4.m4.1.1.1.1.1.3.cmml" type="integer" xref="S1.SS2.p2.4.m4.1.1.1.1.1.3">1</cn></apply><apply id="S1.SS2.p2.4.m4.2.2.2.2.2.cmml" xref="S1.SS2.p2.4.m4.2.2.2.2.2"><csymbol cd="ambiguous" id="S1.SS2.p2.4.m4.2.2.2.2.2.1.cmml" xref="S1.SS2.p2.4.m4.2.2.2.2.2">subscript</csymbol><ci id="S1.SS2.p2.4.m4.2.2.2.2.2.2.cmml" xref="S1.SS2.p2.4.m4.2.2.2.2.2.2">ğ¿</ci><cn id="S1.SS2.p2.4.m4.2.2.2.2.2.3.cmml" type="integer" xref="S1.SS2.p2.4.m4.2.2.2.2.2.3">2</cn></apply><apply id="S1.SS2.p2.4.m4.3.3.3.3.3.cmml" xref="S1.SS2.p2.4.m4.3.3.3.3.3"><times id="S1.SS2.p2.4.m4.3.3.3.3.3.1.cmml" xref="S1.SS2.p2.4.m4.3.3.3.3.3.1"></times><ci id="S1.SS2.p2.4.m4.3.3.3.3.3.2.cmml" xref="S1.SS2.p2.4.m4.3.3.3.3.3.2">â€¦</ci><apply id="S1.SS2.p2.4.m4.3.3.3.3.3.3.cmml" xref="S1.SS2.p2.4.m4.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S1.SS2.p2.4.m4.3.3.3.3.3.3.1.cmml" xref="S1.SS2.p2.4.m4.3.3.3.3.3.3">subscript</csymbol><ci id="S1.SS2.p2.4.m4.3.3.3.3.3.3.2.cmml" xref="S1.SS2.p2.4.m4.3.3.3.3.3.3.2">ğ¿</ci><ci id="S1.SS2.p2.4.m4.3.3.3.3.3.3.3.cmml" xref="S1.SS2.p2.4.m4.3.3.3.3.3.3.3">ğ‘˜</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS2.p2.4.m4.3c">A=\{L_{1},L_{2},...L_{k}\}</annotation><annotation encoding="application/x-llamapun" id="S1.SS2.p2.4.m4.3d">italic_A = { italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ italic_L start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }</annotation></semantics></math> , where any <math alttext="L_{i}" class="ltx_Math" display="inline" id="S1.SS2.p2.5.m5.1"><semantics id="S1.SS2.p2.5.m5.1a"><msub id="S1.SS2.p2.5.m5.1.1" xref="S1.SS2.p2.5.m5.1.1.cmml"><mi id="S1.SS2.p2.5.m5.1.1.2" xref="S1.SS2.p2.5.m5.1.1.2.cmml">L</mi><mi id="S1.SS2.p2.5.m5.1.1.3" xref="S1.SS2.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S1.SS2.p2.5.m5.1b"><apply id="S1.SS2.p2.5.m5.1.1.cmml" xref="S1.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S1.SS2.p2.5.m5.1.1.1.cmml" xref="S1.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S1.SS2.p2.5.m5.1.1.2.cmml" xref="S1.SS2.p2.5.m5.1.1.2">ğ¿</ci><ci id="S1.SS2.p2.5.m5.1.1.3.cmml" xref="S1.SS2.p2.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS2.p2.5.m5.1c">L_{i}</annotation><annotation encoding="application/x-llamapun" id="S1.SS2.p2.5.m5.1d">italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is capable of providing additional fine tuning to any of the models in <math alttext="C" class="ltx_Math" display="inline" id="S1.SS2.p2.6.m6.1"><semantics id="S1.SS2.p2.6.m6.1a"><mi id="S1.SS2.p2.6.m6.1.1" xref="S1.SS2.p2.6.m6.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S1.SS2.p2.6.m6.1b"><ci id="S1.SS2.p2.6.m6.1.1.cmml" xref="S1.SS2.p2.6.m6.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS2.p2.6.m6.1c">C</annotation><annotation encoding="application/x-llamapun" id="S1.SS2.p2.6.m6.1d">italic_C</annotation></semantics></math>, how do we come up with a system <math alttext="S" class="ltx_Math" display="inline" id="S1.SS2.p2.7.m7.1"><semantics id="S1.SS2.p2.7.m7.1a"><mi id="S1.SS2.p2.7.m7.1.1" xref="S1.SS2.p2.7.m7.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S1.SS2.p2.7.m7.1b"><ci id="S1.SS2.p2.7.m7.1.1.cmml" xref="S1.SS2.p2.7.m7.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS2.p2.7.m7.1c">S</annotation><annotation encoding="application/x-llamapun" id="S1.SS2.p2.7.m7.1d">italic_S</annotation></semantics></math> that effectively maps <math alttext="P" class="ltx_Math" display="inline" id="S1.SS2.p2.8.m8.1"><semantics id="S1.SS2.p2.8.m8.1a"><mi id="S1.SS2.p2.8.m8.1.1" xref="S1.SS2.p2.8.m8.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S1.SS2.p2.8.m8.1b"><ci id="S1.SS2.p2.8.m8.1.1.cmml" xref="S1.SS2.p2.8.m8.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS2.p2.8.m8.1c">P</annotation><annotation encoding="application/x-llamapun" id="S1.SS2.p2.8.m8.1d">italic_P</annotation></semantics></math> to a set of adapters <span class="ltx_text ltx_font_italic" id="S1.SS2.p2.9.2">and a model</span> <math alttext="(C_{i},\{L^{\prime}_{1},L^{\prime}_{2},...L^{\prime}_{n}\})" class="ltx_Math" display="inline" id="S1.SS2.p2.9.m9.2"><semantics id="S1.SS2.p2.9.m9.2a"><mrow id="S1.SS2.p2.9.m9.2.2.2" xref="S1.SS2.p2.9.m9.2.2.3.cmml"><mo id="S1.SS2.p2.9.m9.2.2.2.3" stretchy="false" xref="S1.SS2.p2.9.m9.2.2.3.cmml">(</mo><msub id="S1.SS2.p2.9.m9.1.1.1.1" xref="S1.SS2.p2.9.m9.1.1.1.1.cmml"><mi id="S1.SS2.p2.9.m9.1.1.1.1.2" xref="S1.SS2.p2.9.m9.1.1.1.1.2.cmml">C</mi><mi id="S1.SS2.p2.9.m9.1.1.1.1.3" xref="S1.SS2.p2.9.m9.1.1.1.1.3.cmml">i</mi></msub><mo id="S1.SS2.p2.9.m9.2.2.2.4" xref="S1.SS2.p2.9.m9.2.2.3.cmml">,</mo><mrow id="S1.SS2.p2.9.m9.2.2.2.2.3" xref="S1.SS2.p2.9.m9.2.2.2.2.4.cmml"><mo id="S1.SS2.p2.9.m9.2.2.2.2.3.4" stretchy="false" xref="S1.SS2.p2.9.m9.2.2.2.2.4.cmml">{</mo><msubsup id="S1.SS2.p2.9.m9.2.2.2.2.1.1" xref="S1.SS2.p2.9.m9.2.2.2.2.1.1.cmml"><mi id="S1.SS2.p2.9.m9.2.2.2.2.1.1.2.2" xref="S1.SS2.p2.9.m9.2.2.2.2.1.1.2.2.cmml">L</mi><mn id="S1.SS2.p2.9.m9.2.2.2.2.1.1.3" xref="S1.SS2.p2.9.m9.2.2.2.2.1.1.3.cmml">1</mn><mo id="S1.SS2.p2.9.m9.2.2.2.2.1.1.2.3" xref="S1.SS2.p2.9.m9.2.2.2.2.1.1.2.3.cmml">â€²</mo></msubsup><mo id="S1.SS2.p2.9.m9.2.2.2.2.3.5" xref="S1.SS2.p2.9.m9.2.2.2.2.4.cmml">,</mo><msubsup id="S1.SS2.p2.9.m9.2.2.2.2.2.2" xref="S1.SS2.p2.9.m9.2.2.2.2.2.2.cmml"><mi id="S1.SS2.p2.9.m9.2.2.2.2.2.2.2.2" xref="S1.SS2.p2.9.m9.2.2.2.2.2.2.2.2.cmml">L</mi><mn id="S1.SS2.p2.9.m9.2.2.2.2.2.2.3" xref="S1.SS2.p2.9.m9.2.2.2.2.2.2.3.cmml">2</mn><mo id="S1.SS2.p2.9.m9.2.2.2.2.2.2.2.3" xref="S1.SS2.p2.9.m9.2.2.2.2.2.2.2.3.cmml">â€²</mo></msubsup><mo id="S1.SS2.p2.9.m9.2.2.2.2.3.6" xref="S1.SS2.p2.9.m9.2.2.2.2.4.cmml">,</mo><mrow id="S1.SS2.p2.9.m9.2.2.2.2.3.3" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3.cmml"><mi id="S1.SS2.p2.9.m9.2.2.2.2.3.3.2" mathvariant="normal" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3.2.cmml">â€¦</mi><mo id="S1.SS2.p2.9.m9.2.2.2.2.3.3.1" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3.1.cmml">â¢</mo><msubsup id="S1.SS2.p2.9.m9.2.2.2.2.3.3.3" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.cmml"><mi id="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.2.2" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.2.2.cmml">L</mi><mi id="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.3" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.3.cmml">n</mi><mo id="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.2.3" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.2.3.cmml">â€²</mo></msubsup></mrow><mo id="S1.SS2.p2.9.m9.2.2.2.2.3.7" stretchy="false" xref="S1.SS2.p2.9.m9.2.2.2.2.4.cmml">}</mo></mrow><mo id="S1.SS2.p2.9.m9.2.2.2.5" stretchy="false" xref="S1.SS2.p2.9.m9.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.SS2.p2.9.m9.2b"><interval closure="open" id="S1.SS2.p2.9.m9.2.2.3.cmml" xref="S1.SS2.p2.9.m9.2.2.2"><apply id="S1.SS2.p2.9.m9.1.1.1.1.cmml" xref="S1.SS2.p2.9.m9.1.1.1.1"><csymbol cd="ambiguous" id="S1.SS2.p2.9.m9.1.1.1.1.1.cmml" xref="S1.SS2.p2.9.m9.1.1.1.1">subscript</csymbol><ci id="S1.SS2.p2.9.m9.1.1.1.1.2.cmml" xref="S1.SS2.p2.9.m9.1.1.1.1.2">ğ¶</ci><ci id="S1.SS2.p2.9.m9.1.1.1.1.3.cmml" xref="S1.SS2.p2.9.m9.1.1.1.1.3">ğ‘–</ci></apply><set id="S1.SS2.p2.9.m9.2.2.2.2.4.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.3"><apply id="S1.SS2.p2.9.m9.2.2.2.2.1.1.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S1.SS2.p2.9.m9.2.2.2.2.1.1.1.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.1.1">subscript</csymbol><apply id="S1.SS2.p2.9.m9.2.2.2.2.1.1.2.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S1.SS2.p2.9.m9.2.2.2.2.1.1.2.1.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.1.1">superscript</csymbol><ci id="S1.SS2.p2.9.m9.2.2.2.2.1.1.2.2.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.1.1.2.2">ğ¿</ci><ci id="S1.SS2.p2.9.m9.2.2.2.2.1.1.2.3.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.1.1.2.3">â€²</ci></apply><cn id="S1.SS2.p2.9.m9.2.2.2.2.1.1.3.cmml" type="integer" xref="S1.SS2.p2.9.m9.2.2.2.2.1.1.3">1</cn></apply><apply id="S1.SS2.p2.9.m9.2.2.2.2.2.2.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S1.SS2.p2.9.m9.2.2.2.2.2.2.1.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.2.2">subscript</csymbol><apply id="S1.SS2.p2.9.m9.2.2.2.2.2.2.2.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S1.SS2.p2.9.m9.2.2.2.2.2.2.2.1.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.2.2">superscript</csymbol><ci id="S1.SS2.p2.9.m9.2.2.2.2.2.2.2.2.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.2.2.2.2">ğ¿</ci><ci id="S1.SS2.p2.9.m9.2.2.2.2.2.2.2.3.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.2.2.2.3">â€²</ci></apply><cn id="S1.SS2.p2.9.m9.2.2.2.2.2.2.3.cmml" type="integer" xref="S1.SS2.p2.9.m9.2.2.2.2.2.2.3">2</cn></apply><apply id="S1.SS2.p2.9.m9.2.2.2.2.3.3.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3"><times id="S1.SS2.p2.9.m9.2.2.2.2.3.3.1.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3.1"></times><ci id="S1.SS2.p2.9.m9.2.2.2.2.3.3.2.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3.2">â€¦</ci><apply id="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3.3"><csymbol cd="ambiguous" id="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.1.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3.3">subscript</csymbol><apply id="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.2.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3.3"><csymbol cd="ambiguous" id="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.2.1.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3.3">superscript</csymbol><ci id="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.2.2.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.2.2">ğ¿</ci><ci id="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.2.3.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.2.3">â€²</ci></apply><ci id="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.3.cmml" xref="S1.SS2.p2.9.m9.2.2.2.2.3.3.3.3">ğ‘›</ci></apply></apply></set></interval></annotation-xml><annotation encoding="application/x-tex" id="S1.SS2.p2.9.m9.2c">(C_{i},\{L^{\prime}_{1},L^{\prime}_{2},...L^{\prime}_{n}\})</annotation><annotation encoding="application/x-llamapun" id="S1.SS2.p2.9.m9.2d">( italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , { italic_L start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_L start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ italic_L start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } )</annotation></semantics></math> such that the new combination provides additional output information. In the image domain, this "additional information" typically refers to output diversity or quality, commonly measured objectively through the Frechet Inception Distance (FID) or Inception Score (IS), while more emerging holistic, human-approximating methods use Vision Language Models such as GPT-4V.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS2.p3">
<p class="ltx_p" id="S1.SS2.p3.1">Furthermore, this research paper extends the latter problem of model-adapter composition in a standard, lower-end consumer grade hardware setting. We attempt to extend the knowledge of previous work towards also selecting the most appropriate checkpoint in the image domain case. Additionally, we acknowledge that there are various <span class="ltx_text ltx_font_bold" id="S1.SS2.p3.1.1">software and hardware budget constraints</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib9" title="">9</a>]</cite>, and therefore additionally define the token budget <math alttext="T" class="ltx_Math" display="inline" id="S1.SS2.p3.1.m1.1"><semantics id="S1.SS2.p3.1.m1.1a"><mi id="S1.SS2.p3.1.m1.1.1" xref="S1.SS2.p3.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S1.SS2.p3.1.m1.1b"><ci id="S1.SS2.p3.1.m1.1.1.cmml" xref="S1.SS2.p3.1.m1.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.SS2.p3.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S1.SS2.p3.1.m1.1d">italic_T</annotation></semantics></math>, representing the number of tokens sent to AI mechanisms for embedding or generation purposes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib10" title="">10</a>]</cite>. Additionally, as a large portion of the image generation community relies on limited, consumer-grade hardware, we seek to conduct experiments and list out performance over hardware profiles that AI hobbyists and enthusiasts would find useful when considering accompanying hardware.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Past Work</h3>
<div class="ltx_para ltx_noindent" id="S1.SS3.p1">
<p class="ltx_p" id="S1.SS3.p1.1">Research within the diffusion model based image generation domain continues to move at a breakneck pace: customization of output through adapters has happened so far in two notable manners - (1) direct integration at the model level, or (2) retrieval based methods to directly apply adapters on top of checkpoints. To the best of our knowledge, we have not seen any works delving directly into checkpoint selection.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS3.p2">
<p class="ltx_p" id="S1.SS3.p2.1">While model based methods have proven to create higher quality images, we note that these are extremely unfeasible at a large scale, due to direct augmentation of the model causing storage constraints; many of these papers also assume a well defined set of adapters, which is not a steadfast requirement in our case.</p>
</div>
<div class="ltx_para" id="S1.SS3.p3">
<p class="ltx_p" id="S1.SS3.p3.1">We list relevant work addressing model customization for more custom output.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="479" id="S1.F1.g1" src="extracted/5871043/original-stylus.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples of images generated via Stylus</figcaption>
</figure>
<section class="ltx_subsubsection" id="S1.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.3.1 </span>Model Based Methods</h4>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS1.p1">
<p class="ltx_p" id="S1.SS3.SSS1.p1.1">Gu et. al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib11" title="">11</a>]</cite> created the Mix of Show algorithm that directly updates pretrained models at the weight level in order to resolve concept conflicts, as well as to bring extremely unrelated concepts together. While this enhanced cohesion does improve image quality, directly updating a pretrained model templateâ€™s weights per LoRA combination or prompt would be unuseable at a large scale, and time consuming. In another direction, Choi et. al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib12" title="">12</a>]</cite> demonstrate improved adapter performance (and thus, customizability) at the architecture level, applying adapter weights to a specific portion - the attention layers - rather than the entire model.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS1.p2">
<p class="ltx_p" id="S1.SS3.SSS1.p2.1">In another recent work, Yang et. al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib13" title="">13</a>]</cite> creates the LoRA Composer system, which promotes the <span class="ltx_text ltx_font_italic" id="S1.SS3.SSS1.p2.1.1">training free</span> adapter composition approach that we similarly attempt to advance. Rather than altering the model weights, LoRA composer promotes inference time ideas such as latent re-initialization and constraining the latent space to effectively operate for a subsection.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.3.2 </span>Retrieval Based Methods</h4>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS2.p1">
<p class="ltx_p" id="S1.SS3.SSS2.p1.1">The first retrieval based adapter selection approach known came from Luo et. al <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib8" title="">8</a>]</cite>, who developed Stylus (referred to as Stylus or Stylus 1.0 throughout the paper). The system was created to resolve the Adapter Composition problem using retrieval augmented generation (RAG) with downstream re-ranking. This was done by fixing a base model and using a large language model (LLM) to compose adapters in the form of Low Rank Adaptations (LoRAs) based on found titles and description metadata that appear to be the most relevant <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib14" title="">14</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib15" title="">15</a>]</cite>. As previously stated by the authors in their abstract - <span class="ltx_text ltx_font_italic" id="S1.SS3.SSS2.p1.1.1">Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on promptsâ€™ keywords by checking how well they fit the prompt.<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_upright" id="S1.SS3.SSS2.p1.1.1.1.1">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib8" title="">8</a><span class="ltx_text ltx_font_upright" id="S1.SS3.SSS2.p1.1.1.2.2">]</span></cite></span></p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS3.SSS2.p2">
<p class="ltx_p" id="S1.SS3.SSS2.p2.1">Some core contributions from their paper include (1) introducing an early framework for <span class="ltx_text ltx_font_italic" id="S1.SS3.SSS2.p2.1.1">concept mapping</span>, and then (2) associating each adapter with the concept-mapped keywords that can reframe the problem into a retrieval augmented generation situation <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.F1" title="Figure 1 â€£ 1.3 Past Work â€£ 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4 </span>Retrieval Methods Limitations and Opportunities</h3>
<div class="ltx_para ltx_noindent" id="S1.SS4.p1">
<p class="ltx_p" id="S1.SS4.p1.1">With Stylus, there were multiple open challenges revealed in the area of systematic model composition. We first discuss the limitations and failure modes seen from our evaluation of the Stylus system from the image domain perspective and then in the broader attempt at adapter composition.</p>
</div>
<section class="ltx_subsubsection" id="S1.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.4.1 </span>Lack of Task Diversity</h4>
<div class="ltx_para ltx_noindent" id="S1.SS4.SSS1.p1">
<p class="ltx_p" id="S1.SS4.SSS1.p1.1">The Stylus system heavily relies on metadata such as descriptions, titles, and other textual metadata in its retrieval mechanism. While this does prove functional in practice, we find that this commonly leads to improper output generation and low alignment, a problem pervading image generation systems in general <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib16" title="">16</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib17" title="">17</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib18" title="">18</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib19" title="">19</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS4.SSS1.p2">
<p class="ltx_p" id="S1.SS4.SSS1.p2.1">Stylus fosters image diversity through pure randomness, randomly drawing permutations of LoRAs that seem be relevant within reason. While this method did foster diversity, there are visible limitations to the extent of which this diversity reaches, which stems from the lack of vetting adapters with previously tested examples of output <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.F2" title="Figure 2 â€£ 1.4.1 Lack of Task Diversity â€£ 1.4 Retrieval Methods Limitations and Opportunities â€£ 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S1.SS4.SSS1.p3">
<p class="ltx_p" id="S1.SS4.SSS1.p3.1">.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="479" id="S1.F2.g1" src="extracted/5871043/low-diversity.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example of a "low image diversity" generation, source Stylus. The majority of the cars synthetically generated look extremely similar and generic, and all have muted backgrounds. </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.SS4.SSS1.p4">
<p class="ltx_p" id="S1.SS4.SSS1.p4.1">We improve upon the diversity in our system, MANTA, inserting an additional step to find the most appropriate checkpoint to further image diversity, while minimizing token usage and RAM requirements by leveraging previous image prompts using the model as source documents.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.4.2 </span>Low Alignment</h4>
<div class="ltx_para ltx_noindent" id="S1.SS4.SSS2.p1">
<p class="ltx_p" id="S1.SS4.SSS2.p1.1">Similarly, there are examples where the previous image generation system created images with little consideration for how the concepts in the image seek to interact with one another <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib20" title="">20</a>]</cite>. This leads to images which may be seen as diverse, but in an unintentional and negative manner <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.F3" title="Figure 3 â€£ 1.4.2 Low Alignment â€£ 1.4 Retrieval Methods Limitations and Opportunities â€£ 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="299" id="S1.F3.g1" src="extracted/5871043/illogical-alignment.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of a low alignment output from Stylus. Prompt: A stop sign that has the picture of George Bush in place of the letter O.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.5 </span>Current Image Generation Workflow Challenges</h3>
<section class="ltx_subsubsection" id="S1.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.5.1 </span>Image Resolution</h4>
<div class="ltx_para ltx_noindent" id="S1.SS5.SSS1.p1">
<p class="ltx_p" id="S1.SS5.SSS1.p1.1">In developing image generation models, a key component is requiring high image quality at large resolutions. In the context of AI generated images, typical requirements include images with sizes of at least 512 x 512, reduced blurriness and graininess, and a general lack of inconsistencies typically covered through a negative prompt - disfigured faces, malformed body parts (ex: a hand with six fingers), etc <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib21" title="">21</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib22" title="">22</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.5.2 </span>Alignment</h4>
<div class="ltx_para ltx_noindent" id="S1.SS5.SSS2.p1">
<p class="ltx_p" id="S1.SS5.SSS2.p1.1">Image generation models facilitate achieving prompt-output alignment in image generation models for art through attention scaling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib23" title="">23</a>]</cite>. If the base prompt is insufficiently detailed, the model may struggle to incorporate all desired subjects effectively, resulting in incomplete or imbalanced images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib24" title="">24</a>]</cite> . Determining the right combination of prompt elements is crucial to ensure comprehensive and coherent representation of the intended subjects. This involves fine-tuning the attention mechanisms within the model to adequately emphasize each aspect of the prompt. Addressing this issue is vital for producing high-quality, cohesive AI-generated art that faithfully reflects the userâ€™s intentions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.5.3 </span>Image Diversity</h4>
<div class="ltx_para ltx_noindent" id="S1.SS5.SSS3.p1">
<p class="ltx_p" id="S1.SS5.SSS3.p1.1">Consumers of generative models also typically look for control over image diversity, which refers to being able to create images with configurable amounts of variance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib25" title="">25</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib18" title="">18</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS5.SSS3.p2">
<p class="ltx_p" id="S1.SS5.SSS3.p2.1">During the start of the process, when users may be more focused on creating ideas, they typically seek to create a large number of images with higher variance to find a concept that they may seek to pursue. Typically further in the process, they then seek to curb variance to delve deeper and add further detail into an image previously selected during ideation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S1.SS5.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.5.4 </span>Consumer Friendliness</h4>
<div class="ltx_para ltx_noindent" id="S1.SS5.SSS4.p1">
<p class="ltx_p" id="S1.SS5.SSS4.p1.3">AI art users have a consistent and well defined defined hardware profile that often is significantly different from research assumptions of availability of high computational resources<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib26" title="">26</a>]</cite> . Rather, systems running AI art often contain GPUs with VRAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib27" title="">27</a>]</cite> ranging between <math alttext="8" class="ltx_Math" display="inline" id="S1.SS5.SSS4.p1.1.m1.1"><semantics id="S1.SS5.SSS4.p1.1.m1.1a"><mn id="S1.SS5.SSS4.p1.1.m1.1.1" xref="S1.SS5.SSS4.p1.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S1.SS5.SSS4.p1.1.m1.1b"><cn id="S1.SS5.SSS4.p1.1.m1.1.1.cmml" type="integer" xref="S1.SS5.SSS4.p1.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.SS5.SSS4.p1.1.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="S1.SS5.SSS4.p1.1.m1.1d">8</annotation></semantics></math> GB and <math alttext="24" class="ltx_Math" display="inline" id="S1.SS5.SSS4.p1.2.m2.1"><semantics id="S1.SS5.SSS4.p1.2.m2.1a"><mn id="S1.SS5.SSS4.p1.2.m2.1.1" xref="S1.SS5.SSS4.p1.2.m2.1.1.cmml">24</mn><annotation-xml encoding="MathML-Content" id="S1.SS5.SSS4.p1.2.m2.1b"><cn id="S1.SS5.SSS4.p1.2.m2.1.1.cmml" type="integer" xref="S1.SS5.SSS4.p1.2.m2.1.1">24</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.SS5.SSS4.p1.2.m2.1c">24</annotation><annotation encoding="application/x-llamapun" id="S1.SS5.SSS4.p1.2.m2.1d">24</annotation></semantics></math> GB, and RAM of about <math alttext="96" class="ltx_Math" display="inline" id="S1.SS5.SSS4.p1.3.m3.1"><semantics id="S1.SS5.SSS4.p1.3.m3.1a"><mn id="S1.SS5.SSS4.p1.3.m3.1.1" xref="S1.SS5.SSS4.p1.3.m3.1.1.cmml">96</mn><annotation-xml encoding="MathML-Content" id="S1.SS5.SSS4.p1.3.m3.1b"><cn id="S1.SS5.SSS4.p1.3.m3.1.1.cmml" type="integer" xref="S1.SS5.SSS4.p1.3.m3.1.1">96</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.SS5.SSS4.p1.3.m3.1c">96</annotation><annotation encoding="application/x-llamapun" id="S1.SS5.SSS4.p1.3.m3.1d">96</annotation></semantics></math> GB on the upper end.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.SS5.SSS4.p2">
<p class="ltx_p" id="S1.SS5.SSS4.p2.1">Additionally, demand in the industry for configurable AI art systems has been on the rise, where developers seek frameworks that can be easily customized to handle their organizationâ€™s unique usecases. Hence, we attempt to design MANTA with <span class="ltx_text ltx_font_italic" id="S1.SS5.SSS4.p2.1.1">complete model configurability in mind</span>, providing adapters to re-configure any LLM used within our work to an open source LLM or in-house LLM that can further be finetuned. Features such as large context lengths may be expensive for a commercial startups to sustain, hence we design our system with a focus on minimizing the tokens used <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib28" title="">28</a>]</cite>.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Adapters</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Adapters efficiently fine-tune models on specific tasks with minimal parameter changes, reducing
computational and storage requirements while maintaining similar performance to full fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib29" title="">29</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">However, an integral part of the image generation process is finding an appropriate foundational model checkpoints <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib30" title="">30</a>]</cite> to complement these adapters. Our research focuses on locating better checkpoints and obtaining Low-Rank adapters (LoRA) that most appropriately align with the prompt, while maintaining the popular approach within existing open-source communities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib33" title="">33</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Adapter composition has emerged as a crucial mechanism for enhancing the capabilities of foundational models across various applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib36" title="">36</a>]</cite> . In the image domain, combining LoRAs effectively enhances different tasksâ€”concepts, characters, poses, actions, and stylesâ€”together, yielding images of high fidelity that closely align with user specifications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib38" title="">38</a>]</cite>. Our approach advances this further by actively segmenting user prompts into distinct tasks and merging the appropriate adapters for each task.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Retrieval</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Retrieval-based methods, such as <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">retrieval-augmented generation</span> (RAG), significantly improve model responses by adding semantically similar texts from a vast external database <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib39" title="">39</a>]</cite>. These methods convert text to vector embeddings using text encoders, which are then ranked against a user prompt based on similarity metrics. Similarly, MANTA draws inspiration from RAG to encode adapters as vector embeddings. A core limitation to RAG is limited precision, retrieving distracting irrelevant documents. This leads to a â€needle- in-the-haystackâ€ problem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib40" title="">40</a>]</cite>, where more relevant documents are buried further down the list. We leverage recent advances in RAG, namely triplet-loss based searching techniques, to effectively combat this.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>MANTA Overview</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Adapter selection presents distinct challenges compared to existing methods for retrieving text-based documents, as outlined in Section 2. First, computing embeddings for adapters is a novel task, made more difficult without access to training datasets. Furthermore, in the context of image generation, user prompts often specify multiple highly fine-grained tasks. This challenge extends beyond retrieving relevant adapters relative to the entire user prompt, but also matching them with specific tasks within the prompt. Finally, composing multiple adapters can degrade image quality and inject foreign biases into the model. Our retrieval mechanism, using a novel similarity computation outlined below, addresses the challenges above.</p>
</div>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Our Method</h2>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="260" id="S3.F4.g1" src="extracted/5871043/Stylus2.0-Drawing.png" width="449"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S3.F4.6.1">MANTA</span> algorithm. The system consists of four stages - <span class="ltx_text ltx_font_bold" id="S3.F4.7.2">concept development</span>, <span class="ltx_text ltx_font_bold" id="S3.F4.8.3">checkpoint selection</span>, <span class="ltx_text ltx_font_bold" id="S3.F4.9.4">adapter selection</span>, and <span class="ltx_text ltx_font_bold" id="S3.F4.10.5"> refinement</span>. The output refinement procedure simply acts as a pass through for the time being, but serves as a location to insert alignment mechanisms.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The core MANTA system consists of 4 major processes - <span class="ltx_text ltx_font_bold" id="S3.p1.1.1">Structured Concept Development</span>, <span class="ltx_text ltx_font_bold" id="S3.p1.1.2">Detail Enrichment</span>, <span class="ltx_text ltx_font_bold" id="S3.p1.1.3">Strategic Adapter Selection</span>, and <span class="ltx_text ltx_font_bold" id="S3.p1.1.4">Output Refinement</span> <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S3.F4" title="Figure 4 â€£ 3 Our Method â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">In the following sections, we will further explain the processes within each of these steps.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Structured Concept Development</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In this process, we leverage LLMs to analyze a prompt, and segment the prompt into a concept with three attributes - a name, some details describing the concept, and the styles to generate this concept with. For example, if you are attempting to generate an image with the input prompt â€™alienâ€™, then:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">The Concept Name would be â€™alienâ€™</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Details could include: full body, alien creature, three heads, glowing torso, â€¦</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Styles could include: anime style, futuristic, â€¦</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Additionally, we are able to create priorities between the subjects within the prompt by classifying each concept we identify as a <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.1">main subject</span>, or a <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.2">supporting subject</span>. Main subjects are given extreme priority throughout the process, and are used to strategically determine core components of the workflow, such as the best checkpoint, and optimal adapters.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">An example of what this process would take would be an example prompt - â€™i.eâ€™, â€™a techno samurai warrior walking his cyberpunk dogâ€™, and return a dictionary mapping the <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.1">main</span> and <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.2">supporting</span> subjects.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p5">
<span class="ltx_ERROR undefined" id="S3.SS1.p5.1">{spverbatim}</span>
<p class="ltx_p" id="S3.SS1.p5.2">â€™mainâ€™: 
â€™nameâ€™: â€™techno samurai warriorâ€™,
â€™stylesâ€™: [],
â€™detailsâ€™: []
,
â€™supportâ€™: [

â€™nameâ€™: â€™cyberpunk dogâ€™,
â€™stylesâ€™: [],
â€™detailsâ€™: []

],
â€™imageâ€™: â€™stylesâ€™: [], â€™detailsâ€™: []</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1">In the example above, the techno samurai warrior acts as the main concept, with the cyberpunk dog acting as a support.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Benefits of Structured Concept Development</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">By using the Structured Concept Development framework to analyze prompts, we sought to provide the following benefits.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Systematic Variance Insertion</span>: As illustrated in the next section with <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.2">Detail Enhancement</span>, structured concept development provides an avenue to systematically insert controlled amounts of variance, either via checkpoint or adapter, and impact image diversity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">For example, while the artist may have a vague idea of what he wants to pursue in his mind, he would need to move forward with picture that is extremely vivid or well defined to create a high quality output. In practice, this means that the user may specify a short, vaguer prompt to the system and then iteratively obtain a clearer idea of what they want, and thus send lengthier, more comprehensive prompts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">Typically, that would require the user to feed a well defined concept, which can be achieved by using LLMs to additionally increase the details for the concept within the prompt.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p5.1.1">Easy Interpretability for LLMs:</span> We have found that using a loosely structured approach to analyzing the prompt may be more effective than a completely unstructured approach (i.e, extracting key words or topics out of the input prompt) to ensure the relative significance of the concept is preserved.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1">From structured concept development analysis of prompts, concepts can systematically developed by LLMs by simply asking these LLMs to â€™add similar and relevant detailsâ€™ to the list of existing details.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Detail Enhancement</h3>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S3.F5.g1" src="extracted/5871043/detail-enhancement.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Overview of the detail enhancement process. The prompt is analyzed into a main concept and a set of supporting concepts, and then each concept is individually processed through the LLM to come up with more details.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We seek to design our system in a way that users can quickly ideate towards a "first concept" image, and then further refine. One of the core challenges in AI image generation is coming up with a first image "idea", which may further by refined upon. Generally in these cases, we see users using vaguer prompts, hoping to tap into the modelâ€™s latent creativity to supplement them with ideas.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">However, this often leads to extremely vague images if the model isnâ€™t properly fine tuned for that specific idea. To address this, we enable the user to specify extremely vague prompts, and use LLMs to generate reasonable additional details based on the concept. The sample prompt is referenced here <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#A1.SS3.SSS1" title="A.3.1 Detail Enhancement â€£ A.3 Prompts â€£ Appendix A Appendix â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">A.3.1</span></a>
.
Some examples of detail enhancement output for the prompt previously mentioned is shown below, for the techno samurai warrior concept.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<span class="ltx_ERROR undefined" id="S3.SS3.p3.1">{spverbatim}</span>
<p class="ltx_p" id="S3.SS3.p3.2">[
â€™sleek metallic armorâ€™, â€™glowing neon blue circuitsâ€™, â€™retractable energy katanaâ€™, â€™cybernetic enhancementsâ€™,
â€™black visor helmetâ€™, â€™steel-toed combat bootsâ€™, â€™metallic plate gauntletsâ€™,
â€™reinforced synthetic leather waist armorâ€™,
â€¦,
]</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Checkpoint / Adapter Retrieval</h3>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S3.F6.g1" src="extracted/5871043/retrieval-mech.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Overview of the retrieval process for some retrieval data - checkpoints and adapter information in our case. After the detail enhancement process previously listed, concepts are formulated as multiple queries and embedded (embedding now shown for brevity). Alongside a negative query, triplet loss is computed to find the most relevant adapters. </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The checkpoint and adapter selection mechanisms are identical, so we discuss the checkpoint selection mechanism to illustrate how the overall retrieval process functions. See Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S3.F6" title="Figure 6 â€£ 3.4 Checkpoint / Adapter Retrieval â€£ 3 Our Method â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">6</span></a> for a diagram on the retrieval process.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.2">Within the checkpoint selection phase, we attempt to locate a set of relevant checkpoints that are sufficiently relevant to the core concepts inserted into the prompt, that is if there exist a set checkpoints <math alttext="C" class="ltx_Math" display="inline" id="S3.SS4.p2.1.m1.1"><semantics id="S3.SS4.p2.1.m1.1a"><mi id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.1.m1.1d">italic_C</annotation></semantics></math> that have a relevancy score beyond the set relevancy threshold, <math alttext="\omega_{c}" class="ltx_Math" display="inline" id="S3.SS4.p2.2.m2.1"><semantics id="S3.SS4.p2.2.m2.1a"><msub id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><mi id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml">Ï‰</mi><mi id="S3.SS4.p2.2.m2.1.1.3" xref="S3.SS4.p2.2.m2.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2">ğœ”</ci><ci id="S3.SS4.p2.2.m2.1.1.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">\omega_{c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.2.m2.1d">italic_Ï‰ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">Checkpoint Selection is divided into two core steps, <span class="ltx_text ltx_font_italic" id="S3.SS4.p3.1.1">document generation</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS4.p3.1.2">checkpoint retrieval</span>. We leverage Qdrant <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib41" title="">41</a>]</cite>, a state of the art vector database due to its strong interoperability with local and cloud deployments as the underlying vector database, which we insert to during the document generation, and query during retrieval.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Checkpoint Document Generation</h4>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">In this section, we discuss the process through which source documents for checkpoints are generated.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p2">
<p class="ltx_p" id="S3.SS4.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p2.1.1">Past Challenges</span>: The Stylus system attempted to create documents composed of titles, descriptions, and other user provided metadata as the primary point of search for source document construction.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p3">
<p class="ltx_p" id="S3.SS4.SSS1.p3.1">However, we find that we are able to achieve comparable or better performance by querying on the basis of image prompts. That is, we define the checkpoint retrieval problem as finding <span class="ltx_text ltx_font_italic" id="S3.SS4.SSS1.p3.1.1">prompts that articulate our main concept</span> in similar fashion / context to the user inputted prompt.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Checkpoint Document Retrieval</h4>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">To motivate our arrival at solely using image prompts for retrieval, we start back at the previous attempts of retrieval mechanism, starting from the predecessing system:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p2">
<p class="ltx_p" id="S3.SS4.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.p2.1.1">Iteration 1 - User Supplied Metadata</span>: Stylus provided a complete list of user metadata. However, this was extremely token-expensive computation. Descriptions would easily consume over <math alttext="500" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p2.1.m1.1"><semantics id="S3.SS4.SSS2.p2.1.m1.1a"><mn id="S3.SS4.SSS2.p2.1.m1.1.1" xref="S3.SS4.SSS2.p2.1.m1.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p2.1.m1.1b"><cn id="S3.SS4.SSS2.p2.1.m1.1.1.cmml" type="integer" xref="S3.SS4.SSS2.p2.1.m1.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p2.1.m1.1c">500</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS2.p2.1.m1.1d">500</annotation></semantics></math> words, and even with limiting the characters, often didnâ€™t reflect the checkpoints that were the most performant.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p3">
<p class="ltx_p" id="S3.SS4.SSS2.p3.1">An common example of this popped up when trying to generate animal based images. The concept would often include â€™dogâ€™, â€™catâ€™, and other creatures semantically related to animal. However, due to the composition of the platform we tested with - CivitAI - we found that this would often pull in checkpoints dedicated to the topic of â€™furriesâ€™ (humanoid-animal hybrids), which would then hijack the output image generation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p4">
<p class="ltx_p" id="S3.SS4.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.p4.1.1">Iteration 2 - Hybrid Keyword + Embedding Search</span>: To improve the results, we attempted to perform a coarse substring match filter on core concepts, followed up with an embedding based search to find the most relevant. This ran into the challenge of not being a general solution - while this was effective for general prompts involving â€™catsâ€™ and â€™dogsâ€™, extremely specific concepts like â€™white limousineâ€™ had a low likelihood of being in the filter.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p5">
<p class="ltx_p" id="S3.SS4.SSS2.p5.1">An attempted fix was to use a generalization prompt, to repeatedly generalize the concept into a more abstract version of the concept, i.e â€™carâ€™ to â€™vehicleâ€™, however this failed because of LLM unpredictibility - it often outputted words such as â€™transportationâ€™ that were more esoteric.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p6">
<p class="ltx_p" id="S3.SS4.SSS2.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.p6.1.1">Iteration 3 - Multi embedding Search on Prompts</span>: To improve performance while minimizing tokens processed, we performed a multi embedding search, using the concept mapping to split the image into smaller, isolated queries that are then fed through a triplet loss-like function, containing positive and negative queries.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p7">
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="context=\sum min(s(v^{i}_{+})-s(v^{i}_{-}),0.0)" class="ltx_Math" display="block" id="S3.Ex1.m1.2"><semantics id="S3.Ex1.m1.2a"><mrow id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml"><mrow id="S3.Ex1.m1.2.2.3" xref="S3.Ex1.m1.2.2.3.cmml"><mi id="S3.Ex1.m1.2.2.3.2" xref="S3.Ex1.m1.2.2.3.2.cmml">c</mi><mo id="S3.Ex1.m1.2.2.3.1" xref="S3.Ex1.m1.2.2.3.1.cmml">â¢</mo><mi id="S3.Ex1.m1.2.2.3.3" xref="S3.Ex1.m1.2.2.3.3.cmml">o</mi><mo id="S3.Ex1.m1.2.2.3.1a" xref="S3.Ex1.m1.2.2.3.1.cmml">â¢</mo><mi id="S3.Ex1.m1.2.2.3.4" xref="S3.Ex1.m1.2.2.3.4.cmml">n</mi><mo id="S3.Ex1.m1.2.2.3.1b" xref="S3.Ex1.m1.2.2.3.1.cmml">â¢</mo><mi id="S3.Ex1.m1.2.2.3.5" xref="S3.Ex1.m1.2.2.3.5.cmml">t</mi><mo id="S3.Ex1.m1.2.2.3.1c" xref="S3.Ex1.m1.2.2.3.1.cmml">â¢</mo><mi id="S3.Ex1.m1.2.2.3.6" xref="S3.Ex1.m1.2.2.3.6.cmml">e</mi><mo id="S3.Ex1.m1.2.2.3.1d" xref="S3.Ex1.m1.2.2.3.1.cmml">â¢</mo><mi id="S3.Ex1.m1.2.2.3.7" xref="S3.Ex1.m1.2.2.3.7.cmml">x</mi><mo id="S3.Ex1.m1.2.2.3.1e" xref="S3.Ex1.m1.2.2.3.1.cmml">â¢</mo><mi id="S3.Ex1.m1.2.2.3.8" xref="S3.Ex1.m1.2.2.3.8.cmml">t</mi></mrow><mo id="S3.Ex1.m1.2.2.2" rspace="0.111em" xref="S3.Ex1.m1.2.2.2.cmml">=</mo><mrow id="S3.Ex1.m1.2.2.1" xref="S3.Ex1.m1.2.2.1.cmml"><mo id="S3.Ex1.m1.2.2.1.2" movablelimits="false" xref="S3.Ex1.m1.2.2.1.2.cmml">âˆ‘</mo><mrow id="S3.Ex1.m1.2.2.1.1" xref="S3.Ex1.m1.2.2.1.1.cmml"><mi id="S3.Ex1.m1.2.2.1.1.3" xref="S3.Ex1.m1.2.2.1.1.3.cmml">m</mi><mo id="S3.Ex1.m1.2.2.1.1.2" xref="S3.Ex1.m1.2.2.1.1.2.cmml">â¢</mo><mi id="S3.Ex1.m1.2.2.1.1.4" xref="S3.Ex1.m1.2.2.1.1.4.cmml">i</mi><mo id="S3.Ex1.m1.2.2.1.1.2a" xref="S3.Ex1.m1.2.2.1.1.2.cmml">â¢</mo><mi id="S3.Ex1.m1.2.2.1.1.5" xref="S3.Ex1.m1.2.2.1.1.5.cmml">n</mi><mo id="S3.Ex1.m1.2.2.1.1.2b" xref="S3.Ex1.m1.2.2.1.1.2.cmml">â¢</mo><mrow id="S3.Ex1.m1.2.2.1.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.2.cmml"><mo id="S3.Ex1.m1.2.2.1.1.1.1.2" stretchy="false" xref="S3.Ex1.m1.2.2.1.1.1.2.cmml">(</mo><mrow id="S3.Ex1.m1.2.2.1.1.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.2.2.1.1.1.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.2.2.1.1.1.1.1.1.3" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.3.cmml">s</mi><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.1.2" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.2" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml">v</mi><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml">+</mo><mi id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.3" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msubsup><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.3" xref="S3.Ex1.m1.2.2.1.1.1.1.1.3.cmml">âˆ’</mo><mrow id="S3.Ex1.m1.2.2.1.1.1.1.1.2" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.cmml"><mi id="S3.Ex1.m1.2.2.1.1.1.1.1.2.3" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.3.cmml">s</mi><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.2.2" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.2.cmml">â¢</mo><mrow id="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.cmml"><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.2" stretchy="false" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.cmml">(</mo><msubsup id="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.cmml"><mi id="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.2.2" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.2.2.cmml">v</mi><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.3" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.3.cmml">âˆ’</mo><mi id="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.2.3" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.2.3.cmml">i</mi></msubsup><mo id="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.3" stretchy="false" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.Ex1.m1.2.2.1.1.1.1.3" xref="S3.Ex1.m1.2.2.1.1.1.2.cmml">,</mo><mn id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">0.0</mn><mo id="S3.Ex1.m1.2.2.1.1.1.1.4" stretchy="false" xref="S3.Ex1.m1.2.2.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.2b"><apply id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2"><eq id="S3.Ex1.m1.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2"></eq><apply id="S3.Ex1.m1.2.2.3.cmml" xref="S3.Ex1.m1.2.2.3"><times id="S3.Ex1.m1.2.2.3.1.cmml" xref="S3.Ex1.m1.2.2.3.1"></times><ci id="S3.Ex1.m1.2.2.3.2.cmml" xref="S3.Ex1.m1.2.2.3.2">ğ‘</ci><ci id="S3.Ex1.m1.2.2.3.3.cmml" xref="S3.Ex1.m1.2.2.3.3">ğ‘œ</ci><ci id="S3.Ex1.m1.2.2.3.4.cmml" xref="S3.Ex1.m1.2.2.3.4">ğ‘›</ci><ci id="S3.Ex1.m1.2.2.3.5.cmml" xref="S3.Ex1.m1.2.2.3.5">ğ‘¡</ci><ci id="S3.Ex1.m1.2.2.3.6.cmml" xref="S3.Ex1.m1.2.2.3.6">ğ‘’</ci><ci id="S3.Ex1.m1.2.2.3.7.cmml" xref="S3.Ex1.m1.2.2.3.7">ğ‘¥</ci><ci id="S3.Ex1.m1.2.2.3.8.cmml" xref="S3.Ex1.m1.2.2.3.8">ğ‘¡</ci></apply><apply id="S3.Ex1.m1.2.2.1.cmml" xref="S3.Ex1.m1.2.2.1"><sum id="S3.Ex1.m1.2.2.1.2.cmml" xref="S3.Ex1.m1.2.2.1.2"></sum><apply id="S3.Ex1.m1.2.2.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1"><times id="S3.Ex1.m1.2.2.1.1.2.cmml" xref="S3.Ex1.m1.2.2.1.1.2"></times><ci id="S3.Ex1.m1.2.2.1.1.3.cmml" xref="S3.Ex1.m1.2.2.1.1.3">ğ‘š</ci><ci id="S3.Ex1.m1.2.2.1.1.4.cmml" xref="S3.Ex1.m1.2.2.1.1.4">ğ‘–</ci><ci id="S3.Ex1.m1.2.2.1.1.5.cmml" xref="S3.Ex1.m1.2.2.1.1.5">ğ‘›</ci><interval closure="open" id="S3.Ex1.m1.2.2.1.1.1.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1"><apply id="S3.Ex1.m1.2.2.1.1.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1"><minus id="S3.Ex1.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.3"></minus><apply id="S3.Ex1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1"><times id="S3.Ex1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.2"></times><ci id="S3.Ex1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.3">ğ‘ </ci><apply id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.2">ğ‘£</ci><ci id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><plus id="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.1.1.1.1.3"></plus></apply></apply><apply id="S3.Ex1.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2"><times id="S3.Ex1.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.2"></times><ci id="S3.Ex1.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.3">ğ‘ </ci><apply id="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1">subscript</csymbol><apply id="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.2.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1">superscript</csymbol><ci id="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.2.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.2.2">ğ‘£</ci><ci id="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.2.3.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.2.3">ğ‘–</ci></apply><minus id="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.3.cmml" xref="S3.Ex1.m1.2.2.1.1.1.1.1.2.1.1.1.3"></minus></apply></apply></apply><cn id="S3.Ex1.m1.1.1.cmml" type="float" xref="S3.Ex1.m1.1.1">0.0</cn></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.2c">context=\sum min(s(v^{i}_{+})-s(v^{i}_{-}),0.0)</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.2d">italic_c italic_o italic_n italic_t italic_e italic_x italic_t = âˆ‘ italic_m italic_i italic_n ( italic_s ( italic_v start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT ) - italic_s ( italic_v start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT - end_POSTSUBSCRIPT ) , 0.0 )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p8">
<p class="ltx_p" id="S3.SS4.SSS2.p8.2">Where <math alttext="v^{i}_{+}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p8.1.m1.1"><semantics id="S3.SS4.SSS2.p8.1.m1.1a"><msubsup id="S3.SS4.SSS2.p8.1.m1.1.1" xref="S3.SS4.SSS2.p8.1.m1.1.1.cmml"><mi id="S3.SS4.SSS2.p8.1.m1.1.1.2.2" xref="S3.SS4.SSS2.p8.1.m1.1.1.2.2.cmml">v</mi><mo id="S3.SS4.SSS2.p8.1.m1.1.1.3" xref="S3.SS4.SSS2.p8.1.m1.1.1.3.cmml">+</mo><mi id="S3.SS4.SSS2.p8.1.m1.1.1.2.3" xref="S3.SS4.SSS2.p8.1.m1.1.1.2.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p8.1.m1.1b"><apply id="S3.SS4.SSS2.p8.1.m1.1.1.cmml" xref="S3.SS4.SSS2.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p8.1.m1.1.1.1.cmml" xref="S3.SS4.SSS2.p8.1.m1.1.1">subscript</csymbol><apply id="S3.SS4.SSS2.p8.1.m1.1.1.2.cmml" xref="S3.SS4.SSS2.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p8.1.m1.1.1.2.1.cmml" xref="S3.SS4.SSS2.p8.1.m1.1.1">superscript</csymbol><ci id="S3.SS4.SSS2.p8.1.m1.1.1.2.2.cmml" xref="S3.SS4.SSS2.p8.1.m1.1.1.2.2">ğ‘£</ci><ci id="S3.SS4.SSS2.p8.1.m1.1.1.2.3.cmml" xref="S3.SS4.SSS2.p8.1.m1.1.1.2.3">ğ‘–</ci></apply><plus id="S3.SS4.SSS2.p8.1.m1.1.1.3.cmml" xref="S3.SS4.SSS2.p8.1.m1.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p8.1.m1.1c">v^{i}_{+}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS2.p8.1.m1.1d">italic_v start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="v^{i}_{-}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p8.2.m2.1"><semantics id="S3.SS4.SSS2.p8.2.m2.1a"><msubsup id="S3.SS4.SSS2.p8.2.m2.1.1" xref="S3.SS4.SSS2.p8.2.m2.1.1.cmml"><mi id="S3.SS4.SSS2.p8.2.m2.1.1.2.2" xref="S3.SS4.SSS2.p8.2.m2.1.1.2.2.cmml">v</mi><mo id="S3.SS4.SSS2.p8.2.m2.1.1.3" xref="S3.SS4.SSS2.p8.2.m2.1.1.3.cmml">âˆ’</mo><mi id="S3.SS4.SSS2.p8.2.m2.1.1.2.3" xref="S3.SS4.SSS2.p8.2.m2.1.1.2.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p8.2.m2.1b"><apply id="S3.SS4.SSS2.p8.2.m2.1.1.cmml" xref="S3.SS4.SSS2.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p8.2.m2.1.1.1.cmml" xref="S3.SS4.SSS2.p8.2.m2.1.1">subscript</csymbol><apply id="S3.SS4.SSS2.p8.2.m2.1.1.2.cmml" xref="S3.SS4.SSS2.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p8.2.m2.1.1.2.1.cmml" xref="S3.SS4.SSS2.p8.2.m2.1.1">superscript</csymbol><ci id="S3.SS4.SSS2.p8.2.m2.1.1.2.2.cmml" xref="S3.SS4.SSS2.p8.2.m2.1.1.2.2">ğ‘£</ci><ci id="S3.SS4.SSS2.p8.2.m2.1.1.2.3.cmml" xref="S3.SS4.SSS2.p8.2.m2.1.1.2.3">ğ‘–</ci></apply><minus id="S3.SS4.SSS2.p8.2.m2.1.1.3.cmml" xref="S3.SS4.SSS2.p8.2.m2.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p8.2.m2.1c">v^{i}_{-}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS2.p8.2.m2.1d">italic_v start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT - end_POSTSUBSCRIPT</annotation></semantics></math> are positive and negative examples provided by us. In our experiments, we provided a standard negative embedding, combined with a positive embeddings from the concept map defined earlier.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p9">
<p class="ltx_p" id="S3.SS4.SSS2.p9.1">Specifically, for our checkpoint selection, we split our concept map into queries of the following format:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p10">
<span class="ltx_ERROR undefined" id="S3.SS4.SSS2.p10.1">{spverbatim}</span>
<p class="ltx_p" id="S3.SS4.SSS2.p10.2">QUERY FOR CONCEPT = concept_name + " " + concept_details + " " + concept_styles

This query is formulated for the main concept, and the support concepts are added as well to ensure a generally relevant checkpoint and LoRA. Additional query strategies and their corresponding results have been discussed within our Ablations section.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Listed below are the key components used for experimental setup, followed by the experimental procedure used for each individual setup.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Adapter solicitation.</span> Adapters were solicited from civitai.com via API, for which previous authors for the Stylus page created a dataset known as StylusDocs, containing 75K adapters for various Stable Diffusion models. These adapters were stored as embeddings alongside a JSON file containing textual content, which used to populate the Qdrant vector database collections for information on LoRA adapters, as well as checkpoint adapters.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">These vector databases were then quantized via INT8 scalar quantization in order to fit across RAM and 1 Tesla T4 GPU.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Base Image Generation Models.</span> The predecessing paper opted to use Stable Diffusion 1.5 models as their default, and for the sake of thoroughness and providing results relevant to today, we provide results tested with both Stable Diffusion 1.5, and SDXL, a newer standard that has begun to take over the AI art community.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">Base Large Language Models (LLM)s.</span> For the sake of thoroughness, we primarily relied on OpenAIâ€™s gpt-4o model for all LLM generations. However, we also leverage a tool known as liteLLM, which enables users to drop in any model they seek in place of OpenAI, such as Googleâ€™s Gemini API <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib42" title="">42</a>]</cite>, or a local Ollama deployment.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.1">Generation Setup.</span> In order to access Stable Diffusion models and create AI images in a consumer grade setting, the Automatic1111 stable diffusion web user interface was used as an API service. Two core operations were used from the API, which have both been listed below:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p7">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Generation from Prompt:</span> This method was used by Stylus to be able to create itâ€™s images. Internally, given a prompt and negative prompt, the method leveraged Automatic1111â€™s text-to-image generation interface to create images to share with the user.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Generation from Image:</span> This method was used by Stylus to refine existing images that used the <span class="ltx_text ltx_font_italic" id="S4.I1.i2.p1.1.2">Generation from Prompt</span> method previously to refine and improve images. The method leveraged Automatic1111â€™s img-to-img generation interface to refine images previously generated via text in the system.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p8">
<p class="ltx_p" id="S4.SS1.p8.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p8.1.1">Hardware</span>
To simulate the experience of the average AI art user, we sought to replicate their environment by leveraging similar hardware. To that extent, we altered our hardware setup to use the following:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p9">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">GPUs:</span> Under the belief that consumers tend to use lower end GPUs ranging from 8 GB to 24 GB on average, we opted for a single 16 GB VRAM Tesla T4 to conduct our experiments.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">CPUs: </span> 4 CPUs were used in the testing process</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.p1.1.1">RAM: </span> In order to simulate consumer low-budget conditions, consumer grade resources such as lower <math alttext="16" class="ltx_Math" display="inline" id="S4.I2.i3.p1.1.m1.1"><semantics id="S4.I2.i3.p1.1.m1.1a"><mn id="S4.I2.i3.p1.1.m1.1.1" xref="S4.I2.i3.p1.1.m1.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.1.m1.1b"><cn id="S4.I2.i3.p1.1.m1.1.1.cmml" type="integer" xref="S4.I2.i3.p1.1.m1.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.1.m1.1c">16</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i3.p1.1.m1.1d">16</annotation></semantics></math> GB RAM.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p10">
<p class="ltx_p" id="S4.SS1.p10.1">Due to these hardware constraints, only a single instance of the Automatic1111 API was able to be run at one time, in comparison to standard research environment hardware such as the A100, where 4-5 APIs with simultaneous models being provisioned.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experimental Procedures</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Automated Evaluation</h4>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">To simulate a standardized version of human preference, we continue to use the automated evaluation mechanism leveraging vision language models to judge groups of images.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">Specifically, the automated evaluation helps us obtain the primary basis of comparison for our understanding of the systemâ€™s capabilities in <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p2.1.1">image quality, image diversity, and alignment</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1">We ran automated evaluation on a group of <math alttext="500" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p3.1.m1.1"><semantics id="S4.SS2.SSS1.p3.1.m1.1a"><mn id="S4.SS2.SSS1.p3.1.m1.1.1" xref="S4.SS2.SSS1.p3.1.m1.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.1.m1.1b"><cn id="S4.SS2.SSS1.p3.1.m1.1.1.cmml" type="integer" xref="S4.SS2.SSS1.p3.1.m1.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.1.m1.1c">500</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p3.1.m1.1d">500</annotation></semantics></math> generation run on the COCO dataset, comparing the results between images generated by the original Stylus system, MANTA, and a normal, unaugmented-by-LoRAs Stable Diffusion model.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1">Below contain the model preference comparisons of MANTA versus the original Stylus and SD. In each evaluation, the model was fed a criterion for the three categories (diversity, image quality, and alignment), and then provided two sets of images - one from MANTA, and another from the second image generation system being compared.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p5">
<p class="ltx_p" id="S4.SS2.SSS1.p5.1">The vision language model was then asked to rate each of the images for that category, and based on the category, come up with a preference for either images contained within the MANTA group, or the basis of comparison. In the table <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4.T1" title="Table 1 â€£ 4.2.1 Automated Evaluation â€£ 4.2 Experimental Procedures â€£ 4 Results â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">1</span></a>, the total percentage of the times Stylus won has been recorded.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Automated Evaluation MANTA Win Percentage</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.2">Diversity</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.3">Image Quality</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.4">Alignment</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.2.1.1.1">
<span class="ltx_p" id="S4.T1.1.2.1.1.1.1" style="width:39.8pt;">Stylus</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.1.2">0.94</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.1.3">0.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.1.4">0.43</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.3.2.1.1">
<span class="ltx_p" id="S4.T1.1.3.2.1.1.1" style="width:39.8pt;">Base Stable Diffusion 1.5</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.3.2.2">0.85</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.3.2.3">0.75</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.3.2.4">0.51</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Human Evaluation</h4>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.3">Human evaluation was conducted over the same sample of <math alttext="500" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.1.m1.1"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><mn id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><cn id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS2.SSS2.p1.1.m1.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">500</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.1.m1.1d">500</annotation></semantics></math> runs on the COCO dataset prompts. <math alttext="100" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.2.m2.1"><semantics id="S4.SS2.SSS2.p1.2.m2.1a"><mn id="S4.SS2.SSS2.p1.2.m2.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.2.m2.1b"><cn id="S4.SS2.SSS2.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS2.SSS2.p1.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.2.m2.1c">100</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.2.m2.1d">100</annotation></semantics></math> of these outputs were selected for evaluation, and evaluated by <math alttext="4" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.3.m3.1"><semantics id="S4.SS2.SSS2.p1.3.m3.1a"><mn id="S4.SS2.SSS2.p1.3.m3.1.1" xref="S4.SS2.SSS2.p1.3.m3.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.3.m3.1b"><cn id="S4.SS2.SSS2.p1.3.m3.1.1.cmml" type="integer" xref="S4.SS2.SSS2.p1.3.m3.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.3.m3.1c">4</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.3.m3.1d">4</annotation></semantics></math> separate testers.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">Testers were shown a choice of a batch of images created using the original Stylus algorithm, the MANTA algorithm, and the respective base model. Human testers were requested to focus on image diversity and quality. In other words, subjects were asked to come up with a response to the questions below:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p3">
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1">Which set of images appears to be more diverse (varying in content, style, theme)?</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1">Of all the images shown, which image is the highest quality?</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p4">
<p class="ltx_p" id="S4.SS2.SSS2.p4.1">Based on these results, a <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4.F7" title="Figure 7 â€£ 4.2.2 Human Evaluation â€£ 4.2 Experimental Procedures â€£ 4 Results â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">7</span></a> has been compiled of human image preference on image diversity on the <math alttext="100" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p4.1.m1.1"><semantics id="S4.SS2.SSS2.p4.1.m1.1a"><mn id="S4.SS2.SSS2.p4.1.m1.1.1" xref="S4.SS2.SSS2.p4.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p4.1.m1.1b"><cn id="S4.SS2.SSS2.p4.1.m1.1.1.cmml" type="integer" xref="S4.SS2.SSS2.p4.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p4.1.m1.1c">100</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p4.1.m1.1d">100</annotation></semantics></math> COCO samples, as well human image quality preference.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="462" id="S4.F7.g1" src="extracted/5871043/human-preference.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Results of Human Preference, ranked for three methods - MANTA, Stylus, and base Stable Diffusion</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p5">
<p class="ltx_p" id="S4.SS2.SSS2.p5.4">Our human evaluation demonstrates a human preference percentage of <math alttext="55\%" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p5.1.m1.1"><semantics id="S4.SS2.SSS2.p5.1.m1.1a"><mrow id="S4.SS2.SSS2.p5.1.m1.1.1" xref="S4.SS2.SSS2.p5.1.m1.1.1.cmml"><mn id="S4.SS2.SSS2.p5.1.m1.1.1.2" xref="S4.SS2.SSS2.p5.1.m1.1.1.2.cmml">55</mn><mo id="S4.SS2.SSS2.p5.1.m1.1.1.1" xref="S4.SS2.SSS2.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p5.1.m1.1b"><apply id="S4.SS2.SSS2.p5.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p5.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.p5.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p5.1.m1.1.1.1">percent</csymbol><cn id="S4.SS2.SSS2.p5.1.m1.1.1.2.cmml" type="integer" xref="S4.SS2.SSS2.p5.1.m1.1.1.2">55</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p5.1.m1.1c">55\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p5.1.m1.1d">55 %</annotation></semantics></math> in image diversity, and <math alttext="58\%" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p5.2.m2.1"><semantics id="S4.SS2.SSS2.p5.2.m2.1a"><mrow id="S4.SS2.SSS2.p5.2.m2.1.1" xref="S4.SS2.SSS2.p5.2.m2.1.1.cmml"><mn id="S4.SS2.SSS2.p5.2.m2.1.1.2" xref="S4.SS2.SSS2.p5.2.m2.1.1.2.cmml">58</mn><mo id="S4.SS2.SSS2.p5.2.m2.1.1.1" xref="S4.SS2.SSS2.p5.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p5.2.m2.1b"><apply id="S4.SS2.SSS2.p5.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p5.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.p5.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.p5.2.m2.1.1.1">percent</csymbol><cn id="S4.SS2.SSS2.p5.2.m2.1.1.2.cmml" type="integer" xref="S4.SS2.SSS2.p5.2.m2.1.1.2">58</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p5.2.m2.1c">58\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p5.2.m2.1d">58 %</annotation></semantics></math> in image quality respectively for our MANTA algorithm, followed by a <math alttext="30\%" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p5.3.m3.1"><semantics id="S4.SS2.SSS2.p5.3.m3.1a"><mrow id="S4.SS2.SSS2.p5.3.m3.1.1" xref="S4.SS2.SSS2.p5.3.m3.1.1.cmml"><mn id="S4.SS2.SSS2.p5.3.m3.1.1.2" xref="S4.SS2.SSS2.p5.3.m3.1.1.2.cmml">30</mn><mo id="S4.SS2.SSS2.p5.3.m3.1.1.1" xref="S4.SS2.SSS2.p5.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p5.3.m3.1b"><apply id="S4.SS2.SSS2.p5.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p5.3.m3.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.p5.3.m3.1.1.1.cmml" xref="S4.SS2.SSS2.p5.3.m3.1.1.1">percent</csymbol><cn id="S4.SS2.SSS2.p5.3.m3.1.1.2.cmml" type="integer" xref="S4.SS2.SSS2.p5.3.m3.1.1.2">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p5.3.m3.1c">30\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p5.3.m3.1d">30 %</annotation></semantics></math> image diversity and <math alttext="28\%" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p5.4.m4.1"><semantics id="S4.SS2.SSS2.p5.4.m4.1a"><mrow id="S4.SS2.SSS2.p5.4.m4.1.1" xref="S4.SS2.SSS2.p5.4.m4.1.1.cmml"><mn id="S4.SS2.SSS2.p5.4.m4.1.1.2" xref="S4.SS2.SSS2.p5.4.m4.1.1.2.cmml">28</mn><mo id="S4.SS2.SSS2.p5.4.m4.1.1.1" xref="S4.SS2.SSS2.p5.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p5.4.m4.1b"><apply id="S4.SS2.SSS2.p5.4.m4.1.1.cmml" xref="S4.SS2.SSS2.p5.4.m4.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.p5.4.m4.1.1.1.cmml" xref="S4.SS2.SSS2.p5.4.m4.1.1.1">percent</csymbol><cn id="S4.SS2.SSS2.p5.4.m4.1.1.2.cmml" type="integer" xref="S4.SS2.SSS2.p5.4.m4.1.1.2">28</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p5.4.m4.1c">28\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p5.4.m4.1d">28 %</annotation></semantics></math> win rate for the Stylus algorithm <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4.F8" title="Figure 8 â€£ 4.2.2 Human Evaluation â€£ 4.2 Experimental Procedures â€£ 4 Results â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S4.F8.g1" src="extracted/5871043/first_cat_example.png" width="449"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>MANTA and base Stable Diffusion (SD) 1.5 outputs for the prompt <span class="ltx_text ltx_font_italic" id="S4.F8.2.1">A cat on top of a closed laptop on a desk</span>. There is clear variation in MANTA output, as opposed to extremely similar images from SD 1.5.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p6">
<p class="ltx_p" id="S4.SS2.SSS2.p6.1">Human Evaluators observed that the MANTA algorithm regularly provided more prompt-specific diversity. For example, in the figure above, there is a clear variation in style between the three images created by MANTA - the first appears to have a fictional look, the second scene solely featuring the cat in a splayed pose, and the third, with the cat peering into the computer. In the example below <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4.F9" title="Figure 9 â€£ 4.2.2 Human Evaluation â€£ 4.2 Experimental Procedures â€£ 4 Results â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">9</span></a>, we see similar diversity versus Stylus and base Stable Diffusion.</p>
</div>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="479" id="S4.F9.g1" src="extracted/5871043/diversity-stylus-example.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>MANTA, Stylus, and normal Stable Diffusion outputs for the prompt: A cement truck sitting next to a fence. MANTA clearly demonstrates diversity across vehicle construction, color, and background situation while maintaining coherent relevance with other concepts.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p7">
<p class="ltx_p" id="S4.SS2.SSS2.p7.1">Conversely, we can see clear similarities between the images from default Stable Diffusion - the cat always appears to look off into the distance, a complete computer is never shone, and the background always appears to be some form of white rather than scenery.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p8">
<p class="ltx_p" id="S4.SS2.SSS2.p8.1">On the other hand, alignment results seemed hard to confirm. The challenges we faced with alignment typically occured on complex prompts, where all 3 algorithms typically provided outputs that werenâ€™t very relevant to the prompt. Attached is a good example picked from the evaluation run, showing a better observance of the MANTA algorithm (following up from <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S1.F3" title="Figure 3 â€£ 1.4.2 Low Alignment â€£ 1.4 Retrieval Methods Limitations and Opportunities â€£ 1 Introduction â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="479" id="S4.F10.g1" src="extracted/5871043/alignment-results.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Example of an alignment output by the three algorithms - MANTA, Stylus, and base Stable Diffusion. Prompt: A stop sign that has the picture of George Bush in place of the letter O. As demonstrated, alignment is relatively low across all three images - none of the three are able to replace the letter â€™Oâ€™ with George Bush, but MANTA and Stylus do partially approach the prompt on 1 out of three images, with Stylus deviating further on that sample (inclusion of a rendition of the US). </figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Token Count Comparison</h4>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">As commercial models were used in the system, approximate token counts per run are shown in order to help users estimate usage costs. While LLM generations are becoming significantly cheaper, the Stylus image generation systems still require large investments of LLM tokens for high quality output; therefore, we seek to provide an overview of the avenues of improvement MANTA seeks to provide in this area.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1">In order to estimate LLM token count, we leveraged OpenAIâ€™s tiktoken package. We tracked token usage by maintaining a running counter, which would track the number of tokens after every call to an LLM with a prompt, or with a request to embed textual information.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1">Shown is a direct comparison of token counts from run using the previous Stylus iteration, versus the current run <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4.F11" title="Figure 11 â€£ 4.2.3 Token Count Comparison â€£ 4.2 Experimental Procedures â€£ 4 Results â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="220" id="S4.F11.g1" src="extracted/5871043/token-count.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Graph of token count comparisons in LLM usage per image generated between Stylus and MANTA.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p4">
<p class="ltx_p" id="S4.SS2.SSS3.p4.2">As demonstrated by the graph, MANTAâ€™s average token count (to 3 significant figures) of <math alttext="4500" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p4.1.m1.1"><semantics id="S4.SS2.SSS3.p4.1.m1.1a"><mn id="S4.SS2.SSS3.p4.1.m1.1.1" xref="S4.SS2.SSS3.p4.1.m1.1.1.cmml">4500</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p4.1.m1.1b"><cn id="S4.SS2.SSS3.p4.1.m1.1.1.cmml" type="integer" xref="S4.SS2.SSS3.p4.1.m1.1.1">4500</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p4.1.m1.1c">4500</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p4.1.m1.1d">4500</annotation></semantics></math> tokens provides approximately a <math alttext="40" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p4.2.m2.1"><semantics id="S4.SS2.SSS3.p4.2.m2.1a"><mn id="S4.SS2.SSS3.p4.2.m2.1.1" xref="S4.SS2.SSS3.p4.2.m2.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p4.2.m2.1b"><cn id="S4.SS2.SSS3.p4.2.m2.1.1.cmml" type="integer" xref="S4.SS2.SSS3.p4.2.m2.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p4.2.m2.1c">40</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p4.2.m2.1d">40</annotation></semantics></math> times decrease from the original algorithm.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p5">
<p class="ltx_p" id="S4.SS2.SSS3.p5.1">We believe this significant decrease stems from two separate places - pre-processing, and concept generation. The past iteration depended on passing extremely large quantities of user metadata during the pre-processing to embedding models, resulting in extremely large token usage.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p6">
<p class="ltx_p" id="S4.SS2.SSS3.p6.1">We replace user metadata about the checkpoints and adapters with example prompts users have previously used in the past with the weights, and attempt to query for weights which use concepts similarly in the prompt.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p7">
<p class="ltx_p" id="S4.SS2.SSS3.p7.1">Additionally, we remove the reliance Stylus previously maintained on using a <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS3.p7.1.1">concept tagging</span> system, where each adapter would be associated with a series of tags, and then queried against. In contrast, the concept mapping framework is used once on the prompt to find the core parts, and then those core parts are queried to find similar prompts.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Studies</h3>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Concept Enhancement</h4>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">For the purposes of a more direct comparison between MANTA and Stylus performance, weâ€™ve included an ablation detailing the comparison between MANTA. In this ablation, both image generation models were fed the exact enhancement prompts from COCO 2014 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib43" title="">43</a>]</cite>, and then evaluated using automated GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib44" title="">44</a>]</cite> evaluation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1">We obtained the following results, when testing generations with a sample size of <math alttext="N=15" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p2.1.m1.1"><semantics id="S4.SS3.SSS1.p2.1.m1.1a"><mrow id="S4.SS3.SSS1.p2.1.m1.1.1" xref="S4.SS3.SSS1.p2.1.m1.1.1.cmml"><mi id="S4.SS3.SSS1.p2.1.m1.1.1.2" xref="S4.SS3.SSS1.p2.1.m1.1.1.2.cmml">N</mi><mo id="S4.SS3.SSS1.p2.1.m1.1.1.1" xref="S4.SS3.SSS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS1.p2.1.m1.1.1.3" xref="S4.SS3.SSS1.p2.1.m1.1.1.3.cmml">15</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.1.m1.1b"><apply id="S4.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1"><eq id="S4.SS3.SSS1.p2.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1.1"></eq><ci id="S4.SS3.SSS1.p2.1.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1.2">ğ‘</ci><cn id="S4.SS3.SSS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS3.SSS1.p2.1.m1.1.1.3">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.1.m1.1c">N=15</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p2.1.m1.1d">italic_N = 15</annotation></semantics></math>, replicated using the Stylus Docs benchmark. For the purposes of exploration, we explored the various modes the previous Stylus system had, and their performances. Note that we werenâ€™t able to replicate the â€™re-rankerâ€™ mechanism due to repeated internal errors, and were forced to switch it off:</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>No-Concept Evaluation MANTA Win Percentage vs. Stylus</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.1.2">Diversity</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.1.3">Image Quality</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.1.4">Alignment</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.1.1.1">
<span class="ltx_p" id="S4.T2.1.2.1.1.1.1" style="width:28.5pt;">Stylus - Rank</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1.2">0.77</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1.3">0.94</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1.4">0.43</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.2.1.1">
<span class="ltx_p" id="S4.T2.1.3.2.1.1.1" style="width:28.5pt;">Stylus Random</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.3.2.2">0.92</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.3.2.3">0.89</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.3.2.4">0.31</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS1.p3">
<p class="ltx_p" id="S4.SS3.SSS1.p3.2">The table <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4.T2" title="Table 2 â€£ 4.3.1 Concept Enhancement â€£ 4.3 Ablation Studies â€£ 4 Results â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">2</span></a> demonstrates the significant but non-encompassing impact of prompt enhancement via the loose concept framework. While there was a notable decrease in the diversity (<math alttext="-17\%" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p3.1.m1.1"><semantics id="S4.SS3.SSS1.p3.1.m1.1a"><mrow id="S4.SS3.SSS1.p3.1.m1.1.1" xref="S4.SS3.SSS1.p3.1.m1.1.1.cmml"><mo id="S4.SS3.SSS1.p3.1.m1.1.1a" xref="S4.SS3.SSS1.p3.1.m1.1.1.cmml">âˆ’</mo><mrow id="S4.SS3.SSS1.p3.1.m1.1.1.2" xref="S4.SS3.SSS1.p3.1.m1.1.1.2.cmml"><mn id="S4.SS3.SSS1.p3.1.m1.1.1.2.2" xref="S4.SS3.SSS1.p3.1.m1.1.1.2.2.cmml">17</mn><mo id="S4.SS3.SSS1.p3.1.m1.1.1.2.1" xref="S4.SS3.SSS1.p3.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.1.m1.1b"><apply id="S4.SS3.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1"><minus id="S4.SS3.SSS1.p3.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1"></minus><apply id="S4.SS3.SSS1.p3.1.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.2"><csymbol cd="latexml" id="S4.SS3.SSS1.p3.1.m1.1.1.2.1.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.2.1">percent</csymbol><cn id="S4.SS3.SSS1.p3.1.m1.1.1.2.2.cmml" type="integer" xref="S4.SS3.SSS1.p3.1.m1.1.1.2.2">17</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.1.m1.1c">-17\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p3.1.m1.1d">- 17 %</annotation></semantics></math>) and minor increase quality (<math alttext="+4\%" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p3.2.m2.1"><semantics id="S4.SS3.SSS1.p3.2.m2.1a"><mrow id="S4.SS3.SSS1.p3.2.m2.1.1" xref="S4.SS3.SSS1.p3.2.m2.1.1.cmml"><mo id="S4.SS3.SSS1.p3.2.m2.1.1a" xref="S4.SS3.SSS1.p3.2.m2.1.1.cmml">+</mo><mrow id="S4.SS3.SSS1.p3.2.m2.1.1.2" xref="S4.SS3.SSS1.p3.2.m2.1.1.2.cmml"><mn id="S4.SS3.SSS1.p3.2.m2.1.1.2.2" xref="S4.SS3.SSS1.p3.2.m2.1.1.2.2.cmml">4</mn><mo id="S4.SS3.SSS1.p3.2.m2.1.1.2.1" xref="S4.SS3.SSS1.p3.2.m2.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.2.m2.1b"><apply id="S4.SS3.SSS1.p3.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1"><plus id="S4.SS3.SSS1.p3.2.m2.1.1.1.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1"></plus><apply id="S4.SS3.SSS1.p3.2.m2.1.1.2.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1.2"><csymbol cd="latexml" id="S4.SS3.SSS1.p3.2.m2.1.1.2.1.cmml" xref="S4.SS3.SSS1.p3.2.m2.1.1.2.1">percent</csymbol><cn id="S4.SS3.SSS1.p3.2.m2.1.1.2.2.cmml" type="integer" xref="S4.SS3.SSS1.p3.2.m2.1.1.2.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.2.m2.1c">+4\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p3.2.m2.1d">+ 4 %</annotation></semantics></math> ) win rate against Stylus Rank, we still see an remarkable win rate against Stylus, suggesting concept enhancement provided valuable improvement in diversity, but the system could still provide reliable image diversity in not used. We postulate that the gap comes primarily from checkpoint inductive bias - which we further explore in the next ablation study. By repeatedly using the same one model, we believe that the original Stylus system suffers from a significantly higher model-based bias, in comparison to a system that has the flexibility to alternatively consider from a sampled set of closely relevant checkpoints.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Base Checkpoint Variation</h4>
<figure class="ltx_figure" id="S4.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="479" id="S4.F12.g1" src="extracted/5871043/sdxl.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Images generated by Stylus with SDXL 1.0 as the checkpoint architecture.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">We attempted to obtain images from a second, larger AI model to understand their significance on MANTA output. Apart from the expected VRAM increase, we found multiple mounting challenges, such as the lack of checkpoint diversity and number of adapters for said checkpoints.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS2.p2">
<p class="ltx_p" id="S4.SS3.SSS2.p2.1">That being said, there were sufficient checkpoint-adapter pairings within the anime, heroes, and landscape area to test out the first version of the state of the art SDXL image generation model. The images were better, but the quality jump versus standard SD 1.5, which has a more robust ecosystem, was still minimal.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS2.p3">
<p class="ltx_p" id="S4.SS3.SSS2.p3.1">We hoped to test against the latest FLUX-series models released, however, our experimental setup (the Automatic 1111 Web Interface) doesnâ€™t have support for Flux-related architectures. We do expect to see a larger jump in image diversity via FLUX, as it supports significantly large prompt lengths; in our system, larger prompts result in more variance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Configuration Scale can systematically improves Diversity</h4>
<figure class="ltx_figure" id="S4.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="479" id="S4.F13.g1" src="extracted/5871043/lora_cfg.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Examples of images generated with MANTA with various CFG values. SD 1.5 is used as the base model in this case. Prompt: <span class="ltx_text ltx_font_italic" id="S4.F13.2.1">a girl playing on the beach</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">Traditionally, checkpoint users have used the <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p1.1.1">CFG scale</span> argument in order to control how closely a checkpoint model aligns with a given prompt, thus reducing image diversity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS3.p2">
<p class="ltx_p" id="S4.SS3.SSS3.p2.1">However, with the prompt enhancement through concept mapping, we witness trends of image diversity <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS3.p2.1.1">increasing</span> as the CFG scale increases. This is because we provide prompt-induced variance, which, when followed closely, generates variance in the images as much as requested by prompting.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS3.p3">
<p class="ltx_p" id="S4.SS3.SSS3.p3.1">Conversely, images from lower CFG values tend to approximate the modelâ€™s natural variance via generation. In other words, lower CFG images obtain the majority of the variance from the model itself, while high CFG models closely follow prompt variance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS3.p4">
<p class="ltx_p" id="S4.SS3.SSS3.p4.1">Ultimately, the choice of how much variance should one relagate to the model is left to the user. We do want to close this discussion by pointing out that prompt-based variance appears to be more than model induced variance, as compared between CFG 7 and CFG 4 images <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S4.F13" title="Figure 13 â€£ 4.3.3 Configuration Scale can systematically improves Diversity â€£ 4.3 Ablation Studies â€£ 4 Results â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">13</span></a>:</p>
<ul class="ltx_itemize" id="S4.I4">
<li class="ltx_item" id="S4.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I4.i1.p1">
<p class="ltx_p" id="S4.I4.i1.p1.1">CFG 7 images constantly vary their background scene, whereas CFG 4 consistently follows the theme of the beach</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S4.I4.i2.p1">
<p class="ltx_p" id="S4.I4.i2.p1.1">CFG 7 images demonstrate more significant changes in the characteristics of the main subject (the girl)</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S4.I4.i3.p1">
<p class="ltx_p" id="S4.I4.i3.p1.1">CFG 7 images also include handhold baskets, and more varying poses</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper on MANTA, we showcase a system that seeks to provide users with more command over image diversity and quality while retaining alignment. To tailor our system towards the broader audience of AI artists, we focus on a GPU + RAM memory efficient retrieval augmented generation system that delivers reasonable performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.3">As evidence of itâ€™s capabilities, we witness MANTA gaining nearly a <math alttext="50\%" class="ltx_Math" display="inline" id="S5.p2.1.m1.1"><semantics id="S5.p2.1.m1.1a"><mrow id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml"><mn id="S5.p2.1.m1.1.1.2" xref="S5.p2.1.m1.1.1.2.cmml">50</mn><mo id="S5.p2.1.m1.1.1.1" xref="S5.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><apply id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.p2.1.m1.1.1.1.cmml" xref="S5.p2.1.m1.1.1.1">percent</csymbol><cn id="S5.p2.1.m1.1.1.2.cmml" type="integer" xref="S5.p2.1.m1.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">50\%</annotation><annotation encoding="application/x-llamapun" id="S5.p2.1.m1.1d">50 %</annotation></semantics></math> image diversity and quality win rate versus a normal image generation Stable Diffusion 1.5 model, and Stylus. The improvements in image diversity and quality do come with a minor cost to alignment, with the system marginally outpacing a base Stable Diffusion model (<math alttext="51\%" class="ltx_Math" display="inline" id="S5.p2.2.m2.1"><semantics id="S5.p2.2.m2.1a"><mrow id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml"><mn id="S5.p2.2.m2.1.1.2" xref="S5.p2.2.m2.1.1.2.cmml">51</mn><mo id="S5.p2.2.m2.1.1.1" xref="S5.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><apply id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1"><csymbol cd="latexml" id="S5.p2.2.m2.1.1.1.cmml" xref="S5.p2.2.m2.1.1.1">percent</csymbol><cn id="S5.p2.2.m2.1.1.2.cmml" type="integer" xref="S5.p2.2.m2.1.1.2">51</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">51\%</annotation><annotation encoding="application/x-llamapun" id="S5.p2.2.m2.1d">51 %</annotation></semantics></math>) and coming up short with Stylus (<math alttext="43\%" class="ltx_Math" display="inline" id="S5.p2.3.m3.1"><semantics id="S5.p2.3.m3.1a"><mrow id="S5.p2.3.m3.1.1" xref="S5.p2.3.m3.1.1.cmml"><mn id="S5.p2.3.m3.1.1.2" xref="S5.p2.3.m3.1.1.2.cmml">43</mn><mo id="S5.p2.3.m3.1.1.1" xref="S5.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.3.m3.1b"><apply id="S5.p2.3.m3.1.1.cmml" xref="S5.p2.3.m3.1.1"><csymbol cd="latexml" id="S5.p2.3.m3.1.1.1.cmml" xref="S5.p2.3.m3.1.1.1">percent</csymbol><cn id="S5.p2.3.m3.1.1.2.cmml" type="integer" xref="S5.p2.3.m3.1.1.2">43</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.3.m3.1c">43\%</annotation><annotation encoding="application/x-llamapun" id="S5.p2.3.m3.1d">43 %</annotation></semantics></math>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">This iteration contains <math alttext="40\%" class="ltx_Math" display="inline" id="S5.p3.1.m1.1"><semantics id="S5.p3.1.m1.1a"><mrow id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml"><mn id="S5.p3.1.m1.1.1.2" xref="S5.p3.1.m1.1.1.2.cmml">40</mn><mo id="S5.p3.1.m1.1.1.1" xref="S5.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><apply id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1"><csymbol cd="latexml" id="S5.p3.1.m1.1.1.1.cmml" xref="S5.p3.1.m1.1.1.1">percent</csymbol><cn id="S5.p3.1.m1.1.1.2.cmml" type="integer" xref="S5.p3.1.m1.1.1.2">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">40\%</annotation><annotation encoding="application/x-llamapun" id="S5.p3.1.m1.1d">40 %</annotation></semantics></math> improvement in LLM token API usage and optimizations to drive down reliance over large amounts of data, paving the way for an eventual system that can be powered by completely open source LLM models without huge requirements for a large context length.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">This is done through a computationally efficient concept mapping structure, which systematically inserts configurable amounts variance into an image, which, as seen by the CFG variable can be tuned by the user to their usage. This system also provides an early example of a triplet loss-like mechanism for the document retrieval problem.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Consumer Optimizations</h3>
<div class="ltx_para ltx_noindent" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">This paper has placed a strong emphasis on consumer feasibility, and in that, the system has also made concessions in the process that can lead to marginal performance gains if turned off. Specifically, in our implementation, all vector embeddings are quantized to the INT8 format, and sparse embeddings are emphasized to prevent extreme RAM usage.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">Additionally, to further minimize token usage, we mapped each checkpoint or adapter to a single prompt. If multiple prompts were used, or multiple prompts were stacked in a single document, there are chances the results might be more relevant.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Next Steps</h3>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1 </span>Performance Improvements</h4>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.p1.1">The large areas of growth reside within the area of improved alignment. As illustrated by the Stylus paper in a CLIP vs FID Pareto curve, there exists a tradeoff between diversity and alignment â€“ highly diverse images likely have lower prompt-image alignment, which may be consequential in narrow use cases. Across both of the retrieval based methods we see so far (alignment</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS1.p2">
<p class="ltx_p" id="S6.SS2.SSS1.p2.1">We attempted to experiment with using VLMs as a "postprocessor", iterating on taking in images, running img2img, and then modifying the CFG value to improve said alignment. However, vision LLM based control wasnâ€™t effective due to two primary issues - subjectivity and lora controllability. Our postprocessor algorithm would prompt the VLM to come up with a rubric-based response to score where to improve, and then correct a set of attention weights (which would either bring something into relative focus, or relax its significance in the prompt) by multiplying it by a scaling factor. Unfortunately, the VLMâ€™s subjectivity led it to critically receive many concepts in the concept mapping framework as not showing up, which led to extremely large concept weights that eventually eroded visual fidelity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS1.p3">
<p class="ltx_p" id="S6.SS2.SSS1.p3.1">Another point of interest would be exploring various LoRA recommendation policies. Currently, the concept mapping framework boils down the name, detail, and styles of each concept, which are then queried against the database of adapters. While this leads to aggregate, concept-level control, it would be worthwhile to see if lower level control such as querying based on styles and details can provide even better improvements in image diversity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS1.p4">
<p class="ltx_p" id="S6.SS2.SSS1.p4.1">It would also be of interest to see if adding additional human-in-the-loop mechanisms could provide lower level control without too much additional human input. As the prompt and generation are closely powered by an LLM, humans could ask the LLM in a standard chat interface to "<span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p4.1.1">Generate photos of XYZ</span>", and then iteratively refine them through further img2img results. We also believe that while alignment would be a lofty goal to pursue, human intervention in small, iterative amounts would be a functionally adaquate solution for improving alignment results.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2 </span>Future Development</h4>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.p1.1">We have intentionally set up a budget efficient adapter system in order to ensure that other models can effectively be switched in. In future work, we hope to conduct evaluations on various standard open source LLMs being used in place, and testing a â€™completely open sourceâ€™ workflow.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS2.p2">
<p class="ltx_p" id="S6.SS2.SSS2.p2.1">Envisioning demand for further control of generation for niche tasks, we also hope to integrate more autonomy into finetuning if no information is provided, likely through determining criterion in which it would be optimal to create additional LoRAs to systematically ensure a concept is "well defined" within the scope of a generation. For example, if a sports game is sought to be replicated via AI images, such an algorithm might include LoRAs for essential parts like the tennis net, or the various court structures.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS2.p3">
<p class="ltx_p" id="S6.SS2.SSS2.p3.1">Finally, we look forward to a path where the system can cost-effectively become multimodal, being able to factor images and styles as validation for checkpoints previously used. This would enable the system to become closed-loop, as images can be generated, and then assigned a rating that would improve retrieval results.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Reproducibility</h3>
<div class="ltx_para ltx_noindent" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">We include steps here to reproduce key results that we have cited in our paper. As further attempts at reproduction are performed, we hope to improve this section as per their feedback.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">In order to reproduce evaluation over the main results, one can visit the link here: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://drive.google.com/file/d/1NhCmI05nYCNq4luWNvRddfKz3QNCwtIS/view?usp=sharing" title="">https://drive.google.com/file/d/1NhCmI05nYCNq4luWNvRddfKz3QNCwtIS/view?usp=sharing</a>. This link will contain a zip file to the generation run, set for <math alttext="500" class="ltx_Math" display="inline" id="S6.SS3.p2.1.m1.1"><semantics id="S6.SS3.p2.1.m1.1a"><mn id="S6.SS3.p2.1.m1.1.1" xref="S6.SS3.p2.1.m1.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.1.m1.1b"><cn id="S6.SS3.p2.1.m1.1.1.cmml" type="integer" xref="S6.SS3.p2.1.m1.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.1.m1.1c">500</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.1.m1.1d">500</annotation></semantics></math> different prompts on COCO 2014. Please download the zip file, and run the automated evaluation script in our codebase <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#S6.SS4" title="6.4 Code â€£ 6 Discussion â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">6.4</span></a> with the diversity, alignment, and quality parameters for the respective tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Code</h3>
<div class="ltx_para ltx_noindent" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">A publicly available repository for the Python code will be found on GitHub at the following link: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/AnshKetchum/stylus2" title="">https://github.com/AnshKetchum/stylus2</a>. We are still in the process of removing developer-centric debugging print statements, and other artifacts to make the codebase more readable and presentable.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank Michael Luo for his gracious time in providing constructive feedback, and perspective as the original author of the Stylus 1.0 paper. We also thank Saarthak Kapse from Stonybrook University for his kind guidance and domain-specific suggestions on additional ablations.</p>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Use cases</h3>
<div class="ltx_para ltx_noindent" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p1.1.1">AI Art.</span> We discuss some of the examples of how this system can be used to creatively come up with high quality, diverse images that may serve as starting points for further refinement. Our experiments do show case a clear usecase for MANTA in the AI art area.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p2">
<p class="ltx_p" id="A1.SS1.p2.1">We find that MANTA can generate stylistically diverse images that also feature characters in various poses and backgrounds, the results of which have been previously discussed. In particular, we find that the system is especially effective at creating images within the anime area, primarily due to the large amount of training data hosted on the platform in the niche.</p>
</div>
<div class="ltx_para" id="A1.SS1.p3">
<p class="ltx_p" id="A1.SS1.p3.1">Within this area of prompting, including closely associated concepts, even vague prompts resulted in high quality images. For example, a simple prompt such as "a man teaching a boy how to surf" came up with the variety shown <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#A1.F14" title="Figure 14 â€£ A.1 Use cases â€£ Appendix A Appendix â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">14</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="479" id="A1.F14.g1" src="extracted/5871043/ai-art-diversity.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>MANTA output for the prompt - "Teaching a boy how to surf". There is high variance across style and character action. </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A1.SS1.p4">
<p class="ltx_p" id="A1.SS1.p4.1">However, we find that while these images may be high in quality, these image leave enough room for additional improvement through inpainting, upscaling, high-resolution (hires), as well as touch ups such as face restoration.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p5">
<p class="ltx_p" id="A1.SS1.p5.1">For example, in the human case we found that parts of human body details in the image may be malformed. These are commonly the hands, fingers, nose, lips, or any parts that would seem like "small details" in an image but be immediately noticeable if incorrectly portrayed.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p6">
<p class="ltx_p" id="A1.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p6.1.1">Synthetic Data Generation.</span> We do see a strong use case of MANTA as a synthetic "data factory", which leverages checkpoints and LoRAs to create highly diverse images. We present some realistic example that may pass off with some minimal tolerance as a potential training data sample <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#A1.F15" title="Figure 15 â€£ A.1 Use cases â€£ Appendix A Appendix â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">15</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="240" id="A1.F15.g1" src="extracted/5871043/synthetic-data.jpg" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>MANTA output for a synthetic generation usecase involving furniture. Prompt: A bedroom with twin beds and linen. </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A1.SS1.p7">
<p class="ltx_p" id="A1.SS1.p7.1">We also present a simple procedure for any company interested in expanding their dataset with synthetic images using MANTA. Typically, AI models require images at a large scale - between 100K to 1M image on average - for state of the art performance, and we provide a procedure of how Stylus can be used to achieve these gains.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p8">
<p class="ltx_p" id="A1.SS1.p8.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p8.1.1">Synthetic Data Expansion Algorithm</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p9">
<ol class="ltx_enumerate" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para ltx_noindent" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1">Find a dataset <math alttext="D" class="ltx_Math" display="inline" id="A1.I1.i1.p1.1.m1.1"><semantics id="A1.I1.i1.p1.1.m1.1a"><mi id="A1.I1.i1.p1.1.m1.1.1" xref="A1.I1.i1.p1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="A1.I1.i1.p1.1.m1.1b"><ci id="A1.I1.i1.p1.1.m1.1.1.cmml" xref="A1.I1.i1.p1.1.m1.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i1.p1.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="A1.I1.i1.p1.1.m1.1d">italic_D</annotation></semantics></math> resembling the type and niche of your images. Common examples include COCO 2014 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib43" title="">43</a>]</cite>, Pascal VOC, Roboflow 100K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#bib.bib45" title="">45</a>]</cite>, etc.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.I1.i1.p2">
<p class="ltx_p" id="A1.I1.i1.p2.1"><span class="ltx_text ltx_font_italic" id="A1.I1.i1.p2.1.1">Note that these datasets must contain two requirements to be a viable option - images and a prompt "caption" for each image</span></p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.2">Pick a series of adapters <math alttext="A" class="ltx_Math" display="inline" id="A1.I1.i2.p1.1.m1.1"><semantics id="A1.I1.i2.p1.1.m1.1a"><mi id="A1.I1.i2.p1.1.m1.1.1" xref="A1.I1.i2.p1.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="A1.I1.i2.p1.1.m1.1b"><ci id="A1.I1.i2.p1.1.m1.1.1.cmml" xref="A1.I1.i2.p1.1.m1.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i2.p1.1.m1.1c">A</annotation><annotation encoding="application/x-llamapun" id="A1.I1.i2.p1.1.m1.1d">italic_A</annotation></semantics></math> and a set of checkpoints <math alttext="C" class="ltx_Math" display="inline" id="A1.I1.i2.p1.2.m2.1"><semantics id="A1.I1.i2.p1.2.m2.1a"><mi id="A1.I1.i2.p1.2.m2.1.1" xref="A1.I1.i2.p1.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="A1.I1.i2.p1.2.m2.1b"><ci id="A1.I1.i2.p1.2.m2.1.1.cmml" xref="A1.I1.i2.p1.2.m2.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i2.p1.2.m2.1c">C</annotation><annotation encoding="application/x-llamapun" id="A1.I1.i2.p1.2.m2.1d">italic_C</annotation></semantics></math> to be able to create data.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para ltx_noindent" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.4">Run Stylus on <math alttext="D" class="ltx_Math" display="inline" id="A1.I1.i3.p1.1.m1.1"><semantics id="A1.I1.i3.p1.1.m1.1a"><mi id="A1.I1.i3.p1.1.m1.1.1" xref="A1.I1.i3.p1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="A1.I1.i3.p1.1.m1.1b"><ci id="A1.I1.i3.p1.1.m1.1.1.cmml" xref="A1.I1.i3.p1.1.m1.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i3.p1.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="A1.I1.i3.p1.1.m1.1d">italic_D</annotation></semantics></math>, provided the adapter set <math alttext="A" class="ltx_Math" display="inline" id="A1.I1.i3.p1.2.m2.1"><semantics id="A1.I1.i3.p1.2.m2.1a"><mi id="A1.I1.i3.p1.2.m2.1.1" xref="A1.I1.i3.p1.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="A1.I1.i3.p1.2.m2.1b"><ci id="A1.I1.i3.p1.2.m2.1.1.cmml" xref="A1.I1.i3.p1.2.m2.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i3.p1.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="A1.I1.i3.p1.2.m2.1d">italic_A</annotation></semantics></math>, and checkpoint set <math alttext="C" class="ltx_Math" display="inline" id="A1.I1.i3.p1.3.m3.1"><semantics id="A1.I1.i3.p1.3.m3.1a"><mi id="A1.I1.i3.p1.3.m3.1.1" xref="A1.I1.i3.p1.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="A1.I1.i3.p1.3.m3.1b"><ci id="A1.I1.i3.p1.3.m3.1.1.cmml" xref="A1.I1.i3.p1.3.m3.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i3.p1.3.m3.1c">C</annotation><annotation encoding="application/x-llamapun" id="A1.I1.i3.p1.3.m3.1d">italic_C</annotation></semantics></math>. Obtain output coarse dataset of images <math alttext="D^{\prime}" class="ltx_Math" display="inline" id="A1.I1.i3.p1.4.m4.1"><semantics id="A1.I1.i3.p1.4.m4.1a"><msup id="A1.I1.i3.p1.4.m4.1.1" xref="A1.I1.i3.p1.4.m4.1.1.cmml"><mi id="A1.I1.i3.p1.4.m4.1.1.2" xref="A1.I1.i3.p1.4.m4.1.1.2.cmml">D</mi><mo id="A1.I1.i3.p1.4.m4.1.1.3" xref="A1.I1.i3.p1.4.m4.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="A1.I1.i3.p1.4.m4.1b"><apply id="A1.I1.i3.p1.4.m4.1.1.cmml" xref="A1.I1.i3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A1.I1.i3.p1.4.m4.1.1.1.cmml" xref="A1.I1.i3.p1.4.m4.1.1">superscript</csymbol><ci id="A1.I1.i3.p1.4.m4.1.1.2.cmml" xref="A1.I1.i3.p1.4.m4.1.1.2">ğ·</ci><ci id="A1.I1.i3.p1.4.m4.1.1.3.cmml" xref="A1.I1.i3.p1.4.m4.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i3.p1.4.m4.1c">D^{\prime}</annotation><annotation encoding="application/x-llamapun" id="A1.I1.i3.p1.4.m4.1d">italic_D start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT</annotation></semantics></math></p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="A1.I1.i4.p1">
<p class="ltx_p" id="A1.I1.i4.p1.2">Run finetuning to ensure that images in <math alttext="D^{\prime}" class="ltx_Math" display="inline" id="A1.I1.i4.p1.1.m1.1"><semantics id="A1.I1.i4.p1.1.m1.1a"><msup id="A1.I1.i4.p1.1.m1.1.1" xref="A1.I1.i4.p1.1.m1.1.1.cmml"><mi id="A1.I1.i4.p1.1.m1.1.1.2" xref="A1.I1.i4.p1.1.m1.1.1.2.cmml">D</mi><mo id="A1.I1.i4.p1.1.m1.1.1.3" xref="A1.I1.i4.p1.1.m1.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="A1.I1.i4.p1.1.m1.1b"><apply id="A1.I1.i4.p1.1.m1.1.1.cmml" xref="A1.I1.i4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.I1.i4.p1.1.m1.1.1.1.cmml" xref="A1.I1.i4.p1.1.m1.1.1">superscript</csymbol><ci id="A1.I1.i4.p1.1.m1.1.1.2.cmml" xref="A1.I1.i4.p1.1.m1.1.1.2">ğ·</ci><ci id="A1.I1.i4.p1.1.m1.1.1.3.cmml" xref="A1.I1.i4.p1.1.m1.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i4.p1.1.m1.1c">D^{\prime}</annotation><annotation encoding="application/x-llamapun" id="A1.I1.i4.p1.1.m1.1d">italic_D start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT</annotation></semantics></math> are reasonable additions to the data distribution of the previous dataset, <math alttext="D" class="ltx_Math" display="inline" id="A1.I1.i4.p1.2.m2.1"><semantics id="A1.I1.i4.p1.2.m2.1a"><mi id="A1.I1.i4.p1.2.m2.1.1" xref="A1.I1.i4.p1.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="A1.I1.i4.p1.2.m2.1b"><ci id="A1.I1.i4.p1.2.m2.1.1.cmml" xref="A1.I1.i4.p1.2.m2.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i4.p1.2.m2.1c">D</annotation><annotation encoding="application/x-llamapun" id="A1.I1.i4.p1.2.m2.1d">italic_D</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.I1.i4.p2">
<p class="ltx_p" id="A1.I1.i4.p2.1">A simple example of a <span class="ltx_text ltx_font_italic" id="A1.I1.i4.p2.1.1">postprocessing algorithm</span> could be as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.I1.i4.p3">
<span class="ltx_ERROR undefined" id="A1.I1.i4.p3.1">{spverbatim}</span>
<p class="ltx_p" id="A1.I1.i4.p3.2">def postprocess(D, Dâ€™):
set FID real images to D
for each generated_image_set in Dâ€™:
compute FID with the new image_set per caption</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.I1.i4.p4">
<p class="ltx_p" id="A1.I1.i4.p4.1">if FID &lt; threshold:
add to D</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Failure Modes</h3>
<div class="ltx_para ltx_noindent" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">In this section, we discuss various failure modes discovered while developing MANTA.</p>
</div>
<section class="ltx_subsubsection" id="A1.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.1 </span>Concept Overload.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS1.p1">
<p class="ltx_p" id="A1.SS2.SSS1.p1.1">In this situation, MANTA would over-focus on one concept, ignoring or omitting other concepts that would have been relevant. Oftentimes, we found this would happen when it would be challenging to pick a leading <span class="ltx_text ltx_font_italic" id="A1.SS2.SSS1.p1.1.1">main</span> concept. This led to a concept perpetually being subdued to the background or a corner of the image. In the case of latest iteration of MANTA, it would often ignore the secondary concept entirely, resulting in higher fidelity images at a heavy alignment penalty for ignoring a concept <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#A1.F16" title="Figure 16 â€£ A.2.1 Concept Overload. â€£ A.2 Failure Modes â€£ Appendix A Appendix â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">16</span></a>. The other algorithms attempted to superimpose the two concepts, leading to low quality images where the second object wasnâ€™t often created with the high quality of the first.</p>
</div>
<figure class="ltx_figure" id="A1.F16"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="479" id="A1.F16.g1" src="extracted/5871043/concept-overload.jpg" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Example of images for the prompt - <span class="ltx_text ltx_font_italic" id="A1.F16.3.1">A bathtub sits next to a ferris wheel</span>, from MANTA, Stylus, and base Stable Diffusion, separated by rows. The prompt demonstrates an example of <span class="ltx_text ltx_font_italic" id="A1.F16.4.2">concept overload</span>, where one concept is given overwhelming priority with a second concept marginalized or omitted. </figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.2 </span>Concept Relationship Misunderstanding.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS2.p1">
<p class="ltx_p" id="A1.SS2.SSS2.p1.1">Similar to the previous concept, this issue occurs when two concepts intersect. If the model doesnâ€™t seem to have experience understanding how to relate the two objects, it results in naive merger or low diversity output. An example of a failing prompt for this case is <span class="ltx_text ltx_font_italic" id="A1.SS2.SSS2.p1.1.1">A bear carries a pink ball by the river side</span> <a class="ltx_ref" href="https://arxiv.org/html/2409.14363v1#A1.F17" title="Figure 17 â€£ A.2.2 Concept Relationship Misunderstanding. â€£ A.2 Failure Modes â€£ Appendix A Appendix â€£ MANTA - Model Adapter Native generations thatâ€™s Affordable"><span class="ltx_text ltx_ref_tag">17</span></a>. While the output can be considered to be novel and artistically diverse, the diversity isnâ€™t sensibly generated - the interactions of the concepts arenâ€™t in the realm of possibility one would expect.</p>
</div>
<figure class="ltx_figure" id="A1.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="299" id="A1.F17.g1" src="extracted/5871043/concept-relationship.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>MANTA output for <span class="ltx_text ltx_font_italic" id="A1.F17.2.1">A bear carries a pink ball by the river side</span>. While the image does demonstrate diversity, the concept "ball" (the pink object wrapped around the bear) loses its meaning. </figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.3 </span>Adapter Gating.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS3.p1">
<p class="ltx_p" id="A1.SS2.SSS3.p1.1">In this situation, a lack of highly relevant adapters causes the retrieval adapters to produce results with weaker retrieval scores. The Stylus algorithm for LoRA selection can be described by the following pseudocode:</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS3.p2">
<span class="ltx_ERROR undefined" id="A1.SS2.SSS3.p2.1">{spverbatim}</span>
<p class="ltx_p" id="A1.SS2.SSS3.p2.2">def query_loras(prompt,
support_embeddings, k, init_thresh):
load adapters from database D
returned_loras = 
threshold = init_thresh</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS3.p3">
<p class="ltx_p" id="A1.SS2.SSS3.p3.1">while we do not have k returned_loras:
returned_loras = retrieve k adapters
returned_loras = filter out adapters &lt; threshold</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS3.p4">
<p class="ltx_p" id="A1.SS2.SSS3.p4.1">if not enough adapters:
threshold = 0.95 * threshold</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS3.p5">
<p class="ltx_p" id="A1.SS2.SSS3.p5.1">return returned_loras</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS3.p6">
<p class="ltx_p" id="A1.SS2.SSS3.p6.1">However, we experienced edge cases while testing with Stylus Docs, where the adapters returned would often be zero due to the lack of relevant adapters. This would cause cases where the checkpoint would often be limited by itâ€™s training knowledge of the concepts for task image diversity.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.4 </span>Rogue adapter / checkpoints.</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS4.p1">
<p class="ltx_p" id="A1.SS2.SSS4.p1.3">This problem is exacerbated by the fact that we have the capability to sample from two sets <math alttext="C" class="ltx_Math" display="inline" id="A1.SS2.SSS4.p1.1.m1.1"><semantics id="A1.SS2.SSS4.p1.1.m1.1a"><mi id="A1.SS2.SSS4.p1.1.m1.1.1" xref="A1.SS2.SSS4.p1.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS4.p1.1.m1.1b"><ci id="A1.SS2.SSS4.p1.1.m1.1.1.cmml" xref="A1.SS2.SSS4.p1.1.m1.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS4.p1.1.m1.1c">C</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS4.p1.1.m1.1d">italic_C</annotation></semantics></math> and <math alttext="A" class="ltx_Math" display="inline" id="A1.SS2.SSS4.p1.2.m2.1"><semantics id="A1.SS2.SSS4.p1.2.m2.1a"><mi id="A1.SS2.SSS4.p1.2.m2.1.1" xref="A1.SS2.SSS4.p1.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS4.p1.2.m2.1b"><ci id="A1.SS2.SSS4.p1.2.m2.1.1.cmml" xref="A1.SS2.SSS4.p1.2.m2.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS4.p1.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS4.p1.2.m2.1d">italic_A</annotation></semantics></math>, rather than one set of adapters <math alttext="A" class="ltx_Math" display="inline" id="A1.SS2.SSS4.p1.3.m3.1"><semantics id="A1.SS2.SSS4.p1.3.m3.1a"><mi id="A1.SS2.SSS4.p1.3.m3.1.1" xref="A1.SS2.SSS4.p1.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS4.p1.3.m3.1b"><ci id="A1.SS2.SSS4.p1.3.m3.1.1.cmml" xref="A1.SS2.SSS4.p1.3.m3.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS4.p1.3.m3.1c">A</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS4.p1.3.m3.1d">italic_A</annotation></semantics></math>. This problem still significantly perturbs our system, as there is no algorithm employed for ensuring high quality adapters. We uphold previous work implementations by providing manual guardrails through adapter/checkpoint blacklists and word based filters.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Prompts</h3>
<section class="ltx_subsubsection" id="A1.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.1 </span>Detail Enhancement</h4>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS1.p1">
<span class="ltx_ERROR undefined" id="A1.SS3.SSS1.p1.1">{spverbatim}</span>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS1.p2">
<p class="ltx_p" id="A1.SS3.SSS1.p2.1">"""You are helping a candidate create prompts for an image generation model.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS1.p3">
<p class="ltx_p" id="A1.SS3.SSS1.p3.1">Below is an input containing some sort of concept. The concept could be an entity, idea, noun, anything of that sort. Your job is to create a bulleted list of details that further make the idea more tangible. Think of things that the concept is composed of, and then provide me a list of n extremely specific details for that concept.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS1.p4">
<p class="ltx_p" id="A1.SS3.SSS1.p4.1">Here are some examples of extremely detailed responses:</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS1.p5">
<p class="ltx_p" id="A1.SS3.SSS1.p5.1">- Concept: anime</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS1.p6">
<p class="ltx_p" id="A1.SS3.SSS1.p6.1">Response: anime girl, white hair, red dress, thigh highs, slim chest</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS1.p7">
<p class="ltx_p" id="A1.SS3.SSS1.p7.1">- Concept: samurai</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS1.p8">
<p class="ltx_p" id="A1.SS3.SSS1.p8.1">Response: samurai robot warrior, large traditional straw hat,</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS1.p9">
<p class="ltx_p" id="A1.SS3.SSS1.p9.1">Note: Make sure the details directly add to the current character rather than creating new ones. Here is a bad example of details that create additional characters:</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS1.p10">
<p class="ltx_p" id="A1.SS3.SSS1.p10.1">Create a comma separated list of n extremely specific details that provide extremely vivid clarity to the concept. Do not include any verbs at the moment.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS1.p11">
<p class="ltx_p" id="A1.SS3.SSS1.p11.1">Concept: concept"""</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS1.p12">
<p class="ltx_p" id="A1.SS3.SSS1.p12.1">Above is an example of a version of the detail refinement prompt that we fed to an LLM to generate a list of <math alttext="n" class="ltx_Math" display="inline" id="A1.SS3.SSS1.p12.1.m1.1"><semantics id="A1.SS3.SSS1.p12.1.m1.1a"><mi id="A1.SS3.SSS1.p12.1.m1.1.1" xref="A1.SS3.SSS1.p12.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A1.SS3.SSS1.p12.1.m1.1b"><ci id="A1.SS3.SSS1.p12.1.m1.1.1.cmml" xref="A1.SS3.SSS1.p12.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.SSS1.p12.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="A1.SS3.SSS1.p12.1.m1.1d">italic_n</annotation></semantics></math> further details. Using this prompt, we can systematically increase or decrease specificity within the prompt.</p>
</div>
</section>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Jonathan Ho, Ajay Jain, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Rinon Gal, Yuval Alaluf, Yuval Atzmon, OrÂ Patashnik, AmitÂ H. Bermano, Gal Chechik, and Daniel Cohen-Or.

</span>
<span class="ltx_bibblock">An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
David Ha, Andrew Dai, and QuocÂ V. Le.

</span>
<span class="ltx_bibblock">Hypernetworks, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.

</span>
<span class="ltx_bibblock">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
AndrÃ© Bauer, Simon Trapp, Michael Stenger, Robert Leppich, Samuel Kounev, Mark Leznik, Kyle Chard, and Ian Foster.

</span>
<span class="ltx_bibblock">Comprehensive exploration of synthetic data generation: A survey, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Yuying Tang, Ningning Zhang, Mariana Ciancia, and Zhigang Wang.

</span>
<span class="ltx_bibblock">Exploring the impact of ai-generated image tools on professional and non-professional users in the art and design fields, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu.

</span>
<span class="ltx_bibblock">Omnigen: Unified image generation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Michael Luo, Justin Wong, Brandon Trabucco, Yanping Huang, JosephÂ E. Gonzalez, Zhifeng Chen, Ruslan Salakhutdinov, and Ion Stoica.

</span>
<span class="ltx_bibblock">Stylus: Automatic adapter selection for diffusion models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Cristina Silvano, Daniele Ielmini, Fabrizio Ferrandi, Leandro Fiorin, Serena Curzel, Luca Benini, Francesco Conti, Angelo Garofalo, Cristian Zambelli, Enrico Calore, SebastianoÂ Fabio Schifano, Maurizio Palesi, Giuseppe Ascia, Davide Patti, Nicola Petra, DavideÂ De Caro, Luciano Lavagno, Teodoro Urso, Valeria Cardellini, GianÂ Carlo Cardarilli, Robert Birke, and Stefania Perri.

</span>
<span class="ltx_bibblock">A survey on deep learning hardware accelerators for heterogeneous hpc platforms, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Junlin Wang, Siddhartha Jain, Dejiao Zhang, Baishakhi Ray, Varun Kumar, and Ben Athiwaratkun.

</span>
<span class="ltx_bibblock">Reasoning in token economies: Budget-aware evaluation of llm reasoning strategies, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Yuchao Gu, Xintao Wang, JayÂ Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, Yixiao Ge, Ying Shan, and MikeÂ Zheng Shou.

</span>
<span class="ltx_bibblock">Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
JooÂ Young Choi, JaesungÂ R. Park, Inkyu Park, Jaewoong Cho, Albert No, and ErnestÂ K. Ryu.

</span>
<span class="ltx_bibblock">Simple drop-in lora conditioning on attention layers will improve your diffusion model, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Yang Yang, Wen Wang, Liang Peng, Chaotian Song, Yao Chen, Hengjia Li, Xiaolong Yang, Qinglin Lu, Deng Cai, Boxi Wu, and Wei Liu.

</span>
<span class="ltx_bibblock">Lora-composer: Leveraging low-rank adaptation for multi-concept customization in training-free diffusion models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Spurthi Setty, Harsh Thakkar, Alyssa Lee, Eden Chung, and Natan Vidra.

</span>
<span class="ltx_bibblock">Improving retrieval for rag based question answering models on financial documents, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Jialin Dong, Bahare Fatemi, Bryan Perozzi, LinÂ F. Yang, and Anton Tsitsulin.

</span>
<span class="ltx_bibblock">Donâ€™t forget to connect! improving rag with graph-based reranking, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, and RomannÂ M. Weber.

</span>
<span class="ltx_bibblock">Cads: Unleashing the diversity of diffusion models through condition-annealed sampling, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Mariia Zameshina, Olivier Teytaud, and Laurent Najman.

</span>
<span class="ltx_bibblock">Diverse diffusion: Enhancing image diversity in text-to-image generation, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Shanchuan Lin and Xiao Yang.

</span>
<span class="ltx_bibblock">Diffusion model with perceptual loss, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Ji-Hoon Park, Yeong-Joon Ju, and Seong-Whan Lee.

</span>
<span class="ltx_bibblock">Explaining generative diffusion models via visual analysis for interpretable decision-making process.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Expert Systems with Applications</span>, 248:123231, August 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik.

</span>
<span class="ltx_bibblock">Diffusion model alignment using direct preference optimization, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Anna YooÂ Jeong Ha, Josephine Passananti, Ronik Bhaskar, Shawn Shan, Reid Southen, Haitao Zheng, and BenÂ Y. Zhao.

</span>
<span class="ltx_bibblock">Organic or diffused: Can we distinguish human art from ai-generated images?, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Barrett Lattimer, Patrick CHen, Xinyuan Zhang, and YiÂ Yang.

</span>
<span class="ltx_bibblock">Fast and accurate factual inconsistency detection over long documents.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</span>, page 1691â€“1703. Association for Computational Linguistics, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Alexis Newton and Kaustubh Dhole.

</span>
<span class="ltx_bibblock">Is ai art another industrial revolution in the making?, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki.

</span>
<span class="ltx_bibblock">Aligning text-to-image diffusion models with reward backpropagation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
MuhammadÂ Ferjad Naeem, SeongÂ Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo.

</span>
<span class="ltx_bibblock">Reliable fidelity and diversity metrics for generative models, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
u/Dear Ad-798.

</span>
<span class="ltx_bibblock">Advice on building a new pc for stable diffusion.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.reddit.com/r/StableDiffusion/comments/14snc10/advice_on_a_new_pc_build_specialized_for_stable/" title="">https://www.reddit.com/r/StableDiffusion/comments/14snc10/advice_on_a_new_pc_build_specialized_for_stable/</a>, 2023.

</span>
<span class="ltx_bibblock">[Accessed 26-08-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Zixuan Zhou, Xuefei Ning, KeÂ Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai, Xiao-Ping Zhang, Yuhan Dong, and YuÂ Wang.

</span>
<span class="ltx_bibblock">A survey on efficient inference for large language models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Konstantin Pilz, Lennart Heim, and Nicholas Brown.

</span>
<span class="ltx_bibblock">Increased compute efficiency and the diffusion of ai capabilities, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
EdwardÂ J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, LuÂ Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Justin Maier.

</span>
<span class="ltx_bibblock">Civitai: The home of open source generative ai, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan.

</span>
<span class="ltx_bibblock">Peft: State-of-the-art parameter-efficient fine-tuning methods.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/peft" title="">https://github.com/huggingface/peft</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, ThomasÂ L. Griffiths, Yuan Cao, and Karthik Narasimhan.

</span>
<span class="ltx_bibblock">Tree of thoughts: Deliberate problem solving with large language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi.

</span>
<span class="ltx_bibblock">Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas MÃ¼ller, Joe Penna, and Robin Rombach.

</span>
<span class="ltx_bibblock">Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, CristianÂ Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, PunitÂ Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, EricÂ Michael Smith, Ranjan Subramanian, XiaoqingÂ Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, JianÂ Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Nan Liu, Yilun Du, Shuang Li, JoshuaÂ B. Tenenbaum, and Antonio Torralba.

</span>
<span class="ltx_bibblock">Unsupervised compositional concepts discovery with text-to-image generative models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Multi-lora composition for image generation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, and Douwe Kiela.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky.

</span>
<span class="ltx_bibblock">Retrieval augmented generation or long-context llms? a comprehensive study and hybrid approach, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
JamesÂ Jie Pan, Jianguo Wang, and Guoliang Li.

</span>
<span class="ltx_bibblock">Survey of vector database management systems, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, and Johan Schalkwyk.

</span>
<span class="ltx_bibblock">Gemini: A family of highly capable multimodal models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C.Â Lawrence Zitnick, and Piotr DollÃ¡r.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, FlorenciaÂ Leoni Aleman, and Diogo.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Floriana Ciaglia, FrancescoÂ Saverio Zuppichini, Paul Guerrie, Mark McQuade, and Jacob Solawetz.

</span>
<span class="ltx_bibblock">Roboflow 100: A rich, multi-domain object detection benchmark, 2022.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Sep 22 08:36:09 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
