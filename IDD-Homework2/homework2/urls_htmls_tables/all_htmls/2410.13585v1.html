<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation</title>
<!--Generated on Wed Oct 16 22:24:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
" lang="en" name="keywords"/>
<base href="/html/2410.13585v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S1" title="In Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S2" title="In Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S3" title="In Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Framework</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S4" title="In Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Pseudo Multi-Camera Editing Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S5" title="In Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Experiment</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S5.SS1" title="In V Experiment ‣ Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S5.SS2" title="In V Experiment ‣ Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Result</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S6" title="In Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S7" title="In Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S8" title="In Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Acknowledgement</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\patchcmd</span><span class="ltx_ERROR undefined" id="p1.2">\@makecaption</span><span class="ltx_ERROR undefined" id="p1.3">\settototalheight</span>
<p class="ltx_p" id="p1.4">Xygp







</p>
</div>
<h1 class="ltx_title ltx_title_document">Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kuan-Ying Lee<sup class="ltx_sup" id="id1.1.id1">1</sup>, Qian Zhou<sup class="ltx_sup" id="id2.2.id2">2</sup>, Klara Nahrstedt<sup class="ltx_sup" id="id3.3.id3">1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><sup class="ltx_sup" id="id4.4.id1"><span class="ltx_text ltx_font_italic" id="id4.4.id1.1">1</span></sup><span class="ltx_text ltx_font_italic" id="id5.5.id2">University of Illinois, Urbana-Champaign
<br class="ltx_break"/><sup class="ltx_sup" id="id5.5.id2.1">2</sup>City University of Hong Kong
<br class="ltx_break"/></span>kylee5@illinois.edu, qiazhou@cityu.edu.hk, klara@illinois.edu

</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">Multi-camera systems are indispensable in movies, TV shows, and other media.
Selecting the appropriate camera at every timestamp has a decisive impact on production quality and audience preferences. Learning-based view recommendation frameworks can assist professionals in decision-making.
However, they often struggle outside of their training domains. The scarcity of labeled multi-camera view recommendation datasets exacerbates the issue.
Based on the insight that many videos are edited from the original multi-camera videos, we propose transforming regular videos into pseudo-labeled multi-camera view recommendation datasets.
Promisingly, by training the model on pseudo-labeled datasets stemming from videos in the target domain, we achieve a 68% relative improvement in the model’s accuracy in the target domain and bridge the accuracy gap between in-domain and never-before-seen domains.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
<span class="ltx_text" id="id7.id1" style="color:#000000;">cinematography, semi-supervised learning</span>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Multi-camera systems capturing the same scene provide different viewing perspectives and play significant roles in movies, broadcasts, news shows, etc. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib15" title="">15</a>]</cite>.
However, to benefit from multi-camera systems, expertise from cinematography professionals is heavily required. For instance, a video editor has to spend several hours watching pre-recorded videos from all cameras, determining which portions to use, and editing a single-track movie that best narrates the story.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Learning-based multi-camera view recommendation could assist professionals in choosing which camera to switch to by analyzing the past frames that have been selected, thus improving their efficiency.
To facilitate the development of learning-based multi-camera recommendation, Rao et al. propose a dataset called TV shows Multi-camera Editing Dataset (TVMCE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib12" title="">12</a>]</cite>, providing subsampled frames of multi-camera videos and the camera transitions determined by professional videographers.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<p class="ltx_p ltx_align_center" id="S1.F1.1"><span class="ltx_text" id="S1.F1.1.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="561" id="S1.F1.1.1.g1" src="x1.png" width="830"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
 (a) A model trained on a labeled multi-camera editing dataset of a particular domain generalizes poorly to a never-before-seen domain and the accuracy drops significantly.
(b) Our proposed method leverages regular videos to generate pseudo-labeled datasets for the target domain and improve the model’s accuracy. <span class="ltx_text ltx_font_bold" id="S1.F1.3.1">[Best viewed in color.]</span>
</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, videos in TVMCE are limited to a few scenes (stages, stadiums, and concerts) and specific types (broadcast or live stream).
We found that a multi-camera view recommendation model trained on TVMCE has issues generalizing to never-before-seen domains, and its accuracy drops significantly <span class="ltx_text" id="S1.p3.1.1" style="color:#000000;">(31.83 vs. 22.65)</span> when applied to other video scenes (e.g., living rooms) and/or types (e.g., movies).</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Collecting data from the same domain would be the most direct approach to solving the issue.
Yet, such requires multiple synchronized cameras capturing the same event, not to mention the dedicated cinematography expertise required for labeling.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">This paper proposes a methodology for generating pseudo-labeled multi-camera editing data from regular videos with shots, alleviating data scarcity.
With the proposed approach, sufficient data on a given domain (e.g., movie scenes in living rooms) could be obtained.
Our insights stem from two observations. (1) Many shot transitions within a regular video result from camera switches<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_text" id="footnote1.1" style="color:#000000;">Other transitions such as video effect and trimming could be filtered by heuristics or treated as noises.</span></span></span></span>.
Namely, videos are edited from their original multi-camera videos.
(2) In a multiple-camera system, cameras often remain stationary (extrinsics and intrinsics) and are usually responsible for shots of different scales.
Based on these two insights, we perform clustering on shots in a video to simulate different cameras and select the most visually similar shot from each camera as candidates alongside the ground truth to generate pseudo-labeled data.
A model trained on the proposed dataset enjoys a significant improvement in accuracy in the target domain (22.65 vs. 38.14<span class="ltx_text" id="S1.p5.1.1" style="color:#000000;">, cf. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S5.T3" title="TABLE III ‣ V-B Result ‣ V Experiment ‣ Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_tag">III</span></a></span>).
<span class="ltx_text ltx_font_bold" id="S1.p5.1.2">Our contributions</span> are summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We identify the poor domain generalizability of multi-camera view recommendation models.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose generating pseudo-labeled multi-camera editing datasets with regular videos to mitigate the lack of labeled data on an arbitrary domain.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">With the proposed pseudo-labeled multi-camera editing datasets, we achieve a 68% relative improvement in the model’s classification accuracy in the target domain. (<span class="ltx_text" id="S1.I1.i3.p1.1.1" style="color:#000000;">cf. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S5.T3" title="TABLE III ‣ V-B Result ‣ V Experiment ‣ Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_tag">III</span></a></span>).</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Many works have studied multi-camera view recommendation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib12" title="">12</a>]</cite>.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib19" title="">19</a>]</cite> designs two modules that collaboratively make switching decisions on a soccer game based on heuristics like object proximity and view duration.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib11" title="">11</a>]</cite> detects pre-defined events of interest in soccer games and leverages a scheduler to decide which view to broadcast.
These works design heuristics for a specific event or scene, limiting their applicability to other domains.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib8" title="">8</a>]</cite> trains a reinforcement agent to predict video attributes for retrieving the most appropriate view.
Similar to our work, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib3" title="">3</a>]</cite> leverages off-the-shelf videos as an auxiliary to augment the labeled dataset.
The works mentioned above, despite promising, require labeled multi-camera datasets in the target domain.
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib12" title="">12</a>]</cite> puts up a larger-scale multi-camera editing dataset to foster the growth of the sphere. Yet, the dataset is still limited to particular video styles and cannot be generalized to other domains.
Our work differs from the previous works in that we propose a method to transform regular videos of an arbitrary domain into pseudo-labeled multi-camera editing datasets and <span class="ltx_text ltx_font_italic" id="S2.p1.1.1">altogether bypass the need for human-labeled datasets in the target domain</span>.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Framework</span>
</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.p1.1.1">Problem Setup.</span>
This work aims to assist professionals in video editing by recommending camera tracks to switch to, considering the past shots that the professional has selected.
Note that we only focus on which track to switch to, but not whether to switch tracks because multi-camera view recommendation (1) is still nascent and (2) involves a certain amount of subjectivity.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.7">The task is formulated as follows: <math alttext="N" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">italic_N</annotation></semantics></math> temporally synchronized cameras capture the same scene from different angles, producing <math alttext="N" class="ltx_Math" display="inline" id="S3.p2.2.m2.1"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.1d">italic_N</annotation></semantics></math> video tracks.
Given a portion of the past video that has been edited from time <math alttext="T=s" class="ltx_Math" display="inline" id="S3.p2.3.m3.1"><semantics id="S3.p2.3.m3.1a"><mrow id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml"><mi id="S3.p2.3.m3.1.1.2" xref="S3.p2.3.m3.1.1.2.cmml">T</mi><mo id="S3.p2.3.m3.1.1.1" xref="S3.p2.3.m3.1.1.1.cmml">=</mo><mi id="S3.p2.3.m3.1.1.3" xref="S3.p2.3.m3.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><apply id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1"><eq id="S3.p2.3.m3.1.1.1.cmml" xref="S3.p2.3.m3.1.1.1"></eq><ci id="S3.p2.3.m3.1.1.2.cmml" xref="S3.p2.3.m3.1.1.2">𝑇</ci><ci id="S3.p2.3.m3.1.1.3.cmml" xref="S3.p2.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">T=s</annotation><annotation encoding="application/x-llamapun" id="S3.p2.3.m3.1d">italic_T = italic_s</annotation></semantics></math> to <math alttext="T=e" class="ltx_Math" display="inline" id="S3.p2.4.m4.1"><semantics id="S3.p2.4.m4.1a"><mrow id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mi id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2.cmml">T</mi><mo id="S3.p2.4.m4.1.1.1" xref="S3.p2.4.m4.1.1.1.cmml">=</mo><mi id="S3.p2.4.m4.1.1.3" xref="S3.p2.4.m4.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><eq id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1.1"></eq><ci id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2">𝑇</ci><ci id="S3.p2.4.m4.1.1.3.cmml" xref="S3.p2.4.m4.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">T=e</annotation><annotation encoding="application/x-llamapun" id="S3.p2.4.m4.1d">italic_T = italic_e</annotation></semantics></math> of length <math alttext="e-s" class="ltx_Math" display="inline" id="S3.p2.5.m5.1"><semantics id="S3.p2.5.m5.1a"><mrow id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml"><mi id="S3.p2.5.m5.1.1.2" xref="S3.p2.5.m5.1.1.2.cmml">e</mi><mo id="S3.p2.5.m5.1.1.1" xref="S3.p2.5.m5.1.1.1.cmml">−</mo><mi id="S3.p2.5.m5.1.1.3" xref="S3.p2.5.m5.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><apply id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1"><minus id="S3.p2.5.m5.1.1.1.cmml" xref="S3.p2.5.m5.1.1.1"></minus><ci id="S3.p2.5.m5.1.1.2.cmml" xref="S3.p2.5.m5.1.1.2">𝑒</ci><ci id="S3.p2.5.m5.1.1.3.cmml" xref="S3.p2.5.m5.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">e-s</annotation><annotation encoding="application/x-llamapun" id="S3.p2.5.m5.1d">italic_e - italic_s</annotation></semantics></math>, the task is to decide which track to switch to at the coming time <math alttext="T=t" class="ltx_Math" display="inline" id="S3.p2.6.m6.1"><semantics id="S3.p2.6.m6.1a"><mrow id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml"><mi id="S3.p2.6.m6.1.1.2" xref="S3.p2.6.m6.1.1.2.cmml">T</mi><mo id="S3.p2.6.m6.1.1.1" xref="S3.p2.6.m6.1.1.1.cmml">=</mo><mi id="S3.p2.6.m6.1.1.3" xref="S3.p2.6.m6.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><apply id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1"><eq id="S3.p2.6.m6.1.1.1.cmml" xref="S3.p2.6.m6.1.1.1"></eq><ci id="S3.p2.6.m6.1.1.2.cmml" xref="S3.p2.6.m6.1.1.2">𝑇</ci><ci id="S3.p2.6.m6.1.1.3.cmml" xref="S3.p2.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">T=t</annotation><annotation encoding="application/x-llamapun" id="S3.p2.6.m6.1d">italic_T = italic_t</annotation></semantics></math>, where <math alttext="t-e" class="ltx_Math" display="inline" id="S3.p2.7.m7.1"><semantics id="S3.p2.7.m7.1a"><mrow id="S3.p2.7.m7.1.1" xref="S3.p2.7.m7.1.1.cmml"><mi id="S3.p2.7.m7.1.1.2" xref="S3.p2.7.m7.1.1.2.cmml">t</mi><mo id="S3.p2.7.m7.1.1.1" xref="S3.p2.7.m7.1.1.1.cmml">−</mo><mi id="S3.p2.7.m7.1.1.3" xref="S3.p2.7.m7.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.1b"><apply id="S3.p2.7.m7.1.1.cmml" xref="S3.p2.7.m7.1.1"><minus id="S3.p2.7.m7.1.1.1.cmml" xref="S3.p2.7.m7.1.1.1"></minus><ci id="S3.p2.7.m7.1.1.2.cmml" xref="S3.p2.7.m7.1.1.2">𝑡</ci><ci id="S3.p2.7.m7.1.1.3.cmml" xref="S3.p2.7.m7.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.1c">t-e</annotation><annotation encoding="application/x-llamapun" id="S3.p2.7.m7.1d">italic_t - italic_e</annotation></semantics></math> could be variable.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.2">As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S3.F2" title="Figure 2 ‣ III Framework ‣ Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_tag">2</span></a> (a), 16 sub-sampled past selected frames (one out of every five frames) are given as input to the model.
The output is which of the six cameras should be switched to <math alttext="N_{F}" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><msub id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mi id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">N</mi><mi id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">𝑁</ci><ci id="S3.p3.1.m1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">N_{F}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">italic_N start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT</annotation></semantics></math> frames away. Note that <math alttext="N_{F}" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><msub id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml"><mi id="S3.p3.2.m2.1.1.2" xref="S3.p3.2.m2.1.1.2.cmml">N</mi><mi id="S3.p3.2.m2.1.1.3" xref="S3.p3.2.m2.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><apply id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p3.2.m2.1.1.1.cmml" xref="S3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.p3.2.m2.1.1.2.cmml" xref="S3.p3.2.m2.1.1.2">𝑁</ci><ci id="S3.p3.2.m2.1.1.3.cmml" xref="S3.p3.2.m2.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">N_{F}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">italic_N start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT</annotation></semantics></math> could be variable.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="167" id="S3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Model Architecture.
(a) The past encoder encodes all past features to a single feature vector.
Then, a contrastive loss is applied to maximize the cosine similarity between the past and ground-truth features.
(b) The feature extractor encodes a frame by adding a positional embedding to the image feature.
<span class="ltx_text" id="S3.F2.5.1" style="position:relative; bottom:-1.9pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="235" id="S3.F2.5.1.g1" src="x5.png" width="197"/></span>: trainable, <span class="ltx_text" id="S3.F2.6.2" style="position:relative; bottom:-1.9pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="235" id="S3.F2.6.2.g1" src="x4.png" width="218"/></span>: frozen. GT: groundtruth.
The number F<math alttext="{}_{\textit{N}}" class="ltx_Math" display="inline" id="S3.F2.7.m1.1"><semantics id="S3.F2.7.m1.1b"><msub id="S3.F2.7.m1.1.1" xref="S3.F2.7.m1.1.1.cmml"><mi id="S3.F2.7.m1.1.1b" xref="S3.F2.7.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_italic" id="S3.F2.7.m1.1.1.1" xref="S3.F2.7.m1.1.1.1a.cmml">N</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.F2.7.m1.1c"><apply id="S3.F2.7.m1.1.1.cmml" xref="S3.F2.7.m1.1.1"><ci id="S3.F2.7.m1.1.1.1a.cmml" xref="S3.F2.7.m1.1.1.1"><mtext class="ltx_mathvariant_italic" id="S3.F2.7.m1.1.1.1.cmml" mathsize="70%" xref="S3.F2.7.m1.1.1.1">N</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.7.m1.1d">{}_{\textit{N}}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.7.m1.1e">start_FLOATSUBSCRIPT N end_FLOATSUBSCRIPT</annotation></semantics></math> indicates the <math alttext="N^{th}" class="ltx_Math" display="inline" id="S3.F2.8.m2.1"><semantics id="S3.F2.8.m2.1b"><msup id="S3.F2.8.m2.1.1" xref="S3.F2.8.m2.1.1.cmml"><mi id="S3.F2.8.m2.1.1.2" xref="S3.F2.8.m2.1.1.2.cmml">N</mi><mrow id="S3.F2.8.m2.1.1.3" xref="S3.F2.8.m2.1.1.3.cmml"><mi id="S3.F2.8.m2.1.1.3.2" xref="S3.F2.8.m2.1.1.3.2.cmml">t</mi><mo id="S3.F2.8.m2.1.1.3.1" xref="S3.F2.8.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.F2.8.m2.1.1.3.3" xref="S3.F2.8.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.F2.8.m2.1c"><apply id="S3.F2.8.m2.1.1.cmml" xref="S3.F2.8.m2.1.1"><csymbol cd="ambiguous" id="S3.F2.8.m2.1.1.1.cmml" xref="S3.F2.8.m2.1.1">superscript</csymbol><ci id="S3.F2.8.m2.1.1.2.cmml" xref="S3.F2.8.m2.1.1.2">𝑁</ci><apply id="S3.F2.8.m2.1.1.3.cmml" xref="S3.F2.8.m2.1.1.3"><times id="S3.F2.8.m2.1.1.3.1.cmml" xref="S3.F2.8.m2.1.1.3.1"></times><ci id="S3.F2.8.m2.1.1.3.2.cmml" xref="S3.F2.8.m2.1.1.3.2">𝑡</ci><ci id="S3.F2.8.m2.1.1.3.3.cmml" xref="S3.F2.8.m2.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.8.m2.1d">N^{th}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.8.m2.1e">italic_N start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> frame from the video.
<span class="ltx_text ltx_font_bold" id="S3.F2.10.3">[Best viewed in color.]</span>
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.p4">
<p class="ltx_p" id="S3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.p4.1.1">Model Architecture.</span>
As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S3.F2" title="Figure 2 ‣ III Framework ‣ Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_tag">2</span></a>(a), the model consists of two main modules: a <span class="ltx_text ltx_font_italic" id="S3.p4.1.2">feature extractor</span> encoding individual frames and their corresponding metadata into a single vector, and a <span class="ltx_text ltx_font_italic" id="S3.p4.1.3">past encoder</span> aggregating all past features into a learnable latent vector through layer(s) of self-attention.
This latent vector encapsulates holistic information from all the selected frames in the near past, such as visual cues and transitions between past frames.
In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S3.F2" title="Figure 2 ‣ III Framework ‣ Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_tag">2</span></a>(b), the feature extractor has two inputs: the frame and its corresponding frame offset to the candidate frames.
A Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib9" title="">9</a>]</cite> pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib5" title="">5</a>]</cite> encodes the image into an image vector.
A positional embedding encodes how distant the input frame is from the candidate frames. For instance, in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S3.F2" title="Figure 2 ‣ III Framework ‣ Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_tag">2</span></a>(b), the frame offset between the most recent past frame and the candidate frames is (327 - 299) = 28. We use sine and cosine functions of different frequencies as the positional embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib17" title="">17</a>]</cite>.
We only train the past encoder and the latent vector.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p5">
<p class="ltx_p" id="S3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.p5.1.1">Training Objectives.</span>
Following previous work on self-supervised feature learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib4" title="">4</a>]</cite>, we optimize the model by InfoNCE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib10" title="">10</a>]</cite> and maximize the cosine similarity between the past feature and the ground-truth candidate feature while minimizing the cosine similarity between the past feature and all the other candidate features.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="457" id="S3.F3.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
<span class="ltx_text ltx_font_bold" id="S3.F3.6.1">Pseudo Dataset Generation Pipeline.</span> (a) Shots are detected in the input video, and
(b) clustered into groups. Shots within the same cluster are regarded as from the same “pseudo” camera.
(c) A shot is selected as an <span class="ltx_text" id="S3.F3.7.2" style="color:#5BBAF0;">anchor</span>. The succeeding shot is the <span class="ltx_text" id="S3.F3.8.3" style="color:#00E000;">ground truth</span>, while the most similar shot amongst each of the other N-1 pseudo cameras is chosen as a <span class="ltx_text" id="S3.F3.9.4" style="color:#FF0000;">candidate</span>. <span class="ltx_text ltx_font_bold" id="S3.F3.10.5">[Best viewed in color with zoom-in.]</span>
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Pseudo Multi-Camera Editing Dataset</span>
</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">As mentioned in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S1" title="I Introduction ‣ Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_tag">I</span></a>, to deal with the scarcity of labeled multi-camera editing data, we propose transforming regular videos in the target domain to pseudo-labeled multi-camera editing data.
This section demonstrates the detailed procedure of transforming regular videos into pseudo-labeled multi-camera editing datasets.
Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S3.F3" title="Figure 3 ‣ III Framework ‣ Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates creating a pseudo dataset.
First, we detect shots in a video and create pseudo-cameras by clustering the shots. Finally, we select candidates from each pseudo-camera (cluster) to construct an instance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">Camera Switch.</span>
Our insight stems from the observation that
when a professional is editing multi-camera videos, a shot boundary is when they decide to switch from one camera to another.
Though we cannot access the original multi-camera videos, the final video carries partial supervision from which the model could learn.
We first perform shot detection on a given video and obtain <math alttext="N_{S}" class="ltx_Math" display="inline" id="S4.p2.1.m1.1"><semantics id="S4.p2.1.m1.1a"><msub id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">N</mi><mi id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1">subscript</csymbol><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">𝑁</ci><ci id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">N_{S}</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.m1.1d">italic_N start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math> shots.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Pseudo Camera Label Generation.</span>
We made two observations. First, in a multi-camera system, each camera tends to remain stationary and keeps its perspective.
Second, cameras are usually responsible for shots of different scales.
Utilizing the insight that <span class="ltx_text ltx_font_italic" id="S4.p3.1.2">shot scale of each camera tends to stay unchanged</span>, we train a Temporal Segment Network (TSN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib18" title="">18</a>]</cite> for shot type classification on MovieShots <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib13" title="">13</a>]</cite> to extract features for each shot.
Then, we obtain six clusters by running K-Means on the features. Each cluster is treated as a pseudo-camera.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.p4.1.1">Pseudo Instance Construction.</span>
After assigning a pseudo-camera label to every video shot, we select the most visually similar shot from each pseudo-camera as the candidates alongside the ground truth, as in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S3.F3" title="Figure 3 ‣ III Framework ‣ Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_tag">3</span></a>.
We use a ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib7" title="">7</a>]</cite> pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib5" title="">5</a>]</cite> to extract image features for the first and last frames in each shot and normalize the features to unit length.
Then, we compute the cosine similarity of the previous shot’s last-frame feature to the next shot’s first-frame feature as the visual similarity between the two shots.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Experiment</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.4.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">Setup</span>
</h3>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Datasets.</span>
We leverage four datasets: <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.2">TVMCE</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib12" title="">12</a>]</cite> and three pseudo datasets created from <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.3">ClipShots</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib16" title="">16</a>]</cite>, <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.4">Condensed Movies</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib1" title="">1</a>]</cite>, and <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.5">Sitcoms</span> episodes.
<span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.6">TVMCE</span> consists of 6 synchronized camera tracks, totaling 88 hours of recorded videos.
The professionals edit the multi-camera videos into a single-track video, providing 5133 ground-truth camera transition labels.
Following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib12" title="">12</a>]</cite>, 4042 and 1091 transitions are used for training and testing, respectively.
<span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.7">ClipShots</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib16" title="">16</a>]</cite> is a dataset for shot boundary detection.
It contains videos from over 20 categories, ranging widely from movie spotlights to phone videos.
After removing shots with gradual transitions, e.g., fade, we leverage the remaining shots for pseudo-dataset generation. In total, there are 118658 camera transitions.
<span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.8">Condensed Movies</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib1" title="">1</a>]</cite> is proposed to understand the narrative structure of movies and contains key scenes from over 3K videos, providing 152893 camera transitions.
Shots from one episode of each of the four <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.9">Sitcoms</span>: Friends, How I Met Your Mother, The Big Bang Theory, and Two and a Half Men are used for pseudo dataset generation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Implementation Details.</span>
All models are trained for ten epochs, with a learning rate of 1e-5 and a batch size of 2. The mean and standard deviation of results from three seeds are reported for each setup.
TransNet V2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib14" title="">14</a>]</cite> is used for shot detection. TSN pre-trained on MovieShots <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib13" title="">13</a>]</cite> for 60 epochs, reaching a top-1 shot scale classification accuracy of 90.08%, is used for feature extraction. Specifically, features from the penultimate layer are used for shot clustering.
We remove videos with fewer than ten shot transitions, as transitions in these videos are likely from video effects but not actual camera transitions. We also discard shots with gradual transitions.
For evaluation, <span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.2">Classification accuracy</span> of the camera being switched to is used.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.4.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.5.2">Result</span>
</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">Baseline Comparison.</span>
A few works have been proposed for multi-camera view recommendation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib11" title="">11</a>]</cite>. Yet, neither their codes nor their datasets are publicly available.
We re-implement the Temporal and Contextual Transformer (TC Transformer) in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib12" title="">12</a>]</cite> as the primary baseline we compare to.
It achieves state-of-the-art (SoTA) classification accuracy in multi-camera view recommendation on TVMCE dataset.
Note that we remove the training data in the middle of a shot (without camera switches), which improves the accuracy (22.48 in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib12" title="">12</a>]</cite>) of TC Transformer to 28.77.
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S5.T1" title="TABLE I ‣ V-B Result ‣ V Experiment ‣ Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_tag">I</span></a>, the proposed framework outperforms TC Transformer by 11% (<span class="ltx_text" id="S5.SS2.p1.1.2" style="color:#000000;">31.83</span> vs. 28.77).
However, both models suffer when applied to a never-before-seen domain.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T1.2.2.2" style="padding-left:5.0pt;padding-right:5.0pt;">Method<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.m1.1.1" stretchy="false" xref="S5.T1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.m1.1d">↓</annotation></semantics></math> / Test Set<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.T1.2.2.2.m2.1"><semantics id="S5.T1.2.2.2.m2.1a"><mo id="S5.T1.2.2.2.m2.1.1" stretchy="false" xref="S5.T1.2.2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.m2.1b"><ci id="S5.T1.2.2.2.m2.1.1.cmml" xref="S5.T1.2.2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.2.m2.1d">→</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T1.2.2.3" style="padding-left:5.0pt;padding-right:5.0pt;">TVMCE (ID)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.2.2.4" style="padding-left:5.0pt;padding-right:5.0pt;">Sitcoms (OOD)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.2.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.2.3.1.1" style="padding-left:5.0pt;padding-right:5.0pt;">Random</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.2.3.1.2" style="padding-left:5.0pt;padding-right:5.0pt;">16.67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.2.3.1.3" style="padding-left:5.0pt;padding-right:5.0pt;">16.67</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.2.4.2.1" style="padding-left:5.0pt;padding-right:5.0pt;">TC Transformer</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.2.4.2.2" style="padding-left:5.0pt;padding-right:5.0pt;">28.77±0.86</td>
<td class="ltx_td ltx_align_center" id="S5.T1.2.4.2.3" style="padding-left:5.0pt;padding-right:5.0pt;">14.04±1.59</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T1.2.5.3.1" style="padding-left:5.0pt;padding-right:5.0pt;">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T1.2.5.3.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.2.5.3.2.1" style="color:#000000;">31.83±0.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.2.5.3.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T1.2.5.3.3.1">22.65±2.43</span></td>
</tr>
</tbody>
</table>
<br class="ltx_break ltx_centering"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>
<span class="ltx_text ltx_font_bold" id="S5.T1.4.1">Comparison to Baseline.</span>
Multi-camera view recommendation models generalize poorly to a never-before-seen domain.
ID: in-Domain, OOD: out-of-Domain.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Generalizability &amp; Pseudo Dataset Efficacy.</span>
We investigate domain differences from two perspectives: (1) <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.2">video scene</span>, and (2) <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.3">video type</span>. Video scene is where the videos are filmed, e.g., on stage or in a livingroom. Video type is how the contents are presented, e.g., a live performance or a movie.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text" id="S5.SS2.p3.1.1" style="color:#000000;">First, we fix video type and examine the impact of video scene.</span>
Concretely, we split the TVMCE training set into two roughly equal-sized subsets based on the video scenes.
One subset, <span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.2">TVMCE_stage</span>, similar to the test set, consists of videos of stages and concert halls. The other subset, <span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.3">TVMCE_sport</span>, contains sports videos.
In Table <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S5.T2" title="TABLE II ‣ V-B Result ‣ V Experiment ‣ Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_tag">II</span></a>, the model trained on the dissimilar scenes achieves significantly lower accuracy than one trained on the same scenes as the test set.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">Next, we investigate the impact of both video scenes and video types.
In this experiment, all models are evaluated on <span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Sitcoms</span>, consisting of videos that differ in both scenes and types from those in TVMCE.
Specifically, video scenes in TVMCE are mostly stages and sports, whereas video scenes in Sitcoms are kitchens and living rooms. Also, TVMCE contains recorded live performances, whereas Sitcoms contains movies.
Three models are evaluated, each trained on a dataset with different degrees of domain difference to Sitcoms. (1) TVMCE that differs in both video type and scenes, (2) ClipShots differs only in video type, and (3) Condensed Movies that covers the same video type and scenes.
Two observations can be made in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S5.T3" title="TABLE III ‣ V-B Result ‣ V Experiment ‣ Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_tag">III</span></a>.
(1) Pseudo datasets bridge the data scarcity and improve the accuracy in the target domain.
(2) Video type and scene can compound to impact performance. TVMCE, different in both video type and scenes from Sitcoms, performs the worst.
On the contrary, Condensed Movies with the same video type and scenes as Sitcoms achieves the best performance.</p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">Better accuracy of the model trained on pseudo multi-camera editing datasets — ClipShots in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S5.T2" title="TABLE II ‣ V-B Result ‣ V Experiment ‣ Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_tag">II</span></a>, and ClipShots and Condensed Movies in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#S5.T3" title="TABLE III ‣ V-B Result ‣ V Experiment ‣ Pseudo Dataset Generation for Out-of-domain Multi-Camera View Recommendation"><span class="ltx_text ltx_ref_tag">III</span></a> demonstrate their efficacy in improving model performance in the target domain without labeling by professionals.</p>
</div>
<div class="ltx_para" id="S5.SS2.p6">
<p class="ltx_p" id="S5.SS2.p6.1"><span class="ltx_text" id="S5.SS2.p6.1.1" style="color:#000000;">Note that previous work also leverages pseudo labels for evaluation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.13585v1#bib.bib8" title="">8</a>]</cite>; still, an important question is whether pseudo-labeled datasets reflect the performance of human-labeled ones.
We treat the TVMCE test set as unlabeled, construct a pseudo-TVMCE test set, and evaluate the model trained with sports scenes on this set.
The accuracy is similar to the TVMCE test set (23.71±0.12 vs. 25.56±0.87).
</span></p>
</div>
<figure class="ltx_table" id="S5.T2">
<p class="ltx_p ltx_align_center" id="S5.T2.1"><span class="ltx_text" id="S5.T2.1.1" style="font-size:90%;">TVMCE test set — video scenes: <span class="ltx_text" id="S5.T2.1.1.1" style="color:#00E000;">stage, concert hall</span></span>
<br class="ltx_break"/>
<span class="ltx_tabular ltx_align_middle" id="S5.T2.1.2">
<span class="ltx_tbody">
<span class="ltx_tr" id="S5.T2.1.2.1.1">
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T2.1.2.1.1.1" style="padding-left:10.0pt;padding-right:10.0pt;">Train Set</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T2.1.2.1.1.2" style="padding-left:10.0pt;padding-right:10.0pt;">Video Scene</span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.1.2.1.1.3" style="padding-left:10.0pt;padding-right:10.0pt;">Accuracy</span></span>
<span class="ltx_tr" id="S5.T2.1.2.2.2">
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.2.2.2.1" style="padding-left:10.0pt;padding-right:10.0pt;">TVMCE_stage</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.1.2.2.2.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text" id="S5.T2.1.2.2.2.2.1" style="color:#00E000;">stage, concert hall</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2.2.3" style="padding-left:10.0pt;padding-right:10.0pt;">31.83±0.83</span></span>
<span class="ltx_tr" id="S5.T2.1.2.3.3">
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.2.3.3.1" style="padding-left:10.0pt;padding-right:10.0pt;">TVMCE_sport</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.1.2.3.3.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text" id="S5.T2.1.2.3.3.2.1" style="color:#FF0000;">sports</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.2.3.3.3" style="padding-left:10.0pt;padding-right:10.0pt;">25.56±0.87</span></span>
<span class="ltx_tr" id="S5.T2.1.2.4.4">
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt" id="S5.T2.1.2.4.4.1" style="padding-left:10.0pt;padding-right:10.0pt;">ClipShots <span class="ltx_text ltx_font_bold" id="S5.T2.1.2.4.4.1.1">(Pseudo)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt" id="S5.T2.1.2.4.4.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text" id="S5.T2.1.2.4.4.2.1" style="color:#00E000;">broad<sup class="ltx_sup" id="S5.T2.1.2.4.4.2.1.1">1</sup></span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S5.T2.1.2.4.4.3" style="padding-left:10.0pt;padding-right:10.0pt;">26.81±0.22</span></span>
</span>
</span>
<br class="ltx_break"/><sup class="ltx_sup" id="S5.T2.1.3"><span class="ltx_text" id="S5.T2.1.3.1" style="font-size:80%;">1</span></sup><span class="ltx_text" id="S5.T2.1.4" style="font-size:80%;">consists of videos from a broad scene coverage.</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>
<span class="ltx_text ltx_font_bold" id="S5.T2.6.1">Impact of Video Scene.</span> The model trained in different scenes to the test set achieves lower accuracy.
<span class="ltx_text" id="S5.T2.7.2" style="color:#00E000;">Green</span>, and <span class="ltx_text" id="S5.T2.8.3" style="color:#FF0000;">red</span> means same and different.
<span class="ltx_text ltx_font_bold" id="S5.T2.9.4">[Best viewed in color.]</span>
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p7">
<p class="ltx_p" id="S5.SS2.p7.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p7.1.1">Pseudo Dataset Construction Methodology.</span>
<span class="ltx_text" id="S5.SS2.p7.1.2" style="color:#000000;">Three methodologies are investigated: (1) selecting the most visually similar shot from each cluster, (2) selecting one random shot from each cluster, and (3) selecting the five most similar shots <span class="ltx_text ltx_font_italic" id="S5.SS2.p7.1.2.1">without clustering</span>.
On the TVMCE test set, the three models achieve an accuracy of 26.81±0.22, 7.16±0.40, and 26.02±0.49, respectively.</span>
The discrepancy between (1) and (2) shows the importance of selecting the shots that are most visually similar to the candidates.
We conjecture that the ground-truth succeeding shots tend to have a certain amount of overlap in viewpoint with their predecessors, and randomly selected shots with little visual overlap to the previous shot could be quickly ruled out by the model, leading to ineffective training.
The difference between (1) and (3) shows that while clustering cannot recover the actual cameras, it can be used to obtain hard negatives, thus facilitating model training.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.T3.1"><span class="ltx_text" id="S5.T3.1.1" style="font-size:90%;">Test set — video type: <span class="ltx_text" id="S5.T3.1.1.1" style="color:#00E000;">movie</span>, video scenes: <span class="ltx_text" id="S5.T3.1.1.2" style="color:#00E000;">living room, kitchen</span></span>
<br class="ltx_break"/>
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.1.2">
<span class="ltx_thead">
<span class="ltx_tr" id="S5.T3.1.2.1.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T3.1.2.1.1.1">Train Set</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T3.1.2.1.1.2">Video Type</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T3.1.2.1.1.3">Video Scene</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.2.1.1.4">Accuracy</span></span>
<span class="ltx_tr" id="S5.T3.1.2.2.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.2.2.2.1">TVMCE</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.2.2.2.2"><span class="ltx_text" id="S5.T3.1.2.2.2.2.1" style="color:#FF0000;">live</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.1.2.2.2.3"><span class="ltx_text" id="S5.T3.1.2.2.2.3.1" style="color:#FF0000;">stages, sports</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.2.2.2.4">22.65±2.43</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S5.T3.1.2.3.1">
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.2.3.1.1">ClipShots <span class="ltx_text ltx_font_bold" id="S5.T3.1.2.3.1.1.1">(Pseudo)</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.2.3.1.2"><span class="ltx_text" id="S5.T3.1.2.3.1.2.1" style="color:#FF8000;">mixed<sup class="ltx_sup" id="S5.T3.1.2.3.1.2.1.1">1</sup></span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.2.3.1.3"><span class="ltx_text" id="S5.T3.1.2.3.1.3.1" style="color:#00E000;">broad<sup class="ltx_sup" id="S5.T3.1.2.3.1.3.1.1">2</sup></span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.3.1.4">27.61±1.20</span></span>
<span class="ltx_tr" id="S5.T3.1.2.4.2">
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.1.2.4.2.1">Condensed <span class="ltx_text ltx_font_bold" id="S5.T3.1.2.4.2.1.1">(Pseudo)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.1.2.4.2.2"><span class="ltx_text" id="S5.T3.1.2.4.2.2.1" style="color:#00E000;">movie</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.1.2.4.2.3"><span class="ltx_text" id="S5.T3.1.2.4.2.3.1" style="color:#00E000;">broad<sup class="ltx_sup" id="S5.T3.1.2.4.2.3.1.1">2</sup></span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.2.4.2.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.2.4.2.4.1">38.14±1.50</span></span></span>
</span>
</span>
<br class="ltx_break"/><sup class="ltx_sup" id="S5.T3.1.3"><span class="ltx_text" id="S5.T3.1.3.1" style="font-size:80%;">1</span></sup><span class="ltx_text" id="S5.T3.1.4" style="font-size:80%;">consists of videos of movies and live performance.</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.T3.2"><sup class="ltx_sup" id="S5.T3.2.1"><span class="ltx_text" id="S5.T3.2.1.1" style="font-size:80%;">2</span></sup><span class="ltx_text" id="S5.T3.2.2" style="font-size:80%;">consists of videos from a broad scene coverage.</span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">TABLE III: </span>
<span class="ltx_text ltx_font_bold" id="S5.T3.15.1">Impact of Video Scene and Type (Sitcoms).</span>
A more significant domain difference (video scenes + video types) severely impacts accuracy.
Pseudo datasets from a broad range of videos could cover the target video scenes and types, achieving better accuracy.
<span class="ltx_text" id="S5.T3.16.2" style="color:#00E000;">Green</span>, <span class="ltx_text" id="S5.T3.17.3" style="color:#FF8000;">orange</span>, and <span class="ltx_text" id="S5.T3.18.4" style="color:#FF0000;">red</span> means same, covered, and different.
<span class="ltx_text ltx_font_bold" id="S5.T3.19.5">[Best viewed in color.]</span>
</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Discussion</span>
</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1"><span class="ltx_text ltx_font_bold" id="S6.p1.1.1">Application Scenarios.</span>
Based on the experiment results, we divide multi-camera view recommendation into three scenarios.
(1) In a controlled environment where decisions made by professionals for similar events are accessible.
For example, in soccer games, the camera transition patterns are relatively straightforward, and multi-camera videos are available alongside the final broadcast.
In this scenario, supervised learning on the professional-labeled datasets would suffice.
(2) If the model were to be applied to domains where decisions made previously by experts are unavailable, we would suggest generating pseudo datasets on the target domains with off-the-shelf regular videos for training.
(3) If one does not know the explicit target domains and wants to apply the model in the wild, the best strategy would be to collect videos from extensive and different domains and train the model on pseudo datasets transformed from these videos. This would increase the probability of the pseudo datasets covering the target domains.
We note that the proposed pseudo multi-camera editing dataset <span class="ltx_text ltx_font_italic" id="S6.p1.1.2">does not solve the <span class="ltx_text" id="S6.p1.1.2.1" style="color:#000000;">innate</span> domain gap issue</span> once and for all but mitigates it through the availability of regular videos in an arbitrary domain.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we first show that multi-camera view recommendation models struggle to generalize to never-before-seen domains. We then analyze two aspects that could compound and intensify domain mismatch: video scenes and video types.
We propose to leverage regular videos in the target domain to generate pseudo multi-camera editing datasets. We also design a learning framework that optimizes the model parameters with contrastive loss to bring close the current and the succeeding ground-truth shots.
Experiments demonstrate the efficacy of the proposed pseudo multi-camera editing dataset in improving the model’s accuracy in the target domain.
</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Future work involves designing a better training objective to capture the underlying cinematography expertise, which enables professionals to work across video scenes and types.
Another potential direction is to incorporate explicit rules of thumb and conventions in cinematography, e.g., the gradual change from wider to narrower shots and the rule of thirds.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Acknowledgement</span>
</h2>
<div class="ltx_para ltx_noindent" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">This work was funded by National Science Foundation grants NSF CNS 19-00875, NSF CCF 22-17144, NSF CNS 21-06592. Any results and opinions are our own and do not represent views of National Science Foundation.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Max Bain, Arsha Nagrani, Andrew Brown, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Condensed movies: Story based retrieval with contextual embeddings.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">ACCV</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Saugata Biswas, Ernst Kruijff, and Eduardo Veas.

</span>
<span class="ltx_bibblock">View recommendation for multi-camera demonstration-based training.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Multimedia Tools and Applications</span>, 83(7):21765–21800, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Jianhui Chen, Keyu Lu, Sijia Tian, and Jim Little.

</span>
<span class="ltx_bibblock">Learning sports camera selection from internet videos.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">2019 IEEE Winter Conference on Applications of Computer
Vision (WACV)</span>, pages 1682–1691. IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">A simple framework for contrastive learning of visual
representations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">International conference on machine learning</span>, pages
1597–1607. PMLR, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">CVPR</span>, pages 248–255. Ieee, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.

</span>
<span class="ltx_bibblock">Momentum contrast for unsupervised visual representation learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, June 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">CVPR</span>, pages 770–778, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Panwen Hu, Nan Xiao, Feifei Li, Yongquan Chen, and Rui Huang.

</span>
<span class="ltx_bibblock">A reinforcement learning-based automatic video editing method using
pre-trained vision-language model.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 31st ACM International Conference on
Multimedia</span>, pages 6441–6450, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted
windows.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">ICCV</span>, pages 10012–10022, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Aaron van den Oord, Yazhe Li, and Oriol Vinyals.

</span>
<span class="ltx_bibblock">Representation learning with contrastive predictive coding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:1807.03748</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Yingwei Pan, Yue Chen, Qian Bao, Ning Zhang, Ting Yao, Jingen Liu, and Tao Mei.

</span>
<span class="ltx_bibblock">Smart director: An event-driven directing system for live
broadcasting.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">ACM Transactions on Multimedia Computing, Communications, and
Applications (TOMM)</span>, 17(4):1–18, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Anyi Rao, Xuekun Jiang, Sichen Wang, Yuwei Guo, Zihao Liu, Bo Dai, Long Pang,
Xiaoyu Wu, Dahua Lin, and Libiao Jin.

</span>
<span class="ltx_bibblock">Temporal and contextual transformer for multi-camera editing of tv
shows.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2210.08737</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Anyi Rao, Jiaze Wang, Linning Xu, Xuekun Jiang, Qingqiu Huang, Bolei Zhou, and
Dahua Lin.

</span>
<span class="ltx_bibblock">A unified framework for shot type classification based on subject
centric lens.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">ECCV</span>, pages 17–34. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Tomáš Souček and Jakub Lokoč.

</span>
<span class="ltx_bibblock">Transnet v2: An effective deep network architecture for fast shot
transition detection.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2008.04838</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Eckhard Stoll, Stephan Breide, Steve Göring, and Alexander Raake.

</span>
<span class="ltx_bibblock">Automatic camera selection, shot size and video editing in theater
multi-camera recordings.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">IEEE Access</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Shitao Tang, Litong Feng, Zhanghui Kuang, Yimin Chen, and Wei Zhang.

</span>
<span class="ltx_bibblock">Fast video shot transition localization with deep structured models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">ACCV</span>, pages 577–592. Springer, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">NIPS</span>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc
Van Gool.

</span>
<span class="ltx_bibblock">Temporal segment networks: Towards good practices for deep action
recognition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">ECCV</span>, pages 20–36. Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Xueting Wang, Yuki Muramatu, Takatsugu Hirayama, and Kenji Mase.

</span>
<span class="ltx_bibblock">Context-dependent viewpoint sequence recommendation system for
multi-view video.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">2014 IEEE International Symposium on Multimedia</span>, pages
195–202. IEEE, 2014.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Oct 16 22:24:32 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
